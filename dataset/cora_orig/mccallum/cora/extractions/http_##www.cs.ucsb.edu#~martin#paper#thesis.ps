URL: http://www.cs.ucsb.edu/~martin/paper/thesis.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Title: THE DESIGN, IMPLEMENTATION AND EVALUATION OF JADE: A PORTABLE, IMPLICITLY PARALLEL PROGRAMMING LANGUAGE  
Author: Martin C. Rinard 
Degree: A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY By  
Date: September 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The only CHAPTER 2. THE JADE LANGUAGE 20 int shared s; proc (int shared *p) - int shared *lp; int local rd *rp; lp = (&s /* shared pointer */); rp = (&(p [5]) /* local rd pointer */); lp = &(p <ref> [1] </ref>); /* illegal - lp is a shared pointer, &(p [1]) is a local pointer */ *(p + 2 /* local wr pointer */) = 4; - other way to get a shared pointer is to apply the & operator to a shared global variable. <p> THE JADE LANGUAGE 20 int shared s; proc (int shared *p) - int shared *lp; int local rd *rp; lp = (&s /* shared pointer */); rp = (&(p [5]) /* local rd pointer */); lp = &(p <ref> [1] </ref>); /* illegal - lp is a shared pointer, &(p [1]) is a local pointer */ *(p + 2 /* local wr pointer */) = 4; - other way to get a shared pointer is to apply the & operator to a shared global variable. <p> Asynchronous sends return immediately, with the system invisibly buffering the data until the corresponding receive comes along. Actor languages <ref> [1, 2, 3] </ref>, PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs.
Reference: [2] <author> G. Agha. </author> <title> Concurrent object oriented programming. </title> <journal> Communications of the ACM, </journal> <volume> 33(9) </volume> <pages> 125-141, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Asynchronous sends return immediately, with the system invisibly buffering the data until the corresponding receive comes along. Actor languages <ref> [1, 2, 3] </ref>, PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs.
Reference: [3] <author> G. Agha and C. Hewitt. </author> <title> Concurrent programming using Actors. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object Oriented Concurrent Programming, </booktitle> <pages> pages 37-53. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1987. </year>
Reference-contexts: Asynchronous sends return immediately, with the system invisibly buffering the data until the corresponding receive comes along. Actor languages <ref> [1, 2, 3] </ref>, PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs.
Reference: [4] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The programmer may help the system analyze the program by providing information about how the program structures and accesses data, or make policy decisions about the distribution of tasks and data to processors and memories. 5.2.2.1 Parallelizing Compilers Parallelizing compilers <ref> [10, 78, 79, 4] </ref> statically analyze programs to find independent pieces of code. The compiler then generates a parallel program that executes the independent pieces of code concurrently.
Reference: [5] <author> P. America. POOL-T: </author> <title> A parallel object-oriented language. </title> <editor> In A. Yonezawa and M. Tokoro, editors, </editor> <booktitle> Object Oriented Concurrent Programming, </booktitle> <pages> pages 199-220. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1987. </year>
Reference-contexts: The create object and create at object constructs return shared pointers. The only CHAPTER 2. THE JADE LANGUAGE 20 int shared s; proc (int shared *p) - int shared *lp; int local rd *rp; lp = (&s /* shared pointer */); rp = (&(p <ref> [5] </ref>) /* local rd pointer */); lp = &(p [1]); /* illegal - lp is a shared pointer, &(p [1]) is a local pointer */ *(p + 2 /* local wr pointer */) = 4; - other way to get a shared pointer is to apply the & operator to a <p> Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. Concurrent object-oriented languages like COOL [30], Orca [9], POOL-T <ref> [5] </ref> and ABCL/1 [136] hide such primitives behind higher-level constructs. 5.2 Serial Semantics Much of the complexity of parallel execution comes from the fact that parallel tasks can generate many different interleavings of the basic operations, with each interleaving generating a potentially different behavior. <p> These languages augment the semantics of the basic operations in object-oriented programming languages to include parallel execution and synchronization. We first discuss how researchers have augmented the object-oriented model of computation to include the generation of parallel execution. In languages like POOL-T <ref> [5] </ref>, objects are given threads of control which execute concurrently. Languages like ABCL/1 [136] and COOL [30] support the concept of asynchronous methods, which execute concurrently with the invoking thread. The computation can synchronize for the return value using a mechanism similar to MultiLisp futures.
Reference: [6] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Another unsolved problem is determining how to coordinate the mapping of data and computation to memory modules and processors <ref> [6] </ref>. For good performance, the compiler CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 190 must evenly balance the computational load and avoid uncoordinated mappings that generate excessive communication as processors repeatedly fetch remote data. The problem becomes especially complex when different parts of the program access data in different ways.
Reference: [7] <author> A. Appel and K. Li. </author> <title> Virtual memory primitives for user programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> 227 BIBLIOGRAPHY 228 </note>
Reference-contexts: While each of these strategies addresses a fundamental shortcoming of the current Jade communication strategy, they all have drawbacks. Page-based approaches require the implementation to interact with the paging system of the resident operating system. In many operating systems the implementations of the user-level fault handling primitives are inefficient <ref> [7] </ref>, and some operating systems do not provide these primitives at all. On the other hand, using a strategy that dynamically checked each access would impose substantial overhead on each access to a shared object. <p> More recent systems [72, 15] ameliorate this problem by allowing different processors to concurrently write disjoint regions of the same page. Another potential performance problem is the substantial exception handling overhead that operating systems typically impose <ref> [7] </ref>. Page-based systems also interact with the application at the level of the raw address CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 211 space. Each parallel program must have the same address space on all the different machines on which it executes. <p> The extra structure inherent in the Jade object model would allow the implementation to extend techniques from page-based systems for use in heterogeneous computing environments. The only restriction is that the operating system must support user-level paging operations similar to those described in <ref> [7] </ref>. The basic idea is that the Jade object model gives the implementation a machine-independent index space for each shared object. This index space consists of the object's global identifier and the indices of object's elements.
Reference: [8] <author> Arvind and R. Thomas. I-structures: </author> <title> An efficient data type for functional languages. </title> <type> Technical Report MIT/LCS/TM-210, </type> <institution> MIT, </institution> <year> 1981. </year>
Reference-contexts: It is often convenient to generate the individual values that together comprise a large aggregate at different points in the program's execution. Id [101] provides for this functionality via I-structures <ref> [8, 102] </ref>. I-structures contain write-once elements that CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 196 are initially undefined. The program can separately define each element; when an operation attempts to use an undefined element it suspends until it is defined.
Reference: [9] <author> H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. Concurrent object-oriented languages like COOL [30], Orca <ref> [9] </ref>, POOL-T [5] and ABCL/1 [136] hide such primitives behind higher-level constructs. 5.2 Serial Semantics Much of the complexity of parallel execution comes from the fact that parallel tasks can generate many different interleavings of the basic operations, with each interleaving generating a potentially different behavior. <p> We next consider issues associated with enforcing a pure object model. We say that an object-oriented language has a pure object model if the only way to access an object is to invoke a method with the object as the receiver. Orca, for example, enforces a pure object model <ref> [9] </ref>. A pure object model increases the security and modularity of programs by enforcing object encapsulation. It also simplifies the implementation of the language on message-passing platforms. The implementation first distributes the objects across the machine.
Reference: [10] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program paral-lelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Figure 2.2 contains the declaration of a pointer to a shared pointer to a shared double and the declaration of a shared structure which contains an array of pointers to shared vectors of doubles. CHAPTER 2. THE JADE LANGUAGE 14 double shared x; double shared A <ref> [10] </ref>; struct - int i, j, k; double d; shared s; double shared *p; double shared * shared *q; struct - int n, m; double shared *data [N]; shared s; It is also possible for shared objects to contain function pointers. <p> The programmer may help the system analyze the program by providing information about how the program structures and accesses data, or make policy decisions about the distribution of tasks and data to processors and memories. 5.2.2.1 Parallelizing Compilers Parallelizing compilers <ref> [10, 78, 79, 4] </ref> statically analyze programs to find independent pieces of code. The compiler then generates a parallel program that executes the independent pieces of code concurrently.
Reference: [11] <author> P. Barth, R. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: These sources of overhead impose an ever-present performance penalty on the basic form of execution. We discuss these performance problems in Sections 5.3.4.1, 5.3.4.2 and 5.3.4.3. There are also expressiveness problems associated with monotonic languages. As described in <ref> [11] </ref>, the lack of mutable data can force programmers to tediously thread state through multiple layers of function calls. Updating a single variable can force the explicit regeneration of large linked data structures. <p> Id's M-structures <ref> [11] </ref> allow programmers to build data structures that are implicitly synchronized at the level of the individual words of memory. An M-structure is a word of memory augmented with a bit indicating if the word is empty or full. The program reads an M-structure with the take operation.
Reference: [12] <author> F. Baskett, T. Jermoluk, and D. Solomon. </author> <title> The 4D-MP graphics superworkstation: Computing + graphics = 40 MIPS + 40 MFLOPS + 100,000 lighted polygons per second. </title> <booktitle> In Proceedings of COMPCON Spring 88, </booktitle> <pages> pages 468-471, </pages> <year> 1988. </year>
Reference-contexts: We have demonstrated the viability and applicability of these algorithms by implementing Jade on many different computational platforms. Jade implementations currently exist for shared-memory machines such as the Stanford DASH machine [82] and the Silicon Graphics 4D/340 <ref> [12] </ref>, for message-passing machines such as the Intel iPSC/860 [16], and for heterogeneous networks of workstations. While no implementation currently exists for shared-memory machines with incoherent caches such as the Cray T3D, it would be possible to implement Jade on such machines. 60 CHAPTER 3.
Reference: [13] <author> BBN, </author> <title> Cambridge, MA. Butterfly Parallel Processor Overview, </title> <year> 1985. </year>
Reference-contexts: issues associated with each of the following kinds of memory systems: 1) bus-based systems with a single memory module and per-processor caches (like the Silicon Graphics 4D/340), 2) distributed-memory systems with multiple memory modules and caches (like the Stanford DASH machine), 3) distributed-memory systems without caches (like the BBN Butterfly <ref> [13] </ref>) and 4) cache-only-memory systems (like the KSR1 [73]). 3.3.3.1 Bus-Based Systems Bus-based systems have a roughly uniform access time from any processor to the main memory. The access time for cached data is typically significantly lower than the access time to main memory.
Reference: [14] <author> J. Bennett, J. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: We next discuss three different approaches to implementing the shared memory in software: the page-based approach, the region-based approach and the object-based approach. Page-based systems such as Ivy [85], Munin <ref> [15, 14, 29] </ref> and Treadmarks [72] use the virtual-to-physical address-translation hardware to implement a cache consistency protocol at the granularity of pages. The translation hardware is used to detect accesses to remote pages, and the fault handler generates messages that move or copy the required pages from remote processors.
Reference: [15] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Access specifications give the implementation advance notice of precisely which objects a task will access. The implementation can exploit this information to apply communication optimizations such as concurrently fetching multiple remote objects for each task. Many other parallel systems <ref> [85, 15, 19, 44] </ref> only discover when tasks will access data as the tasks actually perform the access, and lack the advance information required to apply sophisticated communication optimizations. <p> We next discuss three different approaches to implementing the shared memory in software: the page-based approach, the region-based approach and the object-based approach. Page-based systems such as Ivy [85], Munin <ref> [15, 14, 29] </ref> and Treadmarks [72] use the virtual-to-physical address-translation hardware to implement a cache consistency protocol at the granularity of pages. The translation hardware is used to detect accesses to remote pages, and the fault handler generates messages that move or copy the required pages from remote processors. <p> A drawback of page-based systems is that the relatively large size of the pages increases the probability of an application suffering from excessive communication caused by false sharing (when multiple processors repeatedly access disjoint regions of a single page in conflicting ways). More recent systems <ref> [72, 15] </ref> ameliorate this problem by allowing different processors to concurrently write disjoint regions of the same page. Another potential performance problem is the substantial exception handling overhead that operating systems typically impose [7]. Page-based systems also interact with the application at the level of the raw address CHAPTER 5.
Reference: [16] <author> R. Berrendorf and J. Helin. </author> <title> Evaluating the basic performance of the Intel iPSC/860 parallel computer. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 4(3) </volume> <pages> 223-240, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We have demonstrated the viability and applicability of these algorithms by implementing Jade on many different computational platforms. Jade implementations currently exist for shared-memory machines such as the Stanford DASH machine [82] and the Silicon Graphics 4D/340 [12], for message-passing machines such as the Intel iPSC/860 <ref> [16] </ref>, and for heterogeneous networks of workstations. While no implementation currently exists for shared-memory machines with incoherent caches such as the Cray T3D, it would be possible to implement Jade on such machines. 60 CHAPTER 3. <p> One clear advantage of message-passing systems is that they present a simple performance model. Every remote interaction is cleanly identified in the program and a simple model accurately predicts the cost of each message transfer <ref> [16] </ref>. It is possible to understand the performance of the remaining serial code using a standard uniprocessor performance model. This simple model makes it much easier to tune the performance of a parallel computation. <p> In Ivy and Munin the unit of transfer is a page; both Midway and SAM rely on the programmer to aggregate the words of memory into coarser-granularity communication units. The motivation for larger-granularity communication is the significant overhead associated with each message <ref> [16] </ref>. Sending a few large messages instead of many small messages reduces the performance impact of this overhead. 5.4.2.5 Synchronization Mechanisms The performance characteristics of message-passing systems have also influenced the design of the provided synchronization mechanisms.
Reference: [17] <author> B. Bershad, E. Lazowska, and H. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year> <note> BIBLIOGRAPHY 229 </note>
Reference-contexts: The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto <ref> [17] </ref> and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> The resulting systems minimize the number of messages required for synchronized access to blocks of shared data and expose the inherent asynchrony of the communication in the programming model. 5.4.2.1 Basic Synchronization Packages Packages such as Presto <ref> [17] </ref> and the ANL macro package [88] and languages such as Modula-3 [98] rely on the hardware to implement the shared memory. The software only provides basic concurrency generation and synchronization primitives such as locks, barriers and condition variables. Such a bare-bones interface minimizes the safety of the programming model.
Reference: [18] <author> B. Bershad and M. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The entry consistency protocol of Midway implements the shared address space at the granularity of program-defined regions <ref> [18, 19] </ref>. The program can define synchronization objects and associate regions of the shared memory with these objects. When the program needs to access part of a region of memory, it acquires a read or write lock on the associated synchronization object.
Reference: [19] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of COMPCON'93, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Access specifications give the implementation advance notice of precisely which objects a task will access. The implementation can exploit this information to apply communication optimizations such as concurrently fetching multiple remote objects for each task. Many other parallel systems <ref> [85, 15, 19, 44] </ref> only discover when tasks will access data as the tasks actually perform the access, and lack the advance information required to apply sophisticated communication optimizations. <p> The entry consistency protocol of Midway implements the shared address space at the granularity of program-defined regions <ref> [18, 19] </ref>. The program can define synchronization objects and associate regions of the shared memory with these objects. When the program needs to access part of a region of memory, it acquires a read or write lock on the associated synchronization object. <p> Future languages and systems will be increasingly organized around the interaction of data and computation, with various declarative mechanisms such as access specifications used to express the relevant information. COOL's locality hints [30], Midway's object usage declarations <ref> [19] </ref>, shared region declarations [118] and the CHICO model of consistency [61] are all examples of this trend. Access specifications give the implementation enough information to automatically generate the communication without forcing the implementation to use a specific communication mechanism.
Reference: [20] <author> A. Black, N. Hutchison, E. Jul, and H. Levy. </author> <title> Object structure in the Emerald system. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <year> 1986. </year>
Reference-contexts: The implementation first distributes the objects across the machine. When a task invokes a method on a remote object, the implementation can either perform a remote procedure call [99] to execute the method remotely as in the Emerald system <ref> [20, 71] </ref> or transfer the object to the invoking processor and execute the method locally. The drawback is that a pure object model can interact with the language's synchronization mechanisms to limit its flexibility.
Reference: [21] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: For each application we briefly summarize the computation that it performs, then describe how we acquired and parallelized the application. * Water A program that evaluates forces and potentials in a system of water molecules in the liquid state. Water is derived from the Perfect Club benchmark MDG <ref> [21] </ref> and performs the same computation. The SPLASH benchmark suite [128] also contains a version of Water. <p> The compiler then generates a parallel program that executes the independent pieces of code concurrently. In message-passing environments the compiler also maps the data onto the processors and generates the communication operations required to transfer remote data to accessing processors. Automatically parallelizing serial programs is an extremely challenging task <ref> [21] </ref>. Despite years of research in this area, several important problems remain to be solved before it is practical to use a compiler to automatically exploit the concurrency in parallelizable computations. <p> Experience manually applying automatable techniques across multiple procedure boundaries suggests that it is possible for a compiler to successfully parallelize the computation if it increases its scope to include large sections of the program <ref> [53, 21] </ref>. For irregular programs there are problems performing dependence analysis that is precise enough to expose the concurrency.
Reference: [22] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: No program should be able to use its network access to interfere with other programs. The Thinking Machines CM-5 [103] provides this functionality by imposing a strict gang scheduling regime. Different programs cannot interfere because they cannot concurrently access the network. More recent systems <ref> [65, 22] </ref> perform protection checks in hardware on an attached co-processor, which enables the operating system to schedule each compute processor independently. The next step is a redesign of the interface that the message-passing system presents to the user program.
Reference: [23] <author> P. Brinch-Hansen. </author> <title> The programming language Concurrent Pascal. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):199-207, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: PARALLEL PROGRAMMING SYSTEMS 208 objects. A basic mechanism is that each method executes with exclusive access to the receiver object. This implicit form of mutual exclusion synchronization originally appeared in the context of monitors [62], and was used in monitor-based languages such as Concurrent Pascal <ref> [23, 24] </ref> and Mesa [81, 95]. Concurrent object-oriented languages also adopt this synchronization mechanism, in part because it meshes well with the concept of a method operating on an object. Many languages relax the exclusive execution constraint to allow the concurrent execution of methods that read the same object.
Reference: [24] <author> P. Brinch-Hansen. </author> <title> The Architecture of Concurrent Programs. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1977. </year>
Reference-contexts: PARALLEL PROGRAMMING SYSTEMS 208 objects. A basic mechanism is that each method executes with exclusive access to the receiver object. This implicit form of mutual exclusion synchronization originally appeared in the context of monitors [62], and was used in monitor-based languages such as Concurrent Pascal <ref> [23, 24] </ref> and Mesa [81, 95]. Concurrent object-oriented languages also adopt this synchronization mechanism, in part because it meshes well with the concept of a method operating on an object. Many languages relax the exclusive execution constraint to allow the concurrent execution of methods that read the same object.
Reference: [25] <author> R. Browning, T. Li, B. Chui, J. Ye, R. Pease, Z. Czyzewski, and D. Joy. </author> <title> Empirical forms for the electron/atom elastic scattering cross sections from 0.1-30keV. </title> <journal> Journal of Applied Physics, </journal> <volume> 76(4), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: I then developed the Jade version starting from that serial C version. The Jade version is the only parallel version of this application that exists. * Search A program that simulates the interaction of electron beams with solids <ref> [25, 26] </ref>. Jun Ye (a graduate student in the Stanford Electrical Engineering Department) developed an initial serial version of Search in C. <p> Each element of the difference array CHAPTER 4. APPLICATIONS EXPERIENCE 121 stores the running sum of the backprojected differences for the corresponding element of the velocity model. As in the Water application, the accumulations commute, associate and have an identity. 4.4.3 Search Search <ref> [25, 26] </ref> is a program from the Stanford Electrical Engineering department. It simulates the interaction of several electron beams at different energy levels with a variety of solids. It uses a Monte-Carlo technique to simulate the elastic scattering of each electron from the electron beam into the solid.
Reference: [26] <author> R. Browning, T. Li, B. Chui, J. Ye, R. Pease, Z. Czyzewski, and D. Joy. </author> <title> Monte-carlo calculations of electron/atom elastic scattering from 0.1-30keV. Scanning, </title> <note> 1994. To appear. </note>
Reference-contexts: I then developed the Jade version starting from that serial C version. The Jade version is the only parallel version of this application that exists. * Search A program that simulates the interaction of electron beams with solids <ref> [25, 26] </ref>. Jun Ye (a graduate student in the Stanford Electrical Engineering Department) developed an initial serial version of Search in C. <p> Each element of the difference array CHAPTER 4. APPLICATIONS EXPERIENCE 121 stores the running sum of the backprojected differences for the corresponding element of the velocity model. As in the Water application, the accumulations commute, associate and have an identity. 4.4.3 Search Search <ref> [25, 26] </ref> is a program from the Stanford Electrical Engineering department. It simulates the interaction of several electron beams at different energy levels with a variety of solids. It uses a Monte-Carlo technique to simulate the elastic scattering of each electron from the electron beam into the solid.
Reference: [27] <author> A. Burns. </author> <title> Programming in Occam 2. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year> <note> BIBLIOGRAPHY 230 </note>
Reference-contexts: The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam <ref> [86, 27] </ref>, CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam <ref> [86, 27] </ref>, CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [28] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The program writes an M-structure with the put operation. This operation writes a new value into the word, leaving it full. M-structures support the mutual exclusion constraints associated with atomically updating a piece of data. Linda <ref> [44, 28] </ref> provides the abstraction of a shared memory consisting of a collection of tuples. The program can insert, read or remove tuples from the tuple space.
Reference: [29] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We next discuss three different approaches to implementing the shared memory in software: the page-based approach, the region-based approach and the object-based approach. Page-based systems such as Ivy [85], Munin <ref> [15, 14, 29] </ref> and Treadmarks [72] use the virtual-to-physical address-translation hardware to implement a cache consistency protocol at the granularity of pages. The translation hardware is used to detect accesses to remote pages, and the fault handler generates messages that move or copy the required pages from remote processors.
Reference: [30] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We were also able to obtain performance numbers for two explicitly parallel versions of this computation: a version written in COOL <ref> [30] </ref> and a version written in the ANL macro package [113]. Although both of these versions perform better than the Jade versions, their performance is not spectacular. <p> Figure 4.46 contains the corresponding speedup curves. The maximum performance occurs in the mi.at version on 24 processors with a speedup of 12.7. We were also able to obtain performance results for an explicitly parallel version written in COOL <ref> [30] </ref>. The COOL version performs significantly better than the Jade versions for larger numbers of processors. We attribute the performance difference to serialized task management overhead in the Jade versions. We next explore the performance of the Jade versions. <p> Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. Concurrent object-oriented languages like COOL <ref> [30] </ref>, Orca [9], POOL-T [5] and ABCL/1 [136] hide such primitives behind higher-level constructs. 5.2 Serial Semantics Much of the complexity of parallel execution comes from the fact that parallel tasks can generate many different interleavings of the basic operations, with each interleaving generating a potentially different behavior. <p> We first discuss how researchers have augmented the object-oriented model of computation to include the generation of parallel execution. In languages like POOL-T [5], objects are given threads of control which execute concurrently. Languages like ABCL/1 [136] and COOL <ref> [30] </ref> support the concept of asynchronous methods, which execute concurrently with the invoking thread. The computation can synchronize for the return value using a mechanism similar to MultiLisp futures. The synchronization mechanisms are all oriented around how the program accesses CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 208 objects. <p> Future languages and systems will be increasingly organized around the interaction of data and computation, with various declarative mechanisms such as access specifications used to express the relevant information. COOL's locality hints <ref> [30] </ref>, Midway's object usage declarations [19], shared region declarations [118] and the CHICO model of consistency [61] are all examples of this trend. Access specifications give the implementation enough information to automatically generate the communication without forcing the implementation to use a specific communication mechanism.
Reference: [31] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <type> Technical Report Caltech-CS-TR-92-13, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The Strand programmer is put in the position of synchronizing multiple reads and writes to mutable data with operations designed to coordinate the production and use of information in a monotonic context. PCN [41, 39] and Compositional C++ <ref> [31] </ref> more fully integrate mutable data into the monotonic model of parallel computation. These languages provide both sequential and parallel composition operators and mutable and definitional (write once) data.
Reference: [32] <author> E. Cooper, S. Nettles, and I. Subramanian. </author> <title> Improving the performance of SML garbage collection using application specific virtual memory management. </title> <booktitle> In Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Because the volume of data accessed between collections is typically larger than the processor cache, the program exhibits poor locality. In fact, the memory system performance can be so bad that paging, and not poor processor cache locality, is the dominant memory system effect <ref> [32] </ref>. We discuss other strategies for eliminating memory management overhead in Section 5.3.4.3. 5.3.4.3 The Copy Problem The lack of mutable data means that programs written in monotonic languages typically generate more copying overhead than programs written in more conventional languages.
Reference: [33] <author> D. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: When programmers use the local pointer mechanism discussed in section 2.2.4, the implementation amortizes the lookup cost over many accesses via the local pointer. 3.5.3 Suppressing Excessive Task Creation An important issue associated with managing dynamic concurrency is the suppression of excessive task creation <ref> [33, 96] </ref>. Jade programs may be able to generate an extremely large number of tasks. Because unexecuted tasks consume memory, excessive task creation stresses the machine's memory system, degrading performance. In extreme cases the machine may be unable to successfully execute the program.
Reference: [34] <author> D. Culler, S. Goldstein, K. Schauser, and T. von Eicken. </author> <title> TAM A compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3) </volume> <pages> 347-370, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The lack of a statically derivable sequential execution order complicates the process of generating tasks large enough to profitably amortize the scheduling overhead. Research performed in the context of Id <ref> [125, 35, 34] </ref> attacks this problem with a two-level scheduling strategy. The compiler first statically partitions the program into threads. The run-time system then dynamically schedules related threads into larger-grain units called quanta.
Reference: [35] <author> D. Culler, K. Schauser, and T. von Eicken. </author> <title> Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The lack of a statically derivable sequential execution order complicates the process of generating tasks large enough to profitably amortize the scheduling overhead. Research performed in the context of Id <ref> [125, 35, 34] </ref> attacks this problem with a two-level scheduling strategy. The compiler first statically partitions the program into threads. The run-time system then dynamically schedules related threads into larger-grain units called quanta.
Reference: [36] <author> H. Dietz and D. Klappholz. </author> <title> Refined Fortran: Another sequential language for parallel programming. </title> <editor> In K. Hwang, S. M. Jacobs, and E. E. Swartzlander, editors, </editor> <booktitle> BIBLIOGRAPHY 231 Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 184-189, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The goal is to expose more concurrency by improving the precision of the compiler's dependence analysis. Refined C [76] and Refined Fortran <ref> [75, 36] </ref> allow programmers to create sets of variables that refer to disjoint regions of memory. When pieces of code access disjoint subsets of such variables, the compiler can statically verify that they can execute concurrently.
Reference: [37] <author> I. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: CHAPTER 4. APPLICATIONS EXPERIENCE 166 4.6.1 Panel Cholesky on the iPSC/860 Table 4.12 contains the execution times for the Panel Cholesky factorization on the iPSC/860. The timing runs factor the BCSSTK15 matrix from the Harwell-Boeing sparse matrix benchmark set <ref> [37] </ref>. The performance numbers only measure the actual numerical factorization, omitting an initial symbolic factorization phase.
Reference: [38] <author> J. Feo, D. Cann, and R. Oldehoeft. </author> <title> A report on the Sisal language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. Id [101], functional languages such as Haskell [67] and Sisal <ref> [38] </ref> and concurrent logic programming languages such as Strand [40] and Parlog [50] are in this group. The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory.
Reference: [39] <author> I. Foster, R. Olson, and S. Tuecke. </author> <title> Productive parallel programming: The PCN approach. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 51-66, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: In this case there is a fundamental mismatch between the two models of computation. The Strand programmer is put in the position of synchronizing multiple reads and writes to mutable data with operations designed to coordinate the production and use of information in a monotonic context. PCN <ref> [41, 39] </ref> and Compositional C++ [31] more fully integrate mutable data into the monotonic model of parallel computation. These languages provide both sequential and parallel composition operators and mutable and definitional (write once) data.
Reference: [40] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Each operation may wait for some information to become available, then generate additional information. Id [101], functional languages such as Haskell [67] and Sisal [38] and concurrent logic programming languages such as Strand <ref> [40] </ref> and Parlog [50] are in this group. The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. <p> The program can separately define each element; when an operation attempts to use an undefined element it suspends until it is defined. It is an error to attempt to define an element twice. 5.3.3 Concurrent Logic Programming Languages Concurrent logic programming languages such as Strand <ref> [40] </ref> and Parlog [50] and their generalization to the family of concurrent constraint-based programming languages [120, 119] present a model of computation based on constraints. The computation consists of a set of parallel agents that incrementally impose constraints on the values of the variables.
Reference: [41] <author> I. Foster and S. Tuecke. </author> <title> Parallel programming with PCN. </title> <type> Technical Report ANL-91/32, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: In this case there is a fundamental mismatch between the two models of computation. The Strand programmer is put in the position of synchronizing multiple reads and writes to mutable data with operations designed to coordinate the production and use of information in a monotonic context. PCN <ref> [41, 39] </ref> and Compositional C++ [31] more fully integrate mutable data into the monotonic model of parallel computation. These languages provide both sequential and parallel composition operators and mutable and definitional (write once) data.
Reference: [42] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Parallel execution comes either from executing independent parts of the program concurrently or from within the basic operations of the language. This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D <ref> [42] </ref>, languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran [132]. <p> For good performance, the compiler CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 190 must evenly balance the computational load and avoid uncoordinated mappings that generate excessive communication as processors repeatedly fetch remote data. The problem becomes especially complex when different parts of the program access data in different ways. Fortran D <ref> [42] </ref> and High Performance Fortan [60] allow programmers to guide the mapping process by providing constructs that specify how to distribute arrays across multiple memory modules. The compiler can then use the data distribution to generate a mapping of computation to processors via the owner computes rule.
Reference: [43] <author> N. Gehani. </author> <title> Capsules: A shared memory access mechanism for concurrent C/C++. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(7) </volume> <pages> 795-811, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The Rosette system [133] allows programmers to identify a set of object states and specify the set of messages that an object in a given state will accept. When an object finishes executing a method it specifies its next state. Capsules <ref> [43] </ref> allow the programmer to declaratively specify conditions that the object's state must meet for it to accept each message. Ideally, the task of synchronizing the entire computation could be effectively decomposed into the task of generating the synchronization required for each object.
Reference: [44] <author> D. Gelernter. </author> <title> Generative communication in Linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Access specifications give the implementation advance notice of precisely which objects a task will access. The implementation can exploit this information to apply communication optimizations such as concurrently fetching multiple remote objects for each task. Many other parallel systems <ref> [85, 15, 19, 44] </ref> only discover when tasks will access data as the tasks actually perform the access, and lack the advance information required to apply sophisticated communication optimizations. <p> The program writes an M-structure with the put operation. This operation writes a new value into the word, leaving it full. M-structures support the mutual exclusion constraints associated with atomically updating a piece of data. Linda <ref> [44, 28] </ref> provides the abstraction of a shared memory consisting of a collection of tuples. The program can insert, read or remove tuples from the tuple space.
Reference: [45] <author> K. Gharachorloo. </author> <title> Memory Consistency Models for Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: These messages then overwrite the obsolete copies. At some point the writing processor must stall until it knows that all of the invalidates or updates have been performed. The exact stall point depends on the strength of the consistency protocol. For a more detailed treatment of consistency protocols see <ref> [45] </ref>. Update and invalidate protocols impair the performance of the system in several ways. First, there is the bandwidth cost of the update or invalidate messages.
Reference: [46] <author> K. Gharachorloo, V. Sarkar, and J. Hennessy. </author> <title> A simple and efficient implementation approach for single assignment languages. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 259-268, </pages> <address> Snowbird, UT, </address> <month> July </month> <year> 1988. </year> <note> BIBLIOGRAPHY 232 </note>
Reference-contexts: If the original array is not used in the subsequent computation, the system can update the array in place instead of generating a copy. It is possible to identify the last use of each array in the computation by compile-time analysis [49], reference counting <ref> [46] </ref>, a combination of compile-time analysis and reference counting [66], or by language-level constructs that ensure that every use of a given array is a last use [51]. Such optimizations also reduce the memory management overhead since they reuse memory without the trip through the garbage collector.
Reference: [47] <author> D. Gifford, P. Jouvelot, J. Lucassen, and M. Sheldon. </author> <title> FX-87 reference manual. </title> <type> Technical Report MIT/LCS/TR-407, </type> <institution> MIT, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: Parallel execution comes either from executing independent parts of the program concurrently or from within the basic operations of the language. This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 <ref> [87, 47] </ref> that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran [132]. <p> The compiler combines this information with an analysis of the pointer-chain paths that different parts of the computation follow to derive a precise estimate of how the computation will access data. The improved precision of the dependence analysis can expose additional opportunities for parallel execution. FX-87 <ref> [87, 47] </ref> contains constructs that programmers use to specify how procedures access data. The system statically analyzes the program, using this information to determine which procedure calls can execute concurrently without violating the serial semantics. FX-87 programmers partition the program's data into a finite, statically determined set CHAPTER 5.
Reference: [48] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations, Second Edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: While it is often possible to express the computation in Jade, the resulting Jade program usually generates more tasks (and more task management overhead) than the cyclic computation. Consider, for example, a standard iterative grid relaxation algorithm such as Successive Over Relaxation (SOR) <ref> [48] </ref>. A programmer using an explicitly parallel language could parallelize the algorithm by subdividing the grid into blocks and assigning one task to each CHAPTER 2. THE JADE LANGUAGE 37 block.
Reference: [49] <author> K. Gopinath. </author> <title> Copy elimination in single assignment languages. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: If the original array is not used in the subsequent computation, the system can update the array in place instead of generating a copy. It is possible to identify the last use of each array in the computation by compile-time analysis <ref> [49] </ref>, reference counting [46], a combination of compile-time analysis and reference counting [66], or by language-level constructs that ensure that every use of a given array is a last use [51]. Such optimizations also reduce the memory management overhead since they reuse memory without the trip through the garbage collector.
Reference: [50] <author> S. Gregory. </author> <title> Parallel Logic Programming in PARLOG: the Language and its Implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: Each operation may wait for some information to become available, then generate additional information. Id [101], functional languages such as Haskell [67] and Sisal [38] and concurrent logic programming languages such as Strand [40] and Parlog <ref> [50] </ref> are in this group. The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. <p> The program can separately define each element; when an operation attempts to use an undefined element it suspends until it is defined. It is an error to attempt to define an element twice. 5.3.3 Concurrent Logic Programming Languages Concurrent logic programming languages such as Strand [40] and Parlog <ref> [50] </ref> and their generalization to the family of concurrent constraint-based programming languages [120, 119] present a model of computation based on constraints. The computation consists of a set of parallel agents that incrementally impose constraints on the values of the variables.
Reference: [51] <author> J. Guzman and P. Hudak. </author> <title> Single-threaded polymorphic lambda calculus. </title> <booktitle> In Proceedings of the Fifth Annual IEEE Symposium on Logic in Computer Science, </booktitle> <pages> pages 333-343, </pages> <address> Philadelphia, PA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It is possible to identify the last use of each array in the computation by compile-time analysis [49], reference counting [46], a combination of compile-time analysis and reference counting [66], or by language-level constructs that ensure that every use of a given array is a last use <ref> [51] </ref>. Such optimizations also reduce the memory management overhead since they reuse memory without the trip through the garbage collector.
Reference: [52] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model <ref> [83, 52, 73] </ref> and the message-passing model [130, 68, 131]. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics.
Reference: [53] <author> M. Hall, K. Kennedy, and K. McKinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Experience manually applying automatable techniques across multiple procedure boundaries suggests that it is possible for a compiler to successfully parallelize the computation if it increases its scope to include large sections of the program <ref> [53, 21] </ref>. For irregular programs there are problems performing dependence analysis that is precise enough to expose the concurrency.
Reference: [54] <author> R. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Most of the work in this area has taken place in the context of the eager functional language Sisal. In particular, Sarkar has developed a partitioning algorithm for Sisal that negotiates the resulting trade-off between concurrency and scheduling overhead [121]. MultiLisp <ref> [54, 55] </ref> relies on the programmer to specify a partitioning. The MultiLisp future construct allows the programmer to explicitly specify the task granularity by declaring that a given function invocation should be evaluated concurrently with the computation after the function.
Reference: [55] <author> R. Halstead, Jr. </author> <title> An assessment of Multilisp: Lessons from experience. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(6) </volume> <pages> 459-501, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Most of the work in this area has taken place in the context of the eager functional language Sisal. In particular, Sarkar has developed a partitioning algorithm for Sisal that negotiates the resulting trade-off between concurrency and scheduling overhead [121]. MultiLisp <ref> [54, 55] </ref> relies on the programmer to specify a partitioning. The MultiLisp future construct allows the programmer to explicitly specify the task granularity by declaring that a given function invocation should be evaluated concurrently with the computation after the function.
Reference: [56] <author> R. Hammel and D. Gifford. </author> <title> FX-87 Performance Measurements: Dataflow Implementation. </title> <type> Technical Report MIT/LCS/TR-421, </type> <institution> MIT, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: In general, many dynamic objects must be mapped to the same region, preventing the programmer from expressing concurrency available between parts of the program that access disjoint sets of such objects. An analysis of the concurrency in FX-87 programs <ref> [56] </ref> found that they failed to exploit important sources of concurrency because they mapped many pieces of data to the same region. The problem was especially severe for programs in which the main potential source of concurrency scaled with the size of the input problem.
Reference: [57] <author> R. Harper. </author> <title> On the type structure of Standard ML. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 211-252, </pages> <month> April </month> <year> 1993. </year> <note> BIBLIOGRAPHY 233 </note>
Reference-contexts: The lack of side effects in monotonic languages means that each piece of memory goes through the collector between writes. Many of the problems associated with garbage collection are the same for parallel monotonic languages and mostly functional languages such as ML <ref> [93, 57] </ref>. One potential performance problem is the instruction overhead of garbage collection. A more serious problem, however, is the interaction with the memory hierarchy. Because the computation must store each generated value into a new memory location, it consumes memory very quickly.
Reference: [58] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 82-85, </pages> <year> 1990. </year>
Reference-contexts: I developed the Jade version of Water starting with serial C version that had been translated from the original Fortran. * String A program that computes a velocity model of the geology between two oil wells. I originally obtained a serial version of String <ref> [58] </ref> written in a combination of C and Fortran from Jerry Harris (a professor in the Stanford Geophysics Department), Mark van Schaack (a graduate student in the Stanford Geophysics Department) and Caroline Lambert (a programmer in the Stanford Geophysics Department) and, with the help of Brian Schmidt (a graduate student in <p> It is easy to see that all of the interactions can be computed concurrently. The only issue is the accumulation of the results into the final data structure. Because the basic accumulation operation is addition, the accumulations commute, associate and have an identity. 4.4.2 String String <ref> [58] </ref> uses seismic travel-time inversion to construct a two-dimensional discrete velocity model of the geological medium between two oil wells. Each element of the velocity model records how fast sound waves travel through the corresponding part of the medium.
Reference: [59] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: When pieces of code access disjoint subsets of such variables, the compiler can statically verify that they can execute concurrently. Typical operations are creating a set of names that refer to disjoint regions of an array and creating an array of pointers that point to distinct data structures. ADDS <ref> [59] </ref> declarations for data structures containing pointers to dynamically allocated data allow programmers to describe the set of data structures that can be reached by following different pointer chains.
Reference: [60] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: PARALLEL PROGRAMMING SYSTEMS 190 must evenly balance the computational load and avoid uncoordinated mappings that generate excessive communication as processors repeatedly fetch remote data. The problem becomes especially complex when different parts of the program access data in different ways. Fortran D [42] and High Performance Fortan <ref> [60] </ref> allow programmers to guide the mapping process by providing constructs that specify how to distribute arrays across multiple memory modules. The compiler can then use the data distribution to generate a mapping of computation to processors via the owner computes rule.
Reference: [61] <author> M. Hill, J. Larus, K. Reinhardt, and D. Wood. </author> <title> Cooperative shared memory: Software and hardware for scalable multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Future languages and systems will be increasingly organized around the interaction of data and computation, with various declarative mechanisms such as access specifications used to express the relevant information. COOL's locality hints [30], Midway's object usage declarations [19], shared region declarations [118] and the CHICO model of consistency <ref> [61] </ref> are all examples of this trend. Access specifications give the implementation enough information to automatically generate the communication without forcing the implementation to use a specific communication mechanism. It is therefore possible to implement parallel languages based on access specifications on a wide variety of machines.
Reference: [62] <author> C. A. R. Hoare. </author> <title> Monitors: An operating system concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: The synchronization mechanisms are all oriented around how the program accesses CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 208 objects. A basic mechanism is that each method executes with exclusive access to the receiver object. This implicit form of mutual exclusion synchronization originally appeared in the context of monitors <ref> [62] </ref>, and was used in monitor-based languages such as Concurrent Pascal [23, 24] and Mesa [81, 95]. Concurrent object-oriented languages also adopt this synchronization mechanism, in part because it meshes well with the concept of a method operating on an object.
Reference: [63] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP <ref> [64, 63] </ref>, CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP <ref> [64, 63] </ref> and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [64] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1985. </year>
Reference-contexts: The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP <ref> [64, 63] </ref>, CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP <ref> [64, 63] </ref> and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [65] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 interconnect elan-elite design. </title> <booktitle> In Proceedings of Hot Interconnects 93, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: No program should be able to use its network access to interfere with other programs. The Thinking Machines CM-5 [103] provides this functionality by imposing a strict gang scheduling regime. Different programs cannot interfere because they cannot concurrently access the network. More recent systems <ref> [65, 22] </ref> perform protection checks in hardware on an attached co-processor, which enables the operating system to schedule each compute processor independently. The next step is a redesign of the interface that the message-passing system presents to the user program. <p> Such an interface eliminates buffering by telling the system the final location of the transferred data before the system actually starts the transfer. The final advantage of a remote read and write interface is that it eliminates unnecessary interrupts at the remote compute processor. With minimal hardware support <ref> [65] </ref> it is possible to perform all remote operations on an agile co-processor designed to quickly handle remote reads and writes.
Reference: [66] <author> P. Hudak and A. Bloss. </author> <title> The aggregate update problem in functional programming systems. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 300-313, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: It is possible to identify the last use of each array in the computation by compile-time analysis [49], reference counting [46], a combination of compile-time analysis and reference counting <ref> [66] </ref>, or by language-level constructs that ensure that every use of a given array is a last use [51]. Such optimizations also reduce the memory management overhead since they reuse memory without the trip through the garbage collector.
Reference: [67] <author> P. Hudak, S. Peyton-Jones, P. Wadler, B. Boutel, J. Fairbairn, J. Fasel, M. Guzman, K. Hammond, J. Hughes, T. Johnsson, D. Kieburtz, R. Nikhil, W. Partain, </author> <title> and BIBLIOGRAPHY 234 J. Peterson. Report on the programming language Haskell: A non-strict, purely functional language (version 1.2). </title> <journal> SIGPLAN Notices, 27(5):Ri-Rx, </journal> <volume> Rl-R163, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. Id [101], functional languages such as Haskell <ref> [67] </ref> and Sisal [38] and concurrent logic programming languages such as Strand [40] and Parlog [50] are in this group. The explicitly parallel group includes systems in which the program explicitly generates parallel tasks.
Reference: [68] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model [83, 52, 73] and the message-passing model <ref> [130, 68, 131] </ref>. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics.
Reference: [69] <author> D. Jefferson. </author> <title> Virtual time. </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: For example, it is possible to parallelize a serial program statically using a parallelizing compiler, dynamically using a system like Jade, or even optimistically using a speculative system like TimeWarp <ref> [70, 69] </ref>. The different approaches often reflect perturbations back into the programming model as the systems either impose additional restrictions that enable specific implementation techniques or require the programmer to help by providing extra information about the program. <p> includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp <ref> [70, 69] </ref> and ParaTran [132]. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. <p> In effect, the dependences are discovered only as the tasks execute, and active enforcement of the dependences via rollback occurs only when a violation has been detected. Both the Time Warp system for distributed simulation <ref> [70, 69] </ref> and the ParaTran system for parallelizing serial Scheme code [132] use a speculative approach. Speculative systems can exploit concurrency available in programs that conservative approaches can not effectively analyze.
Reference: [70] <author> D. Jefferson, B. Beckman, F. Wieland, L. Blume, M. DiLoreto, P. Hontalas, P. Laroche, K. Sturdevant, J. Tupman, V. Warren, J. Wedel, and H. Younger. </author> <title> Distributed simulation and the Time Warp Operating System. </title> <booktitle> In Proceedings of the Eleventh Symposium on Operating Systems Principles, </booktitle> <pages> pages 77-93, </pages> <address> Austin, TX, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: For example, it is possible to parallelize a serial program statically using a parallelizing compiler, dynamically using a system like Jade, or even optimistically using a speculative system like TimeWarp <ref> [70, 69] </ref>. The different approaches often reflect perturbations back into the programming model as the systems either impose additional restrictions that enable specific implementation techniques or require the programmer to help by providing extra information about the program. <p> includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp <ref> [70, 69] </ref> and ParaTran [132]. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. <p> In effect, the dependences are discovered only as the tasks execute, and active enforcement of the dependences via rollback occurs only when a violation has been detected. Both the Time Warp system for distributed simulation <ref> [70, 69] </ref> and the ParaTran system for parallelizing serial Scheme code [132] use a speculative approach. Speculative systems can exploit concurrency available in programs that conservative approaches can not effectively analyze.
Reference: [71] <author> E. Jul, H. Levy, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <year> 1988. </year>
Reference-contexts: The implementation first distributes the objects across the machine. When a task invokes a method on a remote object, the implementation can either perform a remote procedure call [99] to execute the method remotely as in the Emerald system <ref> [20, 71] </ref> or transfer the object to the invoking processor and execute the method locally. The drawback is that a pure object model can interact with the language's synchronization mechanisms to limit its flexibility.
Reference: [72] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: We next discuss three different approaches to implementing the shared memory in software: the page-based approach, the region-based approach and the object-based approach. Page-based systems such as Ivy [85], Munin [15, 14, 29] and Treadmarks <ref> [72] </ref> use the virtual-to-physical address-translation hardware to implement a cache consistency protocol at the granularity of pages. The translation hardware is used to detect accesses to remote pages, and the fault handler generates messages that move or copy the required pages from remote processors. <p> A drawback of page-based systems is that the relatively large size of the pages increases the probability of an application suffering from excessive communication caused by false sharing (when multiple processors repeatedly access disjoint regions of a single page in conflicting ways). More recent systems <ref> [72, 15] </ref> ameliorate this problem by allowing different processors to concurrently write disjoint regions of the same page. Another potential performance problem is the substantial exception handling overhead that operating systems typically impose [7]. Page-based systems also interact with the application at the level of the raw address CHAPTER 5.
Reference: [73] <institution> Kendall Square Research Corporation, Cambridge, MA. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model <ref> [83, 52, 73] </ref> and the message-passing model [130, 68, 131]. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics. <p> of memory systems: 1) bus-based systems with a single memory module and per-processor caches (like the Silicon Graphics 4D/340), 2) distributed-memory systems with multiple memory modules and caches (like the Stanford DASH machine), 3) distributed-memory systems without caches (like the BBN Butterfly [13]) and 4) cache-only-memory systems (like the KSR1 <ref> [73] </ref>). 3.3.3.1 Bus-Based Systems Bus-based systems have a roughly uniform access time from any processor to the main memory. The access time for cached data is typically significantly lower than the access time to main memory.
Reference: [74] <author> A. Klaiber and H. Levy. </author> <title> A comparison of message passing and shared memory architectures for data parallel programs. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Standard shared-memory systems, on the other hand, generate two remote operations for each synchronized access to remote data: a synchronization operation to acquire the right to access the data, then a communication operation to actually perform the access <ref> [74] </ref>. Systems that combine data access and synchronization make it less likely that the program will suffer from synchronization errors. The programmer cannot create a synchronization error by simply forgetting to insert synchronization operations around a given data access.
Reference: [75] <author> D. Klappholz. </author> <title> Refined Fortran: An update. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The goal is to expose more concurrency by improving the precision of the compiler's dependence analysis. Refined C [76] and Refined Fortran <ref> [75, 36] </ref> allow programmers to create sets of variables that refer to disjoint regions of memory. When pieces of code access disjoint subsets of such variables, the compiler can statically verify that they can execute concurrently.
Reference: [76] <author> D. Klappholz, A. Kallis, and X. Kong. </author> <title> Refined C An update. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 331-357. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <note> BIBLIOGRAPHY 235 </note>
Reference-contexts: The goal is to expose more concurrency by improving the precision of the compiler's dependence analysis. Refined C <ref> [76] </ref> and Refined Fortran [75, 36] allow programmers to create sets of variables that refer to disjoint regions of memory. When pieces of code access disjoint subsets of such variables, the compiler can statically verify that they can execute concurrently.
Reference: [77] <author> A. Krishnamurthy, D. Culler, A. Dusseau, S. Goldstein, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These systems provide the functionality required to allow the programmer to control the communication at a low level for efficiency. We start our discussion of these systems by considering the design of Split-C <ref> [77] </ref>. Like the software shared-memory systems discussed in Section 5.4.2.4, Split-C distributes a single shared address space across the memories of a message-passing machine and layers a software implementation of shared memory on top of the message-passing substrate. At CHAPTER 5.
Reference: [78] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: The programmer may help the system analyze the program by providing information about how the program structures and accesses data, or make policy decisions about the distribution of tasks and data to processors and memories. 5.2.2.1 Parallelizing Compilers Parallelizing compilers <ref> [10, 78, 79, 4] </ref> statically analyze programs to find independent pieces of code. The compiler then generates a parallel program that executes the independent pieces of code concurrently.
Reference: [79] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: The programmer may help the system analyze the program by providing information about how the program structures and accesses data, or make policy decisions about the distribution of tasks and data to processors and memories. 5.2.2.1 Parallelizing Compilers Parallelizing compilers <ref> [10, 78, 79, 4] </ref> statically analyze programs to find independent pieces of code. The compiler then generates a parallel program that executes the independent pieces of code concurrently.
Reference: [80] <author> M. Lam and M. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Programs that access the same data at different granularities in different parts of the program exacerbate the problem by forcing the programmer to periodically reallocate the data at the new granularity. An early version of Jade <ref> [80] </ref> avoided this problem by decoupling the units of allocation and synchronization. The units of synchronization (called tokens) were abstract, and their correspondence with the actual data was completely conceptual. The lack of an enforced correspondence dramatically increased the flexibility of the language.
Reference: [81] <author> B. Lampson and D. Redell. </author> <title> Experience with processes and monitors in Mesa. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 105-117, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: PARALLEL PROGRAMMING SYSTEMS 208 objects. A basic mechanism is that each method executes with exclusive access to the receiver object. This implicit form of mutual exclusion synchronization originally appeared in the context of monitors [62], and was used in monitor-based languages such as Concurrent Pascal [23, 24] and Mesa <ref> [81, 95] </ref>. Concurrent object-oriented languages also adopt this synchronization mechanism, in part because it meshes well with the concept of a method operating on an object. Many languages relax the exclusive execution constraint to allow the concurrent execution of methods that read the same object.
Reference: [82] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The programmer obliviously reuses these algorithms every time he or she writes a Jade program. We have demonstrated the viability and applicability of these algorithms by implementing Jade on many different computational platforms. Jade implementations currently exist for shared-memory machines such as the Stanford DASH machine <ref> [82] </ref> and the Silicon Graphics 4D/340 [12], for message-passing machines such as the Intel iPSC/860 [16], and for heterogeneous networks of workstations. <p> The implementation measures the time by reading a clock when the program enters and exits the different segments, adding the difference to a running sum upon exit. On DASH the implementation uses the 32 bit 60 nanosecond counter on the DASH performance monitor chip <ref> [82] </ref> to implement a high-resolution 64 bit clock. The 60 nanosecond counter is the lower half of this clock; the software increments the 32 bit upper half every time the counter wraps. <p> Further investigation yields data consistent with the hypothesis that the performance variation is caused by memory system effects. We used the hardware performance monitor on DASH <ref> [82] </ref> to count the number of cache misses on each processor. The monitor counts local cache misses (misses satisfied from within the cluster issuing the reference) and remote cache misses (misses satisfied CHAPTER 4. APPLICATIONS EXPERIENCE 159 CHAPTER 4. APPLICATIONS EXPERIENCE 160 from of a remote cluster) separately.
Reference: [83] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model <ref> [83, 52, 73] </ref> and the message-passing model [130, 68, 131]. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics.
Reference: [84] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The trade-off is that dynamic analysis generates overhead, which may degrade the performance. 5.2.3.1 Partial Evaluation A branch of compiler research has attempted to extend techniques from parallelizing compilers to programs with dynamic, data-dependent concurrency patterns <ref> [94, 117, 116, 115, 84] </ref>. These compilers use a two-phase partial evaluation approach. The first phase partially evaluates part of the program on its input data, then generates and schedules the resulting task graph and communication operations. The second phase actually executes the computation.
Reference: [85] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: Access specifications give the implementation advance notice of precisely which objects a task will access. The implementation can exploit this information to apply communication optimizations such as concurrently fetching multiple remote objects for each task. Many other parallel systems <ref> [85, 15, 19, 44] </ref> only discover when tasks will access data as the tasks actually perform the access, and lack the advance information required to apply sophisticated communication optimizations. <p> These optimizations and the implementation of a global name space for shared data provide significant software support for programming message-passing machines. We next discuss three different approaches to implementing the shared memory in software: the page-based approach, the region-based approach and the object-based approach. Page-based systems such as Ivy <ref> [85] </ref>, Munin [15, 14, 29] and Treadmarks [72] use the virtual-to-physical address-translation hardware to implement a cache consistency protocol at the granularity of pages.
Reference: [86] <author> INMOS Limited. </author> <title> Occam Programming Manual. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1984. </year> <note> BIBLIOGRAPHY 236 </note>
Reference-contexts: The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam <ref> [86, 27] </ref>, CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam <ref> [86, 27] </ref>, CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [87] <author> J. Lucassen. </author> <title> Types and effects: Towards the integration of functional and imperative programming. </title> <type> Technical Report MIT/LCS/TR-408, </type> <institution> MIT, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Parallel execution comes either from executing independent parts of the program concurrently or from within the basic operations of the language. This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 <ref> [87, 47] </ref> that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran [132]. <p> The compiler combines this information with an analysis of the pointer-chain paths that different parts of the computation follow to derive a precise estimate of how the computation will access data. The improved precision of the dependence analysis can expose additional opportunities for parallel execution. FX-87 <ref> [87, 47] </ref> contains constructs that programmers use to specify how procedures access data. The system statically analyzes the program, using this information to determine which procedure calls can execute concurrently without violating the serial semantics. FX-87 programmers partition the program's data into a finite, statically determined set CHAPTER 5.
Reference: [88] <author> E. Lusk, R. Overbeek, J. Boyle, R. Butler, T. Disz, B. Glickfield, J. Patterson, and R. Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: If the programming environment exposes this nondeterminism to the programmer, it complicates the debugging process by making it difficult to reproduce and eliminate programming errors. 1.1 Explicitly Parallel Systems Programmers have traditionally developed software for parallel machines using explicitly parallel systems <ref> [88, 130] </ref>. These systems provide constructs that programmers use to create parallel tasks. On shared-memory machines the programmer synchronizes the tasks using low-level primitives such as locks, condition variables and barriers. On message-passing CHAPTER 1. <p> For example, the mi.lo version specifies the minimum level of instrumentation and the use of the locality heuristic. The ANL version is the explicitly parallel version from the SPLASH benchmark suite written using the ANL macro package <ref> [88] </ref>. Figure 4.17 contains the corresponding speedup curves for two versions of the application. As for the iPSC/860 speedup curves, the baseline for these speedup curves is the stripped version. The data set is the same as for the iPSC/860 runs. The computation scales almost linearly to 32 processors. <p> Table 4.10 presents the execution times for Volume Rendering; Figure 4.33 presents the corresponding speedup curves. We were able to obtain performance numbers for the explicitly parallel version described in [100]. This version was written using the ANL macro package <ref> [88] </ref>. The Jade performance tails off quickly after 16 processors, while the explicitly parallel version scales almost linearly to 32 processors. We first explore the behavior of the Jade versions, then discuss the reasons for the performance difference between the explicitly parallel version and the Jade versions. CHAPTER 4. <p> The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM [130], Presto [17] and the ANL macro package <ref> [88] </ref> focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> The resulting systems minimize the number of messages required for synchronized access to blocks of shared data and expose the inherent asynchrony of the communication in the programming model. 5.4.2.1 Basic Synchronization Packages Packages such as Presto [17] and the ANL macro package <ref> [88] </ref> and languages such as Modula-3 [98] rely on the hardware to implement the shared memory. The software only provides basic concurrency generation and synchronization primitives such as locks, barriers and condition variables. Such a bare-bones interface minimizes the safety of the programming model.
Reference: [89] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in message passing and shared memory implementations of a standard cell router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 88-96, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Developing the same computation on different machines may therefore lead to radically different programs [126, 114], and it can be difficult to port a program written for one machine to a machine with a substantially different programming interface <ref> [89] </ref>. Given the rapid development of parallel machines, this lack of portability presents a significant obstacle to the widespread use of parallel computation. Organizations that develop or purchase software will be reluctant to invest in parallel software if it will not easily port to new architectures. <p> Requiring the programmer CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 203 to generate code to perform these activities complicates the programming process. A comparison of a fairly complex application written for both a message-passing system [114] and a shared-memory system <ref> [126, 89] </ref> highlights the programming burden that message-passing systems can impose. One clear advantage of message-passing systems is that they present a simple performance model. Every remote interaction is cleanly identified in the program and a simple model accurately predicts the cost of each message transfer [16].
Reference: [90] <author> D. Maydan, S. Amarasinghe, and M. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: For irregular programs there are problems performing dependence analysis that is precise enough to expose the concurrency. While compilers can often successfully analyze the data access patterns of loop nests that manipulate dense matrices <ref> [91, 90] </ref>, automatically analyzing the access patterns of programs that manipulate dynamically linked data structures or use indirect array indices remains an open research problem. Another unsolved problem is determining how to coordinate the mapping of data and computation to memory modules and processors [6].
Reference: [91] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: For irregular programs there are problems performing dependence analysis that is precise enough to expose the concurrency. While compilers can often successfully analyze the data access patterns of loop nests that manipulate dense matrices <ref> [91, 90] </ref>, automatically analyzing the access patterns of programs that manipulate dynamically linked data structures or use indirect array indices remains an open research problem. Another unsolved problem is determining how to coordinate the mapping of data and computation to memory modules and processors [6].
Reference: [92] <author> M. Metcalf and J. Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford Science Publications, </publisher> <year> 1990. </year>
Reference-contexts: This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 <ref> [92] </ref>, and speculative systems like Time Warp [70, 69] and ParaTran [132]. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. <p> Static systems analyze the program before it runs and generate a parallel program that exploits the concurrency available at compile time. Dynamic systems analyze the program at run time and exploit concurrency as the program runs. 5.2.1 Data-Parallel Systems Data-parallel languages such as Fortran 90 <ref> [92] </ref> and C* [111] provide a useful paradigm for programs with regular, data-parallel forms of concurrency. Programmers using these languages view their program as a sequence of operations on large aggregate data structures such as sets or arrays.
Reference: [93] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The Definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: The lack of side effects in monotonic languages means that each piece of memory goes through the collector between writes. Many of the problems associated with garbage collection are the same for parallel monotonic languages and mostly functional languages such as ML <ref> [93, 57] </ref>. One potential performance problem is the instruction overhead of garbage collection. A more serious problem, however, is the interaction with the memory hierarchy. Because the computation must store each generated value into a new memory location, it consumes memory very quickly.
Reference: [94] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The trade-off is that dynamic analysis generates overhead, which may degrade the performance. 5.2.3.1 Partial Evaluation A branch of compiler research has attempted to extend techniques from parallelizing compilers to programs with dynamic, data-dependent concurrency patterns <ref> [94, 117, 116, 115, 84] </ref>. These compilers use a two-phase partial evaluation approach. The first phase partially evaluates part of the program on its input data, then generates and schedules the resulting task graph and communication operations. The second phase actually executes the computation.
Reference: [95] <author> J. Mitchell, W. Maybury, and R. Sweet. </author> <title> Mesa language manual, version 5.0. </title> <type> Technical Report CSL-79-3, </type> <institution> Xerox Palo Alto Research Center, </institution> <month> April </month> <year> 1979. </year>
Reference-contexts: PARALLEL PROGRAMMING SYSTEMS 208 objects. A basic mechanism is that each method executes with exclusive access to the receiver object. This implicit form of mutual exclusion synchronization originally appeared in the context of monitors [62], and was used in monitor-based languages such as Concurrent Pascal [23, 24] and Mesa <ref> [81, 95] </ref>. Concurrent object-oriented languages also adopt this synchronization mechanism, in part because it meshes well with the concept of a method operating on an object. Many languages relax the exclusive execution constraint to allow the concurrent execution of methods that read the same object.
Reference: [96] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year> <note> BIBLIOGRAPHY 237 </note>
Reference-contexts: When programmers use the local pointer mechanism discussed in section 2.2.4, the implementation amortizes the lookup cost over many accesses via the local pointer. 3.5.3 Suppressing Excessive Task Creation An important issue associated with managing dynamic concurrency is the suppression of excessive task creation <ref> [33, 96] </ref>. Jade programs may be able to generate an extremely large number of tasks. Because unexecuted tasks consume memory, excessive task creation stresses the machine's memory system, degrading performance. In extreme cases the machine may be unable to successfully execute the program.
Reference: [97] <author> D. Mundie and D. Fisher. </author> <title> Parallel processing in Ada. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 20-25, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada <ref> [97] </ref> are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [98] <author> G. Nelson, </author> <title> editor. Systems Programming with Modula-3. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 <ref> [98] </ref> integrate concurrency generation and synchronization primitives into the language. <p> The resulting systems minimize the number of messages required for synchronized access to blocks of shared data and expose the inherent asynchrony of the communication in the programming model. 5.4.2.1 Basic Synchronization Packages Packages such as Presto [17] and the ANL macro package [88] and languages such as Modula-3 <ref> [98] </ref> rely on the hardware to implement the shared memory. The software only provides basic concurrency generation and synchronization primitives such as locks, barriers and condition variables. Such a bare-bones interface minimizes the safety of the programming model. In shared-memory systems, synchronization operations usually mediate access to shared data structures.
Reference: [99] <author> J. Nelson. </author> <title> Remote Procedure Call. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: It also simplifies the implementation of the language on message-passing platforms. The implementation first distributes the objects across the machine. When a task invokes a method on a remote object, the implementation can either perform a remote procedure call <ref> [99] </ref> to execute the method remotely as in the Emerald system [20, 71] or transfer the object to the invoking processor and execute the method locally. The drawback is that a pure object model can interact with the language's synchronization mechanisms to limit its flexibility.
Reference: [100] <author> J. Nieh and M. Levoy. </author> <title> Volume rendering on scalable shared-memory MIMD architectures. </title> <type> Technical Report CSL-TR-92-537, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: The basic computational issues are maximizing locality, minimizing overhead and balancing the load. Because rays from adjacent pixels tend to access similar regions of the voxel array, shooting rays from contiguous sets of pixels on the same processor enhances locality. The original developers <ref> [100] </ref> determined that distributing pixels to processors in 8 by 8 chunks yields acceptable locality. Because the rays generate different amounts of computation, a static assignment of 8 by 8 chunks to processors has the potential to generate an unbalanced load. <p> Because the shading table is also allocated in one chunk, the shading-table computation has a similar copyback phase. For the timing runs the application rendered the head dataset described in <ref> [100] </ref>. This dataset has a 256 by 256 by 226 voxel array and a 416 by 416 image array. Each element of the voxel array takes up 4 bytes; each element of the image array takes up 1 byte. <p> Table 4.10 presents the execution times for Volume Rendering; Figure 4.33 presents the corresponding speedup curves. We were able to obtain performance numbers for the explicitly parallel version described in <ref> [100] </ref>. This version was written using the ANL macro package [88]. The Jade performance tails off quickly after 16 processors, while the explicitly parallel version scales almost linearly to 32 processors.
Reference: [101] <author> R. Nikhil. </author> <title> Id version 90.0 reference manual. </title> <type> Technical Report 284-1, </type> <institution> Computation Structures Group, MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. Id <ref> [101] </ref>, functional languages such as Haskell [67] and Sisal [38] and concurrent logic programming languages such as Strand [40] and Parlog [50] are in this group. The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. <p> It is often convenient to generate the individual values that together comprise a large aggregate at different points in the program's execution. Id <ref> [101] </ref> provides for this functionality via I-structures [8, 102]. I-structures contain write-once elements that CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 196 are initially undefined. The program can separately define each element; when an operation attempts to use an undefined element it suspends until it is defined.
Reference: [102] <author> R. Nikhil and K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: It is often convenient to generate the individual values that together comprise a large aggregate at different points in the program's execution. Id [101] provides for this functionality via I-structures <ref> [8, 102] </ref>. I-structures contain write-once elements that CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 196 are initially undefined. The program can separately define each element; when an operation attempts to use an undefined element it suspends until it is defined.
Reference: [103] <author> J. Palmer and G. Steele, Jr. </author> <title> Connection Machine model CM-5 system overview. </title> <booktitle> In Frontiers '92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The main issue is supporting direct user-level access to the network without compromising security. No program should be able to use its network access to interfere with other programs. The Thinking Machines CM-5 <ref> [103] </ref> provides this functionality by imposing a strict gang scheduling regime. Different programs cannot interfere because they cannot concurrently access the network. More recent systems [65, 22] perform protection checks in hardware on an attached co-processor, which enables the operating system to schedule each compute processor independently.
Reference: [104] <author> P. Pierce. </author> <title> The NX/2 operating system. </title> <editor> In Geoffrey Fox, editor, </editor> <booktitle> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <address> Pasadena, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Asynchronous sends return immediately, with the system invisibly buffering the data until the corresponding receive comes along. Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel <ref> [104] </ref> are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization. <p> The drawback is that inserting the polling calls imposes programming overhead and executing the polling operations can degrade the performance. Another option is to set up an interrupt message handler that gets invoked whenever a message arrives. The hrecv construct in the NX/2 message-passing package supports this functionality <ref> [104] </ref>. The drawback is the programming overhead required to synchronize the main computation with the message handlers. The synchronization is typically performed at a low level by periodically disabling and re-enabling the message arrival interrupt.
Reference: [105] <author> J.K. Reid. </author> <title> The Fortran 90 standard. </title> <journal> In IFIP Transactions A (Computer Science and Technology), </journal> <volume> volume A-2, </volume> <pages> pages 343-348, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The implementation then manages the concurrency and performs the machine-specific optimizations required to map the computation efficiently onto the current hardware platform. The resulting parallel program is easier to develop and readily ports to new machines. Existing approaches support restricted computational paradigms. Languages such as Fortran 90 <ref> [105] </ref> and C* [111] support the exploitation of regular, data-parallel forms of concurrency. Programmers using these languages view their program as a sequence of operations on large aggregate data structures such as sets or arrays.
Reference: [106] <author> J. Reppy. </author> <title> Higher-order Concurrency. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Cornell University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Packages like PVM [130], Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP [64, 63], CML <ref> [106] </ref> and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Actor languages [1, 2, 3], PVM [130] and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML <ref> [106] </ref>, Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs. An apparent weakness of synchronous message passing is the imposed serialization.
Reference: [107] <author> M. Rinard. </author> <title> Implicitly synchronized abstract data types: Data structures for modular parallel programming. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software and Applications, </booktitle> <address> Capri, Italy, </address> <month> October </month> <year> 1994. </year> <note> BIBLIOGRAPHY 238 </note>
Reference-contexts: These locality properties allow programmers to develop the abstractions required for modular parallel programming. For example, Jade programmers can build abstract data types that completely encapsulate the Jade constructs required to guide the parallelization process <ref> [107] </ref>. The Jade implementation encapsulates all of the concurrency management code required to exploit task-level concurrency. Jade programmers can therefore concentrate on the semantics of the actual computation, rather than struggling with the low-level synchronization and communication code required to coordinate the parallel execution. <p> Jade's modularity properties support standard software engineering activities. Jade programmers can build abstract data types that completely encapsulate the code required to exploit concurrency available both within and between operations on the abstract data type <ref> [107] </ref>. Each operation contains Jade constructs that specify how it will access the data structures that implement the state of the abstract data type.
Reference: [108] <author> M. Rinard and M. Lam. </author> <booktitle> Semantic foundations of Jade. In Proceedings of the Nineteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 105-118, </pages> <address> Albuquerque, NM, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: In contrast to data-parallel forms of concurrency, task-level concurrency is often irregular and may depend on the input data or on the results of previous computation. The tasks' computations may be heterogeneous, with different tasks executing completely different computations. Jade <ref> [108, 109, 110] </ref> is a high-level, implicitly parallel language designed for exploiting task-level concurrency. A Jade programmer starts with a program written in a sequential, imperative language, then uses Jade constructs to describe how the program accesses data. <p> We establish that if a write occurs before a read to the same object in a sequential execution, the write will occur before the read in all parallel executions. A similar argument establishes that the read will take place before any subsequent (in the sequential execution order) writes. See <ref> [108] </ref> for a more formal treatment of this argument. Consider a write that happens before a read in a sequential execution.
Reference: [109] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Heterogeneous parallel programming in Jade. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 245-256, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: In contrast to data-parallel forms of concurrency, task-level concurrency is often irregular and may depend on the input data or on the results of previous computation. The tasks' computations may be heterogeneous, with different tasks executing completely different computations. Jade <ref> [108, 109, 110] </ref> is a high-level, implicitly parallel language designed for exploiting task-level concurrency. A Jade programmer starts with a program written in a sequential, imperative language, then uses Jade constructs to describe how the program accesses data.
Reference: [110] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In contrast to data-parallel forms of concurrency, task-level concurrency is often irregular and may depend on the input data or on the results of previous computation. The tasks' computations may be heterogeneous, with different tasks executing completely different computations. Jade <ref> [108, 109, 110] </ref> is a high-level, implicitly parallel language designed for exploiting task-level concurrency. A Jade programmer starts with a program written in a sequential, imperative language, then uses Jade constructs to describe how the program accesses data.
Reference: [111] <author> J. Rose and G. Steele. </author> <title> C*: An extended C language for data parallel programming. </title> <type> Technical Report PL 87-5, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <month> April </month> <year> 1987. </year>
Reference-contexts: The resulting parallel program is easier to develop and readily ports to new machines. Existing approaches support restricted computational paradigms. Languages such as Fortran 90 [105] and C* <ref> [111] </ref> support the exploitation of regular, data-parallel forms of concurrency. Programmers using these languages view their program as a sequence of operations on large aggregate data structures such as sets or arrays. The implementation can execute each aggregate operation in parallel by performing the operation on the individual elements concurrently. <p> This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* <ref> [111, 112] </ref> and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran [132]. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. <p> Static systems analyze the program before it runs and generate a parallel program that exploits the concurrency available at compile time. Dynamic systems analyze the program at run time and exploit concurrency as the program runs. 5.2.1 Data-Parallel Systems Data-parallel languages such as Fortran 90 [92] and C* <ref> [111] </ref> provide a useful paradigm for programs with regular, data-parallel forms of concurrency. Programmers using these languages view their program as a sequence of operations on large aggregate data structures such as sets or arrays.
Reference: [112] <author> J. Rose and G. Steele, Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: This group includes serial languages such as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* <ref> [111, 112] </ref> and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran [132]. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation.
Reference: [113] <author> E. Rothberg. </author> <title> Exploiting the memory hierarchy in sequential and parallel sparse Cholesky factorization. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: In this version the columns of the matrix have been aggregated into larger-grain objects called panels. This aggregation increases both the data and task grain sizes. See Rothberg's thesis <ref> [113] </ref> for a comprehensive discussion of parallel sparse Cholesky factorization algorithms, including this one. The serial program stores all of the panels contiguously in one long array. <p> We were also able to obtain performance numbers for two explicitly parallel versions of this computation: a version written in COOL [30] and a version written in the ANL macro package <ref> [113] </ref>. Although both of these versions perform better than the Jade versions, their performance is not spectacular. The COOL version only achieves a speedup of 8 out of 24 processors, while the ANL version only achieves a speedup of 9.9 out of 24 processors. <p> The first factor is an inherent lack of concurrency in the application. Even for the ANL version the maximum speedup is less than 10 for a 24 processor run, with much of the performance loss caused by an inherent lack of concurrency <ref> [113] </ref>. A second performance factor is that the general-purpose object queue and task queue algorithms in the Jade implementation are less efficient than the special-purpose algorithms used in hand-coded implementations.
Reference: [114] <author> J. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics. Developing the same computation on different machines may therefore lead to radically different programs <ref> [126, 114] </ref>, and it can be difficult to port a program written for one machine to a machine with a substantially different programming interface [89]. Given the rapid development of parallel machines, this lack of portability presents a significant obstacle to the widespread use of parallel computation. <p> The program must also manage the memory used to hold remotely generated data. Requiring the programmer CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 203 to generate code to perform these activities complicates the programming process. A comparison of a fairly complex application written for both a message-passing system <ref> [114] </ref> and a shared-memory system [126, 89] highlights the programming burden that message-passing systems can impose. One clear advantage of message-passing systems is that they present a simple performance model.
Reference: [115] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Con-currency: Practice & Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The trade-off is that dynamic analysis generates overhead, which may degrade the performance. 5.2.3.1 Partial Evaluation A branch of compiler research has attempted to extend techniques from parallelizing compilers to programs with dynamic, data-dependent concurrency patterns <ref> [94, 117, 116, 115, 84] </ref>. These compilers use a two-phase partial evaluation approach. The first phase partially evaluates part of the program on its input data, then generates and schedules the resulting task graph and communication operations. The second phase actually executes the computation.
Reference: [116] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The trade-off is that dynamic analysis generates overhead, which may degrade the performance. 5.2.3.1 Partial Evaluation A branch of compiler research has attempted to extend techniques from parallelizing compilers to programs with dynamic, data-dependent concurrency patterns <ref> [94, 117, 116, 115, 84] </ref>. These compilers use a two-phase partial evaluation approach. The first phase partially evaluates part of the program on its input data, then generates and schedules the resulting task graph and communication operations. The second phase actually executes the computation.
Reference: [117] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime parallelization and scheduling of loops. </title> <booktitle> In Proceedings of the 1st Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Fe, NM, </address> <year> 1989. </year> <note> BIBLIOGRAPHY 239 </note>
Reference-contexts: The trade-off is that dynamic analysis generates overhead, which may degrade the performance. 5.2.3.1 Partial Evaluation A branch of compiler research has attempted to extend techniques from parallelizing compilers to programs with dynamic, data-dependent concurrency patterns <ref> [94, 117, 116, 115, 84] </ref>. These compilers use a two-phase partial evaluation approach. The first phase partially evaluates part of the program on its input data, then generates and schedules the resulting task graph and communication operations. The second phase actually executes the computation.
Reference: [118] <author> H. Sandu, B. Gamsa, and S. Zhou. </author> <title> The shared regions approach to software cache coherence on multiprocessors. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Future languages and systems will be increasingly organized around the interaction of data and computation, with various declarative mechanisms such as access specifications used to express the relevant information. COOL's locality hints [30], Midway's object usage declarations [19], shared region declarations <ref> [118] </ref> and the CHICO model of consistency [61] are all examples of this trend. Access specifications give the implementation enough information to automatically generate the communication without forcing the implementation to use a specific communication mechanism.
Reference: [119] <author> V. Saraswat and M. Rinard. </author> <title> Concurrent constraint programming. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 232-245, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: It is an error to attempt to define an element twice. 5.3.3 Concurrent Logic Programming Languages Concurrent logic programming languages such as Strand [40] and Parlog [50] and their generalization to the family of concurrent constraint-based programming languages <ref> [120, 119] </ref> present a model of computation based on constraints. The computation consists of a set of parallel agents that incrementally impose constraints on the values of the variables.
Reference: [120] <author> V. Saraswat, M. Rinard, and P. Panangaden. </author> <title> Semantic foundations of concurrent constraint programming. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 333-352, </pages> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: It is an error to attempt to define an element twice. 5.3.3 Concurrent Logic Programming Languages Concurrent logic programming languages such as Strand [40] and Parlog [50] and their generalization to the family of concurrent constraint-based programming languages <ref> [120, 119] </ref> present a model of computation based on constraints. The computation consists of a set of parallel agents that incrementally impose constraints on the values of the variables.
Reference: [121] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 197 5.3.4.1 Scheduling and Partitioning Monotonic languages typically expose concurrency at the level of the individual operations in the language. The scheduling overhead associated with exploiting concurrency at such a fine-grain level has inspired compiler efforts to partition the operations into larger sequential tasks <ref> [134, 121, 122, 123] </ref>. For good performance the partition should generate sufficient concurrency, minimize communication and successfully amortize the scheduling overhead. Programs written in lazy functional languages, Id and concurrent constraint languages generate an unordered set of operations that become enabled and execute in an information-driven way. <p> Most of the work in this area has taken place in the context of the eager functional language Sisal. In particular, Sarkar has developed a partitioning algorithm for Sisal that negotiates the resulting trade-off between concurrency and scheduling overhead <ref> [121] </ref>. MultiLisp [54, 55] relies on the programmer to specify a partitioning. The MultiLisp future construct allows the programmer to explicitly specify the task granularity by declaring that a given function invocation should be evaluated concurrently with the computation after the function.
Reference: [122] <author> V. Sarkar. </author> <title> Partition and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 197 5.3.4.1 Scheduling and Partitioning Monotonic languages typically expose concurrency at the level of the individual operations in the language. The scheduling overhead associated with exploiting concurrency at such a fine-grain level has inspired compiler efforts to partition the operations into larger sequential tasks <ref> [134, 121, 122, 123] </ref>. For good performance the partition should generate sufficient concurrency, minimize communication and successfully amortize the scheduling overhead. Programs written in lazy functional languages, Id and concurrent constraint languages generate an unordered set of operations that become enabled and execute in an information-driven way.
Reference: [123] <author> V. Sarkar and J. Hennessy. </author> <title> Compile-time partitioning and scheduling of parallel programs. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 17-26, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 197 5.3.4.1 Scheduling and Partitioning Monotonic languages typically expose concurrency at the level of the individual operations in the language. The scheduling overhead associated with exploiting concurrency at such a fine-grain level has inspired compiler efforts to partition the operations into larger sequential tasks <ref> [134, 121, 122, 123] </ref>. For good performance the partition should generate sufficient concurrency, minimize communication and successfully amortize the scheduling overhead. Programs written in lazy functional languages, Id and concurrent constraint languages generate an unordered set of operations that become enabled and execute in an information-driven way.
Reference: [124] <author> D. Scales and M. S. Lam. </author> <title> An efficient shared memory system for distributed memory machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: For parallel compilation the limiting factor on the performance is the disk bandwidth. Dan Scales also developed a Jade version of the Barnes-Hut application from the SPLASH benchmark suite as a preliminary step towards developing a version in SAM <ref> [124] </ref>. This program did not scale well in the current Jade implementation because of because of a synchronization bottleneck associated with the object queue for a key shared object. All of the tasks read this object, and the current implementation serializes all of the resulting object queue operations. <p> Another drawback is the execution overhead required to maintain the time stamps. Like page-based shared-memory systems, Midway interacts with the application at the level of the raw address space and therefore does not run on heterogeneous systems. SAM <ref> [124] </ref>, like several of the concurrent object-oriented systems described in Section CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 212 5.4.2.3, implements the shared address space at the granularity of user defined objects.
Reference: [125] <author> K. Schauser, D. Culler, and T. von Eicken. </author> <title> Compiler-controlled multithreading for lenient parallel languages. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 50-72. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: The lack of a statically derivable sequential execution order complicates the process of generating tasks large enough to profitably amortize the scheduling overhead. Research performed in the context of Id <ref> [125, 35, 34] </ref> attacks this problem with a two-level scheduling strategy. The compiler first statically partitions the program into threads. The run-time system then dynamically schedules related threads into larger-grain units called quanta.
Reference: [126] <author> J. Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1993. </year> <note> BIBLIOGRAPHY 240 </note>
Reference-contexts: Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics. Developing the same computation on different machines may therefore lead to radically different programs <ref> [126, 114] </ref>, and it can be difficult to port a program written for one machine to a machine with a substantially different programming interface [89]. Given the rapid development of parallel machines, this lack of portability presents a significant obstacle to the widespread use of parallel computation. <p> Requiring the programmer CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 203 to generate code to perform these activities complicates the programming process. A comparison of a fairly complex application written for both a message-passing system [114] and a shared-memory system <ref> [126, 89] </ref> highlights the programming burden that message-passing systems can impose. One clear advantage of message-passing systems is that they present a simple performance model. Every remote interaction is cleanly identified in the program and a simple model accurately predicts the cost of each message transfer [16].
Reference: [127] <author> J. Singh and J. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experiences, results, and implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1) </volume> <pages> 27-48, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: THE JADE LANGUAGE 37 block. At every iteration each task would communicate with its neighbor tasks to acquire the latest values of the boundary elements, then recompute the values of the elements in its block. The tasks would continue their computation, interacting until the algorithm converged <ref> [127] </ref>. In this parallel computation data flows cyclically through the tasks over the course of the computation each task both generates data that its neighbor tasks read and reads data generated by its neighbor tasks.
Reference: [128] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Several factors influenced our choice of applications. One major factor was availability. Existing benchmark sets were one source of applications, and three of our applications originally came from the SPLASH benchmark suite <ref> [128] </ref>. We also acquired three applications directly from the research groups that initially developed them. Another factor was the engineering effort involved in manipulating the application. <p> Water is derived from the Perfect Club benchmark MDG [21] and performs the same computation. The SPLASH benchmark suite <ref> [128] </ref> also contains a version of Water. I developed the Jade version of Water starting with serial C version that had been translated from the original Fortran. * String A program that computes a velocity model of the geology between two oil wells.
Reference: [129] <author> M. Smith. </author> <title> Tracing with Pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: One anomaly in the performance numbers is the difference in execution time between the serial, stripped and ANL versions. Further investigation reveals that the difference between the three versions is not caused by significant differences in the computation that they perform. We used the pixie instrumentation utility <ref> [129] </ref> to generate cycle counts for the three versions on DASH assuming a perfect memory system.
Reference: [130] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model [83, 52, 73] and the message-passing model <ref> [130, 68, 131] </ref>. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics. <p> If the programming environment exposes this nondeterminism to the programmer, it complicates the debugging process by making it difficult to reproduce and eliminate programming errors. 1.1 Explicitly Parallel Systems Programmers have traditionally developed software for parallel machines using explicitly parallel systems <ref> [88, 130] </ref>. These systems provide constructs that programmers use to create parallel tasks. On shared-memory machines the programmer synchronizes the tasks using low-level primitives such as locks, condition variables and barriers. On message-passing CHAPTER 1. <p> The explicitly parallel group includes systems in which the program explicitly generates parallel tasks. The tasks then synchronize and communicate using mechanisms such as locks, barriers, message-passing operations and/or shared memory. Packages like PVM <ref> [130] </ref>, Presto [17] and the ANL macro package [88] focus on providing very basic, low-level primitives used to create and coordinate parallel execution. Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. <p> Asynchronous sends return immediately, with the system invisibly buffering the data until the corresponding receive comes along. Actor languages [1, 2, 3], PVM <ref> [130] </ref> and the NX/2 system from Intel [104] are based on asynchronous sends. Synchronous sends block until the corresponding receive executes and the data transfer is complete. CML [106], Occam [86, 27], CSP [64, 63] and Ada [97] are based on synchronous message-passing constructs.
Reference: [131] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Introduction Existing parallel machines present two fundamentally different programming models: the shared-memory model [83, 52, 73] and the message-passing model <ref> [130, 68, 131] </ref>. Even machines that support the same basic model of computation may present interfaces with significantly different functionality and performance characteristics.
Reference: [132] <author> P. Tinker and M. Katz. </author> <title> Parallel execution of sequential Scheme with Paratran. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 28-39, </pages> <address> Snowbird, UT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: as Fortran and C implemented by a parallelizing compiler, Fortran D [42], languages such as Jade and FX-87 [87, 47] that provide information about how a serial program accesses data, data-parallel languages like C* [111, 112] and Fortran 90 [92], and speculative systems like Time Warp [70, 69] and ParaTran <ref> [132] </ref>. The monotonic group consists of systems whose basic computational model is the monotonic accumulation of information over the course of the computation. Each operation may wait for some information to become available, then generate additional information. <p> In effect, the dependences are discovered only as the tasks execute, and active enforcement of the dependences via rollback occurs only when a violation has been detected. Both the Time Warp system for distributed simulation [70, 69] and the ParaTran system for parallelizing serial Scheme code <ref> [132] </ref> use a speculative approach. Speculative systems can exploit concurrency available in programs that conservative approaches can not effectively analyze. Speculative systems have no need to analyze the preceding computation before executing a task, and can execute tasks concurrently even if they have potential dependences.
Reference: [133] <author> C. Tomlinson and V. Singh. </author> <title> Inheritance and synchronization with enabled-sets. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 103-112, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: When the object is ready to accept a message, it executes a construct that specifies the set of messages that it will accept at that point in time. The construct blocks until the object is sent one of the specified messages. The Rosette system <ref> [133] </ref> allows programmers to identify a set of object states and specify the set of messages that an object in a given state will accept. When an object finishes executing a method it specifies its next state.
Reference: [134] <author> K. Traub. </author> <title> Sequential implementation of lenient programming languages. </title> <type> Technical Report MIT/LCS/TR-417, </type> <institution> Massachussets Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 197 5.3.4.1 Scheduling and Partitioning Monotonic languages typically expose concurrency at the level of the individual operations in the language. The scheduling overhead associated with exploiting concurrency at such a fine-grain level has inspired compiler efforts to partition the operations into larger sequential tasks <ref> [134, 121, 122, 123] </ref>. For good performance the partition should generate sufficient concurrency, minimize communication and successfully amortize the scheduling overhead. Programs written in lazy functional languages, Id and concurrent constraint languages generate an unordered set of operations that become enabled and execute in an information-driven way.
Reference: [135] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Like the data-parallel approach, this approach works well for a specific kind of parallelism: the loop-level parallelism present in scientific programs that manipulate dense matrices. The complexity of the highly tuned, machine-specific code that parallelizing compilers generate <ref> [135] </ref> illustrates the need for high-level abstractions that shield programmers from the low-level details of their computations. 1.3 Jade Both the parallelizing compiler and the data-parallel approaches are designed to exploit regular concurrency available within a single operation on aggregate data structures.
Reference: [136] <author> A. Yonezawa, J. Briot, and E. Shibayama. </author> <title> Object oriented concurrent programming in ABCL/1. </title> <booktitle> In Proceedings of the OOPSLA-86 Conference, </booktitle> <pages> pages 258-268, </pages> <address> Portland OR, </address> <month> September </month> <year> 1986. </year>
Reference-contexts: Languages like Occam [86, 27], CSP [64, 63], CML [106] and Modula-3 [98] integrate concurrency generation and synchronization primitives into the language. Concurrent object-oriented languages like COOL [30], Orca [9], POOL-T [5] and ABCL/1 <ref> [136] </ref> hide such primitives behind higher-level constructs. 5.2 Serial Semantics Much of the complexity of parallel execution comes from the fact that parallel tasks can generate many different interleavings of the basic operations, with each interleaving generating a potentially different behavior. <p> We first discuss how researchers have augmented the object-oriented model of computation to include the generation of parallel execution. In languages like POOL-T [5], objects are given threads of control which execute concurrently. Languages like ABCL/1 <ref> [136] </ref> and COOL [30] support the concept of asynchronous methods, which execute concurrently with the invoking thread. The computation can synchronize for the return value using a mechanism similar to MultiLisp futures. The synchronization mechanisms are all oriented around how the program accesses CHAPTER 5. PARALLEL PROGRAMMING SYSTEMS 208 objects.
Reference: [137] <author> S. Zhou, M. Stumm, K. Li, and D. Wortman. </author> <title> Heterogeneous distributed shared memory. </title> <type> Technical Report CSRI-244, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: These restrictions have so far limited page-based systems to homogeneous collections of machines. The lone exception (a heterogeneous prototype described in <ref> [137] </ref>) requires all of the compilers for the different machines to lay out data the same way and to store only one kind of data on each page. The entry consistency protocol of Midway implements the shared address space at the granularity of program-defined regions [18, 19]. <p> For such systems to correctly implement the abstraction of a single coherent address space, each machine must use the same virtual address space for the computation. In heterogeneous systems this restriction imposes unreasonable constraints on the compilers <ref> [137] </ref>. The extra structure inherent in the Jade object model would allow the implementation to extend techniques from page-based systems for use in heterogeneous computing environments. The only restriction is that the operating system must support user-level paging operations similar to those described in [7].
References-found: 137

