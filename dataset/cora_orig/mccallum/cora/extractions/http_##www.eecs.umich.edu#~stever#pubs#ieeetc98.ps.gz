URL: http://www.eecs.umich.edu/~stever/pubs/ieeetc98.ps.gz
Refering-URL: http://www.eecs.umich.edu/~stever/pubs.html
Root-URL: http://www.eecs.umich.edu
Title: Hardware Support for Flexible Distributed Shared Memory  
Author: Steven K. Reinhardt, Robert W. Pfile, and David A. Wood 
Keyword: Index terms: parallel systems, distributed shared memory, cache coherence protocols, fine-grain cache coher ence, coherence protocol optimization, workstation clusters.  
Date: 3  
Note: Copyright 1998 IEEE. Published in IEEE Transactions on Computers, vol. 47, no. 10, Oct. 1998. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.  
Abstract: Workstation-based parallel systems are attractive due to their low cost and competitive uniprocessor performance. However, supporting a cache-coherent global address space on these systems involves significant overheads. We examine two approaches to coping with these overheads. First, DSM-specific hardware can be added to the off-the-shelf component base to reduce overheads. Second, applicationspecific coherence protocols can avoid some overheads by exploiting programmer (or compiler) knowledge of an applications communication patterns. To explore the interaction between these approaches, we simulated four designs that add DSM acceleration hardware to a collection of off-the-shelf workstation nodes. Three of the designs support user-level software coherence protocols, enabling applicationspecific protocol optimizations. To verify the feasibility of our hardware approach, we constructed a prototype of the simplest design. Measured speedups from the prototype match simulation results closely. We find that even with aggressive DSM hardware support, custom protocols can provide significant speedups for some applications. In addition, the custom protocols are generally effective at reducing the impact of other overheads, including those due to less aggressive hardware support and larger network latencies. However, for three of our benchmarks, the additional hardware acceleration provided by our most aggressive design avoids the need to develop more efficient custom protocols. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak ordering - a new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 214, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Tempest also aids the optimization process itself by enabling extended coherence protocols that collect profiling information [34, 57]. This information drives high-level tools that help programmers identify and understand performance bottlenecks that may benefit from custom protocols. Unlike systems that provide relaxed memory consistency models <ref> [1, 18] </ref>, the effects of these custom protocols are limited to programmerspec-ified memory regions and execution phases; the remainder of the program sees a conventional sequentially consistent shared memory. At its simplest, Tempests flexibility lets users select from a menu of available shared-memory protocols, as in Munin [10].
Reference: [2] <author> A. Agarwal, R. Bianchini, D. Chaiken, K. L. Johnson, D. Kranz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In spite of these trends, custom parallel systems still hold an advantage over off-the-shelf clus ters of workstations in their ability to provide a low-overhead, globally coherent shared address space. Custom-built distributed shared memory (DSM) systems such as MITs Alewife <ref> [2] </ref>, Stan 1. EECS Dept., The University of Michigan, 1301 Beal Ave., Ann Arbor, MI 48109-2122. stever@eecs.umich.edu. 2. Yago Systems, 795 Vaqueros Ave., Sunnyvale, CA 94086. pfile@yagosys.com. 3. CS Dept., University of WisconsinMadison, 1210 W. <p> The accu racy is higher for standard shared memory than for the custom protocols (2.5% vs. 3.5% mean error). This trend is reasonable, because custom protocols tend to have bursty communication pat terns which place greater peak demand on the networkthe least accurately modeled component. 6 Related Work Alewife <ref> [2] </ref>, S3.mp [38], and FLASH [28] implement their shared-memory coherence proto cols at least partially in software. These systems forgo the economies of workstation-based nodes for higher performance, incorporating custom memory controllers and, in the case of Alewife, a custom CPU as well.
Reference: [3] <author> T. E. Anderson, D. E. Culler, and D. A. Patterson. </author> <title> A case for NOW (networks of workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1):5464, </volume> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Technological and economic trends make it increasingly cost-effective to assemble distributed memory parallel systems from off-the-shelf workstations and networks <ref> [3] </ref>. Workstations (or, equivalently, high-end personal computers) use the same high-performance microprocessors found in larger systemsbut at a far lower cost per processor, thanks to their much greater market volume. More recently, vendors have begun to advertise high-bandwidth, low-latency switched networks targeted specifically at the cluster market.
Reference: [4] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The original papers reporting on these optimizations (Falsafi et al. [16] for Appbt, Barnes, and EM3D, and Mukherjee et al. [37] for DSMC, moldyn, and unstructured) detail the applications and the evolutionary optimization process. Appbt is a computational fluid dynamics code from the NAS Parallel Benchmarks <ref> [4] </ref>, parallel 8 ized by Burger and Mehta [9]. Each processor works on a subcube of a three-dimensional matrix, sharing values along the subcube faces with its neighbors.
Reference: [5] <author> H. E. Bal and M. F. Kaashoek. </author> <title> Object distribution in Orca using compile-time and runtime techniques. </title> <booktitle> In Proceedings of the Eigth Annual Conference on ObjectOriented Programming Systems, Languages and Applications (OOPSLA 93), </booktitle> <pages> pages 162177, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: We believe that the magnitude of these efforts is due to our lack of experience and the intentionally low level of the Tempest interface, and will be dramatically reduced through software reuse, protocol development tools [12], and compiler support for protocol generation and selection <ref> [5, 11, 15] </ref>. Nevertheless, because Tempest enables a nearly unlimited spectrum of communication optimizations, it provides a good environment for investigating the interaction of these optimizations with hardware support. 3 Hardware support for Tempest To examine the impact of hardware support on both standard and Tempest-optimized shared-1. <p> Orca used compiler analysis with runtime feedback to control object replication policies and update protocols <ref> [5] </ref>. Bianchini et al. [6] propose a hardware accelerator for page-based DSM. As our results in Section 4.3 indicate, hardware support for fine-grain coherence is unlikely to be cost effective when faced with the high latencies (and overheads) of traditional local-area networks.
Reference: [6] <author> R. Bianchini, L. I. Kontothanassis, R. Pinto, M. D. Maria, M. Abud, and C. L. Amorim. </author> <title> Hiding communication latency and coherence overhead in software DSMs. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <pages> pages 198209, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Orca used compiler analysis with runtime feedback to control object replication policies and update protocols [5]. Bianchini et al. <ref> [6] </ref> propose a hardware accelerator for page-based DSM. As our results in Section 4.3 indicate, hardware support for fine-grain coherence is unlikely to be cost effective when faced with the high latencies (and overheads) of traditional local-area networks.
Reference: [7] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: When a block is initially Writable, the bus transaction also retrieves an up-to-date copy, because the data could be modified in a hardware cache. A shadow space <ref> [7, 22, 54] </ref> allows direct manipulation of access control tags from Tempests user-level protocol software. A shadow space is a physical address range as large as, and at a fixed offset from, the machines physical memory address range.
Reference: [8] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: More recently, vendors have begun to advertise high-bandwidth, low-latency switched networks targeted specifically at the cluster market. We expect this trend to continue and for the bandwidth and latency characteristics of cluster networks to approach those of dedicated parallel system interconnects <ref> [8, 19] </ref>. In spite of these trends, custom parallel systems still hold an advantage over off-the-shelf clus ters of workstations in their ability to provide a low-overhead, globally coherent shared address space. Custom-built distributed shared memory (DSM) systems such as MITs Alewife [2], Stan 1. <p> Just as Typhoon-1 leverages high-performance general-purpose CPUs, the Typhoon-0 design also capitalizes on emerging low-latency commercial networks such as Myricoms Myrinet <ref> [8] </ref> and DECs Memory Channel [19]. The custom device contains only fine-grain access control logic. Section 5 describes an FPGA-based prototype of Typhoon-0 that demonstrates the feasibility and relative simplicity of this design. Of course, the lack of network interface/access control integration increases Typhoon-0s overheads relative to Typhoon and Typhoon-1. <p> To emphasize the performance impact of DSM support, the default latency is a fairly aggressive 0.5 ms (100 processor cycles). Although current off-the-shelf networks are typically slower, the last few years have seen rapid performance advances in commercial system-area networks <ref> [8, 19] </ref>. We believe that networks with this level of performance will be available in the near future. Section 4.3 examines the overall performance impact of higher-latency networks. 21 The simulated network supports barrier synchronization in hardware, as in the CM-5. <p> The nodes 33 communicate through a single Myrinet switch with a worst-case latency of 550 ns <ref> [8] </ref>, so virtually all of this time is consumed by the SBus bridge and the Myrinet interface. The primary culprit is control software running on the Myrinet processora 16-bit non-pipelined CISC processor clocked at 25 MHz.
Reference: [9] <author> D. Burger and S. Mehta. </author> <title> Parallelizing appbt for a shared-memory multiprocessor. </title> <type> Technical Report 1286, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Appbt is a computational fluid dynamics code from the NAS Parallel Benchmarks [4], parallel 8 ized by Burger and Mehta <ref> [9] </ref>. Each processor works on a subcube of a three-dimensional matrix, sharing values along the subcube faces with its neighbors. In the original version, processors spin on counters to determine when each column of a neighbors face is available, then fetch the data via demand misses.
Reference: [10] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: At its simplest, Tempests flexibility lets users select from a menu of available shared-memory protocols, as in Munin <ref> [10] </ref>. Programmers or compilers can further optimize performance by developing custom, applicationspecific protocols that optimize coherence traffic based on knowledge of an applications synchronization and sharing patterns. <p> In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it. Research in workstation-based DSM has focused on all-software methods, using page-based coherence [32] or software-only fine-grain coherence [50, 48]. Munin <ref> [10] </ref> was a page-based DSM system that provided a fixed menu of protocols; programmer annotations guided protocol (b) Barnes (a) Appbt 0 8 16 24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 Number of Nodes S e d p Standard simulated Standard
Reference: [11] <author> S. Chandra and J. R. Larus. </author> <title> Optimizing communication in HPF programs on fine-grain distributed shared memory. </title> <booktitle> In Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 100 111, </pages> <month> June </month> <year> 1997. </year> <month> 39 </month>
Reference-contexts: Similarly, just as past improvements in compiler technology have reduced the need to code in assembly language for performance, we hope that future compilers will target Tempest to provide similar levels of optimization without expert programmer involvement <ref> [11] </ref>. 4 mance sensitivity to these parameters as well. However, Typhoon executes the standard versions of three of our benchmarks nearly as quickly as the custom-protocol versions, and faster than the custom-protocol versions on the less aggressive systems. <p> Thus, custom protocols typically do not require non-trivial modification of the original shared-memory source code. The custom protocols themselves currently require significant development effort; however, we seek to remedy this situation through software reuse, high-level tools for protocol development [12], and automatic compiler application of optimized protocols <ref> [11] </ref>. Although Tempest custom protocols use message passing to communicate between nodes, they directly support the higher-level shared-memory abstraction. <p> We believe that the magnitude of these efforts is due to our lack of experience and the intentionally low level of the Tempest interface, and will be dramatically reduced through software reuse, protocol development tools [12], and compiler support for protocol generation and selection <ref> [5, 11, 15] </ref>. Nevertheless, because Tempest enables a nearly unlimited spectrum of communication optimizations, it provides a good environment for investigating the interaction of these optimizations with hardware support. 3 Hardware support for Tempest To examine the impact of hardware support on both standard and Tempest-optimized shared-1.
Reference: [12] <author> S. Chandra, B. Richards, and J. R. Larus. Teapot: </author> <title> Language support for writing memory coherence protocols. </title> <booktitle> In Proceedings of the SIGPLAN 96 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Thus, custom protocols typically do not require non-trivial modification of the original shared-memory source code. The custom protocols themselves currently require significant development effort; however, we seek to remedy this situation through software reuse, high-level tools for protocol development <ref> [12] </ref>, and automatic compiler application of optimized protocols [11]. Although Tempest custom protocols use message passing to communicate between nodes, they directly support the higher-level shared-memory abstraction. <p> We believe that the magnitude of these efforts is due to our lack of experience and the intentionally low level of the Tempest interface, and will be dramatically reduced through software reuse, protocol development tools <ref> [12] </ref>, and compiler support for protocol generation and selection [5, 11, 15].
Reference: [13] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: This optimization modified only two lines in the original program. The remote-write protocol requires approximately 150 lines of code. EM3D models electromagnetic wave propagation through three-dimensional objects by iteratively updating the values at each node of a graph as a function of its neighbors values <ref> [13] </ref>. Each processor owns a contiguous subset of the graph nodes and updates only the nodes it owns. Where a graph edge connects nodes owned by different processors, each processor must fetch new values for the nonlocal nodes on every iteration.
Reference: [14] <author> D. E. Culler, L. T. Liu, R. P. Martin, and C. O. Yoshikawa. </author> <title> Assessing fast network interfaces. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 3543, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The host processors can access the interface SRAM directly using uncached memory operations; however, the interface processor can access host memory only via the DMA engine. Our prototypes interface processors run custom software derived from Berkeleys Active Messages implementation <ref> [14] </ref>. Schoinas et al. [49] describe our modifications and enhancements for Tempest. As in the Berkeley implementation, our software allocates user-accessible send and receive queues in the shared SRAM. <p> Adapting the network model was more involved. The SBus bridge and Myrinet interface are the dominant contributors to message latency (as seen in Section 5.2.1) and are bottlenecks for message throughput <ref> [14] </ref>. Rather than develop detailed models of these complex components, we modified our existing network interface modeladjusting register access costs and adding an occupancy componentto approximate the Myrinets performance characteristics. We tuned the new model until the simulators performance matched the prototypes on a message-passing microbenchmark.
Reference: [15] <author> S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <pages> pages 186197, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: We believe that the magnitude of these efforts is due to our lack of experience and the intentionally low level of the Tempest interface, and will be dramatically reduced through software reuse, protocol development tools [12], and compiler support for protocol generation and selection <ref> [5, 11, 15] </ref>. Nevertheless, because Tempest enables a nearly unlimited spectrum of communication optimizations, it provides a good environment for investigating the interaction of these optimizations with hardware support. 3 Hardware support for Tempest To examine the impact of hardware support on both standard and Tempest-optimized shared-1.
Reference: [16] <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood. </author> <title> Applicationspecific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 380389, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Again, although these changes were performed manually at the lowest level, we expect future development tools to aid or automate this process. The original papers reporting on these optimizations (Falsafi et al. <ref> [16] </ref> for Appbt, Barnes, and EM3D, and Mukherjee et al. [37] for DSMC, moldyn, and unstructured) detail the applications and the evolutionary optimization process. Appbt is a computational fluid dynamics code from the NAS Parallel Benchmarks [4], parallel 8 ized by Burger and Mehta [9]. <p> Computation alternates between two phases: rebuilding the tree to reflect new body positions, and traversing the tree to calculate the forces on each body and update its position. The standard shared-memory version we use incorporates several optimizations not in the original SPLASH code <ref> [16] </ref>. The Tempest-optimized version splits the body structure into three parts and applies a different protocol to each: read-only fields (e.g., the bodys mass) use the default protocol, fields accessed only by the current owner use a custom migratory protocol, and the read-write position field uses a custom update protocol.
Reference: [17] <author> B. Falsafi and D. A. Wood. </author> <title> Scheduling communication on an SMP node parallel machine. </title> <booktitle> In Proceedings of the 3rd International Symposium on High-Performance Computer Architecture (HPCA), </booktitle> <pages> pages 128138, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Second, a general-purpose protocol processor may be used for the applications primary computation when there are no protocol events to handle. Similarly, any processor on a symmetric multiprocessor node may serve as the protocol processor. Falsafi and Wood <ref> [17] </ref> show that dynamically scheduling protocol processing tasks across all available processors is often more efficient than dedicating a protocol processor. Both Typhoon-1 and Typhoon-0 are capable of supporting this dynamic model.
Reference: [18] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1526, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Tempest also aids the optimization process itself by enabling extended coherence protocols that collect profiling information [34, 57]. This information drives high-level tools that help programmers identify and understand performance bottlenecks that may benefit from custom protocols. Unlike systems that provide relaxed memory consistency models <ref> [1, 18] </ref>, the effects of these custom protocols are limited to programmerspec-ified memory regions and execution phases; the remainder of the program sees a conventional sequentially consistent shared memory. At its simplest, Tempests flexibility lets users select from a menu of available shared-memory protocols, as in Munin [10].
Reference: [19] <author> R. B. Gillett. </author> <title> Memory channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1):1218, </volume> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: More recently, vendors have begun to advertise high-bandwidth, low-latency switched networks targeted specifically at the cluster market. We expect this trend to continue and for the bandwidth and latency characteristics of cluster networks to approach those of dedicated parallel system interconnects <ref> [8, 19] </ref>. In spite of these trends, custom parallel systems still hold an advantage over off-the-shelf clus ters of workstations in their ability to provide a low-overhead, globally coherent shared address space. Custom-built distributed shared memory (DSM) systems such as MITs Alewife [2], Stan 1. <p> Just as Typhoon-1 leverages high-performance general-purpose CPUs, the Typhoon-0 design also capitalizes on emerging low-latency commercial networks such as Myricoms Myrinet [8] and DECs Memory Channel <ref> [19] </ref>. The custom device contains only fine-grain access control logic. Section 5 describes an FPGA-based prototype of Typhoon-0 that demonstrates the feasibility and relative simplicity of this design. Of course, the lack of network interface/access control integration increases Typhoon-0s overheads relative to Typhoon and Typhoon-1. <p> To emphasize the performance impact of DSM support, the default latency is a fairly aggressive 0.5 ms (100 processor cycles). Although current off-the-shelf networks are typically slower, the last few years have seen rapid performance advances in commercial system-area networks <ref> [8, 19] </ref>. We believe that networks with this level of performance will be available in the near future. Section 4.3 examines the overall performance impact of higher-latency networks. 21 The simulated network supports barrier synchronization in hardware, as in the CM-5.
Reference: [20] <author> L. Gwennap. </author> <title> Intels P6 bus designed for multiprocessing. </title> <type> Microprocessor Report, 9(7), </type> <month> May 30, </month> <year> 1995. </year>
Reference-contexts: Although this technique cannot be implemented on an unmodified SPARCstation 20, its performance is representative of more recent systems which support deferred responses, either explicitly (like the Intel P6 <ref> [20] </ref>) or using a split-transaction bus. Timing parameters for the Typhoon-0 and Typhoon-1 access control devices are taken from the FPGA-based Typhoon-0 prototype implementation described in Section 5.
Reference: [21] <author> E. Hagersten, A. Saulsbury, and A. Landin. </author> <title> Simple COMA node implementations. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Using simulation, we compare the performance of both standard and custom-protocol versions of our benchmarks on these systems against a baseline hardwaresupported DSM design (similar to Simple COMA <ref> [21] </ref>) that does not allow user protocol customization. While custom protocols provide less relative improvement over standard shared memory on more aggressive hardware, two benchmarks still show dramatic gains 86% and 384%on the most aggressive Tempest system (Typhoon), outperforming the Simple COMA system by nearly the same amount. <p> Only EM3D gains significantly from this optimization. The Tempest systems also support multiple data-structure-dependent block sizes within a single application, which have been shown to improve speedups further [48]. Although the original Simple COMA proposal uses a fixed block size <ref> [21] </ref>, we report the effect of choosing the best per-application block size for that plat form as well. Next we turn to the Tempest-optimized versions described in Section 2.2, whose speedups are indicated by the hatched bars in Figure 6.
Reference: [22] <author> J. Heinlein, K. Gharachorloo, S. A. Dresser, and A. Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 3850, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Although Tempest custom protocols use message passing to communicate between nodes, they directly support the higher-level shared-memory abstraction. In contrast, other systems that seek to integrate message passing and shared memory treat user-level message passing as a complementary alternative torather than a fundamental building block forshared-memory communication <ref> [22, 27, 56] </ref>. To optimize communication in these systems, critical portions of the program must be rewritten in a message-passing style. Of course, if desired, Tempest programmers can also dispense with shared memory and use messages directlyfor example, to implement synchronization primitives. <p> When a block is initially Writable, the bus transaction also retrieves an up-to-date copy, because the data could be modified in a hardware cache. A shadow space <ref> [7, 22, 54] </ref> allows direct manipulation of access control tags from Tempests user-level protocol software. A shadow space is a physical address range as large as, and at a fixed offset from, the machines physical memory address range. <p> None of these systems provides a protected, user-level interface to its mem ory coherence mechanisms, which limits users to a predefined set of system-provided policies. The Alewife and FLASH projects also explored combined message-passing and shared-memory programming models <ref> [27, 22] </ref>. However, both systems primarily viewed user-level message pass ing as an alternative to, rather than a building block for, shared memory. In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it.
Reference: [23] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J. Baxter, J. P. Singh, R. Simoni, K. Gharachorloo, D. Nakahira, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 274285, </pages> <year> 1994. </year>
Reference-contexts: For comparison, the Stanford FLASH designers report remote read miss latencies of 1.11 and 1.45 ms, depending on whether the data is dirty in the remote processors cache <ref> [23] </ref>. 1 Because these funda mental latencies dominate, Typhoon takes only 33% longer to satisfy the miss despite the cost of Table 1. Remote miss latency breakdown for simulated systems. Step Latency (200 MHz cycles) SC Typh.
Reference: [24] <author> M. D. Hill. </author> <title> Multiprocessors should support simple memory consistency models. </title> <journal> IEEE Computer, </journal> <volume> vol. 31. no. 8, </volume> <pages> pp. 2834, </pages> <month> Aug. </month> <year> 1998. </year>
Reference-contexts: By default, applications use a standard software protocol that provides sequentially consistent shared memory, the consistency model generally preferred by most programmers for its simplicity <ref> [24] </ref>.
Reference: [25] <author> P. Keleher, A. L. Cox, and W. Zwanepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1321, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We expect that fine-grain DSM will show a performance advantage on systems with lower overheads and lower network latencies, such as those simulated in Section 4. Other approaches to optimizing communication in DSM systems include relaxed memory models <ref> [25] </ref>, prefetching [35], and special writes that deliver data to other nodes [31, 47]. The Tempest interface can support all of these optimizations; however, we have focused on exploring the limits of optimization by using fully custom protocols.
Reference: [26] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter 94 Usenix Conference, </booktitle> <pages> pages 115131, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Increasing the fraction of execution time due to the network itself diminishes the relative impact of other overheads. Sufficiently high network overheads make any hardware DSM support superfluous, leaving software-only fine-grain [50, 48] or page-based <ref> [26] </ref> systems as the most cost-effective approaches to DSM in this domain. (a) Appbt (b) Barnes 0 16 32 Network Latency/System S e d p Custom protocol Standard SM 0.5 s 2.5 s 5 s 25 s 50 s 8 24 Network Latency/System S e d p 29 4.4 Impact of
Reference: [27] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.-H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Although Tempest custom protocols use message passing to communicate between nodes, they directly support the higher-level shared-memory abstraction. In contrast, other systems that seek to integrate message passing and shared memory treat user-level message passing as a complementary alternative torather than a fundamental building block forshared-memory communication <ref> [22, 27, 56] </ref>. To optimize communication in these systems, critical portions of the program must be rewritten in a message-passing style. Of course, if desired, Tempest programmers can also dispense with shared memory and use messages directlyfor example, to implement synchronization primitives. <p> None of these systems provides a protected, user-level interface to its mem ory coherence mechanisms, which limits users to a predefined set of system-provided policies. The Alewife and FLASH projects also explored combined message-passing and shared-memory programming models <ref> [27, 22] </ref>. However, both systems primarily viewed user-level message pass ing as an alternative to, rather than a building block for, shared memory. In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it.
Reference: [28] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: EECS Dept., The University of Michigan, 1301 Beal Ave., Ann Arbor, MI 48109-2122. stever@eecs.umich.edu. 2. Yago Systems, 795 Vaqueros Ave., Sunnyvale, CA 94086. pfile@yagosys.com. 3. CS Dept., University of WisconsinMadison, 1210 W. Dayton St., Madison, WI 53706-1685. david@cs.wisc.edu. 2 fords FLASH <ref> [28] </ref>, and the SGI Origin 2000 [30] integrate the memory and network control and datapaths at each processormemory node, providing efficient coordination among processor requests, network messages, and DRAM accesses. <p> because the integrated processor will face die size constraints and incur design delay due to component integration and testing, an off-the-shelf processor will almost certainly have higher raw performance. (However, the integrated processors effective performance can be increased by optimizing the microarchi 16 tecture for protocol handling, as in FLASH <ref> [28] </ref>.) Section 4.4 investigates the performance impact of a slower integrated processor. Second, a general-purpose protocol processor may be used for the applications primary computation when there are no protocol events to handle. Similarly, any processor on a symmetric multiprocessor node may serve as the protocol processor. <p> This trend is reasonable, because custom protocols tend to have bursty communication pat terns which place greater peak demand on the networkthe least accurately modeled component. 6 Related Work Alewife [2], S3.mp [38], and FLASH <ref> [28] </ref> implement their shared-memory coherence proto cols at least partially in software. These systems forgo the economies of workstation-based nodes for higher performance, incorporating custom memory controllers and, in the case of Alewife, a custom CPU as well.
Reference: [29] <author> J. R. Larus and E. Schnarr. EEL: </author> <title> Machine-independent executable editing. </title> <booktitle> In Proceedings of the SIGPLAN 95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 291300, </pages> <month> June </month> <year> 1995. </year> <month> 40 </month>
Reference-contexts: To obtain results, application codes are compiled and linked with portable software protocols (written in C using the Tempest interface) and platformspecific Tempest runtime software, exactly as they would be for an actual implementation. A rewriting tool (based on EEL <ref> [29] </ref>) processes the resulting SPARC binaries, replacing memory accesses with calls to the simulator and adding instrumentation to count instruction execution cycles. Direct execution of the modified binaries drives the detailed discrete-event simulator.
Reference: [30] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA highly scalable server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 241251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: EECS Dept., The University of Michigan, 1301 Beal Ave., Ann Arbor, MI 48109-2122. stever@eecs.umich.edu. 2. Yago Systems, 795 Vaqueros Ave., Sunnyvale, CA 94086. pfile@yagosys.com. 3. CS Dept., University of WisconsinMadison, 1210 W. Dayton St., Madison, WI 53706-1685. david@cs.wisc.edu. 2 fords FLASH [28], and the SGI Origin 2000 <ref> [30] </ref> integrate the memory and network control and datapaths at each processormemory node, providing efficient coordination among processor requests, network messages, and DRAM accesses. In contrast, machines assembled from off-the-shelf workstationseven those that add custom hardware to accelerate DSM operationsmust build on top of the workstations existing memory systems.
Reference: [31] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: The access control device masks the arbiter to keep the processor off the bus until the access can be completed <ref> [31] </ref>. Although this technique cannot be implemented on an unmodified SPARCstation 20, its performance is representative of more recent systems which support deferred responses, either explicitly (like the Intel P6 [20]) or using a split-transaction bus. <p> Other approaches to optimizing communication in DSM systems include relaxed memory models [25], prefetching [35], and special writes that deliver data to other nodes <ref> [31, 47] </ref>. The Tempest interface can support all of these optimizations; however, we have focused on exploring the limits of optimization by using fully custom protocols.
Reference: [32] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Virtual memory management allows users to manage a portion of their virtual address space on each node. Users achieve global shared addressing by mapping the same virtual page into physical memory on multiple nodes, much like conventional shared virtual memory systems <ref> [32] </ref>. Fine-grain memory access control helps maintain coherence between these multiple copies by allowing users to tag small (e.g., 32-byte), aligned memory blocks as Invalid, ReadOnly, or Writable. <p> In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it. Research in workstation-based DSM has focused on all-software methods, using page-based coherence <ref> [32] </ref> or software-only fine-grain coherence [50, 48].
Reference: [33] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using simple page placement policies to reduce the cost of cache fills in coherent shared-memory systems. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Applications can specify a pages home node explicitly or use a round-robin or first-touch policy. For benchmarks that have an explicit serial initialization phase, the first-touch policy migrates the page to the node that first references it during the parallel phase <ref> [33] </ref>. Nonhome nodes allocate memory for shared pages on demand via the page fault handler. Ini 6 tially, these pages contain no valid data, so all their blocks are tagged Invalid.
Reference: [34] <author> M. Martonosi, D. Ofelt, and M. Heinrich. </author> <title> Integrating performance monitoring and communication in parallel computers. </title> <booktitle> In Proceedings of the 1996 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 138147, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, the real power of Tempest lies in the opportunity to optimize performance using customized coherence protocols tailored to specific data structures and specific phases within an application. Tempest also aids the optimization process itself by enabling extended coherence protocols that collect profiling information <ref> [34, 57] </ref>. This information drives high-level tools that help programmers identify and understand performance bottlenecks that may benefit from custom protocols.
Reference: [35] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12:87106, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: We expect that fine-grain DSM will show a performance advantage on systems with lower overheads and lower network latencies, such as those simulated in Section 4. Other approaches to optimizing communication in DSM systems include relaxed memory models [25], prefetching <ref> [35] </ref>, and special writes that deliver data to other nodes [31, 47]. The Tempest interface can support all of these optimizations; however, we have focused on exploring the limits of optimization by using fully custom protocols.
Reference: [36] <author> S. S. Mukherjee, S. K. Reinhardt, B. Falsafi, M. Litzkow, S. Huss-Lederman, M. D. Hill, J. R. Larus, and D. A. Wood. </author> <title> Wisconsin Wind Tunnel II: A fast and portable parallel architecture simulator. In Workshop on Performance Analysis and Its Impact on Design (PAID), </title> <month> June </month> <year> 1997. </year>
Reference-contexts: Direct execution of the modified binaries drives the detailed discrete-event simulator. To enable larger systems and data sets, the system nodes are simulated in parallel on a Thinking Machines CM-5 using the Wisconsin Wind Tunnel II <ref> [43, 36] </ref>. 4.1 Microbenchmark To gain insight into the overheads of these systems, we trace a simple remote read miss and break down the latency into its components.
Reference: [37] <author> S. S. Mukherjee, S. D. Sharma, M. D. Hill, J. R. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Again, although these changes were performed manually at the lowest level, we expect future development tools to aid or automate this process. The original papers reporting on these optimizations (Falsafi et al. [16] for Appbt, Barnes, and EM3D, and Mukherjee et al. <ref> [37] </ref> for DSMC, moldyn, and unstructured) detail the applications and the evolutionary optimization process. Appbt is a computational fluid dynamics code from the NAS Parallel Benchmarks [4], parallel 8 ized by Burger and Mehta [9].
Reference: [38] <author> A. Nowatzyk, M. Monger, M. Parkin, E. Kelly, M. Browne, G. Aybay, and D. Lee. S3.mp: </author> <title> A multiprocessor in a matchbox. </title> <booktitle> In Proc. </booktitle> <address> PASA, </address> <year> 1993. </year>
Reference-contexts: This trend is reasonable, because custom protocols tend to have bursty communication pat terns which place greater peak demand on the networkthe least accurately modeled component. 6 Related Work Alewife [2], S3.mp <ref> [38] </ref>, and FLASH [28] implement their shared-memory coherence proto cols at least partially in software. These systems forgo the economies of workstation-based nodes for higher performance, incorporating custom memory controllers and, in the case of Alewife, a custom CPU as well.
Reference: [39] <author> R. W. Pfile. </author> <title> Typhoon-Zero implementation: The Vortex module. </title> <type> Technical Report 1290, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: Due to space limitations, we describe the prototype only briefly, focusing on changes from the simulated Typhoon-0 of Section 4. We then report performance measurements and compare measured speedups with simulation results. Reinhardts Ph.D. thesis [41] describes the prototype system more fully, including operating system support. Pfiles masters report <ref> [39] </ref> details the Vortex hardware implementation. 5.1 Differences between the simulated and constructed systems Our simulated Typhoon-0 system of Section 4 reflects our original design for the prototype. 31 However, constraints encountered during construction induced three changes: the CPU configuration, the method for suspending bus operations, and the network used to
Reference: [40] <author> S. K. Reinhardt. </author> <title> Tempest interface specification (revision 1.2.1). </title> <type> Technical Report 1267, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Users may specify different sets of handlers for different virtual memory pages; thus multiple protocols may coexist peacefully, each managing a distinct set of shared pages. A complete specification of Tempest is available in a technical report <ref> [40] </ref>. 2.1 Standard shared memory using Tempest Tempests mechanisms are sufficient to develop protocol software that transparently supports standard shared-memory applications. Stache, the default Tempest protocol included with the standard Tempest runtime library, is an example of such a protocol.
Reference: [41] <author> S. K. Reinhardt. </author> <title> Mechanisms for Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: The first subsection covers common features; following subsections describe Typhoon, Typhoon-1, and Typhoon-0 in turn. Additional details can be found in previous publications <ref> [41, 44, 45] </ref>. 3.1 Common features To focus on the impact of the organizational differences in our designs, we assume similar technology for the three common componentsthe protocol processor, network interface, and access control logic. The protocol processor is a general-purpose CPU. <p> This devicethe only custom componentis implemented as a FPGA-based plug-in board called Vortex. Due to space limitations, we describe the prototype only briefly, focusing on changes from the simulated Typhoon-0 of Section 4. We then report performance measurements and compare measured speedups with simulation results. Reinhardts Ph.D. thesis <ref> [41] </ref> describes the prototype system more fully, including operating system support.
Reference: [42] <author> S. K. Reinhardt, B. Falsafi, and D. A. Wood. </author> <title> Kernel support for the Wisconsin Wind Tunnel. </title> <booktitle> In Proceedings of the USENIX Symposium on Microkernels and Other Kernel Architectures, </booktitle> <pages> pages 7389, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: To avoid this penalty, we modified the kernels trap vector code to invoke a user handler directly in under 5 ms <ref> [42, 53] </ref>. The suspended thread can be resumed without going through the kernel. The third and most significant change involves the network used to connect the nodes. The prototype employs the Myricom Myrinet, the lowest-latency interconnect commercially available at that time.
Reference: [43] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Direct execution of the modified binaries drives the detailed discrete-event simulator. To enable larger systems and data sets, the system nodes are simulated in parallel on a Thinking Machines CM-5 using the Wisconsin Wind Tunnel II <ref> [43, 36] </ref>. 4.1 Microbenchmark To gain insight into the overheads of these systems, we trace a simple remote read miss and break down the latency into its components.
Reference: [44] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325337, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: This paper discusses both options, focusing on the interaction between them. Can we build hardware that accelerates DSM performance yet provides flexibility to optimize communication? To what extent does hardware support make protocol optimizations unnecessary, and vice versa? We examine these questions in the context of Tempest <ref> [44] </ref>, an interface that lets user-level Node N-1Node 0 Network Cache CPU Support Cache CPU Support 3 (unprivileged) software manage coherence at a fine granularity within a distributed shared address space. <p> Custom protocols also effectively hide the higher overheads of less aggressive hardware; they decrease the performance gap between Typhoon-1 and Typhoon from 222% to 11% and the gap between Typhoon-0 and Typhoon from 427% to 22%. We expand on previous results <ref> [44, 45] </ref> by examining the effect of network latency and protocol-processor speed; we find that custom protocols are generally effective at reducing perfor 1. <p> Section 6 discusses related work, and Section 7 summarizes our conclusions and indicates directions for future work. 2 The Tempest interface The Tempest interface provides a set of primitive mechanisms that allow user-level software e.g., compilers and programmersto construct coherent shared address spaces on distributed-memory systems <ref> [44] </ref>. Tempests flexibility derives from its separation of system-provided mechanisms from user-provided coherence policies (protocols). Tempest also provides portability by presenting mechanisms in an implementation-independent fashion. <p> The first subsection covers common features; following subsections describe Typhoon, Typhoon-1, and Typhoon-0 in turn. Additional details can be found in previous publications <ref> [41, 44, 45] </ref>. 3.1 Common features To focus on the impact of the organizational differences in our designs, we assume similar technology for the three common componentsthe protocol processor, network interface, and access control logic. The protocol processor is a general-purpose CPU. <p> The operating system allows a process to manipulate access control tags for its own data pages by mapping the corresponding shadow space pages into the processs virtual space. These shadow mappings prevent unauthorized accesses and implicitly translate authorized accesses to physical addresses. 3.2 Typhoon Typhoon <ref> [44] </ref> combines the network interface, access control logic, and protocol processor on a 14 single device (see Figure 3). Typhoons full integration enables two categories of optimizations: those that enhance communication between the processor and other components, and those that take advantage of network interface/access control synergy.
Reference: [45] <author> S. K. Reinhardt, R. W. Pfile, and D. A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 3443, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Custom protocols also effectively hide the higher overheads of less aggressive hardware; they decrease the performance gap between Typhoon-1 and Typhoon from 222% to 11% and the gap between Typhoon-0 and Typhoon from 427% to 22%. We expand on previous results <ref> [44, 45] </ref> by examining the effect of network latency and protocol-processor speed; we find that custom protocols are generally effective at reducing perfor 1. <p> The first subsection covers common features; following subsections describe Typhoon, Typhoon-1, and Typhoon-0 in turn. Additional details can be found in previous publications <ref> [41, 44, 45] </ref>. 3.1 Common features To focus on the impact of the organizational differences in our designs, we assume similar technology for the three common componentsthe protocol processor, network interface, and access control logic. The protocol processor is a general-purpose CPU.
Reference: [46] <author> Ross Technology, Inc. </author> <title> SPARC RISC Users Guide: </title> <address> hyperSPARC Edition, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The nodes of the simulated systems are based on the technology used for the Typhoon-0 prototype. Nodes are based on the Sun SPARCStation 20. Processors are modeled after the dual-issue Ross HyperSPARC <ref> [46] </ref> clocked at 200 MHz with a 1 Mbyte direct-mapped data cache. The instruction cache is not modeled; all instruction references are treated as hits. A 50 MHz MBus connects the processor (s), memory, access control and network interface devices within each node.
Reference: [47] <author> E. Rosti, E. Smirni, T. Wagner, A. Apon, and L. Dowdy. </author> <title> The KSR1: Experimentation and modeling of poststore. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 7485, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Other approaches to optimizing communication in DSM systems include relaxed memory models [25], prefetching [35], and special writes that deliver data to other nodes <ref> [31, 47] </ref>. The Tempest interface can support all of these optimizations; however, we have focused on exploring the limits of optimization by using fully custom protocols.
Reference: [48] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <pages> pages 174185, </pages> <month> Oct. </month> <year> 1996. </year> <month> 41 </month>
Reference-contexts: Only EM3D gains significantly from this optimization. The Tempest systems also support multiple data-structure-dependent block sizes within a single application, which have been shown to improve speedups further <ref> [48] </ref>. Although the original Simple COMA proposal uses a fixed block size [21], we report the effect of choosing the best per-application block size for that plat form as well. <p> Higher network latencies also reduce the performance difference between the systems, especially for standard shared memory. Increasing the fraction of execution time due to the network itself diminishes the relative impact of other overheads. Sufficiently high network overheads make any hardware DSM support superfluous, leaving software-only fine-grain <ref> [50, 48] </ref> or page-based [26] systems as the most cost-effective approaches to DSM in this domain. (a) Appbt (b) Barnes 0 16 32 Network Latency/System S e d p Custom protocol Standard SM 0.5 s 2.5 s 5 s 25 s 50 s 8 24 Network Latency/System S e d p <p> In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it. Research in workstation-based DSM has focused on all-software methods, using page-based coherence [32] or software-only fine-grain coherence <ref> [50, 48] </ref>.
Reference: [49] <author> I. Schoinas, B. Falsafi, M. D. Hill, J. R. Larus, C. E. Lucas, S. S. Mukherjee, S. K. Reinhardt, E. Schnarr, and D. A. Wood. </author> <title> Implementing fine-grain distributed shared memory on commodity SMP workstations. </title> <type> Technical Report 1307, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: The host processors can access the interface SRAM directly using uncached memory operations; however, the interface processor can access host memory only via the DMA engine. Our prototypes interface processors run custom software derived from Berkeleys Active Messages implementation [14]. Schoinas et al. <ref> [49] </ref> describe our modifications and enhancements for Tempest. As in the Berkeley implementation, our software allocates user-accessible send and receive queues in the shared SRAM. Each entry holds header information, a few words of message data, and an optional pointer to more message data in host memory.
Reference: [50] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Tempests flexibility derives from its separation of system-provided mechanisms from user-provided coherence policies (protocols). Tempest also provides portability by presenting mechanisms in an implementation-independent fashion. This work exploits Tempests portability by using optimized applications originally developed for an all-software implementation on a generic message-passing machine <ref> [50] </ref> to compare the performance of three different hardware-accelerated platforms. Tempest provides four mechanismstwo types of messaging (active messages and virtual channels), virtual memory management, and fine-grain memory access controlto a set of conventional processes, one per processormemory node, executing a common program text. <p> Next we turn to the Tempest-optimized versions described in Section 2.2, whose speedups are indicated by the hatched bars in Figure 6. Although these applicationspecific protocols were written and optimized for a very different software-only Tempest platformBlizzard-E <ref> [50] </ref> on the CM-5they still provide some improvement over standard shared memory on our hardware accelerated systems. (The lone exception is moldyn on Typhoon, where the custom protocol is 1% (a) Standard shared memory (b) Custom protocols 0 1 2 Benchmark/System N r m l i z e E c . <p> Higher network latencies also reduce the performance difference between the systems, especially for standard shared memory. Increasing the fraction of execution time due to the network itself diminishes the relative impact of other overheads. Sufficiently high network overheads make any hardware DSM support superfluous, leaving software-only fine-grain <ref> [50, 48] </ref> or page-based [26] systems as the most cost-effective approaches to DSM in this domain. (a) Appbt (b) Barnes 0 16 32 Network Latency/System S e d p Custom protocol Standard SM 0.5 s 2.5 s 5 s 25 s 50 s 8 24 Network Latency/System S e d p <p> In particular, memory-to memory message-based data transfers are semantically equivalent to memory-to-memory copies, meaning that users cannot explicitly transfer data without renaming it. Research in workstation-based DSM has focused on all-software methods, using page-based coherence [32] or software-only fine-grain coherence <ref> [50, 48] </ref>.
Reference: [51] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Approximately 100 lines of code were added or modified, a small fraction of the roughly 7,000 lines in the original program. Most changes were repetitive replacements of synchronization statements. The custom protocol itself required about 750 lines of C. Barnes, from the SPLASH benchmark suite <ref> [51] </ref>, simulates the evolution of an N-body gravitational system in discrete time steps. The primary data structure is an oct-tree whose interior nodes represent regions of three-dimensional space; the leaves are the bodies located in the region represented by their parent node.
Reference: [52] <author> Sun Microsystems Inc. </author> <title> SPARC MBus Interface Specification, </title> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The instruction cache is not modeled; all instruction references are treated as hits. A 50 MHz MBus connects the processor (s), memory, access control and network interface devices within each node. The MBus is a 64-bit, multiplexed address/data bus that maintains coherence on 32-byte blocks using a MOESI protocol <ref> [52] </ref>. On a cache miss, main memory returns the critical doubleword 140 ns (7 bus cycles or 28 CPU cycles) after the bus request is issued, with the remaining doublewords following in consecutive cycles.
Reference: [53] <author> C. A. Thekkath and H. M. Levy. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 110119, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: To avoid this penalty, we modified the kernels trap vector code to invoke a user handler directly in under 5 ms <ref> [42, 53] </ref>. The suspended thread can be resumed without going through the kernel. The third and most significant change involves the network used to connect the nodes. The prototype employs the Myricom Myrinet, the lowest-latency interconnect commercially available at that time.
Reference: [54] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 technical summary, </title> <year> 1991. </year>
Reference-contexts: When a block is initially Writable, the bus transaction also retrieves an up-to-date copy, because the data could be modified in a hardware cache. A shadow space <ref> [7, 22, 54] </ref> allows direct manipulation of access control tags from Tempests user-level protocol software. A shadow space is a physical address range as large as, and at a fixed offset from, the machines physical memory address range.
Reference: [55] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a mechanism for integrating communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Tempest provides four mechanismstwo types of messaging (active messages and virtual channels), virtual memory management, and fine-grain memory access controlto a set of conventional processes, one per processormemory node, executing a common program text. Tempests active messages, derived from the Berkeley model <ref> [55] </ref>, allow the sender to specify a handler function that executes on arrival at the destination node. This usersupplied handler con 5 sumes the messages datae.g., by depositing it in memoryand (if necessary) signals its arrival. Virtual channels implement memory-to-memory communication, potentially providing higher bandwidth for longer messages.
Reference: [56] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 219229, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Although Tempest custom protocols use message passing to communicate between nodes, they directly support the higher-level shared-memory abstraction. In contrast, other systems that seek to integrate message passing and shared memory treat user-level message passing as a complementary alternative torather than a fundamental building block forshared-memory communication <ref> [22, 27, 56] </ref>. To optimize communication in these systems, critical portions of the program must be rewritten in a message-passing style. Of course, if desired, Tempest programmers can also dispense with shared memory and use messages directlyfor example, to implement synchronization primitives.
Reference: [57] <author> Z. Xu, J. R. Larus, and B. P. Miller. </author> <title> Shared-memory performance profiling. </title> <booktitle> In Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: However, the real power of Tempest lies in the opportunity to optimize performance using customized coherence protocols tailored to specific data structures and specific phases within an application. Tempest also aids the optimization process itself by enabling extended coherence protocols that collect profiling information <ref> [34, 57] </ref>. This information drives high-level tools that help programmers identify and understand performance bottlenecks that may benefit from custom protocols.
Reference: [58] <author> Y. Zhou, L. Iftode, J. P. Singh, K. Li, B. R. Toonen, I. Schoinas, M. D. Hill, and D. A. Wood. </author> <title> Relaxed consistency and coherence granularity in DSM systems: A performance evaluation. </title> <booktitle> In Sixth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Recent work shows that, on the Typhoon-0 prototype, fine-grain sequentially consistent DSM performs comparably to a coarse-grain DSM using lazy release consistency on the same hardware <ref> [58] </ref>. We expect that fine-grain DSM will show a performance advantage on systems with lower overheads and lower network latencies, such as those simulated in Section 4.
References-found: 58

