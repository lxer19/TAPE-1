URL: http://www.cs.kuleuven.ac.be/~lucdr/filp-papers/juffi.ps
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: Dimensionality Reduction in ILP: A Call To Arms  
Author: Johannes F urnkranz 
Address: Schottengasse 3, A-1010 Wien, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: The recent uprise of Knowledge Discovery in Databases (KDD) has underlined the need for machine learning algorithms to be able to tackle large-scale applications that are currently beyond their scope. One way to address this problem is to use techniques for reducing the dimensionality of the learning problem by reducing the hypothesis space and/or reducing the example space. While research in machine learning has devoted considerable attention to such techniques, they have so far been neglected in ILP research. The purpose of this paper is to motivate research in this area and to present some results on windowing techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [ Ade et al., 1995 ] <author> Hilde Ade, Luc De Raedt, and Maurice Bruynooghe. </author> <title> Declarative bias for specific-to-general ILP systems. </title> <booktitle> Machine Learning, </booktitle> <pages> 20(1-2), </pages> <year> 1995. </year> <title> Special Issue on Bias Evaluation and Selection. </title>
Reference: [ Auer et al., 1995 ] <author> Peter Auer, Wolfgang Maass, and Robert C. Holte. </author> <title> Theory and applications of agnostic PAC-learning with small decision trees. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: There are other ways of reducing the size of the hypothesis space in propositional learning algorithms, such as limiting the length of the learned rules or bounding the depth of a decision tree <ref> [ Holte, 1993; Auer et al., 1995 ] </ref> . However, these static approaches have enjoyed less popularity than the dynamic approaches for identifying relevant feature subsets.
Reference: [ Blockeel and De Raedt, 1997 ] <author> Hendrik Blockeel and Luc De Raedt. </author> <title> Top-down induction of logical decision trees. </title> <type> Technical Report CW 247, </type> <institution> Katholieke Universiteit Leu-ven, Department of Computer Science, Leuven, Belgium, </institution> <month> January </month> <year> 1997. </year>
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: Other subsampling approaches include peepholing [ Catlett, 1991 ] , which uses dynamical subsampling at each node in a decision tree, thus extending an earlier proposal described in <ref> [ Breiman et al., 1984 ] </ref> , partitioning [ Domingos, 1996 ] , which partitions the data into segments of equal size and combines the results obtained on each partition (similar to [ Toivonen, 1996 ] ), and uncertainty sampling [ Lewis and Catlett, 1994 ] , which is closely related
Reference: [ Caruana and Freitag, 1994 ] <author> Rich Caruana and Dayne Fre-itag. </author> <title> Greedy attribute selection. In W.W. </title> <editor> Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pages 2836, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Catlett, 1991 ] <author> Jason Catlett. </author> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> PhD thesis, </type> <institution> Basser Department of Computer Science, University of Sydney, </institution> <year> 1991. </year>
Reference-contexts: Other subsampling approaches include peepholing <ref> [ Catlett, 1991 ] </ref> , which uses dynamical subsampling at each node in a decision tree, thus extending an earlier proposal described in [ Breiman et al., 1984 ] , partitioning [ Domingos, 1996 ] , which partitions the data into segments of equal size and combines the results obtained on <p> In particular, thinking in this framework might result in approaches that develop more general approaches to dimensionality reduction that aim at reducing both hypothesis and example space at the same time. As an example consider the peepholing technique introduced in <ref> [ Catlett, 1991 ] </ref> , where subsampling is used to reliably eliminate unpromising candidate conditions from the hypothesis space. 6 Conclusion In this paper, we have tried to argue that techniques for automatically reducing the complexity of a learning problem, as they are quite common in propositional machine learning approaches, also
Reference: [ Cohen, 1994 ] <author> William W. Cohen. </author> <title> Grammatically biased learning: Learning logic programs using an explicit antecedent description language. </title> <journal> Artificial Intelligence, </journal> <volume> 68(2):303366, </volume> <year> 1994. </year>
Reference: [ Cohen, 1995a ] <author> William W. Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> pages 115123, </booktitle> <address> Lake Tahoe, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Figure 1 shows the results of a comparison of I-RIP, a noise-tolerant rule learning algorithm half-way between I-REP [ Furnkranz and Wid-mer, 1994 ] and RIPPER <ref> [ Cohen, 1995a ] </ref> , and a windowed version of the algorithm in a slightly simplified, discretized version of Quinlan's thyroid domain. 3 The windowed version of the algorithm is able to outperform I-RIP in terms of both run-time and accuracy. 4 For more experimental results, including results in the pseudo-relational
Reference: [ Cohen, 1995b ] <author> William W. Cohen. </author> <title> Learning to classify english text with ILP methods. </title> <editor> In Luc De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, volume 32 of Frontiers in Artificial Intelligence and Applications, </booktitle> <pages> pages 124143. </pages> <publisher> IOS Press, </publisher> <year> 1995. </year>
Reference-contexts: Another simple technique for first-order literal selection is used in <ref> [ Cohen, 1995b ] </ref> , where all relations are discarded which refer to objects that occur with a low frequency in the training set. However, both approaches seem to be limited to a subclass of ILP learning problems.
Reference: [ De Raedt and Bruynooghe, 1990 ] <author> Luc De Raedt and Mau-rice Bruynooghe. </author> <title> Indirect relevance and bias in inductive concept learning. Knowledge Acquisition, </title> <address> 2:365390, </address> <year> 1990. </year>
Reference-contexts: As an example think of an algorithm that attempts to learn a theory in a simple hypothesis space first and only switches to more complex hypothesis spaces if the result in the simple space in unsatisfactory. Such an approach has been realized in CLINT <ref> [ De Raedt and Bruynooghe, 1990 ] </ref> , but could also be imagined for other ILP algorithms.
Reference: [ De Raedt and Dzeroski, 1994 ] <author> Luc De Raedt and Saso Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70:375392, </volume> <year> 1994. </year>
Reference-contexts: However, recently the model-based view of the ILP learning problem, which has originally been advocated for what has been called descriptional ILP <ref> [ De Raedt and Dzeroski, 1994; Wrobel and Dzeroski, 1995 ] </ref> , has also been adapted for classification learning [ De Raedt and Van Laer, 1995; Blockeel and De Raedt, 1997 ] .
Reference: [ De Raedt and Van Laer, 1995 ] <author> Luc De Raedt and Wim Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory (ALT-95). </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference: [ De Raedt, 1996 ] <author> Luc De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and J. Wnek, editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning (MSL-96), </booktitle> <pages> pages 2938, </pages> <address> Fairfax,VA, </address> <year> 1996. </year> <title> Machine Learning and Inference Laboratory, </title> <institution> George Mason University. </institution>
Reference-contexts: In this framework, examples are interpretations, for which the learned theory has to be true <ref> [ De Raedt, 1996 ] </ref> . Many ILP learning problems can be formulated in both settings, which would yield different estimates, when the size of the example space is measured by merely counting the number of positive and negative examples.
Reference: [ Dehaspe and De Raedt, 1996 ] <author> Luc Dehaspe and Luc De Raedt. DLAB: </author> <title> A declarative language bias formalism. </title> <booktitle> In Proceedings of the International Symposium on Methodologies for Intelligent Systems (ISMIS-96), </booktitle> <pages> pages 613622, </pages> <year> 1996. </year>
Reference-contexts: For an overview of such approaches see [ Nedellec et al., 1996 ] . Although in some of these approaches a calculation of the size of the defined hypothesis space is possible (as e.g. in the DLAB formalism <ref> [ Dehaspe and De Raedt, 1996 ] </ref> ), its exact size is usually unknown.
Reference: [ Domingos, 1996 ] <author> Pedro Domingos. </author> <title> Efficient specific-to-general rule induction. </title> <editor> In E. Simoudis and J. Han, editors, </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pages 319322. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Other subsampling approaches include peepholing [ Catlett, 1991 ] , which uses dynamical subsampling at each node in a decision tree, thus extending an earlier proposal described in [ Breiman et al., 1984 ] , partitioning <ref> [ Domingos, 1996 ] </ref> , which partitions the data into segments of equal size and combines the results obtained on each partition (similar to [ Toivonen, 1996 ] ), and uncertainty sampling [ Lewis and Catlett, 1994 ] , which is closely related to windowing, but does not extend the current
Reference: [ Furnkranz and Widmer, 1994 ] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental Reduced Error Pruning. </title> <editor> In W. Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pages 7077, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Furnkranz, 1997a ] <author> Johannes Furnkranz. </author> <title> More efficient win-dowing. </title> <booktitle> In Proceedings of the 14th National Conference on Artificial Intelligence (AAAI-97), </booktitle> <address> Providence, RI, 1997. </address> <publisher> AAAI Press. In press. </publisher>
Reference-contexts: Recently, we have demonstrated that rule learning algorithms are better suited for windowing than decision tree learning algorithms and have proposed improved versions of windowing for rule learning that are able to achieve significant gains in noise-free <ref> [ Furnkranz, 1997a ] </ref> and noisy [ Furnkranz, 1997b ] domains. <p> The same learning strategy was used for the experiments described in <ref> [ Furnkranz, 1997a ] </ref> and [ Furnkranz, 1997b ] , albeit only in a propositional setup. <p> the algorithm in a slightly simplified, discretized version of Quinlan's thyroid domain. 3 The windowed version of the algorithm is able to outperform I-RIP in terms of both run-time and accuracy. 4 For more experimental results, including results in the pseudo-relational KRK domain, we have to refer the reader to <ref> [ Furnkranz, 1997a; 1997b ] </ref> . 5 A Generalized Model of Windowing As we have outlined in the last section, we are convinced that windowing may be a powerful technique for reducing the complexity of a learning problem in domains that contain some redundancy.
Reference: [ Furnkranz, 1997b ] <author> Johannes Furnkranz. </author> <title> Noise-tolerant windowing. </title> <type> Technical Report OEFAI-TR-97-07, </type> <institution> Austrian Research Institute for Artificial Intelligence, </institution> <year> 1997. </year> <note> Submitted to IJCAI-97. </note>
Reference-contexts: Recently, we have demonstrated that rule learning algorithms are better suited for windowing than decision tree learning algorithms and have proposed improved versions of windowing for rule learning that are able to achieve significant gains in noise-free [ Furnkranz, 1997a ] and noisy <ref> [ Furnkranz, 1997b ] </ref> domains. <p> The same learning strategy was used for the experiments described in [ Furnkranz, 1997a ] and <ref> [ Furnkranz, 1997b ] </ref> , albeit only in a propositional setup. <p> Note 3 For a more detailed description of how we modified the domain see <ref> [ Furnkranz, 1997b ] </ref> .
Reference: [ Furnkranz, 1997c ] <author> Johannes Furnkranz. </author> <title> Separate-and-conquer rule learning. </title> <journal> Artificial Intelligence Review, </journal> <note> 1997. To appear. </note>
Reference-contexts: as FOIL and its derivates [ Quinlan and Cameron-Jones, 1995 ] and PROGOL [ Mug procedure WINDOWING (Algorithm,LP) RedLP = INITIALIZEREDUCTION (LP) loop Theory = CALL (Algorithm,RedLP) Q = EVALUATE (Theory,LP) if STOPPINGCRITERION (Q,LP,RedLP) return (Theory) else RedLP = EXPANDREDUCTION (Q,LP,RedLP) gleton, 1995 ] , use a separate-and-conquer learning strategy <ref> [ Furnkranz, 1997c ] </ref> for accumulating the final rule set. The same learning strategy was used for the experiments described in [ Furnkranz, 1997a ] and [ Furnkranz, 1997b ] , albeit only in a propositional setup.
Reference: [ Gamberger, 1995 ] <author> Dragan Gamberger. </author> <title> A minimization approach to propositional inductive learning. </title> <editor> In N. Lavrac and S. Wrobel, editors, </editor> <booktitle> Proceedings of the 8th European Conference on Machine Learning (ECML-95), number 912 in Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 151 160, </pages> <address> Heraclion, Greece, 1995. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The special case of identifying relevant values of attributes that could be used as candidate conditions in a rule learning algorithm has also been called literal selection <ref> [ Gamberger, 1995 ] </ref> . FSS algorithms attempt to dynamically identify candidate conditions that are potentially relevant for the learning problem at hand, and attempt to rule out conditions that appear to be irrelevant. <p> A notable exception is [ Lavrac et al., 1995 ] , where an approach for propositional literal selection <ref> [ Gamberger, 1995 ] </ref> is used in a first-order framework by transforming the first-order problem into a propositional representation [ Lavrac et al., 1991 ] .
Reference: [ Holte, 1993 ] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning, </booktitle> <address> 11:6391, </address> <year> 1993. </year>
Reference-contexts: There are other ways of reducing the size of the hypothesis space in propositional learning algorithms, such as limiting the length of the learned rules or bounding the depth of a decision tree <ref> [ Holte, 1993; Auer et al., 1995 ] </ref> . However, these static approaches have enjoyed less popularity than the dynamic approaches for identifying relevant feature subsets.
Reference: [ John et al., 1994 ] <author> George H. John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. In W.W. </title> <editor> Cohen and H. Hirsh, editors, </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pages 121129, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Kivinen and Mannila, 1994 ] <author> Jyrki Kivinen and Heikki Mannila. </author> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the 13th ACM SIGACT-SIGMOD SIGART Symposium on Principles of Database Systems (PODS-94), </booktitle> <pages> pages 7785, </pages> <year> 1994. </year>
Reference-contexts: One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases <ref> [ Kivinen and Mannila, 1994; Toivonen, 1996 ] </ref> and Intelligent Information Retrieval [ Lewis and Catlett, 1994; Yang, 1996 ] has recognized the importance of dimensionality reduction through subsampling for reducing both, learning time and memory requirements.
Reference: [ Kohavi and Sommerfield, 1995 ] <author> Ron Kohavi and Dan Som-merfield. </author> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology. In U.M. </title> <editor> Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings of the 1st International Conference on Knowledge Discovery and Data Mining (KDD-95), pages 192197. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference: [ Kramer, 1994 ] <author> Stefan Kramer. CN2-MCI: </author> <title> A two-step method for constructive induction. </title> <booktitle> In Proceedings of the ML-COLT-94 Workshop on Constructive Induction and Change of Representation, </booktitle> <year> 1994. </year>
Reference-contexts: In particular, so-called wrapper-approaches to constructive induction, where the theory learned in one iteration is analyzed for the construction of new features for subsequent iterations, might easily be cast into this framework <ref> [ Wnek and Michalski, 1994; Pfahringer, 1994; Kramer, 1994 ] </ref> . With some elaboration, a general algorithm akin to the one described above could also incorporate other procedures for dimensionality reduction, like wrapper approaches to feature subset selection or the improved windowing algorithms we have described in the previous section.
Reference: [ Lavrac et al., 1991 ] <author> Nada Lavrac, Saso Dzeroski, and Marko Grobelnik. </author> <title> Learning nonrecursive definitions of relations with LINUS. </title> <booktitle> In Proceedings of the 5th European Working Session on Learning (EWSL-91), </booktitle> <pages> pages 265281, </pages> <address> Porto, Portugal, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A notable exception is [ Lavrac et al., 1995 ] , where an approach for propositional literal selection [ Gamberger, 1995 ] is used in a first-order framework by transforming the first-order problem into a propositional representation <ref> [ Lavrac et al., 1991 ] </ref> . Another simple technique for first-order literal selection is used in [ Cohen, 1995b ] , where all relations are discarded which refer to objects that occur with a low frequency in the training set.
Reference: [ Lavrac et al., 1995 ] <author> Nada Lavrac, Dragan Gamberger, and Saso Dzeroski. </author> <title> An approach to dimensionality reduction in learning from deductive databases. </title> <editor> In Luc De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 337354, </pages> <address> Heverlee, Belgium, </address> <year> 1995. </year> <institution> Katholieke Universiteit Leuven. </institution>
Reference-contexts: A notable exception is <ref> [ Lavrac et al., 1995 ] </ref> , where an approach for propositional literal selection [ Gamberger, 1995 ] is used in a first-order framework by transforming the first-order problem into a propositional representation [ Lavrac et al., 1991 ] .
Reference: [ Lewis and Catlett, 1994 ] <author> David D. Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases [ Kivinen and Mannila, 1994; Toivonen, 1996 ] and Intelligent Information Retrieval <ref> [ Lewis and Catlett, 1994; Yang, 1996 ] </ref> has recognized the importance of dimensionality reduction through subsampling for reducing both, learning time and memory requirements. <p> a decision tree, thus extending an earlier proposal described in [ Breiman et al., 1984 ] , partitioning [ Domingos, 1996 ] , which partitions the data into segments of equal size and combines the results obtained on each partition (similar to [ Toivonen, 1996 ] ), and uncertainty sampling <ref> [ Lewis and Catlett, 1994 ] </ref> , which is closely related to windowing, but does not extend the current window based on misclassifications, but on the confidence the learner has into its learned theory.
Reference: [ Muggleton, 1995 ] <author> Stephen H. Muggleton. </author> <title> Inverse entailment and Progol. New Generation Computing, </title> <address> 13(3,4):245286, </address> <year> 1995. </year> <note> Special Issue on Inductive Logic Programming. </note>
Reference: [ Nedellec et al., 1996 ] <author> Claire Nedellec, Celine Rouveirol, Hilde Ade, Francesco Bergadano, and Birgit Tausend. </author> <title> Declarative bias in ILP. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, volume 32 of Frontiers in Artificial Intelligence and Applications, </booktitle> <pages> pages 82103. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: For an overview of such approaches see <ref> [ Nedellec et al., 1996 ] </ref> . Although in some of these approaches a calculation of the size of the defined hypothesis space is possible (as e.g. in the DLAB formalism [ Dehaspe and De Raedt, 1996 ] ), its exact size is usually unknown.
Reference: [ Pfahringer, 1994 ] <author> Bernhard Pfahringer. </author> <title> Controlling constructive induction in CiPF: an MDL approach. </title> <editor> In Pavel B. Brazdil, editor, </editor> <booktitle> Proceedings of the 7th European Conference on Machine Learning (ECML-94), Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 242256, </pages> <address> Catania, Sicily, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In particular, so-called wrapper-approaches to constructive induction, where the theory learned in one iteration is analyzed for the construction of new features for subsequent iterations, might easily be cast into this framework <ref> [ Wnek and Michalski, 1994; Pfahringer, 1994; Kramer, 1994 ] </ref> . With some elaboration, a general algorithm akin to the one described above could also incorporate other procedures for dimensionality reduction, like wrapper approaches to feature subset selection or the improved windowing algorithms we have described in the previous section.
Reference: [ Pfahringer, 1995 ] <author> Bernhard Pfahringer. </author> <title> Compression-based feature subset selection. </title> <booktitle> In Proceedings of the IJCAI-95 Workshop on Data Engineering for Inductive Learning, </booktitle> <year> 1995. </year>
Reference: [ Quinlan and Cameron-Jones, 1995 ] <author> John Ross Quinlan and R. M. Cameron-Jones. </author> <title> Induction of logic programs: FOIL and related systems. New Generation Computing, </title> <address> 13(3,4):287312, </address> <year> 1995. </year> <note> Special Issue on Inductive Logic Programming. </note>
Reference-contexts: However, there are different views in ILP on what constitutes an example. Classical approaches, like FOIL <ref> [ Quinlan and Cameron-Jones, 1995 ] </ref> , learn a target concept from positive and negative examples, which should be entailed or not entailed by the theory for the target concept. <p> This enables the windowing procedure to gain efficiency even in domains where only parts of the example space have some redundancy. Many ILP algorithms, such as FOIL and its derivates <ref> [ Quinlan and Cameron-Jones, 1995 ] </ref> and PROGOL [ Mug procedure WINDOWING (Algorithm,LP) RedLP = INITIALIZEREDUCTION (LP) loop Theory = CALL (Algorithm,RedLP) Q = EVALUATE (Theory,LP) if STOPPINGCRITERION (Q,LP,RedLP) return (Theory) else RedLP = EXPANDREDUCTION (Q,LP,RedLP) gleton, 1995 ] , use a separate-and-conquer learning strategy [ Furnkranz, 1997c ] for accumulating
Reference: [ Quinlan, 1983 ] <author> John Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pages 463482. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Another way of improving the efficiency of rule learning algorithms is therefore to use only a subsample of the available examples for learning. Windowing is one technique for identifying an appropriate subsample to learn from. It has been proposed in <ref> [ Quinlan, 1983 ] </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain [ Quinlan, 1983 ] windowing has not played a <p> It has been proposed in <ref> [ Quinlan, 1983 ] </ref> as a supplement to the inductive decision tree learner ID3 to enable it to tackle tasks which would otherwise have exceeded the memory capacity of the computers of those days. Despite first successful experiments in the KRKN domain [ Quinlan, 1983 ] windowing has not played a major role in machine learning research. One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling.
Reference: [ Stahl, 1996 ] <author> Irene Stahl. </author> <title> Predicate invention in Inductive Logic Programming. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, volume 32 of Frontiers in Artificial Intelligence and Applications, </booktitle> <pages> pages 3447. </pages> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: Similarly, many approaches to constructive induction or predicate invention may be viewed in this framework, if the motivation for inventing a new predicate (i.e. shifting the language bias to a more expressive hypothesis language) is the insufficiency of the current hypothesis language <ref> [ Stahl, 1996 ] </ref> . In particular, so-called wrapper-approaches to constructive induction, where the theory learned in one iteration is analyzed for the construction of new features for subsequent iterations, might easily be cast into this framework [ Wnek and Michalski, 1994; Pfahringer, 1994; Kramer, 1994 ] .
Reference: [ Toivonen, 1996 ] <author> Hannu Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proceedings of the 22nd Conference on Very Large Data Bases (VLDB-96), </booktitle> <pages> pages 134145, </pages> <address> Mumbai, India, </address> <year> 1996. </year>
Reference-contexts: One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases <ref> [ Kivinen and Mannila, 1994; Toivonen, 1996 ] </ref> and Intelligent Information Retrieval [ Lewis and Catlett, 1994; Yang, 1996 ] has recognized the importance of dimensionality reduction through subsampling for reducing both, learning time and memory requirements. <p> which uses dynamical subsampling at each node in a decision tree, thus extending an earlier proposal described in [ Breiman et al., 1984 ] , partitioning [ Domingos, 1996 ] , which partitions the data into segments of equal size and combines the results obtained on each partition (similar to <ref> [ Toivonen, 1996 ] </ref> ), and uncertainty sampling [ Lewis and Catlett, 1994 ] , which is closely related to windowing, but does not extend the current window based on misclassifications, but on the confidence the learner has into its learned theory.
Reference: [ Wirth and Catlett, 1988 ] <author> Jarryl Wirth and Jason Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <editor> In J. Laird, editor, </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), </booktitle> <pages> pages 8799, </pages> <address> Ann Arbor, MI, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A good deal of the lack of interest in windowing can be attributed to an empirical study <ref> [ Wirth and Catlett, 1988 ] </ref> . The authors studied windowing with ID3 in various domains and concluded that windowing cannot be recommended as a procedure for improving efficiency.
Reference: [ Wnek and Michalski, 1994 ] <author> Janusz Wnek and Ryszard S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI: A method and experiments. </title> <booktitle> Machine Learning, </booktitle> <address> 14(2):139168, </address> <year> 1994. </year> <title> Special Issue on Evaluating and Changing Representation. </title>
Reference-contexts: In particular, so-called wrapper-approaches to constructive induction, where the theory learned in one iteration is analyzed for the construction of new features for subsequent iterations, might easily be cast into this framework <ref> [ Wnek and Michalski, 1994; Pfahringer, 1994; Kramer, 1994 ] </ref> . With some elaboration, a general algorithm akin to the one described above could also incorporate other procedures for dimensionality reduction, like wrapper approaches to feature subset selection or the improved windowing algorithms we have described in the previous section.
Reference: [ Wrobel and Dzeroski, 1995 ] <author> Stefan Wrobel and Saso Dzeroski. </author> <title> The ILP description learning problem: Towards a general model-level definition of data mining in ILP. </title> <editor> In K. Morik and J. Herrmann, editors, </editor> <booktitle> Annual Workshop of the GI Special Interest Group Machine Learning (GI FG 1.1.3), </booktitle> <address> Dortmund, Germany, </address> <year> 1995. </year>
Reference-contexts: However, recently the model-based view of the ILP learning problem, which has originally been advocated for what has been called descriptional ILP <ref> [ De Raedt and Dzeroski, 1994; Wrobel and Dzeroski, 1995 ] </ref> , has also been adapted for classification learning [ De Raedt and Van Laer, 1995; Blockeel and De Raedt, 1997 ] .
Reference: [ Yang, 1996 ] <author> Yiming Yang. </author> <title> Sampling strategies and learning efficiency in text categorization. </title> <editor> In M. Hearst and H. Hirsh, editors, </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pages 8895. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year> <note> Technical Report SS-96-05. </note>
Reference-contexts: One reason for this is certainly the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, recent work in the areas of Knowledge Discovery in Databases [ Kivinen and Mannila, 1994; Toivonen, 1996 ] and Intelligent Information Retrieval <ref> [ Lewis and Catlett, 1994; Yang, 1996 ] </ref> has recognized the importance of dimensionality reduction through subsampling for reducing both, learning time and memory requirements.
References-found: 40

