URL: http://www.cs.toronto.edu/~hinton/ftp/RGBN.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00271.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: hinton@cs.toronto.edu, zoubin@cs.toronto.edu  
Title: Generative Models for Discovering Sparse Distributed Representations  
Author: Geoffrey E. Hinton and Zoubin Ghahramani 
Note: A modified version to appear in Philosophical Transactions of the Royal Society B,  
Date: May 9, 1997  1997.  
Address: Toronto, Ontario, M5S 1A4, Canada  
Affiliation: Department of Computer Science University of Toronto  
Abstract: We describe a hierarchical, generative model that can be viewed as a non-linear generalization of factor analysis and can be implemented in a neural network. The model uses bottom-up, top-down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demon strate that the network learns to extract sparse, distributed, hierarchical representations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barlow, H. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311. </pages>
Reference: <author> Becker, S. and Hinton, G. </author> <year> (1992). </year> <title> A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355 </volume> <pages> 161-163. </pages>
Reference-contexts: The representation is sparse but not distributed. 9 Discovering depth in simplified stereo pairs Another problem in which discovering the higher order structure of a dataset has presented difficulties for some previous unsupervised learning algorithms is the one-dimensional stereo disparity problem <ref> (Becker and Hinton, 1992) </ref>. We tested the RGBN on a version of this problem with the following generative process. Random dots of uniformly distributed intensities are scattered sparsely on a one-dimensional surface, and the image is blurred with a Gaussian filter.
Reference: <author> Bishop, C. M., Svensen, M., and Williams, C. K. I. </author> <title> (In Press). GTM: A principled alternative to the self-organizing map. </title> <booktitle> Neural Computation. </booktitle>
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: denote averages over the training data. g j (new) = hp (s j = 1jd) di = hp (s j = 1jd)i (4) i (new) = X p (s j = 1jd) (d i g ji ) 2 (5) 3 This is a version of the "Expectation and Maximization" algorithm <ref> (Dempster et al., 1977) </ref> and is guaranteed to raise the likelihood of the observed data unless it is already at a local optimum.
Reference: <author> Devroye, L. </author> <year> (1986). </year> <title> Non-uniform Random Variate Generation. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Durbin, R. and Willshaw, D. </author> <year> (1987). </year> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326(16) </volume> <pages> 689-691. </pages>
Reference: <author> Everitt, B. S. </author> <year> (1984). </year> <title> An Introduction to Latent Variable Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: Maximum likelihood factor analysis <ref> (Everitt, 1984) </ref> consists of finding generative weights and local noise levels for the visible units so as to maximize the likelihood of generating the observed data.
Reference: <author> Frey, B. J. </author> <year> (1997). </year> <title> Continuous sigmoidal belief networks trained using slice sampling. </title> <editor> In Mozer, M., Jordan, M., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Because the network can select which basis functions are appropriate for the data, it can 5 By using a few linear regimes we can crudely approximate units whose output is a smooth non-linear function of y <ref> (Frey, 1997) </ref> and still perform exact Gibbs sampling. 18 of each 2 fi 18 image are the inputs to the left and right eye, respectively.
Reference: <author> Geman, S. and Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741. </pages>
Reference: <author> Gilks, W. R. and Wild, P. </author> <year> (1992). </year> <title> Adaptive rejection sampling for gibbs sampling. </title> <journal> Applied Statistics, </journal> <volume> 41 </volume> <pages> 337-348. </pages>
Reference-contexts: Finally, we should point out that these are just some of the methods that can be used to sample from p (y j ). Implementations using other sampling methods, such as adaptive rejection sampling <ref> (Gilks and Wild, 1992) </ref>, are also possible. 23
Reference: <author> Gregory, R. L. </author> <year> (1970). </year> <title> The Intelligent Eye. Wiedenfeld and Nicolson, </title> <publisher> London. </publisher>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268 </volume> <pages> 1158-1161. </pages>
Reference-contexts: describe an alternative way of making LBN's biologically plausible. 6 The wake-sleep algorithm There is an approximate method of performing perceptual inference in a LBN that leads to a very simple implementation which would be biologically quite plausible if only it were better at extracting the hidden causes of data <ref> (Hinton et al., 1995) </ref>. Instead of using Gibbs sampling, we use a separate set of bottom-up recognition connections to pick binary states for units in one layer given the already selected binary states of units in the layer below. <p> We have not described a mechanism for the formation of topographic maps, but we have given a good computational reason for their existence. 8 Results on a toy task A simple problem that illustrates the need for sparse distributed representations is the noisy bars problem <ref> (Hinton et al., 1995) </ref>. Consider the following multi-stage generative model for K fi K images. The top level decides with equal probabilities whether the image will consist solely of vertical or horizontal bars.
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1983). </year> <title> Optimal perceptual Inference. </title> <booktitle> In Proc. of the IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 448-453. </pages> <address> Wash-ington, DC. </address>
Reference-contexts: For more complex models, we shall have to be content with a perceptual inference process that picks one or a few configurations roughly according to their posterior probabilities <ref> (Hinton and Sejnowski, 1983) </ref>. One advantage of starting with a generative model is that it provides a natural specification of what visual perception ought to do. For example, it specifies exactly how top-down expectations should be used to disambiguate noisy data without unduly distorting reality.
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Mixtures of Gaussians and factor analysis are standard statistical models precisely because the exact computation of the posterior distribution is tractable. 5 From Boltzmann machines to Logistic Belief Nets The Boltzmann machine <ref> (Hinton and Sejnowski, 1986) </ref> was, perhaps, the first neural network learning algorithm to be based on an explicit generative model that used distributed, non-linear representations. Boltzmann machines use stochastic binary units and, with h hidden units, the number of possible representations of each data point is 2 h .
Reference: <author> Horn, B. K. P. </author> <year> (1977). </year> <title> Understanding image intensities. </title> <journal> Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 201-231. </pages>
Reference-contexts: Such models are biologically unrealistic because they do not allow for top-down effects when perceiving noisy or ambiguous data (Mumford, 1994; Gregory, 1970) and they do not explain the prevalence of top-down connections in cortex. In this paper, we take seriously the idea that vision is inverse graphics <ref> (Horn, 1977) </ref> and so we start with a stochastic, generative neural network that uses top-down connections to convert an abstract representation of a scene into an intensity image. This neurally instantiated graphics model is learned and the top-down connection strengths contain the network's visual knowledge of the world.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference-contexts: It is also feasible to combine a generalization of the probit unit that uses its y value to deterministically pick one of m possibilities with a generalization of the linear unit that has m different linear regimes. This is a generative version of the mixture of experts model <ref> (Jacobs et al., 1991) </ref>. The RGBN is a particularly interesting case because the infinite density of [y] + at 0 means that it is very cheap, in coding terms, for units to have outputs of 0, so the network develops sparse representations.
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference-contexts: They are usually inefficient because they do not use a full M-step and slightly wrong because they pick a single winner among the hidden units instead of making the states proportional to the posterior probabilities. Kohonen's self-organizing maps <ref> (Kohonen, 1982) </ref>, Durbin and Willshaw's elastic net (1987), and the generative topographic map (Bishop et al., In Press) are variations of vector quantization or mixture of Gaussian models in which additional constraints are imposed that force neighboring hidden units to have similar generative weight vectors.
Reference: <author> Lee, D. D. and Seung, H. S. </author> <year> (1997). </year> <title> Unsupervised learning by convex and conic coding. </title> <editor> In Mozer, M., Jordan, M., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 24 Lewicki, </note> <author> M. S. and Sejnowski, T. J. </author> <year> (1997). </year> <title> Bayesian unsupervised learning of higher order structure. </title> <editor> In Mozer, M., Jordan, M., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Mumford, D. </author> <year> (1994). </year> <title> Neuronal architectures for pattern-theretic problems. </title> <editor> In Koch, C. and Davis, J. L., editors, </editor> <booktitle> Large-Scale Neuronal Theories of the Brain, </booktitle> <pages> pages 125-152. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference: <author> Neal, R. M. and Dayan, P. </author> <year> (1996). </year> <title> Factor analysis using delta-rule wake-sleep learning. </title> <type> Technical Report No. </type> <institution> 9607 Dept. of Statistics, University of Toronto. </institution>
Reference-contexts: By using appropriate recognition weights it is possible to correctly handle the data-dependent effects of explaining away on the mean of the posterior distribution. But learning these recognition weights in a neural net is tricky <ref> (Neal and Dayan, 1996) </ref>. When the generative weight vectors of the hidden units are not orthogonal, the posterior probability distribution in hidden space has a full covariance matrix. This matrix does not depend on the data, but it does depend on the generative weights (figure 2).
Reference: <author> Olshausen, B. A. and Field, D. J. </author> <year> (1996). </year> <title> Emergence of simple-cell receptive field properties by learning a sparse code for natural images. </title> <journal> Nature, </journal> <volume> 381 </volume> <pages> 607-609. </pages>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: be proportional to the generative weights because the generative weight vectors of the hidden units (known as the factor loadings) do not need to be orthogonal. 2 Generative weight vectors that are not orthogonal give rise to a very important phenomenon known as "explaining away" that occurs during perceptual inference <ref> (Pearl, 1988) </ref>. Suppose that the visible units all have equal noise variances and that two hidden units have generative weight vectors that have a positive scalar product.
Reference: <author> Rumelhart, D. and Zipser, D. </author> <year> (1985). </year> <title> Feature discovery by competitive learning. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 75-112. </pages>
Reference-contexts: Under these assumptions the posterior probabilities in Eq. 3 go to binary values with p (s j = 1jd) = 1 for the Gaussian whose mean is closest to d and 0 otherwise. Competitive learning algorithms <ref> (e.g. Rumelhart and Zipser, 1985) </ref> can generally be viewed as ways of fitting mixture of Gaussians generative models.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536. 25 </pages>
Reference-contexts: If an efficient algorithm can be found for fitting models of this type it is likely to prove even more fruitful than the efficient backpropagation algorithm for multi-layer non-linear regression <ref> (Rumelhart et al., 1986) </ref>. The difficulty lies in the computation of the posterior distribution over hidden states when given a data point. This distribution, or an approximation to it, is required both for learning the generative model and for perceptual inference once the model has been learned.
References-found: 25

