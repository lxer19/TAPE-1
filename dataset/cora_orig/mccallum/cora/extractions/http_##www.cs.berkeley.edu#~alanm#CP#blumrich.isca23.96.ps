URL: http://www.cs.berkeley.edu/~alanm/CP/blumrich.isca23.96.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer  
Author: Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, and Edward W. Felten Jonathan Sandberg 
Address: Princeton NJ 08544  Way, Princeton, NJ 08540  
Affiliation: Department of Computer Science, Princeton University,  Panasonic Technologies, Incorporated, 2 Research  
Note: In Proceedings of the 21st Annual International Symposium on Computer Architecture, April, 1994, pp. 142-153.  
Abstract: The network interfaces of existing multicomputers require a significant amount of software overhead to provide protection and to implement message passing protocols. This paper describes the design of a low-latency, high-bandwidth, virtual memory-mapped network interface for the SHRIMP multicomputer project at Princeton University. Without sacrificing protection, the network interface achieves low latency by using virtual memory mapping and write-latency hiding techniques, and obtains high bandwidth by providing a user-level block data transfer mechanism. We have implemented several message passing primitives in an experimental environment, demonstrating that our approach can reduce the message passing overhead to a few user-level instructions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: By 10 contrast, our automatic-update scheme lets a producer and consumer share data without requiring a time-consuming remote access. Several parallel architectures use multiple threads <ref> [22, 29, 2, 1] </ref> to overlap communication with computation. These approaches require applications or compilers to create multiple threads on each node, and require the node CPU to switch thread contexts very fast.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <year> 1990. </year>
Reference-contexts: By 10 contrast, our automatic-update scheme lets a producer and consumer share data without requiring a time-consuming remote access. Several parallel architectures use multiple threads <ref> [22, 29, 2, 1] </ref> to overlap communication with computation. These approaches require applications or compilers to create multiple threads on each node, and require the node CPU to switch thread contexts very fast.
Reference: [3] <institution> BCPR Services Inc. </institution> <note> EISA Specification, Version 3.12, </note> <year> 1992. </year>
Reference-contexts: These state-of-the-art components allow SHRIMP to take advantage of the latest available technology at a fraction of the cost of current multicomputer systems. The Xpress PC consists of a Pentium CPU [28, 16] with a second-level cache connected to DRAM memory modules and I/O bus adapters (EISA <ref> [3] </ref> or PCI [25]) via the Xpress memory bus. Memory can be cached as write-through or write-back on a per-virtual-page basis, as specified in process page tables. The caches snoop DMA transactions and automatically invalidate corresponding cache lines, keeping consistent with all main memory updates. <p> Peak Bandwidth Applications requiring maximum bandwidth will use the deliberate-update mechanism. Deliberate-update communication takes place as a single burst, driven by DMA engines on both the sending and receiving nodes. The EISA bus on the receiver's side, with a peak burst-mode bandwidth of 33 Mbytes/second <ref> [3] </ref>, is the bottleneck that limits communication bandwidth. All other parts of the datapath have at least twice this bandwidth. 5.2 Software Overhead Because SHRIMP offers user-level communication, applications are free to use customized message passing operations rather than a single, generic mechanism.
Reference: [4] <author> Roberto Bisiani and Mosur Ravishankar. </author> <title> PLUS: A distributed shared-memory system. </title> <booktitle> In Proceedings of 17th ISCA, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Protection is provided between separate partitions, but not between processes within a partition. Since packet headers must be constructed by applications, the message passing overhead is still hundreds of CPU instructions. Memnet [10], Merlin [21] and its successor SESAME [33], the Plus system <ref> [4] </ref>, and Galactica Net [17] use the page-based, automatic-update approach to support shared memory. These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update. Some systems provide communication via direct access to remote memory locations.
Reference: [5] <author> Shekhar Borkar, Robert Cohn, George Cox, Thomas Gross, H.T.Kung, Monica Lam, Margie Levine, Brian Moore, Wire Moore, Craig Peterson, Jim Susman, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of 17th ISCA, </booktitle> <pages> pages 70-81, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [5, 11, 7] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [6] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <note> 1993. Manual HR-04033. 11 </note>
Reference-contexts: Some systems provide communication via direct access to remote memory locations. An early example is Spector's work on a network of Xerox Altos [30]. The most recent example of this approach is the CRAY T3D <ref> [6] </ref>. The disadvantage of this approach is that shared data is stored in only one place, so all but one of the sharers must access it remotely. By 10 contrast, our automatic-update scheme lets a producer and consumer share data without requiring a time-consuming remote access.
Reference: [7] <author> William J. Dally, Roy Davison, J. A. Stuart Fiske, Greg Fyler, John S. Keen, Richard A. Lethin, Michael Noakes, and Peter R. Nuth. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [5, 11, 7] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [8] <author> William J. Dally and Charles L. Seitz. </author> <title> The torus routing chip. </title> <journal> Distributed Computing, </journal> <volume> 1 </volume> <pages> 187-196, </pages> <year> 1986. </year>
Reference-contexts: The Intel Paragon routing backplane is a two-dimensional mesh of Intel iMRC routers [31], which are essentially faster and wider versions of the Caltech Mesh Routing Chip <ref> [8] </ref>. The backplane supports deadlock-free, oblivious wormhole routing [9] and preserves the order of messages from each sender to each receiver. The custom designed SHRIMP network interface is the key system component which connects each Xpress PC system to the processor port of an iMRC router on the routing backplane.
Reference: [9] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: The Intel Paragon routing backplane is a two-dimensional mesh of Intel iMRC routers [31], which are essentially faster and wider versions of the Caltech Mesh Routing Chip [8]. The backplane supports deadlock-free, oblivious wormhole routing <ref> [9] </ref> and preserves the order of messages from each sender to each receiver. The custom designed SHRIMP network interface is the key system component which connects each Xpress PC system to the processor port of an iMRC router on the routing backplane.
Reference: [10] <author> G. S. Delp, D. J. Farber, R. G. Minnich, J. M. Smith, and M. C. Tam. </author> <title> Memory as a network abstraction. </title> <journal> IEEE Network, </journal> <volume> 5(4) </volume> <pages> 34-41, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Protection is provided between separate partitions, but not between processes within a partition. Since packet headers must be constructed by applications, the message passing overhead is still hundreds of CPU instructions. Memnet <ref> [10] </ref>, Merlin [21] and its successor SESAME [33], the Plus system [4], and Galactica Net [17] use the page-based, automatic-update approach to support shared memory. These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update.
Reference: [11] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition, the node is complex and expensive to build. Several projects have taken the approach of lowering communication latency by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [5, 11, 7] </ref>. Writing and reading these registers queues and dequeues data from the FIFOs respectively. While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment.
Reference: [12] <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 interconnect elan elite design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: One solution to the problem of software overhead is to add a separate processor on every node just for message passing [22, 13]. Recent examples of this approach are the Intel Paragon [14] and Meiko CS-2 <ref> [12] </ref>. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [13] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> A message passing coprocessor for distributed memory multicom-puters. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: The main disadvantage is that message passing costs are usually thousands of CPU cycles, with the best implementation [32] still requiring over 100 CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [22, 13] </ref>. Recent examples of this approach are the Intel Paragon [14] and Meiko CS-2 [12]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: [14] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: One solution to the problem of software overhead is to add a separate processor on every node just for message passing [22, 13]. Recent examples of this approach are the Intel Paragon <ref> [14] </ref> and Meiko CS-2 [12]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [15] <author> Intel Corporation. </author> <title> Express Platforms Technical Product Summary: System Overview, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Our network interface uses deliberate updates for user-level block data transfers to achieve high-bandwidth communication with no operating system overhead. 3 SHRIMP System The SHRIMP system is a new multicomputer being designed at Princeton University. Each node in the SHRIMP multicomputer is an Intel Pentium Xpress PC system <ref> [15] </ref> and the interconnect is an Intel Paragon routing backplane. These state-of-the-art components allow SHRIMP to take advantage of the latest available technology at a fraction of the cost of current multicomputer systems.
Reference: [16] <author> Intel Corporation. </author> <title> Pentium Processor Data Book, </title> <year> 1993. </year>
Reference-contexts: These state-of-the-art components allow SHRIMP to take advantage of the latest available technology at a fraction of the cost of current multicomputer systems. The Xpress PC consists of a Pentium CPU <ref> [28, 16] </ref> with a second-level cache connected to DRAM memory modules and I/O bus adapters (EISA [3] or PCI [25]) via the Xpress memory bus. Memory can be cached as write-through or write-back on a per-virtual-page basis, as specified in process page tables. <p> This requires processes to use a single, atomic instruction to request a transfer and determine whether it was started. The current network interface supports deliberate-update transfer initiation using the Pentium compare-and-exchange (CMPXCHG) instruction <ref> [16] </ref>, which generates a read cycle followed by a write cycle if the value returned by the read matches the accumulator.
Reference: [17] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware assist for distributed shared memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Protection is provided between separate partitions, but not between processes within a partition. Since packet headers must be constructed by applications, the message passing overhead is still hundreds of CPU instructions. Memnet [10], Merlin [21] and its successor SESAME [33], the Plus system [4], and Galactica Net <ref> [17] </ref> use the page-based, automatic-update approach to support shared memory. These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update. Some systems provide communication via direct access to remote memory locations.
Reference: [18] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a non-standard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs <ref> [18] </ref>. Protection is provided through the virtual memory system, which controls access to these FIFOs. However, there are a limited number of FIFOs so they must be shared within a partition (subset of nodes), restricting the degree of multiprogramming.
Reference: [19] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Automatic-update pages can be used to experiment with shared virtual memory, using an update-based coherence policy. Updates to shared pages are propagated automatically by the SHRIMP hardware. Because the network interface and the network both maintain FIFO ordering between updates, this shared memory model is PRAM-consistent <ref> [19] </ref>. <p> Hence, our instruction counts are accurate. The implementation environment consists of two i486-based Xpress PCs, connected via a pair of Pipelined RAM (PRAM) network interfaces <ref> [19] </ref>. Each network interface contains 32 Kbytes of dual-ported SRAM which is mapped to the SRAM of the other in a manner similar to a complementary SHRIMP single-write, automatic-update mapping. The PCs run a modified OSF-1/MK AD operating system. <p> These approaches require applications or compilers to create multiple threads on each node, and require the node CPU to switch thread contexts very fast. The idea of automatic-update data delivery in our network interface is derived from the Pipelined RAM network interface <ref> [19] </ref>. The PRAM network interface allows physical memory mapping only for a small amount of memory on the network interface board. 7 Conclusions The SHRIMP project is using commodity components, and a custom-designed network interface to build a new multicomputer with communication based on remote virtual memory mapping.
Reference: [20] <author> Richard J. Littlefield. </author> <title> Characterizing and tuning communications performance for real applications. </title> <booktitle> In Proceedings of the First Intel DELTA Applications Workshop, </booktitle> <pages> pages 179-190, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Compared to these software overheads, hardware communication latencies are almost negligible. For example, on the Intel DELTA multicomputer, sending and receiving a message requires 67 sec, of which less than 1 sec is due to hardware latency <ref> [20] </ref>. The main reason for such high overheads is that communication is usually provided as a service of the operating system. This is expensive because it requires several crossings between user-level and kernel-level for each message, and also because it prevents applications from using the communication hardware in customized ways.
Reference: [21] <author> Creve Maples. </author> <title> A high-performance, memory-based interconnection system for multicomputer environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Protection is provided between separate partitions, but not between processes within a partition. Since packet headers must be constructed by applications, the message passing overhead is still hundreds of CPU instructions. Memnet [10], Merlin <ref> [21] </ref> and its successor SESAME [33], the Plus system [4], and Galactica Net [17] use the page-based, automatic-update approach to support shared memory. These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update.
Reference: [22] <author> R.S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of 19th ISCA, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The main disadvantage is that message passing costs are usually thousands of CPU cycles, with the best implementation [32] still requiring over 100 CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing <ref> [22, 13] </ref>. Recent examples of this approach are the Intel Paragon [14] and Meiko CS-2 [12]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. <p> By 10 contrast, our automatic-update scheme lets a producer and consumer share data without requiring a time-consuming remote access. Several parallel architectures use multiple threads <ref> [22, 29, 2, 1] </ref> to overlap communication with computation. These approaches require applications or compilers to create multiple threads on each node, and require the node CPU to switch thread contexts very fast.
Reference: [23] <author> Steven Nugent. </author> <title> The iPSC/2 direct-connect communication technology. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 51-60, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Recent examples include the NCUBE [24], iPSC/2 <ref> [23] </ref> and iPSC/860. The network interface designs of these machines are very similar. An application sends messages by making operating system calls to initiate DMA data transfers.
Reference: [24] <author> John Palmer. </author> <title> The NCUBE family of high-performance parallel computer systems. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 845-851, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Recent examples include the NCUBE <ref> [24] </ref>, iPSC/2 [23] and iPSC/860. The network interface designs of these machines are very similar. An application sends messages by making operating system calls to initiate DMA data transfers.
Reference: [25] <author> PCI Special Interest Group. </author> <title> PCI Local Bus Specification, Revision 2.0, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: The Xpress PC consists of a Pentium CPU [28, 16] with a second-level cache connected to DRAM memory modules and I/O bus adapters (EISA [3] or PCI <ref> [25] </ref>) via the Xpress memory bus. Memory can be cached as write-through or write-back on a per-virtual-page basis, as specified in process page tables. The caches snoop DMA transactions and automatically invalidate corresponding cache lines, keeping consistent with all main memory updates.
Reference: [26] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: We used the Intel NX/2 csend and crecv primitives, which buffer incoming messages in system-managed memory, and use message types to dispatch messages in a FIFO fashion <ref> [26] </ref>. In our implementation, we restrict the message type to a 16-bit integer, and assume that each message type represents only point-to-point communication (i.e. there is only one sender associated with each message type). We expect SHRIMP to have two main advantages over existing systems when implementing these primitives.
Reference: [27] <author> R.F. Rashid, A. Tevanian, M. Young, D. Golub, R. Baron, D. Black, W. Bolosky, and J. Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architecture. </title> <booktitle> In Proceedings of 2nd International Conference on Architectural Support for Programming Lanugages and Operating Systems, </booktitle> <pages> pages 31-41, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: This solution is satisfactory if there are not too many communication mappings. We can design a more sophisticated solution by exploiting the similarity to the TLB consistency problem, and borrowing standard solutions <ref> [27] </ref>. While space does not permit a detailed discussion of these policies, we can provide a simple example. Before replacing a communication-mapped page, a node's kernel must cause all remote NIPT entries referring to that physical page to be invalidated.
Reference: [28] <author> Avtar Saini. </author> <title> An overview of the Intel Pentium processor. </title> <booktitle> In Compcon Spring '93, </booktitle> <pages> pages 60-62, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: These state-of-the-art components allow SHRIMP to take advantage of the latest available technology at a fraction of the cost of current multicomputer systems. The Xpress PC consists of a Pentium CPU <ref> [28, 16] </ref> with a second-level cache connected to DRAM memory modules and I/O bus adapters (EISA [3] or PCI [25]) via the Xpress memory bus. Memory can be cached as write-through or write-back on a per-virtual-page basis, as specified in process page tables.
Reference: [29] <author> Burton J. Smith. </author> <title> A pipelined, shared resource MIMD computer. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year>
Reference-contexts: By 10 contrast, our automatic-update scheme lets a producer and consumer share data without requiring a time-consuming remote access. Several parallel architectures use multiple threads <ref> [22, 29, 2, 1] </ref> to overlap communication with computation. These approaches require applications or compilers to create multiple threads on each node, and require the node CPU to switch thread contexts very fast.
Reference: [30] <author> Alfred Z. Spector. </author> <title> Performing remote operations efficiently on a local computer network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 246-260, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update. Some systems provide communication via direct access to remote memory locations. An early example is Spector's work on a network of Xerox Altos <ref> [30] </ref>. The most recent example of this approach is the CRAY T3D [6]. The disadvantage of this approach is that shared data is stored in only one place, so all but one of the sharers must access it remotely.
Reference: [31] <author> Roger Traylor and Dave Dunning. </author> <title> Routing chip set for Intel Paragon parallel supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The PC baseboard has a memory extension connector which carries the majority of the Xpress bus signals, as well as a number of system expansion connectors (EISA or PCI). The Intel Paragon routing backplane is a two-dimensional mesh of Intel iMRC routers <ref> [31] </ref>, which are essentially faster and wider versions of the Caltech Mesh Routing Chip [8]. The backplane supports deadlock-free, oblivious wormhole routing [9] and preserves the order of messages from each sender to each receiver.
Reference: [32] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of 19th ISCA, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Even under these constraints, the CM-5 network interface still requires significant user-level message handling overhead. The best case using the active message mechanism requires 3.3 sec, or more than 100 SPARC CPU cycles, for software message handling <ref> [32] </ref>. We require our mechanism to support general multiprogramming, which is needed on parallel systems for the same reasons as on uniprocessor systems: it allows useful processing during I/O and paging, and it supports interactive jobs well. <p> The application makes a system call to receive the message. The network interface based on this approach is simple and can be used for commodity PCs or workstations. The main disadvantage is that message passing costs are usually thousands of CPU cycles, with the best implementation <ref> [32] </ref> still requiring over 100 CPU cycles. One solution to the problem of software overhead is to add a separate processor on every node just for message passing [22, 13]. Recent examples of this approach are the Intel Paragon [14] and Meiko CS-2 [12].
Reference: [33] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Protection is provided between separate partitions, but not between processes within a partition. Since packet headers must be constructed by applications, the message passing overhead is still hundreds of CPU instructions. Memnet [10], Merlin [21] and its successor SESAME <ref> [33] </ref>, the Plus system [4], and Galactica Net [17] use the page-based, automatic-update approach to support shared memory. These systems do not provide a mechanism for high-bandwidth, low-overhead block data transfer such as deliberate update. Some systems provide communication via direct access to remote memory locations.
References-found: 33

