URL: http://www.cs.umn.edu/Research/Agassiz/Paper/gu.lcpc95.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: An Interprocedural Parallelizing Compiler and Its Support for Memory Hierarchy Research  
Author: Trung Nguyen Junjie Gu Zhiyuan Li 
Address: Minneapolis MN 55455, USA  
Affiliation: University of Minnesota,  
Abstract: We present several new compiler techniques employed by our interprocedural parallelizing research compiler, Panorama, to improve loop parallelization and the efficiency of memory references. We first present an overview of the compiler and its associated memory architecture simulation environments. We then present an interprocedural array dataflow analysis, using guarded array regions, for automatic array priva-tization, an interprocedural static profile analysis, and a graph coloring scheme for parallel task assignment and data allocation which aims at reducing remote memory references while maintaining loop parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: By performing static task assignment and data allocation simultaneously, remote memory references may be reduced. Existing works focus on the total elimination of remote references <ref> [1, 24] </ref>. However, such an approach may often result in loop serialization, especially for large programs, thus sacrificing parallelism. Our method does 7.2 not always attempt to totally eliminate remote memory references. <p> For our purpose, we regard each VP as having its own memory module as well. Concurrent with the assignment of parallel loop iterations, we allocate array data to memory modules. Previous works <ref> [1, 24] </ref> find a communication-free partitioning of arrays which, in turn, may sequential-ize many parallel loops. Our goal here is to find heuristics that maximize the local memory accesses without sacrificing parallelism. Currently, we consider only single level parallel loops.
Reference: 2. <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: A recent paper [11] describes our work on this topic in details. We present only an outline in this section. It is worth mentioning that the summary approach has previously been used by Triolet and Irigoin [29], Callahan, Kennedy, Balasundrum and Havlak <ref> [2, 4, 13] </ref>, and more recently, Graham et al [8], to compute data dependences instead of dataflow. Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization.
Reference: 3. <author> W. Blume and R. Eigenmann. </author> <title> Symbolic analysis techniques needed or the effective parallelization of perfect benchmarks. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: It is also worth mentioning that a recent work by Tu and Padua [31] handles certain IF conditions which contain symbolic terms present in array subscripts and loop limits. Our work is not restricted to such IF conditions. Since our experiments [11] and previous works <ref> [3] </ref> suggest that the lack of the handling of IF conditions, symbolic expressions, and call statements can significantly handicap parallelizing compilers, the Panorama compiler deals with all these three aspects.
Reference: 4. <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In ACM SIGPLAN '86 Symp. Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: A recent paper [11] describes our work on this topic in details. We present only an outline in this section. It is worth mentioning that the summary approach has previously been used by Triolet and Irigoin [29], Callahan, Kennedy, Balasundrum and Havlak <ref> [2, 4, 13] </ref>, and more recently, Graham et al [8], to compute data dependences instead of dataflow. Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization.
Reference: 5. <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 68-77, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Although regular array regions cover only restricted array shapes, their operations, such as union, intersection, and difference, can be quite straightforward and efficient. Duesterwald et al <ref> [5] </ref> have examined cases in which they can compute the dependence distance for each reaching definition within a loop. These works, however, do not take IF conditions and routine calls into account. Works in the second path do not attempt to compute reaching-definitions for each individual array reference.
Reference: 6. <author> D. Lenoski et al. </author> <title> The Stanford DASH multiprocessor. </title> <booktitle> Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years, researchers see a growing interest in shared-memory multiprocessors with multi-level memories which provide private caches and/or non-uniform memory accesses (NUMA) <ref> [15, 6, 28] </ref>. In order to deliver the power of such machines to ordinary programs written by a wide range of programmers, it is important for the compilers to improve their ability to automatically identify parallelism in ordinary programs.
Reference: 7. <author> P. Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 2(1) </volume> <pages> 23-53, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We summarize our discussions in section 6. 2 Array dataflow analysis for loop parallelization Previous works on array dataflow analysis for parallelization follow two paths. The first path pursues the goal of computing reaching-definitions for each individual array reference. Feautrier <ref> [7] </ref> suggests to establish a source function for each read reference to indicate which definition defines the value for each dis 7.3 tinct array element. Maydan et al [20, 21] simplify Feautrier's method by using a Last-Write-Tree (LWT) for simpler cases. These works ignore IF statements and routine calls.
Reference: 8. <author> S.L. Graham, S. Lucco, and O. Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 100-111, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: We present only an outline in this section. It is worth mentioning that the summary approach has previously been used by Triolet and Irigoin [29], Callahan, Kennedy, Balasundrum and Havlak [2, 4, 13], and more recently, Graham et al <ref> [8] </ref>, to compute data dependences instead of dataflow. Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization. <p> In order to take account of IF conditions under which an array reference is issued and to represent the set operation results in the presence of symbolic terms, we adopt a reference predicate which further qualifies a regular array region, called a guarded array region (GAR). Graham et al <ref> [8] </ref> use a similar representation, but they do not summarize upward-exposed uses. 2.1 Guarded array regions In this section, we assume that the program contains no recursive calls and that a DO loop does not contain GOTO statements which make premature exits. 7.4 We also assume that the HSG contains no
Reference: 9. <author> E.D. Granston and A.V. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: The use of source functions for computing reaching-definitions typically results in the need for integer programming to determine the feasibility of an arbitrary set of inequalities. Several researchers, including Gross and Steenkiste [10], Rosene [26], and Granston and Veidenbaum <ref> [9] </ref>, in an apparent attempt to avoid potentially expensive symbolic manipulations and integer programming, have adopted regular array regions for their computation of reaching definitions. Although regular array regions cover only restricted array shapes, their operations, such as union, intersection, and difference, can be quite straightforward and efficient.
Reference: 10. <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The use of source functions for computing reaching-definitions typically results in the need for integer programming to determine the feasibility of an arbitrary set of inequalities. Several researchers, including Gross and Steenkiste <ref> [10] </ref>, Rosene [26], and Granston and Veidenbaum [9], in an apparent attempt to avoid potentially expensive symbolic manipulations and integer programming, have adopted regular array regions for their computation of reaching definitions.
Reference: 11. <author> J. Gu, Z. Li, and G. Lee. </author> <title> Symbolic array dataflow analysis for array privatization and program parallelization. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We find it useful to construct a graph that can represent the program's control flow globally and interprocedurally, and use this graph to efficiently propagate the information that is needed by the compiler. The PIPS project first adopts such a graph, HSCG [14], and our hierarchical supergraph (HSG) <ref> [11] </ref> is similar to HSCG. (Our compiler and the PIPS compiler are perhaps the only two research compilers which utilize such interprocedural flow graphs for interproce-dural analysis.) The HSG is a composition of the flow subgraphs of all routines in a program. <p> Works by Li [16] and Tu and Padua [30, 31] follow this second path. Our recent work continues to follow this path and extends the previous works by taking IF conditions and call statements into account. A recent paper <ref> [11] </ref> describes our work on this topic in details. We present only an outline in this section. <p> It is also worth mentioning that a recent work by Tu and Padua [31] handles certain IF conditions which contain symbolic terms present in array subscripts and loop limits. Our work is not restricted to such IF conditions. Since our experiments <ref> [11] </ref> and previous works [3] suggest that the lack of the handling of IF conditions, symbolic expressions, and call statements can significantly handicap parallelizing compilers, the Panorama compiler deals with all these three aspects. <p> A conjunctive term, which is a disconjunction of relational expressions, is marked as unknown only if all the relationals are marked as unknown. Similarly, a multidimensional regular array region is marked as unknown only if all its dimensions are marked as unknown (see <ref> [11] </ref> for details). For any given program segment, we use GAR's to summarize the sets listed below, which are important to our array dataflow analysis for array privatization and loop parallelization. <p> Hence, the operations on GAR's are composed of two simplifiers and array region operations based on basic arithmetic and logical expression operations. Due to the space limit, we discuss only the top-level operations and refer the readers to <ref> [11] </ref> for more details. <p> The sets M OD i 0 (n) and (U E i 0 (n) 7.6 MOD &lt;i 0 (n)) are then expanded across the i 0 index range to form M OD (n) and U E (n) for loop node n. (Details for this expansion are explained in <ref> [11] </ref>). 3. Propagate the array dataflow information. From node e to s, the gar summary algorithm traverses the nodes in g (s; e) in a reverse topological order. <p> Again, we attempt to find a subgraph which contains only one node for each loop or each array and the total mismatch cost is maximum. 5 Experimental results Preliminary experimental results regarding the capability of our interprocedural array dataflow analysis for array privatization can be found in <ref> [11] </ref>. Here we show data, in Figure 3, regarding the efficiency of the compiler both in its memory use and its execution time, since it is important to demonstrate that such an ambitious interprocedural parallelizing compiler can be implemented efficiently so as to be practical.
Reference: 12. <author> W. H. Harrison. </author> <title> Compiler analysis of the value ranges for variables. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year> <month> 7.14 </month>
Reference-contexts: At this point, we may reach three possible situations: - cov evaluates to a constant. Some symbolic terms remain when calculating cov but we can determine the range for cov by using one of the existing techniques <ref> [12] </ref>. 7.9 There are some unknown symbolic terms and, hence, we know nothing about cov.
Reference: 13. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Trans. on Par. and Dist. Systems, </journal> <volume> 2(3), </volume> <year> 1991. </year>
Reference-contexts: A recent paper [11] describes our work on this topic in details. We present only an outline in this section. It is worth mentioning that the summary approach has previously been used by Triolet and Irigoin [29], Callahan, Kennedy, Balasundrum and Havlak <ref> [2, 4, 13] </ref>, and more recently, Graham et al [8], to compute data dependences instead of dataflow. Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization.
Reference: 14. <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the pips project. </title> <booktitle> In Proc. Int. Conf. on Supercomputing, </booktitle> <pages> pages 244-251, </pages> <year> 1991. </year>
Reference-contexts: We find it useful to construct a graph that can represent the program's control flow globally and interprocedurally, and use this graph to efficiently propagate the information that is needed by the compiler. The PIPS project first adopts such a graph, HSCG <ref> [14] </ref>, and our hierarchical supergraph (HSG) [11] is similar to HSCG. (Our compiler and the PIPS compiler are perhaps the only two research compilers which utilize such interprocedural flow graphs for interproce-dural analysis.) The HSG is a composition of the flow subgraphs of all routines in a program.
Reference: 15. <author> D. J. Kuck, E. S. Davidson, D. J. Lawrie, and A. H. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: 1 Introduction In recent years, researchers see a growing interest in shared-memory multiprocessors with multi-level memories which provide private caches and/or non-uniform memory accesses (NUMA) <ref> [15, 6, 28] </ref>. In order to deliver the power of such machines to ordinary programs written by a wide range of programmers, it is important for the compilers to improve their ability to automatically identify parallelism in ordinary programs.
Reference: 16. <author> Z. Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. Int. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: After performing a traditional data dependence analysis, the compiler puts DO loops in three categories: parallelizable, sequential (because the main part of the loops' bodies are covered by large flow dependence cycles), and those requiring further examination after array privatization <ref> [16, 20, 30, 31] </ref>. Array privatization may eliminate output dependences and anti-dependences due to memory conflicts and it is widely recognized as a key to loop parallelization. After array privatization, the compiler re-performs an interprocedural data dependence analysis to recognize new parallelizable loops. <p> They concern themselves of the array dataflow problem for program parallelization only, for which a summary approach seems sufficient and efficient. Array modifications and upward-exposed uses are summarized for code segments, e.g. loop iterations. Works by Li <ref> [16] </ref> and Tu and Padua [30, 31] follow this second path. Our recent work continues to follow this path and extends the previous works by taking IF conditions and call statements into account. A recent paper [11] describes our work on this topic in details. <p> The M OD &lt;i , M OD &gt;i and M OD sets are obtained by three different expansions of M OD i . These sets are then used for array privatization and loop parallelization as described below. The general technique for array privatization is discussed in <ref> [16, 20, 30] </ref>. The most important condition to permit the privatization of array A in loop L is the absence of loop-carried dataflow due to A in L.
Reference: 17. <author> Z. Li. </author> <title> Propagating symbolic relations on an interprocedural and hierarchical control flow graph. </title> <type> Technical Report CSci-93-87, </type> <institution> University of Minnesota, </institution> <year> 1993. </year>
Reference-contexts: Note that the flow subgraph of a routine is never duplicated for different calls to the same routine. Based on the HSG, the compiler computes interprocedural scalar use-def chains <ref> [17] </ref>. Combining such chains with local expression trees, we in effect obtain an interprocedural version of the value graphs which are proposed by Reif and Lewis [25].
Reference: 18. <author> Z. Li and T. N. Nguyen. </author> <title> An empricial study of the work load distribution under static scheduling. </title> <booktitle> In Proc. Int. Conf. on Par. Processing, volume II: Software, </booktitle> <address> St. Charles, IL, </address> <year> 1994. </year>
Reference-contexts: The trace is then fed to an architectural simulator which generates statistics including execution time, cache miss ratio, and network traffic. Previous experimental results using this methodology are reported in <ref> [18, 22, 23] </ref>. A limitation of our trace-generation facility is the lack of sub-traces generated by the execution of library routines.
Reference: 19. <author> Vadim Maslov. </author> <title> Lazy array data-flow dependence analysis. </title> <booktitle> In Proc. of Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 331-325, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Maydan et al [20, 21] simplify Feautrier's method by using a Last-Write-Tree (LWT) for simpler cases. These works ignore IF statements and routine calls. Maslov <ref> [19] </ref> extends Feautrier's work by handling affine IF conditions and symbolic terms. His work, however, does not handle routine calls and does not address the issue of how to obtain the applicable IF conditions and symbolic expressions for each array reference.
Reference: 20. <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Proc. of the 20th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 2-15, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: After performing a traditional data dependence analysis, the compiler puts DO loops in three categories: parallelizable, sequential (because the main part of the loops' bodies are covered by large flow dependence cycles), and those requiring further examination after array privatization <ref> [16, 20, 30, 31] </ref>. Array privatization may eliminate output dependences and anti-dependences due to memory conflicts and it is widely recognized as a key to loop parallelization. After array privatization, the compiler re-performs an interprocedural data dependence analysis to recognize new parallelizable loops. <p> The first path pursues the goal of computing reaching-definitions for each individual array reference. Feautrier [7] suggests to establish a source function for each read reference to indicate which definition defines the value for each dis 7.3 tinct array element. Maydan et al <ref> [20, 21] </ref> simplify Feautrier's method by using a Last-Write-Tree (LWT) for simpler cases. These works ignore IF statements and routine calls. Maslov [19] extends Feautrier's work by handling affine IF conditions and symbolic terms. <p> The M OD &lt;i , M OD &gt;i and M OD sets are obtained by three different expansions of M OD i . These sets are then used for array privatization and loop parallelization as described below. The general technique for array privatization is discussed in <ref> [16, 20, 30] </ref>. The most important condition to permit the privatization of array A in loop L is the absence of loop-carried dataflow due to A in L.
Reference: 21. <author> Dror E. Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The first path pursues the goal of computing reaching-definitions for each individual array reference. Feautrier [7] suggests to establish a source function for each read reference to indicate which definition defines the value for each dis 7.3 tinct array element. Maydan et al <ref> [20, 21] </ref> simplify Feautrier's method by using a Last-Write-Tree (LWT) for simpler cases. These works ignore IF statements and routine calls. Maslov [19] extends Feautrier's work by handling affine IF conditions and symbolic terms.
Reference: 22. <author> T. N. Nguyen, Z. Li, and D. J. Lilja. </author> <title> Efficient use of dynamically tagged directories through compiler analysis. </title> <booktitle> In Proc. Int. Conf. on Par. Processing, volume II: Software, </booktitle> <pages> pages 112-119, </pages> <address> St. Charles, IL, </address> <year> 1993. </year>
Reference-contexts: The trace is then fed to an architectural simulator which generates statistics including execution time, cache miss ratio, and network traffic. Previous experimental results using this methodology are reported in <ref> [18, 22, 23] </ref>. A limitation of our trace-generation facility is the lack of sub-traces generated by the execution of library routines.
Reference: 23. <author> T. N. Nguyen, F. Mounes-Toussi, D. J. Lilja, and Z. Li. </author> <title> A compiler-assisted scheme for adaptive cache coherence enforcement. </title> <booktitle> In Proc. Int. Conf. on Par. Arch. and Compilation Techniques, </booktitle> <pages> pages 69-78, </pages> <year> 1994. </year>
Reference-contexts: We shall discuss our static task profile analysis in Section 3 and discuss a graph coloring algorithm, in Section 4, for performing task assignment and data allocation simultaneously. Our strategy for reducing coherence actions has been presented in a previous paper <ref> [23] </ref>. Therefore, we shall not address this issue further in this paper. To study the effectiveness of the compiler analysis, we are currently developing compiler modules for generating parallel codes for the Silicon Graphics Power Challenge and the Cray T3D supercomputer. <p> Each memory reference can be annotated with any information that the simulator may need, such as the memory address, the processor issuing the reference, and whether the reference is a read, a coherence write, or a non-coherence write <ref> [23] </ref>. During the annotation process, a memory map assigning variables to virtual addresses is created. In addition to annotating memory references and ALU operations, the annotator also inserts markers to identify parallel loops and their chosen scheduling method in the trace. <p> The trace is then fed to an architectural simulator which generates statistics including execution time, cache miss ratio, and network traffic. Previous experimental results using this methodology are reported in <ref> [18, 22, 23] </ref>. A limitation of our trace-generation facility is the lack of sub-traces generated by the execution of library routines.
Reference: 24. <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Trans. on Par. and Dist. Systems, </journal> <volume> 2(4), </volume> <year> 1991. </year>
Reference-contexts: By performing static task assignment and data allocation simultaneously, remote memory references may be reduced. Existing works focus on the total elimination of remote references <ref> [1, 24] </ref>. However, such an approach may often result in loop serialization, especially for large programs, thus sacrificing parallelism. Our method does 7.2 not always attempt to totally eliminate remote memory references. <p> For our purpose, we regard each VP as having its own memory module as well. Concurrent with the assignment of parallel loop iterations, we allocate array data to memory modules. Previous works <ref> [1, 24] </ref> find a communication-free partitioning of arrays which, in turn, may sequential-ize many parallel loops. Our goal here is to find heuristics that maximize the local memory accesses without sacrificing parallelism. Currently, we consider only single level parallel loops.
Reference: 25. <author> J. H. Reif and H. R. Lewis. </author> <title> Symbolic evaluation and the global value graph. </title> <booktitle> In Conf. Record of the Fourth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 104-118, </pages> <year> 1977. </year>
Reference-contexts: Based on the HSG, the compiler computes interprocedural scalar use-def chains [17]. Combining such chains with local expression trees, we in effect obtain an interprocedural version of the value graphs which are proposed by Reif and Lewis <ref> [25] </ref>. The value graphs are used in our compiler for transforming array subscripts and loop control expressions into linear expressions in terms of loop indices and symbolic terms that are loop invariants.
Reference: 26. <author> C. Rosene. </author> <title> Incremental dependence analysis. </title> <type> Technical Report CRPC-TR90044, PhD thesis, </type> <institution> Computer Science Department, Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The use of source functions for computing reaching-definitions typically results in the need for integer programming to determine the feasibility of an arbitrary set of inequalities. Several researchers, including Gross and Steenkiste [10], Rosene <ref> [26] </ref>, and Granston and Veidenbaum [9], in an apparent attempt to avoid potentially expensive symbolic manipulations and integer programming, have adopted regular array regions for their computation of reaching definitions.
Reference: 27. <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: In order to characterize memory activities more precisely while retaining the information provided by parallelizing compilers, we are exploring methods to combine our compiler-annotated traces with library routine traces that can be extracted by tools such as pixie <ref> [27] </ref>. The simulation issue will not be discussed further in this paper. In the remainder of the paper, we expand the discussion of some of the techniques listed above. In addition, we present preliminary results of the Panorama compiler in section 5.
Reference: 28. <author> P. Stenstrom, J. Truman, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proc. Int. Sym. on Comp. Arch., </booktitle> <pages> pages 80-91, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years, researchers see a growing interest in shared-memory multiprocessors with multi-level memories which provide private caches and/or non-uniform memory accesses (NUMA) <ref> [15, 6, 28] </ref>. In order to deliver the power of such machines to ordinary programs written by a wide range of programmers, it is important for the compilers to improve their ability to automatically identify parallelism in ordinary programs.
Reference: 29. <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statments. </title> <booktitle> In ACM SIGPLAN '86 Sym. on Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: A recent paper [11] describes our work on this topic in details. We present only an outline in this section. It is worth mentioning that the summary approach has previously been used by Triolet and Irigoin <ref> [29] </ref>, Callahan, Kennedy, Balasundrum and Havlak [2, 4, 13], and more recently, Graham et al [8], to compute data dependences instead of dataflow. Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization.
Reference: 30. <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proc. Lang. and Compilers for Par. Computing, </booktitle> <pages> pages 500-521, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: After performing a traditional data dependence analysis, the compiler puts DO loops in three categories: parallelizable, sequential (because the main part of the loops' bodies are covered by large flow dependence cycles), and those requiring further examination after array privatization <ref> [16, 20, 30, 31] </ref>. Array privatization may eliminate output dependences and anti-dependences due to memory conflicts and it is widely recognized as a key to loop parallelization. After array privatization, the compiler re-performs an interprocedural data dependence analysis to recognize new parallelizable loops. <p> They concern themselves of the array dataflow problem for program parallelization only, for which a summary approach seems sufficient and efficient. Array modifications and upward-exposed uses are summarized for code segments, e.g. loop iterations. Works by Li [16] and Tu and Padua <ref> [30, 31] </ref> follow this second path. Our recent work continues to follow this path and extends the previous works by taking IF conditions and call statements into account. A recent paper [11] describes our work on this topic in details. We present only an outline in this section. <p> The M OD &lt;i , M OD &gt;i and M OD sets are obtained by three different expansions of M OD i . These sets are then used for array privatization and loop parallelization as described below. The general technique for array privatization is discussed in <ref> [16, 20, 30] </ref>. The most important condition to permit the privatization of array A in loop L is the absence of loop-carried dataflow due to A in L.
Reference: 31. <author> P. Tu and D. Padua. </author> <title> Gated SSA-Based demand-driven symbolic analysis for par-allelizing compilers. </title> <booktitle> In Proc. Int. Conf. on Supercomputing, </booktitle> <pages> pages 414-423, </pages> <month> July </month> <year> 1995. </year> <title> This article was processed using the L A T E X macro package with LLNCS style 7.15 </title>
Reference-contexts: After performing a traditional data dependence analysis, the compiler puts DO loops in three categories: parallelizable, sequential (because the main part of the loops' bodies are covered by large flow dependence cycles), and those requiring further examination after array privatization <ref> [16, 20, 30, 31] </ref>. Array privatization may eliminate output dependences and anti-dependences due to memory conflicts and it is widely recognized as a key to loop parallelization. After array privatization, the compiler re-performs an interprocedural data dependence analysis to recognize new parallelizable loops. <p> They concern themselves of the array dataflow problem for program parallelization only, for which a summary approach seems sufficient and efficient. Array modifications and upward-exposed uses are summarized for code segments, e.g. loop iterations. Works by Li [16] and Tu and Padua <ref> [30, 31] </ref> follow this second path. Our recent work continues to follow this path and extends the previous works by taking IF conditions and call statements into account. A recent paper [11] describes our work on this topic in details. We present only an outline in this section. <p> Their works summarize all array uses while ours summarizes upward-exposed uses which are essential for array privatization. It is also worth mentioning that a recent work by Tu and Padua <ref> [31] </ref> handles certain IF conditions which contain symbolic terms present in array subscripts and loop limits. Our work is not restricted to such IF conditions.
References-found: 31

