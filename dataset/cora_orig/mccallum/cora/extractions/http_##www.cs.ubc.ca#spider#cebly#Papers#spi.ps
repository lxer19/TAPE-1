URL: http://www.cs.ubc.ca/spider/cebly/Papers/spi.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  dearden@cs.ubc.ca  moises@rpal.rockwell.com  
Title: Exploiting Structure in Policy Construction  
Author: Craig Boutilier Richard Dearden Mois es Goldszmidt 
Address: Vancouver, BC V6T 1Z4, CANADA  Vancouver, BC V6T 1Z4, CANADA  444 High Street Palo Alto, CA 94301, U.S.A.  
Affiliation: Department of Computer Science University of British Columbia  Department of Computer Science University of British Columbia  Rockwell Science Center  
Date: August, 1995  
Note: To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal,  
Abstract: Markov decision processes (MDPs) have recently been applied to the problem of modeling decision-theoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploits the variable and propositional independencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with re cent approximation methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A., Bradtke, S., and Singh, S. </author> <year> 1995. </year> <title> Learning to Act using Realtime Dynamic programming. </title> <journal> Artif. Intel., </journal> <volume> 72 </volume> <pages> 81-138. </pages>
Reference: <author> Boutilier, C. and Dearden, R. </author> <year> 1994. </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1016-1022, Seattle. </address>
Reference-contexts: Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including local search methods (Dean et al. 1993; Dearden and Boutilier 1994; Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> In this paper, we describe our investigations of a commonly used algorithm from OR called modified policy iteration (MPI) (Puterman and Shin 1978). We present a new algorithm called structured policy iteration (SPI) which uses the same computational mechanism as MPI. As in <ref> (Boutilier and Dearden 1994) </ref>, we assume a compact representation of an MDP, in this case using a two-slice temporal Bayesian network (Dean and Kanazawa 1989; Darwiche and Goldszmidt 1994) to represent the dependence between variables before and after the occurrence of an action. <p> The two phases of the algorithm, structured successive approximation and structured policy improvement, are described individually. We illustrate the algorithm on a detailed example, and describe the results of our implementation. We refer to the full paper <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a much more detailed description of the algorithm and implementation, and discussion of additional issues. 2 Modified Policy Iteration We assume a DTP problem can be modeled as a completely observable MDP. <p> Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 Go (to opposite location), BuyC (buy coffee), DelC (deliver coffee to user), GetU (get umbrella). Each of these actions has the obvious effect on a state, but may fail with some probability (see <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a full problem specification). We discuss one possible representation for actions and utilities, Bayesian networks, and in the next section show how this information can be exploited in MPI. <p> The conditional relevance of variables can be quantified and trees can be pruned by deleting nodes having the least impact on value, even at intermediate stages. In this way, abstraction methods such as those of <ref> (Boutilier and Dearden 1994) </ref> can be made far more adaptive. Acknowledgements: Thanks to Tom Dean, David Poole and Marty Puterman and an anonymous reviewer for discussions and comments on this paper. This research was supported by NSERC Research Grant OGP0121843, NCE IRIS-II Project IC-7 and Rockwell International.
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> 1994. </year> <title> Exploiting structure in optimal policy construction. </title> <type> Technical Report 94-23, </type> <institution> University of British Columbia, Vancouver. </institution>
Reference-contexts: Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including local search methods (Dean et al. 1993; Dearden and Boutilier 1994; Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> In this paper, we describe our investigations of a commonly used algorithm from OR called modified policy iteration (MPI) (Puterman and Shin 1978). We present a new algorithm called structured policy iteration (SPI) which uses the same computational mechanism as MPI. As in <ref> (Boutilier and Dearden 1994) </ref>, we assume a compact representation of an MDP, in this case using a two-slice temporal Bayesian network (Dean and Kanazawa 1989; Darwiche and Goldszmidt 1994) to represent the dependence between variables before and after the occurrence of an action. <p> The two phases of the algorithm, structured successive approximation and structured policy improvement, are described individually. We illustrate the algorithm on a detailed example, and describe the results of our implementation. We refer to the full paper <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a much more detailed description of the algorithm and implementation, and discussion of additional issues. 2 Modified Policy Iteration We assume a DTP problem can be modeled as a completely observable MDP. <p> Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 Go (to opposite location), BuyC (buy coffee), DelC (deliver coffee to user), GetU (get umbrella). Each of these actions has the obvious effect on a state, but may fail with some probability (see <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a full problem specification). We discuss one possible representation for actions and utilities, Bayesian networks, and in the next section show how this information can be exploited in MPI. <p> The conditional relevance of variables can be quantified and trees can be pruned by deleting nodes having the least impact on value, even at intermediate stages. In this way, abstraction methods such as those of <ref> (Boutilier and Dearden 1994) </ref> can be made far more adaptive. Acknowledgements: Thanks to Tom Dean, David Poole and Marty Puterman and an anonymous reviewer for discussions and comments on this paper. This research was supported by NSERC Research Grant OGP0121843, NCE IRIS-II Project IC-7 and Rockwell International.
Reference: <author> Chapman, D. and Kaelbling, L. P. </author> <year> 1991. </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> IJCAI-91, </booktitle> <address> pp.726-731, Sydney. </address>
Reference-contexts: During the first iteration of SPI, SSA may use the immediate reward tree as its initial structured estimate. In subsequent iterations the initial estimate is the computed value-tree for the previous policy. 5 We note that tree representations of policies are sometimes used in reinforcement learning as well <ref> (Chapman and Kaelbling 1991) </ref>; however, the motivation there is somewhat different. In addition, the ordering of variables in the tree can have a dramatic impact on the size of the representation (see Section 5). Input: Tree (V i ), action a; Output: Tree (V i+1 ) 1.
Reference: <author> Darwiche, A. and Goldszmidt, M. </author> <year> 1994. </year> <title> Action networks: A framework for reasoning about actions and change under uncertainty. </title> <address> UAI-94, pp.136-144, Seattle. </address>
Reference: <author> Dean, T., Kaelbling, L. P., Kirman, J., and Nicholson, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> AAAI-93, </booktitle> <address> pp.574-579, Washington, D.C. </address>
Reference: <author> Dean, T. and Kanazawa, K. </author> <year> 1989. </year> <title> A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150. </pages>
Reference: <author> Dean, T. and Wellman, M. </author> <year> 1991. </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: For this reason, there has been much interest in decision theoretic planning <ref> (Dean and Wellman 1991) </ref>. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP (Dean et al. 1993; Boutilier and Dearden 1994; Tash and Russell 1994).
Reference: <author> Dearden, R. and Boutilier, C. </author> <year> 1994. </year> <title> Integrating planning and execution in stochastic domains. </title> <address> UAI-94, pp.162-169, Seattle. </address>
Reference-contexts: Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including local search methods (Dean et al. 1993; Dearden and Boutilier 1994; Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> Barto, Bradtke and Singh 1995; Tash and Russell 1994) or reducing the state space via abstraction <ref> (Boutilier and Dearden 1994) </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately optimal solutions (whose accuracy can sometimes be bounded a priori (Boutilier and Dearden 1994)). <p> In this paper, we describe our investigations of a commonly used algorithm from OR called modified policy iteration (MPI) (Puterman and Shin 1978). We present a new algorithm called structured policy iteration (SPI) which uses the same computational mechanism as MPI. As in <ref> (Boutilier and Dearden 1994) </ref>, we assume a compact representation of an MDP, in this case using a two-slice temporal Bayesian network (Dean and Kanazawa 1989; Darwiche and Goldszmidt 1994) to represent the dependence between variables before and after the occurrence of an action. <p> The two phases of the algorithm, structured successive approximation and structured policy improvement, are described individually. We illustrate the algorithm on a detailed example, and describe the results of our implementation. We refer to the full paper <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a much more detailed description of the algorithm and implementation, and discussion of additional issues. 2 Modified Policy Iteration We assume a DTP problem can be modeled as a completely observable MDP. <p> Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 Go (to opposite location), BuyC (buy coffee), DelC (deliver coffee to user), GetU (get umbrella). Each of these actions has the obvious effect on a state, but may fail with some probability (see <ref> (Boutilier, Dearden and Goldszmidt 1994) </ref> for a full problem specification). We discuss one possible representation for actions and utilities, Bayesian networks, and in the next section show how this information can be exploited in MPI. <p> The conditional relevance of variables can be quantified and trees can be pruned by deleting nodes having the least impact on value, even at intermediate stages. In this way, abstraction methods such as those of <ref> (Boutilier and Dearden 1994) </ref> can be made far more adaptive. Acknowledgements: Thanks to Tom Dean, David Poole and Marty Puterman and an anonymous reviewer for discussions and comments on this paper. This research was supported by NSERC Research Grant OGP0121843, NCE IRIS-II Project IC-7 and Rockwell International.
Reference: <author> Howard, R. A. </author> <year> 1971. </year> <title> Dynamic Probabilistic Systems. </title> <publisher> Wiley. </publisher>
Reference: <author> Kushmerick, N., Hanks, S., and Weld, D. </author> <year> 1994. </year> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1073-1078, Seattle. </address>
Reference: <author> Poole, D. </author> <year> 1993. </year> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artif. Intel., </journal> <volume> 64(1) </volume> <pages> 81-129. </pages>
Reference-contexts: To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 we consider the variables in Tree (V i ) individually. More precisely, explanations are generated by a process we call abductive repartitioning, quite similar in spirit to probabilistic Horn abduction <ref> (Poole 1993) </ref>. A given traversal of Tree (V i ) induces an ordering of relevant (post-action) variables; we explain variables in Tree (V i ) according to this order (Step 3 of Figure 3).
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Just as network algorithms have proven practical for reasoning under uncertainty, we expect SPI to be quite useful in practice. 1 In Section 2 we briefly describe MDPs and the MPI algorithm; we refer to <ref> (Puterman 1994) </ref> for a more detailed description of MDPs and solution techniques. In Section 3 we discuss our representation of MDPs using decision trees, and in Section 4 we describe the structured policy iteration algorithm. <p> However, their method is restricted to finite-horizon problems, and adopts value iteration, which converges much too slowly on the infinite (or indefinite) horizon problems frequently encountered in planning <ref> (Puterman 1994) </ref>. 2 Thus we restrict attention to stationary policies. For the problems we consider, optimal stationary policies always exist. we want to maximize the expected accumulated discounted rewards over an infinite time period. <p> The estimate V i (s) is given by V (s) = R (s) + fi t2S i1 Modified policy iteration (Puterman and Shin 1978) uses some number of successive approximation steps to produce an estimate of V (s) at step 1. We refer to <ref> (Puterman 1994) </ref> for theoretical and practical advice for choice of good stopping criteria. MPI is used frequently in practice for large state space problems with good results (Puterman 1994). 3 Representation of MDPs It is unreasonable to expect that DTP problems, while for-mulable as MDPs, will be specified in the manner <p> We refer to <ref> (Puterman 1994) </ref> for theoretical and practical advice for choice of good stopping criteria. MPI is used frequently in practice for large state space problems with good results (Puterman 1994). 3 Representation of MDPs It is unreasonable to expect that DTP problems, while for-mulable as MDPs, will be specified in the manner described above. <p> Since the probability of reaching a given partition in V i is a function of the probabilities of the individual variables on its branch, we can build this explanation componentwise: 6 Better stopping criteria are possible <ref> (Puterman 1994) </ref>, but have no bearing on our algorithm. To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 we consider the variables in Tree (V i ) individually.
Reference: <author> Puterman, M. and Shin, M. </author> <year> 1978. </year> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Mgmt. Sci., </journal> <volume> 24 </volume> <pages> 1127-1137. </pages>
Reference-contexts: This third point is especially significant because approximation methods such as abstraction often require that one optimally solve a smaller problem. In this paper, we describe our investigations of a commonly used algorithm from OR called modified policy iteration (MPI) <ref> (Puterman and Shin 1978) </ref>. We present a new algorithm called structured policy iteration (SPI) which uses the same computational mechanism as MPI. <p> V is approximated by a sequence of vectors V 0 ; V 1 ; , each a successively better estimate. The initial estimate V 0 is any random jSj-vector. The estimate V i (s) is given by V (s) = R (s) + fi t2S i1 Modified policy iteration <ref> (Puterman and Shin 1978) </ref> uses some number of successive approximation steps to produce an estimate of V (s) at step 1. We refer to (Puterman 1994) for theoretical and practical advice for choice of good stopping criteria.
Reference: <author> Rivest, R. </author> <year> 1987. </year> <title> Learning decision lists. </title> <journal> Mach. Learn., </journal> <volume> 2 </volume> <pages> 229-246. </pages>
Reference-contexts: This induces the explicit policy (s) = a i iff s j= i . Structured policies can be represented in many ways (e.g., with decision lists <ref> (Rivest 1987) </ref>). We adopt a decision tree representation similar to the representation of probability matrices above. Leaves are labeled with the action to be performed given the partial assignment corresponding to the branch. Thus, if there are k leaf nodes, the state space is partitioned into k subsets or clusters.
Reference: <author> Smith, J., Holtzman, S., and Matheson, J. </author> <year> 1993. </year> <title> Structuring conditional relationships in influence diagrams. Op. </title> <journal> Res., </journal> <volume> 41(2) </volume> <pages> 280-297. </pages>
Reference-contexts: In addition, we use a structured decision tree representation of the conditional probability matrices quantifying the network to exploit propositional independence, that is, independence given a particular variable assignment (see also <ref> (Smith, Holtzman and Math-eson 1993) </ref>). Propositional independence is reflected in the specific quantification of the network, in contrast to the vari To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 able independence captured by the network structure. <p> The post-action nodes have the usual matrices describing the probability of their values given the values of their parents, under action A. We assume that these conditional probability matrices are represented using a decision tree (or if-then rules) <ref> (Smith, Holtzman and Matheson 1993) </ref>. This allows independence among variable assignments to be represented, not just variable independence (as captured by the network structure), and is exploited to great effect below.
Reference: <author> Tash, J. and Russell, S. </author> <year> 1994. </year> <title> Control strategies for a stochastic planner. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1079-1085, Seattle. </address>
Reference: <author> Tatman, J. and Shachter, R. </author> <year> 1990. </year> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Trans. Sys., Man, Cyber., </journal> <volume> 20 </volume> <pages> 365-379. </pages>
References-found: 18

