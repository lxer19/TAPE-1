URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-31.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+97-31
Root-URL: 
Email: bernhard@ai.univie.ac.at  ihw@cs.waikato.ac.nz  
Title: Improving Bagging Performance by Increasing Decision Tree Diversity  
Author: BERNHARD PFAHRINGER IAN H. WITTEN Editor: Philip Chan 
Keyword: Bagging, Boosting, Sampling, Diversity, Decision Trees  
Address: Vienna, Austria  Hamilton, New Zealand  
Affiliation: Austrian Research Institute for AI,  Department of Computer Science, University of Waikato,  
Note: 1-19 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Ensembles of decision trees often exhibit greater predictive accuracy than single trees alone. Bagging and boosting are two standard ways of generating and combining multiple trees. Boosting has been empirically determined to be the more effective of the two, and it has recently been proposed that this may be because it produces more diverse trees than bagging. This paper reports empirical findings that strongly support this hypothesis. We enforce greater decision tree diversity in bagging by a simple modification of the underlying decision tree learner that utilizes randomly-generated decision stumps of predefined depth as the starting point for tree induction. The modified procedure yields very competitive results while still retaining one of the attractive properties of bagging: all iterations are independent. Additionally, we also investigate a possible integration of bagging and boosting. All these ensemble-generating procedures are compared empirically on various domains. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bailey T.L., Elkan C.: </author> <title> Estimating the Accuracy of Learned Concepts, in Bajcsy R.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.895-901, </address> <year> 1993. </year>
Reference-contexts: Presumably, decision tree inducers are not so "weak" after all. To estimate predictive accuracy, stratified five-fold cross-validation was repeated five times. The rationale is this. Single cross-validation can be rather unstable <ref> [1] </ref>, therefore one should average at least a few. Results of five-fold cross-validation do not seem to differ greatly from those of the more commonly chosen ten-fold cross-validation.
Reference: 2. <author> Bauer E., Kohavi R.: </author> <title> An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants, submitted to the Special Issue on Integrating Multiple Learned Models, </title> <journal> Machine Learning Journal, </journal> <year> 1997. </year>
Reference-contexts: However, boosting seems empirically to give better results, and explanations have been sought for this difference in terms of a decomposition into bias and variance <ref> [4, 2] </ref>. Furthermore, it has recently been noticed that boosting tends to produce a more diverse set of classifiers than bagging, and this has been cited as a factor in increased performance [11]. <p> This ensures that it applies to any kind of weak learner. However, if the weak learner can handle weights directly, there is no need to sample: instead all examples can be supplied, with their weights. Like <ref> [14, 2] </ref>, we have chosen this alternative. 1 Therefore we have commented out line 3 of the ADAboost description given in Figure 2. The superior performance of boosting over bagging has been attributed to a reduction of both variance and (statistical) bias, whereas bagging seems to reduce solely variance. <p> Results Tables 3 and 4 give the average predictive error rates, with errorbars showing the measured standard deviation, for all domains and methods. Not surprisingly, no clear single winner emerges. But we can observe some interesting global trends. First, as has been observed by others <ref> [2] </ref>, standard bagging is uniformly better than C4.5|its performance is never worse. On the other hand, the improvement is rather modest most of the time. Bagging's impotence is evident from the Table 2, which summarizes the rankings of each method. Standard bagging never wins in any domain. <p> Bagged boosting seems to have a kind of moderating influence on standard boosting: where plain boosting performs badly, bagging improves it, but it damages performance in domains where boosting excels. Bagged boosting may present a reasonable alternative for noisy domains, where ADAboost sometimes performs badly <ref> [2, 14] </ref>. Initial stumps of depth one and three perform less well in S-bagging than stumps of depth two, on average. These choices may produce too little and too great diversity, respectively. However, we do not think that this indicates that depth two is necessarily superior in general.
Reference: 3. <author> Breiman L.: </author> <title> Bagging Predictors, </title> <journal> Machine Learning, </journal> <volume> 24(2), </volume> <year> 1996. </year>
Reference-contexts: D R A F T October 14, 1997, 7:30pm D R A F T IMPROVING BAGGING 3 2.1. Bagging In an important discovery, Breiman <ref> [3] </ref> noticed that some kind of classifiers, including decision trees, are rather unstable: small changes in the training set can cause classifiers induced from them to be quite different.
Reference: 4. <author> Breiman L.: </author> <title> Bias, Variance and Arcing Classifiers, </title> <institution> University of California, Statistics Department, Berkeley, CA, </institution> <type> Technical Report 460, </type> <year> 1996. </year>
Reference-contexts: However, boosting seems empirically to give better results, and explanations have been sought for this difference in terms of a decomposition into bias and variance <ref> [4, 2] </ref>. Furthermore, it has recently been noticed that boosting tends to produce a more diverse set of classifiers than bagging, and this has been cited as a factor in increased performance [11]. <p> Therefore it is not necessarily true (as claimed in [11], for example) that boosting assigns smaller weights to hypotheses with lower accuracy. A good hypothesis for the current probability distribution might have a considerable error rate on the original training set represented by the initial probability distribution. Unlike both <ref> [4] </ref> and [11], our implementation does not resample and restart in cases where * t exceeds 0.5. In preliminary experiments we found that crossing this threshold is a very good indicator that boosting is failing. <p> Pruning ensembles can be viewed as an a posteriori way of re-simplifying the structures that are learned. But a more direct approach might be feasible for some combinations of domains and learners. As discussed in <ref> [4] </ref>, ensembles seem to address the problems of bias and variance simultaneously|albeit to a different degree depending on the specific ensemble generator. Now if a learner is adjustable to a rich bias space, one could envision ensemble-guided bias adjustment leading to single classifiers that perform well.
Reference: 5. <author> Cortes C., Vapnik V.: </author> <title> Support-Vector Networks, </title> <journal> Machine Learning, </journal> <volume> 20(3), </volume> <year> 1995. </year> <title> D R A F T October 14, 1997, 7:30pm D R A F T IMPROVING BAGGING 19 </title>
Reference-contexts: Generally, all the procedures compared in this paper use either an equal-weight policy or some local form of weight computation. More advanced schemes that take into account all classifiers when deriving weights include "stacked generalization" [17, 16] and "vector support networks" <ref> [5] </ref>. Such schemes might be able to enhance bagging's performance even more, further narrowing the gap between bagging and boosting. Other higher-level issues involve the recently-proposed idea of pruning ensembles [11].
Reference: 6. <author> Efron B., Tibshirani R.: </author> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Examples are drawn at random, with replacement, to yield sets of the same size as the original training set, but which contain on average only about 63.2% of the training examples, the remainder being duplicates. This method is reminiscent of the statistical procedure of "bootstrapping" <ref> [6] </ref>. Input: m examples, desired number of iterations T 1. initialize D (i) = 1=m; 2. for t = 1 to T do 3.
Reference: 7. <author> Freund Y., Schapire R.E.: </author> <title> Experiments with a New Boosting Algorithm, in Saitta L.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Francisco, CA, pp.148-156, </address> <year> 1996. </year>
Reference-contexts: The second author acknowledges generous support from the New Zealand Marsden Fund. Notes 1. We have also experimented with sampling, but our experimental results failed to show a clear difference between bagging and boosting. This accords with results reported in <ref> [7] </ref>, who also employed sampling. However, even using sampling we observed the same qualitative improve ment with more diverse trees that is reported in later sections.
Reference: 8. <author> Harvey W.D., </author> <title> Ginsberg M.L.: Limited Discrepancy Search, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.607-613, </address> <year> 1995. </year>
Reference-contexts: The idea of fixing some root structure at random is inspired by a discovery in heuristic search reported in <ref> [8] </ref>: the likelihood of making a bad heuristic choice is largest close to the root of a search tree.
Reference: 9. <author> Krogh A., Vedelsby J.: </author> <title> Neural Network Ensembles, Cross Validation, and Active Learning, </title> <editor> in Tesauro G., et al., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: The Kappa-Error diagram plots the average predictive error of a pair of classifiers (vertically) against their value (horizontally). Ideally, when combining classifiers in an ensemble, one would like the individual classifiers to be simultaneously maximally distinct and maximally correct <ref> [9] </ref>: a pair of such classifiers would appear at the lower lefthand corner of the diagram.
Reference: 10. <author> Maclin R., Opitz D.: </author> <title> An Empirical Evaluation of Bagging and Boosting, </title> <booktitle> in Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, CA, pp.546-551, </address> <year> 1997. </year>
Reference: 11. <author> Margineantu D.D., Dietterich T.G.: </author> <title> Pruning Adaptive Boosting, </title> <booktitle> in Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Furthermore, it has recently been noticed that boosting tends to produce a more diverse set of classifiers than bagging, and this has been cited as a factor in increased performance <ref> [11] </ref>. The present paper explores the proposition that the higher performance of boosting can be explained by the greater diversity of the classifiers that it produces. <p> The next section describes the various ensemble-generating procedures that we explore. Section 3 explains the experimental design and reports on the results obtained. In Section 4, we review the Kappa-Error diagram of <ref> [11] </ref> as a means of comparison of ensembles, and define a new syntactic similarity measure for decision trees that overcomes some of the deficiencies of the Kappa-Error diagram for this purpose. Finally, Section 5 summarizes and elaborates upon our findings, pointing out interesting directions for further research. 2. <p> The same weight ff t also determines the contribution of the associated hypothesis h t to the final ensemble h. Therefore it is not necessarily true (as claimed in <ref> [11] </ref>, for example) that boosting assigns smaller weights to hypotheses with lower accuracy. A good hypothesis for the current probability distribution might have a considerable error rate on the original training set represented by the initial probability distribution. Unlike both [4] and [11], our implementation does not resample and restart in <p> it is not necessarily true (as claimed in <ref> [11] </ref>, for example) that boosting assigns smaller weights to hypotheses with lower accuracy. A good hypothesis for the current probability distribution might have a considerable error rate on the original training set represented by the initial probability distribution. Unlike both [4] and [11], our implementation does not resample and restart in cases where * t exceeds 0.5. In preliminary experiments we found that crossing this threshold is a very good indicator that boosting is failing. <p> To visualize these differences we have used two techniques: the Kappa-Error diagram of <ref> [11] </ref> and a similar DTSim-Error diagram. 4.1. Kappa Margineantu and Dietterich [11] introduce a diagram based on the well-known Kappa statistic that quantifies the similarity of two classifiers by comparing their predictions. The Kappa statistic is an extensional measure of comparison and can be applied to any categorical classifier. <p> To visualize these differences we have used two techniques: the Kappa-Error diagram of <ref> [11] </ref> and a similar DTSim-Error diagram. 4.1. Kappa Margineantu and Dietterich [11] introduce a diagram based on the well-known Kappa statistic that quantifies the similarity of two classifiers by comparing their predictions. The Kappa statistic is an extensional measure of comparison and can be applied to any categorical classifier. <p> Such schemes might be able to enhance bagging's performance even more, further narrowing the gap between bagging and boosting. Other higher-level issues involve the recently-proposed idea of pruning ensembles <ref> [11] </ref>. It can be quite demanding|particularly of memory|to retain a great number of large classifiers, and it might be possible to decrease this burden by pruning ensembles without prejudicing performance, or perhaps to actually improve performance as one does when pruning decision trees.
Reference: 12. <author> Merz, C.J., Murphy, </author> <title> P.M.: UCI Repository of machine learning databases, </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: All experiments use decision tree induction as the underlying weak learning method. We have chosen a reasonable mix of small- and medium-size databases available at the UCI repository <ref> [12] </ref> for this study. The next section describes the various ensemble-generating procedures that we explore. Section 3 explains the experimental design and reports on the results obtained. <p> SB2 S-bagging with random initial stumps of depth 2. SB3 S-bagging with random initial stumps of depth 3. RB R-bagging (Figure 4) as described in Section 2.4. BB B-boosting (Figure 5) as described in Section 2.5. All datasets used for experiments are from the UC Irvine repository <ref> [12] </ref>. We chose some small and medium-sized datasets that exhibit a good mix with respect to the following characteristics: number of classes, number of examples, number of attributes, and proportion of categorical and numerical attributes.
Reference: 13. <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The procedure induce tree uses a rational reconstruction of the well-known decision tree inducer C4.5 <ref> [13] </ref> to induce and prune decision trees. Our reconstruction deviates from the original C4.5 implementation in just two points. First, missing values are handled differently: they are treated as a distinct value, instead of using the weighting procedure adopted by C4.5.
Reference: 14. <author> Quinlan J.R.: Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5, Proceedings of the AAAI'96 Conference, </booktitle> <year> 1996. </year>
Reference-contexts: This ensures that it applies to any kind of weak learner. However, if the weak learner can handle weights directly, there is no need to sample: instead all examples can be supplied, with their weights. Like <ref> [14, 2] </ref>, we have chosen this alternative. 1 Therefore we have commented out line 3 of the ADAboost description given in Figure 2. The superior performance of boosting over bagging has been attributed to a reduction of both variance and (statistical) bias, whereas bagging seems to reduce solely variance. <p> For instance, overfitting might reasonably explain boosting's performance in the Audiology domain, as there is ample opportunity for overfitting given such a small dataset together with a large number of both attributes and classes. Bad boosting performance in the Colic domain has been observed before <ref> [14] </ref> too, but apparently no explanation for this failure is known. Table 2. Ranking summary: how many times is a method the Nth-best method for some domain. <p> Bagged boosting seems to have a kind of moderating influence on standard boosting: where plain boosting performs badly, bagging improves it, but it damages performance in domains where boosting excels. Bagged boosting may present a reasonable alternative for noisy domains, where ADAboost sometimes performs badly <ref> [2, 14] </ref>. Initial stumps of depth one and three perform less well in S-bagging than stumps of depth two, on average. These choices may produce too little and too great diversity, respectively. However, we do not think that this indicates that depth two is necessarily superior in general.
Reference: 15. <author> Schapire R.E., Freund Y., Bartlett P., Lee W.S.: </author> <title> Boosting the Margin: A new Explanation for the Effectiveness of Voting Methods, </title> <booktitle> in Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: This is a valuable asset, particularly for large datasets. 2.2. Boosting The method of boosting <ref> [15] </ref>, unlike bagging, is a purely sequential procedure. The key idea is to force the induction algorithm to concentrate its effort on examples that it misclassified in previous iterations. For this reason boosting can be called "informed" in the sense that sampling takes previous performance into account. <p> This seems a reasonable choice for decision trees, where further iterations generally yield only marginal gains (unlike other kinds of weak learner, where even thousands of iterations may make a significant difference <ref> [15] </ref>). Presumably, decision tree inducers are not so "weak" after all. To estimate predictive accuracy, stratified five-fold cross-validation was repeated five times. The rationale is this. Single cross-validation can be rather unstable [1], therefore one should average at least a few.
Reference: 16. <author> Ting K.M., Witten I.H.: </author> <title> Stacked Generalization: When does it Work?, </title> <booktitle> in Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, pp.866-875, </address> <year> 1997. </year>
Reference-contexts: Generally, all the procedures compared in this paper use either an equal-weight policy or some local form of weight computation. More advanced schemes that take into account all classifiers when deriving weights include "stacked generalization" <ref> [17, 16] </ref> and "vector support networks" [5]. Such schemes might be able to enhance bagging's performance even more, further narrowing the gap between bagging and boosting. Other higher-level issues involve the recently-proposed idea of pruning ensembles [11].

References-found: 16

