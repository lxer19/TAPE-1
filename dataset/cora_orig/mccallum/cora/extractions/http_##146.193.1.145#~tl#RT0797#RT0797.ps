URL: http://146.193.1.145/~tl/RT0797/RT0797.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00320.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: flba,tl,jdag@inesc.pt  
Title: On-Line Step Size Adaptation  
Author: Lus B. Almeida, Thibault Langlois and Jose D. Amaral INESC/IST, , Rua Alves Redol 
Keyword: Category: Algorithms and Architectures Sub-category: online learning algorithms  
Date: 16, 1997  
Address: Lisbon, Portugal  
Affiliation: 1000  
Note: INESC RT07/97  July  
Abstract: Technical Report Abstract Gradient-based methods are often used for optimization. They form the basis of several neural network training algorithms, including backpropagation. They are known to be slow, however. Several techniques exist for the acceleration of gradient-based optimization, but very few of them are applicable to stochastic (or real-time) optimization. This paper proposes a new step size adaptation technique, designed specifically for accelerating stochastic gradient optimization (and therefore also the real-time training of neural networks). The theoretical basis of the technique is discussed, and an experimental evaluation of the technique's performance is reported.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lus B. Almeida. </author> <title> Multilayered perceptron. </title> <editor> In E. Fiesler and R. Beale, editors, </editor> <booktitle> Handbook of Neural Computation. </booktitle> <publisher> Oxford University Press and IOP Publishing, </publisher> <year> 1996. </year>
Reference-contexts: The weights are updated according to w ij = w ij ij @w ij Momentum and error control can be combined with this technique, yielding a very efficient and robust batch-mode training method <ref> [1] </ref>. This technique is not readily adaptable to stochastic-mode training, because of the need to know the exact value of the gradient. Use of the gradient estimates 2 available in stochastic mode (which we shall designate as sign-based stochas-tic adaptation) can easily lead to erroneous behavior.
Reference: [2] <author> R. Battiti. </author> <title> First- and second- order methods for learning: between steepest descent and newton's methods. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference: [3] <author> S. Becker and Y. Le Cun. </author> <title> Improving the convergence of back-propagation learning with second order methods. </title> <type> Technical Report CRG-TR-88-5, </type> <institution> University of Toronto, </institution> <year> 1986. </year>
Reference: [4] <author> Christian Darken and John Moody. </author> <title> Learning rate schedules for faster stochastic gradient search. </title> <booktitle> In Neural Networks for Signal Processing 2 | Proceedings of the 1992 IEEE Workshop, </booktitle> <address> Piscataway, NJ, 1992. </address> <publisher> IEEE Press. </publisher>
Reference: [5] <author> Christian Darken and John Moody. </author> <title> Towards faster stochastic gradient search. </title> <editor> In Moody, Hanson, and Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <address> Palo Alto, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [6] <author> S. Fahlman. </author> <title> Faster-learning variations on back-propagation: An empirical study. In Connectionist Models Summer School, </title> <institution> Carnegie Mellon, </institution> <year> 1988. </year>
Reference: [7] <author> Todd K. Leen and Genevieve B. Orr. </author> <title> Using curvature information for fast stochastic search. </title> <editor> In M. I. Jordan M.C. Mozer and T. Petsche, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> 9. </volume> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: In other applications of optimization, one sometimes has access only to a noisy estimate of the gradient, making the use of deterministic gradient impossible. Some proposals of stochastic step size adaptation procedures have appeared in the literature [4][5], <ref> [7] </ref>. However, none of them seems to be simple and general enough for widespread use. In this paper we propose a new, simple step size adaptation technique for stochastic gradient optimization, similar in spirit to the deterministic adaptive step sizes technique of [10] (see also [11]).
Reference: [8] <author> M. F. Moller. </author> <title> A scaled conjugate gradient algorithm for fast supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 525-533, </pages> <year> 1993. </year>
Reference-contexts: In the neural networks area, it forms the basis of the widely used backpropagation algorithm [9]. Plain gradient based optimization is known to be slow, however. Several acceleration techniques have appeared over the years: momentum [9], adaptive step sizes [10], second order methods [3][6][2], conjugate gradients <ref> [8] </ref> etc. Most of these techniques (with the notable exception of momentum) were developed only for deterministic optimization (which corresponds to 1 batch-mode training in neural networks). However, both in the training of neural networks and in other optimization problems it is often convenient to use stochastic (on-line) optimization.
Reference: [9] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <booktitle> In PDP : Explorations in the Microstructures of Cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362, </pages> <address> NewYork:IRE, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Gradient descent/ascent is often used for optimization. In the neural networks area, it forms the basis of the widely used backpropagation algorithm <ref> [9] </ref>. Plain gradient based optimization is known to be slow, however. Several acceleration techniques have appeared over the years: momentum [9], adaptive step sizes [10], second order methods [3][6][2], conjugate gradients [8] etc. <p> 1 Introduction Gradient descent/ascent is often used for optimization. In the neural networks area, it forms the basis of the widely used backpropagation algorithm <ref> [9] </ref>. Plain gradient based optimization is known to be slow, however. Several acceleration techniques have appeared over the years: momentum [9], adaptive step sizes [10], second order methods [3][6][2], conjugate gradients [8] etc. Most of these techniques (with the notable exception of momentum) were developed only for deterministic optimization (which corresponds to 1 batch-mode training in neural networks).
Reference: [10] <author> Fernando M. Silva and Luis B. Almeida. </author> <title> Speeding up backpropagation. </title> <editor> In R. Eckmiller, editor, </editor> <booktitle> Advanced Neural Computers, </booktitle> <pages> pages 151-160. </pages> <address> North Hol-land, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Gradient descent/ascent is often used for optimization. In the neural networks area, it forms the basis of the widely used backpropagation algorithm [9]. Plain gradient based optimization is known to be slow, however. Several acceleration techniques have appeared over the years: momentum [9], adaptive step sizes <ref> [10] </ref>, second order methods [3][6][2], conjugate gradients [8] etc. Most of these techniques (with the notable exception of momentum) were developed only for deterministic optimization (which corresponds to 1 batch-mode training in neural networks). <p> However, none of them seems to be simple and general enough for widespread use. In this paper we propose a new, simple step size adaptation technique for stochastic gradient optimization, similar in spirit to the deterministic adaptive step sizes technique of <ref> [10] </ref> (see also [11]). This paper is organized as follows. Section 2 briefly reviews the deterministic adaptation technique. Section 3 presents the new stochastic technique. Section 4 presents experimental results, and Section 5 concludes. In this paper we will use the neural networks nomenclature.
Reference: [11] <author> T. Tollenaere. Supersab: </author> <title> Fast adaptive back propagation with good scaling properties. </title> <booktitle> Neural networks, </booktitle> <volume> 3 </volume> <pages> 561-573, </pages> <year> 1990. </year> <month> 9 </month>
Reference-contexts: However, none of them seems to be simple and general enough for widespread use. In this paper we propose a new, simple step size adaptation technique for stochastic gradient optimization, similar in spirit to the deterministic adaptive step sizes technique of [10] (see also <ref> [11] </ref>). This paper is organized as follows. Section 2 briefly reviews the deterministic adaptation technique. Section 3 presents the new stochastic technique. Section 4 presents experimental results, and Section 5 concludes. In this paper we will use the neural networks nomenclature.
References-found: 11

