URL: ftp://ftp.cs.brown.edu/pub/techreports/97/cs97-14.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-97-14.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [ADA93] <author> Paolo Atzeni and Valeria De Antonellis. </author> <title> Relational Database Theory. </title> <publisher> The Benjamin/Cummins Publishing Company, </publisher> <address> Redwood City, California, </address> <year> 1993. </year>
Reference-contexts: Dependencies among domain features play an important role in decomposition theory for relational databases, and provide computational leverage in time and space <ref> [ADA93] </ref> [Ull88] [KS91]. Instead of keeping a large global relational schema covering all attributes, often small local relational schemas are employed, each covering only a subset of the attributes.
Reference: [AHT90] <editor> James F. Allen, James Hendler, and Austin Tate, editors. </editor> <booktitle> Readings in Planning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, California, </address> <year> 1990. </year>
Reference-contexts: Introduction 1.1 Planning under Uncertainty In classical planning, propositional STRIPS-like operators [FN71a] <ref> [AHT90] </ref> are employed to attain a goal where some state variables (propositions) have specified values. A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators [Cha87] [MR91]. <p> The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] <ref> [AHT90] </ref> [PW92] [BW93] [Wel94]. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [All83] <author> James Allen. </author> <title> Maintaining knowledge about temporal intervals. </title> <journal> Communications of the ACM, </journal> <volume> 26 </volume> <pages> 832-843, </pages> <year> 1983. </year>
Reference-contexts: In both cases, procedure Localized-Reasoning provides substantial computational savings. 5.7 Related Work The related work on the representation of time and temporal reasoning is extensive. There are several formalisms that can represent the temporal relationship among a set of points or intervals in time <ref> [All83] </ref> [Vil82] [MB83] [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus <p> set of points or intervals in time <ref> [All83] </ref> [Vil82] [MB83] [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus [Eva90] [Mea92]. The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort.
Reference: [AWFA87] <author> S. Andreassen, M. Woldbye, B. Falck, and S. Andersen. Munin: </author> <title> A causal probabilistic network for interpretion of electromygraphic findings. </title> <booktitle> In Proceedings AAAI-87, </booktitle> <pages> pages 121-124. </pages> <publisher> AAAI, </publisher> <year> 1987. </year>
Reference-contexts: Dependencies among random variables provide computational leverage in computing marginal probabilities and conditional probabilities. Rather than globally enumeratng possible combination of variable values, local computation and propagation exploiting dependency and result in much better computational efficiency [JLO90] [LS88] [SDDF90] <ref> [AWFA87] </ref>. 8.5 Decomposition Theory in Relational Databases The decomposition theory for relational databases provides an analogy to the decomposition approach for planning under uncertainty in this thesis.
Reference: [BCL + 94] <author> Jerry Burch, Edmund M. Clarke, David Long, Kenneth L. McMil-lan, and David L. Dill. </author> <title> Symbolic model checking for sequential circuit verification. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> 13(4) </volume> <pages> 401-424, </pages> <year> 1994. </year>
Reference-contexts: For model checking og complicated communication protocols, aggregation techniques have been investigated to derive equivalent finite-state machines on the fly to minimize computation time of validation or verification [LY92] [LY94] <ref> [BCL + 94] </ref> [DG97]. Abstraction is a special way of aggregation, in which two states are indistinguishable if they share the same configuration governing the values of a selected subset of variables.
Reference: [BCn89] <author> D. P. Bertsekas and D. A. Casta~non. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34(6) </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] <ref> [BCn89] </ref>. In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] [BDH95] [LD95b] [LD96]. <p> approximation approach is that often all or most of the state variables turn out to be relevant; in this situation, the approximation approach does not help. 8.3 Aggregation and Abstraction When computing an optimal policy for a large Markov decision process, it is useful to aggregate indistinguishable states dynamically [BDG95] <ref> [BCn89] </ref> [Sch84] [DG97] or statically using different notions of indistinguishability to reduce redundancy in computation. The decomposition technique in Chapter 6 employs structural analysis to identify a static aggregation of states before solving the underlying Markov decision process.
Reference: [BD94] <author> Craig Boutilier and Richard Dearden. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94. AAAI, </booktitle> <year> 1994. </year> <month> 166 </month>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty <ref> [BD94] </ref> [BDG95] [DKKN93b] [DKKN93a] [DKKN95] [DHW94] [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances <ref> [BD94] </ref> [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. <p> In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations <ref> [BD94] </ref> [BDG95] [BDH95] [LD95b] [LD96]. Compact representations often exhibit locality and dependency in action dynamics, which indicates regularity and redundancy in the underlying discrete dynamical systems. In many situations, explicit construction of the entire dynamical systems underlying such kinds of planning instances is not necessary. <p> in this thesis can be easily extended to other representation schemas, rather than strictly depending on this specific representation scheme. 2.3.1 Compactly Representing State-transition Relations Instead of explicitly enumerating the state-transition relation D of a discrete dynamical system D, D can be compactly represented as sets of causal rules [DB88a] <ref> [BD94] </ref> [LD94] [KHW94] [LD96]. Each action is associated with a set of causal rules, while each causal rule is basically a state-space operator that partially encodes how the associated action can result in state transitions. <p> It is simple and efficienct, but usually provides no formal quality guarantee of the solutions. 8.2 Approximate Planning Using Abstract Poli cies Instead of determining an optimal policy for a huge Markov decision process M, the approximation approach <ref> [BD94] </ref> pursues policies of suboptimal quality by approximating M with an abstraction of M. * In the approximation approach, a procedure is devised to determine a set of relevant variables by inspecting M.
Reference: [BDG95] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploit--ing structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14. </booktitle> <address> IJ-CAII, </address> <year> 1995. </year>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] <ref> [BDG95] </ref> [DKKN93b] [DKKN93a] [DKKN95] [DHW94] [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency <ref> [BDG95] </ref> [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] [BDH95] [LD95b] [LD96]. <p> the fly in improving computational efficiency <ref> [BDG95] </ref> [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] [BDH95] [LD95b] [LD96]. Compact representations often exhibit locality and dependency in action dynamics, which indicates regularity and redundancy in the underlying discrete dynamical systems. In many situations, explicit construction of the entire dynamical systems underlying such kinds of planning instances is not necessary. <p> Similarly, policies for large factored state spaces can often be efficiently encoded using decision trees that branch on state variables. <ref> [BDG95] </ref>. In the following, we describe a compact representation employed in this thesis for encoding state-transition relations, state-transition probabilities, and action costs. This is primarily for the ease of concretely explaining the algorithms and problem instances presented in this thesis. <p> the approximation approach is that often all or most of the state variables turn out to be relevant; in this situation, the approximation approach does not help. 8.3 Aggregation and Abstraction When computing an optimal policy for a large Markov decision process, it is useful to aggregate indistinguishable states dynamically <ref> [BDG95] </ref> [BCn89] [Sch84] [DG97] or statically using different notions of indistinguishability to reduce redundancy in computation. The decomposition technique in Chapter 6 employs structural analysis to identify a static aggregation of states before solving the underlying Markov decision process.
Reference: [BDH95] <author> Craig Boutilier, Thomas Dean, and Steve Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1995. </year>
Reference-contexts: In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] <ref> [BDH95] </ref> [LD95b] [LD96]. Compact representations often exhibit locality and dependency in action dynamics, which indicates regularity and redundancy in the underlying discrete dynamical systems. In many situations, explicit construction of the entire dynamical systems underlying such kinds of planning instances is not necessary.
Reference: [BDKL92] <author> Kenneth Basye, Thomas Dean, Jak Kirman, and Moises Lejter. </author> <title> A decision-theoretic approach to planning, </title> <journal> perception, and control. IEEE Expert, </journal> <volume> 7(4) </volume> <pages> 58-65, </pages> <year> 1992. </year>
Reference: [Bel61] <author> Richard Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1961. </year>
Reference-contexts: [Put94] [Der70] over Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community <ref> [Bel61] </ref> [How60] [Ber87] [Put94]. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes. Dynamic Programming: The value iteration method [Bel61] and the policy iteration method [How60] are two well-known methods used to generate optimal policies for Markov <p> Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community <ref> [Bel61] </ref> [How60] [Ber87] [Put94]. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes. Dynamic Programming: The value iteration method [Bel61] and the policy iteration method [How60] are two well-known methods used to generate optimal policies for Markov decision processes, both employing the dynamic-programming principle.
Reference: [Ber87] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, N.J., </address> <year> 1987. </year>
Reference-contexts: over Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community [Bel61] [How60] <ref> [Ber87] </ref> [Put94]. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes.
Reference: [BF88] <author> John S. Breese and Michael R. Fehling. </author> <title> Decision-theoretic control of problem solving: </title> <booktitle> Principles and architecture. In Proceedings of the 1988 Workshop on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 30-37, </pages> <year> 1988. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. [Dea88] [Dea87] <ref> [BF88] </ref> [DW91] [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems is to
Reference: [Bol85] <author> Bela Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1985. </year>
Reference: [BS78] <author> Dimitri P. Bertsekas and Steven E. Shreve. </author> <title> Stochastic Optimal Control: The Discrete Time Case, </title> <booktitle> volume 139 of Mathematics in Science and Engineering. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: often available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] <ref> [BS78] </ref> [Put94] as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> Appendix C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] <ref> [BS78] </ref> [DW91] [Put94], and introduce definitions and notation used throughout the thesis. <p> achieved when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] <ref> [BS78] </ref> [DW91] [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S.
Reference: [BW93] <author> A. Barrett and D. Weld. </author> <title> Partial order planning: Evaluating possible efficiency gains. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 71-112, </pages> <year> 1993. </year>
Reference-contexts: The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] [PW92] <ref> [BW93] </ref> [Wel94]. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [Byl94] <author> Tom Bylander. </author> <title> The computational complexity of propositional STRIPS planning. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 161-204, </pages> <year> 1994. </year>
Reference-contexts: The controller also needs to generate such a linear plan as evidence to explain how the linear plan together with certain choices of the associated state-space operators of the primitive tasks can end in the desired or undesirable situation. 3.1.5 Computational Complexity Classical Planning involving propositional STRIPS-like operators is PSPACE-hard <ref> [Byl94] </ref> [Byl96]. The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete [DB88a] [NB94] [LD94] [LD96]. <p> This follows from the facts that classical planning involving propositional STRIPS-like operators is PSPACE-hard <ref> [Byl94] </ref> and that we can transform any instance of the classical STRIPS planning problem involving propositional STRIPS-like operators into an instance of the Markov decision problem in the following 36 way: * propositions in STRIPSplanning are viewed as boolean state variables and implicitly specify a state space, * the truth values
Reference: [Byl96] <author> Tom Bylander. </author> <title> A probabilistic analysis of STRIPS planning. </title> <journal> Artificial Intelligence, </journal> <year> 1996. </year> <month> 167 </month>
Reference-contexts: controller also needs to generate such a linear plan as evidence to explain how the linear plan together with certain choices of the associated state-space operators of the primitive tasks can end in the desired or undesirable situation. 3.1.5 Computational Complexity Classical Planning involving propositional STRIPS-like operators is PSPACE-hard [Byl94] <ref> [Byl96] </ref>. The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete [DB88a] [NB94] [LD94] [LD96].
Reference: [Cha87] <author> David Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelli--gence, </journal> <volume> 32 </volume> <pages> 333-377, </pages> <year> 1987. </year>
Reference-contexts: Introduction 1.1 Planning under Uncertainty In classical planning, propositional STRIPS-like operators [FN71a] [AHT90] are employed to attain a goal where some state variables (propositions) have specified values. A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators <ref> [Cha87] </ref> [MR91]. The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] [PW92] [BW93] [Wel94]. Each planning instance involves a finite set of state variables. <p> A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators <ref> [Cha87] </ref> [MR91]. The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] [PW92] [BW93] [Wel94]. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [Chi94] <author> L. Chittaro. </author> <title> Skeptical and credulous event calculi for supporting modal queries. </title> <booktitle> In Proceedings ECAI-94, </booktitle> <year> 1994. </year>
Reference-contexts: There are several formalisms that can represent the temporal relationship among a set of points or intervals in time [All83] [Vil82] [MB83] [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] <ref> [Chi94] </ref>. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus [Eva90] [Mea92].
Reference: [Chr90] <author> J. Christensen. </author> <title> A hierarchical planner that generates its own abstraction hierarchies. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 1004-1009. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] <ref> [Chr90] </ref> [Kno91a] [YT90] in reducing the overall search effort. The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM [Lan88] in event-based planning.
Reference: [Chv80] <author> Vasek Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] <ref> [Chv80] </ref> like the well known simplex method. <p> This iterative method is based on a reduction to the methods of Kushner and Chen [KC74a] that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle [DW60]. The details of the material presented in this section depend on some understanding of linear programming <ref> [Chv80] </ref> and methods for decomposing and solving large systems [Las70]. We sketch the iterative method in the following, and refer the readers to appendix B and Appendix C for more details. 1.
Reference: [CKL94] <author> Anthony R. Cassandra, Leslie Kaelbling, and Michael Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings AAAI-94, </booktitle> <pages> pages 1023-1028. </pages> <publisher> AAAI, </publisher> <year> 1994. </year>
Reference-contexts: For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem [Lat90a] [Jak94] [GD95] <ref> [CKL94] </ref> [MA95] [DKKN95], is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M = hS; A; Pr;
Reference: [CLR91] <author> T. H. Corman, C. E. Leiserson, and R. L. Rivest. </author> <title> Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Temporal projection regarding one-rule events, totally unordered, with a polynomial-size state space is solvable in polynomial time. Proof. For this special case, the PRJ1 task of temporal projection can be reduced to a graph reachability problem, which is solvable in polynomial time <ref> [CLR91] </ref>. We first construct a directed graph G by mapping (1) each state to a distinct vertex in G and (2) each one-rule event that triggers a state transition from a state u to a state v to an arc (u; v) in G. <p> Finally, we examine the event sequence to determine whether it is consistent with the ordering constraints. Since the bipartite match problem can be solved in polynomial 135 time [PS82] <ref> [CLR91] </ref>, this reduction together with Theorem 5.1 prove that this special case of temporal projection is solvable in polynomial time. 2 Theorem 5.7. <p> Therefore G I = (V; E) is a directed acyclic graph, and the single-source shortest paths problem and thus the graph reachability problem can be solved in O (jV j + jEj) time <ref> [CLR91] </ref>. If we adopt the trivial partition, we have jEj chains, and the length l i equals one for each chain. This gives us jS P j fi 1im (1 + l i )= 2 jPj+jEj vertices, and at most 2 2 (jPj+jEj) arcs.
Reference: [Coo90] <author> Gregory F. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405, </pages> <year> 1990. </year>
Reference: [CW90] <author> Peter E. Caines and S. Wang. COCOLOG: </author> <title> A conditional observer and controller logic for finite machines. </title> <booktitle> In Proceedings of the 29th IEEE Conference on Decision and Control, Hawaii, </booktitle> <year> 1990. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [Kor85] and hierarchies of state-space operators [Sac74a] [Kno91a]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] <ref> [CW90] </ref>. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces [Kae93].
Reference: [DB88a] <author> Thomas Dean and Mark Boddy. </author> <title> Reasoning about partially ordered events. </title> <journal> Artificial Intelligence, </journal> <volume> 36(3) </volume> <pages> 375-399, </pages> <year> 1988. </year>
Reference-contexts: results in this thesis can be easily extended to other representation schemas, rather than strictly depending on this specific representation scheme. 2.3.1 Compactly Representing State-transition Relations Instead of explicitly enumerating the state-transition relation D of a discrete dynamical system D, D can be compactly represented as sets of causal rules <ref> [DB88a] </ref> [BD94] [LD94] [KHW94] [LD96]. Each action is associated with a set of causal rules, while each causal rule is basically a state-space operator that partially encodes how the associated action can result in state transitions. <p> The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete <ref> [DB88a] </ref> [NB94] [LD94] [LD96]. The temporal projection problem can be viewed as a special kind of planning problem where only linear plans of a fixed length (i.e., the number of primitive 26 tasks in the given task network) are considered.
Reference: [DB88b] <author> Thomas Dean and Mark Boddy. </author> <title> Reasoning about partially ordered events. </title> <journal> Artificial Intelligence, </journal> <volume> 36(3) </volume> <pages> 375-399, </pages> <year> 1988. </year>
Reference-contexts: If we consider undesirable states as target states and uniformly associate every undesirable states with a large positive reward, then the task of temporal projection <ref> [DB88b] </ref> [NB94] [LD94] [LD96] is to determine an optimal linear plan that returns a maximum final reward for a given multi-level task network. 3.1.1 Multi-Level Task Networks A multi-level task network specifies a hierarchy of tasks together with the temporal constraints regarding the order of performing these tasks. <p> an example for illustrating algorithmic details, 1 The proofs for all theorems stated in this thesis are provided in Appendix A at the end of this document. 2 The PRJ1 task is considered by Lin and Dean in [LD93] and the PRJ2 task is considered by Dean and Boddy in <ref> [DB88b] </ref> and Nebel and Backstrom in [NB92]. 56 which provides the following problem instance for temporal projection. * P = fX 1 ; X 2 ; X 3 ; X 4 ; X 5 ; X 6 ; X 7 ; X 8 g.
Reference: [D'E63] <author> F. D'Epenoux. </author> <title> Sur un probleme de production et de stockage dans l'aleatoire. </title> <journal> Management Science, </journal> <volume> 10 </volume> <pages> 98-108, </pages> <year> 1963. </year> <month> 168 </month>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] <ref> [D'E63] </ref> [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs <ref> [D'E63] </ref> [Der70] and decomposing large systems gen-erally [DW60] [Las70] and Markov decision processes specifically [KC74a]. <p> B.2 The DW Decomposition and Markov De cision Processes It is well known that we can solve Markov decision processes as linear programs <ref> [D'E63] </ref> [Der70] [KK71a] [Put94] using standard techniques like the simplex method. Kushner and Chen [KC74a] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> Assuming that the target can be reached with probability 1 from any other state, we can reformulate M = (S; A; p; c) as the following linear program <ref> [D'E63] </ref> [KK71a]: P i;a Cost (i; a)E i;a P P E i;a 0; 8i 2 S; 8a 2 A &gt; &gt; &gt; = : : : (5); where E i;a denote the average number of time that action a is taken in state i, and ~ i is the probability that <p> Similarly, we have the following linear program for the criterion of expected discounted cumulative cost <ref> [D'E63] </ref> [KK71a]. min z = i;a Cost (i; a)E i;a P P E i;a 0 8i 2 S; 8a 2 A &gt; &gt; &gt; = : : : (6); where E i;a denote the discounted average number of time that action a is taken in state i, and ~ i is
Reference: [Dea84] <author> Thomas Dean. </author> <title> Managing time maps. </title> <booktitle> In Proceedings of the Canadian Society for Computational Studies of Intelligence. CSCSI, </booktitle> <year> 1984. </year>
Reference-contexts: In both cases, procedure Localized-Reasoning provides substantial computational savings. 5.7 Related Work The related work on the representation of time and temporal reasoning is extensive. There are several formalisms that can represent the temporal relationship among a set of points or intervals in time [All83] [Vil82] [MB83] <ref> [Dea84] </ref>. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus [Eva90] [Mea92].
Reference: [Dea87] <author> Thomas Dean. </author> <title> Planning, execution, and control. </title> <booktitle> In Proceedings of the DARPA Knowledge-Based Planning Workshop, pages 29-1-29-10. DARPA, </booktitle> <year> 1987. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. [Dea88] <ref> [Dea87] </ref> [BF88] [DW91] [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems is
Reference: [Dea88] <author> Thomas Dean. </author> <title> Planning paradigms. </title> <booktitle> In Proceedings of the DARPA Knowledge-Based Planning Workshop (also appeared in the Proceedings of the Third Annual Workshop of Israel Association for Artificial Intelligence, and in the AI Magazine, Fall 1988). DARPA, </booktitle> <year> 1988. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. <ref> [Dea88] </ref> [Dea87] [BF88] [DW91] [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems
Reference: [Der70] <author> Cyrus Derman. </author> <title> Finite State Markovian Decision Processes. </title> <publisher> Cam-bridge University Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: The Markov Decision Problem: The Markov decision problem is a control problem [Put94] <ref> [Der70] </ref> over Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community [Bel61] [How60] <p> Linear Programming: The instances of Markov decision problems can also be cast as linear programs <ref> [Der70] </ref> [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. <p> Goals specify target states as absorbing states and the other states are transient states. Once reaching a target state, a process stays in that state forever without cost <ref> [Der70] </ref> [KK71b] [KC74b]. At times the underlying state space can be symbolically decomposed into regions that have an acyclic interconnection. We refer to these regions as coherent fragments. A Markov decision process will never end in a coherent fragment again after leaving the fragment. <p> The abstract policy has the interpretation of providing a global perspective and indicating for each region the best local policy to use. Generally, it is best to set fl very close to one or use an alternative performance criterion such as average expected cost per step <ref> [Der70] </ref>. The following is an algorithm to construct a global policy using the abstract decision process (P; F ; ; p 0 ; c 0 ). 110 from i to j. 111 1. Set and compute R!S for R;S 2 P . 2. <p> Standard techniques like the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic convergence rate respectively [Put94]. * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space <ref> [Der70] </ref> [Put94]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where (i) both c and k are constants and (ii) n is the size of the state space. k is 3 and 2 respectively when applying the policy iteration <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs [D'E63] <ref> [Der70] </ref> and decomposing large systems gen-erally [DW60] [Las70] and Markov decision processes specifically [KC74a]. <p> B.2 The DW Decomposition and Markov De cision Processes It is well known that we can solve Markov decision processes as linear programs [D'E63] <ref> [Der70] </ref> [KK71a] [Put94] using standard techniques like the simplex method. Kushner and Chen [KC74a] investigate the use of the DW decomposition in solving Markov decision processes as linear programs.
Reference: [DG97] <author> Thomas Dean and Robert Givan. </author> <title> Model minimization in Markov decision processes. </title> <note> In submitted to AAAI-97. AAAI, </note> <year> 1997. </year>
Reference-contexts: is that often all or most of the state variables turn out to be relevant; in this situation, the approximation approach does not help. 8.3 Aggregation and Abstraction When computing an optimal policy for a large Markov decision process, it is useful to aggregate indistinguishable states dynamically [BDG95] [BCn89] [Sch84] <ref> [DG97] </ref> or statically using different notions of indistinguishability to reduce redundancy in computation. The decomposition technique in Chapter 6 employs structural analysis to identify a static aggregation of states before solving the underlying Markov decision process. <p> For model checking og complicated communication protocols, aggregation techniques have been investigated to derive equivalent finite-state machines on the fly to minimize computation time of validation or verification [LY92] [LY94] [BCL + 94] <ref> [DG97] </ref>. Abstraction is a special way of aggregation, in which two states are indistinguishable if they share the same configuration governing the values of a selected subset of variables.
Reference: [DHW94] <author> Denise Draper, Steve Hanks, and Daniel Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> In Second International Conference on AI Planning Systems, </booktitle> <year> 1994. </year>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] [BDG95] [DKKN93b] [DKKN93a] [DKKN95] <ref> [DHW94] </ref> [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework.
Reference: [DK89] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: A factored state-space representation uses state variables to represent different aspects of the overall state of the system. 1 Compact encodings for stochastic processes can be achieved for many applications using factored state-space representations, where the size of the model is usually logarithmic in the size of the state space <ref> [DK89] </ref>. Similarly, policies for large factored state spaces can often be efficiently encoded using decision trees that branch on state variables. [BDG95]. In the following, we describe a compact representation employed in this thesis for encoding state-transition relations, state-transition probabilities, and action costs.
Reference: [DKKN93a] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in AI, </booktitle> <pages> pages 309-316, </pages> <year> 1993. </year>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] [BDG95] [DKKN93b] <ref> [DKKN93a] </ref> [DKKN95] [DHW94] [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> However, the sizes of the underlying state spaces usually grow exponentially in the sizes of the problem instances, which makes the naive approach infeasible. To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] <ref> [DKKN93a] </ref> [DKKN95] approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. <p> In this chapter, we provide an overview of these research areas and briefly describe the underlying work. 8.1 Real-time Planning in Stochastic Domains For time-critical planning applications involving large Markov decision process [DKKN93b] <ref> [DKKN93a] </ref> [DKKN95], often only a small Markov decision process M 0 is considered at a time as a local envelope. * The Markov decision process M 0 composed of (1) a small portion of the original state space together with the associated transition probabilities and action costs, (2) a super sink
Reference: [DKKN93b] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nichol-son. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <pages> pages 574-579. </pages> <publisher> AAAI, </publisher> <year> 1993. </year> <month> 169 </month>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] [BDG95] <ref> [DKKN93b] </ref> [DKKN93a] [DKKN95] [DHW94] [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> However, the sizes of the underlying state spaces usually grow exponentially in the sizes of the problem instances, which makes the naive approach infeasible. To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] <ref> [DKKN93b] </ref> [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs [D'E63] [Der70] and decomposing large systems gen-erally [DW60] [Las70] and Markov decision processes specifically [KC74a]. The approach described in <ref> [DKKN93b] </ref> represents a special case of the framework presented here, in which the partition consists of singleton sets for all of the states in the envelope and a set for all the states in the complement of the envelope. 7.6 Summary The benefit of decomposition techniques is that we are able <p> In this chapter, we provide an overview of these research areas and briefly describe the underlying work. 8.1 Real-time Planning in Stochastic Domains For time-critical planning applications involving large Markov decision process <ref> [DKKN93b] </ref> [DKKN93a] [DKKN95], often only a small Markov decision process M 0 is considered at a time as a local envelope. * The Markov decision process M 0 composed of (1) a small portion of the original state space together with the associated transition probabilities and action costs, (2) a super
Reference: [DKKN95] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):35-74, </volume> <year> 1995. </year>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] [BDG95] [DKKN93b] [DKKN93a] <ref> [DKKN95] </ref> [DHW94] [KHW94]. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> However, the sizes of the underlying state spaces usually grow exponentially in the sizes of the problem instances, which makes the naive approach infeasible. To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] [DKKN93a] <ref> [DKKN95] </ref> approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. <p> For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem [Lat90a] [Jak94] [GD95] [CKL94] [MA95] <ref> [DKKN95] </ref>, is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M = hS; A; Pr; Costi forms <p> In this chapter, we provide an overview of these research areas and briefly describe the underlying work. 8.1 Real-time Planning in Stochastic Domains For time-critical planning applications involving large Markov decision process [DKKN93b] [DKKN93a] <ref> [DKKN95] </ref>, often only a small Markov decision process M 0 is considered at a time as a local envelope. * The Markov decision process M 0 composed of (1) a small portion of the original state space together with the associated transition probabilities and action costs, (2) a super sink state
Reference: [DL93] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153, </pages> <year> 1993. </year>
Reference: [DL95] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings IJCAI 14. </booktitle> <address> IJCAII, </address> <year> 1995. </year>
Reference-contexts: For each decomposition technique developed in thesis, we describe a structural parameter characterizing the locality and dependency exploitable in a problem instance and quantify the resulting computational efficiency in terms of this structural parameter [LD94] [LD95b] <ref> [DL95] </ref> [LD95a]. 1.4 Outline of the Thesis Chapter 2 investigates planning and control over discrete dynamical systems, introduces the definitions and notation used throughout this thesis, and describes AI representational schemas for compactly encoding action dynamics. <p> The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan [RW89b] for an survey of work on discrete event systems that concerns the synthesis of supervisory systems. Dean and Lin <ref> [DL95] </ref> describe a family of techniques for decomposing the computations required to solve large Markov decision processes.
Reference: [DW60] <author> George Dantzig and Philip Wolfe. </author> <title> Decomposition principle for dynamic programs. </title> <journal> Operations Research, </journal> <volume> 8(1) </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] <ref> [DW60] </ref> [Chv80] like the well known simplex method. <p> In this paper, we focus on a particular iterative method that resolves these issues. This iterative method is based on a reduction to the methods of Kushner and Chen [KC74a] that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle <ref> [DW60] </ref>. The details of the material presented in this section depend on some understanding of linear programming [Chv80] and methods for decomposing and solving large systems [Las70]. We sketch the iterative method in the following, and refer the readers to appendix B and Appendix C for more details. 1. <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs [D'E63] [Der70] and decomposing large systems gen-erally <ref> [DW60] </ref> [Las70] and Markov decision processes specifically [KC74a].
Reference: [DW91] <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. [Dea88] [Dea87] [BF88] <ref> [DW91] </ref> [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems is to explicitly <p> C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] [BS78] <ref> [DW91] </ref> [Put94], and introduce definitions and notation used throughout the thesis. <p> when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] [BS78] <ref> [DW91] </ref> [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S.
Reference: [ER60] <author> P. Erdos and A. Renyi. </author> <title> On the evolution of random graphs. </title> <journal> Publications of the Mathematical Institute of the Hungarian Academy of Sciences, </journal> <volume> 5 </volume> <pages> 17-61, </pages> <year> 1960. </year>
Reference: [Eva90] <author> C. Evans. </author> <title> The macro-event calculus: Representing temporal granularity. </title> <booktitle> In Proceedings of PRICAI'90, </booktitle> <year> 1990. </year>
Reference-contexts: Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus <ref> [Eva90] </ref> [Mea92]. The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort.
Reference: [FN71a] <author> Richard Fikes and Nils J. Nilsson. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Introduction 1.1 Planning under Uncertainty In classical planning, propositional STRIPS-like operators <ref> [FN71a] </ref> [AHT90] are employed to attain a goal where some state variables (propositions) have specified values. A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators [Cha87] [MR91]. <p> A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators [Cha87] [MR91]. The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end <ref> [FN71a] </ref> [Cha87] [AHT90] [PW92] [BW93] [Wel94]. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [FN71b] <author> Richard Fikes and Nils J. Nilsson. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Propositions representing fluents in STRIPS operators <ref> [FN71b] </ref> correspond to state variables in a factored state-space representation. 10 * An expression ff is true in a state u if and only if for each variable/value pair hX i ; v i i in ff, v i is the value of X i in u. * Notationally, the variable/value
Reference: [GD94] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Solving time-critical decision-making problems with predictable computational demands. </title> <booktitle> In Second International Conference on AI Planning Systems, </booktitle> <year> 1994. </year> <month> 170 </month>
Reference: [GD95] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Anticipating computational demands when solving time-critical decision-making problems. </title> <editor> In K. Goldberg, D. Halperin, J.C. Latombe, and R. Wilson, editors, </editor> <booktitle> The Algorithmic Foundations of Robotics. </booktitle> <editor> A. K. Peters, </editor> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem [Lat90a] [Jak94] <ref> [GD95] </ref> [CKL94] [MA95] [DKKN95], is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M = hS; A;
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractibil-ity: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Proof. Given a directed graph with a set of forbidden pairs of arcs, the forbidden pairs of arcs problem <ref> [GJ79] </ref> determines whether there is a path between two nodes without using both arcs in any forbidden pair of arcs. It is NP-Complete even if all forbidden pairs are disjoint. <p> Temporal projection regarding one-rule events with an arbitrary partial order and a polynomial-size state space is NP-Complete. Proof. Given a directed graph with a set of forbidden pairs of arcs, the forbidden pairs of arcs problem <ref> [GJ79] </ref> determines whether there is a path between two nodes without using both arcs in any forbidden pair of arcs. This problem is NP-Complete even for directed acyclic graphs.
Reference: [GS93] <author> A. Gerevini and L. Schubert. </author> <title> Efficient temporal reasoning through timegraphs. </title> <booktitle> In Proceedings IJCAI-93. IJCAI, </booktitle> <year> 1993. </year>
Reference-contexts: Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] <ref> [GS93] </ref> [KG77] and in the event calculus [Eva90] [Mea92]. The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort.
Reference: [HM84] <author> Ronald A. Howard and James E. Matheson. </author> <title> Influence diagrams. </title> <editor> In Ronald A. Howard and James E. Matheson, editors, </editor> <title> The Principles and Applications of Decision Analysis. Strategic Decisions Group, </title> <address> Menlo Park, CA 94025, </address> <year> 1984. </year>
Reference-contexts: Often complex decision processes can be broken down into elementary decision stages <ref> [HM84] </ref> [Sha86] [TS90] [SW95] [LD95b]. These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements.
Reference: [How60] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: [Der70] over Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community [Bel61] <ref> [How60] </ref> [Ber87] [Put94]. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes. Dynamic Programming: The value iteration method [Bel61] and the policy iteration method [How60] are two well-known methods used to generate optimal policies for Markov decision <p> decision processes have been well studied by the operations research community [Bel61] <ref> [How60] </ref> [Ber87] [Put94]. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes. Dynamic Programming: The value iteration method [Bel61] and the policy iteration method [How60] are two well-known methods used to generate optimal policies for Markov decision processes, both employing the dynamic-programming principle. The value iteration method starts with an arbitrary value function that estimates for each state the expected cumulative costs of plan execution starting from the state.
Reference: [HS94] <author> Peter Haddawy and Meliani Suwandi. </author> <title> Decision-theoretic planning using inheritance abstraction. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <year> 1994. </year>
Reference-contexts: Aggregation and abstraction are employed as tools for constructing compact and equivialent Markov decision processes or finite automata to improve computational efficiency. Different abstraction techniques have been investigated in classic STRIPS planning [Kno91b] [Ten91] [Sac74b] <ref> [HS94] </ref> to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks [LS88] [JLO90] provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis.
Reference: [HU79] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1979. </year>
Reference-contexts: and the probability governing state transitions are often available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata [Paz71] <ref> [HU79] </ref> [LP81] and Markov decision processes [Paz71] [BS78] [Put94] as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> decomposition principle utilized in Chapter 7, while Appendix C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata [Paz71] <ref> [HU79] </ref> [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94], and introduce definitions and notation used throughout the thesis. <p> satisfying G while the goal G is achieved when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata [Paz71] <ref> [HU79] </ref> [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S.
Reference: [Jak94] <author> Kirman Jak. </author> <title> Predicting Real-Time Planner Performance by Domain Characterization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1994. </year>
Reference-contexts: For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem [Lat90a] <ref> [Jak94] </ref> [GD95] [CKL94] [MA95] [DKKN95], is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M = hS;
Reference: [JLO90] <author> F.V. Jensen, S.L. Lauritzen, and K.G. Olesen. </author> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statisticals Quarterly, </journal> <volume> 4 </volume> <pages> 269-282, </pages> <year> 1990. </year> <month> 171 </month>
Reference-contexts: Different abstraction techniques have been investigated in classic STRIPS planning [Kno91b] [Ten91] [Sac74b] [HS94] to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks [LS88] <ref> [JLO90] </ref> provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis. <p> Dependencies among random variables provide computational leverage in computing marginal probabilities and conditional probabilities. Rather than globally enumeratng possible combination of variable values, local computation and propagation exploiting dependency and result in much better computational efficiency <ref> [JLO90] </ref> [LS88] [SDDF90] [AWFA87]. 8.5 Decomposition Theory in Relational Databases The decomposition theory for relational databases provides an analogy to the decomposition approach for planning under uncertainty in this thesis.
Reference: [Kae93] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: A preliminary report. </title> <booktitle> In Proceedings Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] [CW90]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces <ref> [Kae93] </ref>. The hierarchical policy construction method described in Section 7.2 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [Kae93] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces. <p> In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces <ref> [Kae93] </ref>. The hierarchical policy construction method described in Section 7.2 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [Kae93] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces.
Reference: [KC74a] <author> Harold J. Kushner and Ching-Hui Chen. </author> <title> Decomposition of systems governed by markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-19(5):501-507, </volume> <year> 1974. </year>
Reference-contexts: Although this linear programming approach is not as efficient as the dynamic programming approach [KK71a], it provides valuable insight <ref> [KC74a] </ref> into the relationship between the computational efficiency and structure in problem instances. 18 Chapter 3 Planning and Control: Applications and Computational Challenge In this chapter, we describe the applications of planning problems generically formulated as control problems over discrete dynamical systems in the form of finite automata or Markov decision <p> In this paper, we focus on a particular iterative method that resolves these issues. This iterative method is based on a reduction to the methods of Kushner and Chen <ref> [KC74a] </ref> that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle [DW60]. The details of the material presented in this section depend on some understanding of linear programming [Chv80] and methods for decomposing and solving large systems [Las70]. <p> This resulting partition Q induces a star topology that is critical in applying the techniques in <ref> [KC74a] </ref>. U is the coupling region in the new partition 113 Q; removing the states in U separates the state space into isolated regions, each of which corresponds to a K i , 1 i h. <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs [D'E63] [Der70] and decomposing large systems gen-erally [DW60] [Las70] and Markov decision processes specifically <ref> [KC74a] </ref>. <p> The iterative method described above improves the solution quality on each iteration, and converges to an optimal solution in a finite number of iterations. Proof. The iterative method is equivalent to (i) adopting the partition Q, and then (ii) applying the methods of Kushner and Chen <ref> [KC74a] </ref> that solve Markov decision processes as large linear programs using the Dantzig-Wolfe decomposition principle, which have the properties described above. A more detailed description of the algorithm and its correspondence to the methods of Kushner and Chen [KC74a] is provided in appendix B and Appendix C. 2 141 Appendix B <p> partition Q, and then (ii) applying the methods of Kushner and Chen <ref> [KC74a] </ref> that solve Markov decision processes as large linear programs using the Dantzig-Wolfe decomposition principle, which have the properties described above. A more detailed description of the algorithm and its correspondence to the methods of Kushner and Chen [KC74a] is provided in appendix B and Appendix C. 2 141 Appendix B Iterative Approximation and Linear Programming In this chapter, we explore the relationship between the iterative approximation framework described in Section 5 and the use of the Dantzig-Wolfe decomposition principle in solving Markov decision processes as linear programs. <p> This relationship then provides interesting insight into the iterative approximation framework. In the remainder of this paper, we abbreviate the Dantzig-Wolfe decomposition principle as the DW decomposition, and refer to the work of Kunshner and Chen <ref> [KC74a] </ref> as the DW approach for Markov decision processes. <p> B.2 The DW Decomposition and Markov De cision Processes It is well known that we can solve Markov decision processes as linear programs [D'E63] [Der70] [KK71a] [Put94] using standard techniques like the simplex method. Kushner and Chen <ref> [KC74a] </ref> investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> B.2.2 The DW Approach in Solving Markov Decision Processes Kushner and Chen <ref> [KC74a] </ref> investigate the use of the DW decomposition in solving Markov decision processes formulated as the linear programs in (5) and (6). <p> We briefly summarize the use of DW approach in solving Markov decision processes as linear programs in the following and refer the readers to <ref> [KC74a] </ref> for more details. 1. Choose a partition Q = fU; K 1 ; : : : ; K h g such that S divides into isolated pieces K l 's, 1 l h when we remove U from S. 149 2. <p> Therefore the corresponding iterative method also converges to an optimal solution of the corresponding Markov decision process in a finite number iterations. 152 C.2 Overview of the Iterative Method By appropriately interpreting the DW approach in <ref> [KC74a] </ref> under the iterative approximation framework, it yields the following iterative method. 1. Given a Markov decision process M = (S; A; p; c), we choose an arbitrary (but fixed) partition P of the state space S in advance which divides the state space into disjoint regions. 2. <p> The solution quality is improved iteratively, and converge to optimum. In the following, we (i) provide the entire picture of the iterative method, and (ii) indicate its equivalence to the DW approach that uses the DW decomposition to solve Markov decision processes as linear programs <ref> [KC74a] </ref>. In the remainder of this paper, we focus on the criterion of expected cumulative cost to reach target states.
Reference: [KC74b] <author> Harold J. Kushner and Ching-Hui Chen. </author> <title> Decomposition of systems governed by Markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 19(5) </volume> <pages> 501-507, </pages> <year> 1974. </year>
Reference-contexts: Goals specify target states as absorbing states and the other states are transient states. Once reaching a target state, a process stays in that state forever without cost [Der70] [KK71b] <ref> [KC74b] </ref>. At times the underlying state space can be symbolically decomposed into regions that have an acyclic interconnection. We refer to these regions as coherent fragments. A Markov decision process will never end in a coherent fragment again after leaving the fragment.
Reference: [KG77] <author> Kenneth Kahn and G. Anthony Gorry. </author> <title> Mechanizing temporal knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 9 </volume> <pages> 87-108, </pages> <year> 1977. </year>
Reference-contexts: Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] <ref> [KG77] </ref> and in the event calculus [Eva90] [Mea92]. The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort.
Reference: [KHW94] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic planning. </title> <booktitle> In Proceedings AAAI-94. AAAI, </booktitle> <year> 1994. </year>
Reference-contexts: In this setting, a STRIPS-like operator compactly represents a state-transition function over the state space determined by the set of state variables. Recently, many researchers have devoted effort to generalize this classical planning framework to deal with planning under uncertainty [BD94] [BDG95] [DKKN93b] [DKKN93a] [DKKN95] [DHW94] <ref> [KHW94] </ref>. To cope with uncertainty, it is necessary to further extend the notions of STRIPS-like operators as 1 actions, plans as totally or partially ordered actions, and planning as a goal--achieving task under the classical framework. <p> thesis can be easily extended to other representation schemas, rather than strictly depending on this specific representation scheme. 2.3.1 Compactly Representing State-transition Relations Instead of explicitly enumerating the state-transition relation D of a discrete dynamical system D, D can be compactly represented as sets of causal rules [DB88a] [BD94] [LD94] <ref> [KHW94] </ref> [LD96]. Each action is associated with a set of causal rules, while each causal rule is basically a state-space operator that partially encodes how the associated action can result in state transitions. The following is a formal description of causal rules and their usage in representing state-transition relations.
Reference: [KK71a] <author> H. J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of Markov chains. </title> <journal> International Journal on Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] <ref> [KK71a] </ref> [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. Although this linear programming approach is not as efficient as the dynamic programming approach [KK71a], it provides valuable insight [KC74a] into the relationship between the computational efficiency and structure in <p> The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] <ref> [KK71a] </ref> [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. Although this linear programming approach is not as efficient as the dynamic programming approach [KK71a], it provides valuable insight [KC74a] into the relationship between the computational efficiency and structure in problem instances. 18 Chapter 3 Planning and Control: Applications and Computational Challenge In this chapter, we describe the applications of planning problems generically formulated as control problems over discrete dynamical systems in the form of <p> B.2 The DW Decomposition and Markov De cision Processes It is well known that we can solve Markov decision processes as linear programs [D'E63] [Der70] <ref> [KK71a] </ref> [Put94] using standard techniques like the simplex method. Kushner and Chen [KC74a] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> Assuming that the target can be reached with probability 1 from any other state, we can reformulate M = (S; A; p; c) as the following linear program [D'E63] <ref> [KK71a] </ref>: P i;a Cost (i; a)E i;a P P E i;a 0; 8i 2 S; 8a 2 A &gt; &gt; &gt; = : : : (5); where E i;a denote the average number of time that action a is taken in state i, and ~ i is the probability that state <p> Similarly, we have the following linear program for the criterion of expected discounted cumulative cost [D'E63] <ref> [KK71a] </ref>. min z = i;a Cost (i; a)E i;a P P E i;a 0 8i 2 S; 8a 2 A &gt; &gt; &gt; = : : : (6); where E i;a denote the discounted average number of time that action a is taken in state i, and ~ i is the
Reference: [KK71b] <author> H. J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of markov chains. </title> <journal> International Journal on Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference-contexts: Goals specify target states as absorbing states and the other states are transient states. Once reaching a target state, a process stays in that state forever without cost [Der70] <ref> [KK71b] </ref> [KC74b]. At times the underlying state space can be symbolically decomposed into regions that have an acyclic interconnection. We refer to these regions as coherent fragments. A Markov decision process will never end in a coherent fragment again after leaving the fragment.
Reference: [KL94] <author> Lydia Kavraki and Jean-Claude Latombe. </author> <title> Randomized preprocessing of configuration space for fast path planning. </title> <booktitle> In IEEE International Conference on Robotics & Automation, </booktitle> <pages> pages 2138-2145, </pages> <address> San Diego, </address> <year> 1994. </year>
Reference: [Kno91a] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 686-691. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference-contexts: The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] <ref> [Kno91a] </ref> [YT90] in reducing the overall search effort. The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM [Lan88] in event-based planning. <p> In the area planning and search assuming deterministic action models, there is the work on macro operators [Kor85] and hierarchies of state-space operators [Sac74a] <ref> [Kno91a] </ref>. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] [CW90]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces [Kae93].
Reference: [Kno91b] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 686-691. </pages> <publisher> AAAI, </publisher> <year> 1991. </year> <month> 172 </month>
Reference-contexts: Aggregation and abstraction are employed as tools for constructing compact and equivialent Markov decision processes or finite automata to improve computational efficiency. Different abstraction techniques have been investigated in classic STRIPS planning <ref> [Kno91b] </ref> [Ten91] [Sac74b] [HS94] to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks [LS88] [JLO90] provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis.
Reference: [Kor85] <author> Richard Korf. Macro-operators: </author> <title> A weak method for learning. </title> <journal> Arti--ficial Intelligence, </journal> <volume> 26 </volume> <pages> 35-77, </pages> <year> 1985. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators <ref> [Kor85] </ref> and hierarchies of state-space operators [Sac74a] [Kno91a]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] [CW90].
Reference: [Kor87] <author> Richard Korf. </author> <title> Planning as search: A quantitative approach. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 65-88, </pages> <year> 1987. </year>
Reference-contexts: The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search <ref> [Kor87] </ref> and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort. The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM [Lan88] in event-based planning.
Reference: [KS86] <author> Robert Kowalski and M. J. Sergot. </author> <title> A logic-based calculus of events. </title> <journal> New Generation Computing, </journal> <volume> 4 </volume> <pages> 67-95, </pages> <year> 1986. </year>
Reference-contexts: There are several formalisms that can represent the temporal relationship among a set of points or intervals in time [All83] [Vil82] [MB83] [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in <ref> [KS86] </ref> [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus [Eva90] [Mea92].
Reference: [KS91] <author> H. F. Korth and A Silberschatz. </author> <title> Database System Concepts. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Dependencies among domain features play an important role in decomposition theory for relational databases, and provide computational leverage in time and space [ADA93] [Ull88] <ref> [KS91] </ref>. Instead of keeping a large global relational schema covering all attributes, often small local relational schemas are employed, each covering only a subset of the attributes. This allows database managers to keep small databases over the local relational schemas, rather than a large database over the global schema.
Reference: [KS94] <author> S. Kirkpatrick and B. Selman. </author> <title> Critical behavior in the satisfiability of random boolean formuae. </title> <journal> Science, </journal> <volume> 264 </volume> <pages> 1297-1301, </pages> <month> May </month> <year> 1994. </year>
Reference: [Lan88] <author> Amy L. Lansky. </author> <title> Localized event-based reasoning for multiagent domains. </title> <journal> Computational Intelligence, </journal> <volume> 4(4), </volume> <year> 1988. </year>
Reference-contexts: The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM <ref> [Lan88] </ref> in event-based planning. In GEM, locality is similarly modeled by sets of interrelated events delimited by temporal logic constraints. 5.8 Summary In this chapter, we formally describe a dynamical system modeled by a set of conditions, a set of events, and a set of ordering constraints on the events. <p> The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [LD94] on expediting temporal inference, which in turn builds on the work of Lansky <ref> [Lan88] </ref>, Tenenberg [Ten91] and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory.
Reference: [Las70] <author> Leon S. Lasdon. </author> <title> Optimization Theory for Large Systems. </title> <publisher> Macmillan Company, </publisher> <year> 1970. </year>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] <ref> [Las70] </ref> [DW60] [Chv80] like the well known simplex method. <p> The details of the material presented in this section depend on some understanding of linear programming [Chv80] and methods for decomposing and solving large systems <ref> [Las70] </ref>. We sketch the iterative method in the following, and refer the readers to appendix B and Appendix C for more details. 1. <p> The analysis in this paper of the paper borrows heavily from the work in oper ations research and combinatorial optimization for representing Markov decision 122 processes as linear programs [D'E63] [Der70] and decomposing large systems gen-erally [DW60] <ref> [Las70] </ref> and Markov decision processes specifically [KC74a]. <p> B.1 Linear Programming and the Dantzig-Wolfe Decomposition Principle To cope with linear programs of very large sizes, decomposition principles <ref> [Las70] </ref> that divide a large linear program into many correlated linear programs of smaller sizes have been well studied, among which the DW decomposition may be the most well known. In the following, we describe the general form of the DW decomposition without assuming additional structure in linear programs. <p> In the following, we describe the general form of the DW decomposition without assuming additional structure in linear programs. We refer the readers to <ref> [Las70] </ref> [MB90] for more details about linear programming and the DW decomposition. 142 Consider a linear program of standard form min z = cX X 0 &gt; &gt; &gt; = : : : (1); where X is a N fi 1 column vector composed of N variables x 1 ; x <p> X to AX = b can be properly represented as X = 1ir X j X (j) ; where * X i 's, 1 i r, and X j 's, 1 j t are N fi 1 column vectors representing the finite number of extreme points and extreme rays 1 <ref> [Las70] </ref> [MB90] respectively of the set fxjA 2 X = b 2 g, and 1 The set of extreme rays f X 1 ; : : : ; X t g is any maximal set of linearly independent solutions to A 2 X = 0. * i 's and j 's <p> However, it is infeasible to exhaustively enumerate all these extreme points and extreme rays in general. To cope with this difficulty, a subproblem is devised according to the current simplex multipliers 2 <ref> [Las70] </ref> [MB90] associated with the m + 1 linear constraints in the master problem (3). By solving the subproblem, we can find an extreme point or an extreme ray that gives us a profitable column to enter the basis.
Reference: [Lat90a] <author> Jean-Claude Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer, </publisher> <address> Boston, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: A policy maps the current location and the conditions of the environment into an action to take. For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem <ref> [Lat90a] </ref> [Jak94] [GD95] [CKL94] [MA95] [DKKN95], is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M =
Reference: [Lat90b] <author> Jean-Claude Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1990. </year>
Reference: [LD93] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Exploiting locality in temporal reasoning. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1993. </year>
Reference-contexts: of this chapter, we use the first scenario in Chapter 3.3 as an example for illustrating algorithmic details, 1 The proofs for all theorems stated in this thesis are provided in Appendix A at the end of this document. 2 The PRJ1 task is considered by Lin and Dean in <ref> [LD93] </ref> and the PRJ2 task is considered by Dean and Boddy in [DB88b] and Nebel and Backstrom in [NB92]. 56 which provides the following problem instance for temporal projection. * P = fX 1 ; X 2 ; X 3 ; X 4 ; X 5 ; X 6 ; X
Reference: [LD94] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Exploiting locality in temporal reasoning. </title> <editor> In E. Sandewall and C. Backstrom, editors, </editor> <booktitle> Current Trends in AI Planning, </booktitle> <address> Amsterdam, 1994. </address> <publisher> IOS Press. </publisher>
Reference-contexts: These decomposition techniques improve computational efficiency by exploiting locality and dependency embedded in problem instances. For each decomposition technique developed in thesis, we describe a structural parameter characterizing the locality and dependency exploitable in a problem instance and quantify the resulting computational efficiency in terms of this structural parameter <ref> [LD94] </ref> [LD95b] [DL95] [LD95a]. 1.4 Outline of the Thesis Chapter 2 investigates planning and control over discrete dynamical systems, introduces the definitions and notation used throughout this thesis, and describes AI representational schemas for compactly encoding action dynamics. <p> this thesis can be easily extended to other representation schemas, rather than strictly depending on this specific representation scheme. 2.3.1 Compactly Representing State-transition Relations Instead of explicitly enumerating the state-transition relation D of a discrete dynamical system D, D can be compactly represented as sets of causal rules [DB88a] [BD94] <ref> [LD94] </ref> [KHW94] [LD96]. Each action is associated with a set of causal rules, while each causal rule is basically a state-space operator that partially encodes how the associated action can result in state transitions. The following is a formal description of causal rules and their usage in representing state-transition relations. <p> If we consider undesirable states as target states and uniformly associate every undesirable states with a large positive reward, then the task of temporal projection [DB88b] [NB94] <ref> [LD94] </ref> [LD96] is to determine an optimal linear plan that returns a maximum final reward for a given multi-level task network. 3.1.1 Multi-Level Task Networks A multi-level task network specifies a hierarchy of tasks together with the temporal constraints regarding the order of performing these tasks. <p> The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete [DB88a] [NB94] <ref> [LD94] </ref> [LD96]. The temporal projection problem can be viewed as a special kind of planning problem where only linear plans of a fixed length (i.e., the number of primitive 26 tasks in the given task network) are considered. <p> This makes the problem fall in the class NP, since a decision maker can non-deterministically determine a linear plan as a solution in polynomial time. The temporal projection problem is NP-Complete even in very restricted cases <ref> [LD94] </ref> [LD96]. 3.2 Sequential Decision Networks and Optimal Decision Making 3.2.1 Sequential Decision Networks A decision stage for a decision maker contains a restricted set of possible actions together with the information about transition probabilities and action costs. <p> The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean <ref> [LD94] </ref> on expediting temporal inference, which in turn builds on the work of Lansky [Lan88], Tenenberg [Ten91] and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory.
Reference: [LD95a] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Exploiting locality in temporal reasoning. </title> <journal> Computational Intelligence, </journal> <note> 1995. to appear. 173 </note>
Reference-contexts: For each decomposition technique developed in thesis, we describe a structural parameter characterizing the locality and dependency exploitable in a problem instance and quantify the resulting computational efficiency in terms of this structural parameter [LD94] [LD95b] [DL95] <ref> [LD95a] </ref>. 1.4 Outline of the Thesis Chapter 2 investigates planning and control over discrete dynamical systems, introduces the definitions and notation used throughout this thesis, and describes AI representational schemas for compactly encoding action dynamics.
Reference: [LD95b] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Generating optimal policies for high-level plans with conditional branches and loops. </title> <booktitle> In Proceedings of the Third European Workshop on Planning, </booktitle> <pages> pages 205-218, </pages> <year> 1995. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. [Dea88] [Dea87] [BF88] [DW91] <ref> [LD95b] </ref> [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems is to explicitly construct <p> In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] [BDH95] <ref> [LD95b] </ref> [LD96]. Compact representations often exhibit locality and dependency in action dynamics, which indicates regularity and redundancy in the underlying discrete dynamical systems. In many situations, explicit construction of the entire dynamical systems underlying such kinds of planning instances is not necessary. <p> For each decomposition technique developed in thesis, we describe a structural parameter characterizing the locality and dependency exploitable in a problem instance and quantify the resulting computational efficiency in terms of this structural parameter [LD94] <ref> [LD95b] </ref> [DL95] [LD95a]. 1.4 Outline of the Thesis Chapter 2 investigates planning and control over discrete dynamical systems, introduces the definitions and notation used throughout this thesis, and describes AI representational schemas for compactly encoding action dynamics. <p> Often complex decision processes can be broken down into elementary decision stages [HM84] [Sha86] [TS90] [SW95] <ref> [LD95b] </ref>. These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements. <p> A sequential decision network is composed of many decision-making stages interconnected as a directed graph using conditional branches and loops [SW95] <ref> [LD95b] </ref>. Sequential decision networks compactly represent Markov decision processes with rich structures in the underlying state spaces and action dynamics.
Reference: [LD96] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Localized temporal projection using subgoals and abstraction. </title> <journal> Computational Intelligence, </journal> <note> 1996. To appear. </note>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] [ZW90]. [Dea88] [Dea87] [BF88] [DW91] [LD95b] <ref> [LD96] </ref> and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning problems is to explicitly construct the <p> In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances. In particular, we are interested in planning instances that employ AI-style compact representations [BD94] [BDG95] [BDH95] [LD95b] <ref> [LD96] </ref>. Compact representations often exhibit locality and dependency in action dynamics, which indicates regularity and redundancy in the underlying discrete dynamical systems. In many situations, explicit construction of the entire dynamical systems underlying such kinds of planning instances is not necessary. <p> can be easily extended to other representation schemas, rather than strictly depending on this specific representation scheme. 2.3.1 Compactly Representing State-transition Relations Instead of explicitly enumerating the state-transition relation D of a discrete dynamical system D, D can be compactly represented as sets of causal rules [DB88a] [BD94] [LD94] [KHW94] <ref> [LD96] </ref>. Each action is associated with a set of causal rules, while each causal rule is basically a state-space operator that partially encodes how the associated action can result in state transitions. The following is a formal description of causal rules and their usage in representing state-transition relations. <p> If we consider undesirable states as target states and uniformly associate every undesirable states with a large positive reward, then the task of temporal projection [DB88b] [NB94] [LD94] <ref> [LD96] </ref> is to determine an optimal linear plan that returns a maximum final reward for a given multi-level task network. 3.1.1 Multi-Level Task Networks A multi-level task network specifies a hierarchy of tasks together with the temporal constraints regarding the order of performing these tasks. <p> The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete [DB88a] [NB94] [LD94] <ref> [LD96] </ref>. The temporal projection problem can be viewed as a special kind of planning problem where only linear plans of a fixed length (i.e., the number of primitive 26 tasks in the given task network) are considered. <p> This makes the problem fall in the class NP, since a decision maker can non-deterministically determine a linear plan as a solution in polynomial time. The temporal projection problem is NP-Complete even in very restricted cases [LD94] <ref> [LD96] </ref>. 3.2 Sequential Decision Networks and Optimal Decision Making 3.2.1 Sequential Decision Networks A decision stage for a decision maker contains a restricted set of possible actions together with the information about transition probabilities and action costs.
Reference: [LDK95] <author> Michael Littman, Thomas Dean, and Leslie Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the 14th Conference on Uncertainty in AI, </booktitle> <year> 1995. </year>
Reference-contexts: Assuming finite precision and a constant discount rate fl strictly less than one, both methods converge to an optimal solution in a finite number of iterations, and are polynomial-time algorithms in terms of the sizes of the state spaces [Put94] [PT87a] <ref> [LDK95] </ref>. Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method.
Reference: [LP81] <author> Harry R. Lewis and Christos H. Papadimitriou. </author> <title> Elements of the Theory of Computation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: the probability governing state transitions are often available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata [Paz71] [HU79] <ref> [LP81] </ref> and Markov decision processes [Paz71] [BS78] [Put94] as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> principle utilized in Chapter 7, while Appendix C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata [Paz71] [HU79] <ref> [LP81] </ref> and Markov decision processes [Paz71] [BS78] [DW91] [Put94], and introduce definitions and notation used throughout the thesis. <p> G while the goal G is achieved when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata [Paz71] [HU79] <ref> [LP81] </ref> and Markov decision processes [Paz71] [BS78] [DW91] [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S.
Reference: [LS88] <author> Steffen L. Lauritzen and David J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 50(2) </volume> <pages> 157-194, </pages> <year> 1988. </year>
Reference-contexts: Different abstraction techniques have been investigated in classic STRIPS planning [Kno91b] [Ten91] [Sac74b] [HS94] to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks <ref> [LS88] </ref> [JLO90] provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis. <p> variables, with the dependecies among state variables compactly encoded in the network <ref> [LS88] </ref>[Pea88]. Dependencies among random variables provide computational leverage in computing marginal probabilities and conditional probabilities. Rather than globally enumeratng possible combination of variable values, local computation and propagation exploiting dependency and result in much better computational efficiency [JLO90] [LS88] [SDDF90] [AWFA87]. 8.5 Decomposition Theory in Relational Databases The decomposition theory for relational databases provides an analogy to the decomposition approach for planning under uncertainty in this thesis.
Reference: [LY92] <author> David Lee and Mihalis Yannakakis. </author> <title> Online minimization of transition systems. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1992. </year>
Reference-contexts: For model checking og complicated communication protocols, aggregation techniques have been investigated to derive equivalent finite-state machines on the fly to minimize computation time of validation or verification <ref> [LY92] </ref> [LY94] [BCL + 94] [DG97]. Abstraction is a special way of aggregation, in which two states are indistinguishable if they share the same configuration governing the values of a selected subset of variables.
Reference: [LY94] <author> David Lee and Mihalis Yannakakis. </author> <title> Testing finite state machines: State identification and verification. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(3), </volume> <year> 1994. </year>
Reference-contexts: For model checking og complicated communication protocols, aggregation techniques have been investigated to derive equivalent finite-state machines on the fly to minimize computation time of validation or verification [LY92] <ref> [LY94] </ref> [BCL + 94] [DG97]. Abstraction is a special way of aggregation, in which two states are indistinguishable if they share the same configuration governing the values of a selected subset of variables.
Reference: [MA95] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: For stochastic transportation planning, the agent would like to determine an optimal policy that minimizes the expected cumulative cost (e.g., time and fuel) to reach the destination. Such a stochastic transportation problem [Lat90a] [Jak94] [GD95] [CKL94] <ref> [MA95] </ref> [DKKN95], is a Markov decision problem: * The state variables that model the physical locations and the conditions of the local environment determine the state space S. * The action space A is the union of the actions available at different loca tions. * M = hS; A; Pr; Costi <p> Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] [CW90]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces <ref> [MA95] </ref> and stochastic models and discrete state spaces [Kae93]. The hierarchical policy construction method described in Section 7.2 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [Kae93] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces.
Reference: [MB83] <author> Jitendra Malik and Thomas O. Binford. </author> <title> Reasoning in time and space. </title> <booktitle> In Proceedings IJCAI 8, </booktitle> <pages> pages 343-345. </pages> <address> IJCAII, </address> <year> 1983. </year> <month> 174 </month>
Reference-contexts: In both cases, procedure Localized-Reasoning provides substantial computational savings. 5.7 Related Work The related work on the representation of time and temporal reasoning is extensive. There are several formalisms that can represent the temporal relationship among a set of points or intervals in time [All83] [Vil82] <ref> [MB83] </ref> [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94].
Reference: [MB90] <author> H. D. Sherali M.S. Bazaraa, J. J. Jarvis. </author> <title> Linear Programming and Network Flows. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming <ref> [MB90] </ref> [Las70] [DW60] [Chv80] like the well known simplex method. <p> time and space complexity of this iterative method. * Empirical experience suggests that the Dantzig-Wolfe decomposition principle, which the analysis of the iterative method is based on, allows convergence to within 1-5% of the optimum fairly quickly, although the tail convergence rate can be very slow (see page 325 in <ref> [MB90] </ref>). <p> In the following, we describe the general form of the DW decomposition without assuming additional structure in linear programs. We refer the readers to [Las70] <ref> [MB90] </ref> for more details about linear programming and the DW decomposition. 142 Consider a linear program of standard form min z = cX X 0 &gt; &gt; &gt; = : : : (1); where X is a N fi 1 column vector composed of N variables x 1 ; x 2 <p> to AX = b can be properly represented as X = 1ir X j X (j) ; where * X i 's, 1 i r, and X j 's, 1 j t are N fi 1 column vectors representing the finite number of extreme points and extreme rays 1 [Las70] <ref> [MB90] </ref> respectively of the set fxjA 2 X = b 2 g, and 1 The set of extreme rays f X 1 ; : : : ; X t g is any maximal set of linearly independent solutions to A 2 X = 0. * i 's and j 's must <p> However, it is infeasible to exhaustively enumerate all these extreme points and extreme rays in general. To cope with this difficulty, a subproblem is devised according to the current simplex multipliers 2 [Las70] <ref> [MB90] </ref> associated with the m + 1 linear constraints in the master problem (3). By solving the subproblem, we can find an extreme point or an extreme ray that gives us a profitable column to enter the basis. <p> In particular, if ffi fi is zero or negative, we have reached an optimal solution of the master problem. Both the master problem and the subproblems center around the use of simplex method in solving the linear program in (3). We refer the readers to <ref> [MB90] </ref> for more details about (i) cycling prevention and (ii) the initialization of the simplex method using big-M method. We briefly summarize two important properties about the DW decomposition that are useful in the remainder of this paper. <p> Every artificial variable is associated the cost M , which is a large positive value. M is exactly the value used in big-M method to initiate the simplex method. We refer the readers to <ref> [MB90] </ref> for the details of the big-M method and the issue of choosing an appropriate M . * When applying the big-M method, some of the artificial variables may leave and enter the basis more than one time. <p> Similarly, we allow the ith dummy 2 M is exactly the value used in big-M method to initiate the simplex method. We refer the readers to <ref> [MB90] </ref> about the choice of an appropriate M . 164 policy to reenter the policy repository after it has been excluded from the policy repository.
Reference: [Mea92] <editor> A. Montanari and et al. </editor> <title> Dealing with time granularity in the event calculus. </title> <booktitle> In Proceedings of FGCS'92, </booktitle> <year> 1992. </year>
Reference-contexts: Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94]. The idea of structuring ordering and causal knowledge to speed up temporal reasoning is also present in maintaining knowledge about temporal intervals [All83] [GS93] [KG77] and in the event calculus [Eva90] <ref> [Mea92] </ref>. The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] [YT90] in reducing the overall search effort.
Reference: [MH95] <author> Drew McDermott and James Hendler. </author> <title> Planning: What it is, what it could be, an introduction to the special issue on planning and scheduling. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):1-16, </volume> <year> 1995. </year>
Reference: [MR91] <author> David A. McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 634-639. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference-contexts: Introduction 1.1 Planning under Uncertainty In classical planning, propositional STRIPS-like operators [FN71a] [AHT90] are employed to attain a goal where some state variables (propositions) have specified values. A classical plan is a multiset of totally ordered or partially ordered STRIPS-like operators [Cha87] <ref> [MR91] </ref>. The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] [PW92] [BW93] [Wel94]. Each planning instance involves a finite set of state variables.
Reference: [NB92] <author> Bernhard Nebel and Christer Backstrom. </author> <title> On the computational complexity of temporal projection and plan validation, </title> <year> 1992. </year>
Reference-contexts: 1 The proofs for all theorems stated in this thesis are provided in Appendix A at the end of this document. 2 The PRJ1 task is considered by Lin and Dean in [LD93] and the PRJ2 task is considered by Dean and Boddy in [DB88b] and Nebel and Backstrom in <ref> [NB92] </ref>. 56 which provides the following problem instance for temporal projection. * P = fX 1 ; X 2 ; X 3 ; X 4 ; X 5 ; X 6 ; X 7 ; X 8 g. <p> Theorem 5.3 Temporal projection regarding one-rule events with an arbitrary partial order and a polynomial-size state space is NP-Complete 3 . Corollary 5.1 Temporal projection regarding general events with an arbitrary partial order and an exponential-size state space is NP-Complete. 3 Nebel and Backstrom <ref> [NB92] </ref> prove the NP-Completeness of a closely related case. Their proof technique is adapted here to prove Theorem 5.3 60 The following three theorems demonstrate that temporal projection is more tractable when we have (1) available structure in causal rules or event ordering or (2) a small state space.
Reference: [NB94] <author> Bernhard Nebel and Christer Backstrom. </author> <title> On the computational complexity of temporal projection planning and plan validation. </title> <journal> Artificial Intelligence, </journal> <volume> 66(1) </volume> <pages> 125-160, </pages> <year> 1994. </year>
Reference-contexts: If we consider undesirable states as target states and uniformly associate every undesirable states with a large positive reward, then the task of temporal projection [DB88b] <ref> [NB94] </ref> [LD94] [LD96] is to determine an optimal linear plan that returns a maximum final reward for a given multi-level task network. 3.1.1 Multi-Level Task Networks A multi-level task network specifies a hierarchy of tasks together with the temporal constraints regarding the order of performing these tasks. <p> The Temporal Projection Problem investigated in this thesis is shown to be NP-Complete [DB88a] <ref> [NB94] </ref> [LD94] [LD96]. The temporal projection problem can be viewed as a special kind of planning problem where only linear plans of a fixed length (i.e., the number of primitive 26 tasks in the given task network) are considered.
Reference: [NK94] <author> Ann Nicholson and Leslie Pack Kaelbling. </author> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <year> 1994. </year>
Reference-contexts: However, the sizes of the underlying state spaces usually grow exponentially in the sizes of the problem instances, which makes the naive approach infeasible. To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee <ref> [NK94] </ref> [DKKN93b] [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances [BD94] [TVR96] and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control.
Reference: [Paz71] <author> Azaria Paz. </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: cost and the probability governing state transitions are often available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [Put94] as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> are often available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [Put94] as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> Dantzig-Wolfe decomposition principle utilized in Chapter 7, while Appendix C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94], and introduce definitions and notation used throughout the thesis. <p> while Appendix C describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94], and introduce definitions and notation used throughout the thesis. <p> states satisfying G while the goal G is achieved when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S. <p> is achieved when the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata <ref> [Paz71] </ref> [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] [Put94]. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S.
Reference: [Pea88] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, California, </address> <year> 1988. </year>
Reference: [PS82] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: Finally, we examine the event sequence to determine whether it is consistent with the ordering constraints. Since the bipartite match problem can be solved in polynomial 135 time <ref> [PS82] </ref> [CLR91], this reduction together with Theorem 5.1 prove that this special case of temporal projection is solvable in polynomial time. 2 Theorem 5.7.
Reference: [PT87a] <author> Christos H. Papadimitriou and J. N. Tsitsiklis. </author> <title> The complexity of markov chain decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 441-450, </pages> <year> 1987. </year> <month> 175 </month>
Reference-contexts: Assuming finite precision and a constant discount rate fl strictly less than one, both methods converge to an optimal solution in a finite number of iterations, and are polynomial-time algorithms in terms of the sizes of the state spaces [Put94] <ref> [PT87a] </ref> [LDK95]. Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method.
Reference: [PT87b] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complex-ity of Markov chain decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: The performance criterion F is to minimize the expected cumulative cost to reach the target states. 3.2.6 Computational Complexity Although an instance of the Markov decision problem can be solved in time polynomial in the size of the underlying state space [Put94] <ref> [PT87b] </ref>, the Markov decision problem is PSPACE-hard in general.
Reference: [Put94] <author> M. L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: available also, and it is appropriate to consider more general performance criteria for planning rather than a simple dichotomy between attaining or not attaining a goal. 1.2 Planning and Control In this thesis, we adopt discrete dynamical systems like finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] [BS78] <ref> [Put94] </ref> as the representational and computational foundation for planning under uncertainty. A discrete dynamical system is composed of a finite state space, a finite action space, and an action dynamics governing how actions result in state transitions. <p> describes the technical details about a decomposition technique depicted in Chapter 7. 5 Chapter 2 Planning and Control: Foundations In this chapter, we describe the perspective of planning as control involving discrete dynamical systems in the form of finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] <ref> [Put94] </ref>, and introduce definitions and notation used throughout the thesis. <p> the system ends in a target state in Target (G). 2.2 Finite Automata and Markov Decision Pro cesses In this thesis, we focus on three planning problems generically formulated as control problems over discrete dynamical systems such as finite automata [Paz71] [HU79] [LP81] and Markov decision processes [Paz71] [BS78] [DW91] <ref> [Put94] </ref>. Finite Automata: A finite automaton is a discrete dynamical system D = hS; A; D i whose action dynamics D is simply a state transition relation, D S fi A fi S. <p> The Markov Decision Problem: The Markov decision problem is a control problem <ref> [Put94] </ref> [Der70] over Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community [Bel61] <p> Markov decision processes, and the task is to generate an optimal policy (i.e., a stationary conditional plan) given a Markov decision process and a performance criterion. 2.5 Algorithms for Optimal Control over Markov Decision Processes Markov decision processes have been well studied by the operations research community [Bel61] [How60] [Ber87] <ref> [Put94] </ref>. In this thesis, we extensively utilize the computational machinery developed by the operations research community in computing optimal policies for Markov decision processes. <p> Assuming finite precision and a constant discount rate fl strictly less than one, both methods converge to an optimal solution in a finite number of iterations, and are polynomial-time algorithms in terms of the sizes of the state spaces <ref> [Put94] </ref> [PT87a] [LDK95]. Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. <p> less than one, both methods converge to an optimal solution in a finite number of iterations, and are polynomial-time algorithms in terms of the sizes of the state spaces <ref> [Put94] </ref> [PT87a] [LDK95]. Linear Programming: The instances of Markov decision problems can also be cast as linear programs [Der70] [D'E63] [KK71a] [Put94], and then solved by standard techniques for linear programming [MB90] [Las70] [DW60] [Chv80] like the well known simplex method. <p> The performance criterion F is to minimize the expected cumulative cost to reach the target states. 3.2.6 Computational Complexity Although an instance of the Markov decision problem can be solved in time polynomial in the size of the underlying state space <ref> [Put94] </ref> [PT87b], the Markov decision problem is PSPACE-hard in general. <p> Hierarchical policy construction does not guarantee to produce an optimal policy; however, it does so relatively efficiently and has an intuitive interpretation that makes it suitable for robot navigation domains. For a simple partition of the state space with no aggregation within regions, standard algorithms <ref> [Put94] </ref> on the base-level state space would be dominated by a factor quadratic in the size of the state space (jSj), while hierarchical policy construction would be dominated by the number of regions in the partition (jP j) times the maximum number of neighbors for any region (max R2P jfSjS 2 <p> Modeling the Computation Complexity of the Standard Techniques Note that * Standard techniques like the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic convergence rate respectively <ref> [Put94] </ref>. * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space [Der70] [Put94]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where (i) <p> techniques like the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic convergence rate respectively <ref> [Put94] </ref>. * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space [Der70] [Put94]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where (i) both c and k are constants and (ii) n is the size of the state space. k is 3 and 2 respectively when applying the policy iteration method <p> B.2 The DW Decomposition and Markov De cision Processes It is well known that we can solve Markov decision processes as linear programs [D'E63] [Der70] [KK71a] <ref> [Put94] </ref> using standard techniques like the simplex method. Kushner and Chen [KC74a] investigate the use of the DW decomposition in solving Markov decision processes as linear programs.
Reference: [PW92] <author> J. S. Penberthy and Daniel S. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for ADL. </title> <booktitle> In Proceedings of the 1992 International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 103-114, </pages> <year> 1992. </year>
Reference-contexts: The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] <ref> [PW92] </ref> [BW93] [Wel94]. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [RW89a] <author> Peter Ramadge and Murray Wonham. </author> <title> The control of discrete event systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(1) </volume> <pages> 81-98, </pages> <year> 1989. </year>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems <ref> [RW89a] </ref> [ZW90]. [Dea88] [Dea87] [BF88] [DW91] [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these
Reference: [RW89b] <author> Peter Ramadge and Murray Wonham. </author> <title> The control of discrete event systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(1) </volume> <pages> 81-98, </pages> <year> 1989. </year>
Reference-contexts: The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan <ref> [RW89b] </ref> for an survey of work on discrete event systems that concerns the synthesis of supervisory systems. Dean and Lin [DL95] describe a family of techniques for decomposing the computations required to solve large Markov decision processes.
Reference: [Sac74a] <author> Earl Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 231-272, </pages> <year> 1974. </year>
Reference-contexts: The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning <ref> [Sac74a] </ref> [Chr90] [Kno91a] [YT90] in reducing the overall search effort. The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM [Lan88] in event-based planning. <p> In the area planning and search assuming deterministic action models, there is the work on macro operators [Kor85] and hierarchies of state-space operators <ref> [Sac74a] </ref> [Kno91a]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ZW90] [CW90]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces [Kae93].
Reference: [Sac74b] <author> Earl Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 231-272, </pages> <year> 1974. </year>
Reference-contexts: Aggregation and abstraction are employed as tools for constructing compact and equivialent Markov decision processes or finite automata to improve computational efficiency. Different abstraction techniques have been investigated in classic STRIPS planning [Kno91b] [Ten91] <ref> [Sac74b] </ref> [HS94] to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks [LS88] [JLO90] provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis.
Reference: [Sch84] <author> Paul J. Schweitzer. </author> <title> Aggregation methods for large Markov chains. </title> <editor> In G. Iazola, P. J. Coutois, and A. Hordijk, editors, </editor> <booktitle> Mathemaical Computer Performance and Reliability, </booktitle> <pages> pages 275-302. </pages> <publisher> Elsevier, Amster-dam, Holland, </publisher> <year> 1984. </year>
Reference-contexts: approach is that often all or most of the state variables turn out to be relevant; in this situation, the approximation approach does not help. 8.3 Aggregation and Abstraction When computing an optimal policy for a large Markov decision process, it is useful to aggregate indistinguishable states dynamically [BDG95] [BCn89] <ref> [Sch84] </ref> [DG97] or statically using different notions of indistinguishability to reduce redundancy in computation. The decomposition technique in Chapter 6 employs structural analysis to identify a static aggregation of states before solving the underlying Markov decision process.
Reference: [SDDF90] <author> Ross D. Shachter, Bruce D'Ambrosio, and Brendan A. Del Favero. </author> <title> Symbolic probabilistic inference in belief networks. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 126-131. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: Dependencies among random variables provide computational leverage in computing marginal probabilities and conditional probabilities. Rather than globally enumeratng possible combination of variable values, local computation and propagation exploiting dependency and result in much better computational efficiency [JLO90] [LS88] <ref> [SDDF90] </ref> [AWFA87]. 8.5 Decomposition Theory in Relational Databases The decomposition theory for relational databases provides an analogy to the decomposition approach for planning under uncertainty in this thesis.
Reference: [Sha86] <author> Ross D. Shachter. </author> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 34(6) </volume> <pages> 871-882, </pages> <year> 1986. </year> <month> 176 </month>
Reference-contexts: Often complex decision processes can be broken down into elementary decision stages [HM84] <ref> [Sha86] </ref> [TS90] [SW95] [LD95b]. These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements.
Reference: [SW95] <author> David E. Smith and Mike Williamson. </author> <title> Representation and evalu-ation of plans with loops. </title> <booktitle> To appear in the working notes for the 1995 Stanford Spring Symposium on Extended Theories of Action, </booktitle> <year> 1995. </year>
Reference-contexts: Often complex decision processes can be broken down into elementary decision stages [HM84] [Sha86] [TS90] <ref> [SW95] </ref> [LD95b]. These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements. <p> These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements. A sequential decision network is composed of many decision-making stages interconnected as a directed graph using conditional branches and loops <ref> [SW95] </ref> [LD95b]. Sequential decision networks compactly represent Markov decision processes with rich structures in the underlying state spaces and action dynamics. <p> This simply introduces a p local discount over data governing the effectiveness of dimensionality. 6.5 Related Work The particular representation of Markov processes in terms of sequential decision networks with conditional branches and loops that serves as inspiration for this paper is due to Smith and Williamson <ref> [SW95] </ref>. The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [LD94] on expediting temporal inference, which in turn builds on the work of Lansky [Lan88], Tenenberg [Ten91] and others in exploiting locality planning.
Reference: [Ten91] <author> Josh D. Tenenberg. </author> <title> Abstraction in planning. </title> <editor> In J. F. Allen, H. A. Kautz, R. N. Pelavin, and J. D. Tenenberg, editors, </editor> <booktitle> Reasoning about Plans, </booktitle> <pages> pages 213-280. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, California, </address> <year> 1991. </year>
Reference-contexts: The particular methods for compiling fragment graphs are adapted from the work of Lin and Dean [LD94] on expediting temporal inference, which in turn builds on the work of Lansky [Lan88], Tenenberg <ref> [Ten91] </ref> and others in exploiting locality planning. The general idea of restricting reachability in state space by enabling some actions and disabling others is common in control theory. See Ramadge and Wonhan [RW89b] for an survey of work on discrete event systems that concerns the synthesis of supervisory systems. <p> Aggregation and abstraction are employed as tools for constructing compact and equivialent Markov decision processes or finite automata to improve computational efficiency. Different abstraction techniques have been investigated in classic STRIPS planning [Kno91b] <ref> [Ten91] </ref> [Sac74b] [HS94] to cope with very large search spaces. 8.4 Probabilistic Inference in Bayesian Networks Localized probabilistic inference in Bayesian networks [LS88] [JLO90] provides an analogy to the decomposition approach for planning under uncertainty in this 127 thesis.
Reference: [TS90] <author> Joseph A. Tatman and Ross D. Shachter. </author> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 20(2) </volume> <pages> 365-379, </pages> <year> 1990. </year>
Reference-contexts: Often complex decision processes can be broken down into elementary decision stages [HM84] [Sha86] <ref> [TS90] </ref> [SW95] [LD95b]. These decision stages can be combined to form a sequential decision network using a simple programming language specifying conditionals, loops, and sequences involving the decision tasks as primitive statements.
Reference: [TVR96] <author> John N. Tsitsiklis and Benjamin Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 59-94, </pages> <year> 1996. </year>
Reference-contexts: To cope with this computational barrier, researchers have explored different alternatives such as heuristic algorithms without quality guarantee [NK94] [DKKN93b] [DKKN93a] [DKKN95] approximation algorithms applicable under certain circumstances [BD94] <ref> [TVR96] </ref> and exact algorithms aggregating states on the fly in improving computational efficiency [BDG95] [BCn89]. In this thesis, we present exact decomposition techniques for planning and control. Computational efficiency is attained by exploiting locality and dependency recognized through static analysis of planning instances.
Reference: [Ull88] <author> J. D. Ullman. </author> <title> Principles of Database and Knowledge Base Systems. </title> <publisher> Computer Science Press, </publisher> <address> Potomac, Md, </address> <year> 1988. </year>
Reference-contexts: Dependencies among domain features play an important role in decomposition theory for relational databases, and provide computational leverage in time and space [ADA93] <ref> [Ull88] </ref> [KS91]. Instead of keeping a large global relational schema covering all attributes, often small local relational schemas are employed, each covering only a subset of the attributes. This allows database managers to keep small databases over the local relational schemas, rather than a large database over the global schema.
Reference: [Vil82] <author> Marc Vilain. </author> <title> A system for reasoning about time. </title> <booktitle> In Proceedings AAAI-82. AAAI, </booktitle> <year> 1982. </year>
Reference-contexts: In both cases, procedure Localized-Reasoning provides substantial computational savings. 5.7 Related Work The related work on the representation of time and temporal reasoning is extensive. There are several formalisms that can represent the temporal relationship among a set of points or intervals in time [All83] <ref> [Vil82] </ref> [MB83] [Dea84]. Reasoning about the consequence of events using the logic-based event calculus can be found in [KS86] [Chi94].
Reference: [Wel94] <author> Daniel S. Weld. </author> <title> An introduction to partial-order planning. </title> <journal> AI Magazine, </journal> <month> Winter, </month> <year> 1994. </year>
Reference-contexts: The task of planning is to determine a plan such that by taking the actions (STRIPS-like operators) in an order consistent with the plan a specified goal is achieved at the end [FN71a] [Cha87] [AHT90] [PW92] [BW93] <ref> [Wel94] </ref>. Each planning instance involves a finite set of state variables. The values of the variables at a point in time determine a state. A STRIPS-like operator is only applicable in the states where the precondition associated with the operator is true.
Reference: [YT90] <author> Qiang Yang and Josh D. Tenenberg. Abtweak: </author> <title> Abstracting a nonlinear, least commitment planner. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 204-209. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: The use of subgoals and abstract events in Chaprer 4 is inspired by the similar notions of subgoals and abstraction used in search [Kor87] and in abstract and hierarchical planning [Sac74a] [Chr90] [Kno91a] <ref> [YT90] </ref> in reducing the overall search effort. The idea of using localized reasoning to exploit locality is also studied by Lansky in GEM [Lan88] in event-based planning.
Reference: [ZW90] <author> H. Zhong and W. M. Wonham. </author> <title> On the consistency of hierarchical supervision in discrete-event systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35(10) </volume> <pages> 1125-1134, </pages> <year> 1990. </year> <month> 177 </month>
Reference-contexts: Under this framework, the task of planning is to search for a plan that provides a maximum expected utility regarding a specific performance criterion. This perspective conveniently casts planning under uncertainty as control over discrete dynamical systems [RW89a] <ref> [ZW90] </ref>. [Dea88] [Dea87] [BF88] [DW91] [LD95b] [LD96] and provides the computational leverage of employing existing algorithms developed by the operations research community. 2 1.3 Exploiting Locality and Dependency As many planning problems can be formulated as control problems over compactly encoded discrete dynamical systems, a naive approach to solve these planning <p> In the area planning and search assuming deterministic action models, there is the work on macro operators [Kor85] and hierarchies of state-space operators [Sac74a] [Kno91a]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines <ref> [ZW90] </ref> [CW90]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [MA95] and stochastic models and discrete state spaces [Kae93].
References-found: 118

