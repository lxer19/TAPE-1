URL: http://www.cs.toronto.edu/~revow/papers/pancake.ps.Z
Refering-URL: http://www.cs.utoronto.ca/neuron/elastic.html
Root-URL: 
Title: Recognizing Handwritten Digits Using Mixtures of Linear Models  
Author: Geoffrey E Hinton Michael Revow Peter Dayan 
Address: Toronto, Ontario, Canada M5S 1A4  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bourlard, H & Kamp, </author> <title> Y (1988). Auto-association by Multilayer Perceptrons and Singular Value Decomposition. </title> <journal> Biol. Cybernetics 59, </journal> <pages> 291-294. </pages>
Reference-contexts: We apply this idea to recognising handwritten digits from grey-level pixel images using linear auto-encoders. Linear hidden units for autoencoders are barely worse than non-linear ones when squared reconstruction error is used <ref> [1] </ref>, but have the great computational advantage during training that input-hidden and hidden-output weights can be derived from principal components analysis (PCA) of the training data.
Reference: [2] <author> Bregler, C & Omohundro, </author> <title> SM (1995). Non-linear image interpolation using surface learning. This volume. </title>
Reference-contexts: A similar idea for data compression was used by [9] where vector quantization was used to define sub-classes and PCA was performed within each sub-class (see also <ref> [2] </ref>). We used an iterative method based on the Expectation Maximisation (EM) algorithm [4] to fit mixtures of linear models. The reductio of the local linear approach would have just one training pattern in each model.
Reference: [3] <author> Dayan, P, Hinton, GE, Neal, RM & Zemel, </author> <title> RS (1995). The Helmholtz machine. Neural Computation, </title> <publisher> in press. </publisher>
Reference-contexts: The generative weights of this autoencoder can be obtained using the technique of maximum likelihood factor analysis (which is closely related to PCA) and the resulting architecture and hierarchical variants of it can be formulated as real valued versions of the Helmholtz machine <ref> [3, 6] </ref>. The cost of coding the factors relative to their prior is implicitly included in this formulation, as is the possibility that different input pixels are subject to different amounts of noise.
Reference: [4] <author> Dempster, AP, Laird, NM & Rubin, </author> <title> DB (1976). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Proceedings of the Royal Statistical Society, </journal> <pages> 1-38. </pages>
Reference-contexts: A similar idea for data compression was used by [9] where vector quantization was used to define sub-classes and PCA was performed within each sub-class (see also [2]). We used an iterative method based on the Expectation Maximisation (EM) algorithm <ref> [4] </ref> to fit mixtures of linear models. The reductio of the local linear approach would have just one training pattern in each model. This approach would amount to a nearest neighbour method for recognition using a Euclidean metric for error, a technique which is known to be infelicitous.
Reference: [5] <author> Hastie, T, Simard, P & Sackinger, </author> <title> E (1995). Learning prototype models for tangent distance. This volume. </title>
Reference-contexts: Incorporating information about the tangents as well would encourage the separation of these segments. Care should be taken in generalising this picture to high dimensional spaces. The next section develops the theory behind variants of these systems (which is very similar to that in <ref> [5, 10] </ref>), and section 3 discusses how they perform. 2 Theory Linear auto-encoders embody a model in which variations from the mean of a population along certain directions are cheaper than along others, as measured by the log-unlikelihood of examples. Creating such a generative model is straightforward. <p> This method yielded marginally improved results on both the validation and test sets (Table 2). More adventurous tangent options, including using them in the clustering phase, were explored by <ref> [5, 10] </ref>. PCA models are not ideal as generative models of the data because they say nothing about how to generate values of components in the directions orthogonal to the pancake.
Reference: [6] <author> Hinton, GE, Dayan, P, Frey, BJ, Neal, </author> <title> RM (1995). The wake-sleep algorithm for unsu pervised neural networks. </title> <note> Submitted for publication. </note>
Reference-contexts: The generative weights of this autoencoder can be obtained using the technique of maximum likelihood factor analysis (which is closely related to PCA) and the resulting architecture and hierarchical variants of it can be formulated as real valued versions of the Helmholtz machine <ref> [3, 6] </ref>. The cost of coding the factors relative to their prior is implicitly included in this formulation, as is the possibility that different input pixels are subject to different amounts of noise.
Reference: [7] <author> Hinton, GE & Zemel, </author> <title> RS (1994). Autoencoders, minimum description length and Helmholtz free energy. </title> <editor> In JD Cowan, G Tesauro & J Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Auto-encoders can be viewed in terms of minimum description length descriptions of data in which the input-hidden weights produce a code for a particular case and the hidden-output weights embody a generative model which turns this code back into a close approximation of the original example <ref> [14, 7] </ref>. Code costs (under some prior) and reconstruction error (squared error assuming an isotropic Gaussian misfit model) sum to give the overall code length which can be viewed as a lower bound on the log probability density that the autoencoder assigns to the image.
Reference: [8] <author> Hull, </author> <month> JJ </month> <year> (1994). </year> <title> A database for handwritten text recognition research. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16, </volume> <pages> 550-554. </pages>
Reference-contexts: tt T to the covariance matrix the tangent vectors are never added to the means of the sub-classes. 3 Results We have evaluated the performance of the system on data from the CEDAR CDROM 1 database containing handwritten digits lifted from mail pieces passing through a United States Post Office <ref> [8] </ref>. We divided the br training set of binary Clustering Recognition Raw Errors None None 62 (3.10%) Heavy Light 29 (1.45%) Heavy None 45 (2.25%) Heavy Heavy 90 (4.50%) Table 1: Classification errors on the validation test when different weightings are used for the tangent vectors during clustering and recognition.
Reference: [9] <author> Kambhatla, N & Leen, </author> <title> TK (1994). Fast non-linear dimension reduction. </title> <editor> In JD Cowan, G Tesauro & J Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A similar idea for data compression was used by <ref> [9] </ref> where vector quantization was used to define sub-classes and PCA was performed within each sub-class (see also [2]). We used an iterative method based on the Expectation Maximisation (EM) algorithm [4] to fit mixtures of linear models.
Reference: [10] <author> Schwenk, H & Milgram, </author> <title> M (1995). Transformation invariant autoassociation with ap plication to handwritten character recognition. This volume. </title>
Reference-contexts: Incorporating information about the tangents as well would encourage the separation of these segments. Care should be taken in generalising this picture to high dimensional spaces. The next section develops the theory behind variants of these systems (which is very similar to that in <ref> [5, 10] </ref>), and section 3 discusses how they perform. 2 Theory Linear auto-encoders embody a model in which variations from the mean of a population along certain directions are cheaper than along others, as measured by the log-unlikelihood of examples. Creating such a generative model is straightforward. <p> This method yielded marginally improved results on both the validation and test sets (Table 2). More adventurous tangent options, including using them in the clustering phase, were explored by <ref> [5, 10] </ref>. PCA models are not ideal as generative models of the data because they say nothing about how to generate values of components in the directions orthogonal to the pancake.
Reference: [11] <author> Simard, P, Le Cun, Y & and Denker, </author> <title> J (1993). Efficient pattern recognition using a new transformation distance. </title> <editor> In SJ Hanson, JD Cowan & CL Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> 50-58. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For a given linear model, not counting the code cost implies that deformations of the images along the principal components for the sub-class are free. This is like the metric used by <ref> [11] </ref> except that they explicitly specified the directions in which deformations should be free, rather than learning them from the data. We wished to incorporate information about the preferred directions without losing the summarisation capacity of the local models, and therefore turned to the tangent prop algorithm [12]. <p> We tried a scheme in which the models were trained as described above, but tangent distance <ref> [11] </ref> was used during testing. This method yielded marginally improved results on both the validation and test sets (Table 2). More adventurous tangent options, including using them in the clustering phase, were explored by [5, 10].
Reference: [12] <author> Simard, P, Victorri, B, LeCun, </author> <title> Y & Denker, J (1992). Tangent Prop A formalism for specifying selected invariances in an adaptive network. </title> <editor> In JE Moody, SJ Hanson & RP Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These give rise to somewhat non-linear changes as measured in pixel space. Nearest neighbour methods were dramatically improved methods by defining a metric in which locally linearised versions of these transformations cost nothing <ref> [12] </ref>. In their tangent distance method, each training or test point is represented by a h-dimensional linear subspace, where each of these dimensions corresponds to a linear version of one of the transformations, and distances are measured between subspaces rather than points. <p> A priori knowledge that particular transformations are important can be incorporated using a version of the tangent-prop procedure <ref> [12] </ref>, which is equivalent in this case to adding in slightly transformed versions of the patterns. <p> We wished to incorporate information about the preferred directions without losing the summarisation capacity of the local models, and therefore turned to the tangent prop algorithm <ref> [12] </ref>. Tangent prop takes into account information about how the output of the system should vary locally with particular distortions of the input by penalising the system for having incorrect derivatives in the relevant directions.
Reference: [13] <author> Williams, CKI, Zemel, </author> <title> RS & Mozer, MC (1993). Unsupervised learning of object models. </title> <booktitle> In AAAI Fall 1993 Symposium on Machine Learning in Computer Vision, </booktitle> <pages> 20-24. </pages>
Reference-contexts: In effect a PCA encoder approximates the entire N dimensional distribution of the data with a lower dimensional Gaussian pancake <ref> [13] </ref>, choosing, for optimal data compression, to retain just a few of the PCs.
Reference: [14] <author> Zemel, </author> <title> RS (1993). A Minimum Description Length Framework for Unsupervised Learning. </title> <type> PhD Dissertation, </type> <institution> Computer Science, University of Toronto, Canada. </institution> <note> 1 This research was funded by the Ontario Information Technology Research Centre and NSERC. </note> <editor> We thank Patrice Simard, Chris Williams, Rob Tibshirani and Yann Le Cun for helpful discussions. </editor> <booktitle> Geoffrey Hinton is the Noranda Fellow of the Canadian Institute for Advanced Research. </booktitle>
Reference-contexts: Auto-encoders can be viewed in terms of minimum description length descriptions of data in which the input-hidden weights produce a code for a particular case and the hidden-output weights embody a generative model which turns this code back into a close approximation of the original example <ref> [14, 7] </ref>. Code costs (under some prior) and reconstruction error (squared error assuming an isotropic Gaussian misfit model) sum to give the overall code length which can be viewed as a lower bound on the log probability density that the autoencoder assigns to the image. <p> If instead of assuming an isotropic Gaussian within the pancake we use an ellipsoidal subspace, then we can can take into account the different variances along each of the h principal directions. This is akin to incorporating a code cost <ref> [14] </ref>. Similarly, the squared reconstruction error used in the basic scheme also assumes an isotropic distribution. Again a diagonal covariance matrix may be substituted.
References-found: 14

