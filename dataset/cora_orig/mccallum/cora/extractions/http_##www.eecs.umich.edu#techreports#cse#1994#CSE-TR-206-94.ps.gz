URL: http://www.eecs.umich.edu/techreports/cse/1994/CSE-TR-206-94.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse94.html
Root-URL: http://www.eecs.umich.edu
Email: Email: karent@eecs.umich.edu, sga@eecs.umich.edu  
Phone: Phone: (313)936-2917 Fax: (313)763-4617  
Title: Partitioning Regular Applications for Cache-Coherent Multiprocessors  
Author: Karen A. Tomko and Santosh G. Abraham 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: In all massively parallel systems (MPPs), whether message-passing or shared-address space, the memory is physically distributed for scalability and the latency of accessing remote data is orders of magnitude higher than the processor cycle time. Therefore, the programmer/compiler must not only identify parallelism but also specify the distribution of data among the processor memories in order to obtain reasonable efficiency. Shared-address MPPs provide an easier paradigm for programmers than message passing systems since the communication is automatically handled by the hardware and/or operating system. However, it is just as important to optimize the communication in shared-address systems if high performance is to be achieved. Since communication is implied by the data layout and data reference pattern of the application, the data layout scheme and data access pattern must be controlled by the compiler in order to optimize communication. Machine specific parameters, such as cache size and cache line size, describing the memory hierarchy of the shared-address space machine must be used to tailor the optimization of the application to the memory hierarchy of the MPP. This report focuses on a partitioning methodology to optimize application performance on cache-coherent multiprocessors. We give an algorithm for choosing block-cyclic partitions for scientific programs with regular data structures such as dense linear algebra applications and PDE solvers. We provide algorithms to compute the cache state on exiting a parallel region given the cache state on entry; and methods to compute the overall cache-coherency traffic and choose block-cyclic parameters to optimize cache-coherency traffic. Our approach is demonstrated on two applications. We show that the optimal partition computed by our algorithm matches the experimentally observed optimum and we show the effect of cache line size on partition performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Santosh G. Abraham and David E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to 22 reduce cache coherence traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):318--328, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: In addition to these projects there are some smaller groups that have also done work relevant to our own. Automatic data partitioning algorithms which minimize coherency traffic have also been developed by Hudak and Abraham <ref> [22, 1] </ref> and by Agarwal, Kranz and Natarajan [2]. Hudak and Abraham have developed automatic partitioning techniques for regular data-parallel loops with array accesses that have unit-coefficient linear subscripts. Agarwal, Kranz and Natarajan generate optimal block partitions for cache-coherent multiprocessors.
Reference: [2] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic partitioning of parallel loops for cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 2-11, </pages> <year> 1993. </year>
Reference-contexts: In addition to these projects there are some smaller groups that have also done work relevant to our own. Automatic data partitioning algorithms which minimize coherency traffic have also been developed by Hudak and Abraham [22, 1] and by Agarwal, Kranz and Natarajan <ref> [2] </ref>. Hudak and Abraham have developed automatic partitioning techniques for regular data-parallel loops with array accesses that have unit-coefficient linear subscripts. Agarwal, Kranz and Natarajan generate optimal block partitions for cache-coherent multiprocessors. They generalize the program model to handle any array index expressions that are affine functions of loop indices. <p> The data footprints for array references are calculated and combined to determine the cache usage. A partition is chosen which minimizes that footprint in the cache. An approximation is used to combine data footprints for references having different strides. Like <ref> [2] </ref>, we support array index expressions that are affine functions of loop indices, in addition we support block-cyclic data partitions which they do not handle. Heuristic techniques for automatic data partitioning have been developed as part of the PARADIGM compiler.
Reference: [3] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: CFT generation is performed in most compilers during parsing so we do not describe it here <ref> [3] </ref>. The second step of the algorithm performs two functions, pruning sequential branches and marking nodes as parallel or not parallel.
Reference: [4] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <year> 1993. </year>
Reference-contexts: they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF <ref> [32, 4] </ref>, Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
Reference: [5] <author> Daya Atapattu and Dennis Gannon. </author> <title> Building analytical models into an interactive performance prediction tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 521-530, </pages> <year> 1989. </year>
Reference-contexts: Similarly, we evaluate the performance of different partitioning schemes for an application program, and then determine which scheme is optimal by our criterion. We support a larger space of partitioning schemes than Balasundaram et al. who do not handle block-cyclic partitions. Atapattu and Gannon <ref> [5] </ref> have created a performance prediction tool for the Alliant FX/8, which has a single shared memory and shared cache. The tool performs assembly level analysis of vector and scalar code for the Alliant. The shared memory is modeled with a simple queuing model.
Reference: [6] <author> Vasanth Balasundaram, Geoffrey Fox, Ken Kennedy, and Ulrich Kremer. </author> <title> A static performance estimator to guide data prtitioning decisions. </title> <booktitle> In Proceedings of the 3rd ACM Sigplan Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 213-223, </pages> <year> 1991. </year>
Reference-contexts: Like our work the authors calculate the amount of data transferred between processors. However, PPPT supports only block data distributions, a subset of the block-cyclic partitions that we support. Balasundaram et al. <ref> [6] </ref> statically evaluate the performance of different partitioning schemes for 1 programs executed on distributed memory multiprocessors. They assume that a rectangular block data partitioning is specified and that communication primitives have been inserted in the code.
Reference: [7] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 41-53, </pages> <year> 1989. </year>
Reference-contexts: The following array summary techniques have been proposed: 2 Regions by Triolet et al [28], Linearization by Burke and Cytron [9], Atom Images by Li and Yew [26], and Data Access Descriptors by Balasundaram and Kennedy <ref> [7] </ref>. Our cyclic regular section descriptor is an extension to regular section descriptors used at Rice University for interprocedural analysis [10, 18] as part of the Fortran D project.
Reference: [8] <author> Eric Boyd, John-David Wellman, Santosh Abraham, and Edward Davidson. </author> <title> Evaluating the communication performance of MPPs using synthetic sparse matrix multiplication workloads. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 240-250, </pages> <year> 1993. </year>
Reference-contexts: A state diagram of the coherency protocol for the KSR1 is given in Figure 3. 3.1 KSR1 Architecture The Kendall Square Research KSR1 was used to evaluate our methods. We give a brief description of the architecture here, much of which has been taken from <ref> [31, 8] </ref>. The KSR1 is characterized by a hierarchical ring interconnection network and cache-only memory architecture. Each cell, consisting of a 20 megahertz processor, a 512 kilobyte subcache, and a 32 megabyte local cache, is connected to a unidirectional pipelined slotted ring.
Reference: [9] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of SIGPLAN '86 Symp. Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <month> june </month> <year> 1986. </year>
Reference-contexts: Array summary methods are methods to represent the set of array elements accessed within a region of code. The following array summary techniques have been proposed: 2 Regions by Triolet et al [28], Linearization by Burke and Cytron <ref> [9] </ref>, Atom Images by Li and Yew [26], and Data Access Descriptors by Balasundaram and Kennedy [7]. Our cyclic regular section descriptor is an extension to regular section descriptors used at Rice University for interprocedural analysis [10, 18] as part of the Fortran D project.
Reference: [10] <author> David Callahan and Ken Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: Our cyclic regular section descriptor is an extension to regular section descriptors used at Rice University for interprocedural analysis <ref> [10, 18] </ref> as part of the Fortran D project. We propose adding an additional parameter to the RSD notation in order to represent the subarray assigned to processor when a block cyclic data distribution is used.
Reference: [11] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 114-124, </pages> <year> 1992. </year>
Reference-contexts: A similar technique is used by Carr and Kennedy in <ref> [11] </ref>. Unfortunately, if there are unknows other that k in the bounds expressions then our technique may not work because we may be unable to perform CRSD combining operations. SIZEOF () produces a product of linear functions in k.
Reference: [12] <author> Marina Chen and Yu Hu. </author> <title> Optimizations for compiling iterative spatial loops to massively parallel machines. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-19, </pages> <year> 1992. </year>
Reference-contexts: states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal <ref> [25, 12] </ref>, and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own. <p> Figure 3 shows an example of a program that fits our model. Our program model encompasses a wider class of loops than presented by Chen and Hu in <ref> [12] </ref> or Abraham and Hudak [23], who assume that all loops are perfectly nested and that DO loops can not occur within DOALL nests.
Reference: [13] <author> Christine Eisenbeis, William Jalby, Daniel Windheiser, and Francois Bodin. </author> <title> A strategy for array management in local memory. </title> <type> Technical Report 1262, </type> <institution> Institut National de Recherche en Informa-tique et en Automatique, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: The tool performs assembly level analysis of vector and scalar code for the Alliant. The shared memory is modeled with a simple queuing model. The miss rate is supplied by the user but can be calculated using the reference window work described in <ref> [15, 13] </ref>. Our work performs source level analysis of parallelized code for the KSR1. Like the Alliant FX/8, the KSR1 is a shared memory machine; unlike the FX/8, processors in the KSR1 have private caches. We estimate coherency cache miss rate based on program analysis.
Reference: [14] <author> Thomas Fahringer and Hans Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 207-219, </pages> <year> 1993. </year>
Reference-contexts: Many researchers have developed tools for performance analysis of shared memory MPPs. We describe their work below, and the limitations of the methodologies for our problem. Fahringer and Zima <ref> [14] </ref> have developed a performance prediction tool (PPPT) which statically computes many parameters that can be used to calculate parallel program performance. Among the parameters that they calculate are the amount of data transferred between processors and number of uniprocessor cache misses.
Reference: [15] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The tool performs assembly level analysis of vector and scalar code for the Alliant. The shared memory is modeled with a simple queuing model. The miss rate is supplied by the user but can be calculated using the reference window work described in <ref> [15, 13] </ref>. Our work performs source level analysis of parallelized code for the KSR1. Like the Alliant FX/8, the KSR1 is a shared memory machine; unlike the FX/8, processors in the KSR1 have private caches. We estimate coherency cache miss rate based on program analysis.
Reference: [16] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> PARADIGM: A compiler for automatic data distribution on multicomputers. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 87-96, </pages> <year> 1993. </year>
Reference-contexts: exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal [25, 12], and PARADIGM <ref> [30, 16] </ref>. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
Reference: [17] <author> S.K.S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 301-305, </pages> <year> 1993. </year>
Reference-contexts: Message generation requires determining exactly what data must be communicated between processors and identifying the sending and receiving processors. Similarly, we need to determine the amount of data that must be communicated between processors for a given set of partition parameters. Gupta, et al. <ref> [17] </ref> provide closed form solutions for the generation of communication sets for distributed-memory machines. They use a virtual processor approach to find the communication sets for block-cyclic distributions. The communication sets are efficiently generated at run time for a user (or compiler) specified data distribution.
Reference: [18] <author> Paul Havlak and Ken Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <year> 1991. </year>
Reference-contexts: Our cyclic regular section descriptor is an extension to regular section descriptors used at Rice University for interprocedural analysis <ref> [10, 18] </ref> as part of the Fortran D project. We propose adding an additional parameter to the RSD notation in order to represent the subarray assigned to processor when a block cyclic data distribution is used. <p> CRSDs are a generalization of RSDs defined by Havlak and Kennedy in <ref> [18] </ref>. RSDs have the same format and meaning as the triplet notation used in FORTRAN 90. An RSD is comprised of three fields for each dimension of an array: a lower bound, an upper bound, and a stride. <p> A cyclic distribution assigns every pth element to the same processor. A block cyclic distribution divides the data space in to blocks and assigns every pth block to the same processor. We have extended the regular section descriptor defined in <ref> [18] </ref> to include a repeat field in order to represent block cyclic data distributions.
Reference: [19] <institution> High Performance Fortran Forum, CITI/CRPC, Box 1892, Rice University, </institution> <address> Houston, TX 77251. </address> <month> Draft: </month> <title> High Performance Fortran Language Specification, </title> <year> 1993. </year>
Reference-contexts: model is similar to ours, they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN <ref> [19] </ref>, Vienna FORTRAN [33], SUIF [32, 4], Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research.
Reference: [20] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <institution> Technical Report Rice COMP TR91-149, Rice University, Department of Computer Science, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Their coherency model is similar to ours, they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D <ref> [20, 21] </ref>, High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research.
Reference: [21] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 1-14, </pages> <year> 1992. </year>
Reference-contexts: Their coherency model is similar to ours, they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D <ref> [20, 21] </ref>, High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research.
Reference: [22] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compile-time optimization of near-neighbor communication for scalable shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15 </volume> <pages> 368-381, </pages> <year> 1992. </year>
Reference-contexts: In addition to these projects there are some smaller groups that have also done work relevant to our own. Automatic data partitioning algorithms which minimize coherency traffic have also been developed by Hudak and Abraham <ref> [22, 1] </ref> and by Agarwal, Kranz and Natarajan [2]. Hudak and Abraham have developed automatic partitioning techniques for regular data-parallel loops with array accesses that have unit-coefficient linear subscripts. Agarwal, Kranz and Natarajan generate optimal block partitions for cache-coherent multiprocessors.
Reference: [23] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compiling Parallel Loops for High Performance Computers: Partitioning, Data Assignment, and Remapping. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Figure 3 shows an example of a program that fits our model. Our program model encompasses a wider class of loops than presented by Chen and Hu in [12] or Abraham and Hudak <ref> [23] </ref>, who assume that all loops are perfectly nested and that DO loops can not occur within DOALL nests. <p> The partition parameters are p v ; p h thenumber of processors in the vertical and horizontal dimension respectively and v; h the block sizes in the vertical and horizontal dimensions. Our optimization process is a generalization of the work by Hudak and Abraham presented in <ref> [23] </ref>. The first stage of step five is simplification of the cost function. This includes simplifying some terms and eliminating others.
Reference: [24] <author> James Larus, Satish Chandra, and David Wood. CICO: </author> <title> A practical shared-memory programming performance model. </title> <type> Technical Report Report No. 1171, </type> <institution> University of Wisconson-Madison, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Our work performs source level analysis of parallelized code for the KSR1. Like the Alliant FX/8, the KSR1 is a shared memory machine; unlike the FX/8, processors in the KSR1 have private caches. We estimate coherency cache miss rate based on program analysis. Larus et al. <ref> [24] </ref> propose the check-in, check-out (CICO) performance model for cache-coherent shared-memory parallel computers. Similar to our work the authors estimate the cost of coherency traffic. However, they rely on user annotations not program analysis to determine the state of a cache block.
Reference: [25] <author> Jingke Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal <ref> [25, 12] </ref>, and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
Reference: [26] <author> Z. Li and P.-C. Yew. </author> <title> Efficient interprocedural analysis for program parallelization and restructuring. </title> <booktitle> In Proceedings of ACM/SIGPLAN Symp. Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pages 85-99, </pages> <month> july </month> <year> 1988. </year>
Reference-contexts: Array summary methods are methods to represent the set of array elements accessed within a region of code. The following array summary techniques have been proposed: 2 Regions by Triolet et al [28], Linearization by Burke and Cytron [9], Atom Images by Li and Yew <ref> [26] </ref>, and Data Access Descriptors by Balasundaram and Kennedy [7]. Our cyclic regular section descriptor is an extension to regular section descriptors used at Rice University for interprocedural analysis [10, 18] as part of the Fortran D project.
Reference: [27] <author> Wen mei W. Hwu, Thomas M. Conte, and Pohua P. Chang. </author> <title> Comparing software and hardware schemes for reducing the cost of branches. </title> <booktitle> In Proceedings of the Sixteenth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 224-233, </pages> <year> 1989. </year>
Reference: [28] <author> R.Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of SIGPLAN '86 Symp. Compiler Construction, </booktitle> <pages> pages 176-185, </pages> <month> june </month> <year> 1986. </year>
Reference-contexts: Array summary methods are methods to represent the set of array elements accessed within a region of code. The following array summary techniques have been proposed: 2 Regions by Triolet et al <ref> [28] </ref>, Linearization by Burke and Cytron [9], Atom Images by Li and Yew [26], and Data Access Descriptors by Balasundaram and Kennedy [7].
Reference: [29] <author> James E. Smith. </author> <title> A study of branch prediction strategies. </title> <booktitle> In Proceedings of the Eigth Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 135-148, </pages> <year> 1981. </year>
Reference: [30] <author> Ernesto Su, Daniel Palermo, and Prithviraj Banerjee. </author> <title> Automating parallelization of regular computations for distributed-memory multicomputers in the paradigm compiler. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 30-38, </pages> <year> 1993. </year>
Reference-contexts: exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF [32, 4], Crystal [25, 12], and PARADIGM <ref> [30, 16] </ref>. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
Reference: [31] <author> Daniel Windheiser, Eric Boyd, Eric Hao, Santosh Abraham, and Edward Davidson. </author> <title> KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 454-461, </pages> <year> 1993. </year>
Reference-contexts: A state diagram of the coherency protocol for the KSR1 is given in Figure 3. 3.1 KSR1 Architecture The Kendall Square Research KSR1 was used to evaluate our methods. We give a brief description of the architecture here, much of which has been taken from <ref> [31, 8] </ref>. The KSR1 is characterized by a hierarchical ring interconnection network and cache-only memory architecture. Each cell, consisting of a 20 megahertz processor, a 512 kilobyte subcache, and a 32 megabyte local cache, is connected to a unidirectional pipelined slotted ring. <p> = Exclusive Owner NO = Nonexclusive Owner Pread Pwrite Nread, R Nwrite Nread, R Pwrite Nwrite Nwrite Pwrite Pread Nread, R Pread Pread Nread, RSP Nread, RQP Nwrite Pwrite Notation N = Network P = Processor R = May Respond RQP = Request Path RSP = Response Path Reproduced from <ref> [31] </ref> system. The KSR architecture is diagramed in Figure 3.1. The size of a local cache subblock, called a subpage, is 128 bytes and serves as the unit of transfer between processors. Communication requests by any processor proceed around the ring in the direction of ring communication. <p> The input program may be 4 ALLCACHE Directory ALLCACHE Directory Local Cache Directory Local Cache Processor& Subcaches ALLCACHE Engine:1 ALLCACHE Engine:0 ALLCACHE Group:0 (AG:0) ALLCACHE Group:1 (AG:1) Local Cache Directory Local Cache Processor& Subcaches Reproduced from <ref> [31] </ref> Memory Component Memory Size Memory Access (Mbytes) Read (Cycles) Each Subcache 0.25 2 (1 per clock) Local Cache (existing page) 32.0 (allocated block) 23.4 (unallocated block) 49.2 Remote Cache 32.0 each (allocated page AE:0) (1024 total) 135-175 (allocated page AE:1) (34816 total) 470-600 Table 1: Memory Size and Read Access <p> (Mbytes) Read (Cycles) Each Subcache 0.25 2 (1 per clock) Local Cache (existing page) 32.0 (allocated block) 23.4 (unallocated block) 49.2 Remote Cache 32.0 each (allocated page AE:0) (1024 total) 135-175 (allocated page AE:1) (34816 total) 470-600 Table 1: Memory Size and Read Access Latencies of the KSR1 Reproduced from <ref> [31] </ref> 5 program JACOBI real a (n; m); b (n; m) align a (i; j) with b (i; j) do k = 1; 1000 doall j = 2; m 1 doall i = 2; n 1 a (i; j) = (b (i 1; j) + b (i + 1; j) +
Reference: [32] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN [33], SUIF <ref> [32, 4] </ref>, Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
Reference: [33] <author> H. Zima and B. Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> February </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: to ours, they model three states (idle, shared, and exclusive) and they do not model contention or conflict misses. 2.2 Compilation of Regular Applications There are several compiler projects for regular parallel applications, some of the most important projects are FORTRAN D [20, 21], High Performance FORTRAN [19], Vienna FORTRAN <ref> [33] </ref>, SUIF [32, 4], Crystal [25, 12], and PARADIGM [30, 16]. These groups have done work in data partitioning, interprocedural analysis and message generation which we draw on in our research. In addition to these projects there are some smaller groups that have also done work relevant to our own.
References-found: 33

