URL: http://vibes.cs.uiuc.edu/Publications/Papers/Goddard99.ps
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Email: fsimitci, reedg@cs.uiuc.edu  
Title: Adaptive Disk Striping for Parallel Input/Output  
Author: Huseyin Simitci Daniel A. Reed 
Address: Urbana, Illinois  
Affiliation: Department of Computer Science University of Illinois  
Abstract: As disk capacities continue to rise more rapidly than transfer rates, adaptive, redundant striping smoothly trades capacity for higher performance. We developed a fuzzy logic rule base for adaptive, redundant striping of files across multiple disks. This rule base is based on a queuing model of disk contention that includes file request sizes and disk hardware parameters. At low loads, the rule base stripes aggressively to minimize response time. As loads rise, it stripes less aggressively to maximize aggregate throughput. This adaptive striping rule base is incorporated into our second generation Portable Parallel File System (PPFS II). Experimental results showed that the analytical models of disk striping are capable of accurately predicting file system behavior. Also, it is shown that, depending on the access pattern, adaptive striping can double the input/output performance compared to striping with fixed distribution parameters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Holton and R. Das, XFS: </author> <title> A Next Generation Journalled 64-Bit Filesystem With Guaranteed Rate I/O. Silicon Graphics, </title> <publisher> Inc. </publisher>
Reference-contexts: 1 Introduction As new high-performance computing systems, achieving multi-teraflops and beyond quickly emerge, the performance of storage subsystems remains an obstacle to utilizing the full power of these systems. Moreover extant parallel file systems (e.g., SGI XFS <ref> [1] </ref> or IBM GPFS [2]) cannot deliver the full hardware input/output bandwidth to these parallel applications. <p> The fuzzy variables RequestRate and Request-Width, can have the fuzzy values given in Figure 13. RequestRate has values INFREQUENT, OCCASIONAL, FREQUENT, and CONTINUOUS to denote the rate of request arrivals. The rate will be scaled to be in the interval <ref> [0; 1] </ref> to make the rules portable to various systems. We also normalize Re-questWidth by the total number of devices, which makes RequestWidth to be in the interval [0; 1], also. Then, a RequestWidth of 1:0 denotes that all the devices are used. <p> The rate will be scaled to be in the interval <ref> [0; 1] </ref> to make the rules portable to various systems. We also normalize Re-questWidth by the total number of devices, which makes RequestWidth to be in the interval [0; 1], also. Then, a RequestWidth of 1:0 denotes that all the devices are used. With this definition, the fuzzy values in Figure 13 can be used for the fuzzy linguistic variable RequestWidth.
Reference: [2] <institution> IBM Corp., </institution> <month> GPFS: </month> <title> A Parallel File System, </title> <month> April </month> <year> 1998. </year>
Reference-contexts: 1 Introduction As new high-performance computing systems, achieving multi-teraflops and beyond quickly emerge, the performance of storage subsystems remains an obstacle to utilizing the full power of these systems. Moreover extant parallel file systems (e.g., SGI XFS [1] or IBM GPFS <ref> [2] </ref>) cannot deliver the full hardware input/output bandwidth to these parallel applications.
Reference: [3] <institution> NSF Terascale Computing Initiative, "Teras-cale and Petascale Computing: Digital Reality in the New Millenium." </institution> <note> http://rrbhpnt.asc.cise-nsf.gov/NSFReport.htm. </note>
Reference-contexts: Emerging distributed applications with time-varying input/output demands <ref> [3, 4] </ref> will exacerbate this situation. To support multi-teraflop applications manipulating multi-petabyte data sets, next-generation file systems will have to stripe data over thousands of secondary and tertiary storage devices [5, 6, 4].
Reference: [4] <author> P. H. Smith and J. V. Rosendale, </author> <title> "Data and Visualization Corridors," </title> <type> Tech. Rep. </type> <institution> CACR-164, CACR, CALTECH, </institution> <month> September </month> <year> 1998. </year>
Reference-contexts: Emerging distributed applications with time-varying input/output demands <ref> [3, 4] </ref> will exacerbate this situation. To support multi-teraflop applications manipulating multi-petabyte data sets, next-generation file systems will have to stripe data over thousands of secondary and tertiary storage devices [5, 6, 4]. <p> Emerging distributed applications with time-varying input/output demands [3, 4] will exacerbate this situation. To support multi-teraflop applications manipulating multi-petabyte data sets, next-generation file systems will have to stripe data over thousands of secondary and tertiary storage devices <ref> [5, 6, 4] </ref>. However, we have to balance the need to decrease transfer time using striping and the need to make multiple, independent transfers. Such a complex task will require file systems that can intelligently make adaptive file distribution decisions.
Reference: [5] <author> K. Salem and H. Garcia-Molina, </author> <title> "Disk Striping," </title> <booktitle> in Proceedings of the 2 nd International Conference on Data Engineering, </booktitle> <pages> pp. 336-342, </pages> <publisher> ACM, </publisher> <month> Feb. </month> <year> 1986. </year>
Reference-contexts: Emerging distributed applications with time-varying input/output demands [3, 4] will exacerbate this situation. To support multi-teraflop applications manipulating multi-petabyte data sets, next-generation file systems will have to stripe data over thousands of secondary and tertiary storage devices <ref> [5, 6, 4] </ref>. However, we have to balance the need to decrease transfer time using striping and the need to make multiple, independent transfers. Such a complex task will require file systems that can intelligently make adaptive file distribution decisions. <p> Finally, we conclude by presenting the directions of future work and summarizing the implications of adaptive disk striping policies in x8. 2 Related work Disk striping optimization. One way to remedy the performance difference between computing elements and storage devices is to stripe data across several storage devices <ref> [5] </ref>, effectively increasing the overall data throughput. This distribution technique is foundational to RAID systems [11], and striping file systems [12, 13]. But the effectiveness of this technique is dependent on the configuration of the storage system and the characteristics of the workloads using it.
Reference: [6] <author> T. Sterling, P. Messina, and P. H. Smith, </author> <title> Enabling Technologies for Petaflops Computing. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Emerging distributed applications with time-varying input/output demands [3, 4] will exacerbate this situation. To support multi-teraflop applications manipulating multi-petabyte data sets, next-generation file systems will have to stripe data over thousands of secondary and tertiary storage devices <ref> [5, 6, 4] </ref>. However, we have to balance the need to decrease transfer time using striping and the need to make multiple, independent transfers. Such a complex task will require file systems that can intelligently make adaptive file distribution decisions. <p> This work points out the importance of file specific striping tuning even in a shared-memory architecture. Redundant storage. Several researchers have observed that disk areal densities are increasing much faster than access latency times (seek plus rotation) are decreasing <ref> [19, 6] </ref>. This fact only exacerbates the lagging disk access times. This improvement difference implies some trade-offs. Namely, one can use the extra capacity to store copies of the data files redundantly.
Reference: [7] <author> J. V. Huber, C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal, </author> <title> "PPFS: A High-Performance Portable Parallel File System," </title> <booktitle> in Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pp. 385-394, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: These models will also allow prediction of the input/output behavior of peta-scale machines with hundreds of thousands of disks. Several studies of parallel file systems <ref> [7, 8] </ref> have shown the importance of matching underlying file system policies with the application's access patterns. Mismatched policies and access patterns can significantly reduce input/output performance. <p> This exploration builds atop our earlier work on physical and logical, input/output pattern comparisons [9, 10] and portable, parallel file systems <ref> [7] </ref>. It integrates real-time performance data, automatic access pattern classification, and fuzzy logic controls for choosing and configuring flexible policies. The foundation of the research is a prototype software library called PPFS II (Portable Parallel File System II). <p> Almost all parallel file systems provide users with some way to customize the file system policies. For example, Intel Paragon's PFS [31] and IBM SP2's PIOFS [32] allow users to dictate certain file distribution parameters such as striping widths and striping units. First generation PPFS (Portable Parallel File System) <ref> [7, 33, 34, 35, 36] </ref> is an input/output library, which is portable across parallel systems and workstation clusters. PPFS has a rich interface for application control of data placement and file system policies.
Reference: [8] <author> D. A. Reed, C. L. Elford, T. Madhyastha, W. H. Scullin, R. A. Aydt, and E. Smirni, </author> <title> "I/O, Performance Analysis, and Performance Data Immersion," </title> <booktitle> in Proceedings of MASCOTS '96, </booktitle> <pages> pp. 1-12, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: These models will also allow prediction of the input/output behavior of peta-scale machines with hundreds of thousands of disks. Several studies of parallel file systems <ref> [7, 8] </ref> have shown the importance of matching underlying file system policies with the application's access patterns. Mismatched policies and access patterns can significantly reduce input/output performance. <p> Input/output characterization. There are a number of studies that present models of physical disk [23, 24, 25] and disk-array [17] access behavior. Also, the logical and physical patterns of application input/output in parallel scientific applications have been studied extensively <ref> [9, 26, 8, 27, 10, 28] </ref>. These studies have shown that parallel applications exhibit a wide variety of input/output request patterns. Insights from these studies led to new, parallel file-system application programming interface (API) standardization efforts like the SIO API [29] and the MPI-IO API [30]. Flexible parallel file systems.
Reference: [9] <author> H. Simitci and D. A. Reed, </author> <title> "A Comparison of Logical and Physical Parallel I/O Patterns," </title> <journal> International Journal of High Performance Computing Applications, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 364-380, </pages> <year> 1998. </year>
Reference-contexts: This exploration builds atop our earlier work on physical and logical, input/output pattern comparisons <ref> [9, 10] </ref> and portable, parallel file systems [7]. It integrates real-time performance data, automatic access pattern classification, and fuzzy logic controls for choosing and configuring flexible policies. The foundation of the research is a prototype software library called PPFS II (Portable Parallel File System II). <p> Input/output characterization. There are a number of studies that present models of physical disk [23, 24, 25] and disk-array [17] access behavior. Also, the logical and physical patterns of application input/output in parallel scientific applications have been studied extensively <ref> [9, 26, 8, 27, 10, 28] </ref>. These studies have shown that parallel applications exhibit a wide variety of input/output request patterns. Insights from these studies led to new, parallel file-system application programming interface (API) standardization efforts like the SIO API [29] and the MPI-IO API [30]. Flexible parallel file systems.
Reference: [10] <author> E. Smirni and D. A. Reed, </author> <title> "Workload Characterization of Input/Output Intensive Parallel Applications," </title> <booktitle> in Proceedings of the 9th International Conference on Modelling Techniques and Tools for 13 Computer Performance Evaluation, </booktitle> <pages> pp. 169-180, </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1997. </year>
Reference-contexts: This exploration builds atop our earlier work on physical and logical, input/output pattern comparisons <ref> [9, 10] </ref> and portable, parallel file systems [7]. It integrates real-time performance data, automatic access pattern classification, and fuzzy logic controls for choosing and configuring flexible policies. The foundation of the research is a prototype software library called PPFS II (Portable Parallel File System II). <p> Input/output characterization. There are a number of studies that present models of physical disk [23, 24, 25] and disk-array [17] access behavior. Also, the logical and physical patterns of application input/output in parallel scientific applications have been studied extensively <ref> [9, 26, 8, 27, 10, 28] </ref>. These studies have shown that parallel applications exhibit a wide variety of input/output request patterns. Insights from these studies led to new, parallel file-system application programming interface (API) standardization efforts like the SIO API [29] and the MPI-IO API [30]. Flexible parallel file systems.
Reference: [11] <author> D. Patterson, P. Chen, G. Gibson, and R. H. Katz, </author> <title> "Introduction to Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> in Proceedings of IEEE Compcon, </booktitle> <pages> pp. 112-117, </pages> <month> Spring </month> <year> 1989. </year>
Reference-contexts: One way to remedy the performance difference between computing elements and storage devices is to stripe data across several storage devices [5], effectively increasing the overall data throughput. This distribution technique is foundational to RAID systems <ref> [11] </ref>, and striping file systems [12, 13]. But the effectiveness of this technique is dependent on the configuration of the storage system and the characteristics of the workloads using it. Cormen and Kotz [14] point out that asymptotically optimal disk I/O algorithms require flexible striping parameters.
Reference: [12] <author> J. H. Hartman and J. K. Ousterhout, </author> <title> "The Zebra Striped Network File System," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 13, </volume> <pages> pp. 274-310, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: One way to remedy the performance difference between computing elements and storage devices is to stripe data across several storage devices [5], effectively increasing the overall data throughput. This distribution technique is foundational to RAID systems [11], and striping file systems <ref> [12, 13] </ref>. But the effectiveness of this technique is dependent on the configuration of the storage system and the characteristics of the workloads using it. Cormen and Kotz [14] point out that asymptotically optimal disk I/O algorithms require flexible striping parameters.
Reference: [13] <author> P. C. Dibble, M. L. Scott, and C. S. Ellis, </author> <title> "Bridge: </title>
Reference-contexts: One way to remedy the performance difference between computing elements and storage devices is to stripe data across several storage devices [5], effectively increasing the overall data throughput. This distribution technique is foundational to RAID systems [11], and striping file systems <ref> [12, 13] </ref>. But the effectiveness of this technique is dependent on the configuration of the storage system and the characteristics of the workloads using it. Cormen and Kotz [14] point out that asymptotically optimal disk I/O algorithms require flexible striping parameters.
References-found: 13

