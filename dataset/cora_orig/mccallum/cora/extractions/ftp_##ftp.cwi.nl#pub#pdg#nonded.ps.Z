URL: ftp://ftp.cwi.nl/pub/pdg/nonded.ps.Z
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: pdg@cwi.nl  
Title: The Minimum Description Length Principle and Non-Deductive Inference  
Author: P. Grunwald 
Address: P.O. Box 4079, 1009 AB Amsterdam, The Netherlands  
Affiliation: CWI,  
Abstract-found: 0
Intro-found: 1
Reference: [ Goldszmidt et al., 1993 ] <author> M. Goldszmidt, P. Morris, and J. Pearl. </author> <title> A maximum entropy approach to nonmono-tonic reasoning. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3) </volume> <pages> 220-232, </pages> <year> 1993. </year>
Reference-contexts: We can now formally define the ME principle as: given outcome space O and constraints K, one should prefer the distribution P ME = arg max E [ log P (D)] (4) The Maximum Entropy Principle has seen several applications in AI-oriented uncertainty reasoning; see for example <ref> [ Goldszmidt et al., 1993; Grove et al., 1994 ] </ref> . <p> In this longer paper we will also discuss how MDL/ME can be used for logical theories and qualitative knowledge. Though we have neglected this fact in this short paper, it should be stressed here that this, too, is possible <ref> [ Goldszmidt et al., 1993; Paris and Vencovska, 1990 ] </ref> .
Reference: [ Grove et al., 1994 ] <author> A.J. Grove, J.Y. Halpern, and D. Koller. </author> <title> Random worlds and maximum entropy. </title> <journal> Journal of AI Research, </journal> <volume> 2 </volume> <pages> 33-88, </pages> <year> 1994. </year>
Reference-contexts: We can now formally define the ME principle as: given outcome space O and constraints K, one should prefer the distribution P ME = arg max E [ log P (D)] (4) The Maximum Entropy Principle has seen several applications in AI-oriented uncertainty reasoning; see for example <ref> [ Goldszmidt et al., 1993; Grove et al., 1994 ] </ref> . <p> Furthermore, one of its chief shortcomings is that it does not allow for learning <ref> [ Grove et al., 1994 ] </ref> . <p> Furthermore, one of its chief shortcomings is that it does not allow for learning [ Grove et al., 1994 ] . The constraints of form (3) allow one to express knowledge that permit several forms of plausible reasoning, among which abductive, but not inductive reasoning, as is discussed in <ref> [ Grove et al., 1994; Paris and Vencovska, 1990 ] </ref> . 4 MDL and ME It turns out that under quite general conditions, the MDL Principle permits the same inferences as the ME Principle.
Reference: [ Jaynes, 1978 ] <author> E.T. Jaynes. </author> <title> Where do we stand on maximum entropy? In R.D. </title> <editor> Levine and M. Tribus, editors, </editor> <booktitle> The Maximum Entropy Formalism, </booktitle> <pages> pages 15-118. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: Several other methods of statistical inference can be seen as special cases of the MDL Principle; we focus on one of them: The Maximum Entropy (ME) Principle. 3 The ME Principle and Reasoning under Uncertainty The ME Principle has been introduced by Jaynes <ref> [ Jaynes, 1978 ] </ref> as a general means of reasoning in the presence of uncertainty. <p> But also, every other code is worse than c fl on some data sequences: every code other than c fl has a worse worst-case behaviour than c fl . In the presence of constraints things get more interesting. Consider the famous `Brandeis Dice' example <ref> [ Jaynes, 1978 ] </ref> : we are given a six-sided die and the additional information that the expected outcome of throwing the die is 4.5 rather than the more familiar 3.5, i.e. we have the constraint set K = fE [X] = 4:5g.
Reference: [ Paris and Vencovska, 1990 ] <author> J.B. Paris and A. Ven-covska. </author> <title> A note on the inevitability of maximum entropy. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 4(3) </volume> <pages> 183-224, </pages> <year> 1990. </year>
Reference-contexts: The set of constraints K then consist of a number of such statements. The preferred models of a logical theory are arrived at by first determining P ME according to (4) and then picking the models with the highest P ME -probability. `Inevitability' of ME In <ref> [ Paris and Vencovska, 1990; Shore and Johnson, 1980 ] </ref> some theorems are proved which state that under certain, quite general assumptions, ME is the only consistent way to do plausible inference. <p> Furthermore, one of its chief shortcomings is that it does not allow for learning [ Grove et al., 1994 ] . The constraints of form (3) allow one to express knowledge that permit several forms of plausible reasoning, among which abductive, but not inductive reasoning, as is discussed in <ref> [ Grove et al., 1994; Paris and Vencovska, 1990 ] </ref> . 4 MDL and ME It turns out that under quite general conditions, the MDL Principle permits the same inferences as the ME Principle. <p> In this longer paper we will also discuss how MDL/ME can be used for logical theories and qualitative knowledge. Though we have neglected this fact in this short paper, it should be stressed here that this, too, is possible <ref> [ Goldszmidt et al., 1993; Paris and Vencovska, 1990 ] </ref> .
Reference: [ Rissanen, 1989 ] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: As such it has been applied numerous times in the fields of statistics and machine learning, generally with very good results. Its theoretical properties have been well-investigated (see for example the works by J. Rissanen, who introduced MDL in its present form <ref> [ Rissanen, 1989; 1996 ] </ref> ). <p> Specifically, if we want to explain our data in terms of a specific hypothesis, then the more such a hypothesis compresses the data, the better it explains it. Most of the work done on the MDL Principle is actually about the predictive MDL Principle <ref> [ Rissanen, 1989 ] </ref> in which a third, very important idea is added: if we learn something from a given set of data, then we may want to use it (the `something' we learned) to predict future data, i.e. data that is not known to us during the learning process but <p> We note here that most (according to Rissanen: all ) hypotheses that may ever be of interest to us can be cast in probabilistic terms, even if they do not explicitly refer to any probability at all. This perhaps somewhat surprising claim is quite forcefully defended in <ref> [ Rissanen, 1989 ] </ref> , chapter 2. Throughout this paper we assume that all our hypotheses are indeed probabilistic. The question is then how to arrive at a mathematically precise definition of the MDL Principle. <p> This is one of the facts which have made ME so controversial. From the M DL point of view, we look at probabilities neither as beliefs nor as frequencies, but rather as (things isomorphic to) codelengths. It can be shown <ref> [ Rissanen, 1989 ] </ref> that short codelengths (i.e. high probabilities) directly relate to good prediction capabilities: it is easy to prove that, if the given constraints on the probability distributions indeed hold, predicting outcomes using P MDL minimizes the worst-case prediction error. <p> Here `prediction error' is measured in terms of the logarithmic loss <ref> [ Rissanen, 1989 ] </ref> ; a low logarithmic loss implies a low square prediction error, a notion that may be more familiar to the reader. In the presence of the disjunctive constraint, it may be possible that the `true' distribution is such that E [X] = 4:5.
Reference: [ Rissanen, 1996 ] <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Trans. Inf. Theory, </journal> <volume> 1(42) </volume> <pages> 40-47, </pages> <year> 1996. </year>
Reference-contexts: Throughout this paper we assume that all our hypotheses are indeed probabilistic. The question is then how to arrive at a mathematically precise definition of the MDL Principle. Actually, a completely satisfactory solution to this problem has only been found very recently <ref> [ Rissanen, 1996 ] </ref> . The way it is defined there, the MDL Principle says the following: Suppose we are given a set of candidate hypotheses H for a set of possible data O. <p> ; D n will turn out to be, predicting according to P fl will yield a prediction error that is very close to the prediction error of the optimal H 2 H, i.e. the H that has lowest prediction error on D 1 ; : : : ; D n <ref> [ Rissanen, 1996 ] </ref> . In MDL hypothesis selection, P fl is used to deduce the single H 2 H that best explains the data. <p> MDL and Bayesian Statistics It turns out that MDL is closely related to Bayesian Statistics; in many contexts, MDL and Bayesian Statistics allow for the same inferences. But interestingly at least in its current form, <ref> [ Rissanen, 1996 ] </ref> - MDL avoids one of the central problems of Bayesian Statistics, i.e. the problem of how to choose a good prior distribution over all the candidate hypotheses. <p> This prior distribution turns out to be optimal in the sense that if it is used for prediction, it gives the lowest possible worst-case prediction error <ref> [ Rissanen, 1996 ] </ref> .
Reference: [ Shore and Johnson, 1980 ] <author> J.E. Shore and R.W. John-son. </author> <title> Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-26:26-37, </volume> <month> jan </month> <year> 1980. </year> [ <note> Spiegelhalter et al., 1993 ] D.J. Spiegelhalter, A.P. </note> <author> Dawid, S.L. Lauritzen, and R.G. Cowell. </author> <title> Bayesian analysis in expert systems (with discussion). </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 204-283, </pages> <year> 1993. </year>
Reference-contexts: The set of constraints K then consist of a number of such statements. The preferred models of a logical theory are arrived at by first determining P ME according to (4) and then picking the models with the highest P ME -probability. `Inevitability' of ME In <ref> [ Paris and Vencovska, 1990; Shore and Johnson, 1980 ] </ref> some theorems are proved which state that under certain, quite general assumptions, ME is the only consistent way to do plausible inference.
References-found: 7

