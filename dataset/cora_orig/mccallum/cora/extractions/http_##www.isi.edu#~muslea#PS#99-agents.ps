URL: http://www.isi.edu/~muslea/PS/99-agents.ps
Refering-URL: http://www.isi.edu/~muslea/papers.html
Root-URL: http://www.isi.edu
Email: robocup-sim@isi.edu  
Title: On being a teammate: Experiences acquired in the design of RoboCup teams.  
Author: Stacy Marsella, Jafar Adibi, Yaser Al-Onaizan, Gal A. Kaminka, Ion Muslea, Marcello Tallis,Milind Tambe 
Date: October 5, 1998  
Address: 4676 Admiralty Way Marina del Rey,CA 90292  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Abstract: Increasingly, multi-agent systems are being designed for a variety of complex, dynamic domains. Effective agent interactions in such domains raise some of most fundamental research challenges for agent-based systems, in teamwork, multi-agent learning and agent modeling. The RoboCup research initiative, particularly the simulation league of RoboCup, has been proposed to pursue such multi-agent research challenges, using the common testbed of simulation soccer. However, despite the significant popularity of RoboCup, researchers have often not extracted the general lessons learned from their participations in RoboCup. This is what we attempt to do here. We have fielded two teams, ISIS97 and ISIS98, in RoboCup competitions. These teams have been in the top four teams in these competitions. We compare the teams, and attempt to analyze and generalize the lessons learned. This analysis reveals several surprises, pointing out lessons for teamwork and for multi-agent learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Bersano-Begey, P. G. Kenny, and E. H. Durfee. </author> <title> Agent teamwork, adaptive learning, and adversarial planning in robocup using a prs architecture. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Another RoboCup effort focusing on explicit team plans and roles is <ref> [1] </ref>. They define levels of teamwork, starting at basic roles (which are static throughout the game), and building on top of those with formations and team plans for carrying out more complex tactics. These teamwork levels determine the agents coordination responsibilities and prioritize its actions.
Reference: [2] <author> S. Ch'ng and L. Padgham. </author> <title> Team description: Royal merlbourne knights. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Some researchers investigating teamwork in RoboCup have used explicit team plans and roles, but they have relied on domain-dependent communication and coordination. A typical example includes work by Chng and Padgham <ref> [2] </ref>. They present an elaborate analysis of roles in motivating teamwork and team plans. In this scheme, agents dynamically adopt and abandon roles in pre-defined tactics. The responsibilities and actions of each agent are determined by its current role in the current plan.
Reference: [3] <author> P. R. Cohen and H. J. Levesque. </author> <title> Teamwork. </title> <journal> Nous, </journal> <volume> 35, </volume> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Team operators constitute activities that the agent takes on as part of a team or subteam. ISIS97 and ISIS98 share the same general-purpose framework for teamwork modelling, STEAM [10]. STEAM models team members' responsibilities and joint commitments <ref> [3] </ref> in a domain-independent fashion. As a result, it enables team members to autonomously reason about coordination and communication, improving teamwork flexibility. The DEFEND-GOAL team operator demonstrates a part of STEAM. It is executed by the defender subteam.
Reference: [4] <author> H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. </author> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proceedings of the first international conference on autonomous agents, </booktitle> <year> 1997. </year>
Reference-contexts: To pursue research challenges such as these and stimulate research in multi-agents in general, the RoboCup research initiative has proposed simulation and robotic soccer as a common, unified testbed for multi-agent research <ref> [4] </ref> (www.robocup.org). The RoboCup initiative has proved extremely popular with researchers, with annual competitions in several different leagues. Of particular interest in this paper is the simulation league, which has attracted the largest number of participants.
Reference: [5] <author> H. Kitano, M. Tambe, P. Stone, S. Coradesci, H. Matsubara, M. Veloso, I. Noda, E. Osawa, and M. Asada. </author> <title> The robocup synthetic agents' challenge. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Of particular interest in this paper is the simulation league, which has attracted the largest number of participants. The stated research goals of the simulation league are to investigate the areas of multi-agent teamwork, agent modeling, and multi-agent learning <ref> [5] </ref>. Yet, the lessons learned by researchers participating in RoboCup, particularly the simulation league, have largely not been reported in a form that would be accessible to the research community at large. There are just a few notable exceptions [9].
Reference: [6] <author> S. Luke, Hohn C., J. Farris, G. Jackson, and J. Hendler. </author> <title> Co-evolving soccer softbot team coordination with genetic programming. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidel-berg, Germany, </address> <year> 1998. </year>
Reference: [7] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: With this approach, skills for different 3 modules (skills) within individual agents were learned separately, using differ-ent learning techniques. In particular, one of the skills, to pick a direction to shoot into the opponents' goal while avoiding opponents, was learned off-line using C4.5 <ref> [7] </ref>. Another skill, to intercept the ball, used a mix of off-line and on-line relied on reinforcement learning. One of the key surprises here was the degree to which individual agents specialized in their individual roles. <p> To address these problems, we decided to rely on automated, o*ine learning of the shooting rules. A human expert created a set of shooting situations, and selected the optimal shooting direction for each such situation. The learning system trained on these shooting scenarios. C4.5 <ref> [7] </ref> was used as the learning system, in part because it has the appropriate expressive power to express game situations and can handle both missing attributes and a large number of training cases.
Reference: [8] <author> A. S. Rao, A. Lucas, D. Morley, M. Selvestrel, and G. Murray. </author> <title> Agent-oriented architecture for air-combat simulation. </title> <type> Technical Report Technical Note 42, </type> <institution> The Australian Artificial Intelligence Institute, </institution> <year> 1993. </year>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [11, 8, 12] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information.
Reference: [9] <author> P. Stone and M. Veloso. </author> <title> Using decision tree confidence factors for multia-gent control. In RoboCup-97: The first robot world cup soccer games and conferences. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1998. </year>
Reference-contexts: Yet, the lessons learned by researchers participating in RoboCup, particularly the simulation league, have largely not been reported in a form that would be accessible to the research community at large. There are just a few notable exceptions <ref> [9] </ref>. <p> Our application learning in ISIS agents is similar to some of the other investigations of learning in RoboCup agents. For instance, Luke et al.[6] use genetic programming to build agents that learn to use their basic individual skills in coordination. Stone and Veloso <ref> [9] </ref> present a related approach, in which the agents learn a decision tree which enables them to select a receipient for a pass. 7 Lessons Learned from RoboCup Challenges of teamwork and multi-agent learning are critical in the design of multi-agent systems, and these are two of the critical research challenges
Reference: [10] <author> M. Tambe. </author> <title> Towards flexible teamwork. </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> 7 </volume> <pages> 83-124, </pages> <year> 1997. </year>
Reference-contexts: The analysis does reveal several general lessons in the areas of teamwork and multi-agent learning. With respect to teamwork, in the past, we have reported on our ability to reuse STEAM, a general model of teamwork, in RoboCup <ref> [10] </ref>. This paper takes a step further, evaluating the effectiveness of STEAM in RoboCup, to improve our understanding of the utility of general teamwork models. It also provides an analysis of techniques for the division of team responsibilities among individuals. <p> The hierarchy has two types of operators: Individual operators represent goals/plans that the player makes and executes as an individual. Team operators constitute activities that the agent takes on as part of a team or subteam. ISIS97 and ISIS98 share the same general-purpose framework for teamwork modelling, STEAM <ref> [10] </ref>. STEAM models team members' responsibilities and joint commitments [3] in a domain-independent fashion. As a result, it enables team members to autonomously reason about coordination and communication, improving teamwork flexibility. The DEFEND-GOAL team operator demonstrates a part of STEAM. It is executed by the defender subteam. <p> In this way, agents coordinate their defense of the goal. All the communication decisions are handled automatically by STEAM. 4 Analysis of Teamwork 4.1 Lessons in (Re)using a Teamwork Model In past work, we have focused on STEAM's reuse in our ISIS teams <ref> [10] </ref>, illustrating that a significant portion was reused, and that it enabled reduced development time. The use of the teamwork model is a shared similarity between ISIS97 and ISIS98. However, a key unresolved issue is measuring 6 the contribution of STEAM to ISIS's performance.
Reference: [11] <author> M. Tambe, W. L. Johnson, R. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb. </author> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine, </journal> <volume> 16(1), </volume> <month> Spring </month> <year> 1995. </year>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [11, 8, 12] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information. <p> The lower-level does not make any decisions. Instead, all decision-making rests with the higher level, implemented in the Soar integrated AI architecture <ref> [11] </ref>. Once the Soar-based higher-level reaches a decision, it communicates with the lower-level, which then sends the relevant action information to the simulator. Soar's operation involves dynamically executing an operator (reactive plan) hierarchy. The operator hierarchy shown in Figure 1 illustrates a portion of the operator hierarchy for ISIS player-agents.
Reference: [12] <author> M. Williamson, K. Sycara, and K. Decker. </author> <title> Executing decision-theoretic plans in multi-agent environments. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Plan Execution: Problems and Issues, </booktitle> <month> November </month> <year> 1996. </year> <month> 18 </month>
Reference-contexts: For each of these research problems, the uncertainty and the presence of multiple cooperative and non-cooperative agents, only conspires to exacerbate the difficulty. Consider for instance the challenge of multi-agent teamwork, which has become a critical requirement across a wide range of multi-agent domains <ref> [11, 8, 12] </ref>. Here, an agent team must address the challenge of designing roles for individuals (i.e., dividing up team responsibilities based on individuals' capabilities), doing so with fairness, and reorganizing roles based on new information.
References-found: 12

