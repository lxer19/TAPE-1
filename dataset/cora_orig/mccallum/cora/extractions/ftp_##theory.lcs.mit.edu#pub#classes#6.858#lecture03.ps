URL: ftp://theory.lcs.mit.edu/pub/classes/6.858/lecture03.ps
Refering-URL: http://theory.lcs.mit.edu/~mona/lectures.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Outline Review of Winnow Algorithm Perceptrons VC-dimension and Mistake Bounds 3.1 Review of Winnow Algorithm
Author: Lecturer: Ron Rivest Scribe: Ping Huang 
Date: 3 September 14, 1994  
Note: 6.858/18.428 Machine Learning Lecture  The  It can be shown that O(k lg n) is the best possible mistake bound on this class of  3-1  
Abstract: We established in the last lecture that the Winnow algorithm [2] can learn the concept class of monotone disjunctions C k = fx i 1 _ x i 2 _ : : : _ x i k g in an on-line prediction model, making O(k lg n) mistakes when there are a total of n input variables. Note that the existence of such a mistakes bound does not guarantee that the learner ever successfully converges to exactly the right concept. The Winnow algorithm as we outlined it can only deal with a very restricted set of problems: monotone disjunctions of k variables. How can we extend it? One relatively simple extension might be to learn arbitrary disjunctions of k variables. We could do this by having 2n input variables for Winnow such that for i = 1 : : : n, x i 0 = x i and x 2i 0 = x i . Another more complex extension might be to learn the concept class k-DNF (disjunctive normal form with k literals or fewer per term); we can accomplish this by taking as input variables for the Winnow algorithm the O(n k ) members of the set of all such possible conjunctions. Various other transformations exist to apply Winnow to other classes of functions. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Avrim Blum, </author> <title> "Learning Boolean Functions in an Infinite Attribute Space". </title> <booktitle> Machine Learning, </booktitle> <month> October </month> <year> 1992, </year> <month> 9:4:373-386. </month>
Reference: [2] <author> Nick Littlestone: </author> <title> "Learning quickly when irrelevant attributes abound: A new linear-threshhold algorithm". </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference: [3] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: The perceptron training procedure is due to Frank Rosenblatt [4], and the proof of convergence we give is adapted from a proof given in Minsky and Papert's book Perceptrons <ref> [3] </ref>. First, we make some assumptions necessary to the proof. Assumption 1 9w; fi such that w; fi can strictly (i.e., w x &gt; fi, not just ) classify all examples correctly. Assumption 2 fi = 0 (without loss of generality). Justification: a non-zero fi is just a bias.
Reference: [4] <author> F. Rosenblatt: </author> <title> "The Perceptron: A probabilistic model for information storage and organization in the brain". </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-407, </pages> <year> 1958. </year> <note> Article reprinted in Neurocomputing (MIT Press, </note> <year> 1988). </year>
Reference-contexts: The perceptron training procedure is due to Frank Rosenblatt <ref> [4] </ref>, and the proof of convergence we give is adapted from a proof given in Minsky and Papert's book Perceptrons [3]. First, we make some assumptions necessary to the proof.
References-found: 4

