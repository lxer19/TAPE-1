URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92207-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Relaxing SIMD Control Flow Constraints using Loop Transformations  
Author: Reinhard v. Hanxleden Ken Kennedy 
Note: From the Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, San Fran-cisco, CA, 1992.  
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Many loop nests in scientific codes contain a paralleliz-able outer loop but have an inner loop for which the number of iterations varies between different iterations of the outer loop. When running this kind of loop nest on a SIMD machine, the SIMD-inherent restriction to single program counter common to all processors will cause a performance degradation relative to comparable MIMD implementations. This problem is not due to limited parallelism or bad load balance, it is merely a problem of control flow. This paper presents a loop transformation, which we call loop flattening, that overcomes this limitation by letting each processor advance to the next loop iteration containing useful computation, if there is such an iteration for the given processor. We study a concrete example derived from a molecular dynamics code and compare performance results for flattened and unflat-tened versions of this kernel on two SIMD machines, the CM-2 and the DECmpp 12000. We then evaluate loop flattening from the compiler's perspective in terms of applicability, cost, profitability, and safety. We conclude with arguing that loop flattening, whether performed by the programmer or by the compiler, introduces negligible overhead and can significantly improve the performance of scientific codes for solving irregular problems. fl This research was supported by the Center for Research on Parallel Computation, a National Science Foundation Science and Technology Center. Use of the DECmpp 12000 was provided by the Center for Research on Parallel Computation under NSF Cooperative Agreement No. CCR-8809615 with support from Digital Equipment Corporation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Bemmerl, A. Bode, O. Hansen, and T. Ludwig. </author> <title> A testbed for dynamic loadbalancing on distributed memory multiprocessors. </title> <note> PUMA Working Paper 14, </note> <institution> Technical University Munich, Munchen, Germany, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Load balancing is a difficult problem in itself which has been frequently addressed in the literature <ref> [1, 9, 11] </ref>. Once this problem is solved, we can usually expect good performance when running such a loop nest on a shared-memory or distributed-memory MIMD (Multiple Instruction, Multiple Data) machine.
Reference: [2] <author> H. Berryman, J. Saltz, W. Gropp, and R. Mirchandaney. </author> <title> Krylov methods preconditioned with incompletely factored matrices on the CM-2. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 186-190, </pages> <year> 1990. </year>
Reference-contexts: many scientific programs solving 5 DO At 1 = 1, N DO pr = 1, pCnt (At 1 ) At 2 = partners (At 1 , pr) F (At 1 ) = F (At 1 ) + Force (At 1 , At 2 ) ENDDO ENDDO calculation NBFORCE. irregular problems <ref> [2, 19, 22, 23] </ref>. One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind [6, 7, 10].
Reference: [3] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model <ref> [3] </ref>. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set [22].
Reference: [4] <author> T. Braunl. </author> <title> Structured SIMD programing in Parallaxis. </title> <journal> Structured Programming, </journal> <volume> 10(3) </volume> <pages> 121-132, </pages> <year> 1989. </year>
Reference-contexts: However, this kind of loop nest causes special problems for SIMD (Single Instruction, Multiple Data) architectures because of the restricted control flow on these machines <ref> [4] </ref>. If the number of iterations of the inner loops varies from one outer loop iteration to the next, then the restriction to a common program counter makes a nave SIMD implementation inefficient.
Reference: [5] <author> P. Christy. </author> <title> Virtual processors considered harmful. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Therefore, using the L 1 u loops does not automatically 8 superoxide dismutase molecule, using different cutoff radii. result in savings by reducing the number of processed layers; however, we have to pay the additional overhead of checking on each layer whether it is active <ref> [5] </ref>. This overhead is saved in the L 2 u version. 5.4 The input data We ran our test case for the bovine superoxide dis-mutase molecule (SOD ), which has N = 6968 atoms.
Reference: [6] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind <ref> [6, 7, 10] </ref>.
Reference: [7] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization strategies for a molecular dynamics program. In Intel Supercomputer University Partners Conference, </title> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind <ref> [6, 7, 10] </ref>.
Reference: [8] <author> G. C. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: We will give program examples in different variants of pseudo Fortran: F77 Strictly sequential Fortran 77 (possibly a "dusty deck" program). F77D - F77 enhanced with distribution statements as proposed in Fortran D <ref> [8] </ref> and High Performance Fortran [12]. An important goal of F77D is to provide a basis for efficient compilation towards both MIMD and SIMD distributed memory machines, so it should not contain any constructs which are specific to either architecture.
Reference: [9] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Multiprocessors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Load balancing is a difficult problem in itself which has been frequently addressed in the literature <ref> [1, 9, 11] </ref>. Once this problem is solved, we can usually expect good performance when running such a loop nest on a shared-memory or distributed-memory MIMD (Multiple Instruction, Multiple Data) machine.
Reference: [10] <author> W. F. van Gunsteren and H. J. C. Berendsen. GRO-MOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind <ref> [6, 7, 10] </ref>.
Reference: [11] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: Load balancing is a difficult problem in itself which has been frequently addressed in the literature <ref> [1, 9, 11] </ref>. Once this problem is solved, we can usually expect good performance when running such a loop nest on a shared-memory or distributed-memory MIMD (Multiple Instruction, Multiple Data) machine.
Reference: [12] <institution> Proceedings of the High Performance Fortran Forum, Hous-ton, TX, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: We will give program examples in different variants of pseudo Fortran: F77 Strictly sequential Fortran 77 (possibly a "dusty deck" program). F77D - F77 enhanced with distribution statements as proposed in Fortran D [8] and High Performance Fortran <ref> [12] </ref>. An important goal of F77D is to provide a basis for efficient compilation towards both MIMD and SIMD distributed memory machines, so it should not contain any constructs which are specific to either architecture.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: A sufficient condition is that the loop into which we lift an inner loop body can be parallelized, which might be hard to detect, especially if indirect addressing occurs. However, this is already a necessary condition for parallelizing loops in general, and therewith a standard problem for parallelizing compilers <ref> [13] </ref>. The same technology developed there can be applied here.
Reference: [14] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: When safety is ensured, either by user information (like a FORALL loop header) or by "heroic dependence analysis," we expect that the systematic loop flattening transformation, as described in Section 4, can be implemented efficiently into compilers like the Fortran D compiler in the ParaScope programming environment <ref> [14] </ref>. 7 Related Work The restricted control flow of pure SIMD programming has been addressed by several researchers. Philippsen and Tichy introduce two variants of a FORALL statement, a synchronous version and an asynchronous one [17]. The asynchronous FORALL allows multiple threads of control to coexist.
Reference: [15] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Furthermore, this potential waste of processing time can not only occur for small arrays, but it is encountered whenever array sizes are not exact multiples of Gran <ref> [15] </ref>. On the CM-2, using the slicewise compiler results in Gran = P fl 4=32 = P=8 (32 processors per FPA, vector length 4); i.e., we can economically use arrays whose total sizes are arbitrary multiples of P=8.
Reference: [16] <institution> MasPar Computer Corporation, Sunnyvale, CA. MasPar Fortran Reference Manual, </institution> <year> 1991. </year>
Reference-contexts: F77 MIMD A Fortran 77 version to run on a MIMD machine, which assumes a separate name space for each processor. F90 SIMD A Fortran 90 version to run on a SIMD machine, similar to Connection Machine Fortran [21] or MasPar Fortran <ref> [16] </ref>.
Reference: [17] <author> M. Philippsen and W. F. Tichy. </author> <title> Modula-2 fl and its compilation. </title> <booktitle> In First International Conference of the Aus-trian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Philippsen and Tichy introduce two variants of a FORALL statement, a synchronous version and an asynchronous one <ref> [17] </ref>. The asynchronous FORALL allows multiple threads of control to coexist. This can either be emulated using stacks of MASK bits, or it can be implemented directly in an MSIMD machine which contains multiple program counters.
Reference: [18] <author> C. D. Polychronopoulos. </author> <title> Loop coalescing: A compiler transformation for parallel machines. </title> <editor> In S. Sahni, editor, </editor> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1987. </year> <institution> Pennsylvania State University Press. </institution>
Reference-contexts: Loop flattening bears similarities to loop coalescing in that it also manipulates loop control flow, but it is very different in its motivation and final outcome <ref> [18] </ref>. Loop coalescing merges iteration variables to achieve a higher degree of parallelism and to allow a more flexible distribution of inner loop iterations among the processors. Although loop flattening can also simplify load balancing, the transformation per se does not change which loop iterations a processor executes.
Reference: [19] <author> J. Saltz, S. Petiton, H. Berryman, and A. Rifkin. </author> <title> Performance effects of irregular communication patterns on massively parallel multicomputers. </title> <type> ICASE Report 91-12, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: many scientific programs solving 5 DO At 1 = 1, N DO pr = 1, pCnt (At 1 ) At 2 = partners (At 1 , pr) F (At 1 ) = F (At 1 ) + Force (At 1 , At 2 ) ENDDO ENDDO calculation NBFORCE. irregular problems <ref> [2, 19, 22, 23] </ref>. One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind [6, 7, 10].
Reference: [20] <author> J. Shen and J. A. McCammon. </author> <title> Molecular dynamics simulation of Superoxide interacting with Superoxide Dismutase. </title> <journal> 12 Chemical Physics, </journal> <volume> 158 </volume> <pages> 191-198, </pages> <year> 1991. </year>
Reference-contexts: For atom i, the atoms close enough to i are pre-computed into an array partners (i; 1:pCnt (i)). This precomputation can be quite expensive in itself and is usually done only every k simulation steps, where k = 10 is one common value <ref> [20] </ref>. code can be parallelized by partitioning the set of all atoms into P disjoint subsets and assigning one subset to each processor p. To achieve load balancing, the sum over the number of the partners of the atoms in a processor's subset should be roughly equal across the processors. <p> SOD is a catalytic enzyme composed of two identical subunits, each with 151 amino-acid residues and two metal atoms <ref> [20] </ref>. interaction partners, pCnt max and pCnt ave , which indicate the computational workloads for different cutoff radii. As expected, both values increase cubicly with the cutoff radius.
Reference: [21] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, </note> <year> 1991. </year>
Reference-contexts: F77 MIMD A Fortran 77 version to run on a MIMD machine, which assumes a separate name space for each processor. F90 SIMD A Fortran 90 version to run on a SIMD machine, similar to Connection Machine Fortran <ref> [21] </ref> or MasPar Fortran [16].
Reference: [22] <author> S. Tomboulian and M. Pappas. </author> <title> Indirect addressing and load balancing for faster solutions to the Mandelbrot set on SIMD architectures. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 443-450, </pages> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: many scientific programs solving 5 DO At 1 = 1, N DO pr = 1, pCnt (At 1 ) At 2 = partners (At 1 , pr) F (At 1 ) = F (At 1 ) + Force (At 1 , At 2 ) ENDDO ENDDO calculation NBFORCE. irregular problems <ref> [2, 19, 22, 23] </ref>. One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind [6, 7, 10]. <p> Loop flattening can also be used to process multiple array segments of different lengths per processor, as introduced in Blelloch's V-RAM model [3]. Thus it can be viewed as a generalization of substituting direct addressing with indirect addressing as Tomboulian and Pappas did for computing the Mandelbrot set <ref> [22] </ref>. Loop flattening bears similarities to loop coalescing in that it also manipulates loop control flow, but it is very different in its motivation and final outcome [18].
Reference: [23] <author> M. Willebeek-LeMair and A. P. Reeves. </author> <title> Solving nonuniform problems on SIMD computers: Case study on region growing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 135-149, </pages> <year> 1990. </year>
Reference-contexts: If the number of iterations of the inner loops varies from one outer loop iteration to the next, then the restriction to a common program counter makes a nave SIMD implementation inefficient. As observed in a case study implementing an image processing algorithm on the Massively Parallel Processor <ref> [23, page 143] </ref>: "... the complexity of each iteration in the SIMD environment is dominated by the largest region in the image. <p> many scientific programs solving 5 DO At 1 = 1, N DO pr = 1, pCnt (At 1 ) At 2 = partners (At 1 , pr) F (At 1 ) = F (At 1 ) + Force (At 1 , At 2 ) ENDDO ENDDO calculation NBFORCE. irregular problems <ref> [2, 19, 22, 23] </ref>. One example is the GROMOS molecular dynamics program, which contains several interesting kernels of this kind [6, 7, 10].
References-found: 23

