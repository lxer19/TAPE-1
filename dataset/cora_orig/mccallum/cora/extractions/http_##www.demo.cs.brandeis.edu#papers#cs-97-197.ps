URL: http://www.demo.cs.brandeis.edu/papers/cs-97-197.ps
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: -pablo,sklar,hugues,pollack-@cs.brandeis.edu  
Title: The Internet as a Virtual Ecology: Coevolutionary Arms Races Between Human and Artificial Populations  
Author: Pablo Funes, Elizabeth Sklar, Hugues Juill and Jordan Pollack 
Address: 415 South St., Waltham MA 02254 USA  
Affiliation: Volen Center for Complex Systems Brandeis University  
Date: 1  
Pubnum: CS-97-197  COMPUTER SCIENCE TECHNICAL REPORT CS-97-197  
Abstract: In this paper, we propose that learning complex behaviors can be achieved in a coevolutionary environment where one population consists of the human users of an interactive adaptive software tool and the opposing population is artificial, generated by a coevolutionary learning engine. We take advantage of the Internet, a connected community where people and software coexist. A new kind of adaptive agent can exploit its interactions with thousands of usersinside a virtual nicheto learn in a coevolutionary human-robot arms race. Our model is Tron, a simple dynamic game where introspective self-play quickly leads to collusive stagnation. We describe an application where thousands of small programs are sent to play with people through the Java interpreter running in their web browsers. The feedback provided by these agents is collected in our server and used to augment an ever improving fitness landscape for local robot-robot games. Speciation and fitness sharing provide diversity to challenge humans with a variety of different strategies. In this way, we obtain an evolving environment where human as well as artificial adaptation are simultaneously taking place. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Axelrod, R. M. </author> <title> (1984) The Evolution of Cooperation. </title> <address> New York, </address> <publisher> Basic Books. </publisher>
Reference-contexts: This technique protects the species (it is not surprising that 5 out of the 10 best players from its generation behave similarly) while trying to inict damage on other strategies. The Tit-for-Tat strategy described by Axelrod <ref> [1] </ref> for the Iterated Prisoners Dilemma (IPD) game uses a similar approach. It maintains cooperation for as long as the other player does. But it retaliates after a defection, defecting as well.
Reference: [2] <author> Beasley, D., Bull, D. R. and Martin, R. R. </author> <year> (1993). </year> <title> A sequential niche technique for multimodal function optimization. </title> <booktitle> Evolutionary Computation 1(2). </booktitle> <pages> 101-125. </pages>
Reference-contexts: To start a new generation, the 100 current robots are sorted by fitness. The worst 10 are eliminated and replaced by 10 fresh robots, supplied by the background process. A new generation begins. The fitness of robots is a shared fitness measure designed to promote speciation <ref> [2, 9] </ref> by giving points for doing better than average against a human player, and negative points for doing worse than average.
Reference: [3] <author> Darwen, P. J. </author> <title> (1996) Co-evolutionary Learning by Automatic Modularisation with Specia-tion. </title> <institution> University of New South Wales, </institution> <year> 1996. </year>
Reference: [4] <author> Floreano, D. and Mondada, F. </author> <year> (1994). </year> <title> Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural Network Driven Robot. </title> <editor> In D. Cliff, P. Husbands, J.-A. Meyer, and S. Wilson (Eds.), </editor> <booktitle> From Animals to Animats III, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Robots provided they are reliable enough can run repeated trials of the same experiment over a long time in order to learn using evolutionary computation techniques. Floreano and Mondada <ref> [4, 5] </ref> run their robots for several days in order to evolve controllers for basic tasks.
Reference: [5] <author> Floreano, D. and Mondada, F. </author> <title> Evolution of Homing Navigation in a Real Mobile Robot. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics--Part B: Cybernetics, </journal> <volume> 26(3), </volume> <pages> 396-407, </pages> <year> 1996. </year>
Reference-contexts: Robots provided they are reliable enough can run repeated trials of the same experiment over a long time in order to learn using evolutionary computation techniques. Floreano and Mondada <ref> [4, 5] </ref> run their robots for several days in order to evolve controllers for basic tasks.
Reference: [6] <author> Funes, </author> <title> Pablo (1996). The Tron Game: An experiment in Artificial Life and Evolutionary Techniques. </title> <note> Unpublished. </note>
Reference-contexts: Every sensor returns a maximum value of 1 for an immediate obstacle (i.e. a wall in an adjacent pixel), a lower number for an obstacle further away, and 0 when there are no walls in sight. CS-97-197 6 2.3 Learning Tron by Self-Play In earlier exploratory experiments <ref> [6] </ref>, we used a genetic algorithm to learn the weights of a perceptron network to play tron. It became evident that while this simple architecture is capable of coding players that could perform interestingly when facing human opponents, such good weights were difficult to find in evolutionary or coevolutionary scenarios.
Reference: [7] <author> Goldberg, David E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: calls for three basic ingredients: (a) a representation that is capable of encoding each candidate solution, (b) mutation and crossover operators that can be applied to such a representation, and (c) a fitness function that is the standard measure against which to test each candidate solution during the iterative process <ref> [8, 7] </ref>. In natural as in artificial evolution, a population moves toward fitness optimality while maintaining variation over all the dimensions of the genetic space, including those dimensions that are not being selected.
Reference: [8] <author> Holland, John H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: calls for three basic ingredients: (a) a representation that is capable of encoding each candidate solution, (b) mutation and crossover operators that can be applied to such a representation, and (c) a fitness function that is the standard measure against which to test each candidate solution during the iterative process <ref> [8, 7] </ref>. In natural as in artificial evolution, a population moves toward fitness optimality while maintaining variation over all the dimensions of the genetic space, including those dimensions that are not being selected.
Reference: [9] <author> Juill, H. and Pollack, J. </author> <title> (1996) Dynamics of Co-evolutionary Learning. </title> <booktitle> Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: To start a new generation, the 100 current robots are sorted by fitness. The worst 10 are eliminated and replaced by 10 fresh robots, supplied by the background process. A new generation begins. The fitness of robots is a shared fitness measure designed to promote speciation <ref> [2, 9] </ref> by giving points for doing better than average against a human player, and negative points for doing worse than average.
Reference: [10] <author> Juill, H. and Pollack, J. </author> <year> (1996). </year> <title> Co-evolving Intertwined Spirals. </title> <booktitle> in Proceedings of the Fifth Annual Conference on Evolutionary Programming, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Neural networks are thought to have generalization capabilities [3 ch. 3, 23], successfully inducing, for example, a good backgammon player from a set of suggested moves [21]. Supporters of the Genetic Programing paradigm [11] will suggest that this may be the case for GP as well [17]. In <ref> [10] </ref>, Juill and Pollack argue that the dynamics of coevolutionary fitness help them get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern.
Reference: [11] <author> Koza, John R. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Neural networks are thought to have generalization capabilities [3 ch. 3, 23], successfully inducing, for example, a good backgammon player from a set of suggested moves [21]. Supporters of the Genetic Programing paradigm <ref> [11] </ref> will suggest that this may be the case for GP as well [17]. In [10], Juill and Pollack argue that the dynamics of coevolutionary fitness help them get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern. <p> This form of collusion is a frequent suboptimal equilibrium that prevents learning robot strategies by self-play in a coevolutionary arms race. CS-97-197 7 2.4 GP representation for Tron Robots In the present study, we use Genetic Programming (GP) <ref> [11] </ref> as a means for coding artificial Tron players. The set of terminals is -_A,_B,..., _H (the eight sensors) and (random constants between 0 and 1)-. The functions are -+, -, * (arithmetic operations),% (safe division), IFLTE (if a b then-else), RIGHT (turn right) and LEFT (left turn)-.
Reference: [12] <author> Mataric, M and Cliff, D. </author> <year> (1996). </year> <title> Challenges In Evolving Controllers for Physical Robots. In Evolutionary Robotics, </title> <journal> special issue of Robotics and Autonomous Systems, </journal> <volume> Vol. 19, No. 1. </volume> <pages> 67-83. </pages>
Reference-contexts: Floreano and Mondada [4, 5] run their robots for several days in order to evolve controllers for basic tasks. Most evolutionary roboticists have preferred to rely on computer simulations to provide them with faster evaluations, but the crafting of appropriate simulations is also very difficult <ref> [12] </ref>. 1.3 Using Humans for Fitness Evolution of interactive (that which is used by humans) adaptive software faces similar difficulties. On the one hand, it is nearly impossible to design a fitness function whose virtual environment will prepare it to meet the enormous variation of human responses.
Reference: [13] <author> Miller, G. F. and Cliff, D. </author> <title> (1994) Protean Behavior in Dynamic Games: Arguments for the Co-Evolution of Pursuit-Evasion Tactics. </title> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB94). </booktitle> <editor> D Cliff, P. Husbands, J.-A Meyer and S W Wilson, eds. </editor> <publisher> MIT Press Bradford Books, pp.411--420. </publisher>
Reference-contexts: Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. Koza [11 ch. 12] and others [17] evolved players for the game of Pacman. There has been important research in pursuer-evader games <ref> [16, 13] </ref> as well as contests in simulated physics environments [20].
Reference: [14] <author> Pollack, J. B., and Blair, </author> <title> A.D. </title> <booktitle> (1997) Why did TD-Gammon work? Advances in Neural CS-97-197 20 Information Processing Systems 9. </booktitle> <pages> 10-16. </pages>
Reference-contexts: It became evident that while this simple architecture is capable of coding players that could perform interestingly when facing human opponents, such good weights were difficult to find in evolutionary or coevolutionary scenarios. Collusion <ref> [14] </ref> was likely to appear in most evolutionary runs in the form of live and let live strategies such as that shown in Figure 4. make tightest spirals in order to stay as far from the opponent as possible.
Reference: [15] <author> Pollack, J. B., Blair, A. and Land, M.(1996). </author> <title> Coevolution of A Backgammon Player. </title> <booktitle> Proceedings Artificial Life V, </booktitle> <editor> C. Langton, (Ed), </editor> <publisher> MIT Press. </publisher>
Reference-contexts: The fitness landscape (even in the coevolutionary case, where the landscape is redefined in every generation) might be an insufficient sample of the larger problem defined by the whole game and the way humans approach it. While learning backgammon <ref> [22, 15] </ref> is a success for coevolution, this same approach has failed in most other cases. Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. Koza [11 ch. 12] and others [17] evolved players for the game of Pacman.
Reference: [16] <author> Reynolds, C.W. </author> <year> (1994). </year> <title> Competition, Coevolution and the Game of Tag, </title> <booktitle> Proceedings of Artificial Life IV. </booktitle> <editor> R. Brooks and P. Maes, eds. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. Koza [11 ch. 12] and others [17] evolved players for the game of Pacman. There has been important research in pursuer-evader games <ref> [16, 13] </ref> as well as contests in simulated physics environments [20].
Reference: [17] <author> Rosca, J. P. </author> <year> (1996). </year> <title> Generality versus Size in Genetic Programming. </title> <booktitle> Proceedings of the Genetic Programming 1996 Conference (GP-96). </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: Neural networks are thought to have generalization capabilities [3 ch. 3, 23], successfully inducing, for example, a good backgammon player from a set of suggested moves [21]. Supporters of the Genetic Programing paradigm [11] will suggest that this may be the case for GP as well <ref> [17] </ref>. In [10], Juill and Pollack argue that the dynamics of coevolutionary fitness help them get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern. <p> While learning backgammon [22, 15] is a success for coevolution, this same approach has failed in most other cases. Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. Koza [11 ch. 12] and others <ref> [17] </ref> evolved players for the game of Pacman. There has been important research in pursuer-evader games [16, 13] as well as contests in simulated physics environments [20].
Reference: [18] <author> Rosin, C. D. </author> <title> (1997) Coevolutionary Search Among Adversaries. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: The new training set T is initialized to the empty set and then new members are added one at a time, choosing the highest according to the following shared fitness function: (3) This selection function is adapted from <ref> [18] </ref> and acts to decrease the relevance of a case that has already been covered, that is, when there is already a player in the training set that beats it.
Reference: [19] <author> Samuel, A. L. </author> <title> (1959) Some Studies in Machine Learning Using the Game of Checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 3, </volume> <pages> 210-229. </pages>
Reference-contexts: The GP function they obtain indeed defines two roughly spiral surfaces that continue outside the boundary of the original test points. 1.5 Learning game playing Game playing is one of the traditional domains of AI research. Ever since Samuels early experiments with checkers <ref> [19] </ref>, we have hoped that the computer would be able to make good use of experience, improving its skills by learning from its mistakes and successes.
Reference: [20] <author> Sims, K. </author> <title> (1994) Evolving 3D Morphology and Behavior by Competition. </title> <booktitle> Artificial Life IV Proceedings, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Koza [11 ch. 12] and others [17] evolved players for the game of Pacman. There has been important research in pursuer-evader games [16, 13] as well as contests in simulated physics environments <ref> [20] </ref>.
Reference: [21] <editor> Tesauro, G. </editor> <booktitle> (1989) Neurogammon Wins Computer Olympiad. Neural Computation I, </booktitle> <pages> 321-323. </pages>
Reference-contexts: Neural networks are thought to have generalization capabilities [3 ch. 3, 23], successfully inducing, for example, a good backgammon player from a set of suggested moves <ref> [21] </ref>. Supporters of the Genetic Programing paradigm [11] will suggest that this may be the case for GP as well [17]. In [10], Juill and Pollack argue that the dynamics of coevolutionary fitness help them get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern. <p> In his work with the game of backgammon, Tesauro began collecting samples from human games to provide a fitness measure for training neural networks <ref> [21] </ref>. Later, he abandoned this methodology and used introspective self-play [22]. Of the earlier approach, he argued that building human expertise into an evaluation function [...] has been found to be an extraordinarily difficult undertaking [22, p. 59].
Reference: [22] <author> Tesauro, G. </author> <title> (1995) Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3): </volume> <pages> 58-68. </pages>
Reference-contexts: In his work with the game of backgammon, Tesauro began collecting samples from human games to provide a fitness measure for training neural networks [21]. Later, he abandoned this methodology and used introspective self-play <ref> [22] </ref>. Of the earlier approach, he argued that building human expertise into an evaluation function [...] has been found to be an extraordinarily difficult undertaking [22, p. 59]. Learning to play a game by self-play involves a problem of transfer as well. <p> Later, he abandoned this methodology and used introspective self-play [22]. Of the earlier approach, he argued that building human expertise into an evaluation function [...] has been found to be an extraordinarily difficult undertaking <ref> [22, p. 59] </ref>. Learning to play a game by self-play involves a problem of transfer as well. <p> The fitness landscape (even in the coevolutionary case, where the landscape is redefined in every generation) might be an insufficient sample of the larger problem defined by the whole game and the way humans approach it. While learning backgammon <ref> [22, 15] </ref> is a success for coevolution, this same approach has failed in most other cases. Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. Koza [11 ch. 12] and others [17] evolved players for the game of Pacman.
Reference: [23] <author> Wolpert, D. H. </author> <year> (1990). </year> <title> A Mathematical Theory of Generalization. </title> <booktitle> Complex Systems 4: </booktitle> <pages> 151-249. </pages>
Reference-contexts: The fact that an algorithm performs well in a certain group of test cases does not usually mean that it will generalize to a wider range of situations. Neural networks are thought to have generalization capabilities <ref> [3 ch. 3, 23] </ref>, successfully inducing, for example, a good backgammon player from a set of suggested moves [21]. Supporters of the Genetic Programing paradigm [11] will suggest that this may be the case for GP as well [17].
References-found: 23

