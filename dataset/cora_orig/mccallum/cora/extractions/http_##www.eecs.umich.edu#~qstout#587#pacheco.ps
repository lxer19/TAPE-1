URL: http://www.eecs.umich.edu/~qstout/587/pacheco.ps
Refering-URL: http://www.eecs.umich.edu/~qstout/587/index.html
Root-URL: http://www.eecs.umich.edu
Email: peter@usfca.edu  
Title: A User's Guide to MPI  
Author: Peter S. Pacheco 
Note: Contents  
Date: March 30, 1998  
Address: San Francisco San Francisco, CA 94117  
Affiliation: Department of Mathematics University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Geoffrey Fox, et al., </author> <title> Solving Problems on Concurrent Processors, </title> <address> Engle-wood Cliffs, NJ, </address> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: derived type that corresponds to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths [3]; MPI_Aint displacements [3]; MPI_Aint addresses [4]; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist <ref> [1] </ref> = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses <p> MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist <ref> [1] </ref> = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements <p> = MPI_FLOAT; typelist <ref> [1] </ref> = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it <p> number of elements of each type */ 20 block_lengths [0] = block_lengths <ref> [1] </ref> = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* Build_derived_type */ The first three <p> type */ 20 block_lengths [0] = block_lengths <ref> [1] </ref> = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* Build_derived_type */ The first three statements specify the types of <p> A topology is a structure imposed on the processes in a communicator that allows the processes to be addressed in different ways. In order to illustrate these ideas, we will develop code to implement Fox's algorithm <ref> [1] </ref> for multiplying two square matrices. 5.1 Fox's Algorithm We assume that the factor matrices A = (a ij ) and B = (b ij ) have order n. <p> Process 0 in new group has rank ranks in old group [0] in old group, process 1 in new group has rank ranks in old group <ref> [1] </ref> in old group, etc. The final command int MPI_Comm_create (MPI_Comm old_comm, MPI_Group new_group, MPI_Comm* new_comm) associates a context with the group new group and creates the communicator new comm. All of the processes in new group belong to the group underlying old comm. <p> Since we don't need to preserve the ordering of the processes in MPI COMM WORLD, we should allow the system to reorder. Having made all these decisions, we simply execute the following code. MPI_Comm grid_comm; int dimensions [2]; int wrap_around [2]; int reorder = 1; dimensions [0] = dimensions <ref> [1] </ref> = q; wrap_around [0] = wrap_around [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, wrap_around, reorder, &grid_comm); After executing this code, the communicator grid comm will contain all the processes in MPI COMM WORLD (possibly reordered), and it will have a two-dimensional cartesian coordinate system associated. <p> Having made all these decisions, we simply execute the following code. MPI_Comm grid_comm; int dimensions [2]; int wrap_around [2]; int reorder = 1; dimensions [0] = dimensions <ref> [1] </ref> = q; wrap_around [0] = wrap_around [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, wrap_around, reorder, &grid_comm); After executing this code, the communicator grid comm will contain all the processes in MPI COMM WORLD (possibly reordered), and it will have a two-dimensional cartesian coordinate system associated. <p> Note that both of these functions are local. 5.6 MPI Cart sub We can also partition a grid into grids of lower dimension. For example, we can create a communicator for each row of the grid as follows. int varying_coords [2]; MPI_Comm row_comm; varying_coords [0] = 0; varying_coords <ref> [1] </ref> = 1; MPI_Cart_sub (grid_comm, varying_coords, &row_comm); The call to MPI Cart sub creates q new communicators. The varying coords argument is an array of boolean. It specifies whether each dimension "belongs" to the new communicator. <p> Hence we assigned varying coords [0] the value 0 | the first coordinate doesn't vary | and we assigned varying coords <ref> [1] </ref> the value 1 | the second coordinate 39 varies. On each process, the new communicator is returned in row comm. In order to create the communicators for the columns, we simply reverse the assignments to the entries in varying coords. MPI_Comm col_comm; varying_coords [0] = 1; varying_coords [1] = 0; <p> varying coords <ref> [1] </ref> the value 1 | the second coordinate 39 varies. On each process, the new communicator is returned in row comm. In order to create the communicators for the columns, we simply reverse the assignments to the entries in varying coords. MPI_Comm col_comm; varying_coords [0] = 1; varying_coords [1] = 0; MPI_Cart_sub (grid_comm, varying_coord, col_comm); Note the similarity of MPI Cart sub to MPI Comm split. They perform similar functions | they both partition a communicator into a collection of new communicators. <p> allocated in the * calling routine. 40 */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions [2]; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions <ref> [1] </ref> = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = coordinates [0]; grid-&gt;my_col = coordinates [1]; /* Set up row and column communicators */ varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid-&gt;comm, <p> */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions [2]; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions <ref> [1] </ref> = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = coordinates [0]; grid-&gt;my_col = coordinates [1]; /* Set up row and column communicators */ varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;row_comm)); varying_coords [0] = 1; varying_coords <p> Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions <ref> [1] </ref> = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = coordinates [0]; grid-&gt;my_col = coordinates [1]; /* Set up row and column communicators */ varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;row_comm)); varying_coords [0] = 1; varying_coords [1] = 0; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;col_comm)); - /* Setup_grid */ Notice that since each of our communicators has an associated topology, we constructed them <p> dimensions [0] = dimensions <ref> [1] </ref> = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = coordinates [0]; grid-&gt;my_col = coordinates [1]; /* Set up row and column communicators */ varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;row_comm)); varying_coords [0] = 1; varying_coords [1] = 0; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;col_comm)); - /* Setup_grid */ Notice that since each of our communicators has an associated topology, we constructed them using the topology construction functions | MPI - Cart create and MPI Cart sub <p> = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = coordinates [0]; grid-&gt;my_col = coordinates <ref> [1] </ref>; /* Set up row and column communicators */ varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;row_comm)); varying_coords [0] = 1; varying_coords [1] = 0; MPI_Cart_sub (grid-&gt;comm, varying_coords, &(grid-&gt;col_comm)); - /* Setup_grid */ Notice that since each of our communicators has an associated topology, we constructed them using the topology construction functions | MPI - Cart create and MPI Cart sub | rather than the more general communicator construction functions MPI Comm create
Reference: [2] <author> William Gropp, Ewing Lusk, and Anthony Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface, </title> <address> Cambridge, MA, </address> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It is intended for use by programmers who have some experience using C but little experience with message-passing. It is based on parts of the book [6], which is to be published by Morgan Kaufmann. For comprehensive guides to MPI see [4], [5] and <ref> [2] </ref>. For an extended, elementary introduction, see [6]. Acknowledgments. My thanks to nCUBE and the USF faculty development fund for their support of the work that went into the preparation of this Guide. <p> to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths [3]; MPI_Aint displacements [3]; MPI_Aint addresses [4]; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist <ref> [2] </ref> = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses <p> /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist <ref> [2] </ref> = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses <p> = MPI_FLOAT; typelist <ref> [2] </ref> = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ <p> block_lengths [0] = block_lengths [1] = block_lengths <ref> [2] </ref> = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* Build_derived_type */ The first three statements specify the types of the members of <p> = block_lengths <ref> [2] </ref> = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses [3]); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* Build_derived_type */ The first three statements specify the types of the members of the derived type, and the <p> Since we don't need to preserve the ordering of the processes in MPI COMM WORLD, we should allow the system to reorder. Having made all these decisions, we simply execute the following code. MPI_Comm grid_comm; int dimensions <ref> [2] </ref>; int wrap_around [2]; int reorder = 1; dimensions [0] = dimensions [1] = q; wrap_around [0] = wrap_around [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, wrap_around, reorder, &grid_comm); After executing this code, the communicator grid comm will contain all the processes in MPI COMM WORLD (possibly reordered), and it will <p> Since we don't need to preserve the ordering of the processes in MPI COMM WORLD, we should allow the system to reorder. Having made all these decisions, we simply execute the following code. MPI_Comm grid_comm; int dimensions <ref> [2] </ref>; int wrap_around [2]; int reorder = 1; dimensions [0] = dimensions [1] = q; wrap_around [0] = wrap_around [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, wrap_around, reorder, &grid_comm); After executing this code, the communicator grid comm will contain all the processes in MPI COMM WORLD (possibly reordered), and it will have a two-dimensional <p> In order for a process to determine its coordinates, it simply calls the function MPI Cart coords: int coordinates <ref> [2] </ref>; int my_grid_rank; 37 MPI_Comm_rank (grid_comm, &my_grid_rank); MPI_Cart_coords (grid_comm, my_grid_rank, 2, coordinates); Notice that we needed to call MPI Comm rank in order to get the process rank in grid comm. <p> Note that both of these functions are local. 5.6 MPI Cart sub We can also partition a grid into grids of lower dimension. For example, we can create a communicator for each row of the grid as follows. int varying_coords <ref> [2] </ref>; MPI_Comm row_comm; varying_coords [0] = 0; varying_coords [1] = 1; MPI_Cart_sub (grid_comm, varying_coords, &row_comm); The call to MPI Cart sub creates q new communicators. The varying coords argument is an array of boolean. It specifies whether each dimension "belongs" to the new communicator. <p> my_row; /* My row number */ int my_col; /* My column number */ int my_rank; /* My rank in the grid communicator */ - GRID_INFO_TYPE; /* We assume space for grid has been allocated in the * calling routine. 40 */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions <ref> [2] </ref>; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions [1] = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank <p> row number */ int my_col; /* My column number */ int my_rank; /* My rank in the grid communicator */ - GRID_INFO_TYPE; /* We assume space for grid has been allocated in the * calling routine. 40 */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions <ref> [2] </ref>; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions [1] = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords <p> int my_col; /* My column number */ int my_rank; /* My rank in the grid communicator */ - GRID_INFO_TYPE; /* We assume space for grid has been allocated in the * calling routine. 40 */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions <ref> [2] </ref>; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions [1] = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, <p> My column number */ int my_rank; /* My rank in the grid communicator */ - GRID_INFO_TYPE; /* We assume space for grid has been allocated in the * calling routine. 40 */ void Setup_grid (GRID_INFO_TYPE* grid) - int old_rank; int dimensions <ref> [2] </ref>; int periods [2]; int coordinates [2]; int varying_coords [2]; /* Set up Global Grid Information */ MPI_Comm_size (MPI_COMM_WORLD, &(grid-&gt;p)); MPI_Comm_rank (MPI_COMM_WORLD, &old_rank); grid-&gt;q = (int) sqrt ((double) grid-&gt;p); dimensions [0] = dimensions [1] = grid-&gt;q; periods [0] = periods [1] = 1; MPI_Cart_create (MPI_COMM_WORLD, 2, dimensions, periods, 1, &(grid-&gt;comm)); MPI_Comm_rank (grid-&gt;comm, &(grid-&gt;my_rank)); MPI_Cart_coords (grid-&gt;comm, grid-&gt;my_rank, 2, coordinates); grid-&gt;my_row = <p> It also contains a large number of nice examples of uses of the various MPI functions. So it is considerably more than just a reference. Currently, several members of the MPI Forum are working on an annotated version of the MPI standard [5]. The book <ref> [2] </ref> is a tutorial introduction to MPI. It provides numerous complete examples of MPI programs. The book [6] contains a tutorial introduction to MPI (on which this guide is based). It also contains a more general introduction to parallel processing and the programming of message-passing machines.
Reference: [3] <author> Brian W. Kernighan and Dennis M. Ritchie, </author> <title> The C Programming Language, 2nd ed., </title> <address> Englewood Cliffs, NJ, </address> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Copying. This Guide may be freely copied and redistributed provided such copying and redistribution is not done for profit. 3 2 Greetings! The first C program that most of us saw was the "Hello, world!" program in Kernighan and Ritchie's classic text, The C Programming Language <ref> [3] </ref>. It simply prints the message "Hello, world!" A variant that makes some use of multiple processes is to have each process send a greeting to another process. In MPI, the processes involved in the execution of a parallel program are identified by a sequence of non-negative integers. <p> Such a type is called a derived datatype. In order to see how this works, let's write a function that will build a derived type that corresponds to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths <ref> [3] </ref>; MPI_Aint displacements [3]; MPI_Aint addresses [4]; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type <p> Such a type is called a derived datatype. In order to see how this works, let's write a function that will build a derived type that corresponds to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths <ref> [3] </ref>; MPI_Aint displacements [3]; MPI_Aint addresses [4]; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths <p> Such a type is called a derived datatype. In order to see how this works, let's write a function that will build a derived type that corresponds to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths <ref> [3] </ref>; MPI_Aint displacements [3]; MPI_Aint addresses [4]; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths <p> = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths [1] = block_lengths [2] = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses <ref> [3] </ref>); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* <p> = 1; /* Calculate the displacements of the members * relative to indata */ MPI_Address (indata, &addresses [0]); MPI_Address (&(indata-&gt;a), &addresses [1]); MPI_Address (&(indata-&gt;b), &addresses [2]); MPI_Address (&(indata-&gt;n), &addresses <ref> [3] </ref>); displacements [0] = addresses [1] - addresses [0]; displacements [1] = addresses [2] - addresses [0]; displacements [2] = addresses [3] - addresses [0]; /* Create the derived type */ MPI_Type_struct (3, block_lengths, displacements, typelist, message_type_ptr); /* Commit it so that it can be used */ MPI_Type_commit (message_type_ptr); - /* Build_derived_type */ The first three statements specify the types of the members of the derived type, and the next specifies the
Reference: [4] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard , International Journal of Supercomputer Applications, </title> <note> vol 8, nos 3/4, 1994. Also available as Technical Report CS-94-230, </note> <institution> Computer Science Dept., University of Tennessee, Knoxville, TN, </institution> <year> 1994. </year>
Reference-contexts: It is intended for use by programmers who have some experience using C but little experience with message-passing. It is based on parts of the book [6], which is to be published by Morgan Kaufmann. For comprehensive guides to MPI see <ref> [4] </ref>, [5] and [2]. For an extended, elementary introduction, see [6]. Acknowledgments. My thanks to nCUBE and the USF faculty development fund for their support of the work that went into the preparation of this Guide. <p> This makes sense. In general, the receiving process may not know the exact size of the message being sent. So MPI allows a message to be received as long as there is sufficient storage allocated. If there isn't sufficient storage, an overflow error occurs <ref> [4] </ref>. The arguments dest and source are, respectively, the ranks of the receiving and the sending processes. <p> The parameters count and datatype have the same function that they have in MPI Send and MPI Recv: they specify the extent of the message. However, unlike the point-to-point functions, MPI insists that in collective communication count and datatype be the same on all the processes in the communicator <ref> [4] </ref>. The reason for this is that in some collective operations (see below), a single process will receive data from many other processes, and in order for a program to determine how much data has been received, it would need an entire array of return statuses. <p> For details see <ref> [4] </ref>. 15 As an example, let's rewrite the last few lines of the trapezoid rule pro- gram. . . . /* Add up the integrals calculated by each process */ MPI_Reduce (&integral, &total, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD); /* Print the result */ . . . <p> In particular, even though total only has significance on process 0, each process must supply an argument. 3.4 Other Collective Communication Functions MPI supplies many other collective communication functions. We briefly enumerate some of these here. For full details, see <ref> [4] </ref>. * int MPI_Barrier (MPI_Comm comm) MPI Barrier provides a mechanism for synchronizing all the processes in the communicator comm. <p> Such a type is called a derived datatype. In order to see how this works, let's write a function that will build a derived type that corresponds to INDATA TYPE. void Build_derived_type (INDATA_TYPE* indata, MPI_Datatype* message_type_ptr)- int block_lengths [3]; MPI_Aint displacements [3]; MPI_Aint addresses <ref> [4] </ref>; MPI_Datatype typelist [3]; /* Build a derived datatype consisting of * two floats and an int */ /* First specify the types */ typelist [0] = MPI_FLOAT; typelist [1] = MPI_FLOAT; typelist [2] = MPI_INT; /* Specify the number of elements of each type */ 20 block_lengths [0] = block_lengths <p> Note that we calculated the addresses of the members of indata with MPI Address rather than C's & operator. The reason for this is that ANSI C does not require that a pointer be an int (although this is commonly the case). See <ref> [4] </ref>, for a more detailed discussion of this point. Note also that the type of array of displacements is MPI Aint | not int. This is a special type in MPI. It allows for the possibility that addresses are too large to be stored in an int. <p> We will only discuss intra-communicators. The interested reader is referred to <ref> [4] </ref> for details on the use of inter-communicators. A minimal (intra-)communicator is composed of * a Group, and * a Context. A group is an ordered collection of processes. <p> MPI Comm group and MPI Group incl, are both local operations. That is, there is no communication among processes involved in their execution. However, MPI Comm create is a collective operation. All the processes in old comm must call MPI Comm create with the same arguments. The Standard <ref> [4] </ref> gives three reasons for this: 1. It allows the implementation to layer MPI Comm create on top of reg ular collective communications. 34 2. It provides additional safety. 3. It permits implementations to avoid communication related to context creation. <p> The Standard <ref> [4] </ref> is over 200 pages long and it defines more than 125 functions. As a consequence, this Guide has covered only a small fraction of MPI, and many readers will fail to find a discussion of functions that they would find very useful in their applications. <p> Inter-communicators. Recollect that MPI provides two types of communicators: intra-communicators and inter-communicators. Inter 44 communicators can be used for point-to-point communications between processes belonging to distinct intra-communicators. There are many other functions available to users of MPI. If we haven't discussed a facility you need, please consult the Standard <ref> [4] </ref> to determine whether it is part of MPI. 6.2 Implementations of MPI If you don't have an implementation of MPI, there are three versions that are freely available by anonymous ftp from the following sites. * Argonne National Lab/Mississippi State University. <p> The Argonne MPI web page. 45 Each of these sites contains a wealth of information about MPI. Of particular note, the Mississippi State page contains a bibliography of papers on MPI, and the Argonne page contains a collection of test MPI programs. The MPI Standard <ref> [4] </ref> is currently available from each of the sites above. This is, of course, the definitive statement of what MPI is. So if you're not clear on something, this is the final arbiter. It also contains a large number of nice examples of uses of the various MPI functions.
Reference: [5] <author> Steve Otto, et al., </author> <title> MPI Annotated Reference Manual, </title> <address> Cambridge, MA, </address> <publisher> MIT Press, to appear. </publisher>
Reference-contexts: It is intended for use by programmers who have some experience using C but little experience with message-passing. It is based on parts of the book [6], which is to be published by Morgan Kaufmann. For comprehensive guides to MPI see [4], <ref> [5] </ref> and [2]. For an extended, elementary introduction, see [6]. Acknowledgments. My thanks to nCUBE and the USF faculty development fund for their support of the work that went into the preparation of this Guide. <p> It also contains a large number of nice examples of uses of the various MPI functions. So it is considerably more than just a reference. Currently, several members of the MPI Forum are working on an annotated version of the MPI standard <ref> [5] </ref>. The book [2] is a tutorial introduction to MPI. It provides numerous complete examples of MPI programs. The book [6] contains a tutorial introduction to MPI (on which this guide is based). It also contains a more general introduction to parallel processing and the programming of message-passing machines.
Reference: [6] <author> Peter S. Pacheco, </author> <title> Parallel Programming with MPI, </title> <address> San Francisco, CA, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year> <month> 51 </month>
Reference-contexts: This User's Guide is a brief tutorial introduction to some of the more important features of MPI for C programmers. It is intended for use by programmers who have some experience using C but little experience with message-passing. It is based on parts of the book <ref> [6] </ref>, which is to be published by Morgan Kaufmann. For comprehensive guides to MPI see [4], [5] and [2]. For an extended, elementary introduction, see [6]. Acknowledgments. <p> It is based on parts of the book <ref> [6] </ref>, which is to be published by Morgan Kaufmann. For comprehensive guides to MPI see [4], [5] and [2]. For an extended, elementary introduction, see [6]. Acknowledgments. My thanks to nCUBE and the USF faculty development fund for their support of the work that went into the preparation of this Guide. <p> So it is considerably more than just a reference. Currently, several members of the MPI Forum are working on an annotated version of the MPI standard [5]. The book [2] is a tutorial introduction to MPI. It provides numerous complete examples of MPI programs. The book <ref> [6] </ref> contains a tutorial introduction to MPI (on which this guide is based). It also contains a more general introduction to parallel processing and the programming of message-passing machines.
References-found: 6

