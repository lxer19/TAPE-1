URL: http://www.cs.umn.edu/Users/dept/users/kumar/direct_siam.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Design and Implementation of a Scalable Parallel Direct Solver for Sparse Symmetric Positive Definite Systems  
Note: Vipin Kumar  
Abstract: Preliminary Results fl Abstract Solving large sparse systems of linear equations is at the core of many problems in engineering and scientific computing. It has long been a challenge to develop parallel formulations of sparse direct solvers due to several different complex steps involved in the process. In this paper, we describe one of the first efficient, practical, and robust parallel solvers for sparse symmetric positive definite linear systems that we have developed and discuss the algorithmic and implementation issues involved in its development. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anshul Gupta, George Karypis and Vipin Kumar, </author> <title> Highly Scalable Parallel Algorithms for Sparse Matrix Factorization, </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute. Related papers are available via WWW at URL: http://www.cs.umn.edu/~kumar. y IBM T.J.Watson Research Center, Yorktown Heights, NY 10598 z Department of Computer Science, University of Minnesota, Minneapolis, MN 55455 1 2 factorization we recently developed <ref> [1] </ref>. This algorithm is able to achieve high computational rates (of over 20 GFlops on a 1024 processor Cray T3D) and it successfully parallelizes computationally the most expensive phase of the sparse solver. <p> Details about the parallel algorithm for the ordering phase can be found in [2]. 2 Parallel Numerical Factorization For the numerical factorization phase we use a highly scalable algorithm that we developed recently <ref> [1] </ref> that is based on the multifrontal algorithm [5]. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. The algorithm performs a postorder traversal of the elimination tree associated with A. <p> Parallel Symbolic Factorization are shown in the form of a logical mesh labeled with P. The frontal matrix of each supernode is distributed among this logical mesh using a bitmask based block-cyclic scheme <ref> [1] </ref>. Figure 1 (b) shows such a distribution for unit blocksize. This distribution ensures that the extend-add operations required by the multifrontal algorithm can be performed in parallel with each processor exchanging roughly half of its data only with its partner from the other subcube. <p> Refer to [3] for details. 5 Analysis and Preliminary Experimental Results Table 1 shows the serial and parallel time complexities of the various phases of our parallel direct solver for matrices corresponding to 2-D and 3-D finite element meshes <ref> [2, 1, 3] </ref>. From this table we see that the overall time complexity is dominated by the numerical factorization phase. The isoefficiency function of the parallel solver is determined by the numerical factorization phase and it is O (p 1:5 ) for both 2-D and 3-D finite element problems.
Reference: [2] <author> George Karypis and Vipin Kumar, </author> <title> Parallel multilevel graph partitioning, </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year>
Reference-contexts: This algorithm is able to achieve high computational rates (of over 20 GFlops on a 1024 processor Cray T3D) and it successfully parallelizes computationally the most expensive phase of the sparse solver. Fill reducing ordering is obtained using a parallel formulation of the multilevel nested dissection algorithm <ref> [2] </ref> that has been found to be effective in producing orderings that are suited for parallel factorization. For symbolic factorization and solution of triangular systems, we have developed parallel algorithms that utilize the same data distribution as used by the numerical factorization algorithm. <p> In this paper we briefly describe the parallel algorithms for numerical factorization, symbolic factorization and solution of triangular systems, and present some preliminary experimental results on an IBM SP2. Details about the parallel algorithm for the ordering phase can be found in <ref> [2] </ref>. 2 Parallel Numerical Factorization For the numerical factorization phase we use a highly scalable algorithm that we developed recently [1] that is based on the multifrontal algorithm [5]. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. <p> Refer to [3] for details. 5 Analysis and Preliminary Experimental Results Table 1 shows the serial and parallel time complexities of the various phases of our parallel direct solver for matrices corresponding to 2-D and 3-D finite element meshes <ref> [2, 1, 3] </ref>. From this table we see that the overall time complexity is dominated by the numerical factorization phase. The isoefficiency function of the parallel solver is determined by the numerical factorization phase and it is O (p 1:5 ) for both 2-D and 3-D finite element problems.
Reference: [3] <author> Mahesh Joshi and Vipin Kumar, </author> <title> Two-Dimensional Scalable Parallel Algorithms for Solution of Triangular Systems, </title> <type> Technical Report, </type> <institution> Department of Computer Science, University of Minnesota, MN, </institution> <year> 1997. </year>
Reference-contexts: This update vector needs to be sent to the processors that store the first column of the L matrix of the parent supernode. Because of the bitmask based block-cyclic distribution, this can be done by using at most two communication steps <ref> [3] </ref>. The details of the two-dimensional pipelined dense forward elimination algorithm are illustrated in Figure 3 (b) for a hypothetical supernode. The solutions are computed by the processors owning diagonal elements of L matrix and flow down along a column. <p> First, the computation proceeds from the top supernode of the tree down to the leaf. Second, the computed solution that gets communicated across the levels of the supernodal tree instead of accumulated updates and this is achieved with at most one communication per processor. Refer to <ref> [3] </ref> for details. 5 Analysis and Preliminary Experimental Results Table 1 shows the serial and parallel time complexities of the various phases of our parallel direct solver for matrices corresponding to 2-D and 3-D finite element meshes [2, 1, 3]. <p> Refer to [3] for details. 5 Analysis and Preliminary Experimental Results Table 1 shows the serial and parallel time complexities of the various phases of our parallel direct solver for matrices corresponding to 2-D and 3-D finite element meshes <ref> [2, 1, 3] </ref>. From this table we see that the overall time complexity is dominated by the numerical factorization phase. The isoefficiency function of the parallel solver is determined by the numerical factorization phase and it is O (p 1:5 ) for both 2-D and 3-D finite element problems.
Reference: [4] <author> Vipin Kumar, Ananth Grama, Anshul Gupta and George Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms Benjamin/Cummings, </title> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: From this table we see that the overall time complexity is dominated by the numerical factorization phase. The isoefficiency function of the parallel solver is determined by the numerical factorization phase and it is O (p 1:5 ) for both 2-D and 3-D finite element problems. As discussed in <ref> [4] </ref>, the isoefficiency function of the dense Cholesky factorization algorithm is also O (p 1:5 ).
Reference: [5] <author> J. W. H. Liu, </author> <title> The Multifrontal Method for Sparse Matrix Solution: </title> <journal> Theory and Practice, SIAM Review, vol.34, </journal> <volume> no.1, </volume> <pages> pp. 82-109, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Details about the parallel algorithm for the ordering phase can be found in [2]. 2 Parallel Numerical Factorization For the numerical factorization phase we use a highly scalable algorithm that we developed recently [1] that is based on the multifrontal algorithm <ref> [5] </ref>. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as follows. Consider an N fiN matrix A. The algorithm performs a postorder traversal of the elimination tree associated with A.
References-found: 5

