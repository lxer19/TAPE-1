URL: http://www.cs.nmsu.edu/~wiebe/pubs/papers/arpa94.ps
Refering-URL: http://www.cs.nmsu.edu/~wiebe/pubs/index.html
Root-URL: http://www.cs.nmsu.edu
Title: A New Approach to Word Sense Disambiguation  
Author: Rebecca Bruce and Janyce Wiebe 
Address: Las Cruces, NM 88003  
Affiliation: The Computing Research Lab New Mexico State University  
Abstract: This paper presents and evaluates models created according to a schema that provides a description of the joint distribution of the values of sense tags and contextual features that is potentially applicable to a wide range of content words. The models are evaluated through a series of experiments, the results of which suggest that the schema is particularly well suited to nouns but that it is also applicable to words in other syntactic categories. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bishop, Y. M.; Fienberg, S.; and Holland, </author> <title> P (1975). Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> Cam-bridge: The MIT Press. </publisher>
Reference-contexts: analogous to analysis of variance (ANOVA) models (<ref> [1] </ref>. The log-linear model expresses the population mean as the sum of the con-tributions of the "effects" of the variables and the interactions between variables; it is the logarithm of the mean that is linear in these effects. Under certain sampling plans (see [1] for details), data consisting of the observed values of a number of contextual features and the corresponding sense tags of an ambiguous word can be described by a multinomial distribution in which each distinct combination of the values of the contextual features and the sense tag identifies a unique category
Reference: 2. <author> Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. </author> <year> (1991). </year> <title> Word Sense Disambiguation Using Statistical Methods. </title> <booktitle> Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL-91), </booktitle> <pages> pp. 264-304. </pages>
Reference-contexts: Most previous efforts to formulate a probabilistic classifier for word-sense disambiguation did not attempt to systematically identify the interdependencies among contextual features that can be used to classify the meaning of an ambiguous word. Many researchers have performed disambiguation on the basis of only a single feature ([6], [15], <ref> [2] </ref>), while others who do consider multiple contextual features assume that all contextual features are either conditionally independent given the sense of the word ([8], [14]) or fully independent ([10], [16]). <p> This strategy for selecting collocation-specific variables is simpler than that used by many other researchers ([6], [15], <ref> [2] </ref>). This simpler method was chosen to support work we plan to do in the future (eliminating the need for sense-tagged data; see section 6). In using this strategy, we do, however, run the risk of reducing the informativeness of the variables.
Reference: 3. <author> Bruce, Rebecca and Wiebe, Janyce. </author> <title> Word-Sense Disambiguation Using Decomposable Models. </title> <type> Unpublished manuscript. </type>
Reference-contexts: We chose a particular set of contextual features and, using this method, identified a model incorporating these features for use in disambiguating the noun interest. These features, which are assigned automatically, are of three types: morphological, collocation-specific, and class-based, with part-of-speech (POS) categories serving as the word classes (see <ref> [3] </ref> for how the features were chosen). The results of using the model to disambiguate the noun interest were encouraging. We suspect that the model provides a description of the distribution of sense tags and contextual features that is applicable to a wide range of content words. <p> The complete process of feature selection and model selection is described in <ref> [3] </ref>. Here, we describe the extension of that model to other content words. In essence, what we are describing is not a single model, but a model schema. The values of the variables included in the model change with the word being disambiguated as stated below.
Reference: 4. <author> Church, K. and W. </author> <title> Gale (1991). A Comparison of the Enhanced Good-Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol 5, </volume> <pages> pp. 19-54. </pages>
Reference-contexts: To date, much of the work in statistical NLP has focused on parameter estimation ([11], [13], [12], <ref> [4] </ref>).
Reference: 5. <author> Church, Kenneth W and Hanks, </author> <title> Patrick (1990). Word Association Norms, Mutual Information, and Lexicography, </title> <journal> Computational Linguistics, </journal> <volume> Vol. 16, No. 1, </volume> <pages> pp. 22-29. </pages>
Reference-contexts: To date, much of the work in statistical NLP has focused on parameter estimation ([11], [13], [12], [4]). Of the research directed toward identifying the optimum form of model, most has been concerned with the selection of individually informative features ([2], <ref> [5] </ref>), with relatively little attention directed toward the identification of an optimum approximation to the joint distribution of the values of the contextual features and object classes.
Reference: 6. <author> Dagan, I.; Itai, A.; and Schwall, U. </author> <year> (1991). </year> <title> Two Languages Are More Informative Than One. </title> <booktitle> Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL-91), </booktitle> <pages> pp. 130-137. </pages>
Reference: 7. <author> Dempster, A., N. Laird, and D. </author> <title> Rubin (1977). Maximum Likelihood from Incomplete Data Via the EM Algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> Vol 39, </volume> <pages> pp. 1-38. </pages>
Reference: 8. <author> Gale, W.; Church, K.; and Yarowsky, D. </author> <year> (1992). </year> <title> A Method for Disambiguating Word Senses in a Large Corpus. </title> <journal> AT&T Bell Laboratories Statistical Research Report No. </journal> <volume> 104. </volume>
Reference: 9. <author> Havranek, </author> <title> Tomas (1984). A Procedure for Model Search in Multidimensional Contingency Tables. </title> <type> Biometrics 40: </type> <pages> 95-100. </pages>
Reference-contexts: The "best fitting" model, in the sense that the significance according to the reference O 2 value is largest, is then selected. The exhaustive search of decomposable models was conducted as described in <ref> [9] </ref>. Approximating the joint distribution of all variables with a model containing only the most important systematic interactions among variables limits the number of parameters to be estimated, supports computational efficiency, and provides an understanding of the data.
Reference: 10. <author> Hearst, </author> <title> Marti (1991). Toward Noun Homonym Disambiguation|Using Local Context in Large Text Corpora. </title> <booktitle> Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research Using Corpora, </booktitle> <pages> pp. 1-22. </pages>
Reference: 11. <author> Jelinek, F. and R. </author> <title> Mercer (1980). Interpolated Estimation of Markov Source Parameters from Sparse Data. </title> <booktitle> Proceedings Workshop on Pattern Recognition in Practice, </booktitle> <address> May 21-23, Amsterdam: </address> <publisher> North-Holland. </publisher>
Reference: 12. <author> Katz, S. M. </author> <year> (1987). </year> <title> Estimation of Probabilities From Sparse Data for the Language Model Component of a Speech Recognizer. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> Vol ASSP-35, </volume> <pages> pp. 400-401. </pages>
Reference-contexts: To date, much of the work in statistical NLP has focused on parameter estimation ([11], [13], <ref> [12] </ref>, [4]).
Reference: 13. <author> Nadas, A. </author> <year> (1984). </year> <title> Estimation of Probabilities in the Language Model of the IBM Speech Recognition System. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> Vol ASSP-32, </volume> <pages> pp. 859-861. </pages>
Reference-contexts: To date, much of the work in statistical NLP has focused on parameter estimation ([11], <ref> [13] </ref>, [12], [4]).
Reference: 14. <author> Yarowsky, David (1992). </author> <title> Word-Sense Disambiguating Using Statistical Models of Roget's Categories Trained on Large Corpora. </title> <booktitle> Proceedings of the 15th International Conference on Computational Linguistics (COLING-92). </booktitle>
Reference-contexts: Many researchers have performed disambiguation on the basis of only a single feature ([6], [15], [2]), while others who do consider multiple contextual features assume that all contextual features are either conditionally independent given the sense of the word ([8], <ref> [14] </ref>) or fully independent ([10], [16]). In earlier work, we describe a method for identifying an appropriate model for use in disambiguating a word given a set of contextual features.
Reference: 15. <author> Yarowsky, David (1993). </author> <title> One Sense Per Collocation. </title> <booktitle> Proceedings of the Speech and Natural Language ARPA Workshop, </booktitle> <address> March 1993, Princeton, NJ. </address>
Reference-contexts: Most previous efforts to formulate a probabilistic classifier for word-sense disambiguation did not attempt to systematically identify the interdependencies among contextual features that can be used to classify the meaning of an ambiguous word. Many researchers have performed disambiguation on the basis of only a single feature ([6], <ref> [15] </ref>, [2]), while others who do consider multiple contextual features assume that all contextual features are either conditionally independent given the sense of the word ([8], [14]) or fully independent ([10], [16]). <p> This strategy for selecting collocation-specific variables is simpler than that used by many other researchers ([6], <ref> [15] </ref>, [2]). This simpler method was chosen to support work we plan to do in the future (eliminating the need for sense-tagged data; see section 6). In using this strategy, we do, however, run the risk of reducing the informativeness of the variables.

References-found: 15

