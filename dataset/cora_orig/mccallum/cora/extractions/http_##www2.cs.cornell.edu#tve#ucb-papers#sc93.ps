URL: http://www2.cs.cornell.edu/tve/ucb-papers/sc93.ps
Refering-URL: http://www2.cs.cornell.edu/tve/pub.html
Root-URL: http://www.cs.cornell.edu
Title: Parallel Programming in Split-C  
Author: David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: We introduce the Split-C language, a parallel extension of C intended for high performance programming on distributed memory multiprocessors, and demonstrate the use of the language in optimizing parallel programs. Split-C provides a global address space with a clear concept of locality and unusual assignment operators. These are used as tools to reduce the frequency and cost of remote access. The language allows a mixture of shared memory, message passing, and data parallel programming styles while providing efficient access to the underlying machine. We demonstrate the basic language concepts using regular and irregular parallel programs and give performance results for various stages of program optimization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution Time Support for Adaptive Scientific Algorithms on Distributed Memory Multiprocessors. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The type qualifier global can be used 2 Some of the optimizations we use here to demonstrate Split-C features are built into parallel Grid libraries like the Parti system <ref> [1] </ref>. with any pointer type (except a pointer to a function), and global pointers can be declared anywhere that standard pointers can be declared. Construction: A global pointer may be constructed using the function toglobal, which takes a processor number and a local pointer.
Reference: [2] <author> J. Boyle, R. Butler, T. Disz, B. Glickfeld, E. Lusk, R. Overbeek, J.Patterson, and R. Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: Split-C supports these models by programming conventions, rather than enforcing them through language constraints. Split-C borrows heavily from shared memory models in providing several threads of control within a global address space [10, 12]. Virtues of this approach include: allowing familiar languages to be used with modest enhancements <ref> [6, 3, 2] </ref>, making global data structures explicit, rather than being implicit in the pattern of sends and receives, and allowing for powerful linked data structures. This was illustrated for the EM3D problem above; applications that demonstrate irregularity in both time and space [4, 18] also benefit from these features.
Reference: [3] <author> E. Brooks. PCP: </author> <title> A Parallel Extension of C that is 99% Fat Free. </title> <type> Technical Report UCRL-99673, </type> <institution> LLNL, </institution> <year> 1988. </year>
Reference-contexts: Split-C supports these models by programming conventions, rather than enforcing them through language constraints. Split-C borrows heavily from shared memory models in providing several threads of control within a global address space [10, 12]. Virtues of this approach include: allowing familiar languages to be used with modest enhancements <ref> [6, 3, 2] </ref>, making global data structures explicit, rather than being implicit in the pattern of sends and receives, and allowing for powerful linked data structures. This was illustrated for the EM3D problem above; applications that demonstrate irregularity in both time and space [4, 18] also benefit from these features.
Reference: [4] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an Irregular Application on a Distributed Memory Multiprocessor. </title> <booktitle> In Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: This was illustrated for the EM3D problem above; applications that demonstrate irregularity in both time and space <ref> [4, 18] </ref> also benefit from these features. Split-C differs from previous shared memory languages by providing a rich set of memory operations, not simply read and write.
Reference: [5] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Optimizing global operations on regular data structures is encouraged by defining a simple storage layout for global matrices. In some cases, the way to minimize the number of remote accesses is to program the layout to ensure communication balance <ref> [5] </ref>. For example, an n-point FFT can be performed on p processors with a single remap communication step if p 2 n [5]. In other cases, e.g., blocked matrix multiplication, the particular assignment of blocks to processors less important than load balance and block size. <p> In some cases, the way to minimize the number of remote accesses is to program the layout to ensure communication balance <ref> [5] </ref>. For example, an n-point FFT can be performed on p processors with a single remap communication step if p 2 n [5]. In other cases, e.g., blocked matrix multiplication, the particular assignment of blocks to processors less important than load balance and block size. The approach to global matrices in Split-C stems from the work on data parallel languages, especially HPF [8] and C* [15].
Reference: [6] <author> F. Darema, D. George, V. Norton, and G. Pfister. </author> <title> A Single-Program-Multiple Data Computational Model for EPEX/FORTRAN. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 11-24, </pages> <year> 1988. </year>
Reference-contexts: Split-C supports these models by programming conventions, rather than enforcing them through language constraints. Split-C borrows heavily from shared memory models in providing several threads of control within a global address space [10, 12]. Virtues of this approach include: allowing familiar languages to be used with modest enhancements <ref> [6, 3, 2] </ref>, making global data structures explicit, rather than being implicit in the pattern of sends and receives, and allowing for powerful linked data structures. This was illustrated for the EM3D problem above; applications that demonstrate irregularity in both time and space [4, 18] also benefit from these features.
Reference: [7] <author> M. Dubois and C. Scheurich. </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The same is true for reads; any read waits for the value to be returned. In other words, only a single outstanding read or write is allowed from a given processor; this ensures that the completion order of reads and writes match their issue order <ref> [7] </ref>. The ordering of puts is defined only between sync operations. 5 Signaling Stores The discussion above emphasizes the "local computation" view of pulling portions of a global data structure to the processor.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Draft, </type> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: In other cases, e.g., blocked matrix multiplication, the particular assignment of blocks to processors less important than load balance and block size. The approach to global matrices in Split-C stems from the work on data parallel languages, especially HPF <ref> [8] </ref> and C* [15]. A key design choice was to avoid run-time shapes or dope vectors, because these are inconsistent with C and with the philosophy of least surprises. Split-C does not have the ease of portability of the HPF proposal or of higher level parallel languages.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimziations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Split-C differs from previous shared memory languages by providing a rich set of memory operations, not simply read and write. It does not rely on novel architectural features, nor does it assume communication has enormous overhead, thereby making bulk operations the only reasonable form of communication <ref> [16, 9] </ref>. These differences arise because of differences in the implementation assumptions. Split-C multiply on 64 Sparc processor CM-5. is targeted toward distributed memory multiprocessors with fast, flexible network hardware, including the Thinking Machines CM-5, Meiko CS-2, Cray T3D and others.
Reference: [10] <author> B. A. C. Inc. </author> <title> TC2000 Technical Product Summary. </title> <year> 1989. </year>
Reference-contexts: Declaration: A global pointer is declared by appending the qualifier global to the pointer type declaration (e.g., int *global g; or int *global garray <ref> [10] </ref>;). <p> Split-C supports these models by programming conventions, rather than enforcing them through language constraints. Split-C borrows heavily from shared memory models in providing several threads of control within a global address space <ref> [10, 12] </ref>. Virtues of this approach include: allowing familiar languages to be used with modest enhancements [6, 3, 2], making global data structures explicit, rather than being implicit in the pattern of sends and receives, and allowing for powerful linked data structures.
Reference: [11] <author> B. W. Kernighan and S. Lin. </author> <title> An Efficient Heuristic Procedure for Partitioning Graphs. </title> <journal> Bell System Technical Journal, </journal> <volume> 49 </volume> <pages> 291-307, </pages> <year> 1970. </year>
Reference-contexts: On the CM-5, the local accesses and floating point multiply-add cost roughly 3s and a remote access costs roughly 14s. There are numerous techniques for partitioning graphs to obtain an even balance of nodes and a minimum number of remote edges, e.g., <ref> [11, 14] </ref>. Thus, for an optimized program there would be a separate initialization step to reorganize the global graph using a cost model of the computational kernel.
Reference: [12] <author> D. Lenoski, J. Laundon, K. Gharachorloo, A. Gupta, and J. L. Hennessy. </author> <title> The Directory Based Cache Co-herance Protocol for the DASH Multiprocessor. </title> <booktitle> In In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: Split-C supports these models by programming conventions, rather than enforcing them through language constraints. Split-C borrows heavily from shared memory models in providing several threads of control within a global address space <ref> [10, 12] </ref>. Virtues of this approach include: allowing familiar languages to be used with modest enhancements [6, 3, 2], making global data structures explicit, rather than being implicit in the pattern of sends and receives, and allowing for powerful linked data structures.
Reference: [13] <author> N. K. Madsen. </author> <title> Divergence Preserving Discrete Surface Integral Methods for Maxwell's Curl Equations Using Non-Orthogonal Unstructured Grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: can be unified in the Split-C context, and Section 9 summarizes our findings. 2 An Example Irregular Application To illustrate the novel aspects of Split-C for parallel programs, we use a small, but rather tricky example application, EM3D, that models the propagation of electromagnetic waves through objects in three dimensions <ref> [13] </ref>. A preprocessing step casts this into a simple computation on an irregular bipartite graph containing nodes representing electric and magnetic field values. In EM3D, an object is divided into a grid of convex polyhedral cells (typically nonorthogonal hexahedra). <p> Edge labels (weights) represent the coefficients of the linear functions, for example, W flff is the weight used for computing ff's contribution to fl's value. Because the grids are static, these weights are constant values, which are calculated in a preprocessing step <ref> [13] </ref>. 1 typedef struct node_t - 2 double value; /* Field value */ 3 int edge_count; 4 double *coeffs; /* Edge weights */ 5 double *(*values); /* Dependency list */ 6 struct node_t *next; 7 - graph_node; 8 9 void compute_E () 10 - 11 graph_node *n; 12 int i; 14
Reference: [14] <author> A. Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partitioning Sparse Matrices with Eigenvectors of Graphs. </title> <journal> Siam J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: On the CM-5, the local accesses and floating point multiply-add cost roughly 3s and a remote access costs roughly 14s. There are numerous techniques for partitioning graphs to obtain an even balance of nodes and a minimum number of remote edges, e.g., <ref> [11, 14] </ref>. Thus, for an optimized program there would be a separate initialization step to reorganize the global graph using a cost model of the computational kernel.
Reference: [15] <author> J. Rose and G. Steele Jr. </author> <title> C*: An Extended C Language for Data Parallel Programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <volume> Vol. 2, </volume> <pages> pages 2-16, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: In other cases, e.g., blocked matrix multiplication, the particular assignment of blocks to processors less important than load balance and block size. The approach to global matrices in Split-C stems from the work on data parallel languages, especially HPF [8] and C* <ref> [15] </ref>. A key design choice was to avoid run-time shapes or dope vectors, because these are inconsistent with C and with the philosophy of least surprises. Split-C does not have the ease of portability of the HPF proposal or of higher level parallel languages.
Reference: [16] <author> A. Skjellum. </author> <title> Zipcode: A Portable Communication Layer for High Performance Multicomputing Practice and Experience. </title> <type> Unpublished draft, </type> <month> March </month> <year> 1991. </year>
Reference-contexts: Split-C differs from previous shared memory languages by providing a rich set of memory operations, not simply read and write. It does not rely on novel architectural features, nor does it assume communication has enormous overhead, thereby making bulk operations the only reasonable form of communication <ref> [16, 9] </ref>. These differences arise because of differences in the implementation assumptions. Split-C multiply on 64 Sparc processor CM-5. is targeted toward distributed memory multiprocessors with fast, flexible network hardware, including the Thinking Machines CM-5, Meiko CS-2, Cray T3D and others.
Reference: [17] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: These seem to capture most of the useful elements of shared memory, message passing, and data parallel programming in a common, familiar context. Split-C is currently implemented on the Thinking Machines Corp. CM-5, building from GCC fl Send e-mail to: Split-C@boing.CS.Berkeley.EDU and Active Messages <ref> [17] </ref> and implementations are underway for architectures with more aggressive support for global access. It has been used extensively as a teaching tool in parallel computing courses and hosts a wide variety of applications. Split-C may also be viewed as a compilation target for higher level parallel languages.
Reference: [18] <author> C.-P. Wen and K. Yelick. </author> <title> Parallel Timing Simulation on a Distributed Memory Multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, California, </address> <month> November </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: This was illustrated for the EM3D problem above; applications that demonstrate irregularity in both time and space <ref> [4, 18] </ref> also benefit from these features. Split-C differs from previous shared memory languages by providing a rich set of memory operations, not simply read and write.
References-found: 18

