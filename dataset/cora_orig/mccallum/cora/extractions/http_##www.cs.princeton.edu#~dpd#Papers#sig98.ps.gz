URL: http://www.cs.princeton.edu/~dpd/Papers/sig98.ps.gz
Refering-URL: http://www.cs.princeton.edu/~dpd/Research.html
Root-URL: http://www.cs.princeton.edu
Title: MAPS: Multiresolution Adaptive Parameterization of Surfaces  
Author: Aaron W. F. Lee Wim Sweldens Peter Schroder David Dobkin 
Keyword: CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation Display Algorithms, Viewing Algorithms; I.3.5 [Computer Graphics]: ComputationalGeometry and Object Modeling Curve, Surface, Solid and Object Representations, Hierarchy and Geometric Transformations, Object Hierarchies. Additional Key Words and Phrases: Meshes, surface parameterization, mesh simplification, remeshing, texture mapping, multiresolution, subdivision surfaces, Loop scheme.  
Affiliation: Princeton University  Bell Laboratories  Caltech Lawrence Cowsar Bell Laboratories  Princeton University  
Abstract: Figure 1: Overview of our algorithm. Top left: a scanned input mesh (courtesy Cyber-ware). Next the parameter or base domain, obtained through mesh simplification. Top right: regions of the original mesh colored accordingto their assigned base domain triangle. Bottom left: adaptive remeshing with subdivision connectivity (e = 1%). Bottom Abstract We construct smooth parameterizations of irregular connectivity triangulations of arbitrary genus 2-manifolds. Our algorithm uses hierarchical simplification to efficiently induce a parameterization of the original mesh over a base domain consisting of a small number of triangles. This initial parameterization is further improved through a hierarchical smoothing procedure based on Loop subdivision applied in the parameter domain. Our method supports both fully automatic and user constrained operations. In the latter, we accommodate point and edge constraints to force the alignment fl wailee@cs.princeton.edu wim@bell-labs.com ps@cs.caltech.edu cowsar@bell-labs.com dpd@cs.princeton.edu of iso-parameter lines with desired features. We show how to use the parameterization for fast, hierarchical subdivision connectivity remeshing with guaranteed error bounds. The remeshing algorithm constructs an adaptively subdivided mesh directly without first resorting to uniform subdivision followed by subsequent sparsifica-tion. It thus avoids the exponential cost of the latter. Our parameterizations are also useful for texture mapping and morphing applications, among others. middle: multiresolution edit.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BAJAJ, C. L., BERNADINI, F., CHEN, J., AND SCHIKORE, D. R. </author> <title> Automatic Reconstruction of 3D CAD Models. </title> <type> Tech. Rep. 96-015, </type> <institution> Purdue University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: This work falls into two main categories: (1) algorithms which build a smoothly parameterized approximation of a set of samples (e.g. <ref> [14, 1, 17] </ref>), and (2) algorithms which remesh an existing mesh with the goal of applying classical multiresolution approaches [7, 8]. A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification [4]. <p> Since the approach only uses subdivision, small features in the original mesh can only be resolved accurately by increasing the number of triangles in the base domain accordingly. A similar approach, albeit using A-patches, was described by Bajaj and co-workers <ref> [1] </ref>. From the point of view of constructing parameterizations, the main drawback of algorithms in this class is that the number of triangles in the base domain depends heavily on the geometric complexity of the goal surface. This problem was addressed in work of Krishnamurthy and Levoy [17].
Reference: [2] <author> BROWN, P. J. C., AND FAIGLE, C. T. </author> <title> A Robust Efficient Algorithm for Point Location in Triangulations. </title> <type> Tech. rep., </type> <institution> Cambridge University, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: This is a standard point location problem in an irregular triangulation. We use the point location algorithm of Brown and Faigle <ref> [2] </ref> which avoids looping that can occur with non-Delaunay meshes [10, 9].
Reference: [3] <author> CERTAIN, A., POPOVI C, J., DEROSE, T., DUCHAMP, T., SALESIN, D., AND STUETZLE, W. </author> <title> Interactive Multiresolution Surface Viewing. </title> <booktitle> In Computer Graphics (SIGGRAPH 96 Proceedings), </booktitle> <pages> 91-98, </pages> <year> 1996. </year>
Reference-contexts: Multiresolution representations are now established as a funda-mental component in addressing these issues. Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces <ref> [19, 20, 7, 3] </ref>. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) [12]; see [11] for a review. <p> Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. <p> Using such a mapping, the original surface can be remeshed using subdivision connectivity. After this conversion step, adaptive simplification, compression, progressive transmission, rendering, and editing become simple and efficient operations <ref> [3, 8, 28] </ref>. Eck et al. arrive at the base domain through a Voronoi tiling of the original mesh. <p> During remeshing we take advantage of the original fine to coarse hierarchy to output a sparse, adaptive, subdivision connectivity mesh directly without resorting to a depth first oracle [22] or the need to produce a uniform subdivision connectivity mesh at exponential cost followed by wavelet thresholding <ref> [3] </ref>. 2 Hierarchical Surface Representation In this section we describe the main components of our algorithm, coarsification and map construction. We begin by fixing our notation. 2.1 Notation When describing surfaces mathematically, it is useful to separate the topological and geometric information. <p> The standard wavelet algorithms used, e.g., in image compression, start from the finest level, compute the wavelet transform, and then obtain an efficient representation by discarding small wavelet coefficients. Eck et al. [7, 8] as well as Certain et al. <ref> [3] </ref> follow a similar approach: remesh using a uniformly subdivided grid followed by decimation through wavelet thresholding. This has the drawback that in order to resolve a small local feature on the original mesh, one may need to subdivide to a very fine level.
Reference: [4] <author> COHEN, J., MANOCHA, D., AND OLANO, M. </author> <title> Simplifying Polygonal Models Using Successive Mappings. </title> <booktitle> In Proceedings IEEE Visualization 97, </booktitle> <pages> 395-402, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification <ref> [4] </ref>. We emphasize that our goal is the construction of mappings when none are given. <p> However, finding such a plane, which may not even exist, can be expensive and involves linear programming <ref> [4] </ref>. Instead, we use the conformal map z a [6] which minimizes metric distortion to map the neighborhood of a removed vertex into the plane. Let fig be a vertex to be removed.
Reference: [5] <author> DOBKIN, D., AND KIRKPATRICK, D. </author> <title> A Linear Algorithm for Determining the Separation of Convex Polyhedra. </title> <booktitle> Journal of Algorithms 6 (1985), </booktitle> <pages> 381-392. </pages>
Reference-contexts: The depth of these hierarchies appears reasonable in practice, though can vary considerably for the same dataset [13]. Our approach is similar in spirit, but inspired by the hierarchy proposed by Dobkin and Kirkpatrick (DK) <ref> [5] </ref>, which guarantees that the number of levels L is O (logN). While the original DK hierarchy is built for convex polyhedra, we show how the idea behind DK can be used for general polyhedra. <p> Consequently, no more than half of the vertices can be of outdegree 12 or the top vertex removal, in the middle half-edge collapse, and edge collapse at the bottom. more. Thus it is guaranteed that at least 1=24 of the vertices will be removed at each level <ref> [5] </ref>. In practice, it turns out one can remove roughly 1=4 of the vertices reflecting the fact that the graph is four-colorable. Given that a constant fraction can be removed on each level, the number of levels behaves as O (log N).
Reference: [6] <author> DUCHAMP, T., CERTAIN, A., DEROSE, T., AND STUETZLE, W. </author> <title> Hierarchical Computationof PL harmonicEmbeddings. </title> <type> Tech. rep., </type> <institution> University of Washington, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Runtimes for the algorithm can be long because of the many harmonic map computations. This problem was recently addressed by Duchamp and co-workers <ref> [6] </ref>, who reduced the harmonic map computations from their initial O (N 2 ) complexity to O (N log N) through hierarchical preconditioning. The hierarchy construction they employed for use in a multigrid solver is related to our hierarchy construction. <p> However, finding such a plane, which may not even exist, can be expensive and involves linear programming [4]. Instead, we use the conformal map z a <ref> [6] </ref> which minimizes metric distortion to map the neighborhood of a removed vertex into the plane. Let fig be a vertex to be removed.
Reference: [7] <author> ECK, M., DEROSE, T., DUCHAMP, T., HOPPE, H., LOUNSBERY, M., AND STUETZLE, W. </author> <title> Multiresolution Analysis of Arbitrary Meshes. </title> <booktitle> In Computer Graphics (SIGGRAPH 95 Proceedings), </booktitle> <pages> 173-182, </pages> <year> 1995. </year>
Reference-contexts: Multiresolution representations are now established as a funda-mental component in addressing these issues. Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces <ref> [19, 20, 7, 3] </ref>. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) [12]; see [11] for a review. <p> This work falls into two main categories: (1) algorithms which build a smoothly parameterized approximation of a set of samples (e.g. [14, 1, 17]), and (2) algorithms which remesh an existing mesh with the goal of applying classical multiresolution approaches <ref> [7, 8] </ref>. A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification [4]. We emphasize that our goal is the construction of mappings when none are given. <p> Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. <p> The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. To overcome this problem, Eck and co-workers <ref> [7] </ref> developed an algorithm to compute smooth parameterizations of high resolution polyhedral meshes over a low face count base domain. Using such a mapping, the original surface can be remeshed using subdivision connectivity. <p> Eck et al. arrive at the base domain through a Voronoi tiling of the original mesh. Using a sequence of local harmonic maps, a parameterization which is smooth over each triangle in the base domain and which meets with C 0 continuity at base domain edges <ref> [7, Plate 1 (f)] </ref> is constructed. Runtimes for the algorithm can be long because of the many harmonic map computations. <p> We use a fast coar-sification strategy to define the base domain, avoiding the potential difficulties of finding Voronoi tiles <ref> [7, 16] </ref>. Since our algorithm proceeds from fine to coarse, correspondence problems found in coarse to fine strategies [17] are avoided, and all features are correctly resolved. We use conformal maps for continued remapping during coarsification to immediately produce a global parameterization of the original mesh. <p> We use conformal maps for continued remapping during coarsification to immediately produce a global parameterization of the original mesh. This map is further improved through the use of a hierarchical Loop smoothing procedure obviating the need for iterative numerical solvers <ref> [7] </ref>. Since the procedure is performed globally, derivative discontinuities at the edges of the base domain are avoided [7]. In contrast to fully automatic methods [7], the algorithm supports vertex and edge tags [14] to constrain the parameterization to align with selected features; however, the user is not required to specify <p> This map is further improved through the use of a hierarchical Loop smoothing procedure obviating the need for iterative numerical solvers <ref> [7] </ref>. Since the procedure is performed globally, derivative discontinuities at the edges of the base domain are avoided [7]. In contrast to fully automatic methods [7], the algorithm supports vertex and edge tags [14] to constrain the parameterization to align with selected features; however, the user is not required to specify the entire patch network [17]. <p> This map is further improved through the use of a hierarchical Loop smoothing procedure obviating the need for iterative numerical solvers <ref> [7] </ref>. Since the procedure is performed globally, derivative discontinuities at the edges of the base domain are avoided [7]. In contrast to fully automatic methods [7], the algorithm supports vertex and edge tags [14] to constrain the parameterization to align with selected features; however, the user is not required to specify the entire patch network [17]. <p> The alternative is to use some brute force triangle unflipping mechanism. We have found the following scheme to work well: adjust the parameter values of every vertex whose 2-neighborhood contains a flipped triangle, by replacing them with the averaged parameter values of its 1-ring neighbors <ref> [7] </ref>. base domain triangle is a bijection, triangles do not in general get mapped to triangles. <p> We also show how to efficiently construct an adaptive remeshing with guaranteed error bounds. 4.1 Uniform Remeshing Since P is a bijection, we can use P 1 to map the base domain to the original mesh. We follow the strategy used in <ref> [7] </ref>: regu larly (1:4) subdivide the base domain and use the inverse map to obtain a regular connectivity remeshing. <p> The standard wavelet algorithms used, e.g., in image compression, start from the finest level, compute the wavelet transform, and then obtain an efficient representation by discarding small wavelet coefficients. Eck et al. <ref> [7, 8] </ref> as well as Certain et al. [3] follow a similar approach: remesh using a uniformly subdivided grid followed by decimation through wavelet thresholding. <p> Note how the iso-parameter lines conform perfectly to the initially tagged features. This dataset demonstrates one of the advantages of our method inclusion of feature constraintsover the earlier work of Eck et al. <ref> [7] </ref>. In the original PM paper [12, Figure 12], Hoppe shows the simplification of the fandisk based on Eck's algorithm which does not use tagging. He points out that the multiresolution approximation is quite poor at low triangle counts and consequently requires many triangles to achieve high accuracy.
Reference: [8] <author> ECK, M., AND HOPPE, H. </author> <title> Automatic Reconstruction of B-Spline Surfaces of Arbitrary Topological Type. </title> <booktitle> In Computer Graphics (SIGGRAPH 96 Proceedings), </booktitle> <pages> 325-334, </pages> <year> 1996. </year>
Reference-contexts: This work falls into two main categories: (1) algorithms which build a smoothly parameterized approximation of a set of samples (e.g. [14, 1, 17]), and (2) algorithms which remesh an existing mesh with the goal of applying classical multiresolution approaches <ref> [7, 8] </ref>. A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification [4]. We emphasize that our goal is the construction of mappings when none are given. <p> Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. <p> Using such a mapping, the original surface can be remeshed using subdivision connectivity. After this conversion step, adaptive simplification, compression, progressive transmission, rendering, and editing become simple and efficient operations <ref> [3, 8, 28] </ref>. Eck et al. arrive at the base domain through a Voronoi tiling of the original mesh. <p> The standard wavelet algorithms used, e.g., in image compression, start from the finest level, compute the wavelet transform, and then obtain an efficient representation by discarding small wavelet coefficients. Eck et al. <ref> [7, 8] </ref> as well as Certain et al. [3] follow a similar approach: remesh using a uniformly subdivided grid followed by decimation through wavelet thresholding.
Reference: [9] <author> GARLAND, M., AND HECKBERT, P. S. </author> <title> Fast Polygonal Approximation of Terrains and Height Fields. </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-181, CS Dept., Carnegie Mellon U., </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: This is a standard point location problem in an irregular triangulation. We use the point location algorithm of Brown and Faigle [2] which avoids looping that can occur with non-Delaunay meshes <ref> [10, 9] </ref>.
Reference: [10] <author> GUIBAS, L., AND STOLFI, J. </author> <title> Primitives for the Manipulation of General Subdivisions and the Computationof Voronoi Diagrams. </title> <journal> ACM Transactions on Graphics 4, </journal> <month> 2 (April </month> <year> 1985), </year> <pages> 74-123. </pages>
Reference-contexts: This is a standard point location problem in an irregular triangulation. We use the point location algorithm of Brown and Faigle [2] which avoids looping that can occur with non-Delaunay meshes <ref> [10, 9] </ref>.
Reference: [11] <author> HECKBERT, P. S., AND GARLAND, M. </author> <title> Survey of Polygonal Surface Simplification Algorithms. </title> <type> Tech. rep., </type> <institution> Carnegie Mellon University, </institution> <year> 1997. </year>
Reference-contexts: Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces [19, 20, 7, 3]. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) [12]; see <ref> [11] </ref> for a review. In either case, the objective is to represent triangulated 2-manifolds in an efficient and flexible way, and to use this description in fast algorithms addressing the challenges mentioned above. Our approach fits in the first group, but draws on ideas from the second group.
Reference: [12] <author> HOPPE, H. </author> <title> Progressive Meshes. </title> <booktitle> In Computer Graphics (SIGGRAPH 96 Proceedings), </booktitle> <pages> 99-108, </pages> <year> 1996. </year>
Reference-contexts: Multiresolution representations are now established as a funda-mental component in addressing these issues. Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces [19, 20, 7, 3]. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) <ref> [12] </ref>; see [11] for a review. In either case, the objective is to represent triangulated 2-manifolds in an efficient and flexible way, and to use this description in fast algorithms addressing the challenges mentioned above. Our approach fits in the first group, but draws on ideas from the second group. <p> Several approaches for such mesh simplification have been proposed, most notably progressive meshes (PM) <ref> [12] </ref>. In PM the basic operation is the edge collapse. A sequence of such atomic operations is prioritized based on approximation error. The linear sequence of edge collapses can be partially ordered based on topological dependence [25, 13], which defines levels in a hierarchy. <p> Note how the iso-parameter lines conform perfectly to the initially tagged features. This dataset demonstrates one of the advantages of our method inclusion of feature constraintsover the earlier work of Eck et al. [7]. In the original PM paper <ref> [12, Figure 12] </ref>, Hoppe shows the simplification of the fandisk based on Eck's algorithm which does not use tagging. He points out that the multiresolution approximation is quite poor at low triangle counts and consequently requires many triangles to achieve high accuracy. <p> He points out that the multiresolution approximation is quite poor at low triangle counts and consequently requires many triangles to achieve high accuracy. The comparison between our Figure 13 and Figure 12 in <ref> [12] </ref> demonstrates that our multires-olution algorithm which incorporates feature tagging solves these problems. Another example of constrained parameterization and subsequent adaptive remeshing is shown in Figure 14. The original dataset (100000 triangles) is shown on the left. The red lines indicate user supplied feature constraints which may facilitate subsequent animation.
Reference: [13] <author> HOPPE, H. </author> <title> View-Dependent Refinement of Progressive Meshes. </title> <booktitle> In Computer Graphics (SIGGRAPH 97 Proceedings), </booktitle> <pages> 189-198, </pages> <year> 1997. </year>
Reference-contexts: In PM the basic operation is the edge collapse. A sequence of such atomic operations is prioritized based on approximation error. The linear sequence of edge collapses can be partially ordered based on topological dependence <ref> [25, 13] </ref>, which defines levels in a hierarchy. The depth of these hierarchies appears reasonable in practice, though can vary considerably for the same dataset [13]. <p> The linear sequence of edge collapses can be partially ordered based on topological dependence [25, 13], which defines levels in a hierarchy. The depth of these hierarchies appears reasonable in practice, though can vary considerably for the same dataset <ref> [13] </ref>. Our approach is similar in spirit, but inspired by the hierarchy proposed by Dobkin and Kirkpatrick (DK) [5], which guarantees that the number of levels L is O (logN).
Reference: [14] <author> HOPPE, H., DEROSE, T., DUCHAMP, T., HALSTEAD, M., JIN, H., MCDON-ALD, J., SCHWEITZER, J., AND STUETZLE, W. </author> <title> Piecewise Smooth Surface Reconstruction. </title> <booktitle> In Computer Graphics (SIGGRAPH 94 Proceedings), </booktitle> <pages> 295-302, </pages> <year> 1994. </year>
Reference-contexts: This work falls into two main categories: (1) algorithms which build a smoothly parameterized approximation of a set of samples (e.g. <ref> [14, 1, 17] </ref>), and (2) algorithms which remesh an existing mesh with the goal of applying classical multiresolution approaches [7, 8]. A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification [4]. <p> We emphasize that our goal is the construction of mappings when none are given. In the following two sections, we discuss related work and contrast it to our approach. 1.1.1 Approximation of a Given Set of Samples Hoppe and co-workers <ref> [14] </ref> describe a fully automatic algorithm to approximate a given polyhedral mesh with Loop subdivision patches [18] respecting features such as edges and corners. Their algorithm uses a non-linear optimization procedure taking into account approximation error and the number of triangles of the base domain. <p> Since the procedure is performed globally, derivative discontinuities at the edges of the base domain are avoided [7]. In contrast to fully automatic methods [7], the algorithm supports vertex and edge tags <ref> [14] </ref> to constrain the parameterization to align with selected features; however, the user is not required to specify the entire patch network [17]. <p> Tag all the edges in the path as feature edges. First tag v 1 and v I , so called dart points <ref> [14] </ref>, as unremovable so they are guaranteed to end up in the base domain. Let v i be the first vertex on the interior of the path which gets marked for removal in the DK hierarchy, say, when going from level l to l 1. <p> When the mesh is tagged, we cannot apply smoothing across the tagged edges since this would break the alignment with the features. Therefore, we use modified versions of Loop which can deal with corners, dart points and feature edges <ref> [14, 23, 26] </ref> (see Figure 13). but this time with respect to a Loop smoothed parameterization.
Reference: [15] <author> KIRKPATRICK, D. </author> <title> Optimal Search in Planar Subdivisions. </title> <journal> SIAM J. Comput. </journal> <volume> 12 (1983), </volume> <pages> 28-35. </pages>
Reference-contexts: A note on complexity: The point location algorithm is essen tially a walk on the finest level mesh with complexity O ( p erarchical point location algorithms, which have asymptotic complexity O (logN), exist <ref> [15] </ref> but have a much larger constant. Given that we schedule the queries in a systematic order, we almost always have an excellent starting guess and observe a constant number of steps.
Reference: [16] <author> KLEIN, A., CERTAIN, A., DEROSE, T., DUCHAMP, T., AND STUETZLE, W. </author> <title> Vertex-based Delaunay Triangulation of Meshes of Arbitrary Topological Type. </title> <type> Tech. rep., </type> <institution> University of Washington, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: The hierarchy construction they employed for use in a multigrid solver is related to our hierarchy construction. The initial Voronoi tile construction relies on a number of heuris-tics which render the overall algorithm fragile (for an improved version see <ref> [16] </ref>). Moreover, there is no explicit control over the number of triangles in the base domain or the placement of patch boundaries. The algorithm generates only uniformly subdivided meshes which later can be decimated through classical wavelet methods. <p> We use a fast coar-sification strategy to define the base domain, avoiding the potential difficulties of finding Voronoi tiles <ref> [7, 16] </ref>. Since our algorithm proceeds from fine to coarse, correspondence problems found in coarse to fine strategies [17] are avoided, and all features are correctly resolved. We use conformal maps for continued remapping during coarsification to immediately produce a global parameterization of the original mesh.
Reference: [17] <author> KRISHNAMURTHY, V., AND LEVOY, M. </author> <title> Fitting Smooth Surfaces to Dense Polygon Meshes. </title> <booktitle> In Computer Graphics (SIGGRAPH 96 Proceedings), </booktitle> <pages> 313-324, </pages> <year> 1996. </year>
Reference-contexts: This work falls into two main categories: (1) algorithms which build a smoothly parameterized approximation of a set of samples (e.g. <ref> [14, 1, 17] </ref>), and (2) algorithms which remesh an existing mesh with the goal of applying classical multiresolution approaches [7, 8]. A related, though quite different problem, is the maintenance of a given parameterization during mesh simplification [4]. <p> From the point of view of constructing parameterizations, the main drawback of algorithms in this class is that the number of triangles in the base domain depends heavily on the geometric complexity of the goal surface. This problem was addressed in work of Krishnamurthy and Levoy <ref> [17] </ref>. They approximate densely sampled geometry with bi-cubic spline patches and displacement maps. Arguing that a fully automatic system cannot put iso-parameter lines where a skilled animator would want them, they require the user to lay out the entire network of top level spline patch boundaries. <p> Additionally, given that the procedure works from coarse to fine, it is possible for the procedure to latch onto the wrong surface in regions of high curvature <ref> [17, Figure 7] </ref>. 1.1.2 Remeshing Lounsbery and co-workers [19, 20] were the first to propose algorithms to extend classical multiresolution analysis to arbitrary topology surfaces. Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. [22, 7, 27, 8, 3, 28]). <p> We use a fast coar-sification strategy to define the base domain, avoiding the potential difficulties of finding Voronoi tiles [7, 16]. Since our algorithm proceeds from fine to coarse, correspondence problems found in coarse to fine strategies <ref> [17] </ref> are avoided, and all features are correctly resolved. We use conformal maps for continued remapping during coarsification to immediately produce a global parameterization of the original mesh. This map is further improved through the use of a hierarchical Loop smoothing procedure obviating the need for iterative numerical solvers [7]. <p> In contrast to fully automatic methods [7], the algorithm supports vertex and edge tags [14] to constrain the parameterization to align with selected features; however, the user is not required to specify the entire patch network <ref> [17] </ref>. <p> This indicates how user supplied constraints force domain patches to align with desired features. Other enforced patch boundaries are the eyebrows, center of the nose, and middle of lips (see red lines in left image). This example illustrates how one places constraints like Krishnamurthy and Levoy <ref> [17] </ref>. We remove the need in their algorithms to specify the entire base domain. A user may want to control patch outlines for editing in one region (e.g., on the face), but may not care about what happens in other regions (e.g., the back of the head).
Reference: [18] <author> LOOP, C. </author> <title> Smooth Subdivision Surfaces Based on Triangles. </title> <type> Master's thesis, </type> <institution> University of Utah, Department of Mathematics, </institution> <year> 1987. </year>
Reference-contexts: In the following two sections, we discuss related work and contrast it to our approach. 1.1.1 Approximation of a Given Set of Samples Hoppe and co-workers [14] describe a fully automatic algorithm to approximate a given polyhedral mesh with Loop subdivision patches <ref> [18] </ref> respecting features such as edges and corners. Their algorithm uses a non-linear optimization procedure taking into account approximation error and the number of triangles of the base domain. The result is a smooth parameterization of the original polyhedral mesh over the base domain.
Reference: [19] <author> LOUNSBERY, M. </author> <title> Multiresolution Analysis for Surfaces of Arbitrary Topological Type. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Multiresolution representations are now established as a funda-mental component in addressing these issues. Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces <ref> [19, 20, 7, 3] </ref>. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) [12]; see [11] for a review. <p> Additionally, given that the procedure works from coarse to fine, it is possible for the procedure to latch onto the wrong surface in regions of high curvature [17, Figure 7]. 1.1.2 Remeshing Lounsbery and co-workers <ref> [19, 20] </ref> were the first to propose algorithms to extend classical multiresolution analysis to arbitrary topology surfaces. Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. [22, 7, 27, 8, 3, 28]).
Reference: [20] <author> LOUNSBERY, M., DEROSE, T., AND WARREN, J. </author> <title> Multiresolution Analysis for Surfaces of Arbitrary TopologicalType. </title> <journal> Transactions on Graphics 16, </journal> <month> 1 (January </month> <year> 1997), </year> <pages> 34-73. </pages>
Reference-contexts: Multiresolution representations are now established as a funda-mental component in addressing these issues. Two schools exist. One approach extends classical multiresolution analysis and subdivision techniques to arbitrary topology surfaces <ref> [19, 20, 7, 3] </ref>. The alternative is more general and is based on sequential mesh simplification, e.g., progressive meshes (PM) [12]; see [11] for a review. <p> Additionally, given that the procedure works from coarse to fine, it is possible for the procedure to latch onto the wrong surface in regions of high curvature [17, Figure 7]. 1.1.2 Remeshing Lounsbery and co-workers <ref> [19, 20] </ref> were the first to propose algorithms to extend classical multiresolution analysis to arbitrary topology surfaces. Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. [22, 7, 27, 8, 3, 28]).
Reference: [21] <author> M UCKE, E. P. </author> <title> Shapes and Implementations in Three-Dimensional Geometry. </title> <type> Technical Report UIUCDCS-R-93-1836, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: The application was written in C++ using standard computational geometry data structures, see e.g. <ref> [21] </ref>, and all timings re ported in this section were measured on a 200 MHz PentiumPro per sonal computer. finest (L = 15) with 12946, intermediate (l = 8) with 1530, and coarsest (l = 0) with 168 triangles. Feature edges, dart and corner vertices survive on the base domain.
Reference: [22] <author> SCHR ODER, P., AND SWELDENS, W. </author> <title> Spherical Wavelets: Efficiently Representing Functions on the Sphere. </title> <booktitle> In Computer Graphics (SIGGRAPH 95 Proceedings), Annual Conference Series, </booktitle> <year> 1995. </year>
Reference-contexts: Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. <p> During remeshing we take advantage of the original fine to coarse hierarchy to output a sparse, adaptive, subdivision connectivity mesh directly without resorting to a depth first oracle <ref> [22] </ref> or the need to produce a uniform subdivision connectivity mesh at exponential cost followed by wavelet thresholding [3]. 2 Hierarchical Surface Representation In this section we describe the main components of our algorithm, coarsification and map construction.
Reference: [23] <author> SCHWEITZER, J. E. </author> <title> Analysis and Application of Subdivision Surfaces. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1996. </year>
Reference-contexts: When the mesh is tagged, we cannot apply smoothing across the tagged edges since this would break the alignment with the features. Therefore, we use modified versions of Loop which can deal with corners, dart points and feature edges <ref> [14, 23, 26] </ref> (see Figure 13). but this time with respect to a Loop smoothed parameterization.
Reference: [24] <author> SPANIER, E. H. </author> <title> Algebraic Topology. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: We begin by fixing our notation. 2.1 Notation When describing surfaces mathematically, it is useful to separate the topological and geometric information. To this end we introduce some notation adapted from <ref> [24] </ref>.
Reference: [25] <author> XIA, J. C., AND VARSHNEY, A. </author> <title> Dynamic View-Dependent Simplification for Polygonal Models. </title> <booktitle> In Proceedings Visualization 96, </booktitle> <pages> 327-334, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: In PM the basic operation is the edge collapse. A sequence of such atomic operations is prioritized based on approximation error. The linear sequence of edge collapses can be partially ordered based on topological dependence <ref> [25, 13] </ref>, which defines levels in a hierarchy. The depth of these hierarchies appears reasonable in practice, though can vary considerably for the same dataset [13].
Reference: [26] <author> ZORIN, D. </author> <title> Subdivisionand MultiresolutionSurface Representations. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: When the mesh is tagged, we cannot apply smoothing across the tagged edges since this would break the alignment with the features. Therefore, we use modified versions of Loop which can deal with corners, dart points and feature edges <ref> [14, 23, 26] </ref> (see Figure 13). but this time with respect to a Loop smoothed parameterization.
Reference: [27] <author> ZORIN, D., SCHR ODER, P., AND SWELDENS, W. </author> <title> Interpolating Subdivision for Meshes with Arbitrary Topology. </title> <booktitle> In Computer Graphics (SIGGRAPH 96 Proceedings), </booktitle> <pages> 189-192, </pages> <year> 1996. </year>
Reference-contexts: Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources.
Reference: [28] <author> ZORIN, D., SCHR ODER, P., AND SWELDENS, W. </author> <title> Interactive Multiresolution Mesh Editing. </title> <booktitle> In Computer Graphics (SIGGRAPH 97 Proceedings), </booktitle> <pages> 259-268, </pages> <year> 1997. </year>
Reference-contexts: Because of its connection to the mathematical foundations of wavelets, this approach has proven very attractive (e.g. <ref> [22, 7, 27, 8, 3, 28] </ref>). The central requirement of these methods is that the input mesh have subdivision connectivity. This is generally not true for meshes derived from 3D scanning sources. <p> Using such a mapping, the original surface can be remeshed using subdivision connectivity. After this conversion step, adaptive simplification, compression, progressive transmission, rendering, and editing become simple and efficient operations <ref> [3, 8, 28] </ref>. Eck et al. arrive at the base domain through a Voronoi tiling of the original mesh. <p> In order to be able to compute the Loop smoothing map L on an adaptively subdivided grid, the grid needs to satisfy a vertex restriction criterion, i.e., if a vertex has a triangle incident to it with depth i, then it must have a complete 1-ring at level i 1 <ref> [28] </ref>. This restriction may necessitate subdividing some triangles even if they are below the error threshold. <p> We present a final example in Figure 1. The original mesh (96966 triangles) is shown on the top left, with the adaptive, subdivision connectivity remesh on the bottom left. This remesh was subsequently edited in a interactive multiresolution editing system <ref> [28] </ref> and the result is shown on the bottom middle. 6 Conclusions and Future Research We have described an algorithm which establishes smooth parameterizations for irregular connectivity, 2-manifold triangular meshes of arbitrary topology.
References-found: 28

