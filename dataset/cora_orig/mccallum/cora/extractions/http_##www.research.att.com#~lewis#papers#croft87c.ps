URL: http://www.research.att.com/~lewis/papers/croft87c.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Title: An Approach to Natural Language Processing for Document Retrieval  
Author: W. Bruce Croft David D. Lewis 
Date: April 15, 1996  
Note: (Appeared (with different pagination) in Tenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-87), New Orleans, LA, June 3-5, 1987, pp. 26-32.)  
Address: Amherst, MA 01003  
Affiliation: Computer and Information Science Department University of Massachusetts,  
Abstract: Document retrieval systems have been restricted, by the nature of the task, to techniques that can be used with large numbers of documents and broad domains. The most effective techniques that have been developed are based on the statistics of word occurrences in text. In this paper, we describe an approach to using natural language processing (NLP) techniques for what is essentially a natural language problem the comparison of a request text with the text of document titles and abstracts. The proposed NLP techniques are used to develop a request model based on "conceptual case frames" and to compare this model with the texts of candidate documents. The request model is also used to provide information to statistical search techniques that identify the candidate documents. As part of a preliminary evaluation of this approach, case frame representations of a set of requests from the CACM collection were constructed. Statistical searches carried out using dependency and relative importance information derived from the request models indicate that performance benefits can be obtained.
Abstract-found: 1
Intro-found: 1
Reference: [ALSH85] <author> Alshawi, H.; Boguraev, B.; Briscoe, T. </author> <title> "Towards a Dictionary Support Environment for Real Time Parsing". </title> <type> Technical Report, </type> <institution> Computer Laboratory, University of Cambridge, </institution> <year> 1985. </year>
Reference-contexts: Such an approach is untenable with current NLP techniques. However, some preprocessing, can be done both to improve indexing, and to save work for later parsing. The indexing task will use some superficial syntactic analysis, aided by one of the large, machine-readable dictionaries that have recently become available <ref> [ALSH85] </ref>. The next step, processing of the user query, involves using NLP in a fairly traditional mode; i.e. attempting to produce a complete representation of the meaning of the text.
Reference: [BECK75] <author> Becker, J. D. </author> <title> "The Phrasal Lexicon". </title> <type> Bolt, </type> <institution> Beranek, and Newman Inc. </institution> <note> Report No. 3081, </note> <month> May </month> <year> 1975. </year>
Reference-contexts: Our solution to this problem is based on an extension of the concept of a phrasal lexicon <ref> [BECK75] </ref>. The idea is that certain text structures larger than words are used essentially as a unit and occur frequently enough that they should have lexicon entries similar to individual words.
Reference: [BIRN81] <author> Birnbaum, L.; Selfridge, M. </author> <title> "Conceptual Analysis of Natural Language." In Inside Computer Understanding : Five Programs Plus Miniatures. Edited by R. </title> <editor> Schank and C. Riesbeck, </editor> <address> 318-353. </address> <publisher> Hillsdale : Lawrence Erlbaum, </publisher> <year> 1981. </year>
Reference-contexts: On the other hand, limitations on domain knowledge also limit the effectiveness of more semantics-oriented parsing techniques. Our proposed solution is, as one might expect, a compromise between the two classes of parsers. The overall model will be that of an expectation-based parser <ref> [SCHA75, BIRN81, CULL86] </ref> one of the more successful types of semantics-based parsers. (Other names used for this type of parser include "conceptual analyzer", "request-based parser", and "situation-action parser".) Briefly, these parsers associate a case frame [BRUC75] with certain words (particularly verbs) in their lexicons.
Reference: [BRUC75] <author> Bruce, B. </author> <title> "Case Systems for Natural Language." </title> <journal> Artificial Intelligence, </journal> <volume> 6: </volume> <month> 327-360; </month> <year> 1975. </year>
Reference-contexts: The overall model will be that of an expectation-based parser [SCHA75, BIRN81, CULL86] one of the more successful types of semantics-based parsers. (Other names used for this type of parser include "conceptual analyzer", "request-based parser", and "situation-action parser".) Briefly, these parsers associate a case frame <ref> [BRUC75] </ref> with certain words (particularly verbs) in their lexicons. Each case frame represents some real-world action, and contains slots for the various entities that take part in that action.
Reference: [CROF81] <author> Croft, W. B. </author> <title> "Document Representation in Probabilistic Models of Information Re-trieval". </title> <journal> Journal of the American Society of Information Science, </journal> <volume> 32: </volume> <month> 451-457; </month> <year> 1981. </year>
Reference-contexts: The representations of documents and information needs (requests) that are used for this model are simply sets of unweighted words or index terms. This basic approach can be extended to incorporate weighted terms <ref> [CROF81] </ref> or requests structured using Boolean operators [CROF86a]. Statistical indexing and retrieval techniques are efficient and are more effective in terms of finding relevant documents than searches based on Boolean queries and exact matching [SALT83b]. <p> The strategy developed from the probabilistic model by Croft <ref> [CROF81, CROF86a] </ref> can make use of information about the relative importance of terms and about dependencies between terms. This information is derived from the relative importance of slots in case frames and the term groupings that represent concepts.
Reference: [CROF84] <author> Croft, W.B. </author> <title> "A Comparison of the Cosine Correlation and the Modified Probabilistic Model". </title> <journal> Information Technology, </journal> <volume> 2: </volume> <month> 113-114; </month> <year> 1984. </year>
Reference-contexts: To test the effectiveness of these various methods we used them in combination with a probabilistic retrieval incorporating inverse document frequency and within document frequency weights. The use of these two weights is equivalent to the tf.idf model <ref> [SALT83b, CROF84] </ref> which is regarded as one of the best statistical search strategies. If any of these methods provided additional performance gains above and beyond those provided by the best standard methods, this would be an encouraging preliminary result.
Reference: [CROF86a] <author> Croft, W. B. </author> <title> "Boolean Queries and Term Dependencies in Probabilistic Retrieval Models". </title> <journal> Journal of the American society for Information Science, </journal> <volume> 37: </volume> <month> 71-77; </month> <year> 1986. </year>
Reference-contexts: The representations of documents and information needs (requests) that are used for this model are simply sets of unweighted words or index terms. This basic approach can be extended to incorporate weighted terms [CROF81] or requests structured using Boolean operators <ref> [CROF86a] </ref>. Statistical indexing and retrieval techniques are efficient and are more effective in terms of finding relevant documents than searches based on Boolean queries and exact matching [SALT83b]. The major disadvantage of these techniques is that the absolute level of performance in terms of effectiveness is still quite low. <p> The strategy developed from the probabilistic model by Croft <ref> [CROF81, CROF86a] </ref> can make use of information about the relative importance of terms and about dependencies between terms. This information is derived from the relative importance of slots in case frames and the term groupings that represent concepts. <p> When this was done to the 32 queries from the Communications of the ACM (CACM) collection which were used in <ref> [CROF86a] </ref> the average number of terms per request was 9.6, as opposed to 12.6 terms when a standard stoplist is used. The difference comes from the fact that a term is present in the REST representation only if it is important to the meaning of the query. <p> These dependent term groups were then used to modify the rankings of documents retrieved by a probabilistic retrieval, as was done in <ref> [CROF86a] </ref>. The algorithm used to generate dependent term groups from a REST representation is described below. It is based on generating clauses (sets of related frame names) and then replacing the frame names in the clauses with corresponding sets of terms. <p> Also, as in METHOD-2 text group: -algorithm, divide, conquer <br>- 0-order clauses: -METHOD-2-, -ACTION-1, METHOD-1 <br>- 1-order clauses: -ACTION-1-, -METHOD-1-, -ACTION-1, METHOD-2-, -METHOD-1, METHOD-2 <br>- dependent term groups: -algorithm, divide, conquer- .66 -divide, conquer- 1.0 <ref> [CROF86a] </ref>, only the top 100 documents from the initial retrieval were reranked using information from the dependent term groups.
Reference: [CROF86b] <author> Croft, W.B. </author> <title> "User-Specified Domain Knowledge for Document Retrieval". </title> <booktitle> Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> 201-206, </pages> <address> Pisa, Italy, </address> <year> 1986. </year>
Reference-contexts: The emphasis in the operation of the I 3 R system is on the identification of important concepts or topics in a user's request and the acquisition of knowledge about the domain of the request <ref> [CROF86b] </ref>. The request model that is constructed is used to provide information about important index terms and term dependencies to statistical retrieval strategies. <p> Since it would be inappropriate to expect the user to build REST structures to represent this information, we instead follow <ref> [CROF86b] </ref> and allow the user to enter a set of vocabulary terms and indicate whether certain linguistic or thesaurus-like relationships (such as SYNONYM and GENERALIZATION) hold between them.
Reference: [CROF86c] <author> Croft, W. B.; Thompson, R. </author> <title> "I 3 R: A New Approach to the Design of Document Retrieval Systems". </title> <journal> journal of the American Society for Information Science, </journal> <note> (to appear). </note>
Reference-contexts: The major disadvantage of these techniques is that the absolute level of performance in terms of effectiveness is still quite low. A number of suggestions have been made to address this problem. The design of the I 3 R system <ref> [CROF86c] </ref> was based on the observation that many of the retrieval errors made by a document retrieval system were the result of inadequate representations of information needs.
Reference: [CULL86] <author> Cullingford, Richard E. </author> <title> Natural Language Processing: A Knowledge-Engineering Approach. </title> <publisher> Totowa : Rowman & Littlefield, </publisher> <year> 1986. </year>
Reference-contexts: On the other hand, limitations on domain knowledge also limit the effectiveness of more semantics-oriented parsing techniques. Our proposed solution is, as one might expect, a compromise between the two classes of parsers. The overall model will be that of an expectation-based parser <ref> [SCHA75, BIRN81, CULL86] </ref> one of the more successful types of semantics-based parsers. (Other names used for this type of parser include "conceptual analyzer", "request-based parser", and "situation-action parser".) Briefly, these parsers associate a case frame [BRUC75] with certain words (particularly verbs) in their lexicons.
Reference: [DEJO79] <author> De Jong, </author> <title> G.F. "Skimming Stories in Real Time: An Experiment in Integrated Understanding." </title> <type> Research Report 158, </type> <institution> Yale University Department of Computer Science, </institution> <address> New Haven, Connecticut, </address> <year> 1979. </year>
Reference-contexts: The fact that the meaning representation is built incrementally also adds to robustness, since if the parse fails due to some difficult text structure, the partially formed representation is still available for matching. This style of parsing is similar to text skimming <ref> [DEJO79, TAIT82] </ref>, though research on that subject has not focused on searching for particular concepts. The above gives the flavor of our approach to applying NLP to the document retrieval task.
Reference: [DILL83] <author> Dillon, M.; Gray, </author> <title> A.S. "FASIT: A fully automatic syntactically based indexing system." </title> <journal> Journal of the American Society for Information Science. </journal> <volume> 34 </volume> <month> 99-108; </month> <year> 1983. </year>
Reference-contexts: these searches are compared to those obtained with standard indexed queries. 2 The Approach 2.1 A Representation for Science and Technology The idea of representing text in terms of concepts or units of meaning other than single words has been a theme of much research in document retrieval (for example, <ref> [SPAR74, DILL83] </ref>). Usually these units are fragments of the original text, such as noun phrases, which are extracted without the use of sophisticated NLP techniques. The problem with this approach is that all the system knows about these fragments is that their words are somehow related.
Reference: [RIJS79] <author> Van Rijsbergen, C. J. </author> <title> Information Retrieval. Second Edition. </title> <publisher> Butterworths, </publisher> <address> London; 1979. </address>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or specific permission. Copyright 1987 ACM 089791-232-2/87/0006/0026|75 cents 1 importance. For example, the probabilistic model <ref> [RIJS79] </ref> estimates the probability of relevance of a document using Bayesian classification theory. The representations of documents and information needs (requests) that are used for this model are simply sets of unweighted words or index terms.
Reference: [SALT83b] <author> Salton, G.; Fox, E.A.; Wu, H. </author> <title> "Extended Boolean information retrieval." </title> <journal> Communications of the ACM. </journal> <volume> 26 </volume> <month> 1022-1036; </month> <year> 1983. </year>
Reference-contexts: This basic approach can be extended to incorporate weighted terms [CROF81] or requests structured using Boolean operators [CROF86a]. Statistical indexing and retrieval techniques are efficient and are more effective in terms of finding relevant documents than searches based on Boolean queries and exact matching <ref> [SALT83b] </ref>. The major disadvantage of these techniques is that the absolute level of performance in terms of effectiveness is still quite low. A number of suggestions have been made to address this problem. <p> To test the effectiveness of these various methods we used them in combination with a probabilistic retrieval incorporating inverse document frequency and within document frequency weights. The use of these two weights is equivalent to the tf.idf model <ref> [SALT83b, CROF84] </ref> which is regarded as one of the best statistical search strategies. If any of these methods provided additional performance gains above and beyond those provided by the best standard methods, this would be an encouraging preliminary result.
Reference: [SCHA75] <editor> Schank, R. C., ed. </editor> <booktitle> Conceptual Information Processing. </booktitle> <publisher> Amsterdam : North Holland, </publisher> <year> 1975. </year>
Reference-contexts: On the other hand, limitations on domain knowledge also limit the effectiveness of more semantics-oriented parsing techniques. Our proposed solution is, as one might expect, a compromise between the two classes of parsers. The overall model will be that of an expectation-based parser <ref> [SCHA75, BIRN81, CULL86] </ref> one of the more successful types of semantics-based parsers. (Other names used for this type of parser include "conceptual analyzer", "request-based parser", and "situation-action parser".) Briefly, these parsers associate a case frame [BRUC75] with certain words (particularly verbs) in their lexicons.
Reference: [SMEA86] <author> Smeaton, </author> <title> A.F. "Incorporating Syntactic Information into a Document Retrieval Strategy: An Investigation." </title> <booktitle> Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> 103-113, </pages> <address> Pisa, Italy, </address> <year> 1986. </year>
Reference-contexts: There is some evidence that even simple NLP techniques can provide useful information for document re <br>- trieval systems <ref> [SMEA86, THUR86] </ref>. * The importance of an individual request and the user's interpretation of the meaning of that request provides an inherent limitation on the NLP involved [SPAR84].
Reference: [SPAR74] <author> Sparck Jones, K. </author> <title> "Automatic Indexing". </title> <journal> Journal of Documentation, </journal> <volume> 30: </volume> <month> 393-432; </month> <year> 1974. </year>
Reference-contexts: these searches are compared to those obtained with standard indexed queries. 2 The Approach 2.1 A Representation for Science and Technology The idea of representing text in terms of concepts or units of meaning other than single words has been a theme of much research in document retrieval (for example, <ref> [SPAR74, DILL83] </ref>). Usually these units are fragments of the original text, such as noun phrases, which are extracted without the use of sophisticated NLP techniques. The problem with this approach is that all the system knows about these fragments is that their words are somehow related.
Reference: [SPAR84] <author> Sparck Jones, K.; Tait, J. I. </author> <title> "Automatic Search Term Variant Generation". </title> <journal> Journal of Documentation, </journal> <volume> 40: </volume> <month> 50-66; </month> <year> 1984. </year>
Reference-contexts: There is some evidence that even simple NLP techniques can provide useful information for document re <br>- trieval systems [SMEA86, THUR86]. * The importance of an individual request and the user's interpretation of the meaning of that request provides an inherent limitation on the NLP involved <ref> [SPAR84] </ref>. Rather than attempting to process all document texts independent of individual requests, the NLP component of a document retrieval system should be used to analyze a request plus the texts of only those documents that potentially address the particular information needs expressed in that request.
Reference: [TAIT82] <author> Tait, J.I. </author> <title> "Automatic Summarizing of English Texts." </title> <type> Technical Report 47, </type> <institution> University of Cambridge Computer Laboratory, </institution> <address> Cambridge, England, </address> <year> 1982. </year>
Reference-contexts: The fact that the meaning representation is built incrementally also adds to robustness, since if the parse fails due to some difficult text structure, the partially formed representation is still available for matching. This style of parsing is similar to text skimming <ref> [DEJO79, TAIT82] </ref>, though research on that subject has not focused on searching for particular concepts. The above gives the flavor of our approach to applying NLP to the document retrieval task.
Reference: [THUR86] <author> Thurmair, G. "REALIST: </author> <title> Retrieval Aids by Linguistics and Statistics." </title> <booktitle> Proceedings of the ACM SIGIR International Conference on Research and Development in Information Retrieval, </booktitle> <pages> 138-143, </pages> <address> Pisa, Italy, </address> <year> 1986. </year>
Reference-contexts: There is some evidence that even simple NLP techniques can provide useful information for document re <br>- trieval systems <ref> [SMEA86, THUR86] </ref>. * The importance of an individual request and the user's interpretation of the meaning of that request provides an inherent limitation on the NLP involved [SPAR84].
Reference: [WOOD70] <author> Woods, W. A. </author> <title> "Transition Network Grammars for Natural Language Analysis." </title> <journal> Communications of the ACM. </journal> <volume> 13 </volume> <month> 591-606; </month> <year> 1970. </year>
Reference-contexts: The idea is that certain text structures larger than words are used essentially as a unit and occur frequently enough that they should have lexicon entries similar to individual words. Our intention is to represent these phrasal units as small, syntactic, transition net parsers <ref> [WOOD70] </ref>, and to associate the case frames to be used for expectation-based parsing with these phrasal units rather than with individual words, as is the usual method.
References-found: 21

