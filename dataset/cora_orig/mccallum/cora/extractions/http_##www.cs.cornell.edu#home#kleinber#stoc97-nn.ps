URL: http://www.cs.cornell.edu/home/kleinber/stoc97-nn.ps
Refering-URL: http://www.cs.cornell.edu/home/kleinber/kleinber.html
Root-URL: 
Title: Two Algorithms for Nearest-Neighbor Search in High Dimensions An algorithm for finding "-approximate nearest neighbors
Author: Jon M. Kleinberg 
Address: San Jose CA 95120,  Ithaca NY 14853.  
Affiliation: IBM Almaden Research Center,  Department of Computer Science, Cornell University,  
Note: (i)  on leave from  
Date: February 7, 1997  
Abstract: Representing data as points in a high-dimensional space, so as to use geometric methods for indexing, is an algorithmic technique with a wide array of uses. It is central to a number of areas such as information retrieval, pattern recognition, and statistical data analysis; many of the problems arising in these applications can involve several hundred or several thousand dimensions. We consider the nearest-neighbor problem for d-dimensional Euclidean space: we wish to pre-process a database of n points so that given a query point, one can efficiently determine its nearest neighbors in the database. There is a large literature on algorithms for this problem, in both the exact and approximate cases. The more sophisticated algorithms typically achieve a query time that is logarithmic in n at the expense of an exponential dependence on the dimension d; indeed, even the average-case analysis of heuristics such as k-d trees reveals an exponential dependence on d in the query time. In this work, we develop a new approach to the nearest-neighbor problem, based on a method for combining randomly chosen one-dimensional projections of the underlying point set. From this, we obtain the following two results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, P. Gemmell, M. Harchol, R.M. Karp, C. Kenyon, </author> <title> "Selection in the presence of noise: </title> <booktitle> the design of playoff systems," Proc. 5th ACM-SIAM SODA, </booktitle> <year> 1994. </year>
Reference-contexts: We note that the analysis of tournaments with unreliable comparisons has been the subject of a number of previous papers (see e.g. <ref> [21, 1] </ref> and the references therein); however, the actual models considered in these papers are technically fairly distinct from the constraints imposed by our application here. 2 Some Geometric Lemmas In this section, we prove some of the geometric lemmas required for the analysis of our algorithms.
Reference: [2] <author> P.K. Agarwal, J. Matousek, </author> <title> "Ray shooting and parametric search," </title> <booktitle> Proc. 24th ACM STOC, </booktitle> <year> 1992. </year>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. <ref> [42, 32, 2] </ref> and others) have required a query time of at least (exp (d) log n).
Reference: [3] <author> N. Alon, J. Spencer, </author> <title> The Probabilistic Method, </title> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: We now want to consider the VC-dimension of the collection of exceptional sets. In order to make this exposition self-contained, we present the following definitions (see e.g. <ref> [41, 3] </ref> for more background; we work within the more general framework of the former paper). A range space is a pair (P; R), where P = (; F ; ) is a probability space and R F is a collection of measurable subsets of . <p> Each range in (P d1 ; R ) is a Boolean combination of half-spaces with four literals, and thus a theorem of Dudley <ref> [19, 3] </ref> implies that its VC-dimension is at most d 0 = 8 (d + 1) log (4d + 4).
Reference: [4] <author> S. Arya, </author> <title> Nearest Neighbor Searching and Applications, </title> <type> PhD thesis, </type> <institution> University of Mary-land technical report CS-TR-3490, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively. Arya, Mount, et al. <ref> [5, 4] </ref> obtain an algorithm with query time O (exp (d) " d log n) and pre-processing O (n log n). Clarkson [12] obtained a different algorithm which improves the dependence on " to exp (d) " (d1)=2 .
Reference: [5] <author> S. Arya, D. Mount, N. Netanyahu, R. Silverman, A. Wu, </author> <title> "An optimal algorithm for approximate nearest neighbor searching in fixed dimensions," </title> <booktitle> Proc. 5th ACM-SIAM SODA, </booktitle> <year> 1994. </year> <note> Extended version appears as University of Maryland technical report CS-TR-3568, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively. Arya, Mount, et al. <ref> [5, 4] </ref> obtain an algorithm with query time O (exp (d) " d log n) and pre-processing O (n log n). Clarkson [12] obtained a different algorithm which improves the dependence on " to exp (d) " (d1)=2 . <p> This reflects the well-known "curse of dimensionality" [12] that appears throughout computational geometry; it is particularly unfortunate in the present setting, since the dimension is quite large in many of the applications cited above. To quote from Arya, Mount, et al. <ref> [5] </ref>, ". . . if the dimension is significantly larger than log n (as it for a number of practical instances), there are no approaches we know of that are significantly faster than brute-force search." In this work, we develop a new approach to the nearest-neighbor problem, based on a method
Reference: [6] <author> J.L. Bentley, </author> <title> "Multidimensional binary search trees used for associative searching," </title> <journal> Comm. ACM, </journal> <volume> 18(1975), </volume> <pages> pp. 509-517. </pages>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. [42, 32, 2] and others) have required a query time of at least (exp (d) log n). Indeed, even the average-case analysis of heuristics such as k-d trees <ref> [22, 6, 26, 39] </ref>, for restricted cases such as input points distributed uniformly over a bounded subset of R d , reveals an exponential dependence on d in the query time.
Reference: [7] <author> M.W. Berry, S.T. Dumais, </author> <title> A.T. Shippy, "A case study of latent semantic indexing," U.T. </title> <type> Knoxville technical report CS-95-271, </type> <month> January </month> <year> 1995. </year>
Reference-contexts: standard vector-space methods in information retrieval [37, 38, 8, 15] can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred <ref> [15, 7] </ref>. Multimedia retrieval applications such as [23] can also involve several hundred dimensions. In this paper, we consider one of the most commonly studied settings of the nearest-neighbor problem, both theoretically and in applications: namely, the case of points in R d with the Euclidean metric.
Reference: [8] <author> C. Buckley, A. Singhal, M. Mitra, and G. Salton, </author> <title> "New Retrieval Approaches Using SMART: </title> <booktitle> TREC 4," Proceedings of the Fourth Text Retrieval Conference, </booktitle> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year> <month> 16 </month>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval <ref> [37, 38, 8, 15] </ref>, pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40]. <p> While the design of theoretically efficient nearest-neighbor algorithms has been oriented mainly towards a small number of dimensions, many of the core applications of the nearest-neighbor problem take place in a very large number of dimensions. For example, the application of standard vector-space methods in information retrieval <ref> [37, 38, 8, 15] </ref> can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred [15, 7].
Reference: [9] <author> P.B. Callahan, </author> <title> S.R. Kosaraju, "A decomposition of multi-dimensional point sets with applications to k-nearest-neighbors and n-body potential fields," </title> <booktitle> Proc. 24th ACM STOC, </booktitle> <year> 1992. </year>
Reference: [10] <author> B. Chor, M. Sudan, </author> <title> "A geometric approach to betweenness," </title> <booktitle> Proc. 3rd European Symposium on Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d . The use of random projections onto lines has appeared in a number of contexts in recent high-dimensional geometric constructions and algorithms (e.g. <ref> [30, 24, 28, 31, 10] </ref>); thus it is worth our discussing what we gain from this approach in the setting of the nearest-neighbor problem.
Reference: [11] <author> K. Clarkson, </author> <title> "A randomized algorithm for closest-point queries," </title> <journal> SIAM J. Computing, </journal> <volume> 17(1988), </volume> <pages> pp. 830-847. </pages>
Reference-contexts: have been first undertaken by Dobkin and Lipton [17]; they provided an algorithm with query time O (2 d log n) and pre-processing O (n 2 d+1 ). (We use the term "pre-processing" to refer to the sum of the pre-processing time and storage required.) This was improved by Clarkson <ref> [11] </ref>; he gave an algorithm with query time O (exp (d) log n) and pre-processing O (n dd=2e (1+") ), where exp (d) denotes a function that grows at least as quickly as 2 d .
Reference: [12] <author> K. Clarkson, </author> <title> "An algorithm for approximate closest-point queries," </title> <booktitle> Proc. 10th ACM Symp. on Computational Geometry, </booktitle> <year> 1994. </year>
Reference-contexts: Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively. Arya, Mount, et al. [5, 4] obtain an algorithm with query time O (exp (d) " d log n) and pre-processing O (n log n). Clarkson <ref> [12] </ref> obtained a different algorithm which improves the dependence on " to exp (d) " (d1)=2 . Again, these approaches result in query times that grow exponentially with d. One can make several observations at this point. <p> Further, when one requires storage costs to be polynomial in n (for variable d), it appears that no algorithms are known with query times that improve on brute-force search once d is comparable to log n. This reflects the well-known "curse of dimensionality" <ref> [12] </ref> that appears throughout computational geometry; it is particularly unfortunate in the present setting, since the dimension is quite large in many of the applications cited above.
Reference: [13] <author> E. Cohen, D. Lewis, </author> <title> "Approximating matrix multiplication for pattern recognition tasks," </title> <booktitle> Proc. 8th ACM-SIAM SODA, </booktitle> <year> 1997. </year>
Reference-contexts: Such problems are well-solved in a small number of dimensions; but in high dimensions, Cohen and Lewis have recently observed <ref> [13] </ref> that even for approximate or average-case versions, the trivial O (dn 2 ) algorithm is the best known if one does not resort to fast matrix multiplication.
Reference: [14] <author> T.M. Cover, P.E. Hart, </author> <title> "Nearest neighbor pattern classification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13(1967), </volume> <pages> pp. 21-27. </pages>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition <ref> [14, 18] </ref>, statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40].
Reference: [15] <author> S. Deerwester, S. Dumais, T. Landauer, G. Furnas, R. Harshman, </author> <title> "Indexing by latent semantic analysis," </title> <journal> J. Soc. Info. Sci., </journal> <volume> 41(1990), </volume> <pages> pp. 391-407. </pages>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval <ref> [37, 38, 8, 15] </ref>, pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40]. <p> While the design of theoretically efficient nearest-neighbor algorithms has been oriented mainly towards a small number of dimensions, many of the core applications of the nearest-neighbor problem take place in a very large number of dimensions. For example, the application of standard vector-space methods in information retrieval <ref> [37, 38, 8, 15] </ref> can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred [15, 7]. <p> For example, the application of standard vector-space methods in information retrieval [37, 38, 8, 15] can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing <ref> [15] </ref>, but these typically are not used to reduce the dimension below several hundred [15, 7]. Multimedia retrieval applications such as [23] can also involve several hundred dimensions. <p> standard vector-space methods in information retrieval [37, 38, 8, 15] can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred <ref> [15, 7] </ref>. Multimedia retrieval applications such as [23] can also involve several hundred dimensions. In this paper, we consider one of the most commonly studied settings of the nearest-neighbor problem, both theoretically and in applications: namely, the case of points in R d with the Euclidean metric.
Reference: [16] <author> L. Devroye, T.J. Wagner, </author> <title> "Nearest neighbor methods in discrimination," </title> <journal> Handbook of Statistics, </journal> <volume> vol. 2, </volume> <editor> P.R. Krishnaiah, L.N. Kanal, eds., </editor> <publisher> North-Holland, </publisher> <year> 1982. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis <ref> [35, 16] </ref>, data compression [27], and multimedia databases [36, 23, 40].
Reference: [17] <author> D. Dobkin, R. Lipton, </author> <title> "Multidimensional search problems," </title> <journal> SIAM J. Computing, </journal> <volume> 5(1976), </volume> <pages> pp. 181-186. </pages>
Reference-contexts: There is a voluminous literature on algorithms for the Euclidean nearest-neighbor problem, and we do not attempt a comprehensive survey of it here. Establishing upper bounds on the time required to answer a nearest-neighbor query in R d appears to have been first undertaken by Dobkin and Lipton <ref> [17] </ref>; they provided an algorithm with query time O (2 d log n) and pre-processing O (n 2 d+1 ). (We use the term "pre-processing" to refer to the sum of the pre-processing time and storage required.) This was improved by Clarkson [11]; he gave an algorithm with query time O <p> As such, the algorithms appear to be simpler to implement even than the original method of Dobkin and Lipton <ref> [17] </ref>. Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d .
Reference: [18] <author> R.O. Duda, P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition <ref> [14, 18] </ref>, statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40]. <p> number of reasons among other things, the use of fixed-precision arithmetic will typically mean that the above "exact" algorithms are in fact producing answers that are only approximate anyway; and perhaps more importantly, the methods used for mapping features to numerical coordinates in many of the applications cited above (e.g. <ref> [18, 37, 23, 36] </ref>) have been chosen on heuristic grounds, and so often an "exact" answer is no more valuable than a close approximation. Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively.
Reference: [19] <author> R.M. Dudley, </author> <title> "Central limit theorems for empirical measures," </title> <journal> Annals of Prob., </journal> <volume> 6(1978), </volume> <pages> pp. 899-929. </pages>
Reference-contexts: Each range in (P d1 ; R ) is a Boolean combination of half-spaces with four literals, and thus a theorem of Dudley <ref> [19, 3] </ref> implies that its VC-dimension is at most d 0 = 8 (d + 1) log (4d + 4).
Reference: [20] <author> H. Edelsbrunner, </author> <title> Algorithms in Combinatorial Geometry, </title> <publisher> Springer, </publisher> <year> 1987. </year>
Reference-contexts: Thus, W x;y = H (xy) " H (x+y) [ H (xy) " H (x+y) : Radon's theorem (see e.g. <ref> [20] </ref>) implies that the VC-dimension of half-spaces in S d1 is d + 1. <p> Proof. A realizable trace corresponds to a full-dimensional cell in the arrangement of hyper-planes fv ` x = v ` p ij : 1 ` L; 1 i; j ng: By a standard result on hyperplane arrangements (see e.g. <ref> [20, Thm 1.3] </ref>), the number of such full-dimensional cells is at most d X i &lt; 2 Ln 2 ! d = O (n log d) 2d : (One can obtain a somewhat tighter bound in the present case by making use of the fact that many of the pairs of <p> For this one could use the algorithm given in <ref> [20] </ref>, with a running time of O (L d n 2d ). Correctness. Finally, let us show that the algorithm correctly answers approximate nearest-neighbor queries.
Reference: [21] <author> U. Feige, D. Peleg, P. Raghavan, E. Upfal, </author> <title> "Computing with unreliable information," </title> <booktitle> Proc. 22nd ACM STOC, </booktitle> <year> 1990. </year>
Reference-contexts: We note that the analysis of tournaments with unreliable comparisons has been the subject of a number of previous papers (see e.g. <ref> [21, 1] </ref> and the references therein); however, the actual models considered in these papers are technically fairly distinct from the constraints imposed by our application here. 2 Some Geometric Lemmas In this section, we prove some of the geometric lemmas required for the analysis of our algorithms.
Reference: [22] <author> R.A. Finkel, J.L. Bentley, </author> <title> "Quad trees a data structure for retrieval on composite keys," </title> <journal> Acta Inform. </journal> <volume> 4(1974), </volume> <pages> pp. 1-9. </pages>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. [42, 32, 2] and others) have required a query time of at least (exp (d) log n). Indeed, even the average-case analysis of heuristics such as k-d trees <ref> [22, 6, 26, 39] </ref>, for restricted cases such as input points distributed uniformly over a bounded subset of R d , reveals an exponential dependence on d in the query time.
Reference: [23] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, P. </author> <title> Yanker "Query by image and video content: </title> <booktitle> the QBIC system," IEEE Computer 28(1995), </booktitle> <pages> pp. 23-32. </pages>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases <ref> [36, 23, 40] </ref>. <p> Multimedia retrieval applications such as <ref> [23] </ref> can also involve several hundred dimensions. In this paper, we consider one of the most commonly studied settings of the nearest-neighbor problem, both theoretically and in applications: namely, the case of points in R d with the Euclidean metric. <p> number of reasons among other things, the use of fixed-precision arithmetic will typically mean that the above "exact" algorithms are in fact producing answers that are only approximate anyway; and perhaps more importantly, the methods used for mapping features to numerical coordinates in many of the applications cited above (e.g. <ref> [18, 37, 23, 36] </ref>) have been chosen on heuristic grounds, and so often an "exact" answer is no more valuable than a close approximation. Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively.
Reference: [24] <author> P. Frankl, H. Maehara, </author> <title> "The Johnson-Lindenstrauss lemma and the sphericity of some graphs," </title> <journal> J. Combinatorial Theory Ser. B, </journal> <volume> 44(1988), </volume> <pages> pp. 355-362. </pages>
Reference-contexts: Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d . The use of random projections onto lines has appeared in a number of contexts in recent high-dimensional geometric constructions and algorithms (e.g. <ref> [30, 24, 28, 31, 10] </ref>); thus it is worth our discussing what we gain from this approach in the setting of the nearest-neighbor problem. <p> In particular, the strongest result on distance-preserving projections of point sets is due to Frankl and Maehara <ref> [24] </ref>, strengthening a bound of Johnson and Lindenstrauss [30]; they show that projecting P onto a random subspace of dimension roughly 9" 2 log n preserves all relative inter-point distances to within a factor of 1 + ", with high probability. <p> Let " = 1 3 ". The algorithm proceeds as follows. (1) First, by a result of Frankl and Maehara <ref> [24] </ref>, we can project P into a random subspace of dimension O (" 2 log n) and preserve all relative inter-point distances to within a factor of 1 + ", with high probability.
Reference: [25] <author> M.L. Fredman, R.E. Tarjan, </author> <title> "Fibonacci heaps and their uses in improved network optimization algorithms," </title> <journal> Journal of the ACM 34(1987), </journal> <pages> pp. 596-615. 17 </pages>
Reference-contexts: As a proof, observe that a node of G with maximal out-degree is an apex. To construct an apex ordering, one iteratively deletes an apex and updates the out-degrees of all other nodes. Using a Fibonacci heap <ref> [25] </ref>, deletion can be performed in O (log n) time per operation, while decreasing the out-degree of each other node can be done in amortized constant time per operation. 8 Building the Data Structure.
Reference: [26] <author> J.K. Friedman, J.L. Bentley, R.A. Finkel, </author> <title> "An algorithm for finding best matches in logarithmic expected time," </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 3(1977), </volume> <pages> pp. 209-226. </pages>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. [42, 32, 2] and others) have required a query time of at least (exp (d) log n). Indeed, even the average-case analysis of heuristics such as k-d trees <ref> [22, 6, 26, 39] </ref>, for restricted cases such as input points distributed uniformly over a bounded subset of R d , reveals an exponential dependence on d in the query time.
Reference: [27] <author> A. Gersho, R.M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis [35, 16], data compression <ref> [27] </ref>, and multimedia databases [36, 23, 40].
Reference: [28] <author> M. Goemans, </author> <title> D.P. Williamson, "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming," </title> <journal> Journal of the ACM, </journal> <volume> 42(1995), </volume> <pages> pp. 1115-1145. </pages>
Reference-contexts: Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d . The use of random projections onto lines has appeared in a number of contexts in recent high-dimensional geometric constructions and algorithms (e.g. <ref> [30, 24, 28, 31, 10] </ref>); thus it is worth our discussing what we gain from this approach in the setting of the nearest-neighbor problem.
Reference: [29] <author> H. Hotelling, </author> <title> "Analysis of a complex of statistical variables into principal components," </title> <journal> J. Educational Psychology, </journal> <volume> 27(1933), </volume> <pages> pp. 417-441. </pages>
Reference-contexts: For example, the application of standard vector-space methods in information retrieval [37, 38, 8, 15] can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis <ref> [29] </ref> and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred [15, 7]. Multimedia retrieval applications such as [23] can also involve several hundred dimensions.
Reference: [30] <author> W.B. Johnson, J. Lindenstrauss, </author> <title> "Extensions of Lipschitz mappings into Hilbert space," </title> <booktitle> Contemporary Mathematics 26(1984), </booktitle> <pages> pp. 189-206. </pages>
Reference-contexts: Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d . The use of random projections onto lines has appeared in a number of contexts in recent high-dimensional geometric constructions and algorithms (e.g. <ref> [30, 24, 28, 31, 10] </ref>); thus it is worth our discussing what we gain from this approach in the setting of the nearest-neighbor problem. <p> In particular, the strongest result on distance-preserving projections of point sets is due to Frankl and Maehara [24], strengthening a bound of Johnson and Lindenstrauss <ref> [30] </ref>; they show that projecting P onto a random subspace of dimension roughly 9" 2 log n preserves all relative inter-point distances to within a factor of 1 + ", with high probability.
Reference: [31] <author> D. Karger, R. Motwani and M. Sudan, </author> <title> "Approximate graph coloring by semidefinite programming," </title> <booktitle> Proc. 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1994, </year> <pages> pp. 2-13. </pages>
Reference-contexts: Techniques and Basic Definitions We build our data structures from the projections of the set P onto random lines through the origin in R d . The use of random projections onto lines has appeared in a number of contexts in recent high-dimensional geometric constructions and algorithms (e.g. <ref> [30, 24, 28, 31, 10] </ref>); thus it is worth our discussing what we gain from this approach in the setting of the nearest-neighbor problem.
Reference: [32] <author> J. Matousek, </author> <title> "Reporting points in halfspaces," </title> <booktitle> Proc. 32nd IEEE FOCS, </booktitle> <year> 1991. </year>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. <ref> [42, 32, 2] </ref> and others) have required a query time of at least (exp (d) log n).
Reference: [33] <author> S. Meiser, </author> <title> "Point location in arrangements of hyperplanes," </title> <journal> Information and Computation, </journal> <volume> 106(1993), </volume> <pages> pp. 286-303. </pages>
Reference-contexts: One exception to this phenomenon is a recent algorithm of Meiser <ref> [33] </ref> (designed, as are some of the above algorithms, for the more general problem of point location in an arrangement of hyperplanes); it obtains a query time of O (d 5 log n) with storage O (n d+" ). 1 It is natural to try improving the computational requirements by only
Reference: [34] <author> R. Motwani, P. Raghavan, </author> <title> Randomized Algorithms, </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: Correctness. We will be making use of the following tail inequality (see e.g. <ref> [34] </ref>). Lemma 4.2 Let X 1 ; : : : ; X r be i.i.d. Bernoulli trials with success probability p, let X = P i X i , and let = rp. Then Pr [X &lt; (1 fl)] &lt; e 1 : A corollary of this is the following. <p> Then Pr X 2 2 fl 2 Proof. Pr X 2 1 + fl)r = Pr [X &lt; (1 fl)] &lt; e 1 : Combining this bound with Lemma 4.1, we have 1 We use the standard assumption (see e.g. Motwani and Raghavan's book <ref> [34] </ref>) of a RAM model in which one can access and randomly choose array indices of polynomial size in constant time. 13 Lemma 4.4 Let (1 + fl)d (p i ; q) d (p j ; q). Let 0 be a randomly chosen sub-multiset of .
Reference: [35] <institution> Panel on Discriminant Analysis and Clustering, National Research Council, Discriminant Analysis and Clustering, National Academy Press, </institution> <year> 1988. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis <ref> [35, 16] </ref>, data compression [27], and multimedia databases [36, 23, 40].
Reference: [36] <author> A. Pentland, R.W. Picard, and S. Sclaroff, "Photobook: </author> <title> tools for content-based manipulation of image databases," </title> <booktitle> Proceedings of the SPIE Conference on Storage and Retrieval of Image and Video Databases II, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases <ref> [36, 23, 40] </ref>. <p> number of reasons among other things, the use of fixed-precision arithmetic will typically mean that the above "exact" algorithms are in fact producing answers that are only approximate anyway; and perhaps more importantly, the methods used for mapping features to numerical coordinates in many of the applications cited above (e.g. <ref> [18, 37, 23, 36] </ref>) have been chosen on heuristic grounds, and so often an "exact" answer is no more valuable than a close approximation. Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively.
Reference: [37] <author> C.J. van Rijsbergen, </author> <note> Information Retrieval, Butterworths, 1979. Also at http://dcs.glasgow.ac.uk/Keith/Preface.html. </note>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval <ref> [37, 38, 8, 15] </ref>, pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40]. <p> While the design of theoretically efficient nearest-neighbor algorithms has been oriented mainly towards a small number of dimensions, many of the core applications of the nearest-neighbor problem take place in a very large number of dimensions. For example, the application of standard vector-space methods in information retrieval <ref> [37, 38, 8, 15] </ref> can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred [15, 7]. <p> number of reasons among other things, the use of fixed-precision arithmetic will typically mean that the above "exact" algorithms are in fact producing answers that are only approximate anyway; and perhaps more importantly, the methods used for mapping features to numerical coordinates in many of the applications cited above (e.g. <ref> [18, 37, 23, 36] </ref>) have been chosen on heuristic grounds, and so often an "exact" answer is no more valuable than a close approximation. Finding "-approximate nearest neighbors, for arbitrarily small " &gt; 0, has also been studied extensively.
Reference: [38] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval <ref> [37, 38, 8, 15] </ref>, pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases [36, 23, 40]. <p> While the design of theoretically efficient nearest-neighbor algorithms has been oriented mainly towards a small number of dimensions, many of the core applications of the nearest-neighbor problem take place in a very large number of dimensions. For example, the application of standard vector-space methods in information retrieval <ref> [37, 38, 8, 15] </ref> can result in problems with several thousand dimensions; much work has been devoted to "dimension-reduction" techniques such as principal component analysis [29] and latent semantic indexing [15], but these typically are not used to reduce the dimension below several hundred [15, 7].
Reference: [39] <author> H. Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. [42, 32, 2] and others) have required a query time of at least (exp (d) log n). Indeed, even the average-case analysis of heuristics such as k-d trees <ref> [22, 6, 26, 39] </ref>, for restricted cases such as input points distributed uniformly over a bounded subset of R d , reveals an exponential dependence on d in the query time.
Reference: [40] <editor> A.W.M. Smeulders and R. Jain, editors. </editor> <title> Image Databases and Multi-media Search. </title> <booktitle> Proceedings of the First International Workshop, </booktitle> <address> IDB-MMS '96, Amsterdam. </address> <publisher> Amsterdam University Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction The nearest-neighbor problem is central to a wide range of areas in which computational techniques are applied. Nearest-neighbor-based methods appear, for example, in algorithms for information retrieval [37, 38, 8, 15], pattern recognition [14, 18], statistics and data analysis [35, 16], data compression [27], and multimedia databases <ref> [36, 23, 40] </ref>.
Reference: [41] <author> V.N. Vapnik, A.Y. Chervonenkis, </author> <title> "On the uniform convergence of relative frequencies of events to their probabilities," </title> <journal> Theory of Prob. App., </journal> <volume> 16(1971), </volume> <pages> pp. 264-280. </pages>
Reference-contexts: We now want to consider the VC-dimension of the collection of exceptional sets. In order to make this exposition self-contained, we present the following definitions (see e.g. <ref> [41, 3] </ref> for more background; we work within the more general framework of the former paper). A range space is a pair (P; R), where P = (; F ; ) is a probability space and R F is a collection of measurable subsets of . <p> A finite subset A of is said to be a fl-sample of the range space (P; R) if for all R 2 R, we have fi fi fi jR " Aj (R) fi fi fl: A theorem of Vapnik and Chervonenkis <ref> [41] </ref> says, subject to some technical conditions, that if (P; R) has VC-dimension at most k, for a natural number k, then a finite set A of size ` fl 2 k log fl 2 + log ffi is a fl-sample of (P; R) with probability at least 1 ffi.
Reference: [42] <author> A.C. Yao, F.F. Yao, </author> <title> "A general approach to d-dimensional geometric queries," </title> <booktitle> Proc. 17th ACM STOC, </booktitle> <year> 1985. </year> <month> 18 </month>
Reference-contexts: Most of the subsequent approaches and extensions (e.g. <ref> [42, 32, 2] </ref> and others) have required a query time of at least (exp (d) log n).
References-found: 42

