URL: http://www.cs.ucsb.edu/~tyang/papers/PPoPP97.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Email: fcfu,tyangg@cs.ucsb.edu  
Title: Space and Time Efficient Execution of Parallel Irregular Computations  
Author: Cong Fu and Tao Yang 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Solving problems of large sizes is an important goal for parallel machines with multiple CPU and memory resources. In this paper, issues of efficient execution of overhead-sensitive parallel irregular computation under memory constraints are addressed. The irregular parallelism is modeled by task dependence graphs with mixed granularities. The trade-off in achieving both time and space efficiency is investigated. The main difficulty of designing efficient run-time system support is caused by the use of fast communication primitives available on modern parallel architectures. A run-time active memory management scheme and new scheduling techniques are proposed to improve memory utilization while retaining good time efficiency, and a theoretical analysis on correctness and performance is provided. This work is implemented in the context of RAPID system [5] which provides run-time support for parallelizing irregular code on distributed memory machines and the effectiveness of the proposed techniques is verified on sparse Cholesky and LU factorization with partial pivoting. The experimental results on Cray-T3D show that solvable problem sizes can be increased substantially under limited memory capacities and the loss of execution efficiency caused by the extra memory managing overhead is reasonable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. E. Blelloch, P. B. Gibbons, and Y. Matias. </author> <title> Provably Efficient Scheduling for Languages with Fine-Grained Parallelism. </title> <booktitle> In Proceedings of 7th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers. Most of previous research on scheduling [16, 19, 20] does not address memory issues. In <ref> [1] </ref>, a dynamic scheduling algorithm for directed acyclic graphs is proposed with memory space usage S 1 =p + O (D) on each processor, where S 1 is the sequential space requirement, p is the total number of processors and D is the depth of a DAG. <p> This paper assumes that each processor has a maximum space limit and the goal is to make the data space cost to be close to S 1 =p per processor in order to solve large-scale problems. The scheduling scheme we use is static in the run-time preprocessing stage while <ref> [1] </ref> and [2] use dynamic scheduling. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in par-allelizing sparse code with mixed granularities.
Reference: [2] <author> R. Blumfoe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: </author> <title> An Efficient Multi-threaded Runtime System. </title> <booktitle> In Proceedings of Fifth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207-216, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This work provides a solid theoretical ground for space-efficient scheduling, and it is still an open research problem how to integrate their techniques in practical systems as indicated by the authors. Their space model is different from ours and assumes a globally shared memory pool. The Cilk <ref> [2] </ref> run-time system addresses the space efficiency issue and its space complexity is O (S 1 ) per processor. The RAPID [5] uses at most S 1 space per processor. <p> The scheduling scheme we use is static in the run-time preprocessing stage while [1] and <ref> [2] </ref> use dynamic scheduling. This is mainly because in practice it is difficult to minimize the run-time control overhead of dynamic scheduling in par-allelizing sparse code with mixed granularities.
Reference: [3] <author> S. Chakrabarti, J. Demmel, and K. Yelick. </author> <title> Modeling the Benefits of Mixed Data and Task Parallelism. </title> <booktitle> In Proceedings of 7th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 74-83, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, Figure 2 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 2 (b) and (c) is that on processor 1, T [7; 8] is executed at time 6 by RCP while T <ref> [3; 10] </ref> is chosen instead at time 6 by MPO. <p> For MPO, T <ref> [3; 10] </ref> has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T [7; 8]'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6. <p> Another possible application of the DTS algorithm is to guide data placement in memory for shared data objects to improve both spatial and temporal locality. One future work is to extend our results for more complicated task dependence structures <ref> [3, 9, 13] </ref>. The experiments have also revealed that other space limiting factors, such as the storage of dependence information in our current implementation, affect the ability to process larger problem instances.
Reference: [4] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: The address then must be invalidated on other processors since this data object may have a new address at that processor. Data renaming would avoid this problem <ref> [4] </ref>, but it creates more complexity in indexing data objects and memory optimization. * Address buffering. We will also use the RMA fea ture to transfer addresses. Without providing address buffering, a processor cannot re-send address information unless the destination processor has read the previous address package.
Reference: [5] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 237-244, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The applications that have been used to demonstrate the effectiveness of this parallelism model are sparse matrix problems <ref> [5, 17] </ref>. The main difficulties for parallelizing sparse code are that the granularity of computation and communication is non-uniform, dependence structures may change dynamically and code performance is sensitive to the software overhead introduced by system support. <p> We present scheduling techniques for optimizing memory utilization and minimizing the potential overhead for space allocation and address notification. The proposed techniques are implemented in the RAPID software tool <ref> [5] </ref> which parallelizes irregular applications at run-time. With a carefully designed communication protocol that utilizes direct memory access, RAPID delivers good performance for sparse Cholesky and LU with pivoting [5, 6]. <p> The proposed techniques are implemented in the RAPID software tool [5] which parallelizes irregular applications at run-time. With a carefully designed communication protocol that utilizes direct memory access, RAPID delivers good performance for sparse Cholesky and LU with pivoting <ref> [5, 6] </ref>. <p> Their space model is different from ours and assumes a globally shared memory pool. The Cilk [2] run-time system addresses the space efficiency issue and its space complexity is O (S 1 ) per processor. The RAPID <ref> [5] </ref> uses at most S 1 space per processor. This paper assumes that each processor has a maximum space limit and the goal is to make the data space cost to be close to S 1 =p per processor in order to solve large-scale problems. <p> A transformed dependence graph contains true dependencies only. An extension to the classical task graph model is that commuting tasks can be marked in a task graph so that it can capture parallelism arising from commutative operations. The details on this parallelism model are in <ref> [5, 7] </ref> and this paper deals with scheduling and execution of a transformed task graph with an acyclic structure (DAG). The proposed memory optimizing techniques are intended for executing general task parallelism. <p> The details on this parallelism model are in [5, 7] and this paper deals with scheduling and execution of a transformed task graph with an acyclic structure (DAG). The proposed memory optimizing techniques are intended for executing general task parallelism. The experiments are conducted in the context of RAPID <ref> [5] </ref> which is a run-time system that uses an inspector/executor approach [15] to par-allelize irregular computations by embodying graph scheduling techniques to optimize interleaved communication and computation with mixed granularities. Its API includes a set of library functions for specifying irregular data objects and tasks that access these objects. <p> RMA is available in many modern parallel architectures and workstation clusters [10], but it requires that remote data addresses be known in advance. The second technique for exploiting asynchronous irregular parallelism prevents us from discovering any regularity of space usage. In the original RAPID implementation <ref> [5] </ref>, each processor allocates its volatile space at once and notifies object addresses to collaborating processors. As a result, the volatile space is quite large. For example, in our previous sparse Cholesky factorization experiments [5], the size of volatile object space could be 5 times as high as the size of <p> In the original RAPID implementation <ref> [5] </ref>, each processor allocates its volatile space at once and notifies object addresses to collaborating processors. As a result, the volatile space is quite large. For example, in our previous sparse Cholesky factorization experiments [5], the size of volatile object space could be 5 times as high as the size of permanent object space. <p> If the available amount of memory is 8 for each processor, then there are 2 units of memory for volatile objects on P 1 . In addition to the MAPs at the beginning of two task chains, there is another MAP right after task T <ref> [5; 10] </ref> on P 1 at which space for d 3 and d 5 will be freed and space for d 7 is allocated. The address for d 7 on P 1 is then notified to P 0 . <p> We analyze this issue and prove that this will never happen by using the property of a transformed graph called dependence-completeness <ref> [5] </ref>. Theorem 1 The execution with the active memory management is deadlock free and has no data inconsistency. Proof: First we observe the following fact. <p> Minimizing memory requirements can reduce the number of MAPs, which would reduce the execution overhead. However it requires reordering tasks in an execution. In the next section we will discuss the trade-off between memory and time-efficient scheduling optimizations. 4 Space and time efficient scheduling In <ref> [5] </ref> a time-efficient scheduling algorithm which contains a two-stage mapping process has been used. At the first stage tasks are clustered to exploit data locality using DSC [21] or the owner-compute rule, i.e., all the tasks that modify the same object are assigned to the same cluster. <p> Therefore the corollary is proven. Corollary 2 For the 1-D column-block based sparse LU task graphs [6], a DTS schedule is executable under S 1 =p + w space per processor at run-time. For the 2-D block based sparse Cholesky graphs <ref> [5] </ref>, a DTS schedule is executable under S 1 =p + w space per processor. Here w is the size of the largest column block of the partitioned input matrix and normally w &lt;< S 1 =p. <p> Each data object is a column block of size at most w. According to Corollary 1, each processor will at most need w volatile space to execute a DTS schedule for sparse LU. In the 2-D block based sparse Cholesky approach <ref> [5] </ref>, task graphs can be structured layer by layer. Each layer represents the elimination process on the submatrix starting from column block k. Let N be the number of blocks in both dimensions of the input matrix.
Reference: [6] <author> C. Fu and T. Yang. </author> <title> Sparse LU Factorization with Partial Pivoting on Distributed Memory Machines. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing'96, </booktitle> <address> Pitts-burgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The proposed techniques are implemented in the RAPID software tool [5] which parallelizes irregular applications at run-time. With a carefully designed communication protocol that utilizes direct memory access, RAPID delivers good performance for sparse Cholesky and LU with pivoting <ref> [5, 6] </ref>. <p> The task communication protocol of RAPID is optimized to have low overhead and as a result, the RAPID is able to deliver good performance for sparse code such as Cholesky factorization and triangular solvers. It also produces reasonable performance for sparse Gaussian Elimination with partial pivoting <ref> [6] </ref> which is an open parallelization problem in the literature. We have also used this system in parallelizing Newton's method to solve nonlinear systems. We define some terms used in the task parallelism model as follows. <p> Therefore each slice is associated with only one data object which implies that the h defined in Theorem 2 is 1. Therefore the corollary is proven. Corollary 2 For the 1-D column-block based sparse LU task graphs <ref> [6] </ref>, a DTS schedule is executable under S 1 =p + w space per processor at run-time. For the 2-D block based sparse Cholesky graphs [5], a DTS schedule is executable under S 1 =p + w space per processor. <p> increase #MAPs PT increase P=2 0% 1 1 1 1 1 1 P=8 1% 2.00 11.1% 5.63 37.5% 1 1 P=32 2.1% 1.72 13.8% 2.38 15.6% 3.06 16.7% Table 3: Effectiveness of the run-time execution scheme for sparse LU on Cray-T3D. eliminate communication in partial pivoting and row swapping operations <ref> [6] </ref>. Each node of T3D has 64 MB memory and can reach 103 MFLOPS with the BLAS-3 DGEMM routine. The RMA primitive SHMEM PUT can achieve 2.7s overhead with 128 MB/s bandwidth. <p> The number of nonzero elements (after fill-in, the same below) involved is 3.88 million. The MFLOPS calculation does not include extra floating point operations introduced by the over-estimation of the static factorization scheme <ref> [6] </ref>. The new system can now solve BCSSTK33 if we take data from column/row 1 up to 6080 with 9.49 millions of non-zeros (i.e., the problem size is increased by 145%). The absolute performance is listed in Table 8.
Reference: [7] <author> C. Fu and T. Yang. </author> <title> Run-time Techniques for Exploiting Irregular Task Parallelism on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1997. Accepted for publication. Also as UCSB technical report TRCS97-03. </note>
Reference-contexts: A transformed dependence graph contains true dependencies only. An extension to the classical task graph model is that commuting tasks can be marked in a task graph so that it can capture parallelism arising from commutative operations. The details on this parallelism model are in <ref> [5, 7] </ref> and this paper deals with scheduling and execution of a transformed task graph with an acyclic structure (DAG). The proposed memory optimizing techniques are intended for executing general task parallelism. <p> If P x is in the MAP state, since it will proceed to REC state eventually, the proof is similar to the above. As for the data consistency, the proof is similar to the one in <ref> [7] </ref> given the task graph is dependence complete. Apparently the main overhead for execution under memory constraint in this scheme is caused by the insertion of MAPs. Minimizing memory requirements can reduce the number of MAPs, which would reduce the execution overhead. However it requires reordering tasks in an execution. <p> For example, Figure 2 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 2 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; <p> The ordering difference between Figure 2 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> For MPO, T [3; 10] has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T <ref> [7; 8] </ref>'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6.
Reference: [8] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In D. </title> <editor> Hsu, A. Rosenberg, and D. Sotteau, editors, </editor> <booktitle> Interconnections Networks and Mappings and Scheduling Parallel Computation, </booktitle> <pages> pages 139-172. </pages> <publisher> American Math. Society, </publisher> <year> 1995. </year>
Reference-contexts: RAPID is targeted at irregular applications which involve iterative computation and have invariant or slowly changed dependence structures, such as those in sparse matrix computation and N-body galaxy simulations <ref> [8, 17] </ref>. The task communication protocol of RAPID is optimized to have low overhead and as a result, the RAPID is able to deliver good performance for sparse code such as Cholesky factorization and triangular solvers. <p> For example, Figure 2 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 2 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; <p> The ordering difference between Figure 2 (b) and (c) is that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> that on processor 1, T <ref> [7; 8] </ref> is executed at time 6 by RCP while T [3; 10] is chosen instead at time 6 by MPO. The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T <ref> [8] </ref>; T [8; 9] with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T <ref> [8; 9] </ref> with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> For MPO, T [3; 10] has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T <ref> [7; 8] </ref>'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6.
Reference: [9] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic Extraction of Functinal Parallelism from Ordinary Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <year> 1992. </year>
Reference-contexts: The reason is that for RCP, T [7; 8] has a longer path from this task to an exit task (the path is T [7; 8]; T [8]; T <ref> [8; 9] </ref> with length 4 because communication delay is also included) than other unscheduled tasks on P 1 . <p> Another possible application of the DTS algorithm is to guide data placement in memory for shared data objects to improve both spatial and temporal locality. One future work is to extend our results for more complicated task dependence structures <ref> [3, 9, 13] </ref>. The experiments have also revealed that other space limiting factors, such as the storage of dependence information in our current implementation, affect the ability to process larger problem instances.
Reference: [10] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> Implementing Active Messages and Split-C for SCI Clusters and Some Architectural Implications. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Our work uses hardware support for directly accessing remote memory, which is available in several modern parallel architectures and workstation clusters <ref> [10, 18] </ref>. Benefits of the direct remote memory access mechanism are also identified in the fast communication research such as active messages. Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers. <p> The first technique requires hardware support to directly deposit data from user space on one processor to another without buffering and hand-shaking overhead. RMA is available in many modern parallel architectures and workstation clusters <ref> [10] </ref>, but it requires that remote data addresses be known in advance. The second technique for exploiting asynchronous irregular parallelism prevents us from discovering any regularity of space usage. In the original RAPID implementation [5], each processor allocates its volatile space at once and notifies object addresses to collaborating processors. <p> If the available amount of memory is 8 for each processor, then there are 2 units of memory for volatile objects on P 1 . In addition to the MAPs at the beginning of two task chains, there is another MAP right after task T <ref> [5; 10] </ref> on P 1 at which space for d 3 and d 5 will be freed and space for d 7 is allocated. The address for d 7 on P 1 is then notified to P 0 . <p> For example, Figure 2 (b) is a schedule produced by RCP while (c) is a schedule produced by MPO. The ordering difference between Figure 2 (b) and (c) is that on processor 1, T [7; 8] is executed at time 6 by RCP while T <ref> [3; 10] </ref> is chosen instead at time 6 by MPO. <p> For MPO, T <ref> [3; 10] </ref> has a higher memory priority 1 because data d 3 and d 10 are all available locally at time 6, and T [7; 8]'s memory priority is 0.5 because the space for d 7 has not been allocated before time 6.
Reference: [11] <author> X. Li. </author> <title> Sparse Gaussian Elimination on High Performance Computers. </title> <type> PhD thesis, CS, </type> <institution> UC Berkeley, </institution> <year> 1996. </year>
Reference-contexts: A 2-D block data mapping is used, which can expose more parallelism and give better scalability [14]. 2) Sparse LU (Gaussian Elimination) with partial pivoting. This problem has a unpredictable dynamic dependence structure. Its paralleliza-tions on shared memory platforms are addressed in <ref> [11] </ref>. But its efficient parallelization on distributed memory machines still remains an open problem in the scientific computing literature.
Reference: [12] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Each task reads/writes a subset of data objects. Data dependence graphs (DDG) derived from partitioned code for modeling the interaction among tasks normally have three types of dependencies: true, anti and output <ref> [12] </ref>. In a DDG, some of anti or output dependence edges could be redundant if they are subsumed by other true data dependence edges. Other anti/output dependence edges can be eliminated by program transformation. A transformed dependence graph contains true dependencies only.
Reference: [13] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A Convex Programming Approach for Exploiting Data and Functional Parallelism. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 116-125, </pages> <year> 1994. </year>
Reference-contexts: Another possible application of the DTS algorithm is to guide data placement in memory for shared data objects to improve both spatial and temporal locality. One future work is to extend our results for more complicated task dependence structures <ref> [3, 9, 13] </ref>. The experiments have also revealed that other space limiting factors, such as the storage of dependence information in our current implementation, affect the ability to process larger problem instances.
Reference: [14] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The task graph has a static dependence structure if the nonzero pattern of a sparse matrix is given at the run-time preprocessing stage. A 2-D block data mapping is used, which can expose more parallelism and give better scalability <ref> [14] </ref>. 2) Sparse LU (Gaussian Elimination) with partial pivoting. This problem has a unpredictable dynamic dependence structure. Its paralleliza-tions on shared memory platforms are addressed in [11]. But its efficient parallelization on distributed memory machines still remains an open problem in the scientific computing literature.
Reference: [15] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berry-man. </author> <title> Run-Time Scheduling and Execution of Loops on Message Passing Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: The proposed memory optimizing techniques are intended for executing general task parallelism. The experiments are conducted in the context of RAPID [5] which is a run-time system that uses an inspector/executor approach <ref> [15] </ref> to par-allelize irregular computations by embodying graph scheduling techniques to optimize interleaved communication and computation with mixed granularities. Its API includes a set of library functions for specifying irregular data objects and tasks that access these objects.
Reference: [16] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Benefits of the direct remote memory access mechanism are also identified in the fast communication research such as active messages. Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers. Most of previous research on scheduling <ref> [16, 19, 20] </ref> does not address memory issues.
Reference: [17] <author> R. Schreiber. </author> <title> Scalability of Sparse Direct Solvers, volume 56 of Graph Theory and Sparse Matrix Computation (Edited by Alan George and John R. </title> <editor> Gilbert and Joseph W.H. </editor> <booktitle> Liu), </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: The applications that have been used to demonstrate the effectiveness of this parallelism model are sparse matrix problems <ref> [5, 17] </ref>. The main difficulties for parallelizing sparse code are that the granularity of computation and communication is non-uniform, dependence structures may change dynamically and code performance is sensitive to the software overhead introduced by system support. <p> RAPID is targeted at irregular applications which involve iterative computation and have invariant or slowly changed dependence structures, such as those in sparse matrix computation and N-body galaxy simulations <ref> [8, 17] </ref>. The task communication protocol of RAPID is optimized to have low overhead and as a result, the RAPID is able to deliver good performance for sparse code such as Cholesky factorization and triangular solvers.
Reference: [18] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling Synchronization and Data Transfer in Message Passing Systems of Parallel Computers. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 1-10, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Our work uses hardware support for directly accessing remote memory, which is available in several modern parallel architectures and workstation clusters <ref> [10, 18] </ref>. Benefits of the direct remote memory access mechanism are also identified in the fast communication research such as active messages. Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers.
Reference: [19] <author> R. Wolski and J. Feo. </author> <title> Program Parititoning for NUMA Multiprocessor Computer Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> 1993. </year>
Reference-contexts: Benefits of the direct remote memory access mechanism are also identified in the fast communication research such as active messages. Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers. Most of previous research on scheduling <ref> [16, 19, 20] </ref> does not address memory issues.
Reference: [20] <author> T. Yang and A. Gerasoulis. </author> <title> List Scheduling with and without Communication Delays. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1321-1344, </pages> <year> 1992. </year>
Reference-contexts: Benefits of the direct remote memory access mechanism are also identified in the fast communication research such as active messages. Thus we expect other software system researchers can also benefit from our results in using fast communication support to design software layers. Most of previous research on scheduling <ref> [16, 19, 20] </ref> does not address memory issues. <p> For simplicity of the description, we assume that each task modifies only one object in this section. The second stage is to order tasks on each processor to overlap communication with computation so that maximum inter-processor parallelism is explored. This ordering algorithm is called RCP <ref> [20] </ref>. A RCP schedule is time efficient, but may not be space-efficient because it executes tasks in the order of importance based on the critical path information, which may require more memory to hold volatile objects.
Reference: [21] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling Parallel Tasks on An Unbounded Number of Processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(9) </volume> <pages> 951-967, </pages> <year> 1994. </year> <note> A short version is in Proceedings of Supercomputing'91. </note>
Reference-contexts: In the next section we will discuss the trade-off between memory and time-efficient scheduling optimizations. 4 Space and time efficient scheduling In [5] a time-efficient scheduling algorithm which contains a two-stage mapping process has been used. At the first stage tasks are clustered to exploit data locality using DSC <ref> [21] </ref> or the owner-compute rule, i.e., all the tasks that modify the same object are assigned to the same cluster. Clusters are then mapped to physical processors using a load balancing criterion. For simplicity of the description, we assume that each task modifies only one object in this section.
References-found: 21

