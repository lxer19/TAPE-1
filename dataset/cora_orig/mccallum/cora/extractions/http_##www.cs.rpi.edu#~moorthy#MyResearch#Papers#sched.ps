URL: http://www.cs.rpi.edu/~moorthy/MyResearch/Papers/sched.ps
Refering-URL: http://www.cs.rpi.edu/~moorthy/MyResearch/compgt.html
Root-URL: http://www.cs.rpi.edu
Email: gilderm@cs.rpi.edu  moorthy@cs.rpi.edu  
Phone: FAX: (518) 276-4033  
Title: Automatic Source-Code Parallelization Using HICOR Objects  
Author: Mark R. Gilder Mukkai S. Krishnamoorthy 
Address: Troy, New York 12180  
Affiliation: Rensselaer Polytechnic Institute Department of Computer Science  
Pubnum: (518) 276-4857  (518) 276-6911  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho and M. Ganapathi. </author> <title> Efficient Tree Pattern Matching: An Aid to Code Generation. </title> <booktitle> In Twelfth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 334-340, </pages> <address> New York, NY, </address> <month> January </month> <year> 1985. </year>
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <month> March </month> <year> 1986. </year>
Reference: [3] <author> W. Baxter and H. R. Bauer, III. </author> <title> The Program Dependence Graph and Vectorization. </title> <booktitle> In Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 1-11, </pages> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [4] <author> M. Burke, R. Cytron, J. Ferrante, and Wilson Hsieh. </author> <title> Automatic Generation of Nested, Fork-Join Parallelism. </title> <journal> Journal of Supercomputing, </journal> <volume> 2(3) </volume> <pages> 71-88, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [5] <author> B. W. Char, K. O. Geddes, G. H. Gonnet, M. B. Monagagn, and S. M. Watt. </author> <title> Maple Reference Manual, </title> <year> 1988. </year>
Reference-contexts: The object oriented approach employed clearly has its advantages over traditional approaches and has helped us greatly in the design and implementation of our system. This, in turn, has facilitated our ability to interface with other graph and algorithmic packages such as GraphPack [19] for visualization purposes and Maple <ref> [5] </ref> which generates C code. The success of our system is attributed to the construction of objects and associated methods to encapsulate the underlying intermediate representation.
Reference: [6] <author> K. D. Cooper and K. Kennedy. </author> <title> Fast Interprocedural Alias Analysis. </title> <booktitle> In Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59, </pages> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [7] <author> R. Duncan. </author> <title> A Survey of Parallel Computer Architectures. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 5-16, </pages> <month> February </month> <year> 1990. </year>
Reference: [8] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The Program Dependence Graph and Its Use in Optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: In the PDG representation nodes are statements and predicate expressions. Edges incident to a node represent both data dependence and control dependence. The PDG allows different program transformations. Details of this representation and algorithms based on this form are presented in <ref> [8] </ref>. In HTG, there are simple nodes (representing single tasks), compound nodes (nodes consisting of other tasks in HTG) and a loop node representing a task that is a loop whose iteration body is an HTG. Details of this representation and algorithms are presented in [14].
Reference: [9] <editor> D. Gelernter, A. Nicolau, and D. Padua. </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Also, the loop which initializes a, i.e., object 11, can be executed in parallel with the assignment statement represented by object 12. We should mention that the system also determines loop-level dependencies using techniques described in <ref> [9, 13, 23] </ref>. <p> The driver routine presented in Figure 29 remains unchanged. Other methods may be used to extract ranges of independent loop iterations 39 adn to check for dependencies between linear array index equations. Other methods have been implemented to perform much of the loop-level parallelization techniques as described in <ref> [9, 13, 23] </ref>. In order to demonstrate the parallelization capabilities of the system several sample programs have been written and tested. The programs evaluated were written in both ANSI C and Fortran and contain no compiler directives.
Reference: [10] <author> M. R. Gilder. </author> <title> An Object Oriented Intermediate Code Representation For An Architecturally Independent Parallelizing Compiler. </title> <type> PhD thesis, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <year> 1993. </year>
Reference-contexts: Section 4 describes the implementation of the Compile-Time Scheduler; Section 5 illustrates the Target Code Generation scheme employed in mapping the output of the Compile-Time Scheduler onto a particular architecture, and provides an evaluation of the overall system. 2 System Overview <ref> [10, 12] </ref>. In particular, source languages are mapped into an architecturally independent intermediate language called CICL (Common Intermediate Code Language) through the use of specific front-end Language Translators. Once performed, the equivalent HICOR instance is generated. The HICOR instance may be viewed as the memory-resident instantiation of the CICL instance. <p> A detailed description of all HICOR objects and available methods may be found in <ref> [10, 12] </ref>. From these available methods dependency information for any portion of the HICOR representation may be rendered. For example, Figure 4 illustrates the dependency graph generated by invoking the GenerateDependencies method on object 94 of the MinMax function. <p> Tools constructed in this manner are inherently simpler since they can manipulate the representation at a much higher level of abstraction. Other tools constructed using this approach are described in <ref> [10, 11] </ref>. The object oriented approach employed clearly has its advantages over traditional approaches and has helped us greatly in the design and implementation of our system.
Reference: [11] <author> M. R. Gilder and M. S. Krishnamoorthy. </author> <title> A Viable Object Oriented Intermediate Code Representation for Parallel Architectures. </title> <type> Technical Report TR 92-35, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Tools constructed in this manner are inherently simpler since they can manipulate the representation at a much higher level of abstraction. Other tools constructed using this approach are described in <ref> [10, 11] </ref>. The object oriented approach employed clearly has its advantages over traditional approaches and has helped us greatly in the design and implementation of our system.
Reference: [12] <author> M. R. Gilder and M. S. Krishnamoorthy. </author> <title> An Object-Oriented Intermediate Code Representation for the Development of Parallelization Tools. </title> <note> Journal of Object-Oriented Programming (To Appear), </note> <year> 1994. </year>
Reference-contexts: In particular, the intermediate code generated by the compiler front-end is explicitly represented by objects instantiated from a specialized class hierarchy called HICOR 2 2 SYSTEM OVERVIEW (Hierarchical Intermediate Code Object Representation) <ref> [12] </ref>. Each object supports a formal interface mechanism through which internal object information is made available. This added level of abstraction allows flexible parallelization tools to be developed without regard to the underlying structure of the internal representation. <p> Section 4 describes the implementation of the Compile-Time Scheduler; Section 5 illustrates the Target Code Generation scheme employed in mapping the output of the Compile-Time Scheduler onto a particular architecture, and provides an evaluation of the overall system. 2 System Overview <ref> [10, 12] </ref>. In particular, source languages are mapped into an architecturally independent intermediate language called CICL (Common Intermediate Code Language) through the use of specific front-end Language Translators. Once performed, the equivalent HICOR instance is generated. The HICOR instance may be viewed as the memory-resident instantiation of the CICL instance. <p> A detailed description of all HICOR objects and available methods may be found in <ref> [10, 12] </ref>. From these available methods dependency information for any portion of the HICOR representation may be rendered. For example, Figure 4 illustrates the dependency graph generated by invoking the GenerateDependencies method on object 94 of the MinMax function.
Reference: [13] <author> M. Girkar and C. Polychronopoulos. </author> <title> Compiling Issues for Supercomputers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 164-173, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of. <p> Also, the loop which initializes a, i.e., object 11, can be executed in parallel with the assignment statement represented by object 12. We should mention that the system also determines loop-level dependencies using techniques described in <ref> [9, 13, 23] </ref>. <p> The driver routine presented in Figure 29 remains unchanged. Other methods may be used to extract ranges of independent loop iterations 39 adn to check for dependencies between linear array index equations. Other methods have been implemented to perform much of the loop-level parallelization techniques as described in <ref> [9, 13, 23] </ref>. In order to demonstrate the parallelization capabilities of the system several sample programs have been written and tested. The programs evaluated were written in both ANSI C and Fortran and contain no compiler directives.
Reference: [14] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic Extraction of Functional Level Parallelism from Ordinary Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <month> March </month> <year> 1993. </year> <note> 46 REFERENCES </note>
Reference-contexts: In HTG, there are simple nodes (representing single tasks), compound nodes (nodes consisting of other tasks in HTG) and a loop node representing a task that is a loop whose iteration body is an HTG. Details of this representation and algorithms are presented in <ref> [14] </ref>. The HTG representation has more structure than the PDG. In our hierarchical representation, the abstraction for individual statements, blocks of statements, conditional statements, procedural statements, and iterative constructs arise naturally because of the object-oriented specification of the intermediate code.
Reference: [15] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical Dependence Testing. </title> <booktitle> In Proc. of the SIGPLAN '91 Conf. on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference: [16] <author> M. Harrold and M. Soffa. </author> <title> Computation of Interprocedural Definition and Use Dependencies. </title> <booktitle> In Proc. 1990 IEEE Int. Conf. on Computing Languages, </booktitle> <pages> pages 297-306, </pages> <address> New Orleans, </address> <month> March </month> <year> 1990. </year>
Reference: [17] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural Slicing Using Dependence Graphs. </title> <booktitle> In Proceedings of the ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [18] <author> K. Kennedy, K. S. Mckinley, and C. W. Tseng. </author> <title> Interactive Parallel Programming Using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Also, since the granularity required to achieve a speed-up varies with respect to a particular architecture, tools constructed using our representation need not be altered when applied to different architectures due to the formal interface mechanisms provided. Current research efforts, such as ParaScope at Rice University <ref> [18] </ref>, PTRAN at IBM T. J. Wat-son Research Center [31], and the Typhoon parallelizing compiler project at Cornell University [24], address this issue but are with respect to a single programming language and are not explicitly based on object-oriented principles.
Reference: [19] <author> M. S. Krishnamoorthy, A. Suess, M. Onghena, and F. Oxall. </author> <title> Improvements to Graph-Pack: A System to Manipulate Graphs and Digraphs. In DIMACS Workshop on Computational Support for Discrete Mathematics, </title> <note> 1993 (To appear). </note>
Reference-contexts: The object oriented approach employed clearly has its advantages over traditional approaches and has helped us greatly in the design and implementation of our system. This, in turn, has facilitated our ability to interface with other graph and algorithmic packages such as GraphPack <ref> [19] </ref> for visualization purposes and Maple [5] which generates C code. The success of our system is attributed to the construction of objects and associated methods to encapsulate the underlying intermediate representation.
Reference: [20] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence Graphs and Compiler Optimizations. </title> <booktitle> In Eighth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference: [21] <author> C. McCreary and H. Gill. </author> <title> Automatic Determination of Grain Size for Efficient Parallel Processing. </title> <journal> Communications of the ACM, </journal> <volume> 32(9) </volume> <pages> 1073-1078, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [22] <author> R. E. Miller and J. W. Thatcher. </author> <title> Complexity of Computer Computations. </title> <publisher> Plenum Press, </publisher> <address> New York, NY, </address> <year> 1972. </year>
Reference-contexts: Before formally defining the ACSP problem, we first present the General Scheduling Problem (GSP) which has been proven to be NP-complete by Karp <ref> [22] </ref>. This result also appears in [34]. We later use this result to prove that ACSP is NP-complete.
Reference: [23] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Also, the loop which initializes a, i.e., object 11, can be executed in parallel with the assignment statement represented by object 12. We should mention that the system also determines loop-level dependencies using techniques described in <ref> [9, 13, 23] </ref>. <p> The driver routine presented in Figure 29 remains unchanged. Other methods may be used to extract ranges of independent loop iterations 39 adn to check for dependencies between linear array index equations. Other methods have been implemented to perform much of the loop-level parallelization techniques as described in <ref> [9, 13, 23] </ref>. In order to demonstrate the parallelization capabilities of the system several sample programs have been written and tested. The programs evaluated were written in both ANSI C and Fortran and contain no compiler directives.
Reference: [24] <author> K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill. </author> <title> Dependence Flow Graphs: An Algebraic Approach to Program Dependencies. </title> <type> Technical Report TR 90-1152, </type> <institution> Cornell University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Current research efforts, such as ParaScope at Rice University [18], PTRAN at IBM T. J. Wat-son Research Center [31], and the Typhoon parallelizing compiler project at Cornell University <ref> [24] </ref>, address this issue but are with respect to a single programming language and are not explicitly based on object-oriented principles. In this paper, we present another approach to accomplishing this for multiple source languages and multiple target architectures.
Reference: [25] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference: [26] <author> J. R. Punin, M. R. Gilder, and M. S. Krishnamoorthy. </author> <title> A GUI for Parallel Code Generation. </title> <booktitle> In progress. </booktitle>
Reference-contexts: The system is open-ended and interfaces nicely to other packages which either produce C, Pascal, or Fortran as output; or accept C as input. Furthermore, a GUI has been developed for the system which provides the user with a visual representation of 3 the parallelizations performed <ref> [26] </ref>.
Reference: [27] <author> S. Richardson and M. Ganapathi. </author> <title> Code Optimization Across Procedures. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 42-50, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [28] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The notation used throughout the remainder of this Section is may be found in Appendix A. Before presenting the algorithm we make the following observations (similar observations are made in <ref> [28] </ref>): Proposition 4.3 Given a DAG D produced by the Hierarchical Dependency Analyzer the optimal execution time for any schedule, ! opt , is bounded by ! opt maxf!; T 1 =kg where ! is the cost of the critical path of D, T 1 is the sequential execution time of
Reference: [29] <author> V. Sarkar. </author> <title> Automatic partitioning of a program dependence graph into parallel tasks. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(5) </volume> <pages> 779-804, </pages> <month> September </month> <year> 1991. </year> <note> REFERENCES 47 </note>
Reference-contexts: This is especially true of C-source code. This problem has been addressed in earlier work. For a comprehensive description we refer to page 799 in <ref> [29] </ref>. Our work differs significantly in that our model uses a less restrictive cost function and a much more general program model (since the actual objects to be scheduled may be user defined). Also, we differ from Sarkar's work [29] by providing explicit support for multiple source languages and multiple target <p> For a comprehensive description we refer to page 799 in <ref> [29] </ref>. Our work differs significantly in that our model uses a less restrictive cost function and a much more general program model (since the actual objects to be scheduled may be user defined). Also, we differ from Sarkar's work [29] by providing explicit support for multiple source languages and multiple target architectures. The objects manipulated by the scheduling algorithm are explicit instances of the source code represented with associated algorithms (methods) for data extraction and manipulation.
Reference: [30] <author> R. W. Stevens. </author> <title> UNIX Network Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: ); T 3 .GenCode (); g with edges translated to shared memory and semaphore control statements 38 5 TARGET CODE GENERATION AND EVALUATION Our design incorporates the use of a driver routine which creates a separate Unix process for each function, P i , via the fork () system call <ref> [30] </ref>. As each process executes, the control primitives, signal and wait, are used to control access to shared memory. Hence, the process executing P 3 , can not execute the code for task T 3 until both executing processes for P 1 and P 2 have sent their respective signals.
Reference: [31] <author> B. K. Szymanski. </author> <title> Parallel Functional Languages and Compilers. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: Current research efforts, such as ParaScope at Rice University [18], PTRAN at IBM T. J. Wat-son Research Center <ref> [31] </ref>, and the Typhoon parallelizing compiler project at Cornell University [24], address this issue but are with respect to a single programming language and are not explicitly based on object-oriented principles. In this paper, we present another approach to accomplishing this for multiple source languages and multiple target architectures.
Reference: [32] <author> Y. Tanaka, K. Iwasawa, S. Gotoo, and Yukio Umetani. </author> <title> Compiling Techniques for First-Order Linear Recurrences on a Vector Computer. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 174-181, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In particular, useful information that can be quickly attained from high-level source-code constructs are often lost during the translation to an intermediate representation thereby complicating the design of parallelization tools. More importantly, current compilers do not easily allow the incorporation of newly proposed tools <ref> [3, 4, 6, 13, 17, 21, 27, 32] </ref> for lack of a formal interface in terms of the intermediate representation. Hence, the development of general-purpose parallelization tools becomes an expensive proposition. There are at least two other forms of intermediate representations that we are aware of.
Reference: [33] <author> A. E. Terrano, S. M. Dunn, and J. E. Peters. </author> <title> Using an Architectural Knowledge Base to Generate Code for Parallel Computers. </title> <journal> Communications of the ACM, </journal> <volume> 32(9) </volume> <pages> 1065-1072, </pages> <month> September </month> <year> 1989. </year>
Reference: [34] <author> J. D. Ullman. </author> <title> NP-Complete Scheduling Problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 10 </volume> <pages> 384-393, </pages> <year> 1975. </year> <title> 48 A NOTATION </title>
Reference-contexts: Before formally defining the ACSP problem, we first present the General Scheduling Problem (GSP) which has been proven to be NP-complete by Karp [22]. This result also appears in <ref> [34] </ref>. We later use this result to prove that ACSP is NP-complete.
References-found: 34

