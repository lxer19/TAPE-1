URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/92.TR418.sharing.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/ricardo/pubs.html
Root-URL: 
Title: Dynamic Sharing and Backward Compatibility on 64-Bit Machines  
Author: William E. Garrett, Ricardo Bianchini, Leonidas Kontothanassis, R. Andrew McCallum, Jeffery Thomas, Robert Wisniewski, and Michael L. Scott -garrett, ricardo, kthanasi, mccallum, thomas, bob, 
Note: This work was supported in part by NSF grants CCR-9005633 and CDA-8822724, and by Brazilian CAPES and NUTES/UFRJ fellowships. Authors' email addresses:  
Address: Rochester, NY 14627-0226  
Affiliation: University of Rochester Computer Science Department  
Pubnum: TR 418  
Email: scott-@cs.rochester.edu.  
Date: April 1992  
Abstract: As an alternative to communication via messages or files, shared memory has the potential to be simpler, faster, and less wasteful of space. Unfortunately, the mechanisms available for sharing in most multi-user operating systems are difficult to use. As a result, shared memory tends to appear primarily in self-contained parallel applications, where library or compiler support can take care of the messy details. We see a tremendous opportunity to extend the advantages of sharing across application boundaries. We believe that these advantages can be realized without introducing major changes to the Unix programming model. In particular, we believe that it is both possible and desirable to incorporate shared memory segments into the hierarchical file system name space. Our approach has two components: First, we use dynamic linking to allow programs to access shared data and code in the same way they access ordinary (private) variables and functions. Second, we unify memory and files into a single-level store that facilitates the sharing of pointers. This second component is made feasible by the 64-bit addresses of emerging microprocessors. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. T. Almes, A. P. Black, E. D. Lazowska and J. D. Noe, </author> <title> ``The Eden System: A Technical Review,'' </title> <journal> IEEE Transactions on Software Engineering SE-11:1 (January 1985), </journal> <pages> pp. 43-59. </pages>
Reference-contexts: This latter decision provides us with a rich set of ready-made tools for perusal and management of long-lived segments. 3.6.3. Software Implementation of Large Address Spaces Many distributed systems provide large name spaces with software interpretation. Some provide names for heavyweight objects <ref> [1, 16] </ref>; others for communication ports [14, 42]; still others for mappable segments [33]. In any case, objects in the distributed name space cannot be accessed via hardware addressing modes (except perhaps with a temporary mapping), and must generally be treated differently from addressable local objects.
Reference: [2] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska and H. M. Levy, </author> <title> ``Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism,'' </title> <journal> ACM Transactions on Computer Systems 10:1 (February 1992), </journal> <pages> pp. 53-79. </pages> <booktitle> Originally presented at the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 13-16 October </month> <year> 1991. </year>
Reference-contexts: For lighter-weight synchronization, blocking mechanisms can be implemented in user space by providing standard interfaces to thread schedulers [37], and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections <ref> [2, 18, 37] </ref>, or by relinquishing the processor when a lock is unavailable [8, 29]. 3.5.2. Garbage Collection When a Unix process finishes execution or terminates abnormally, its private segments can be reclaimed. The same cannot be said of segments shared between processes.
Reference: [3] <author> B. N. Bershad, D. T. Ching, E. D. Lazowska, J. Sanislo and M. Schwartz, </author> <title> ``A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:8 (August 1987), </journal> <pages> pp. 880-894. </pages>
Reference-contexts: The code required to save and restore information in files and message buffers is a major contributor to software complexity, and much research has been aimed at reducing this burden (e.g. through data description languages [32] and RPC stub generators <ref> [3, 26] </ref>). (4) When supported by hardware, shared memory is generally faster than either messages or files, since operating system overhead and copying costs can often be avoided.
Reference: [4] <author> B. N. Bershad and C. B. Pinkerton, </author> <title> ``Watchdogs Extending the UNIX File System,'' </title> <journal> Computing Systems, </journal> <note> Spring 1988. Also Technical Report 87-12-06, </note> <institution> Department of Computer Science, University of Washington, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Generalization of the Unix File System Interface Our assignment of names to shared segments is one in a long series of new uses for the file system naming hierarchy. Pseudo-terminals and Unix domain sockets are now commonplace, but were missing in early versions of Unix. Bershad and Pinkerton <ref> [4] </ref> describe a general-purpose mechanism for installing a user-provided process that intercepts operations on a specified file. Gifford et al. [22] describe a similar mechanism that allows the user to simulate files that are created on demand.
Reference: [5] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy and D. B. Wagner, </author> <title> ``An Open Environment for Building Parallel Programming Systems,'' </title> <booktitle> Proceedings of the First ACM Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <month> 19-21 July </month> <year> 1988, </year> <pages> pp. 1-9. </pages> <booktitle> In ACM SIGPLAN Notices 23:9. </booktitle>
Reference-contexts: On a shared memory multiprocessor this communication occurs via shared variables. In most parallel environments global variables are considered to be shared between the the threads of an application while local variables are private to a thread. In systems like Presto <ref> [5] </ref>, however, both shared and private global variables are permitted. Presto was originally designed to run on a Sequent multiprocessor under the Dynix operating system. The Dynix compilers provide language extensions that allow the programmer to distinguish explicitly between shared and private variables.
Reference: [6] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska and H. M. Levy, </author> <title> ``Lightweight Remote Procedure Call,'' </title> <journal> ACM Transactions on Computer Systems 8:1 (February 1990), </journal> <pages> pp. 37-55. </pages> <booktitle> Originally presented at the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989. </year>
Reference-contexts: Work by Bershad and Anderson, for example <ref> [6] </ref>, indicates that message passing should be built on top of shared memory when possible. (5) As an implementation technique, sharing of read-only objects can save significant amounts of disk space and memory. <p> Use of Shared Memory to Improve Performance Several previous projects have explored the use of shared memory to improve the performance of cross-address-space process interactions. Bershad <ref> [6] </ref> uses buffers mapped into sending and receiving address spaces to speed the implementation of intra-machine remote procedure calls. The implementation of the Lynx distributed programming language for the BBN Butterfly employs a similar optimization [50]. <p> In their work on lightweight and user-level remote procedure calls, Bershad et al. argue that the speed of optimized interfaces permits a much more modular style of system construction than has been the norm to date <ref> [6, 7] </ref>. The growing interest in microkernels [67] suggests that this philosophy is catching on. In effect, the microker-nel argument is that the proliferation of boundaries becomes acceptable when crossing these boundaries is cheap.
Reference: [7] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska and H. M. Levy, </author> <title> ``User-Level Interprocess Communication for Shared Memory Multiprocessors,'' </title> <journal> ACM Transactions on Computer Systems 9:2 (May 1991), </journal> <pages> pp. 175-198. </pages>
Reference-contexts: In their work on lightweight and user-level remote procedure calls, Bershad et al. argue that the speed of optimized interfaces permits a much more modular style of system construction than has been the norm to date <ref> [6, 7] </ref>. The growing interest in microkernels [67] suggests that this philosophy is catching on. In effect, the microker-nel argument is that the proliferation of boundaries becomes acceptable when crossing these boundaries is cheap.
Reference: [8] <author> D. L. Black, </author> <title> ``Scheduling Support for Concurrency and Parallelism in the Mach Operating System,'' </title> <booktitle> Computer 23:5 (May 1990), </booktitle> <pages> pp. 35-43. </pages>
Reference-contexts: user space by providing standard interfaces to thread schedulers [37], and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections [2, 18, 37], or by relinquishing the processor when a lock is unavailable <ref> [8, 29] </ref>. 3.5.2. Garbage Collection When a Unix process finishes execution or terminates abnormally, its private segments can be reclaimed. The same cannot be said of segments shared between processes. Sharing introduces (or at least exacerbates) the problem of garbage collection.
Reference: [9] <author> P. Brinch Hansen, </author> <title> Programming a Personal Computer, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: The incredible productivity of Lisp environments such as Genera [60], and of the Pilot [48] and Cedar [57] projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift [15] testifies to their efficiency. Open systems have attracted the attention of language designers as well <ref> [9, 64] </ref>, and are an important part of the commercial market for personal computers. Unfortunately, the flexibility and efficiency of open operating systems is obtained at a serious price. Protection is available only to the extent that it is provided by high-level language compilers.
Reference: [10] <author> L. A. Call, D. L. Cohrs and B. P. Miller, </author> <title> ``CLAM an Open System for Graphical User Interfaces,'' </title> <booktitle> OOPSLA'87 Conference Proceedings, </booktitle> <month> 4-8 October </month> <year> 1987, </year> <pages> pp. 277-286. </pages> <booktitle> In ACM SIGPLAN Notices 22:12 (December 1987). </booktitle> <pages> 31 </pages>
Reference-contexts: Dynamic linking is an integral part of single-user open operating systems such as Cedar [57], and has been implemented under Unix as part of such self-contained environments as Emerald [27] and the Portable Common Runtime [61]. The CLAM user interface system <ref> [10] </ref> and SOS distributed object system [21] load C++ classes dynamically; the latter is based on the Andrew project's Camphor dynamic linker. Our work differs from these projects in its use of dynamic linking to share potentially writable objects transparently, between ordinary Unix programs. 4. Implementation Details 4.1.
Reference: [11] <author> J. B. Carter, J. K. Bennett and W. Zwaenepoel, </author> <title> ``Implementation and Performance of Munin,'' </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 14-16 October </month> <year> 1991, </year> <pages> pp. 152-164. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 25:5. </booktitle>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [12] <author> J. S. Chase, H. M. Levy, E. D. Lazowska and M. Baker-Harvey, </author> <title> ``Lightweight Shared Objects in a 64-Bit Operating System,'' </title> <type> Technical Report 92-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: At the same time, we plan to retain the ability to overload addresses within a reserved, private portion of the 64-bit space. This ability is in contrast to the strict single-translation approach of systems such as Opal <ref> [12, 13] </ref>. We discuss this issue further in section 3.2; for the moment, suffice it to note that private modules (including the main module of every process) are linked into the private, overloaded portion of the address space, but public modules are linked at their globally-understood address. <p> Because of hardware limitations, neither is able to provide machine-readable pointers that are globally meaningful. More recently, researchers at the University of Washington have designed a system called Opal that provides a single-level store on 64-bit machines [13] and supports a user-level object system <ref> [12] </ref>. By adopting a single, global virtual-to-physical mapping, Opal is able to realize all the advantages of cross-address-space sharing.
Reference: [13] <author> J. S. Chase, H. M. Levy, M. Baker-Harvey and E. D. Lazowska, </author> <title> ``How to Use a 64-Bit Virtual Address Space,'' </title> <type> Technical Report 92-03-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: At the same time, we plan to retain the ability to overload addresses within a reserved, private portion of the 64-bit space. This ability is in contrast to the strict single-translation approach of systems such as Opal <ref> [12, 13] </ref>. We discuss this issue further in section 3.2; for the moment, suffice it to note that private modules (including the main module of every process) are linked into the private, overloaded portion of the address space, but public modules are linked at their globally-understood address. <p> Some existing programs (generally not good ones) assume that they are linked at a particular address. Most existing programs are created by compilers that use absolute addressing modes to access static data, and assume that the data are private. Many create new processes via fork. Chase et al. <ref> [13] </ref> observe that the Unix fork mechanism is based in a fundamental way on the use of static, private data at fixed addresses. Their Opal system, which adopts a strict, single global translation, dispenses with fork in favor of an RPC-based mechanism for animating a newly-created protection domain. <p> A different philosophical position is taken in systems such as Multics [44], Hydra [65], and Opal <ref> [13] </ref>, which clearly separate code from data and speak explicitly of processes executing in shared code but using private (static) data. Multics employs an elaborate hardware/software mechanism in which references to static data are made indirectly through a base register and process-private link segment. <p> Hydra employs capabilities that are interpreted by the kernel. Because of hardware limitations, neither is able to provide machine-readable pointers that are globally meaningful. More recently, researchers at the University of Washington have designed a system called Opal that provides a single-level store on 64-bit machines <ref> [13] </ref> and supports a user-level object system [12]. By adopting a single, global virtual-to-physical mapping, Opal is able to realize all the advantages of cross-address-space sharing.
Reference: [14] <author> D. Cheriton, </author> <title> ``The V Kernel A Software Base for Distributed Systems,'' </title> <booktitle> IEEE Software 1:2 (April 1984), </booktitle> <pages> pp. 19-42. </pages>
Reference-contexts: This latter decision provides us with a rich set of ready-made tools for perusal and management of long-lived segments. 3.6.3. Software Implementation of Large Address Spaces Many distributed systems provide large name spaces with software interpretation. Some provide names for heavyweight objects [1, 16]; others for communication ports <ref> [14, 42] </ref>; still others for mappable segments [33]. In any case, objects in the distributed name space cannot be accessed via hardware addressing modes (except perhaps with a temporary mapping), and must generally be treated differently from addressable local objects.
Reference: [15] <author> D. Clark, </author> <title> ``The Structuring of Systems Using Upcalls,'' </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985, </year> <pages> pp. 171-180. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 19:5. </booktitle>
Reference-contexts: The incredible productivity of Lisp environments such as Genera [60], and of the Pilot [48] and Cedar [57] projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift <ref> [15] </ref> testifies to their efficiency. Open systems have attracted the attention of language designers as well [9, 64], and are an important part of the commercial market for personal computers. Unfortunately, the flexibility and efficiency of open operating systems is obtained at a serious price.
Reference: [16] <author> P. Dasgupta, R. J. LeBlanc, Jr. and W. F. Appelbe, </author> <title> ``The Clouds Distributed Operating System: Functional Description, Implementation Details and Related Work,'' </title> <booktitle> Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <month> 13-17 June </month> <year> 1988, </year> <pages> pp. 2-9. </pages>
Reference-contexts: This latter decision provides us with a rich set of ready-made tools for perusal and management of long-lived segments. 3.6.3. Software Implementation of Large Address Spaces Many distributed systems provide large name spaces with software interpretation. Some provide names for heavyweight objects <ref> [1, 16] </ref>; others for communication ports [14, 42]; still others for mappable segments [33]. In any case, objects in the distributed name space cannot be accessed via hardware addressing modes (except perhaps with a temporary mapping), and must generally be treated differently from addressable local objects.
Reference: [17] <author> Dobberpuhl and others, </author> <title> ``A 200mhz 64 Bit Dual Issue CMOS Microprocessor,'' </title> <booktitle> Proceedings of the International Solid-State Circuits Conference, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: We therefore adopted uniform addressing for in-core code and data in our earlier Psyche system [52, 53], arguing that the advent of 64-bit architectures would soon eliminate the scarcity of virtual addresses. With the recent release of microprocessors such as the MIPS R4000 and the DEC Alpha <ref> [17] </ref>, we believe that uniform addressing can be adopted without hesitation for large, multi-user systems. Moreover, the truly enormous amount of space addressable in 64 bits makes it possible to extend uniform addressing into the file system, and to unify the entire memory hierarchy into an unsegmented single-level store.
Reference: [18] <author> J. Edler, J. Lipkis and E. Schonberg, </author> <title> ``Process Management for Highly Parallel UNIX Systems,'' </title> <note> Ultracomputer Note #136, </note> <author> Courant Institute, N. Y. U., </author> <month> April </month> <year> 1988. </year>
Reference-contexts: For lighter-weight synchronization, blocking mechanisms can be implemented in user space by providing standard interfaces to thread schedulers [37], and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections <ref> [2, 18, 37] </ref>, or by relinquishing the processor when a lock is unavailable [8, 29]. 3.5.2. Garbage Collection When a Unix process finishes execution or terminates abnormally, its private segments can be reclaimed. The same cannot be said of segments shared between processes.
Reference: [19] <author> C. N. Fischer and R. J. LeBlanc, Jr., </author> <title> Crafting a Compiler, </title> <address> Benjamin/Cummings, Menlo Park, CA, </address> <year> 1988. </year>
Reference-contexts: In a related case study, we have examined our compiler for the Lynx distributed programming language [54, 56], designed around scanner and parser generators developed at the University of Wisconsin <ref> [19] </ref>. The Wisconsin tools produce numeric tables for separately-developed drivers. They come with drivers written in Pascal. At startup, these drivers read the tables from files. They translate them into appropriate data structures, and then begin to parse the user's program.
Reference: [20] <author> B. Fleisch and G. Popek, </author> <title> ``Mirage: A Coherent Distributed Shared Memory Design,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 211-223. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [21] <author> P. Gautron and M. Shapiro, </author> <title> ``Two Extensions to C++: A Dynamic Link Editor and Inner Data,'' </title> <booktitle> Proceedings of the USENIX C++ Workshop, </booktitle> <month> November </month> <year> 1987, </year> <pages> pp. 23-32. </pages>
Reference-contexts: Dynamic linking is an integral part of single-user open operating systems such as Cedar [57], and has been implemented under Unix as part of such self-contained environments as Emerald [27] and the Portable Common Runtime [61]. The CLAM user interface system [10] and SOS distributed object system <ref> [21] </ref> load C++ classes dynamically; the latter is based on the Andrew project's Camphor dynamic linker. Our work differs from these projects in its use of dynamic linking to share potentially writable objects transparently, between ordinary Unix programs. 4. Implementation Details 4.1.
Reference: [22] <author> D. Gifford, P. Jouvelot, M. Sheldon and J. W. O'Toole, Jr., </author> <title> ``Semantic File Systems,'' </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 13-16 October </month> <year> 1991, </year> <pages> pp. 16-25. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 25:5. </booktitle>
Reference-contexts: Pseudo-terminals and Unix domain sockets are now commonplace, but were missing in early versions of Unix. Bershad and Pinkerton [4] describe a general-purpose mechanism for installing a user-provided process that intercepts operations on a specified file. Gifford et al. <ref> [22] </ref> describe a similar mechanism that allows the user to simulate files that are created on demand. Our plans (section 2.2) for a directory containing images of running programs were directly inspired by Killian [30].
Reference: [23] <author> R. A. Gingell, J. P. Moran and W. A. Shannon, </author> <title> ``Virtual Memory Architecture in SunOS,'' </title> <booktitle> USENIX Association Conference Proceedings, </booktitle> <month> June </month> <year> 1987, </year> <pages> pp. 81-94. </pages>
Reference-contexts: It suffered something of a hiatus in the 1970s, but has now been incorporated into most variants of Unix. The Berkeley mmap facility was designed, though never actually included, as part of the 4.2 and 4.3 BSD releases [34]; it appears in several commercial systems, including SunOS <ref> [23] </ref> and IRIX. AT&T's shm facility became available in Unix System V and its derivatives.
Reference: [24] <author> M. Herlihy and B. Liskov, </author> <title> ``A Value Transmission Method for Abstract Data Types,'' </title> <journal> ACM Transactions on Programming Languages and Systems 4:4 (October 1982), </journal> <pages> pp. 527-551. </pages>
Reference-contexts: The complexity of this saving and restoring is a perennial complaint of compiler writers, and much research has been devoted to automating the process [32, 41]. 8 Similar work has occurred in the message-passing community <ref> [24] </ref>. With pointers permitted in files, and with a global consensus on the location of every segment, pointer-rich data structures can be left in their original form when saved across program executions.
Reference: [25] <author> W. W. Ho and R. A. Olsson, </author> <title> ``An Approach to Genuine Dynamic Linking,'' </title> <journal> Software Practice and Experience 21:4 (April 1991), </journal> <pages> pp. 375-390. </pages>
Reference-contexts: Given the opportunity, we will adopt the SunOS jump-table-based lazy linking mechanism as an optimization: modules first accessed by calling a (named) function will be linked without fault-handling overhead. Both SunOS and dld <ref> [25] </ref> provide library routines that allow the user to link object modules into a running program. Dld will resolve undefined references in the modules it brings in, allowing them to point into the main program or into other dynamically-loaded modules. <p> In addition, lds must create any static public modules that do not yet exist, and must initialize those objects from their templates. 4.2. Dynamic Linker Our early work used Ho's <ref> [25] </ref> dynamic linker dld on the Sun Sparcstation. After moving to the SGI it became apparent that many of the features we were planning to implement would require a major restructuring of dld.
Reference: [26] <author> M. B. Jones, R. F. Rashid and M. R. Thompson, ``Matchmaker: </author> <title> An Interface Specification Language for Distributed Processing,'' </title> <booktitle> Conference Record of the Twelfth ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1985, </year> <pages> pp. 225-235. 32 </pages>
Reference-contexts: The code required to save and restore information in files and message buffers is a major contributor to software complexity, and much research has been aimed at reducing this burden (e.g. through data description languages [32] and RPC stub generators <ref> [3, 26] </ref>). (4) When supported by hardware, shared memory is generally faster than either messages or files, since operating system overhead and copying costs can often be avoided.
Reference: [27] <author> E. Jul, H. Levy, N. Hutchinson and A. Black, </author> <title> ``Fine-Grained Mobility in the Emerald System,'' </title> <journal> ACM Transactions on Computer Systems 6:1 (February 1988), </journal> <pages> pp. 109-133. </pages> <booktitle> Originally presented at the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <address> Austin, TX, </address> <month> 8-11 November </month> <year> 1987. </year>
Reference-contexts: In a few systems, compiler support has been integrated with a sophisticated run-time environment to provide the appearance of uniform naming in a very large name space. The LOOM system [28] implements a 32-bit Smalltalk environment on a 16-bit machine. The Emerald system <ref> [27] </ref> provides a uniform object model on a distributed network of machines. The term pointer swizzling is used to describe systems that transparently manage two distinct sets of addresses for objects: long addresses on secondary storage, and short addresses in main memory. <p> Our use of dynamic linking is reminiscent of several other systems. Dynamic linking is an integral part of single-user open operating systems such as Cedar [57], and has been implemented under Unix as part of such self-contained environments as Emerald <ref> [27] </ref> and the Portable Common Runtime [61]. The CLAM user interface system [10] and SOS distributed object system [21] load C++ classes dynamically; the latter is based on the Andrew project's Camphor dynamic linker.
Reference: [28] <author> T. Kaehler, </author> <title> ``Virtual Memory on a Narrow Machine for an Object-Oriented Language,'' </title> <booktitle> OOPSLA'86 Conference Proceedings, </booktitle> <month> 29 September - 2 October </month> <year> 1986, </year> <pages> pp. 87-106. </pages> <booktitle> In ACM SIGPLAN Notices 21:11. </booktitle>
Reference-contexts: In a few systems, compiler support has been integrated with a sophisticated run-time environment to provide the appearance of uniform naming in a very large name space. The LOOM system <ref> [28] </ref> implements a 32-bit Smalltalk environment on a 16-bit machine. The Emerald system [27] provides a uniform object model on a distributed network of machines.
Reference: [29] <author> A. R. Karlin, K. Li, M. S. Manasse and S. Owicki, </author> <title> ``Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor,'' </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 13-16 October </month> <year> 1991, </year> <pages> pp. 41-55. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 25:5. </booktitle>
Reference-contexts: user space by providing standard interfaces to thread schedulers [37], and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections [2, 18, 37], or by relinquishing the processor when a lock is unavailable <ref> [8, 29] </ref>. 3.5.2. Garbage Collection When a Unix process finishes execution or terminates abnormally, its private segments can be reclaimed. The same cannot be said of segments shared between processes. Sharing introduces (or at least exacerbates) the problem of garbage collection.
Reference: [30] <author> T. J. Killian, </author> <title> ``Processes as Files,'' </title> <booktitle> Proceedings of the Usenix Software Tools Users Group Summer Conference, </booktitle> <month> 12-15 June </month> <year> 1984, </year> <pages> pp. 203-207. </pages>
Reference-contexts: We treat the interfaces as alternative views of a single underlying abstraction. This is in some sense the philosophy behind the Berkeley mmap call; we take it a step further by providing names for ``unnamed'' memory objects (in a manner inspired by Killian's /proc directory <ref> [30] </ref>), and by providing every byte of secondary storage with a unique virtual address. To some extent, we return to the philosophy of Multics, but with true global pointers, a flat address space, and Unix-style naming, protection, and sharing. <p> Memory segments that traditionally have no file system name, such as private text, data, and stack segments, remain nameless in our prototype system. On a 64-bit machine, we plan to place them under a special ``/proc'' hierarchy in the file system, as suggested by Killian <ref> [30] </ref>. We have considered several possible organizations for /proc. It is difficult to evaluate these organizations, however; we doubt our ability to anticipate all the uses there could be for the 10 directory. When our 64-bit machines arrive, we are likely to start with a minimalist approach. <p> Gifford et al. [22] describe a similar mechanism that allows the user to simulate files that are created on demand. Our plans (section 2.2) for a directory containing images of running programs were directly inspired by Killian <ref> [30] </ref>. In his system, the address space of each process appears in the file system as /proc/nnnnn, where nnnnn is the process id number. These files can be inspected and modified with the usual file access functions, lseek, read and write.
Reference: [31] <author> E. J. Koldinger, H. M. Levy, J. S. Chase and S. J. Eggers, </author> <title> ``The Protection Lookaside Buffer: Efficient Protection for Single Address-Space Computers,'' </title> <type> Technical Report 91-11-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: These decisions have opportunity costs: we cannot count on compilers to implement protection, collect garbage, or generate self-descriptive data structures, nor can we pursue hardware optimizations based on the use of a single virtual-to-physical translation <ref> [31] </ref>. From our perspective, these costs are overshadowed by the benefits of backward compatibility. 3.2. Address Space Organization suggested in earlier sections, we will reserve a 32-bit portion of the 64-bit virtual address space for private code and data.
Reference: [32] <author> D. A. Lamb, </author> <title> ``IDL: Sharing Intermediate Representations,'' </title> <journal> ACM Transactions on Programming Languages and Systems 9:3 (July 1987), </journal> <pages> pp. 297-318. </pages>
Reference-contexts: The code required to save and restore information in files and message buffers is a major contributor to software complexity, and much research has been aimed at reducing this burden (e.g. through data description languages <ref> [32] </ref> and RPC stub generators [3, 26]). (4) When supported by hardware, shared memory is generally faster than either messages or files, since operating system overhead and copying costs can often be avoided. <p> The complexity of this saving and restoring is a perennial complaint of compiler writers, and much research has been devoted to automating the process <ref> [32, 41] </ref>. 8 Similar work has occurred in the message-passing community [24]. With pointers permitted in files, and with a global consensus on the location of every segment, pointer-rich data structures can be left in their original form when saved across program executions.
Reference: [33] <author> P. J. Leach, P. H. Levine, B. P. Douros, J. A. Hamilton, D. L. Nelson and B. L. Stumpf, </author> <title> ``The Architecture of an Integrated Local Network,'' </title> <journal> IEEE Journal on Selected Areas in Communications 5 (November 1983), </journal> <pages> pp. 842-857. </pages>
Reference-contexts: Software Implementation of Large Address Spaces Many distributed systems provide large name spaces with software interpretation. Some provide names for heavyweight objects [1, 16]; others for communication ports [14, 42]; still others for mappable segments <ref> [33] </ref>. In any case, objects in the distributed name space cannot be accessed via hardware addressing modes (except perhaps with a temporary mapping), and must generally be treated differently from addressable local objects.
Reference: [34] <author> S. J. Leffler, M. K. McKusick, M. J. Karels and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> The Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: It suffered something of a hiatus in the 1970s, but has now been incorporated into most variants of Unix. The Berkeley mmap facility was designed, though never actually included, as part of the 4.2 and 4.3 BSD releases <ref> [34] </ref>; it appears in several commercial systems, including SunOS [23] and IRIX. AT&T's shm facility became available in Unix System V and its derivatives.
Reference: [35] <author> K. Li and P. Hudak, </author> <title> ``Memory Coherence in Shared Virtual Memory Systems,'' </title> <journal> ACM Transactions on Computer Systems 7:4 (November 1989), </journal> <pages> pp. 321-359. </pages> <booktitle> Originally presented at the Fifth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> 11-13 August </month> <year> 1986. </year>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [36] <author> K. Li and R. Schaefer, </author> <title> ``A Hypercube Shared Virtual Memory System,'' </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [37] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc and E. P. Markatos, </author> <title> ``First-Class User-Level Threads,'' </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 14-16 October </month> <year> 1991, </year> <pages> pp. 110-121. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 25:5. </booktitle>
Reference-contexts: Synchronization Files are seldom write-shared, and message passing subsumes synchronization. When accessing shared memory, however, processes must synchronize explicitly. Unix already includes kernel-supported semaphores. For lighter-weight synchronization, blocking mechanisms can be implemented in user space by providing standard interfaces to thread schedulers <ref> [37] </ref>, and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections [2, 18, 37], or by relinquishing the processor when a lock is unavailable [8, 29]. 3.5.2. <p> For lighter-weight synchronization, blocking mechanisms can be implemented in user space by providing standard interfaces to thread schedulers [37], and several researchers have demonstrated that spin locks can be used successfully in user space as well, by preventing, avoiding, or recovering from preemption during critical sections <ref> [2, 18, 37] </ref>, or by relinquishing the processor when a lock is unavailable [8, 29]. 3.5.2. Garbage Collection When a Unix process finishes execution or terminates abnormally, its private segments can be reclaimed. The same cannot be said of segments shared between processes.
Reference: [38] <author> B. D. Marsh, </author> <title> ``Multi-Model Parallel Programming,'' </title> <type> Ph. D. Thesis, TR 413, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Calls of this sort are reminiscent of semi-model operations in Psyche <ref> [38, 40] </ref>; the interface to a Psyche server consists of subroutines that transfer into the server's domain only when one of a well-defined set of conditions makes such a transfer essential.
Reference: [39] <author> B. D. Marsh, C. M. Brown, T. J. LeBlanc, M. L. Scott, T. G. Becker, P. Das, J. Karlsson and C. A. Quiroz, </author> <title> ``The Rochester Checkers Player: Multi-Model Parallel Programming for Animate Vision,'' </title> <booktitle> Computer 25:2 (February 1992), </booktitle> <pages> pp. 12-19. </pages>
Reference: [40] <author> B. D. Marsh, C. M. Brown, T. J. LeBlanc, M. L. Scott, T. G. Becker, P. Das, J. Karlsson and C. A. Quiroz, </author> <title> ``Operating System Support for Animate Vision,'' </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> to appear. Earlier version published as TR 374, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> June </month> <year> 1991. </year> <month> 33 </month>
Reference-contexts: Calls of this sort are reminiscent of semi-model operations in Psyche <ref> [38, 40] </ref>; the interface to a Psyche server consists of subroutines that transfer into the server's domain only when one of a well-defined set of conditions makes such a transfer essential.
Reference: [41] <author> C. R. Morgan, </author> <title> ``Special Issue on the Interface Description Language IDL,'' </title> <journal> ACM SIG-PLAN Notices 22:11 (November 1987). </journal>
Reference-contexts: The complexity of this saving and restoring is a perennial complaint of compiler writers, and much research has been devoted to automating the process <ref> [32, 41] </ref>. 8 Similar work has occurred in the message-passing community [24]. With pointers permitted in files, and with a global consensus on the location of every segment, pointer-rich data structures can be left in their original form when saved across program executions.
Reference: [42] <author> S. J. Mullender, G. van Rossum, A. S. Tanenbaum, R. van Renesse and H. van Staveren, </author> <title> ``Amoeba: A Distributed Operating System for the 1990s,'' </title> <booktitle> Computer 23:5 (May 1990), </booktitle> <pages> pp. 44-53. </pages>
Reference-contexts: This latter decision provides us with a rich set of ready-made tools for perusal and management of long-lived segments. 3.6.3. Software Implementation of Large Address Spaces Many distributed systems provide large name spaces with software interpretation. Some provide names for heavyweight objects [1, 16]; others for communication ports <ref> [14, 42] </ref>; still others for mappable segments [33]. In any case, objects in the distributed name space cannot be accessed via hardware addressing modes (except perhaps with a temporary mapping), and must generally be treated differently from addressable local objects.
Reference: [43] <author> B. Nitzberg and V. Lo, </author> <title> ``Distributed Shared Memory: A Survey of Issues and Algorithms,'' </title> <booktitle> Computer 24:8 (August 1991), </booktitle> <pages> pp. 52-60. </pages>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [44] <author> E. I. Organick, </author> <title> The Multics System: An Examination of Its Structure, </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1972. </year>
Reference-contexts: 1. Introduction Memory sharing between arbitrary processes is at least as old as Multics <ref> [44] </ref>. It suffered something of a hiatus in the 1970s, but has now been incorporated into most variants of Unix. <p> A different philosophical position is taken in systems such as Multics <ref> [44] </ref>, Hydra [65], and Opal [13], which clearly separate code from data and speak explicitly of processes executing in shared code but using private (static) data. Multics employs an elaborate hardware/software mechanism in which references to static data are made indirectly through a base register and process-private link segment. <p> Our work is an attempt to provide some of the benefits gained in an open operating system while still providing a standard means of protection. 3.6.2. Shared Memory Operating Systems Several other projects have attempted to encourage sharing in multi-user systems. Multics <ref> [44] </ref> and Hydra [65] are probably the best-known examples. Both provide a single-level store, but were implemented on narrow-address machines. Multics employs a segmented address space and relies on elaborate hardware addressing modes and procedure calling sequences to maintain segment registers and tables.
Reference: [45] <author> J. Ousterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer and J. Thompson, </author> <title> ``A Trace-Driven Analysis of the UNIX 4.2 BSD File System,'' </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985, </year> <pages> pp. 15-24. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 19:5. </booktitle>
Reference-contexts: We plan to exclude /tmp and /proc; files created in these directories are meant to be transient. As a result, associations can probably be guaranteed to last for days or weeks: Ousterhout et al.'s (admittedly dated) study of file access patterns <ref> [45] </ref> reports a rate of only about 500 file creations and 500 deletions per hour, even including /tmp. A related problem occurs when renaming files.
Reference: [46] <author> J. Ousterhout, </author> <title> ``Push Technology, Not Abstractions,'' </title> <booktitle> ACM SIGOPS Operating Systems Review 26:1 (January 1992), </booktitle> <pages> pp. 7-11. </pages> <booktitle> Overhead slides from a panel presentation at the Thirteenth ACM Symposium on Operating Systems Principles. </booktitle>
Reference-contexts: Invocation of precedent has proven to be a useful aid to self-discipline, keep ing discussions focused on issues that really matter. (3) We retain a set of abstractions which, while certainly not perfect, have a proven track record of usefulness <ref> [46] </ref>. For issues on which we defer to Unix, the results will most likely be acceptable. We should emphasize that we are not committed to complete compatibility with Unix. We consider ourselves free to make changes to anything that really needs changing.
Reference: [47] <author> U. Ramachandran and M. Y. A. Khalidi, </author> <title> ``An Implementation of Distributed Shared Memory,'' </title> <booktitle> Proceedings of the First USENIX Workshop on Experiences Building Distributed and Multiprocessor Systems, </booktitle> <month> 5-6 October, </month> <year> 1989, </year> <pages> pp. 21-38. </pages>
Reference-contexts: Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. The growing popularity of distributed shared memory systems <ref> [11, 20, 35, 36, 43, 47] </ref> suggests that programmers will adopt a sharing model even at the expense of performance. (2) Shared memory facilitates transparent, asynchronous interaction between processes, and shares with files the advantage of not requiring that the interacting processes be active con currently. (3) When interacting processes agree
Reference: [48] <author> D. D. Redell, Y. K. Dalal, T. R. Horsley, H. C. Lauer, W. C. Lynch, P. R. McJones, H. G. Murray and S. C. Purcell, </author> <title> ``Pilot: An Operating System for a Personal Computer,'' </title> <journal> Communications of the ACM 23:2 (February 1980), </journal> <pages> pp. 81-92. </pages>
Reference-contexts: In summary, we believe it is time for the advantages of memory sharing, long understood in the open operating systems community <ref> [48, 57, 60] </ref>, to be extended into environments with multiple users and hardware-enforced protection domains. <p> Flexibility stems from the opportunity to modify, invoke, or build upon existing pieces of code. Efficiency stems from the lack of heavyweight context switches, data movement across narrow 19 interfaces, or unnecessary layers of abstraction. The incredible productivity of Lisp environments such as Genera [60], and of the Pilot <ref> [48] </ref> and Cedar [57] projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift [15] testifies to their efficiency. Open systems have attracted the attention of language designers as well [9, 64], and are an important part of the commercial market for personal computers.
Reference: [49] <author> M. Rozier and others, </author> <title> ``Chorus Distributed Operating Systems,'' </title> <booktitle> Computing Systems 1:4 (Fall 1988), </booktitle> <pages> pp. 305-370. </pages>
Reference-contexts: AT&T's shm facility became available in Unix System V and its derivatives. More recently, memory sharing via inheritance has been incorporated in the versions of Unix for several commercial multiprocessors, and the external pager mechanisms of Mach [66] and Chorus <ref> [49] </ref> can be used to establish data sharing between arbitrary processes. Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing.
Reference: [50] <author> M. L. Scott and A. L. Cox, </author> <title> ``An Empirical Study of Message-Passing Overhead,'' </title> <booktitle> Proceedings of the Seventh International Conference on Distributed Computing Systems, </booktitle> <month> 21-25 September </month> <year> 1987, </year> <pages> pp. 536-543. </pages>
Reference-contexts: Bershad [6] uses buffers mapped into sending and receiving address spaces to speed the implementation of intra-machine remote procedure calls. The implementation of the Lynx distributed programming language for the BBN Butterfly employs a similar optimization <ref> [50] </ref>. Mach [66] uses copy-on-write page sharing to optimize intra-computer data transfers, and its external pagers can be used to facilitate data sharing between processes. 3.6.5.
Reference: [51] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Design Rationale for Psyche, a General-Purpose Multiprocessor Operating System,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, V. II Software, </booktitle> <month> 15-19 August </month> <year> 1988, </year> <pages> pp. 255-262. </pages>
Reference-contexts: We discuss implementation details in section 4, provide examples of the use of our tools in section 5, and conclude in section 6. 2. Overview Our emphasis on shared memory has its roots in the Psyche project <ref> [51, 52] </ref>. Our focus in Psyche was on mechanisms and conventions that allow processes from dissimilar programming models (e.g. Lynx threads and Multilisp futures) to share data abstractions, and to synchronize correctly [37-40, 53]. Fundamental to this work was the assumption that sharing would occur both within and among applications.
Reference: [52] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Evolution of an Operating System for Large-Scale Shared-Memory Multiprocessors,'' </title> <type> TR 309, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: It also makes virtual addresses an extremely scare resource on 32-bit machines. We believe that pointers are crucial for realizing the full potential of shared memory. We therefore adopted uniform addressing for in-core code and data in our earlier Psyche system <ref> [52, 53] </ref>, arguing that the advent of 64-bit architectures would soon eliminate the scarcity of virtual addresses. With the recent release of microprocessors such as the MIPS R4000 and the DEC Alpha [17], we believe that uniform addressing can be adopted without hesitation for large, multi-user systems. <p> We discuss implementation details in section 4, provide examples of the use of our tools in section 5, and conclude in section 6. 2. Overview Our emphasis on shared memory has its roots in the Psyche project <ref> [51, 52] </ref>. Our focus in Psyche was on mechanisms and conventions that allow processes from dissimilar programming models (e.g. Lynx threads and Multilisp futures) to share data abstractions, and to synchronize correctly [37-40, 53]. Fundamental to this work was the assumption that sharing would occur both within and among applications.
Reference: [53] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <booktitle> ``Multi-Model Parallel Programming in Psyche,'' Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> 14-16 March, </month> <year> 1990, </year> <pages> pp. 70-78. </pages> <booktitle> In ACM SIGPLAN Notices 25:3. </booktitle>
Reference-contexts: It also makes virtual addresses an extremely scare resource on 32-bit machines. We believe that pointers are crucial for realizing the full potential of shared memory. We therefore adopted uniform addressing for in-core code and data in our earlier Psyche system <ref> [52, 53] </ref>, arguing that the advent of 64-bit architectures would soon eliminate the scarcity of virtual addresses. With the recent release of microprocessors such as the MIPS R4000 and the DEC Alpha [17], we believe that uniform addressing can be adopted without hesitation for large, multi-user systems. <p> Overview Our emphasis on shared memory has its roots in the Psyche project [51, 52]. Our focus in Psyche was on mechanisms and conventions that allow processes from dissimilar programming models (e.g. Lynx threads and Multilisp futures) to share data abstractions, and to synchronize correctly <ref> [37-40, 53] </ref>. Fundamental to this work was the assumption that sharing would occur both within and among applications. Our current work [55] can be considered an attempt to make that sharing commonplace in the context of traditional operating systems.
Reference: [54] <author> M. L. Scott, </author> <title> ``The Lynx Distributed Programming Language: Motivation, Design, and Experience,'' </title> <booktitle> Computer Languages 16:3/4 (1991), </booktitle> <pages> pp. 209-233. </pages> <note> Earlier version published as TR 308, ``An Overview of Lynx,'' </note> <institution> Computer Science Department, University of Rochester, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: Segments thus saved are position-dependent, but for the compiler writer this is not a problem; the idea is simply to transfer the data between passes. In a related case study, we have examined our compiler for the Lynx distributed programming language <ref> [54, 56] </ref>, designed around scanner and parser generators developed at the University of Wisconsin [19]. The Wisconsin tools produce numeric tables for separately-developed drivers. They come with drivers written in Pascal. At startup, these drivers read the tables from files.
Reference: [55] <author> M. L. Scott and W. Garrett, </author> <title> ``Shared Memory Ought to be Commonplace,'' </title> <booktitle> Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <month> 23-24 April </month> <year> 1992. </year>
Reference-contexts: Lynx threads and Multilisp futures) to share data abstractions, and to synchronize correctly [37-40, 53]. Fundamental to this work was the assumption that sharing would occur both within and among applications. Our current work <ref> [55] </ref> can be considered an attempt to make that sharing commonplace in the context of traditional operating systems. We use dynamic linking to allow processes to access shared code and data with the same syntax employed for private code and data.
Reference: [56] <author> M. L. Scott, </author> <title> ``LYNX Reference Manual,'' </title> <type> BPR 7, </type> <institution> Computer Science Department, University of Rochester, </institution> <note> August 1986 (revised). 34 </note>
Reference-contexts: Segments thus saved are position-dependent, but for the compiler writer this is not a problem; the idea is simply to transfer the data between passes. In a related case study, we have examined our compiler for the Lynx distributed programming language <ref> [54, 56] </ref>, designed around scanner and parser generators developed at the University of Wisconsin [19]. The Wisconsin tools produce numeric tables for separately-developed drivers. They come with drivers written in Pascal. At startup, these drivers read the tables from files.
Reference: [57] <author> D. Swinehart, P. Zellweger, R. Beach and R. Hagmann, </author> <title> ``A Structural View of the Cedar Programming Environment,'' </title> <journal> ACM Transactions on Programming Languages and Systems 8:4 (October 1986), </journal> <pages> pp. 419-490. </pages>
Reference-contexts: In summary, we believe it is time for the advantages of memory sharing, long understood in the open operating systems community <ref> [48, 57, 60] </ref>, to be extended into environments with multiple users and hardware-enforced protection domains. <p> Efficiency stems from the lack of heavyweight context switches, data movement across narrow 19 interfaces, or unnecessary layers of abstraction. The incredible productivity of Lisp environments such as Genera [60], and of the Pilot [48] and Cedar <ref> [57] </ref> projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift [15] testifies to their efficiency. Open systems have attracted the attention of language designers as well [9, 64], and are an important part of the commercial market for personal computers. <p> Neither dld nor the explicitly-invoked Sun routines will resolve undefined references in the main program; they simply return pointers to the newly-available symbols. Our use of dynamic linking is reminiscent of several other systems. Dynamic linking is an integral part of single-user open operating systems such as Cedar <ref> [57] </ref>, and has been implemented under Unix as part of such self-contained environments as Emerald [27] and the Portable Common Runtime [61]. The CLAM user interface system [10] and SOS distributed object system [21] load C++ classes dynamically; the latter is based on the Andrew project's Camphor dynamic linker.
Reference: [58] <author> R. H. Thomas and W. Crowther, </author> <title> ``The Uniform System: An Approach to Runtime Support for Large Scale Shared Memory Parallel Processors,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, V. II Software, </booktitle> <month> 15-19 August </month> <year> 1988, </year> <pages> pp. 245-254. </pages>
Reference-contexts: We adopted a single translation for in-core code and data in Psyche, but were not entirely happy with the result. We were forced, for example, to change the semantics of BBN's Uniform System library <ref> [58] </ref> in order to port it to Psyche. 2 We fear that other programs and programming environments (e.g. Lisp interpreters that use address bits for tags) may also insist on the ability to overload addresses.
Reference: [59] <author> W. F. Tichy, </author> <title> ``Design, Implementation, and Evaluation of a Revision Control System,'' </title> <booktitle> Proceedings of the Sixth International Conference on Software Engineering, </booktitle> <month> September </month> <year> 1982. </year>
Reference-contexts: We would also like to put it back at its previous address if it is deleted temporarily in the process of being updated (this happens, for example, when checking files in and out of rcs <ref> [59] </ref>). Our solution is to remember the addresses of deleted files, and to be careful about when we re-use addresses. When a file is removed, the kernel remembers its name and address.
Reference: [60] <author> J. H. Walker, D. A. Moon, D. L. Weinreb and M. McMahon, </author> <title> ``The Symbolics Genera Programming Environment,'' </title> <journal> IEEE Software 4:6 (November 1987), </journal> <pages> pp. 36-45. </pages>
Reference-contexts: In summary, we believe it is time for the advantages of memory sharing, long understood in the open operating systems community <ref> [48, 57, 60] </ref>, to be extended into environments with multiple users and hardware-enforced protection domains. <p> Flexibility stems from the opportunity to modify, invoke, or build upon existing pieces of code. Efficiency stems from the lack of heavyweight context switches, data movement across narrow 19 interfaces, or unnecessary layers of abstraction. The incredible productivity of Lisp environments such as Genera <ref> [60] </ref>, and of the Pilot [48] and Cedar [57] projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift [15] testifies to their efficiency.
Reference: [61] <author> M. Weiser, A. Demers and C. Hauser, </author> <title> ``The Portable Common Runtime Approach to Interoperability,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 114-122. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 23:5. </booktitle>
Reference-contexts: Our use of dynamic linking is reminiscent of several other systems. Dynamic linking is an integral part of single-user open operating systems such as Cedar [57], and has been implemented under Unix as part of such self-contained environments as Emerald [27] and the Portable Common Runtime <ref> [61] </ref>. The CLAM user interface system [10] and SOS distributed object system [21] load C++ classes dynamically; the latter is based on the Andrew project's Camphor dynamic linker.
Reference: [62] <author> M. Weiser, L. P. Deutsch and P. B. Kessler, </author> <title> ``UNIX Needs a True Integrated Environment: CASE Closed,'' </title> <type> Technical Report CSL-89-4, </type> <note> Xerox PARC, 1989. Earlier version published as ``Toward a Single Milieu,'' UNIX Review 6:11. </note>
Reference-contexts: We concur with Weiser et al. <ref> [62] </ref> that the proliferation of languages and protection boundaries in Unix will make it difficult to realize the full flexibility of the open system model. <p> supplant the use of files in traditional Unix utilities? (5) In general, how much of the power and flexibility of open operating systems can be extended to an environment with multiple users and languages? Many of the issues involved in this last question are under investigation at Xerox PARC (see <ref> [62] </ref> in particular). The multiple languages of Unix, and the reliance on kernel protection, pose serious obstacles to the construction of integrated programming environments. It is not clear whether all of these obstacles can be overcome, but there is certainly much room for improvement.
Reference: [63] <author> P. R. Wilson, </author> <title> ``Pointer Swizzling at Page Fault Time: Efficiently Supporting Huge Address Spaces on Standard Hardware,'' </title> <journal> ACM SIGARCH Computer Architecture News 19:4 (June 1991), </journal> <pages> pp. 6-13. </pages>
Reference-contexts: The Emerald system [27] provides a uniform object model on a distributed network of machines. The term pointer swizzling is used to describe systems that transparently manage two distinct sets of addresses for objects: long addresses on secondary storage, and short addresses in main memory. Wilson <ref> [63] </ref> has observed that by implementing swizzling in the kernel's virtual memory system, it is possible to maintain consistent short addresses for all processes on the machine with a high degree of efficiency.
Reference: [64] <author> N. Wirth, </author> <title> ``From Programming Language Design to Computer Construction,'' </title> <journal> Communications of the ACM 28:2 (February 1985), </journal> <pages> pp. 159-164. </pages> <note> The 1984 Turing Award Lecture. </note>
Reference-contexts: The incredible productivity of Lisp environments such as Genera [60], and of the Pilot [48] and Cedar [57] projects at Xerox PARC, testify to the usefulness of open systems. Clark's experience with Swift [15] testifies to their efficiency. Open systems have attracted the attention of language designers as well <ref> [9, 64] </ref>, and are an important part of the commercial market for personal computers. Unfortunately, the flexibility and efficiency of open operating systems is obtained at a serious price. Protection is available only to the extent that it is provided by high-level language compilers.
Reference: [65] <author> W. A. Wulf, R. Levin and S. P. Harbison, Hydra/C.mmp: </author> <title> An Experimental Computer System, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: A different philosophical position is taken in systems such as Multics [44], Hydra <ref> [65] </ref>, and Opal [13], which clearly separate code from data and speak explicitly of processes executing in shared code but using private (static) data. Multics employs an elaborate hardware/software mechanism in which references to static data are made indirectly through a base register and process-private link segment. <p> Our work is an attempt to provide some of the benefits gained in an open operating system while still providing a standard means of protection. 3.6.2. Shared Memory Operating Systems Several other projects have attempted to encourage sharing in multi-user systems. Multics [44] and Hydra <ref> [65] </ref> are probably the best-known examples. Both provide a single-level store, but were implemented on narrow-address machines. Multics employs a segmented address space and relies on elaborate hardware addressing modes and procedure calling sequences to maintain segment registers and tables. Hydra employs capabilities that are interpreted by the kernel.
Reference: [66] <author> M. Young, A. Tevanian, R. Rashid, D. Golub, J. Eppinger, J. Chew, W. Bolosky, D. Black and R. Baron, </author> <title> ``The Duality of Memory and Communication in the Implementation of a Multiprocessor Operating System,'' </title> <booktitle> Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <month> 8-11 November </month> <year> 1987, </year> <pages> pp. 63-76. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 21:5. </booktitle>
Reference-contexts: AT&T's shm facility became available in Unix System V and its derivatives. More recently, memory sharing via inheritance has been incorporated in the versions of Unix for several commercial multiprocessors, and the external pager mechanisms of Mach <ref> [66] </ref> and Chorus [49] can be used to establish data sharing between arbitrary processes. Shared memory has several important advantages over interaction via files or messages. (1) Many programmers find shared memory more conceptually appealing than message passing. <p> Bershad [6] uses buffers mapped into sending and receiving address spaces to speed the implementation of intra-machine remote procedure calls. The implementation of the Lynx distributed programming language for the BBN Butterfly employs a similar optimization [50]. Mach <ref> [66] </ref> uses copy-on-write page sharing to optimize intra-computer data transfers, and its external pagers can be used to facilitate data sharing between processes. 3.6.5.
Reference: [67] <institution> Usenix Workshop on MicroKernels and other Kernel Architectures, </institution> <address> Seattle, WA, </address> <month> 27-28 April </month> <year> 1992. </year>
Reference-contexts: In their work on lightweight and user-level remote procedure calls, Bershad et al. argue that the speed of optimized interfaces permits a much more modular style of system construction than has been the norm to date [6, 7]. The growing interest in microkernels <ref> [67] </ref> suggests that this philosophy is catching on. In effect, the microker-nel argument is that the proliferation of boundaries becomes acceptable when crossing these boundaries is cheap.
References-found: 67

