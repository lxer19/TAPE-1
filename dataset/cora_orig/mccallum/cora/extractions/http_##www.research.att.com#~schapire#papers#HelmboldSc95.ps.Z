URL: http://www.research.att.com/~schapire/papers/HelmboldSc95.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: dph@cse.ucsc.edu  schapire@research.att.com  
Title: Machine Learning 27(1):51-68, 1997. Predicting nearly as well as the best pruning of a decision tree  
Author: DAVID P. HELMBOLD ROBERT E. SCHAPIRE 
Address: Santa Cruz, CA 95064  600 Mountain Avenue, Murray Hill, NJ 07974  
Affiliation: Computer and Information Sciences, University of California,  AT&T Labs,  
Abstract: Many algorithms for inferring a decision tree from data involve a two-phase process: First, a very large decision tree is grown which typically ends up "over-fitting" the data. To reduce over-fitting, in the second phase, the tree is pruned using one of a number of available methods. The final tree is then output and used for classification on test data. In this paper, we suggest an alternative approach to the pruning phase. Using a given unpruned decision tree, we present a new method of making predictions on test data, and we prove that our algorithm's performance will not be "much worse" (in a precise technical sense) than the predictions made by the best reasonably small pruning of the given decision tree. Thus, our procedure is guaranteed to be competitive (in terms of the quality of its predictions) with any pruning algorithm. We prove that our procedure is very efficient and highly robust. Our method can be viewed as a synthesis of two previously studied techniques. First, we apply Cesa-Bianchi et al.'s [4] results on predicting using "expert advice" (where we view each pruning as an "expert") to obtain an algorithm that has provably low prediction loss, but that is com-putationally infeasible. Next, we generalize and apply a method developed by Buntine [3], [2] and Willems, Shtarkov and Tjalkens [20], [21] to derive a very efficient implementation of this procedure. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: In this model, at each time step t = 1; : : : ; T , the learner receives an instance x t and must generate a prediction ^y t 2 <ref> [0; 1] </ref>. After an outcome y t 2 f0; 1g is observed (which can be thought of as the label or correct classification of the instance x t ), the learner suffers loss jy t ^y t j. <p> This can be modeled in our setting by storing multiple prediction rules (one for each rejected split) at the nodes (see Section 5). According to Breiman et al. <ref> [1] </ref>, pages 87-92, predicting with the leaves of a decision tree will often have error at most twice that of the best pruning. <p> For example, in Figure 2, we have indicated the predictions associated with the leaves of the two prunings. These predictions are real numbers in <ref> [0; 1] </ref>, whose interpretation is discussed further below. Note that, since both prunings contain the leaves FT and FF, both prunings give the same prediction whenever an instance reaches one of these leaves. <p> In this model, learning takes place in a sequence of trials t = 1; : : :; T . At each time step t, an instance x t is observed, and each pruning P generates a prediction ~ t P 2 <ref> [0; 1] </ref>. The master algorithm combines these 6 D.P. HELMBOLD AND R.E. SCHAPIRE predictions to produce its own prediction ^y t 2 [0; 1]. Finally, feedback y t 2 f0; 1g is observed. <p> At each time step t, an instance x t is observed, and each pruning P generates a prediction ~ t P 2 <ref> [0; 1] </ref>. The master algorithm combines these 6 D.P. HELMBOLD AND R.E. SCHAPIRE predictions to produce its own prediction ^y t 2 [0; 1]. Finally, feedback y t 2 f0; 1g is observed. For example, if the path for instance x t starts with TT then the prunings in Figure 2 make the predictions 0:9 and 0:6. <p> As discussed above, the prediction ~ t P of the pruning P is given, intuitively, by a mini-expert at the leaf reached by P. That is, we assume formally that each 1 node u of T generates a prediction pred t (u) 2 <ref> [0; 1] </ref> for instance x t , and furthermore, that ~ t P = pred t (leaf P (x t )) (1) for all P. The loss of the master algorithm at time t is j^y t y t j. We can interpret the prediction ^y t 2 [0; 1] as <p> (u) 2 <ref> [0; 1] </ref> for instance x t , and furthermore, that ~ t P = pred t (leaf P (x t )) (1) for all P. The loss of the master algorithm at time t is j^y t y t j. We can interpret the prediction ^y t 2 [0; 1] as the bias of a probabilistic prediction in f0; 1g which is 1 with probability ^y t , and 0 with probability 1 ^y t . <p> This prior favors those prunings which are small and thus unlikely to reflect noise in the training set. (In small prunings, each leaf's prediction will tend to be based on more examples; see the discussion of bias versus variance in Breiman et al. <ref> [1] </ref>, pages 87-92.) Although the master algorithm can run with any prior on the prunings, this 2 jPj prior enables us to efficiently implement the master algorithm as described in Section 4. <p> At each time step, the learner computes its prediction as ^y t = F fi (r t ) where fi 2 <ref> [0; 1] </ref> is a parameter of the algorithm, and r t is a weighted average 2 of the predictions of the experts: r t = P w t P P P The function F fi need only be bounded 1 + 2 ln ( 2 F fi (r) 2 ln ( <p> Then the loss of the master algorithm given in Figure 4 is at most L P ln (1=fi) + jPj ln (2) PREDICTING NEARLY AS WELL AS THE BEST PRUNING 13 Input: template tree T access to predictions pred t (u) of mini-experts parameter fi 2 <ref> [0; 1] </ref> Initialize weight 1 (u) = weight 1 (u) = 1 for all nodes u in T Do for t = 1; 2; : : : * Prediction: Given x t 2 fl Compute weighted predictions wpred t (u) for each subtree using the rule: wpred t 8 &gt; &gt; <p> The prediction at time t of such a labeled pruning is denoted ~ t P;I . At each time step t, each node u generates a prediction pred t (u; i) 2 <ref> [0; 1] </ref> for i = 1; : : : ; n where i is the index of a mini-expert at node u. <p> By defining a template tree in which the root node tests the last symbol, its children test the PREDICTING NEARLY AS WELL AS THE BEST PRUNING 17 Input: template tree T access to predictions pred t (u; i) of mini-experts parameter fi 2 <ref> [0; 1] </ref> Initialize weight 1 (u; i) = weight 1 (u) = 1 for all nodes u in T , and i = 1; : : :; n.
Reference: 2. <author> Wray Buntine. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73, </pages> <year> 1992. </year>
Reference-contexts: Thus, the resulting algorithm is very robust. A naive implementation of this procedure would require computation time linear in the number of prunings of T ; obviously, this is infeasible. However, we show how techniques used by Buntine [3], <ref> [2] </ref> and Willems, Shtarkov and Tjalkens [20], [21] can be generalized and applied to our setting, yielding a very efficient implementation requiring computation time at each trial t that is linear in the length of the path defined by the instance x t in the tree T (and therefore is bounded <p> Various authors have presented techniques for averaging a family of decision trees [6], [8], [11]. In particular, using a Bayesian formulation, Buntine [3], <ref> [2] </ref> gave a method called Bayesian smoothing for averaging the class-probability predictions of all possible prunings of a given decision tree. <p> Obviously, we cannot efficiently maintain all of the weights w t P explicitly since there are far too many prunings to consider. Instead, we use a more subtle data structure, similar to the ones used by Buntine [3], <ref> [2] </ref> and Willems, Shtarkov and Tjalkens [20], [21], that can be used to compute the prediction ^y t of the master algorithm. The size of this data structure is proportional to the number of nodes in T (or, more accurately, to the number of nodes that have actually been visited).
Reference: 3. <author> Wray Lindsay Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: Thus, the resulting algorithm is very robust. A naive implementation of this procedure would require computation time linear in the number of prunings of T ; obviously, this is infeasible. However, we show how techniques used by Buntine <ref> [3] </ref>, [2] and Willems, Shtarkov and Tjalkens [20], [21] can be generalized and applied to our setting, yielding a very efficient implementation requiring computation time at each trial t that is linear in the length of the path defined by the instance x t in the tree T (and therefore is <p> Various authors have presented techniques for averaging a family of decision trees [6], [8], [11]. In particular, using a Bayesian formulation, Buntine <ref> [3] </ref>, [2] gave a method called Bayesian smoothing for averaging the class-probability predictions of all possible prunings of a given decision tree. <p> Obviously, we cannot efficiently maintain all of the weights w t P explicitly since there are far too many prunings to consider. Instead, we use a more subtle data structure, similar to the ones used by Buntine <ref> [3] </ref>, [2] and Willems, Shtarkov and Tjalkens [20], [21], that can be used to compute the prediction ^y t of the master algorithm. <p> HELMBOLD AND R.E. SCHAPIRE the empty string. Thus, the following lemma, which gives an efficient method of computing g, implies a method of computing sums of the form in equation (6). This lemma generalizes the proofs given for various special cases by Buntine <ref> [3] </ref>, Lemma 6.5.1 and Willems, Shtarkov and Tjalkens [21], Appendices III and IV. Lemma 1 Let g, g be as above.
Reference: 4. <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: The learner computes its predictions using predictions ~ t P that are generated in a natural way by each pruning P of the given unpruned tree T . We first show how an algorithm developed and analyzed by Cesa-Bianchi et al. <ref> [4] </ref> can be applied immediately to obtain a learning algorithm whose loss is bounded by a function that, for any pruning P, is linear in the prediction loss of P and the size of P (roughly, the number of nodes in the pruning). <p> For the moment, we assume that computation time is not a consideration. In this case, we can use the algorithm described by Cesa-Bianchi et al. <ref> [4] </ref>, which is an extension of Littlestone and Warmuth's randomized weighted majority algorithm [10], and is related to Vovk's aggregating strategies [16]. <p> Cesa-Bianchi et al. <ref> [4] </ref> give several suitable F fi functions. <p> We now show how Lemma 1 can be used to compute the ratio r t of equation (4) efficiently. This will allow us to efficiently simulate the master algorithm of Cesa-Bianchi et al. <ref> [4] </ref>. For any node u, we define the "weight" of u at time step t, written weight t (u), as u's contribution on the first t 1 time steps to the weight decrease of any tree P which contains u as a leaf. <p> We choose the initial weights to be PREDICTING NEARLY AS WELL AS THE BEST PRUNING 15 w 1 P;I = 2 jPj n jleaves (P)j : As before, applying the results of Cesa-Bianchi et al. <ref> [4] </ref> immediately gives us a bound on the loss of the resulting master algorithm.
Reference: 5. <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Computational Learning Theory: Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> pages 23-37. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Our method is easily extended to other loss functions provided that there exists a multiplicative weight-update algorithm of the appropriate form. For instance, such algorithms are given by Vovk [16], Kivinen and Warmuth [7], and Freund and Schapire <ref> [5] </ref>. Acknowledgments Thanks to Jason Catlett, William Cohen, Yoav Freund, Ron Kohavi, Jonathan Oliver, Alon Orlitsky, Dana Ron, Linda Sellie, Bruce Sherrod, Yoram Singer, Man-fred Warmuth, and Marcelo Weinberger for many helpful discussions.
Reference: 6. <author> Trevor Hastie and Daryl Pregibon. </author> <title> Shrinking trees. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1990. </year>
Reference-contexts: Various authors have presented techniques for averaging a family of decision trees <ref> [6] </ref>, [8], [11]. In particular, using a Bayesian formulation, Buntine [3], [2] gave a method called Bayesian smoothing for averaging the class-probability predictions of all possible prunings of a given decision tree.
Reference: 7. <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Using experts for predicting continuous outcomes. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> EuroCOLT '93, </volume> <pages> pages 109-120. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <title> PREDICTING NEARLY AS WELL AS THE BEST PRUNING 19 </title>
Reference-contexts: Our method is easily extended to other loss functions provided that there exists a multiplicative weight-update algorithm of the appropriate form. For instance, such algorithms are given by Vovk [16], Kivinen and Warmuth <ref> [7] </ref>, and Freund and Schapire [5]. Acknowledgments Thanks to Jason Catlett, William Cohen, Yoav Freund, Ron Kohavi, Jonathan Oliver, Alon Orlitsky, Dana Ron, Linda Sellie, Bruce Sherrod, Yoram Singer, Man-fred Warmuth, and Marcelo Weinberger for many helpful discussions.
Reference: 8. <author> Suk Wah Kwok and Chris Carter. </author> <title> Multiple decision trees. </title> <editor> In Ross D. Shachter, Tod S. Levitt, Laveen N. Kanal, and John F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pages 327-335. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: Various authors have presented techniques for averaging a family of decision trees [6], <ref> [8] </ref>, [11]. In particular, using a Bayesian formulation, Buntine [3], [2] gave a method called Bayesian smoothing for averaging the class-probability predictions of all possible prunings of a given decision tree.
Reference: 9. <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Thus, our procedure is guaranteed to be competitive (in terms of the quality of its predictions) with any pruning algorithm. 2 D.P. HELMBOLD AND R.E. SCHAPIRE Formally, we study this problem in the on-line learning framework introduced by Littlestone <ref> [9] </ref> and extended by Littlestone and Warmuth [10] and others. In this model, at each time step t = 1; : : : ; T , the learner receives an instance x t and must generate a prediction ^y t 2 [0; 1].
Reference: 10. <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Thus, our procedure is guaranteed to be competitive (in terms of the quality of its predictions) with any pruning algorithm. 2 D.P. HELMBOLD AND R.E. SCHAPIRE Formally, we study this problem in the on-line learning framework introduced by Littlestone [9] and extended by Littlestone and Warmuth <ref> [10] </ref> and others. In this model, at each time step t = 1; : : : ; T , the learner receives an instance x t and must generate a prediction ^y t 2 [0; 1]. <p> Their algorithm is closely related to work by Vovk [16] and Littlestone and Warmuth <ref> [10] </ref>. Note that this is a "worst-case" analysis in the sense that it does not rely on statistical assumptions of any kind regarding the source of the data that is being observed. Thus, the resulting algorithm is very robust. <p> The goal of our learning algorithm is to compete against the performance of the best, reasonably small such pruning by combining the predictions of all of the prunings. We study learning in the on-line prediction model used by Littlestone and Warmuth <ref> [10] </ref> and others. In this model, learning takes place in a sequence of trials t = 1; : : :; T . At each time step t, an instance x t is observed, and each pruning P generates a prediction ~ t P 2 [0; 1]. <p> For the moment, we assume that computation time is not a consideration. In this case, we can use the algorithm described by Cesa-Bianchi et al. [4], which is an extension of Littlestone and Warmuth's randomized weighted majority algorithm <ref> [10] </ref>, and is related to Vovk's aggregating strategies [16]. This algorithm was called P (fi) in Cesa-Bianchi et al.'s notation, but we refer to it simply as the "master algorithm." The algorithm maintains a weight w t P &gt; 0 for each pruning P.
Reference: 11. <author> Jonathan J. Oliver and David Hand. </author> <title> Averaging over decision stumps. </title> <booktitle> In Machine Learning: ECML-94, </booktitle> <pages> pages 231-241. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Various authors have presented techniques for averaging a family of decision trees [6], [8], <ref> [11] </ref>. In particular, using a Bayesian formulation, Buntine [3], [2] gave a method called Bayesian smoothing for averaging the class-probability predictions of all possible prunings of a given decision tree.
Reference: 12. <author> Jonathan J. Oliver and David J. </author> <title> Hand. On pruning and averaging decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 430-437, </pages> <year> 1995. </year>
Reference-contexts: A primary contribution of the current paper is the distillation of key elements of these previously known methods, and synthesis with other learning-theory results leading to broader learning applications. In independent work, Oliver and Hand <ref> [12] </ref> have experimented with averaging over different prunings of decision trees. Their results show that in some cases averaging outperforms the prunings generated by C4.5.
Reference: 13. <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: 1. Introduction Many algorithms for inferring a decision tree from data, such as C4.5 <ref> [13] </ref>, involve a two step process: In the first step, a very large decision tree is grown to match the data. If the training data contains noise then this large tree typically "over-fits" the data, giving quite poor performance on the test set.
Reference: 14. <author> Jorma Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-29(5):656-664, </volume> <month> September </month> <year> 1983. </year>
Reference-contexts: Learning results on such suffix trees were presented by Ron, Singer and Tishby [15]. Similar tree machines have been used to represent finite memory sources in the information theory community, and they form the core of Rissanen's Context algo rithm for universal data compression <ref> [14] </ref> (see also [17], [18], [19]). In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens [20], [21].
Reference: 15. <author> Dana Ron, Yoram Singer, and Naftali Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 35-46, </pages> <year> 1994. </year>
Reference-contexts: Such a pruning, in this case, is equivalent to a rule for determining one of several variable-length contexts, which in turn can be used to predict the next symbol. Learning results on such suffix trees were presented by Ron, Singer and Tishby <ref> [15] </ref>. Similar tree machines have been used to represent finite memory sources in the information theory community, and they form the core of Rissanen's Context algo rithm for universal data compression [14] (see also [17], [18], [19]). In work more 18 D.P. HELMBOLD AND R.E.
Reference: 16. <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: Their algorithm is closely related to work by Vovk <ref> [16] </ref> and Littlestone and Warmuth [10]. Note that this is a "worst-case" analysis in the sense that it does not rely on statistical assumptions of any kind regarding the source of the data that is being observed. Thus, the resulting algorithm is very robust. <p> For the moment, we assume that computation time is not a consideration. In this case, we can use the algorithm described by Cesa-Bianchi et al. [4], which is an extension of Littlestone and Warmuth's randomized weighted majority algorithm [10], and is related to Vovk's aggregating strategies <ref> [16] </ref>. This algorithm was called P (fi) in Cesa-Bianchi et al.'s notation, but we refer to it simply as the "master algorithm." The algorithm maintains a weight w t P &gt; 0 for each pruning P. <p> Our method is easily extended to other loss functions provided that there exists a multiplicative weight-update algorithm of the appropriate form. For instance, such algorithms are given by Vovk <ref> [16] </ref>, Kivinen and Warmuth [7], and Freund and Schapire [5]. Acknowledgments Thanks to Jason Catlett, William Cohen, Yoav Freund, Ron Kohavi, Jonathan Oliver, Alon Orlitsky, Dana Ron, Linda Sellie, Bruce Sherrod, Yoram Singer, Man-fred Warmuth, and Marcelo Weinberger for many helpful discussions.
Reference: 17. <author> M. J. Weinberger, A. Lempel, and J. Ziv. </author> <title> Universal coding of finite-memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(3) </volume> <pages> 1002-1014, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Learning results on such suffix trees were presented by Ron, Singer and Tishby [15]. Similar tree machines have been used to represent finite memory sources in the information theory community, and they form the core of Rissanen's Context algo rithm for universal data compression [14] (see also <ref> [17] </ref>, [18], [19]). In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens [20], [21].
Reference: 18. <author> Marcelo J. Weinberger, Neri Merhav, and Meir Feder. </author> <title> Optimal sequential probability assignment for individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40(2) </volume> <pages> 384-396, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Learning results on such suffix trees were presented by Ron, Singer and Tishby [15]. Similar tree machines have been used to represent finite memory sources in the information theory community, and they form the core of Rissanen's Context algo rithm for universal data compression [14] (see also [17], <ref> [18] </ref>, [19]). In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens [20], [21].
Reference: 19. <author> Marcelo J. Weinberger, Jorma J. Rissanen, and Meir Feder. </author> <title> A universal finite memory source. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 643-652, </pages> <year> 1995. </year>
Reference-contexts: Learning results on such suffix trees were presented by Ron, Singer and Tishby [15]. Similar tree machines have been used to represent finite memory sources in the information theory community, and they form the core of Rissanen's Context algo rithm for universal data compression [14] (see also [17], [18], <ref> [19] </ref>). In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens [20], [21].
Reference: 20. <author> F. M. J. Willems, Y. M. Shtarkov, and Tj. J. Tjalkens. </author> <title> Context tree weighting: a sequential universal source coding procedure for FSMX sources. </title> <booktitle> In Proceedings 1993 IEEE International Symposium on Information Theory, </booktitle> <pages> page 59, </pages> <year> 1993. </year>
Reference-contexts: Thus, the resulting algorithm is very robust. A naive implementation of this procedure would require computation time linear in the number of prunings of T ; obviously, this is infeasible. However, we show how techniques used by Buntine [3], [2] and Willems, Shtarkov and Tjalkens <ref> [20] </ref>, [21] can be generalized and applied to our setting, yielding a very efficient implementation requiring computation time at each trial t that is linear in the length of the path defined by the instance x t in the tree T (and therefore is bounded by the depth of T ). <p> Although our method is very similar to Buntine's, his is designed for use on a batch of examples, while ours uses efficient incremental updates of the data structure in an on-line setting. Willems, Shtarkov and Tjalkens <ref> [20] </ref>, [21] presented their technique in a much narrower context in which the decision trees considered were assumed to have a very particular form, and the goal was data compression rather than prediction. <p> Obviously, we cannot efficiently maintain all of the weights w t P explicitly since there are far too many prunings to consider. Instead, we use a more subtle data structure, similar to the ones used by Buntine [3], [2] and Willems, Shtarkov and Tjalkens <ref> [20] </ref>, [21], that can be used to compute the prediction ^y t of the master algorithm. The size of this data structure is proportional to the number of nodes in T (or, more accurately, to the number of nodes that have actually been visited). <p> In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens <ref> [20] </ref>, [21]. However, these authors focus on predicting a distribution of symbols for coding purposes, rather than simply predicting what the next symbol will be. Our method is easily extended to other loss functions provided that there exists a multiplicative weight-update algorithm of the appropriate form. <p> Acknowledgments Thanks to Jason Catlett, William Cohen, Yoav Freund, Ron Kohavi, Jonathan Oliver, Alon Orlitsky, Dana Ron, Linda Sellie, Bruce Sherrod, Yoram Singer, Man-fred Warmuth, and Marcelo Weinberger for many helpful discussions. Thanks also to Meier Feder for (indirectly) bringing references <ref> [20] </ref>, [21] to our attention, and to the anonymous reviewers for their careful reading and helpful comments. Notes 1. Actually, we only use the predictions of node u when u is a prefix of x t . 2.
Reference: 21. <author> Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. </author> <title> The context tree weighting method: basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <year> 1995. </year>
Reference-contexts: Thus, the resulting algorithm is very robust. A naive implementation of this procedure would require computation time linear in the number of prunings of T ; obviously, this is infeasible. However, we show how techniques used by Buntine [3], [2] and Willems, Shtarkov and Tjalkens [20], <ref> [21] </ref> can be generalized and applied to our setting, yielding a very efficient implementation requiring computation time at each trial t that is linear in the length of the path defined by the instance x t in the tree T (and therefore is bounded by the depth of T ). <p> Although our method is very similar to Buntine's, his is designed for use on a batch of examples, while ours uses efficient incremental updates of the data structure in an on-line setting. Willems, Shtarkov and Tjalkens [20], <ref> [21] </ref> presented their technique in a much narrower context in which the decision trees considered were assumed to have a very particular form, and the goal was data compression rather than prediction. <p> Obviously, we cannot efficiently maintain all of the weights w t P explicitly since there are far too many prunings to consider. Instead, we use a more subtle data structure, similar to the ones used by Buntine [3], [2] and Willems, Shtarkov and Tjalkens [20], <ref> [21] </ref>, that can be used to compute the prediction ^y t of the master algorithm. The size of this data structure is proportional to the number of nodes in T (or, more accurately, to the number of nodes that have actually been visited). <p> SCHAPIRE the empty string. Thus, the following lemma, which gives an efficient method of computing g, implies a method of computing sums of the form in equation (6). This lemma generalizes the proofs given for various special cases by Buntine [3], Lemma 6.5.1 and Willems, Shtarkov and Tjalkens <ref> [21] </ref>, Appendices III and IV. Lemma 1 Let g, g be as above. <p> In work more 18 D.P. HELMBOLD AND R.E. SCHAPIRE closely related to the results presented here, an efficient algorithm for averaging over prunings of such trees was presented by Willems, Shtarkov and Tjalkens [20], <ref> [21] </ref>. However, these authors focus on predicting a distribution of symbols for coding purposes, rather than simply predicting what the next symbol will be. Our method is easily extended to other loss functions provided that there exists a multiplicative weight-update algorithm of the appropriate form. <p> Acknowledgments Thanks to Jason Catlett, William Cohen, Yoav Freund, Ron Kohavi, Jonathan Oliver, Alon Orlitsky, Dana Ron, Linda Sellie, Bruce Sherrod, Yoram Singer, Man-fred Warmuth, and Marcelo Weinberger for many helpful discussions. Thanks also to Meier Feder for (indirectly) bringing references [20], <ref> [21] </ref> to our attention, and to the anonymous reviewers for their careful reading and helpful comments. Notes 1. Actually, we only use the predictions of node u when u is a prefix of x t . 2.
References-found: 21

