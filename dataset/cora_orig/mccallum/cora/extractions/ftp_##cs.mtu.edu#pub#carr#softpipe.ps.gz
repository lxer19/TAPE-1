URL: ftp://cs.mtu.edu/pub/carr/softpipe.ps.gz
Refering-URL: http://wwwipd.ira.uka.de/~hopp/seminar97.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: carr@cs.mtu.edu  chding@cs.mtu.edu  sweany@cs.mtu.edu  
Phone: Voice: (906) 487-2958 Fax: (906) 487-2933  Voice: (906) 487-3492 Fax: (906) 487-2933  Voice: (906) 487-3392 Fax: (906) 487-2933  
Title: Improving Software Pipelining With Unroll-and-Jam  
Author: Steve Carr Chen Ding Philip Sweany 
Note: This research is partially supported by NSF Grant CCR-9409341 and NSF Grant CCR-9308348.  
Date: May 30, 1995  
Address: Houghton MI 49931-1295  Houghton MI 49931-1295  Houghton MI 49931-1295  
Affiliation: Department of Computer Science Michigan Technological University  Department of Computer Science Michigan Technological University  Department of Computer Science Michigan Technological University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aiken, A., and Nicolau, A. </author> <title> Optimal loop parallelization. </title> <booktitle> In Conference on Programming Language Design and Implementation (Atlanta Georgia, </booktitle> <month> June </month> <year> 1988), </year> <booktitle> SIGPLAN '88, </booktitle> <pages> pp. 308-317. </pages>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method <ref> [1, 2] </ref> and Allan's petri-net pipelining technique [4]. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern.
Reference: [2] <author> Aiken, A., and Nicolau, A. </author> <title> Perfect Pipelining: A New Loop Optimization Technique. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming, Springer Verlag Lecture Notes in Computer Science, </booktitle> <address> #300 (Atlanta, GA, </address> <month> March </month> <year> 1988), </year> <pages> pp. 221-235. </pages>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method <ref> [1, 2] </ref> and Allan's petri-net pipelining technique [4]. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern.
Reference: [3] <author> Allan, V., Jones, R., Lee, R., and Allan, S. </author> <title> Software Pipelining. </title> <journal> ACM Computer Surveys. </journal> <note> to appear. </note>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [3, 15, 17, 19, 24] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> This single node is then scheduled along with all other nodes in the surrounding control construct. This hierarchical reduction continues until a single node represents the entire loop. Allan et al. <ref> [3] </ref> divide software pipelining techniques into two general categories called kernel recognition methods and modulo scheduling methods. In the kernel recognition technique, a loop is unrolled an "appropriate" number of times, yielding a representation for N loops bodies which is then scheduled. <p> Lam's hierarchical reduction is a modulo scheduling method as is Warter's [23, 24] enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. Rau [17] provides a detailed discussion of an implementation of modulo scheduling, while Allan et al. <ref> [3] </ref> provide a thorough survey of software pipelining methods. 2.3 Modulo Scheduling Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling [23], uses a modulo scheduling algorithm, we shall investigate modulo scheduling in a bit more detail. <p> Remember that this postlude and prelude are required to initialize and then drain the "software pipe." To visualize how this works, consider the following abstract notation for a loop, 3 3 This notation is a slightly modified version of one used by Allan et al. <ref> [3] </ref>. 9 L = fABCDg n indicating that the loop will execute n times and includes operations A-D. This notation is not meant to suggest any particular ordering on A-D, merely to suggest that these "operations" are included in the schedule for the loop.
Reference: [4] <author> Allan, V., Rajagopalan, M., and Lee, R. </author> <title> Software Pipelining: Petri Net Pacemaker. In Working Conference on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism (Orlando, </title> <address> FL, </address> <month> January 20-22 </month> <year> 1993). </year>
Reference-contexts: After scheduling the N copies of the loop, some pattern recognition technique is used to identify a repeating kernel within the schedule. Examples of kernel recognition methods are Aiken and Nicolau's perfect pipelining method [1, 2] and Allan's petri-net pipelining technique <ref> [4] </ref>. In contrast to kernel recognition methods, modulo scheduling does not schedule multiple iterations of a loop and then look for a pattern. Instead, modulo scheduling selects a schedule for one iteration of the loop such that, when that schedule is repeated, no resource or dependence constraints are violated.
Reference: [5] <author> Callahan, D., Carr, S., and Kennedy, K. </author> <title> Improving register allocation for subscripted variables. </title> <journal> SIGPLAN Notices 25, </journal> <month> 6 (June </month> <year> 1990), </year> <pages> 53-65. </pages> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation. </booktitle> <pages> 19 </pages>
Reference-contexts: In our matrix multiply example in Figure 1, the original loop has a balance of 1 because c (i,j) can be allocated to a register using scalar replacement <ref> [5, 10] </ref>.
Reference: [6] <author> Callahan, D., Cocke, J., and Kennedy, K. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing 5 (1988), </journal> <pages> 334-358. </pages>
Reference-contexts: Parallelism is normally inhibited by either inner-loop recurrences, or by a mismatch between the resource requirements of a loop and the resources provide by the the target architecture. Unroll-and-jam is a transformation that can be used to increase the parallelism in the innermost loop body <ref> [6, 9] </ref>. Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies 2 of inner-loop recurrences that are parallel with all other copies [6]. <p> Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies 2 of inner-loop recurrences that are parallel with all other copies <ref> [6] </ref>. These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. <p> prelude and postlude code to initialize and drain, respectively, the software pipeline. * Perform loop conditioning to counter the side effects of modulo variable expansion. 3 Unroll-and-Jam Callahan,et al., have shown that unrolling inner loops does not help in the presence of recurrences nor does it help with resource problems <ref> [6] </ref>. However, unroll-and-jam is a transformation that can be used to improve the ILP available to a software pipelining algorithm [6, 9]. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more parallelism into an innermost loop body. <p> However, unroll-and-jam is a transformation that can be used to improve the ILP available to a software pipelining algorithm <ref> [6, 9] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more parallelism into an innermost loop body. <p> In particular, we assume that it performs strength reduction, optimizes for machine addressing modes, allocates registers globally (via a coloring scheme) and schedules the pipelines. To measure the performance of program loops given the above assumptions, we use the notion of balance defined by Callahan, et al. <ref> [6] </ref>. 12 3.1.1 Machine Balance A computer is balanced when it can operate in a steady state manner with both memory accesses and floating-point operations being performed at peak speed.
Reference: [7] <author> Callahan, D., Cooper, K., Hood, R., Kennedy, K., and Torczon, L. </author> <title> ParaScope: A parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing (Athens, </booktitle> <address> Greece, </address> <month> June </month> <year> 1987). </year>
Reference-contexts: These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria [8] that is based upon the ParaScope programming environment <ref> [7] </ref>. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallelism) architectures, called Rocket [21].
Reference: [8] <author> Carr, S. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: These two effects increase the parallelism available to a software pipelining algorithm. This paper measures the effectiveness of unroll-and-jam at improving the initiation interval for software-pipelined loops. In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria <ref> [8] </ref> that is based upon the ParaScope programming environment [7]. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallelism) architectures, called Rocket [21].
Reference: [9] <author> Carr, S., and Kennedy, K. </author> <title> Improving the ratio of memory operations to floating-point operations in loops. </title> <journal> ACM Transactions on Programming Languages and Systems 16, </journal> <volume> 6 (1994), </volume> <pages> 1768-1810. </pages>
Reference-contexts: Parallelism is normally inhibited by either inner-loop recurrences, or by a mismatch between the resource requirements of a loop and the resources provide by the the target architecture. Unroll-and-jam is a transformation that can be used to increase the parallelism in the innermost loop body <ref> [6, 9] </ref>. Unroll-and-jam can reduce the number of memory operations that need to be issued per floating-point operation in order to alleviate resource constraint problems. In addition, unroll-and-jam creates copies 2 of inner-loop recurrences that are parallel with all other copies [6]. <p> However, unroll-and-jam is a transformation that can be used to improve the ILP available to a software pipelining algorithm <ref> [6, 9] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more parallelism into an innermost loop body. <p> This is because there is a better match between the resource demands of the loop and the resources provided by the machine. 3.1.3 Applying Unroll-and-Jam In A Compiler Previous work has used the following optimization problem to guide unroll-and-jam <ref> [9] </ref>: objective function: min jfi L fi M j 0 constraint: R L R M where R L is the number of register required by a loop, R M is the register-set size of the target architecture, and jfi L fi M j 0 is the balance norm having the following <p> The solution to the objective function for a particular loop nest can be optimized in O (log R M ) steps when unroll-and-jam is applied to one loop and O (R M ) steps when unroll-and-jam is applied to two loops <ref> [9] </ref>. After unroll-and-jam guided by the above objective function, a loop may still contain pipeline interlock, leaving idle computational cycles. <p> We expect this heuristic to be needed rarely as unrolling for loop balance will likely remove interlock. In previous work, Carr and Kennedy show that unroll-and-jam guided by the previous optimization formula improves the performance of the hardware pipelining on the IBM RS/6000 <ref> [9] </ref>. Their experiment presents execution-time improvements to validate their claims.
Reference: [10] <author> Carr, S., and Kennedy, K. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <journal> Software Practice and Experience 24, </journal> <month> 1 (Jan. </month> <year> 1994), </year> <pages> 51-77. </pages>
Reference-contexts: In our matrix multiply example in Figure 1, the original loop has a balance of 1 because c (i,j) can be allocated to a register using scalar replacement <ref> [5, 10] </ref>.
Reference: [11] <author> Charlesworth, A. </author> <title> An approach to scientific array processing: The architectural design of the AP 120B/FPS 164 family. </title> <booktitle> Computer (Sept. </booktitle> <year> 1981), </year> <pages> 18-27. </pages>
Reference-contexts: POSTLUDE (INNERMOST LOOP): 1. nop POSTLUDE (MIDDLE LOOP): 1. nop 3. nop 5. nop 7. nop 2.2 Software Pipelining Methods Software pipelining of loops was first reported by Charlesworth <ref> [11] </ref> who described an algorithm used in generating hand-written assembly code for the Floating Point System AP-120B family of array processors. 1 Actually, software pipelining could improve upon this by overlapping two iterations of the loop within the 2-instruction loop body, yielding a scheduling requiring only 96 cycles.
Reference: [12] <author> Goff, G., Kennedy, K., and Tseng, C.-W. </author> <title> Practical dependence testing. </title> <journal> SIGPLAN Notices 26, </journal> <month> 6 (June </month> <year> 1991), </year> <pages> 15-29. </pages> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Since we intend to overlap different loop iterations we need to consider loop-carried dependence as well as loop-independent dependence, but well-known algorithms provide this information <ref> [14, 12] </ref>. Once the DDG is constructed for the loop, modulo scheduling attempts to identify the smallest number of instructions which might separate different loop iterations. This minimum initiation interval (II min ) represents the shortest time interval between the initiation of consecutive loop iterations.
Reference: [13] <author> Kennedy, K., and McKinley, K. </author> <title> Optimizing for parallelism and memory hierarchy. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing (Washington, </booktitle> <address> DC, </address> <month> July </month> <year> 1992), </year> <pages> pp. 323-334. </pages>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 4 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [13, 16, 25] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
Reference: [14] <author> Kuck, D. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Since we intend to overlap different loop iterations we need to consider loop-carried dependence as well as loop-independent dependence, but well-known algorithms provide this information <ref> [14, 12] </ref>. Once the DDG is constructed for the loop, modulo scheduling attempts to identify the smallest number of instructions which might separate different loop iterations. This minimum initiation interval (II min ) represents the shortest time interval between the initiation of consecutive loop iterations.
Reference: [15] <author> Lam, M. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <journal> SIGPLAN Notices 23, </journal> <month> 7 (July </month> <year> 1988), </year> <pages> 318-328. </pages> <booktitle> Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [3, 15, 17, 19, 24] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> Neither algorithm can pipeline loops containing conditional statements. More recent software pipelining efforts have removed the restriction that only single-block loops can be pipelined. With GURPR [19], Su et al. updated the URPR algorithm to consider arbitrary control flow. Lam <ref> [15] </ref> defines a method she calls hierarchical reduction which pipelines loops with any block-structured control flow. Hierarchical reduction schedules the blocks of an innermost loop, starting with the innermost control constructs. <p> For this paper, however, we compute the value of RecII (and thus II min ) for comparison purposes. After finding a schedule for the loop body requiring II instructions, it may be necessary to perform modulo variable expansion <ref> [15] </ref> to circumvent inter-interval dependences which can occur due to register reuse. To overcome such inter-interval dependences, the loop body schedule may need to be copied M times, where M is the number of different loop iterations represented within the loop body schedule.
Reference: [16] <author> Mowry, T. C., Lam, M. S., and Gupta, A. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (Boston, </booktitle> <address> Massachusetts, </address> <year> 1992), </year> <pages> pp. 62-75. </pages>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 4 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [13, 16, 25] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
Reference: [17] <author> Rau, B. R. </author> <title> Iterative modulo scheduling: An algorithm for software pipelining loops. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture (MICRO-27) (San Jose, </booktitle> <address> CA, </address> <month> December </month> <year> 1994), </year> <pages> pp. 63-74. </pages>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [3, 15, 17, 19, 24] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> Lam's hierarchical reduction is a modulo scheduling method as is Warter's [23, 24] enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. Rau <ref> [17] </ref> provides a detailed discussion of an implementation of modulo scheduling, while Allan et al. [3] provide a thorough survey of software pipelining methods. 2.3 Modulo Scheduling Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling [23], uses a modulo scheduling algorithm, we shall investigate modulo scheduling in a bit <p> While Warter's method provides a general framework for our software pipelining, the actual modulo scheduling technique implemented in Rocket closely follows Rau <ref> [17] </ref>. Modulo scheduling assumes that a single data-dependence graph (DDG) can be built for a loop. To build a single DDG for a loop requires some method of treating the entire loop as a single basic block. <p> Following Rau <ref> [17] </ref>, to find RecII we iterate on potential values for RecII, building a matrix, MinDist, for each possible II value. M inDist [i; j] is defined to be the minimum interval between loop operations i 7 and j which maintains data dependence integrity.
Reference: [18] <author> Su, B., Ding, S., and Jin, L. </author> <title> An improvement of trace scheduling for global microcode compaction. </title> <booktitle> In Proceedings of the 17th Microprogramming Workshop (MICRO-17) (New Orleans, </booktitle> <address> LA, </address> <month> Nov </month> <year> 1984), </year> <pages> pp. 78-85. </pages>
Reference-contexts: Touzeau [22] restricts software pipelining to loops of a single Fortran statement. Su et al. defines two algorithms, each with different restrictions: URCR <ref> [18] </ref> limits the number of loop iterations which can be overlapped to two; URPR [20] does not restrict the number of overlapped iterations in the loop but insists that the number of loop iterations be known at compile time.
Reference: [19] <author> Su, B., Ding, S., Wang, J., and Xia, J. </author> <title> GURPR A Method for Global Software Pipelining. </title> <booktitle> In Proceedings of the 20th Microprogramming Workshop (MICRO-20) (Colorado Springs, </booktitle> <publisher> CO, </publisher> <month> December </month> <year> 1987), </year> <pages> pp. 97-105. </pages>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [3, 15, 17, 19, 24] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> In addition, both URCR and URPR apply only to loops consisting of a single basic block. Neither algorithm can pipeline loops containing conditional statements. More recent software pipelining efforts have removed the restriction that only single-block loops can be pipelined. With GURPR <ref> [19] </ref>, Su et al. updated the URPR algorithm to consider arbitrary control flow. Lam [15] defines a method she calls hierarchical reduction which pipelines loops with any block-structured control flow. Hierarchical reduction schedules the blocks of an innermost loop, starting with the innermost control constructs.
Reference: [20] <author> Su, B., Ding, S., and Xia, J. </author> <title> URPR an extension of URCR for software pipelining. </title> <booktitle> In Proceedings of the 19th Microprogramming Workshop (MICRO-19) (New York, </booktitle> <address> NY, </address> <month> December </month> <year> 1986), </year> <pages> pp. 94-103. </pages>
Reference-contexts: Touzeau [22] restricts software pipelining to loops of a single Fortran statement. Su et al. defines two algorithms, each with different restrictions: URCR [18] limits the number of loop iterations which can be overlapped to two; URPR <ref> [20] </ref> does not restrict the number of overlapped iterations in the loop but insists that the number of loop iterations be known at compile time. In addition, both URCR and URPR apply only to loops consisting of a single basic block. Neither algorithm can pipeline loops containing conditional statements.
Reference: [21] <author> Sweany, P. H., and Beaty, S. J. </author> <title> Overview of the Rocket retargetable C compiler. </title> <type> Tech. Rep. </type> <institution> CS-94-01, Department of Computer Science, Michigan Technological University, Houghton, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: In our experiments, unroll-and-jam is performed by a Fortran source-to-source transformer called Memoria [8] that is based upon the ParaScope programming environment [7]. Software pipelining is performed by a retargetable compiler for ILP (Instruction-Level Parallelism) architectures, called Rocket <ref> [21] </ref>. We present experimental evidence that suggests that Memoria can be quite effective at improving the initiation interval generated by Rocket. 2 Software Pipelining While local and global instruction scheduling can, together, exploit considerable parallelism for non-loop code, to best exploit instruction-level parallelism within loops requires software pipelining.
Reference: [22] <author> Touzeau, R. </author> <title> A FORTRAN compiler for the FPS-164 scientific computer. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Compiler Construction (1984), </booktitle> <pages> pp. 48-57. 20 </pages>
Reference-contexts: We chose the presented schedule to simplify the discussion. 2 Again, a better schedule is possible, one requiring 84 cycles. 5 Early software pipelining generally dealt with the inherent complexity of finding an optimal schedule by limiting the program constructs on which software pipelining is utilized. Touzeau <ref> [22] </ref> restricts software pipelining to loops of a single Fortran statement.
Reference: [23] <author> Warter, N., Haab, G., and Bockhaus, J. </author> <title> Enhanced Modulo Scheduling for Loops with Conditional Branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture (MICRO-25) (Portland, </booktitle> <address> OR, </address> <month> December 1-4 </month> <year> 1992), </year> <pages> pp. 170-179. </pages>
Reference-contexts: Once that minimum initiation interval is determined, instruction scheduling attempts to match that minimum schedule while respecting resource and 6 dependence constraints. Lam's hierarchical reduction is a modulo scheduling method as is Warter's <ref> [23, 24] </ref> enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop. <p> Rau [17] provides a detailed discussion of an implementation of modulo scheduling, while Allan et al. [3] provide a thorough survey of software pipelining methods. 2.3 Modulo Scheduling Since Rocket's software pipelining, patterned after Warter's enhanced modulo scheduling <ref> [23] </ref>, uses a modulo scheduling algorithm, we shall investigate modulo scheduling in a bit more detail. While Warter's method provides a general framework for our software pipelining, the actual modulo scheduling technique implemented in Rocket closely follows Rau [17].
Reference: [24] <author> Warter, N. J., Mahlke, S. A., mei W. Hwu, W., and Rau, B. R. </author> <title> Reverse if-conversion. </title> <journal> SIGPLAN Notices 28, </journal> <month> 6 (June </month> <year> 1993), </year> <pages> 290-299. </pages> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: As a result of these improvements, today's microprocessors can perform more operations per machine cycle than their predecessors. To take advantage of these architectural improvements, advanced compiler optimizations such as software pipelining have been developed <ref> [3, 15, 17, 19, 24] </ref>. Software pipelining allows iterations of a loop to be overlapped with one another in order to take advantage of the maximum parallelism in a loop body. <p> Once that minimum initiation interval is determined, instruction scheduling attempts to match that minimum schedule while respecting resource and 6 dependence constraints. Lam's hierarchical reduction is a modulo scheduling method as is Warter's <ref> [23, 24] </ref> enhanced modulo scheduling which uses IF-conversion to produce a single super-block to represent a loop.
Reference: [25] <author> Wolf, M. E., and Lam, M. S. </author> <title> A data locality optimizing algorithm. </title> <journal> SIGPLAN Notices 26, </journal> <month> 6 (June </month> <year> 1991), </year> <pages> 30-44. </pages> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation. </booktitle> <pages> 21 </pages>
Reference-contexts: L = M L where M L is the number of memory operations in loop L and F L is the number of floating-point operations in L. 4 This model assigns a uniform cost to memory references under the assumption that compiler optimizations can be performed to attain cache locality <ref> [13, 16, 25] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture.
References-found: 25

