URL: http://www.cs.rochester.edu/u/sandhya/papers/cabm95.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Email: skgupta@pdos.lcs.mit.edu schaffer@cs.rice.edu alc@cs.rice.edu sandhya@cs.rice.edu -willy@cs.rice.edu  
Phone: Phone: (713) 527-8101 x3813 FAX: (713) 285-5930  
Title: Integrating Parallelization Strategies for Linkage Analysis  
Author: Sandeep K. Gupta Alejandro A. Schaffer Alan L. Cox Sandhya Dwarkadas Willy Zwaenepoel 
Note: Address for correspondence: Alejandro A. Schaffer  Present address:  
Address: Houston  Houston  Houston  Houston  Houston  6100 Main, Houston, TX 77005-1892.  Technology Square, Cambridge, MA 02139;  
Affiliation: Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Computer Science Rice University  Department of Computer Science, MS132, Rice University,  Laboratory for Computer Science, Massachusetts Institute of Technology, 545  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. H. Blanton, J. R. Heckenlively, A. W. Cottingham, J. Friedman, L. A. Sadler, M. Wagner, L. H. Friedman, and S. P. Daiger. </author> <title> Linkage mapping of autosomal 25 dominant retinitis pigmentosa (RP1) to the pericentric region of human chro-mosome 8. </title> <journal> Genomics, </journal> <volume> 11 </volume> <pages> 857-869, </pages> <year> 1991. </year>
Reference-contexts: Stephen P. Daiger at the University of Texas Health Science Center at Houston. As shown in <ref> [1] </ref>, this pedigree had to be split into three pieces because computation on the whole family together was prohibitively long.
Reference: [2] <author> Roger D. Chamberlain, Mark A. Franklin, Gregory D. Peterson, and Michael A. Province. </author> <title> Genetic epidemiology, parallel algorithms, and workstation networks. </title> <type> Technical Report WUCCRC-94-14, </type> <institution> Washington University, </institution> <year> 1994. </year>
Reference-contexts: His paper does not describe any implementation. We experimentally implemented an algorithm similar to Schork's, but found that it slowed down ILINK measurably. Some of the problems we encountered are mentioned briefly in the Discussion at the end of of the paper. Chamberlain, Franklin, Peterson, and Province <ref> [2] </ref> parallelized the gradient computation in the GEMINI optimization package using PVM in the context of two other application programs in genetics. <p> ILINK optimization problems typically have dimension only 2 or 3. We note also that <ref> [2] </ref> did not parallelize those function evaluations that update the parameters, which are therefore done one at a time by one processor each. They mention that it is important to integrate parallelization strategies for individual function evaluations with a strategy to parallelize the gradient estimation. <p> It is not unusual for sequential ILINK runs to take weeks; because of the need to run many different tests, we picked ILINK instances of moderate size to use in our timing experiments. Because ILINK problems are low-dimensional (unlike the applications of GEMINI considered in <ref> [2] </ref>), we often have more processors available than likelihood evaluations to do. To balance the load, we want the same number of processors to work on each likelihood function evaluation. To assign the function evaluations to processor sets, we use the following greedy algorithm.
Reference: [3] <author> R. W. Cottingham Jr., R. M. Idury, and A. A. Schaffer. </author> <title> Faster sequential genetic linkage computations. </title> <journal> American Journal of Human Genetics, </journal> <volume> 53 </volume> <pages> 252-263, </pages> <year> 1993. </year>
Reference-contexts: In this paper we report on a multi-level attempt to parallelize the ILINK program from the LINKAGE package [15, 13, 16], the most widely-used software package for linkage analysis. This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs <ref> [3, 21] </ref> in LINKAGE and parallelized one of them (ILINK) [5]. There are several different levels at which linkage computations may be paral-lelized. In practice, linkage analysis computations usually consist of several different runs corresponding to different orders of possible genes and/or different candidate recombination fraction vectors. <p> A major advance of the LINKAGE package over its predecessor LIPED [19] is that LINKAGE supports multilocus analysis [15]. The LINKAGE package contains four related principal programs, LODSCORE, ILINK, LINKMAP, and MLINK. The FASTLINK package <ref> [3, 21] </ref> contains significantly faster sequential versions of these four programs. FASTLINK does not include the many auxiliary programs for preprocessing the data and postprocessing the results that come with LINKAGE.
Reference: [4] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <year> 1993. </year>
Reference-contexts: In essence, TreadMarks provides a shared memory abstraction to the programmer, and implements this abstraction efficiently using the underlying message passing system <ref> [8, 4] </ref>. The programmer writes the parallel program as if it were intended for a shared-memory multiprocessor, but the TreadMarks system enables the program to run on a network multicomputer. Since we did the performance measurements on our first parallel ILINK, Tread-Marks has been improved in various ways.
Reference: [5] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs [3, 21] in LINKAGE and parallelized one of them (ILINK) <ref> [5] </ref>. There are several different levels at which linkage computations may be paral-lelized. In practice, linkage analysis computations usually consist of several different runs corresponding to different orders of possible genes and/or different candidate recombination fraction vectors. Also, any given run may involve likelihood computations for multiple pedigrees. <p> However, after simple syntactic changes, it is also possible to run our parallel implementation on a shared-memory multiprocessor computer. The shared-memory programming model supported by TreadMarks is easier to use than message passing. We tried our parallel implementation on 5 data sets, 3 of which were used in <ref> [5] </ref>. We used an 8-processor network with either a 100Mb/s ATM (Asynchronous Transfer Mode) switch, or 10Mb/s Ethernet. Using the faster ATM network, speedup on the 3 common data sets improved from 5.38, 3.15, and 5.73 to 6.04, 3.91, and 6.33 respectively. <p> In either case, LINKMAP generates a fixed, and computable a priori, set of candidate vectors and computes the likelihood for each. The routines to compute the likelihood for any particular are essentially the same in ILINK and LINKMAP (as well as LODSCORE and MLINK). As in <ref> [5] </ref>, we focus on parallelizing ILINK because it has the longest typical running times and is the hardest of the four programs to parallelize. Most of the routines that we have modified are shared between all four programs in the sequential FASTLINK code. <p> In contrast, a shared-memory multiprocessor is a single machine containing several processors that are connected by a specially-designed bus or dedicated network. Some tradeoffs between these two types of parallel computers are discussed in the Methods section of <ref> [5] </ref>. In a network multicomputer, processors communicate by passing messages with send and receive operations, while a shared-memory multiprocessor supports communication by reading and writing globally accessible memory. Most sequential programs, including ILINK, are more easily parallelized by writing code in terms of shared memory. <p> Uniprocessor execution times are given as well so that execution time differences may be inferred. We use two different network types the commonly available Ethernet networks and the emerging ATM networks. For consistency we use exactly the same three disease data sets and one sample run each from <ref> [5] </ref>. We also use one sample run each from two large data sets that we obtained subsequent to the publication of [5]. <p> For consistency we use exactly the same three disease data sets and one sample run each from <ref> [5] </ref>. We also use one sample run each from two large data sets that we obtained subsequent to the publication of [5]. The new data sets appear to be more representative of the kinds of data sets that cause users of sequential ILINK to want faster alternatives. * RP01: data on a large family, UCLA-RP01, with autosomal dominant retini-tis pigmentosa (RP1) from the laboratory of Dr. Stephen P. <p> The families have no loops. 13 RP01-3 BAD CLP ADNIV LGMD 4682 833 4085 9570 13011 Table 1: Uniprocessor execution times in seconds on a DECStation-5000/240 More detailed descriptions of the first three sets of pedigrees are given in <ref> [5] </ref> and diagrams can be found in the papers cited for each data set. <p> Comparing the version with no changes and the version with both changes on 8 processors, the ATM network speedups improved from (5.52,3.43,5.96,6.69,6.45) to (6.04,3.91,6.33,7.02,6.88) on the five data sets respectively, The speedups on the first three data sets for our first implementation <ref> [5] </ref> were (5.38,3.15,5.73) respectively. Comparing the version with no changes and the version with both changes on 8 processors the Ethernet network speedups improved from (3.97,2.28,5.32,5.09,5.40) to (4.86,2.80,5.71,6.30,6.34). The speedups on the first three data sets for our first implementation [5] were (3.82, 1.86, and 5.09) respectively. <p> on the first three data sets for our first implementation <ref> [5] </ref> were (5.38,3.15,5.73) respectively. Comparing the version with no changes and the version with both changes on 8 processors the Ethernet network speedups improved from (3.97,2.28,5.32,5.09,5.40) to (4.86,2.80,5.71,6.30,6.34). The speedups on the first three data sets for our first implementation [5] were (3.82, 1.86, and 5.09) respectively. The amount of improvement caused by adaptive loadbalancing and parallel gradient evaluation vary widely from data set to data set.
Reference: [6] <author> T. M. Goradia, K. Lange, P. L. Miller, and P. M. Nadkarni. </author> <title> Fast computation of genetic likelihoods on human pedigree data. </title> <booktitle> Human Heredity, </booktitle> <volume> 42 </volume> <pages> 42-62, </pages> <year> 1992. </year>
Reference-contexts: They treat the evaluation of each likelihood for one pedigree as a separate task that can be assigned to one processor. If there are enough tasks, the load can be balanced effectively using a work-queue approach. Goradia, et al <ref> [6] </ref> did a similar parallelization of the linkage analysis program MENDEL [11, 12]. Vaughan [25] did a parallel implementation of LINKMAP using the ISIS package. Her algorithm does parallelize the case of one pedigree and one likelihood, but she did not present any data on running times.
Reference: [7] <author> J. T. Hecht, Y. Wang, B. Connor, S. H. Blanton, and S. P. Daiger. Non-syndromic cleft lip and palate: </author> <title> No evidence of linkage to hla or factor 13a. </title> <journal> American Journal of Human Genetics, </journal> <volume> 52 </volume> <pages> 1230-1233, </pages> <year> 1993. </year>
Reference-contexts: David R. Cox and Richard M. Myers at the University of California at San Francisco [17]. * CLP: Data on 12 families with autosomal dominant nonsyndromic cleft lip and palate (CLP) from the laboratory of Dr. Jacqueline T. Hecht at the University of Texas Health Science Center at Houston <ref> [7] </ref>. * ADNIV: Data on 1 large family with autosomal dominant neovascular inflammatory vitroretinopathy (ADNIV) provided by Drs. Ed Stone and Brian Nichols from the University of Iowa [24]. The family has 93 individuals, of whom 37 have unknown genotypes at all non-disease loci, and 9 have some unknown genotypes.
Reference: [8] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In essence, TreadMarks provides a shared memory abstraction to the programmer, and implements this abstraction efficiently using the underlying message passing system <ref> [8, 4] </ref>. The programmer writes the parallel program as if it were intended for a shared-memory multiprocessor, but the TreadMarks system enables the program to run on a network multicomputer. Since we did the performance measurements on our first parallel ILINK, Tread-Marks has been improved in various ways.
Reference: [9] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proc. 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-131, </pages> <year> 1994. </year>
Reference-contexts: Both of our parallel implementations are written in a shared memory programming style using the TreadMarks distributed shared memory system <ref> [9] </ref>, which is under development at Rice University. TreadMarks is a runtime library that enables a shared memory program to run on a network of workstations. Networks of workstations are quite common in research institutions and are more readily available than shared-memory multiprocessors. <p> To enable programmers to use networks of computers without writing message-passing programs we have developed a software distributed shared memory (DSM) system for network multicomputers called TreadMarks <ref> [9] </ref>. In essence, TreadMarks provides a shared memory abstraction to the programmer, and implements this abstraction efficiently using the underlying message passing system [8, 4].
Reference: [10] <author> J. M. Lalouel. </author> <title> GEMINI a computer program for optimization of general nonlinear functions. </title> <type> Technical Report 14, </type> <institution> University of Utah, Department of Medical Biophysics and Computing, </institution> <address> Salt Lake City, Utah, </address> <year> 1979. </year>
Reference-contexts: For our purposes there is one fundamental distinction between ILINK and LINKMAP. ILINK starts with one candidate vector and improves through a numerical optimization algorithm implemented in the GEMINI package <ref> [10] </ref>; all of the components in the vector can change from one estimate to the next.
Reference: [11] <author> K. Lange, D. Weeks, and M. Boehnke. </author> <title> Programs for pedigree analysis: MENDEL, FISHER, </title> <booktitle> and dGene. Genetic Epidemiology, </booktitle> <volume> 5 </volume> <pages> 471-473, </pages> <year> 1988. </year>
Reference-contexts: If there are enough tasks, the load can be balanced effectively using a work-queue approach. Goradia, et al [6] did a similar parallelization of the linkage analysis program MENDEL <ref> [11, 12] </ref>. Vaughan [25] did a parallel implementation of LINKMAP using the ISIS package. Her algorithm does parallelize the case of one pedigree and one likelihood, but she did not present any data on running times. She was primarily concerned with balancing the load on a heterogeneous network.
Reference: [12] <author> K. Lange and D. E. Weeks. </author> <title> Efficient computation of lod scores genotype elimination, genotype redefinition, and hybrid maximum likelihood algorithms. </title> <journal> Annals of Human Genetics, </journal> <volume> 53 </volume> <pages> 67-83, </pages> <year> 1989. </year> <month> 26 </month>
Reference-contexts: If there are enough tasks, the load can be balanced effectively using a work-queue approach. Goradia, et al [6] did a similar parallelization of the linkage analysis program MENDEL <ref> [11, 12] </ref>. Vaughan [25] did a parallel implementation of LINKMAP using the ISIS package. Her algorithm does parallelize the case of one pedigree and one likelihood, but she did not present any data on running times. She was primarily concerned with balancing the load on a heterogeneous network.
Reference: [13] <author> G. M. Lathrop and J. M. Lalouel. </author> <title> Easy calculations of lod scores and genetic risks on small computers. </title> <journal> American Journal of Human Genetics, </journal> <volume> 36 </volume> <pages> 460-465, </pages> <year> 1984. </year>
Reference-contexts: As data collection methods have improved, the size and complexity of linkage analysis problems have grown much faster than the speed of readily available computers. In this paper we report on a multi-level attempt to parallelize the ILINK program from the LINKAGE package <ref> [15, 13, 16] </ref>, the most widely-used software package for linkage analysis. This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs [3, 21] in LINKAGE and parallelized one of them (ILINK) [5].
Reference: [14] <author> G. M. Lathrop and J. M. Lalouel. </author> <title> Efficient computations in multilocus linkage analysis. </title> <journal> American Journal of Human Genetics, </journal> <volume> 42 </volume> <pages> 498-505, </pages> <year> 1988. </year>
Reference-contexts: Central differences are more time-consuming than forward differences because they require twice as many likelihood function evaluations. The merits of both methods in LINKAGE are discussed and analyzed experimentally in <ref> [14] </ref>. The currently distributed versions of ILINK in both LINKAGE and FASTLINK start using forward differences and switch to central differences if successive values of appear sufficiently similar. As suggested by Miller et al. [18] in the context of LINKMAP, performing likelihood function evaluations in parallel is a good strategy.
Reference: [15] <author> G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for multilocus linkage analysis in humans. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 81 </month> <pages> 3443-3446, </pages> <year> 1984. </year>
Reference-contexts: As data collection methods have improved, the size and complexity of linkage analysis problems have grown much faster than the speed of readily available computers. In this paper we report on a multi-level attempt to parallelize the ILINK program from the LINKAGE package <ref> [15, 13, 16] </ref>, the most widely-used software package for linkage analysis. This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs [3, 21] in LINKAGE and parallelized one of them (ILINK) [5]. <p> The task of estimating the vector for more than two loci, is called multilocus analysis. A major advance of the LINKAGE package over its predecessor LIPED [19] is that LINKAGE supports multilocus analysis <ref> [15] </ref>. The LINKAGE package contains four related principal programs, LODSCORE, ILINK, LINKMAP, and MLINK. The FASTLINK package [3, 21] contains significantly faster sequential versions of these four programs. FASTLINK does not include the many auxiliary programs for preprocessing the data and postprocessing the results that come with LINKAGE.
Reference: [16] <author> G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. </author> <title> Multilocus linkage analysis in humans: detection of linkage and estimation of recombination. </title> <journal> American Journal of Human Genetics, </journal> <volume> 37 </volume> <pages> 482-498, </pages> <year> 1985. </year>
Reference-contexts: As data collection methods have improved, the size and complexity of linkage analysis problems have grown much faster than the speed of readily available computers. In this paper we report on a multi-level attempt to parallelize the ILINK program from the LINKAGE package <ref> [15, 13, 16] </ref>, the most widely-used software package for linkage analysis. This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs [3, 21] in LINKAGE and parallelized one of them (ILINK) [5].
Reference: [17] <author> A. Law, C. W. Richard III, R. W. Cottingham Jr ., G. M. Lathrop, D. R. Cox, and R. M. Myers. </author> <title> Genetic linkage analysis of bipolar affective disorder in an old order amish pedigree. </title> <journal> Human Genetics, </journal> <volume> 88 </volume> <pages> 562-568, </pages> <year> 1992. </year>
Reference-contexts: David R. Cox and Richard M. Myers at the University of California at San Francisco <ref> [17] </ref>. * CLP: Data on 12 families with autosomal dominant nonsyndromic cleft lip and palate (CLP) from the laboratory of Dr. Jacqueline T.
Reference: [18] <author> P. L. Miller, P. Nadkarni, J. E. Gelernter, N. Carriero, A. J. Pakstis, and K. K. </author> <title> Kidd. Parallelizing genetic linkage analysis: A case study for applying parallel computation in molecular biology. </title> <journal> Computers and Biomedical Research, </journal> <volume> 24 </volume> <pages> 234-248, </pages> <year> 1991. </year>
Reference-contexts: In this section, we briefly review attempts by other research groups to parallelize linkage analysis and review a few aspects of our first parallel implementation needed to understand the subsequent sections. Miller, et al. <ref> [18] </ref> did a parallel implementation of LINKMAP using the Linda package. Their parallelization strategy assumes that that there are many pedigrees and/or many candidate vectors. They treat the evaluation of each likelihood for one pedigree as a separate task that can be assigned to one processor. <p> The merits of both methods in LINKAGE are discussed and analyzed experimentally in [14]. The currently distributed versions of ILINK in both LINKAGE and FASTLINK start using forward differences and switch to central differences if successive values of appear sufficiently similar. As suggested by Miller et al. <ref> [18] </ref> in the context of LINKMAP, performing likelihood function evaluations in parallel is a good strategy. However, in their tests they had enough function evaluations to assign each evaluation to only one processor.
Reference: [19] <author> J. Ott. </author> <title> Estimation of the recombination fraction in human pedigrees- efficient computation of the likelihood for human linkage studies. </title> <journal> American Journal of Human Genetics, </journal> <volume> 26 </volume> <pages> 588-597, </pages> <year> 1974. </year>
Reference-contexts: The task of estimating the vector for more than two loci, is called multilocus analysis. A major advance of the LINKAGE package over its predecessor LIPED <ref> [19] </ref> is that LINKAGE supports multilocus analysis [15]. The LINKAGE package contains four related principal programs, LODSCORE, ILINK, LINKMAP, and MLINK. The FASTLINK package [3, 21] contains significantly faster sequential versions of these four programs.
Reference: [20] <author> J. Ott. </author> <title> Analysis of Human Genetic Linkage. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore and London, </address> <year> 1991. </year> <note> Revised edition. </note>
Reference-contexts: The probability of recombination is called the recombination fraction and denoted by . Most programs in common usage estimate using a maximum likelihood approach. A thorough treatment of maximum likelihood linkage analysis is given in Ott's book <ref> [20] </ref>. In this section we highlight a few aspects of the LINKAGE/FASTLINK software package that are relevant to our parallel implementation of ILINK. The recombination fraction can be generalized to more than two loci.
Reference: [21] <author> A. A. Schaffer, S. K. Gupta, K. Shriram, and R. W. Cottingham Jr. </author> <title> Avoiding recomputation in linkage analysis. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 225-237, </pages> <year> 1994. </year>
Reference-contexts: In this paper we report on a multi-level attempt to parallelize the ILINK program from the LINKAGE package [15, 13, 16], the most widely-used software package for linkage analysis. This work represents a continuation of the FASTLINK project in which we have significantly speeded up the main sequential programs <ref> [3, 21] </ref> in LINKAGE and parallelized one of them (ILINK) [5]. There are several different levels at which linkage computations may be paral-lelized. In practice, linkage analysis computations usually consist of several different runs corresponding to different orders of possible genes and/or different candidate recombination fraction vectors. <p> A major advance of the LINKAGE package over its predecessor LIPED [19] is that LINKAGE supports multilocus analysis [15]. The LINKAGE package contains four related principal programs, LODSCORE, ILINK, LINKMAP, and MLINK. The FASTLINK package <ref> [3, 21] </ref> contains significantly faster sequential versions of these four programs. FASTLINK does not include the many auxiliary programs for preprocessing the data and postprocessing the results that come with LINKAGE. <p> When pedigrees have loops, each function evaluation does multiple traversals of the pedigree. It would be natural to do the different loop traversals in parallel, but this is tricky because different traversals will take different amounts of time. Furthermore, Schaffer, Gupta, Shriram, and Cottingham <ref> [21] </ref> made a substantial improvement in the loop handling strategy in version 2.0 of FASTLINK. We are currently working on other fundamental improvements to the sequential algorithms for handling loops.
Reference: [22] <author> N. Schork. </author> <title> The Parallel Computation of Pedigree Likelihoods. </title> <booktitle> In Proc. First International Conference on Intelligent Systems for Molecular Biology, </booktitle> <pages> pages 371-379, </pages> <year> 1993. </year>
Reference-contexts: We implemented a strategy of partitioning the input pedigrees and having different processors work on different nuclear families. Our implementation was similar to a theoretical proposal of Schork <ref> [22] </ref>. We found that partitioning the input pedigrees actually slowed the program down measurably; therefore, we do not devote any space in the body of the paper to explaining our partitioning implementation, but we briefly explain the problems we encountered in the Discussion at the end. <p> Vaughan [25] did a parallel implementation of LINKMAP using the ISIS package. Her algorithm does parallelize the case of one pedigree and one likelihood, but she did not present any data on running times. She was primarily concerned with balancing the load on a heterogeneous network. Schork <ref> [22] </ref> proposed parallelizing the computation of one likelihood by assigning the probability updates of different nuclear families to different sets of processors. His paper does not describe any implementation. We experimentally implemented an algorithm similar to Schork's, but found that it slowed down ILINK measurably. <p> We investigated the possibility of splitting up the likelihood function evaluation by assigning different nuclear families to different processors. This had been proposed by Schork <ref> [22] </ref> in a theoretical paper. We have not reported in detail on our trial implementation because we found that it consistently increased the running time measurably. Qualitative observations of our implementation of Schork's idea in ILINK suggest the following difficulties: 1.
Reference: [23] <author> M. C. Speer, L. H. Yamaoka, J. H. Gilchrist, C. P. Gaskell, J. M. Stajich, J. M. Vance, Z. Kazantsev, A. Lastra, C. S. Haynes, J. S. Beckmann, D. Cohen, J. L. Weber, A. D. Roses, and M. A. Pericak-Vance. </author> <title> Confirmation of genetic heterogeneity in limb-girdle muscular dystrophy: Linkage of an autosomal dominant form to chromosome 5q. </title> <journal> Am. J. Hum. Genet., </journal> <volume> 50 </volume> <pages> 1211-1217, </pages> <year> 1992. </year>
Reference-contexts: The family has no loops. * LGMD: Data on 4 families with one form of limb-girdle muscular dystrophy (LGMD) provided by Drs. Marcy Speer and Margaret Pericak-Vance from Duke University <ref> [23] </ref>. Altogether the families have 416 individuals, 269 of which are in the huge family number 39 (discussed at length in [23]). <p> Marcy Speer and Margaret Pericak-Vance from Duke University <ref> [23] </ref>. Altogether the families have 416 individuals, 269 of which are in the huge family number 39 (discussed at length in [23]).
Reference: [24] <author> E. M. Stone, A. E. Kimura, J. C. Folk, S. R. Bennett, B. E. Nichols, L. M. Streb, and V. C. </author> <title> Sheffield. Genetic linkage of autosomal dominant neovascu-lar inflammatory vitreoretinopathy to chromosome 11q13. Human Molecular Genetics, </title> <booktitle> 1 </booktitle> <pages> 685-689, </pages> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Jacqueline T. Hecht at the University of Texas Health Science Center at Houston [7]. * ADNIV: Data on 1 large family with autosomal dominant neovascular inflammatory vitroretinopathy (ADNIV) provided by Drs. Ed Stone and Brian Nichols from the University of Iowa <ref> [24] </ref>. The family has 93 individuals, of whom 37 have unknown genotypes at all non-disease loci, and 9 have some unknown genotypes. The family has no loops. * LGMD: Data on 4 families with one form of limb-girdle muscular dystrophy (LGMD) provided by Drs.
Reference: [25] <author> M. S. Vaughan. </author> <title> A distributed approach to human genetic linkage analysis. M. S. </title> <type> Thesis, </type> <institution> Department of Computer Science, Duke University, </institution> <year> 1991. </year> <month> 28 </month>
Reference-contexts: If there are enough tasks, the load can be balanced effectively using a work-queue approach. Goradia, et al [6] did a similar parallelization of the linkage analysis program MENDEL [11, 12]. Vaughan <ref> [25] </ref> did a parallel implementation of LINKMAP using the ISIS package. Her algorithm does parallelize the case of one pedigree and one likelihood, but she did not present any data on running times. She was primarily concerned with balancing the load on a heterogeneous network.
References-found: 25

