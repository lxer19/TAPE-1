URL: http://www.cs.indiana.edu/hyplan/dmath/teach1.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/dmath.html
Root-URL: http://www.cs.indiana.edu
Email: sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Teaching a Smarter Learner  
Author: Sally A. Goldman H. David Mathias 
Date: April 21, 1994  
Address: St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  Department of Computer Science Washington University  
Abstract: We introduce a formal model of teaching in which the teacher is tailored to a particular learner, yet the teaching protocol is designed so that no collusion is possible. Not surprisingly, such a model remedies the non-intuitive aspects of other models in which the teacher must successfully teach any consistent learner. We prove that any class that can be exactly identified by a deterministic polynomial-time algorithm with access to a very rich set of example-based queries is teachable by a computationally unbounded teacher and a polynomial-time learner. In addition, we present other general results relating this model of teaching to various previous results. We also consider the problem of designing teacher/learner pairs in which both the teacher and learner are polynomial-time algorithms and describe teacher/learner pairs for the classes of 1-decision lists and Horn sentences. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Howard Aizenstein, Lisa Hellerstein, and Leonard Pitt. </author> <title> Read-thrice DNF is hard to learn with membership and equivalence queries. </title> <booktitle> In 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 523-532, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: First of all, an interesting open question is to determine whether there exist teacher/learner pairs in which the learner requires only polynomial computation time for such classes as 2-quasi Horn Sentences and read-thrice DNF that appear difficult to learn from queries <ref> [2, 1] </ref>. (It is easily shown for these problems that if the teacher is given an arbitrary representation of the target then the teacher must perform a satis-fiability problem to even decide if there are any positive examples and thus could not run in polynomial time assuming P 6= N P
Reference: [2] <author> D. Angluin, M. Frazier, and L. Pitt. </author> <title> Learning conjunctions of Horn clauses. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 147-164, </pages> <year> 1992. </year>
Reference-contexts: A Horn sentence is a conjunction of Horn clauses. Angluin, Frazier and Pitt <ref> [2] </ref> gave a polynomial-time algorithm to exactly identify an m-clause Horn sentence using O (mn) equivalence queries, O (m 2 n) membership queries and, ~ O (m 2 n 2 ) time 5 , where n is the number of variables in the instance space. <p> The total time required by the teacher is O (m 2 n+m 3 ). The length of the teaching set output is at most 2m. Finally, the learner runs what is essentially the standard algorithm for learning Horn sentences <ref> [2] </ref>. Figure 4 summarizes the algorithms of T and L. We now show that this learner and teacher are a valid T/L-pair. <p> First of all, an interesting open question is to determine whether there exist teacher/learner pairs in which the learner requires only polynomial computation time for such classes as 2-quasi Horn Sentences and read-thrice DNF that appear difficult to learn from queries <ref> [2, 1] </ref>. (It is easily shown for these problems that if the teacher is given an arbitrary representation of the target then the teacher must perform a satis-fiability problem to even decide if there are any positive examples and thus could not run in polynomial time assuming P 6= N P
Reference: [3] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: = 0 ) f (x 1 ) = 1)) justifying-assign (v i ) 1 f v i !0 (x 1 ) = f v i !1 (x 1 ) partial-equivalence (h) 1 (h (x 1 ) = f (x 1 )) _ (h (x 1 ) 6= fl) by Angluin <ref> [3] </ref>. A justifying assignment for an input variable is an instance whose classification changes if the value of the variable is changed. <p> Because our model incorporates a very powerful set of queries, classes that may not be learnable using membership and equivalence queries are T/L-teachable. In particular, from Angluin's result <ref> [3] </ref> that pattern languages can be exactly identified in polynomial time using only restricted superset queries, we get that pattern languages are semi-poly T/L-teachable. Lange and Wiehagen independently presented an algorithm to learn pattern languages from good examples [17].
Reference: [4] <author> Martin Anthony, Graham Brightwell, Dave Cohen, and John Shawe-Taylor. </author> <title> On exact specification by examples. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 311-318. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year> <month> 32 </month>
Reference-contexts: 1 Introduction Recently, there has been interest in developing formal models of teaching <ref> [4, 10, 11, 16, 27] </ref> through which we can develop a better understanding of how a teacher can most effectively aid a learner in accomplishing a learning task. <p> Furthermore, this hardness result has been embedded into several other hardness results such as that for teaching full decision lists [16] and teaching linearly separable Boolean functions <ref> [4] </ref>. Thus these hardness results appear to be due to a defect in the model rather than an intrinsic difficulty in teaching these classes. Similar problems to the ones discussed above emerge when comparing these teaching models to the self-directed learning model 2 . <p> Independently, Shinohara and Miyano [27] introduced an equivalent notion of teachability in which a class is teachable by examples if there exists a polynomial size sample under which all consistent learners will exactly identify the target. In other related work, Anthony, Brightwell, Cohen, and Shawe-Taylor <ref> [4] </ref> compute bounds on the size of the smallest sample with which only the target function is consistent for subclasses of linearly separable Boolean functions. <p> In particular, this contrasts the negative result of Jackson and Tompkins [16] that the class of 1-decision lists is not teachable without trusted information, and the negative result of Anthony et. al <ref> [4] </ref> that linearly separable Boolean 15 functions are not efficiently teachable. In fact, Bshouty's [7] result that arbitrary deci-sion trees are learnable with membership and equivalence queries implies that a much broader class than 1-decision lists is T/L-teachable with a polynomial-time learner.
Reference: [5] <author> J. Barzdin and R. Freivald. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: In fact, Bshouty's [7] result that arbitrary deci-sion trees are learnable with membership and equivalence queries implies that a much broader class than 1-decision lists is T/L-teachable with a polynomial-time learner. Letting A in the proof of Theorem 2 be the halving algorithm <ref> [5, 18] </ref>, we immediately get the following corollary. Corollary 4 Any representation class C is T/L-teachable (by a computationally unbounded teacher and learner) with a teaching set of length at most log jCj.
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The Vapnik-Chervonenkis dimension of C, denoted vcd (C), is defined to be the smallest d for which no set of d + 1 points is shattered by C. Blumer et al. <ref> [6] </ref> have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [28]. Related to the VC-dimension are the notions of maximal and maximum concept classes [8, 30].
Reference: [7] <author> Nader H. Bshouty. </author> <title> Exact learning via the monotone theory. </title> <booktitle> In 34th Annual Symposium on Foundations of Computer Science, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: In particular, this contrasts the negative result of Jackson and Tompkins [16] that the class of 1-decision lists is not teachable without trusted information, and the negative result of Anthony et. al [4] that linearly separable Boolean 15 functions are not efficiently teachable. In fact, Bshouty's <ref> [7] </ref> result that arbitrary deci-sion trees are learnable with membership and equivalence queries implies that a much broader class than 1-decision lists is T/L-teachable with a polynomial-time learner. Letting A in the proof of Theorem 2 be the halving algorithm [5, 18], we immediately get the following corollary.
Reference: [8] <author> Sally Floyd. </author> <title> On Space-bounded Learning and the Vapnik-Chervonenkis Dimension. </title> <type> PhD thesis, </type> <institution> International Computer Science Institute, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Also the number of examples required by the teacher is at most the maximum number of mistakes made by any self-directed learning algorithm. Furthermore, using our model there is an interesting 5 relationship between teaching and data compression. Applying the results of Floyd <ref> [8] </ref> we obtain that for any maximum class C there is a teacher/learner pair for which at most vcd (C) examples are presented. <p> Since, by the conditions of the theorem, the target f is the minimum hypothesis consistent with F (S f ), the hypothesis output by the learner cannot be affected by the additional examples added to the teaching set by the adversary. 2 Many of the space-bounded learning algorithms that Floyd <ref> [8] </ref> presents satisfy the conditions of Theorem 9 and thus we immediately obtain results for our teaching model. To state these results, we must define the Vapnik-Chervonenkis dimension [29]. Let X be any instance space, and C be a concept class over X . <p> Blumer et al. [6] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [28]. Related to the VC-dimension are the notions of maximal and maximum concept classes <ref> [8, 30] </ref>. A concept class is maximal if adding any concept to the class increases the VC dimension of the class. <p> A concept class C over X is maximum if for every finite subset Y X , the class C, when restricted to be a class over Y , contains d (jY j) concepts. Floyd <ref> [8] </ref> shows that if C is a maximum class of VC-dimension d on the set X , then there is a data compression scheme of size d for C.
Reference: [9] <author> Michael Frazier. </author> <title> Matters Horn and Other Features in the Computational Learning Theory Landscape: The Notion of Membership. </title> <type> PhD thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: To do this the teacher simply runs the algorithm by Angluin, Frazier and Pitt before generating its teaching set. This ratio bound appears in Frazier's thesis <ref> [9] </ref>. 22 T/L pair for Horn sentences: Teacher: T (f ) ; Repeat until no clause is changed For all pairs of clauses in the target If ant (C i ) ae ant (C j ) then replace C j by (ant (C j ) ^ cons (C i ) )
Reference: [10] <author> Rusins Freivalds, Efim Kinber, and Rolf Wiehagen. </author> <title> Inference from good examples. </title> <journal> Theoretical Computer Science, </journal> <volume> 110 </volume> <pages> 131-144, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, there has been interest in developing formal models of teaching <ref> [4, 10, 11, 16, 27] </ref> through which we can develop a better understanding of how a teacher can most effectively aid a learner in accomplishing a learning task. <p> Within the inductive inference paradigm, Freivalds, Kinber and Wiehagen <ref> [10] </ref> and Lange and Wiehagen [17] have examined inference from "good examples". Good examples are chosen by a helpful teacher to reduce the number of examples required.
Reference: [11] <author> S. A. Goldman and M. J. Kearns. </author> <title> On the complexity of teaching. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 303-314. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <note> To appear, Journal of Computer and System Sciences. </note>
Reference-contexts: 1 Introduction Recently, there has been interest in developing formal models of teaching <ref> [4, 10, 11, 16, 27] </ref> through which we can develop a better understanding of how a teacher can most effectively aid a learner in accomplishing a learning task. <p> In previous work on developing formal models of teaching, in order to deal with the collusion issue the goal of pairing the teacher and learner has been completely sacrificed. Namely, in Goldman and Kearns <ref> [11] </ref> the learner has effectively been replaced by an adversarial learner who attempts not to learn whereas the goal of such work is to allow the teacher and learner to work together. <p> Gold-man, Rivest and Schapire [12] introduced the model of teacher-directed learning, a variant of the on-line learning model in which a helpful teacher selects the instances, and applied it to the problem of learning binary relations and total orders. Building upon this framework, Goldman and Kearns <ref> [11] </ref> defined a formal model of teaching in which they measured the complexity of teaching by the minimum number of examples that must be presented to any consistent learner so that the learner outputs a hypothesis logically equivalent to the target function. <p> Additionally, the teacher must add to the teaching set a set of examples that uniquely identify the function represented by f (i.e. a teaching sequence as defined by Goldman and Kearns <ref> [11] </ref>). The learner chooses an ordered k-tuple of instances from the teaching set and identifies its associated f 2 C. If every instance in the teaching set is labeled according to f then f is the target function 11 and is output by the learner.
Reference: [12] <author> S. A. Goldman, R. L. Rivest, and R. E. Schapire. </author> <title> Learning binary relations and total orders. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(5) </volume> <pages> 1006-1034, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Again, because the teacher must successfully teach all consistent learners, a "smart" self-directed learner can perform 1 A learner is consistent if its hypotheses are consistent with all previously seen examples. 2 A self-directed learner <ref> [12, 13] </ref> is a learner that selects the presentation order for the instances. In this model, the learning complexity is measured according to the number of incorrect predictions made. 2 better on its own than with the teacher's guidance. <p> For both classes, the sample complexity is asymptotically less than that for the best known learning algorithm. 2 Previous Work We now briefly review the theoretical work studying the complexity of teaching. Gold-man, Rivest and Schapire <ref> [12] </ref> introduced the model of teacher-directed learning, a variant of the on-line learning model in which a helpful teacher selects the instances, and applied it to the problem of learning binary relations and total orders.
Reference: [13] <author> Sally A. Goldman and Robert H. Sloan. </author> <title> The power of self-directed learning. </title> <type> Technical Report WUCS-92-49, </type> <institution> Washington University, Department of Computer Science, </institution> <month> November </month> <year> 1992. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: Again, because the teacher must successfully teach all consistent learners, a "smart" self-directed learner can perform 1 A learner is consistent if its hypotheses are consistent with all previously seen examples. 2 A self-directed learner <ref> [12, 13] </ref> is a learner that selects the presentation order for the instances. In this model, the learning complexity is measured according to the number of incorrect predictions made. 2 better on its own than with the teacher's guidance. <p> Lange and Wiehagen independently presented an algorithm to learn pattern languages from good examples [17]. It is also easily shown that the self-directed learning model <ref> [13] </ref> can be simulated in our model. We now give the definition of the self-directed learning model.
Reference: [14] <author> S. Goldwasser, S. Goldwasser, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <booktitle> In 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 291-304, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In their model there are teacher/learner pairs in which the teacher chooses examples tailored to a particular learner. To avoid collusion between the teacher and learner, they consider the interaction between the teacher and learner as a modified prover-verifier session <ref> [14] </ref> in which the learner and teacher can collude but no adversarial substitute teacher can cause the learner to output a hypothesis inconsistent with the sample.
Reference: [15] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning nested differences of intersection-closed concept classes. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 165-196, </pages> <year> 1990. </year> <note> Special issue for COLT 89. 33 </note>
Reference-contexts: Applying the results of Floyd [8] we obtain that for any maximum class C there is a teacher/learner pair for which at most vcd (C) examples are presented. Likewise, from the results of Helmbold, Sloan and Warmuth <ref> [15] </ref>, it follows that for any intersection-closed class C, the nested difference of p functions from C can be taught in our model with at most p vcd (C) examples. <p> Corollary 10 For any maximum class C, there is a valid T/L pair such that the optimal teaching set has length at most the VC-dimension of C. We can show the corresponding result for intersection-closed classes by applying results from Helmbold, Sloan and Warmuth <ref> [15] </ref>.
Reference: [16] <author> Jeffrey Jackson and Andrew Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 319-326. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, there has been interest in developing formal models of teaching <ref> [4, 10, 11, 16, 27] </ref> through which we can develop a better understanding of how a teacher can most effectively aid a learner in accomplishing a learning task. <p> Jackson and Tompkins <ref> [16] </ref> are able to show that anything learnable with membership and equivalence queries is teachable with trusted information. However, their method for preventing collusion still requires that the teacher successfully teaches any consistent learner. <p> While, this class is quite simple and thus should be easy to teach, Goldman and Kearns show that jCj 1 examples are required to teach it in their model. Furthermore, this hardness result has been embedded into several other hardness results such as that for teaching full decision lists <ref> [16] </ref> and teaching linearly separable Boolean functions [4]. Thus these hardness results appear to be due to a defect in the model rather than an intrinsic difficulty in teaching these classes. <p> Namely, in Goldman and Kearns [11] the learner has effectively been replaced by an adversarial learner who attempts not to learn whereas the goal of such work is to allow the teacher and learner to work together. Jackson and Tompkins <ref> [16] </ref> attempted to return to such pairing of the teacher and learner but to prevent the most blatant of coding schemes they required that the learner must still succeed if the teacher was replaced by an adversarial substitute. <p> In this paper we formalize this notion of collusion and prove that any teacher/learner pair that is valid under our model does not perform any such collusion. Our new model starts with a teacher/learner pair as in the model introduced by Jackson and Tompkins <ref> [16] </ref>. However, unlike in their work, we only require that if the teacher is replaced by an adversarial "substitute" that embeds the teaching set of the true teacher within his teaching set, then the learner will still output a hypothesis that is logically equivalent to the target function. <p> Our new teaching model is most closely related to the model introduced by Jackson and Tomkins <ref> [16] </ref>. In their model there are teacher/learner pairs in which the teacher chooses examples tailored to a particular learner. <p> As an immediate consequence of this result we know that many classes (namely, all of those for which exact-identification is efficiently achieved with queries) are T/L-teachable with an efficient learner. In particular, this contrasts the negative result of Jackson and Tompkins <ref> [16] </ref> that the class of 1-decision lists is not teachable without trusted information, and the negative result of Anthony et. al [4] that linearly separable Boolean 15 functions are not efficiently teachable.
Reference: [17] <author> Stefan Lange and Rolf Wiehagen. </author> <title> Polynomial-time inference of arbitrary pattern languages. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 361-370, </pages> <year> 1991. </year>
Reference-contexts: Within the inductive inference paradigm, Freivalds, Kinber and Wiehagen [10] and Lange and Wiehagen <ref> [17] </ref> have examined inference from "good examples". Good examples are chosen by a helpful teacher to reduce the number of examples required. In both, encoding is avoided by requiring that the inference task is accomplished even when the learner is presented with any superset of the set of teacher-chosen examples. <p> Neither of these results, however, offer careful proof that this method actually prevents collusion between the teacher and learner. (In fact, neither of these papers really address the issue of collusion.) Lange and Wiehagen <ref> [17] </ref> examine learning pattern languages and show that this can be achieved with good examples. Our new teaching model is most closely related to the model introduced by Jackson and Tomkins [16]. In their model there are teacher/learner pairs in which the teacher chooses examples tailored to a particular learner. <p> In particular, from Angluin's result [3] that pattern languages can be exactly identified in polynomial time using only restricted superset queries, we get that pattern languages are semi-poly T/L-teachable. Lange and Wiehagen independently presented an algorithm to learn pattern languages from good examples <ref> [17] </ref>. It is also easily shown that the self-directed learning model [13] can be simulated in our model. We now give the definition of the self-directed learning model.
Reference: [18] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: In fact, Bshouty's [7] result that arbitrary deci-sion trees are learnable with membership and equivalence queries implies that a much broader class than 1-decision lists is T/L-teachable with a polynomial-time learner. Letting A in the proof of Theorem 2 be the halving algorithm <ref> [5, 18] </ref>, we immediately get the following corollary. Corollary 4 Any representation class C is T/L-teachable (by a computationally unbounded teacher and learner) with a teaching set of length at most log jCj.
Reference: [19] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: no justifying assignment, or as a counterexample it returns two instances that provide a justifying assignment for v i . (The notation f v i !0 denotes the function obtained from f by fixing v i = 0.) Finally, in a partial equivalence query (as defined by Maass and Turan <ref> [19] </ref>) the learner can present a hypothesis h : X ! f0; 1; flg, and is either told that all specified instances are correct or is given an x 2 X such that h (x) 2 f0; 1g and x is misclassified by h. 13 Proof: We prove this result by
Reference: [20] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: As another example, we could consider the class of learners that only select a minimal consistent hypothesis. This corresponds to requiring that the learner always selects an element from the set of most specific concepts in Mitchell's version space <ref> [20] </ref>. 30 Another interesting variation is one in which the learner is not required to exactly identify the target, but rather needs only output an *-good approximation to the target. (Romanik and Smith [23, 24] consider a PAC-style criterion in their work.) Finally, another interesting model is one in which there
Reference: [21] <author> B. K. Natarajan. </author> <title> On learning Boolean functions. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 296-304, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: In other related work, Anthony, Brightwell, Cohen, and Shawe-Taylor [4] compute bounds on the size of the smallest sample with which only the target function is consistent for subclasses of linearly separable Boolean functions. Natarajan <ref> [21] </ref> defines a dimension measure for classes of Boolean functions that measures the complexity of a class C by the length of the shortest example sequence for which the target function 6 is the unique, most specific function from C consistent with the sample. <p> For example, one class of learners that would be interesting to study is that of learners that always select a minimum (in terms of the number of instances classified as positive) consistent hypothesis. In fact, this type of learner was studied by Natarajan <ref> [21] </ref> in terms of one-sided learning. As another example, we could consider the class of learners that only select a minimal consistent hypothesis.
Reference: [22] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: Jackson and Tomkins show that the class of 1-decisions lists with no irrelevant variables is teachable in their model. They prove, however, that 1-decision lists are not teachable in their model without trusted information. 5.1 Decision Lists As defined by Rivest <ref> [22] </ref>, a 1-decision list (1-DL) over the set V n = fv 1 ; v 2 ; : : : ; v n g of n Boolean variables is an ordered list of nodes f = hn 1 ; : : : ; n r i where node n i (for
Reference: [23] <author> Kathleen Romanik. </author> <title> Approximate testing and learnability. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 327-332. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Their model requires the teacher to present the shortest example sequence so that any learner using a particular algorithm (namely, the nearest-neighbor algorithm) learns the target. However, their work does not address the issue of preventing the teacher and learner from colluding. Romanik and Smith <ref> [23, 24] </ref> propose a testing problem that involves specifying, for a given target function, a set of test points that can be used to determine if a tested object is equivalent to the target. <p> the learner always selects an element from the set of most specific concepts in Mitchell's version space [20]. 30 Another interesting variation is one in which the learner is not required to exactly identify the target, but rather needs only output an *-good approximation to the target. (Romanik and Smith <ref> [23, 24] </ref> consider a PAC-style criterion in their work.) Finally, another interesting model is one in which there are two distinct stages. The first stage operates as in our current model except that the teacher is not required to provide examples sufficient for exact identification.
Reference: [24] <author> Kathleen Romanik and Carl Smith. </author> <title> Testing geometric objects. </title> <type> Technical Report UMIACS-TR-90-69, </type> <institution> University of Maryland College Park, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: Their model requires the teacher to present the shortest example sequence so that any learner using a particular algorithm (namely, the nearest-neighbor algorithm) learns the target. However, their work does not address the issue of preventing the teacher and learner from colluding. Romanik and Smith <ref> [23, 24] </ref> propose a testing problem that involves specifying, for a given target function, a set of test points that can be used to determine if a tested object is equivalent to the target. <p> the learner always selects an element from the set of most specific concepts in Mitchell's version space [20]. 30 Another interesting variation is one in which the learner is not required to exactly identify the target, but rather needs only output an *-good approximation to the target. (Romanik and Smith <ref> [23, 24] </ref> consider a PAC-style criterion in their work.) Finally, another interesting model is one in which there are two distinct stages. The first stage operates as in our current model except that the teacher is not required to provide examples sufficient for exact identification.
Reference: [25] <author> Steven Salzberg, Arthur Delcher, David Heath, and Simon Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Salzberg, Delcher, Heath and Kasif <ref> [25] </ref> have also considered a model of learning with a helpful teacher. Their model requires the teacher to present the shortest example sequence so that any learner using a particular algorithm (namely, the nearest-neighbor algorithm) learns the target.
Reference: [26] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Define d (m) = &gt; &lt; P d i i for m d If C is a concept class of VC-dimension d on a finite set X with jX j = m, then the cardinality of C is at most d (m) <ref> [26, 29] </ref>. A concept class C over X is maximum if for every finite subset Y X , the class C, when restricted to be a class over Y , contains d (jY j) concepts.
Reference: [27] <author> Ayumi Shinohara and Satoru Miyano. </author> <title> Teachability in computational learning. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 337-347, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, there has been interest in developing formal models of teaching <ref> [4, 10, 11, 16, 27] </ref> through which we can develop a better understanding of how a teacher can most effectively aid a learner in accomplishing a learning task. <p> Independently, Shinohara and Miyano <ref> [27] </ref> introduced an equivalent notion of teachability in which a class is teachable by examples if there exists a polynomial size sample under which all consistent learners will exactly identify the target.
Reference: [28] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <month> 34 </month>
Reference-contexts: Blumer et al. [6] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant <ref> [28] </ref>. Related to the VC-dimension are the notions of maximal and maximum concept classes [8, 30]. A concept class is maximal if adding any concept to the class increases the VC dimension of the class.
Reference: [29] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative fre-quencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: To state these results, we must define the Vapnik-Chervonenkis dimension <ref> [29] </ref>. Let X be any instance space, and C be a concept class over X . A finite set Y X is shattered by C if fc " Y j c 2 Cg = 2 Y . <p> Define d (m) = &gt; &lt; P d i i for m d If C is a concept class of VC-dimension d on a finite set X with jX j = m, then the cardinality of C is at most d (m) <ref> [26, 29] </ref>. A concept class C over X is maximum if for every finite subset Y X , the class C, when restricted to be a class over Y , contains d (jY j) concepts.
Reference: [30] <author> E. Welzl. </author> <title> Complete range spaces. </title> <type> Unpublished manuscript, </type> <year> 1987. </year> <month> 35 </month>
Reference-contexts: Blumer et al. [6] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [28]. Related to the VC-dimension are the notions of maximal and maximum concept classes <ref> [8, 30] </ref>. A concept class is maximal if adding any concept to the class increases the VC dimension of the class.
References-found: 30

