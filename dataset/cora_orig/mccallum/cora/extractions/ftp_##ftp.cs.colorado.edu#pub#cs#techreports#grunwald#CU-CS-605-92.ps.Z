URL: ftp://ftp.cs.colorado.edu/pub/cs/techreports/grunwald/CU-CS-605-92.ps.Z
Refering-URL: http://www.cs.gatech.edu/people/home/jmankoff/dataflow.html
Root-URL: 
Title: Data Flow Equations for Explicitly Parallel Programs  
Author: Dirk Grunwald and Harini Srinivasan 
Date: July 1992  
Address: Boulder Campus Box 430 Boulder, CO 80309  Boulder  
Affiliation: Department of Computer Science University of Colorado at  ffi University of Colorado at  
Pubnum: CU-CS-605-92  
Abstract: Technical Report CU-CS-605-92 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <address> AddisonWesley, Reading, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction In this paper, we describe a data flow framework <ref> [1] </ref> for computing the reaching definition information in parallel programs with a view of being able to optimize such programs. <p> Then we define inherited and synthesized attributes in a set of data flow equations, and solve these equations. This section discusses the data flow equations to solve the reaching definitions problem in sequential programs <ref> [1] </ref>. The reaching definition problem is to find the set of definitions of a variable `v' that can reach a particular use of `v'. This is also referred to as the ud-chaining problem in the literature. <p> In such a system, the order of traversal of the CFG only affects the convergence rate of the different sets to their fixpoint. It has been proven that a depth first traversal of the CFG helps reduce the number of iterations to five in most practical cases <ref> [1] </ref>. The CFG for the sequential program in Figure 1 is given in Figure 2. Variable `j' is defined at nodes (1) and (4); call these j 1 and j 4 respectively. The reaching definitions for the use of `j' at node (6) are j 1 and j 4 . <p> Nodes (2) and (7) represent fork nodes and nodes (11) and (10) are the respective join nodes. Sequential, parallel and synchronization edges are identified in this figure as indicated. 5 Data Flow Equations for Parallel Sections In Section 2, we reviewed the data flow equations from <ref> [1] </ref> to compute the reaching definition information at any point in a sequential program. In this Section, we extend these equations to handle the Parallel Sections construct.
Reference: [2] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> Compile-time detection of race conditions in a parallel program. </title> <booktitle> In Proc. 3rd International Conference on Supercomputing, </booktitle> <pages> pages 175-185, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Second, we are interested in being able to achieve good execution rates of such programs on high performance architectures. Though extensive work has been done on analyzing parallel programs for potential races <ref> [6, 4, 2] </ref>, little work has been done on analyzing parallel programs with an aim of being able to perform code optimizations. Midkiff and Padua [7] point out the difficulties in optimizing explicitly parallel programs. Our work focuses on developing an intermediate representation for optimizing parallel programs. <p> The Parallel Flow Graph (PFG) is similar to the Synchronized Control Flow Graph [4] and the Program Execution Graph <ref> [2] </ref>. A PFG is basically a directed graph with nodes representing extended basic blocks in the program and edges representing either sequential control flow, parallel control flow or synchronization.
Reference: [3] <author> D. Callahan and J. Subhlok. </author> <title> Static Analysis of low-level synchronization. </title> <booktitle> In Proc. of the ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 100-111, </pages> <address> Madison, WA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: In the next section, we extend these data flow equations to consider event synchronization between parallel Sections. 6 Including the effect of Synchronization We extend the data flow equations in the previous section to consider event synchronization by using the preserved sets formulation given in <ref> [3] </ref>. Synchronization using post/wait occurs between different threads that execute in parallel. Synchronization edges carry data flow information, i.e., they propagate values of variables from the thread that posted the event to the thread that is waiting for the event to be posted. <p> It is also possible that these multiple posts and waits are executed conditionally. Thus, a synchronization edge does not always imply an execution order. We are, however, interested in the potential execution order for computing the reaching definition information. Preserved sets, as defined in <ref> [3] </ref> give precisely the set of nodes that execute before a given node, defined as follows: Definition 1 A node n j 2 Preserved (n i ) if and only if for all parallel executions x, if n j and n i are both executed, n j is completed before n <p> However, Callahan and Subhlok <ref> [3] </ref> have shown that computing this information is Co-NP Hard and have given a data flow framework to compute a conservative approximation to the Preserved sets. The approximate Preserved sets are computed as the least fixpoint of a set of data flow equations over the Parallel Flow Graph. <p> For this example, the data flow formulation for Preserved sets given in <ref> [3] </ref> will be able to determine the Preserved sets of the wait node accurately. However, since this data flow formulation is conservative, we may not be always able to compute the exact Preserved sets for any node.
Reference: [4] <author> David Callahan, Ken Kennedy, and Jaspal Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming [10], </booktitle> <pages> pages 21-30. </pages>
Reference-contexts: Second, we are interested in being able to achieve good execution rates of such programs on high performance architectures. Though extensive work has been done on analyzing parallel programs for potential races <ref> [6, 4, 2] </ref>, little work has been done on analyzing parallel programs with an aim of being able to perform code optimizations. Midkiff and Padua [7] point out the difficulties in optimizing explicitly parallel programs. Our work focuses on developing an intermediate representation for optimizing parallel programs. <p> The Parallel Flow Graph (PFG) is similar to the Synchronized Control Flow Graph <ref> [4] </ref> and the Program Execution Graph [2]. A PFG is basically a directed graph with nodes representing extended basic blocks in the program and edges representing either sequential control flow, parallel control flow or synchronization.
Reference: [5] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We anticipate the use of some technology from data dependence analysis to analyze these constructs. Earlier work [14, 11, 13, 12] has looked at translating explicitly parallel programs to their Static Single Assignment intermediate representation <ref> [5] </ref>, a more efficient dataflow representation. We hope to extend these results on post/wait synchronization to that representation as well. 20
Reference: [6] <author> Anne Dinning and Edith Schonberg. </author> <title> An empirical comparison of monitoring algorithms for access anomaly detection. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming [10], </booktitle> <pages> pages 1-10. </pages>
Reference-contexts: Second, we are interested in being able to achieve good execution rates of such programs on high performance architectures. Though extensive work has been done on analyzing parallel programs for potential races <ref> [6, 4, 2] </ref>, little work has been done on analyzing parallel programs with an aim of being able to perform code optimizations. Midkiff and Padua [7] point out the difficulties in optimizing explicitly parallel programs. Our work focuses on developing an intermediate representation for optimizing parallel programs.
Reference: [7] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Issues in the optimization of parallel programs. </title> <editor> In David Padua, editor, </editor> <booktitle> Proc. 1990 International Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 105-113, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year> <institution> Penn State Press. </institution>
Reference-contexts: Though extensive work has been done on analyzing parallel programs for potential races [6, 4, 2], little work has been done on analyzing parallel programs with an aim of being able to perform code optimizations. Midkiff and Padua <ref> [7] </ref> point out the difficulties in optimizing explicitly parallel programs. Our work focuses on developing an intermediate representation for optimizing parallel programs.
Reference: [8] <author> Steven S. Muchnick and Neil D. Jones. </author> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: the data flow equations for computing reaching definition information in sequential programs, to handle parallel constructs and in Section 6 we extend these equations to handle synchronization in the form of post/wait statements. 2 Global Data Flow Analysis The problem of global data flow analysis can be explained as follows <ref> [8] </ref>: given the control flow structure, we must discern the nature of the data flow (which definitions of program quantities can affect which uses) within the program. Data flow problems are often posed as a system of equations based on the Control Flow Graph of the program.
Reference: [9] <author> Parallel Computing Forum. </author> <title> PCF Parallel FORTRAN Extensions. </title> <journal> FORTRAN Forum, </journal> <volume> 10(3), </volume> <month> September </month> <year> 1991. </year> <note> (special issue). </note>
Reference-contexts: In our work we consider the parallel extensions to FORTRAN as specified by the Parallel Computing Forum <ref> [9] </ref>, which is the basis of the ANSI committee X3H5 standardization effort. As mentioned earlier, the performance of parallel programs on existing and future high performance architectures depends to a great extent on the ability to perform aggressive code optimizations, particularly scalar optimizations across parallel constructs. <p> This example illustrates how the In and Out sets are computed for sequential programs, given the Gen and Kill sets. In the following sections, we derive an analogous procedure for explicitly parallel programs. 3 Parallel Constructs In this paper, we only consider the Parallel Sections construct <ref> [9] </ref>. The Parallel Sections construct is used to specify parallel execution of explicitly identified sections of code. Each section of code is interpreted as a parallel thread, and must be data independent except where an appropriate synchronization mechanism is used. <p> Synchronization using post/wait occurs between different threads that execute in parallel. Synchronization edges carry data flow information, i.e., they propagate values of variables from the thread that posted the event to the thread that is waiting for the event to be posted. According to <ref> [9] </ref>, it must be possible to execute each post before 13 Node Gen Kill ParKill 1 fa 1 ; b 1 ; c 1 g fa 3 ; b 3 ; b 5 ; c 7 g 3 fa 3 ; b 3 g fa 1 ; b 1 g fb
Reference: [10] <institution> Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </institution> <address> Seattle, Washington, March 1990. </address> <publisher> ACM Press. </publisher>
Reference: [11] <author> Harini Srinivasan. </author> <title> Analyzing programs with explicit parallelism. M.S. </title> <type> thesis 91-TH-006, </type> <institution> Oregon Graduate Institute of Science and Technology, Dept. of Computer Science and Engineering, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: In the future, we propose to extend the data flow equations to handle Parallel Do, another parallel construct specified by PCF FORTRAN. We anticipate the use of some technology from data dependence analysis to analyze these constructs. Earlier work <ref> [14, 11, 13, 12] </ref> has looked at translating explicitly parallel programs to their Static Single Assignment intermediate representation [5], a more efficient dataflow representation. We hope to extend these results on post/wait synchronization to that representation as well. 20
Reference: [12] <author> Harini Srinivasan and Dirk Grunwald. </author> <title> An Efficient Construction of Parallel Static Single Assignment Form for Structured Parallel Programs. </title> <type> Technical Report CU-CS-564-91, </type> <institution> University of Colorado at Boulder., </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: and the presence of multiple values at 7 such wait statements indicates potential anomalies. 1 Similarly at a join node, multiple values for a variable reaching that node indicates a potential anomaly in the Parallel Sections construct. 4 Parallel Flow Graph In this section, we describe the Parallel Flow Graph <ref> [12] </ref>, a data structure used to represent control flow, parallelism and synchronization in explicitly parallel programs. The Parallel Flow Graph (PFG) is similar to the Synchronized Control Flow Graph [4] and the Program Execution Graph [2]. <p> In the future, we propose to extend the data flow equations to handle Parallel Do, another parallel construct specified by PCF FORTRAN. We anticipate the use of some technology from data dependence analysis to analyze these constructs. Earlier work <ref> [14, 11, 13, 12] </ref> has looked at translating explicitly parallel programs to their Static Single Assignment intermediate representation [5], a more efficient dataflow representation. We hope to extend these results on post/wait synchronization to that representation as well. 20
Reference: [13] <author> Harini Srinivasan and Michael Wolfe. </author> <title> Analyzing programs with explicit parallelism. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David A. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 405-419. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 21 </month>
Reference-contexts: In the future, we propose to extend the data flow equations to handle Parallel Do, another parallel construct specified by PCF FORTRAN. We anticipate the use of some technology from data dependence analysis to analyze these constructs. Earlier work <ref> [14, 11, 13, 12] </ref> has looked at translating explicitly parallel programs to their Static Single Assignment intermediate representation [5], a more efficient dataflow representation. We hope to extend these results on post/wait synchronization to that representation as well. 20
Reference: [14] <author> Michael Wolfe and Harini Srinivasan. </author> <title> Data structures for optimizing programs with explicit parallelism. </title> <editor> In H. P. Zima, editor, </editor> <booktitle> Parallel Computation: First International ACPC Conference, volume 591 of Lecture Notes in Computer Science, </booktitle> <pages> pages 139-156. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year> <month> 22 </month>
Reference-contexts: In the future, we propose to extend the data flow equations to handle Parallel Do, another parallel construct specified by PCF FORTRAN. We anticipate the use of some technology from data dependence analysis to analyze these constructs. Earlier work <ref> [14, 11, 13, 12] </ref> has looked at translating explicitly parallel programs to their Static Single Assignment intermediate representation [5], a more efficient dataflow representation. We hope to extend these results on post/wait synchronization to that representation as well. 20
References-found: 14

