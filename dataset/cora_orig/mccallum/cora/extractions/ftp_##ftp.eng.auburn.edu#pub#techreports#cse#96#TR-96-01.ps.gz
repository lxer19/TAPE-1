URL: ftp://ftp.eng.auburn.edu/pub/techreports/cse/96/TR-96-01.ps.gz
Refering-URL: ftp://ftp.eng.auburn.edu/pub/techreports/README.html
Root-URL: 
Email: mccreary@eng.auburn.edu, clevem1@eng.auburn.edu  
Title: The Problem With Critical Path Scheduling Algorithms  
Author: C. L. McCreary, M. A. Cleveland, A. A. Khan 
Address: Auburn, AL 36849  
Affiliation: Department of Computer Science and Engineering Auburn University  
Abstract: Multi-processor scheduling is an NP-hard problem, so heuristic solutions must be found. Several critical path heuristics have been proposed, but these must be empirically tested to see if their solutions are adequate. This paper examines the performance of four critical path heuristics and finds that they are unable to perform well on the types of graphs which are likely to be created by compilers for parallel machines. The cause for this failure is examined. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. </author> <title> Sarkar , Partitioning and scheduling parallel programs for execution on multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: A sequential program is commonly represented as a Program Dependence Graph (PDG) which is a directed acyclic graph (DAG) with node and edge weights <ref> [1] </ref>. Each vertex in a PDG denotes a task and is associated with weight, which is its processing time. Each edge denotes the precedence relation between the two tasks, and the weight of the edge represents the communication cost incurred if the two tasks are assigned to different processors. <p> The partitioning and assignment are known as the scheduling problem. The problem is also called grain size determination [2], the clustering problem [3, 4], and internalization pre-pass <ref> [1] </ref>. The partitioning/scheduling problem is intractable, and heuristics are required to find sub-optimal solutions. As the result, there are no performance guarantees for scheduling heuristics for general graphs. Many researchers have proposed algorithms, but little work has been done to determine the performance of the resulting schedules. <p> The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path. <p> Section 6 looks at the reasons critical path heuristics fail, and section 7 examines expected performance on real machines, sumarizing the implications of our findings. 2 Problem Definition and Assumptions The problem of parallelizing the PDG to achieve minimal parallel time is NP-complete <ref> [1] </ref>. This process usually involves two steps that are either treated as distince parts of the algorithm or are combined in a single step: partitioning and scheduling. Partitioning combines tasks from the PDG into groups of tasks called grains to be executed on the same processor. <p> The algorithm also looks ahead to the start times of a node's children when selecting a processor.[13] 4 The Experiment For parallel programs, granularity as defined by Sarkar in <ref> [1] </ref> is: the average size of a sequential unit of computation in the program. This does not consider the weight of the communication between the tasks.
Reference: [2] <author> B. Kruatrachue and T. Lewis, </author> <title> "Grain Size Determination for Parallel Processing," </title> <journal> IEEE Software, </journal> <pages> pages 23-32, </pages> <month> Jan </month> <year> 1988. </year>
Reference-contexts: Given a PDG, the graph is partitioned into appropriately sized groups of nodes (grain) which are assigned to processors of a parallel machine. The partitioning and assignment are known as the scheduling problem. The problem is also called grain size determination <ref> [2] </ref>, the clustering problem [3, 4], and internalization pre-pass [1]. The partitioning/scheduling problem is intractable, and heuristics are required to find sub-optimal solutions. As the result, there are no performance guarantees for scheduling heuristics for general graphs. <p> A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled. <p> Extending the list scheduling heuristic in classical scheduling [6], these algorithms use greedy heuristics and schedule tasks in a certain order. Task duplications have been used in <ref> [2, 14, 18] </ref> to reduce the communication costs. * Graph decomposition method [20, 21]: Based on graph decomposition theory, the method parses a graph into a hierarchy (tree) of subgraphs representing the independent/precedence relationship among groups of tasks. <p> The architecture is a network of an arbitrary number of homogeneous processors. 3. Duplication of tasks in separate grains is not allowed. Several heuristics <ref> [2, 14, 18] </ref> have been developed that take advantage of this option, but duplication adds additional complexity to an already intractable problem that none of our competing methods use. 4. Tasks communicate only before starting and after completing their execution. 5.
Reference: [3] <author> S. Kim and J. Browne, </author> <title> "A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures," </title> <booktitle> Proceedings of 1988 IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 1-8. </pages> <year> IEEE,1988. </year>
Reference-contexts: Given a PDG, the graph is partitioned into appropriately sized groups of nodes (grain) which are assigned to processors of a parallel machine. The partitioning and assignment are known as the scheduling problem. The problem is also called grain size determination [2], the clustering problem <ref> [3, 4] </ref>, and internalization pre-pass [1]. The partitioning/scheduling problem is intractable, and heuristics are required to find sub-optimal solutions. As the result, there are no performance guarantees for scheduling heuristics for general graphs. <p> The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path.
Reference: [4] <author> W. Yu, </author> <title> LU Decomposition on a Multiprocessing System with Communication Delay. </title> <type> PhD thesis, </type> <institution> University of California, Berkley, </institution> <year> 1984. </year>
Reference-contexts: Given a PDG, the graph is partitioned into appropriately sized groups of nodes (grain) which are assigned to processors of a parallel machine. The partitioning and assignment are known as the scheduling problem. The problem is also called grain size determination [2], the clustering problem <ref> [3, 4] </ref>, and internalization pre-pass [1]. The partitioning/scheduling problem is intractable, and heuristics are required to find sub-optimal solutions. As the result, there are no performance guarantees for scheduling heuristics for general graphs. <p> The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path.
Reference: [5] <author> E. Coffman, </author> <title> Computer and Job-Shop Scheduling Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year>
Reference-contexts: Since neither analytic nor experimental results are known, the goal of this paper is to study some promising techniques, experimentally observe their performance and analze their strengths and weaknesses. In classical scheduling, communication costs are not considered <ref> [5, 6] </ref>. Introducing the communication cost is necessary because communication between processors does take considerable time in real parallel systems, especially in distributed memory systems where communication costs tend to be high relative to processor speed.
Reference: [6] <author> R. Graham, E. Lawer, J. Lenstra, and A. Rinnooy Kan, </author> <title> "Optimization and Approximation in Deterministic Sequencing and Scheduling," </title> <journal> SIAM Journal of Applied Math., </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: Since neither analytic nor experimental results are known, the goal of this paper is to study some promising techniques, experimentally observe their performance and analze their strengths and weaknesses. In classical scheduling, communication costs are not considered <ref> [5, 6] </ref>. Introducing the communication cost is necessary because communication between processors does take considerable time in real parallel systems, especially in distributed memory systems where communication costs tend to be high relative to processor speed. <p> For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled. Extending the list scheduling heuristic in classical scheduling <ref> [6] </ref>, these algorithms use greedy heuristics and schedule tasks in a certain order. <p> Gerasoulis et. al. [10] call graphs with granularity &gt; 1 coarse grained. It is proven that for any coarse grained graph, any list scheduling heuristic gives a schedule within a factor of 2 of that of the optimal schedule <ref> [6] </ref>.
Reference: [7] <author> C. Papadimitrou and J. Ullman, </author> <title> "A Communication Time Tradeoff,". </title> <journal> SIAM Journal of Computing, </journal> <volume> 16(4) </volume> <pages> 639-646, </pages> <month> August </month> <year> 1976. </year>
Reference-contexts: The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism <ref> [7] </ref>. Many researchers have studied the problem and proposed solutions. <p> A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled.
Reference: [8] <author> A. Gerasoulis and T. Yang, </author> <title> "A Comparison of Clustering Heuristics for Scheduling DAGs on Multiprocessors," </title> <booktitle> The Proceedings of the ACM 1990 International Supercomputing Conference. Association of Computing Machinery, </booktitle> <year> 1990. </year>
Reference-contexts: The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path. <p> Paths are shortened by removing communication requirements (zeroing edges) and combining the adjacent tasks into a grain. This approach has received the most attention. A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang <ref> [8] </ref>. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics [2, 7, 14, 15, 16, 17, 18]: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme.
Reference: [9] <author> T. Yang and A. Gerasoulis, </author> <title> "A Fast Scheduling Algorithm for DAGs on an Unbounded Number of Processors," </title> <booktitle> The Proceedings of the 5th ACM Internation Conference on Supercomputing. Association of Computing Machinery, </booktitle> <year> 1991. </year>
Reference: [10] <author> A. Gerasoulis, S. Vengopal, and T. Yang, </author> <title> "Clustering Task Graphs for Message Passing Architectures," </title> <booktitle> Proceedings of ACM Interational Conference on Supercomputing, </booktitle> <pages> pages 447-456, </pages> <year> 1990. </year>
Reference-contexts: The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path. <p> This does not consider the weight of the communication between the tasks. To take into account of the edge weights, we follow the concept of granularity of a parallel program as proposed by Gerasoulis and Yang <ref> [10] </ref> to be the ratio of the computation cost to communication cost. Definition: granularity of a weighted DAG is the average ratio of node weight to maximum adjacent outgoing edge weight. <p> For granularities less than 1, more efficient schedules are produced when PDG nodes are combined more frequently to produce a clustered granularity greater than 1. When granularity is high, more parallelism can be extracted from the PDG. Gerasoulis et. al. <ref> [10] </ref> call graphs with granularity &gt; 1 coarse grained. It is proven that for any coarse grained graph, any list scheduling heuristic gives a schedule within a factor of 2 of that of the optimal schedule [6].
Reference: [11] <author> M. Wu and D. Gadjski, "Hypertool: </author> <title> A Programming Aid for Message-Passing Systems," </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 101-119, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The challenge in the extended scheduling problem is to consider the trade-off between communication time and degree of parallelism [7]. Many researchers have studied the problem and proposed solutions. Based on the techniques employed, the earlier methods can be classified into the following three categories: * Critical path heuristics <ref> [1, 3, 4, 8, 10, 11] </ref>: For DAGs with edge weights and node weights, a path weight is defined to be the sum of the weights of both nodes and edges on the path. <p> The objective function of all the heuristics is to minimize parallel time. 3 The Heuristics This section introduces four critical path heuristics; the dominant sequence clustering (DSC) [22] algorithm by Gerasoulis and Yang, the modified critical path (MCP) <ref> [11] </ref> algorithm by Wu and Gajski, the Hu algorithm as modified by Lewis and El-Rewini [19], and the dynamic critical path (DCP) algorithm by Kwok and Ahmad [13].
Reference: [12] <author> T. Hu, </author> <title> "Parallel Sequencing and Assembly Line Problems," </title> <journal> Operations Research, </journal> <volume> 9 </volume> <pages> 841-848, </pages> <year> 1961. </year>
Reference-contexts: A critical path is a path of greatest weight from a source node to a sink node. Extending the critical path method due to Hu <ref> [12] </ref> in classical scheduling, these algorithms try to shorten the longest execution path in the DAG. Paths are shortened by removing communication requirements (zeroing edges) and combining the adjacent tasks into a grain. This approach has received the most attention.
Reference: [13] <author> Kwok Yu-Kwong and Ahmad Ishfaq, </author> <title> "A Static Scheduling Algorithm Using Dynamic Critical Path For Assigning Parallel Algorithms Onto Multiprocessors," </title> <booktitle> Proceedings of the 1994 International Conference on Parallel processing, </booktitle> <year> 1994. </year>
Reference-contexts: This section introduces four critical path heuristics; the dominant sequence clustering (DSC) [22] algorithm by Gerasoulis and Yang, the modified critical path (MCP) [11] algorithm by Wu and Gajski, the Hu algorithm as modified by Lewis and El-Rewini [19], and the dynamic critical path (DCP) algorithm by Kwok and Ahmad <ref> [13] </ref>. In critical path techniques, partitions are formed by examining the current critical path, zeroing an edge (combining the incident nodes into a cluster) in that path and repeating the process on the new critical path.
Reference: [14] <author> H. Anger, J. Hwang, and Y. C. Chow, </author> <title> "Scheduling with Sufficient Loosely Coupled Processors," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 87-92, </pages> <year> 1990. </year>
Reference-contexts: A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled. <p> Extending the list scheduling heuristic in classical scheduling [6], these algorithms use greedy heuristics and schedule tasks in a certain order. Task duplications have been used in <ref> [2, 14, 18] </ref> to reduce the communication costs. * Graph decomposition method [20, 21]: Based on graph decomposition theory, the method parses a graph into a hierarchy (tree) of subgraphs representing the independent/precedence relationship among groups of tasks. <p> The architecture is a network of an arbitrary number of homogeneous processors. 3. Duplication of tasks in separate grains is not allowed. Several heuristics <ref> [2, 14, 18] </ref> have been developed that take advantage of this option, but duplication adds additional complexity to an already intractable problem that none of our competing methods use. 4. Tasks communicate only before starting and after completing their execution. 5.
Reference: [15] <author> J. Hwang, Y. Chow, F. Anger, and B. Lee, </author> <title> "Scheduling Precedence Graphs in Systems with Interprocessor Communication Times," </title> <journal> SIAM Journal of Computing, </journal> <volume> 18(2) </volume> <pages> 1-8, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled.
Reference: [16] <author> C. Lee, J. Hwang, Y. Chow, and F. </author> <title> Anger, "Multiprocessor Scheduling with Interprocessor Communication Delays," </title> <journal> Operations Research Letters, </journal> <volume> 7(3) </volume> <pages> 141-147, </pages> <year> 1988. </year>
Reference-contexts: A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled.
Reference: [17] <author> Z. Liu, </author> <title> "A Note on Graham's Bound," </title> <journal> Information Processing Letters, </journal> <volume> 36 </volume> <pages> 1-5, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled.
Reference: [18] <author> H. El-Rewini and T. Lewis, </author> <title> "Scheduling Parallel Program Tasks onto Arbitrary Target Machines," </title> <journal> IEEE Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 138-153, </pages> <year> 1990. </year>
Reference-contexts: A taxonomy of these techniques as well as a comparison of four specific heuristics can be found in the work of Gerasoulis and Yang [8]. In this paper we include experimental results from four critical path algorithms, DSC, DCP, MCP and HU. * List scheduling heuristics <ref> [2, 7, 14, 15, 16, 17, 18] </ref>: These algorithms assign priorities to the tasks and schedule them according to a list priority scheme. For example, a high priority might be given to a task with many heavily weighted incident edges or to a task whose neighbors have already been scheduled. <p> Extending the list scheduling heuristic in classical scheduling [6], these algorithms use greedy heuristics and schedule tasks in a certain order. Task duplications have been used in <ref> [2, 14, 18] </ref> to reduce the communication costs. * Graph decomposition method [20, 21]: Based on graph decomposition theory, the method parses a graph into a hierarchy (tree) of subgraphs representing the independent/precedence relationship among groups of tasks. <p> The architecture is a network of an arbitrary number of homogeneous processors. 3. Duplication of tasks in separate grains is not allowed. Several heuristics <ref> [2, 14, 18] </ref> have been developed that take advantage of this option, but duplication adds additional complexity to an already intractable problem that none of our competing methods use. 4. Tasks communicate only before starting and after completing their execution. 5.
Reference: [19] <author> H. El-Rewini and T. Lewis, "Parallex: </author> <title> A Tool for Parallel Program Scheduling," </title> <journal> IEEE Journal of Parallel and Distributed Computing, </journal> <year> 1990. </year>
Reference-contexts: of all the heuristics is to minimize parallel time. 3 The Heuristics This section introduces four critical path heuristics; the dominant sequence clustering (DSC) [22] algorithm by Gerasoulis and Yang, the modified critical path (MCP) [11] algorithm by Wu and Gajski, the Hu algorithm as modified by Lewis and El-Rewini <ref> [19] </ref>, and the dynamic critical path (DCP) algorithm by Kwok and Ahmad [13]. In critical path techniques, partitions are formed by examining the current critical path, zeroing an edge (combining the incident nodes into a cluster) in that path and repeating the process on the new critical path. <p> Hu obtains a priority by finding the level. All nodes that have no unscheduled predecessors are put in a free list in decreasing priority order. The top node from the free node list is then scheduled after all of its predecessors scheduled are put into the free list. <ref> [19] </ref> 3.4 Dynamic Critical Path DCP by Kwok and Ahmad determines the order of node evaluation dynamically by finding the critical path after each step in scheduling.
Reference: [20] <author> C. McCreary and H. Gill, </author> <title> "Automatic Determination of Grain Size for Efficient Parallel Processing," </title> <journal> Communications of the ACM, </journal> <volume> 32(9) </volume> <pages> 1073-1078, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Extending the list scheduling heuristic in classical scheduling [6], these algorithms use greedy heuristics and schedule tasks in a certain order. Task duplications have been used in [2, 14, 18] to reduce the communication costs. * Graph decomposition method <ref> [20, 21] </ref>: Based on graph decomposition theory, the method parses a graph into a hierarchy (tree) of subgraphs representing the independent/precedence relationship among groups of tasks. Communication and execution costs are applied to the tree to determine the grain size that results in the most efficient schedule.
Reference: [21] <author> C. McCreary, J. Thompson, H. Gill, T. Smith, and Y. Zhu, </author> <title> "Partitioning and Scheduling Using Graph Decomposition," </title> <institution> Department of Computer Science and Engineering CSE-93-06, Auburn University, </institution> <year> 1993. </year>
Reference-contexts: Extending the list scheduling heuristic in classical scheduling [6], these algorithms use greedy heuristics and schedule tasks in a certain order. Task duplications have been used in [2, 14, 18] to reduce the communication costs. * Graph decomposition method <ref> [20, 21] </ref>: Based on graph decomposition theory, the method parses a graph into a hierarchy (tree) of subgraphs representing the independent/precedence relationship among groups of tasks. Communication and execution costs are applied to the tree to determine the grain size that results in the most efficient schedule.
Reference: [22] <author> T. Yang and A. Gerasoulis, </author> <title> "A Fast Scheduling Algorithm for DAGs on an Unbounded Number of Processors," </title> <booktitle> The Proceedings of the 5th ACM International Conference on Supercomputing, </booktitle> <pages> pages 633-642. </pages> <institution> Association of Computing Machinery, </institution> <year> 1991. </year>
Reference-contexts: Tasks may send and receive data in parallel and may multicast messages to more than one receiver. 7. The objective function of all the heuristics is to minimize parallel time. 3 The Heuristics This section introduces four critical path heuristics; the dominant sequence clustering (DSC) <ref> [22] </ref> algorithm by Gerasoulis and Yang, the modified critical path (MCP) [11] algorithm by Wu and Gajski, the Hu algorithm as modified by Lewis and El-Rewini [19], and the dynamic critical path (DCP) algorithm by Kwok and Ahmad [13]. <p> The heuristics differ in their method of selecting the edge (s) to be zeroed and identifying the new dominant sequence. The detailed algorithms are found in the Appendix. 3.1 Dominant Sequence Clustering The two major goals of DSC <ref> [22] </ref> are to directly attempt to reduce the dominant sequence of the graph, and to reduce the complexity of the algorithm, to O ((v + e) log v).
Reference: [23] <author> T. Dunigan, </author> <title> "Communication Performance of Intel Touch Tone Delta Mesh," </title> <address> ORNL/TM-11983, </address> <month> Jan, </month> <year> 1992. </year>
Reference-contexts: On real systems, it is fair to ask what granularities are expected. For distributed memory multiprocessor systems, the overhead required for message passing can be measured and there are several studies that do just that <ref> [23, 24] </ref>. Dunigan measures round-trip time for 1000 iterations and computes one-hop and six-hop data rates on three distribute memory machines, the mesh-connected touchsone DELTA and two hypercubes, iPSC/860 and Ncube 6400. The hop penalty is negligible on the DELTA and barely significant on the hypercubes.
Reference: [24] <author> C. McCreary, M. McArdle, and J. McCreary, </author> <title> "Hypercube Communication Performance," </title> <booktitle> ISMM Conference on Parallel and Distributed Computing and Systems, </booktitle> <address> Pittsburgh, </address> <year> 1992. </year>
Reference-contexts: On real systems, it is fair to ask what granularities are expected. For distributed memory multiprocessor systems, the overhead required for message passing can be measured and there are several studies that do just that <ref> [23, 24] </ref>. Dunigan measures round-trip time for 1000 iterations and computes one-hop and six-hop data rates on three distribute memory machines, the mesh-connected touchsone DELTA and two hypercubes, iPSC/860 and Ncube 6400. The hop penalty is negligible on the DELTA and barely significant on the hypercubes.
Reference: [25] <author> C. McCreary and H. Gill, </author> <title> "Efficient Exploitation of Concurrency Using Graph Decomposition," </title> <booktitle> Proceedings 1990 IEEE International Conference on Parallel Processing, pages II-199. IEEE, </booktitle> <year> 1990 </year> <month> 10 </month>
References-found: 25

