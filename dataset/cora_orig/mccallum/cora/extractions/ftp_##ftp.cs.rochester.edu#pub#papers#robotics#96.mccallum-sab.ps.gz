URL: ftp://ftp.cs.rochester.edu/pub/papers/robotics/96.mccallum-sab.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00134.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mccallum@cs.rochester.edu  
Phone: (716) 275-2527 FAX: (716) 461-2018  
Title: Learning to Use Selective Attention and Short-Term Memory in Sequential Tasks  
Author: Andrew Kachites McCallum 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: This paper presents U-Tree, a reinforcement learning algorithm that uses selective attention and short-term memory to simultaneously address the intertwined problems of large perceptual state spaces and hidden state. By combining the advantages of work in instance-based (or memory-based) learning and work with robust statistical tests for separating noise from task structure, the method learns quickly, creates only task-relevant state distinctions, and handles noise well. U-Tree uses a tree-structured representation, and is related to work on Prediction Suffix Trees [ Ron et al., 1994 ] , Parti-game [ Moore, 1993 ] , G-algorithm [ Chap-man and Kaelbling, 1991 ] , and Variable Resolution Dynamic Programming [ Moore, 1991 ] . It builds on Utile Suffix Memory [ McCallum, 1995c ] , which only used short-term memory, not selective perception. The algorithm is demonstrated solving a highway driving task in which the agent weaves around slower and faster traffic. The agent uses active perception with simulated eye movements. The environment has hidden state, time pressure, stochasticity, over 21,000 world states and over 2,500 percepts. From this environment and sensory system, the agent uses a utile distinction test to build a tree that represents depth-three memory where necessary, and has just 143 internal statesfar fewer than the 2500 3 states that would have resulted from a fixed-sized history-window ap proach.
Abstract-found: 1
Intro-found: 1
Reference: [ Agre and Chapman, 1987 ] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> an implementation of a theory of activity. </title> <booktitle> In AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference: [ Ballard et al., 1996 ] <author> D. H. Ballard, M. M. Hayhoe, P. K. Pook, and R. Rao. </author> <title> Deictic codes for the embodiment of cognition. </title> <journal> Brain and Behavioral Sciences, </journal> <note> 1996. [To appear earlier version available as National Resource Laboratory for the study of Brain and Behavior TR95.1, January 1995, </note> <institution> U. of Rochester]. </institution>
Reference: [ Bellman, 1957 ] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: We continue down the tree until a leaf is reached. For this leaf, s: T (s) T (s) [ fT t g (4) 3. For each step in the world, the agent does one step of value iteration <ref> [ Bellman, 1957 ] </ref> , with the leaves of the tree as states. 3 2 If one is concerned about the size of the instance chain, we can limit its growth by simply discarding the oldest instance before adding each new instance, once some reasonably sized limit has been reached.
Reference: [ Chapman and Kaelbling, 1991 ] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Learning from delayed reinforcement in a complex domain. </title> <booktitle> In Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: Ideas from four algorithms in particular inspired the workings of U-Tree; these are: Probabilistic Suffix Tree Learning [ Ron et al., 1994 ] , Parti-game [ Moore, 1993 ] , G-algorithm <ref> [ Chapman and Kaelbling, 1991 ] </ref> and Variable Resolution Dynamic Programming [ Moore, 1991 ] . All four of the algorithms use trees to represent distinctions and grow the trees in order to learn finer distinctions. Probabilistic Suffix Tree.
Reference: [ Chapman, 1989 ] <author> David Chapman. </author> <title> Penguins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10(4) </volume> <pages> 45-50, </pages> <year> 1989. </year>
Reference-contexts: or 1 This paper does not provide an introduction to the basics of reinforcement learning; see instead [ Kaelbling et al., 1995 ] or [ McCallum, 1995a ] . der to separate noise from task structure and keep only those short-term memories that help predict reward. (3) Like the G-algorithm <ref> [ Chapman, 1989 ] </ref> , the agent can select which individual features, or dimensions of perception, to attend to.
Reference: [ Drescher, 1991 ] <author> Gary Drescher. </author> <title> Made-up Minds: A Constructivist Approach to Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: However, a disadvantage of VRDP is that is it partitions the state space into high resolution everywhere the agent visits, potentially making many irrelevant distinctions. U-Tree, on the other hand, creates only utile distinctions. U-Tree is also related to Drescher's Schema Mechanism <ref> [ Drescher, 1991 ] </ref> , in that both build complex representations from simpler primitives using covariance statistics.
Reference: [ Kaelbling et al., 1995 ] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> An introduction to reinforcement learning. </title> <editor> In Luc Steels, editor, </editor> <booktitle> The Biology and Technology of Autonomous Systems. </booktitle> <publisher> Elsevier, </publisher> <year> 1995. </year>
Reference-contexts: in order to learn quickly and discover multi-element feature conjunctions easily. (2) Like Utile Distinction Memory [ McCallum, 1993 ] , the algorithm uses a robust statistical technique, a utile distinction test in or 1 This paper does not provide an introduction to the basics of reinforcement learning; see instead <ref> [ Kaelbling et al., 1995 ] </ref> or [ McCallum, 1995a ] . der to separate noise from task structure and keep only those short-term memories that help predict reward. (3) Like the G-algorithm [ Chapman, 1989 ] , the agent can select which individual features, or dimensions of perception, to attend
Reference: [ McCallum, 1993 ] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In The Proceedings of the Tenth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: to combine the advantages of several previous algorithms: (1) Like Parti-game [ Moore, 1993 ] and Nearest Sequence Memory [ McCallum, 1995b ] , the algorithm is instance-based, making efficient use of raw experience in order to learn quickly and discover multi-element feature conjunctions easily. (2) Like Utile Distinction Memory <ref> [ McCallum, 1993 ] </ref> , the algorithm uses a robust statistical technique, a utile distinction test in or 1 This paper does not provide an introduction to the basics of reinforcement learning; see instead [ Kaelbling et al., 1995 ] or [ McCallum, 1995a ] . der to separate noise from <p> When a split is made, the agent starts the split test again to look for further possible splits that the new distinction may bring to light. 4 Related Work U-Tree, like Utile Suffix Memory, inherits much of its technique and desired features from Utile Distinction Memory <ref> [ McCallum, 1993 ] </ref> and Nearest Sequence Memory [ McCal-lum, 1995b ] , but many of its ideas come from the combination of other algorithms too.
Reference: [ McCallum, 1995a ] <author> Andrew Kachites McCallum. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: multi-element feature conjunctions easily. (2) Like Utile Distinction Memory [ McCallum, 1993 ] , the algorithm uses a robust statistical technique, a utile distinction test in or 1 This paper does not provide an introduction to the basics of reinforcement learning; see instead [ Kaelbling et al., 1995 ] or <ref> [ McCallum, 1995a ] </ref> . der to separate noise from task structure and keep only those short-term memories that help predict reward. (3) Like the G-algorithm [ Chapman, 1989 ] , the agent can select which individual features, or dimensions of perception, to attend to. <p> By using a fringe-depth greater than one, U-Tree can successfully discover useful epistatic combinations of features, in the form of multi-term conjunctions. 3 Details of the Algorithm This section describes the steps of the U-Tree algorithm in more detail. Still further description of U-Tree can be found in <ref> [ McCallum, 1995a ] </ref> . 3.1 Notation The interaction between the agent and its environment is described by actions, rewards, and observations. <p> This represents a 32 percent improvement of my hand-coded policy, and a 91 percent improvement over the random policy. The learned policy has 51 leaves. The full policy tree created after 10,000 steps can be found in <ref> [ McCallum, 1995a ] </ref> . Features that are not relevant to the task, like color, for example, are never used to distinguish states. Not only does the agent perform better than the policy I wrote, it also seems to drive much more aggressively. <p> For more detailed explanation and discussion of all these ideas, see <ref> [ McCallum, 1995a ] </ref> . Acknowledgments This work has benefited from discussions with many colleagues, including: Dana Ballard, Andrew Moore, Leslie Kaelbling, and Jonas Karlsson. This material is based upon work supported by NSF under Grant no. IRI-8903582 and by NIH/PHS under Grant no. 1 R24 RR06853-02.
Reference: [ McCallum, 1995b ] <author> R. Andrew McCallum. </author> <title> Instance-based state identification for reinforcement learning. </title> <booktitle> In Advances of Neural Information Processing Systems (NIPS 7), </booktitle> <pages> pages 377-384, </pages> <year> 1995. </year>
Reference-contexts: short-term memory to augment a perceptual state space that is missing crucial features due to hidden state. 2.1 U-Tree Ancestors The algorithm can be seen as the result of an effort to combine the advantages of several previous algorithms: (1) Like Parti-game [ Moore, 1993 ] and Nearest Sequence Memory <ref> [ McCallum, 1995b ] </ref> , the algorithm is instance-based, making efficient use of raw experience in order to learn quickly and discover multi-element feature conjunctions easily. (2) Like Utile Distinction Memory [ McCallum, 1993 ] , the algorithm uses a robust statistical technique, a utile distinction test in or 1 This
Reference: [ McCallum, 1995c ] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning. </title> <booktitle> In The Proceedings of the Twelfth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: U-Tree is a direct descendant of Utile Suffix Memory (USM) <ref> [ McCallum, 1995c ] </ref> , which also incorporates techniques (1) and (2) above; the new feature of U-Tree is it ability to divide a percept into components, to choose to ignore some of those components, and thus to perform selective perception. 2.2 How it Works U-Tree uses two key structures: a
Reference: [ Moore and Atkeson, 1993 ] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances of Neural Information Processing Systems (NIPS 5). </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Additionally this can help the agent deal with a changing environment. 3 If computational limitations or the number of states makes full value iteration too expensive, we can instead do Prioritized Sweeping <ref> [ Moore and Atkeson, 1993 ] </ref> , Q-DYNA [ Peng and Williams, 1992 ] or even Q-learning [ Watkins, 1989 ] .
Reference: [ Moore, 1991 ] <author> Andrew W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 333-337, </pages> <year> 1991. </year>
Reference-contexts: Ideas from four algorithms in particular inspired the workings of U-Tree; these are: Probabilistic Suffix Tree Learning [ Ron et al., 1994 ] , Parti-game [ Moore, 1993 ] , G-algorithm [ Chapman and Kaelbling, 1991 ] and Variable Resolution Dynamic Programming <ref> [ Moore, 1991 ] </ref> . All four of the algorithms use trees to represent distinctions and grow the trees in order to learn finer distinctions. Probabilistic Suffix Tree. U-Tree has more in common with Probabilistic Suffix Tree Learning (PSTL) than any other algorithm.
Reference: [ Moore, 1993 ] <author> Andrew W. Moore. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> In Advances of Neural Information Processing Systems (NIPS 6), </booktitle> <pages> pages 711-718. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: large perceptual state space, and it also uses short-term memory to augment a perceptual state space that is missing crucial features due to hidden state. 2.1 U-Tree Ancestors The algorithm can be seen as the result of an effort to combine the advantages of several previous algorithms: (1) Like Parti-game <ref> [ Moore, 1993 ] </ref> and Nearest Sequence Memory [ McCallum, 1995b ] , the algorithm is instance-based, making efficient use of raw experience in order to learn quickly and discover multi-element feature conjunctions easily. (2) Like Utile Distinction Memory [ McCallum, 1993 ] , the algorithm uses a robust statistical technique, <p> Ideas from four algorithms in particular inspired the workings of U-Tree; these are: Probabilistic Suffix Tree Learning [ Ron et al., 1994 ] , Parti-game <ref> [ Moore, 1993 ] </ref> , G-algorithm [ Chapman and Kaelbling, 1991 ] and Variable Resolution Dynamic Programming [ Moore, 1991 ] . All four of the algorithms use trees to represent distinctions and grow the trees in order to learn finer distinctions. Probabilistic Suffix Tree.
Reference: [ Peng and Williams, 1992 ] <author> Jing Peng and R. J. Williams. </author> <title> Efficient learning and planning within the Dyna framework. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <year> 1992. </year>
Reference-contexts: Additionally this can help the agent deal with a changing environment. 3 If computational limitations or the number of states makes full value iteration too expensive, we can instead do Prioritized Sweeping [ Moore and Atkeson, 1993 ] , Q-DYNA <ref> [ Peng and Williams, 1992 ] </ref> or even Q-learning [ Watkins, 1989 ] . However, the small number of agent internal states created by U-Tree's utile distinction nature often makes value iteration feasible where it would not have previously been possible with typically exponential fixed-granularity state space divisions.
Reference: [ Quinlan, 1995 ] <author> J. R. Quinlan. </author> <title> MDL and categorical theories (continued). </title> <booktitle> In The Proceedings of the Twelfth International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: noisy perception better by explicitly representing it using techniques from probabilistic decision trees. (4) We could perform more efficient exploration by keeping exploration statistics in the nodes of the fringe. (5) We could use information-theoretic techniques, such as MDL, to aid the search for good combinations of splits; for example <ref> [ Quinlan, 1995 ] </ref> . (6) We could avoid the use of a fringe by first clustering instances in reward space, then searching for features that separate the clusters. For more detailed explanation and discussion of all these ideas, see [ McCallum, 1995a ] .
Reference: [ Ron et al., 1994 ] <author> Dana Ron, Yoram Singer, and Naftali Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proceedings Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: All the instances in a cluster are used together to calculate utility estimates for that state. The structure that controls this clustering is a modified version of a finite state machine called Prediction Suffix Tree (PST) <ref> [ Ron et al., 1994 ] </ref> . A PST can be thought of as an order-n Markov model, with varying n in different parts of state space. U-Tree modifies (squares). <p> Ideas from four algorithms in particular inspired the workings of U-Tree; these are: Probabilistic Suffix Tree Learning <ref> [ Ron et al., 1994 ] </ref> , Parti-game [ Moore, 1993 ] , G-algorithm [ Chapman and Kaelbling, 1991 ] and Variable Resolution Dynamic Programming [ Moore, 1991 ] . All four of the algorithms use trees to represent distinctions and grow the trees in order to learn finer distinctions.
Reference: [ Ullman, 1984 ] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year> <title> (Also in: Visual Cognition, </title> <editor> S. Pinker ed., </editor> <year> 1985). </year>
Reference: [ Watkins, 1989 ] <author> Chris Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: Additionally this can help the agent deal with a changing environment. 3 If computational limitations or the number of states makes full value iteration too expensive, we can instead do Prioritized Sweeping [ Moore and Atkeson, 1993 ] , Q-DYNA [ Peng and Williams, 1992 ] or even Q-learning <ref> [ Watkins, 1989 ] </ref> . However, the small number of agent internal states created by U-Tree's utile distinction nature often makes value iteration feasible where it would not have previously been possible with typically exponential fixed-granularity state space divisions.
References-found: 19

