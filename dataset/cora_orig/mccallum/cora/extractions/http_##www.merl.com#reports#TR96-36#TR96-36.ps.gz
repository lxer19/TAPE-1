URL: http://www.merl.com/reports/TR96-36/TR96-36.ps.gz
Refering-URL: http://www.merl.com/reports/TR96-36/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jbt@psyche.mit.edu  freeman@merl.com  
Author: Joshua B. Tenenbaum William T. Freeman 
Address: Cambridge, MA 02139  201 Broadway Cambridge, MA 02139  
Affiliation: Dept. of Brain and Cognitive Sciences Massachusetts Institute of Technology  MERL, Mitsubishi Electric Res. Lab.  
Note: in Adv. in Neural Info. Proc. Systems, volume 9, MIT Press, 1997.  
Abstract: We seek to analyze and manipulate two factors, which we generically call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor structure. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMs), which generalizes earlier work on factorial mixture models [7, 3]. Significant performance improvement on a benchmark speech dataset shows the benefits of our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Caruana. </author> <title> Learning many related tasks at the same time with backpropagation. </title> <booktitle> In Adv. in Neural Info. Proc. Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 657-674, </pages> <year> 1995. </year>
Reference-contexts: Our work naturally combines two current themes in the connectionist learning literature: factorial learning [7, 3] and learning a family of many related tasks <ref> [1, 12] </ref> to fonts at left. The test data were all the Monaco letters except those shown at right. The synthesized Monaco letters compare well with the missing ones. facilitate task transfer.
Reference: [2] <author> W. T. Freeman and J. B. Tenenbaum. </author> <title> Learning bilinear models for two-factor problems in vision. </title> <type> TR 96-37, </type> <address> MERL, 201 Broadway, Cambridge, MA 02139, </address> <year> 1996. </year>
Reference-contexts: This section focuses on the asymmetric model and its use in extrapolation and classification. Training and adaptation procedures for the symmetric model are similar and based on algorithms in [10, 11]. In <ref> [2] </ref>, we describe these procedures and their application to extrapolation and translation tasks. 3.1 Training Let n sc be the number of observations in style s and content c, and let m sc = P be the sum of these observations. <p> Bilinear models offer an easy way to improve performance using style labels which are frequently available for many classification tasks. 5 Pointers to other work and conclusions We discuss the extrapolation and translation problems in <ref> [2] </ref>. Here we summarize results. Figure 3 shows extrapolation of a partially observed font (Monaco) to the unseen letters (see also the gridfont work of [8, 4]). During training, we presented all letters of the five fonts shown at the left.
Reference: [3] <author> Z. Ghahramani. </author> <title> Factorial learning and the EM algorithm. </title> <booktitle> In Adv. in Neural Info. Proc. Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 617-624, </pages> <year> 1995. </year>
Reference-contexts: Such data raises a number of learning problems. Extracting a hidden two-factor structure given only the raw observations has received significant attention <ref> [7, 3] </ref>, but unsupervised factorial learning of this kind has yet to prove tractable for our focus: real-world data with subtly interacting factors. We work in a more supervised setting, where labels for style or content may be available during training or testing. <p> We have used this approach to translate across shape or lighting conditions for images of faces, and to translate across illumination color for color measurements (assuming small specular reflections). Our work naturally combines two current themes in the connectionist learning literature: factorial learning <ref> [7, 3] </ref> and learning a family of many related tasks [1, 12] to fonts at left. The test data were all the Monaco letters except those shown at right. The synthesized Monaco letters compare well with the missing ones. facilitate task transfer.
Reference: [4] <author> I. Grebert, D. G. Stork, R. Keesing, and S. Mims. </author> <title> Connectionist generalization for production: An example from gridfont. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 699-710, </pages> <year> 1992. </year>
Reference-contexts: Here we summarize results. Figure 3 shows extrapolation of a partially observed font (Monaco) to the unseen letters (see also the gridfont work of <ref> [8, 4] </ref>). During training, we presented all letters of the five fonts shown at the left. To accomodate many shape topologies, we described letters by the warps of black particles from a reference shape into the letter shape.
Reference: [5] <author> P. W. Hallinan. </author> <title> A low-dimensional representation of human faces for arbitrary lighting conditions. </title> <booktitle> In Proc. IEEE CVPR, </booktitle> <pages> pages 995-999, </pages> <year> 1994. </year>
Reference-contexts: The empirical success of linear models in many pattern recognition applications with single-factor data (e.g. "eigenface" models of faces under varying identity but constant illumination and pose [15], or under varying illumination but constant identity and pose <ref> [5] </ref> ), makes bilinear models a natural choice when two such factors vary independently across the data set. Also, many of the computationally desirable properties of linear models extend to bilinear models.
Reference: [6] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <journal> IEEE Pat. Anal. Mach. Intell., </journal> (18):607-616, 1996. 
Reference-contexts: Robinson [13] obtained 51% correct vowel classification on the test set with a multi-layer perceptron and 56% with a 1-nearest neighbor (1-NN) classifier, the best performance of the many standard techniques he tried. Hastie and Tibshirani <ref> [6] </ref> recently obtained 62% correct using their discriminant adaptive nearest neighbor algorithm, the best result we know of for an approach that does not model speaker style.
Reference: [7] <author> G. E. Hinton and R. Zemel. Autoencoders, </author> <title> minimum description length, and Helmholtz free energy. </title> <booktitle> In Adv. in Neural Info. Proc. Systems, </booktitle> <volume> volume 6, </volume> <year> 1994. </year>
Reference-contexts: Such data raises a number of learning problems. Extracting a hidden two-factor structure given only the raw observations has received significant attention <ref> [7, 3] </ref>, but unsupervised factorial learning of this kind has yet to prove tractable for our focus: real-world data with subtly interacting factors. We work in a more supervised setting, where labels for style or content may be available during training or testing. <p> We have used this approach to translate across shape or lighting conditions for images of faces, and to translate across illumination color for color measurements (assuming small specular reflections). Our work naturally combines two current themes in the connectionist learning literature: factorial learning <ref> [7, 3] </ref> and learning a family of many related tasks [1, 12] to fonts at left. The test data were all the Monaco letters except those shown at right. The synthesized Monaco letters compare well with the missing ones. facilitate task transfer.
Reference: [8] <author> D. Hofstadter. </author> <title> Fluid Concepts and Creative Analogies. </title> <publisher> Basic Books, </publisher> <year> 1995. </year>
Reference-contexts: We must estimate parameter vectors a s and b c describing style s and content c, respectively, and W , parameters for f that are independent of style and content but govern their interaction. In terms of Fig. 1 (and in the spirit of <ref> [8] </ref>), the model represents what the elements of each row have in common independent of column (a s ), what the elements of each column have in common independent of row (b c ), and what all elements have in common independent of row and column (W ). <p> Here we summarize results. Figure 3 shows extrapolation of a partially observed font (Monaco) to the unseen letters (see also the gridfont work of <ref> [8, 4] </ref>). During training, we presented all letters of the five fonts shown at the left. To accomodate many shape topologies, we described letters by the warps of black particles from a reference shape into the letter shape.
Reference: [9] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The generic bilinear calibration-estimation problem. </title> <note> Intl. J. Comp. Vis., 1997. in press. </note>
Reference-contexts: Finally, we identify A and B as the desired parameter estimates in stacked form (see also <ref> [9, 14] </ref>), A = 6 A 1 A N s 7 fi fl The model dimensionality J can be chosen in various standard ways: by a priori considerations, by requiring a sufficiently good approximation to the data (as measured by mean squared error or some more subjective metric), or by looking
Reference: [10] <author> J. R. Magnus and H. Neudecker. </author> <title> Matrix differential calculus with applications in statistics and econometrics. </title> <publisher> Wiley, </publisher> <year> 1988. </year>
Reference-contexts: Model complexity can be controlled by varying model dimensionality to achieve a compromise between reproduction of the training data and generalization during testing. Finally, the approach extends to multilinear models <ref> [10] </ref>, for data generated by three or more interacting factors. We have explored two bilinear models for Eq. 1. <p> This section focuses on the asymmetric model and its use in extrapolation and classification. Training and adaptation procedures for the symmetric model are similar and based on algorithms in <ref> [10, 11] </ref>. In [2], we describe these procedures and their application to extrapolation and translation tasks. 3.1 Training Let n sc be the number of observations in style s and content c, and let m sc = P be the sum of these observations.
Reference: [11] <author> D. H. Marimont and B. A. Wandell. </author> <title> Linear models of surface and illuminant spectra. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 9(11) </volume> <pages> 1905-1913, </pages> <year> 1992. </year>
Reference-contexts: This section focuses on the asymmetric model and its use in extrapolation and classification. Training and adaptation procedures for the symmetric model are similar and based on algorithms in <ref> [10, 11] </ref>. In [2], we describe these procedures and their application to extrapolation and translation tasks. 3.1 Training Let n sc be the number of observations in style s and content c, and let m sc = P be the sum of these observations.
Reference: [12] <author> S. M. Omohundro. </author> <title> Family discovery. </title> <booktitle> In Adv. in Neural Info. Proc. Sys., </booktitle> <volume> vol. 8, </volume> <year> 1995. </year>
Reference-contexts: Our work naturally combines two current themes in the connectionist learning literature: factorial learning [7, 3] and learning a family of many related tasks <ref> [1, 12] </ref> to fonts at left. The test data were all the Monaco letters except those shown at right. The synthesized Monaco letters compare well with the missing ones. facilitate task transfer.
Reference: [13] <author> A. Robinson. </author> <title> Dynamic error propagation networks. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Dept., </institution> <year> 1989. </year>
Reference-contexts: The data consist of 6 samples of each of 11 vowels uttered by 15 speakers of British English (originally collected by David Deterding, from the CMU neural-bench ftp archive). Each data vector consists of 10 parameters computed from a linear predictive analysis of the digitized speech. Robinson <ref> [13] </ref> compared many learning algorithms trained to categorize vowels from the first 8 speakers (4 male and 4 female) and tested on samples from the remaining 7 speakers (4 male and 3 female). <p> The few training styles makes this problem difficult and a good showcase for our approach. Robinson <ref> [13] </ref> obtained 51% correct vowel classification on the test set with a multi-layer perceptron and 56% with a 1-nearest neighbor (1-NN) classifier, the best performance of the many standard techniques he tried.
Reference: [14] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: a factorization method. </title> <journal> Intl. J. Comp. Vis., </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: Finally, we identify A and B as the desired parameter estimates in stacked form (see also <ref> [9, 14] </ref>), A = 6 A 1 A N s 7 fi fl The model dimensionality J can be chosen in various standard ways: by a priori considerations, by requiring a sufficiently good approximation to the data (as measured by mean squared error or some more subjective metric), or by looking
Reference: [15] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. Cog. Neurosci., </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: These simple models are still complex enough to model subtle interactions of style and content. The empirical success of linear models in many pattern recognition applications with single-factor data (e.g. "eigenface" models of faces under varying identity but constant illumination and pose <ref> [15] </ref>, or under varying illumination but constant identity and pose [5] ), makes bilinear models a natural choice when two such factors vary independently across the data set. Also, many of the computationally desirable properties of linear models extend to bilinear models. <p> To render an image of a particular person in a particular pose, the pose-specific basis vectors are mixed according to the person specific coefficients. Note that the basis vectors for each pose look like eigenfaces <ref> [15] </ref> in the appropriate style of each pose.
References-found: 15

