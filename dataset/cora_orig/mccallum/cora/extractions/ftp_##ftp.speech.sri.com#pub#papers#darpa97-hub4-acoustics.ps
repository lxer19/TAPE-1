URL: ftp://ftp.speech.sri.com/pub/papers/darpa97-hub4-acoustics.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Title: Acoustic Modeling for the SRI Hub4 Partitioned Evaluation Continuous Speech Recognition System  
Author: Ananth Sankar, Larry Heck, and Andreas Stolcke 
Address: Menlo Park, California  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: We describe the developmentof the SRI systemevaluatedin the 1996 DARPA continuous speech recognition (CSR) Hub4 partitioned evaluation (PE). The task for the Hub4 evaluation was to recognize speech from broadcast television and radio shows. Recognizingsuch speech by machines poses many challenges. First, the segments to be recognized could be very long. This introduces a problem in training and recognition because of the consequent increased system memory requirement. A simple segmentation technique is used to break long segments into shorter, more manageable lengths. The speech from broadcast news sources exhibits a variety of difficult acoustic conditions, such as spontaneous speech, band-limited speech, and speech in the presence of noise, music, or background speakers. Such background conditions lead to significant degradation in performance. We describe techniques, based on acoustic adaptation, that adapt recognition models to the different acoustic background conditions, so as to improve recognition performance. We also present a novel algorithm that clusters the test data segments so that the resulting clusters are homogeneous with respect to speakers. This is followed by acoustic adaptation to the individual clusters, resulting in a significant performance improvement. Finally, we briefly describe our studies in language modeling for the Hub4 evaluation which is detailed further in another paper in these proceedings. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> F. Weng, A. Stolcke, and A. Sankar, </author> <title> Hub-4 Language Modeling using Domain Interpolation and Data Clustering, </title> <booktitle> in Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> (Chantilly, VA), </address> <year> 1997. </year>
Reference-contexts: These models are then used to recognize the data in these clusters. In Section 7, we briefly describe our language model (LM) techniques for this evaluation. Our language modeling work is presented in greater detail in another paper in these proceedings <ref> [1] </ref>. We conclude in Section 8 with a summary of our work. 2. A high-level description of the SRI Hub4 PE system For 1996, the Hub4 evaluation was divided into two individual problems. <p> The condition-adapted models were then separately adapted to each test cluster (see Section 5). 5. N-best lists were generated using the models adapted to each test cluster. These lists were then rescored with larger trigram and fourgram LMs <ref> [1] </ref> as described in Section 6. data. The data from other focus conditions were similarly processed, except that the seed models used for adaptation to the F2 data were trained on Switchboard and Macrophone databases. 3. <p> These groups were chosen because the word-error rates were similar within them. The word-error rate with the test-cluster-adapted models and bigram LMs was 37.0%. When the N-best lists were rescored with the same acoustic models but with larger trigram and fourgram interpolated LMs <ref> [1] </ref>, the error rate was 33.1%. This was the best performance we achieved on our development test set. Our error rate on the evaluation test data with this system was 33.3%. <p> A fourgram interpolated language model was trained using these different databases. Our language modeling studies for this evaluation are described in detail in another paper in these proceedings <ref> [1] </ref>. That paper also describes our studies of techniques to adapt the LMs to the acoustic focus conditions (since speaking styles could be correlated with these conditions), and to different topics. 8. Summary and conclusions We have described the SRI system for the 1996 DARPA Hub4 PE. <p> However, adapting to the individual clusters proved to be even more important and gave a 9% improvement over the condition-specific models. This paper focused mainly on the acoustic modeling components of the system. Our language modeling work is described in detail in another paper in these proceedings <ref> [1] </ref>.
Reference: 2. <author> R. Stern, </author> <title> Specification of the 1996 Hub4 Broadcast News Evaluation, </title> <booktitle> in Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> (Chantilly, VA), </address> <year> 1997. </year>
Reference-contexts: Each segment contained speech from a single speaker. In addition, the segments were homogeneous with respect to the acoustic background condition or speech style. The segments were classified into seven different acoustic focus conditions, F0, F1, F2, F3, F4, F5, FX, as described in <ref> [2] </ref>, and the labels were provided for use in the evaluation. The SRI Hub4 PE system used in the 1996 DARPA CSR evaluations involved the following stages in processing the recognition data: 1.
Reference: 3. <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, </author> <title> Large-Vocabulary Dictation Using SRI's DECIPHER(TM) Speech Recognition System: </title> <booktitle> Progressive-Search Techniques, in Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. </pages> <address> II319II322, </address> <year> 1993. </year>
Reference-contexts: For each focus condition, we generated word lattices for each segment by using the lattice generation algorithm described in <ref> [3] </ref>. We used condition-specific acoustic models estimated with the training data for each focus condition, using maximum-likelihood transformation-based adaptation techniques [4, 5, 6].
Reference: 4. <author> V. Digalakis, D. Rtischev, and L. Neumeyer, </author> <title> Speaker Adaptation Using Constrained Reestimation of Gaussian Mixtures, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 357366, </pages> <year> 1995. </year>
Reference-contexts: For each focus condition, we generated word lattices for each segment by using the lattice generation algorithm described in [3]. We used condition-specific acoustic models estimated with the training data for each focus condition, using maximum-likelihood transformation-based adaptation techniques <ref> [4, 5, 6] </ref>. The condition-specific models were estimated by adapting seed models that were trained using either the Wall Street Journal (WSJ) SI-284 database, or the Switchboard and Macro-phone databases (see Section 4). <p> Adaptation during training Since only 35 hours of transcribed training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. <p> training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. <p> We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation [6] and the variances using a variance scaling transform [5, 6]. Other feature-and model-space transformations have also been detailed in our earlier work <ref> [4, 5, 6] </ref>. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. In this approach, a separate matrix affine transform is used to transform the cepstrum, delta cepstrum, and delta-delta cepstrum. <p> As described in previous work <ref> [4, 5, 6] </ref>, the parameters of the transformations are estimated by maximizing the likelihood of adaptation data from the new acoustic environment. Separate transforms are used for different Gaussian clusters as in [4], including a separate transform for the Gaussians corresponding to the silence model. <p> As described in previous work [4, 5, 6], the parameters of the transformations are estimated by maximizing the likelihood of adaptation data from the new acoustic environment. Separate transforms are used for different Gaussian clusters as in <ref> [4] </ref>, including a separate transform for the Gaussians corresponding to the silence model. We have made use of the transforms in Equations 1 through 3, and 4, for the Hub4 PE evaluation system.
Reference: 5. <author> A. Sankar and C.-H. Lee, </author> <title> A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 4, </volume> <pages> pp. 190 202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: For each focus condition, we generated word lattices for each segment by using the lattice generation algorithm described in [3]. We used condition-specific acoustic models estimated with the training data for each focus condition, using maximum-likelihood transformation-based adaptation techniques <ref> [4, 5, 6] </ref>. The condition-specific models were estimated by adapting seed models that were trained using either the Wall Street Journal (WSJ) SI-284 database, or the Switchboard and Macro-phone databases (see Section 4). <p> Adaptation during training Since only 35 hours of transcribed training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. <p> training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. <p> The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation [6] and the variances using a variance scaling transform <ref> [5, 6] </ref>. Other feature-and model-space transformations have also been detailed in our earlier work [4, 5, 6]. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. <p> We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation [6] and the variances using a variance scaling transform [5, 6]. Other feature-and model-space transformations have also been detailed in our earlier work <ref> [4, 5, 6] </ref>. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. In this approach, a separate matrix affine transform is used to transform the cepstrum, delta cepstrum, and delta-delta cepstrum. <p> As described in previous work <ref> [4, 5, 6] </ref>, the parameters of the transformations are estimated by maximizing the likelihood of adaptation data from the new acoustic environment. Separate transforms are used for different Gaussian clusters as in [4], including a separate transform for the Gaussians corresponding to the silence model.
Reference: 6. <author> L. Neumeyer, A. Sankar, and V. Digalakis, </author> <title> A Comparative Study of Speaker Adaptation Techniques, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <pages> pp. 11271130, </pages> <year> 1995. </year>
Reference-contexts: For each focus condition, we generated word lattices for each segment by using the lattice generation algorithm described in [3]. We used condition-specific acoustic models estimated with the training data for each focus condition, using maximum-likelihood transformation-based adaptation techniques <ref> [4, 5, 6] </ref>. The condition-specific models were estimated by adapting seed models that were trained using either the Wall Street Journal (WSJ) SI-284 database, or the Switchboard and Macro-phone databases (see Section 4). <p> Adaptation during training Since only 35 hours of transcribed training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. <p> training data was available for all the focus conditions, we decided that the best strategy to train models for each condition would be to use maximum-likelihood (ML) transformation-based adaptation techniques <ref> [4, 5, 6] </ref> to adapt seed models to each condition. We have previously developed algorithms for ML transformation-based acoustic adaptation [4, 5, 6]. The general idea is to transform the test domain acoustic features or the trained HMM parameters to reduce the mismatch between them. The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. <p> The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation <ref> [6] </ref> and the variances using a variance scaling transform [5, 6]. Other feature-and model-space transformations have also been detailed in our earlier work [4, 5, 6]. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. <p> The parameters of the transformation are estimated by maximizing the likelihood of adaptation data available from the test domain. We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation [6] and the variances using a variance scaling transform <ref> [5, 6] </ref>. Other feature-and model-space transformations have also been detailed in our earlier work [4, 5, 6]. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. <p> We have developed techniques to adapt both the HMM means using a block-diagonal affine transformation [6] and the variances using a variance scaling transform [5, 6]. Other feature-and model-space transformations have also been detailed in our earlier work <ref> [4, 5, 6] </ref>. Full matrix affine transformations of the HMM mean vector have been previously studied [8]. However, we found that the block-diagonal approach was more robust. In this approach, a separate matrix affine transform is used to transform the cepstrum, delta cepstrum, and delta-delta cepstrum. <p> As described in previous work <ref> [4, 5, 6] </ref>, the parameters of the transformations are estimated by maximizing the likelihood of adaptation data from the new acoustic environment. Separate transforms are used for different Gaussian clusters as in [4], including a separate transform for the Gaussians corresponding to the silence model.
Reference: 7. <author> A. Sankar, F. Beaufays, and V. Digalakis, </author> <title> Training Data Clustering for Improved Speech Recognition, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <year> 1995. </year>
Reference-contexts: These recognition hypotheses were later used to adapt the condition adapted models to the test conditions. 4. The test segments for each focus condition were clustered by using an agglomerative clustering algorithm <ref> [7] </ref>. The condition-adapted models were then separately adapted to each test cluster (see Section 5). 5. N-best lists were generated using the models adapted to each test cluster. These lists were then rescored with larger trigram and fourgram LMs [1] as described in Section 6. data. <p> The distance between two clusters was then computed as the maximum distance between segments in the two clusters [9]. In our work, we used a Gaussian mixture model (GMM) to model each test segment. This procedure was previously described by us in <ref> [7] </ref>, but applied to cluster the training data speakers. In the work reported here, we used it to cluster the test data segments.
Reference: 8. <author> C. J. Legetter and P. C. Woodland, </author> <title> Flexible Speaker Adaptation Using Maximum Likelihood Linear Regression, </title> <booktitle> in Proceedings of the Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 110115, </pages> <year> 1995. </year>
Reference-contexts: Other feature-and model-space transformations have also been detailed in our earlier work [4, 5, 6]. Full matrix affine transformations of the HMM mean vector have been previously studied <ref> [8] </ref>. However, we found that the block-diagonal approach was more robust. In this approach, a separate matrix affine transform is used to transform the cepstrum, delta cepstrum, and delta-delta cepstrum.
Reference: 9. <author> R. Duda and P. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: The distance between two clusters was then computed as the maximum distance between segments in the two clusters <ref> [9] </ref>. In our work, we used a Gaussian mixture model (GMM) to model each test segment. This procedure was previously described by us in [7], but applied to cluster the training data speakers. In the work reported here, we used it to cluster the test data segments.
Reference: 10. <author> S. M. Katz, </author> <title> Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 35, no. 3, </volume> <pages> pp. 400401, </pages> <year> 1987. </year>
Reference-contexts: This is summarized in Table 4. Thus, the increase in the vocabulary size gave a small improvement over the system we used for the evaluation. 7. Language models The lattices used by our system were generated by a 20,000-word vocabulary, bigram back-off <ref> [10] </ref> LM trained using the 1996 H4 LM training texts and the transcripts for the H4 acoustic training data provided by LDC and NIST.
References-found: 10

