URL: http://www.cs.princeton.edu/prism/papers-ps/ocean-jpdc.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Finding and Exploiting Parallelism in an Ocean Simulation  
Author: L. Hennessy 
Date: 1, May 1992.  
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Note: John  Journal of Parallel and Distributed Computing, v. 15, no.  
Abstract: Program: Experience, Results and Implications Abstract How programmers will code and compile large application programs for execution on parallel processors is perhaps the biggest challenge facing the widespread adoption of multiprocessing. To gain insight into this problem, an ocean simulation application was converted to a parallel version. The parallel program demonstrated near-linear speed-up on an Encore Multimax, a sixteen processor bus-based shared-memory machine. Parallelizing an existing sequential application| not just a single loop or computational kernel|leads to interesting insights about what issues are significant in the process of finding and implementing parallelism, and what the major challenges are. Three levels of approach to the problem of finding parallelism|loop-level parallelization, program restructuring, and algorithm modification|were attempted, with widely varying results. Loop-level parallelization did not scale sufficiently. High level restructuring was useful for much of the application, but obtaining an efficient parallel program required algorithmic changes to one portion of it. Implementation issues for scalable performance, such as data locality and synchronization, are also discussed. The nature, requirements and success of the various transformations lend insight into the design of parallelizing tools and parallel programming environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amdahl, G.M. </author> <title> Validity of the Single Processor Approach to Achieving Large-scale Computing Capabilities. </title> <booktitle> AFIPS Conference Proceedings, 30 (1967), </booktitle> <pages> pp. 483-489. </pages>
Reference-contexts: If no parallelism is involved in the execution of even the least time-consuming of these routines, the speedup obtainable with an arbitrarily large number of processors will be limited to about 5 <ref> [1] </ref>. The profile output excludes several subroutines from the equation-solving software package, subroutines that we find we can ignore in our parallelization.
Reference: [2] <author> Chandra, R., Gupta, A., and Hennessy, J.L. </author> <title> COOL: A Language for Parallel Programming. </title> <editor> In Gelernter, D., Nicolau, A., and Padua, D. (Eds.),. </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990, </year> <pages> pp. 126-148. </pages>
Reference-contexts: Back substitution steps can be similarly pipelined, and processor utilization thereby increased. The dependence structure of this algorithm has been quite naturally expressed in a concurrent object-oriented language with futures <ref> [2] </ref>. "Shared-column" partitioning complicates the situation, requiring separate but simultaneous synchronization among disjoint subsets of processors. 6.2 The Iterative Method Parallelism in the direct algorithm is clearly limited by the serial section in the work for a column.
Reference: [3] <author> Fox, G., Johnson, M., Lyzenga, G., Otto, S., Salmon, J., and Walker, D. </author> <title> Solving Problems on Concurrent Processors, </title> <journal> Vol. </journal> <volume> 1, </volume> <publisher> Prentice Hall, </publisher> <year> 1988, </year> <note> Chap. 7. </note>
Reference-contexts: ordering, we can justify using the following parallel iterations: In any iteration, all processors start to work on their grid partitions at the same time, ordering their computation as in Eq. (3) but without regard for updates in other partitions. (This method is also mentioned by Fox et al in <ref> [3] </ref>.) Now, the relative timing of the processes within an iteration determines whether an inter-partition boundary element uses old or updated near-neighbour values, and the result is no longer statically predictable.
Reference: [4] <institution> FX/FORTRAN Programmer's Handbook, Alliant Computer Systems Corporation, Littleton, </institution> <address> MA, </address> <month> Jul. </month> <year> 1988. </year>
Reference: [5] <author> Golub, G.H., and Van Loan, </author> <title> C.F. Matrix Computations, Second Edition, </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989, </year> <journal> Chap. </journal> <volume> 10. </volume> <pages> 22 </pages>
Reference-contexts: Others, such as the conjugate gradient method, could have been chosen as well. The latter might provide faster convergence under certain conditions, and are also useful if appropriate values for the parameters needed by some classical methods (such as SOR) are difficult to ascertain <ref> [5] </ref>. In general, finding a parallel iterative algorithm that converges quickly for a given problem, domain size and accuracy is a difficult task.
Reference: [6] <author> Holland, W.R. </author> <title> The Role of Mesoscale Eddies in the General Circulation of the Ocean|Numerical Ex--periments using a Wind-Driven Quasi-Geostrophic Model. </title> <journal> J. Physical Oceanography, </journal> <month> 8 (May </month> <year> 1978), </year> <pages> pp. 363-392. </pages>
Reference: [7] <editor> Lusk, E.L., and Overbeek, R.A. </editor> <title> Use of Monitors in FORTRAN: A Tutorial on the Barrier, Self-scheduling DO-Loop, and Askfor Monitors. </title> <institution> Argonne National Laboratory Tech. </institution> <type> Rep. ANL-84-51, Rev. 1, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <month> Jun. </month> <year> 1987. </year>
Reference-contexts: Restructuring the program, after understanding the algorithms used, to make it more amenable to large grained parallelism. 3. Changing algorithms when the original ones were not efficiently parallelizable. The last two levels were implemented in explicitly parallel FORTRAN programs, using the PARMACS macro package for parallel constructs <ref> [7] </ref>. As we will see, automatic parallelization by a compiler was found to be insufficient and to not scale beyond a few processors.
Reference: [8] <author> Singh, J.P., and Hennessy, J.L. </author> <title> An Empirical Investigation of the Effectiveness and Limitations of Automatic Parallelization. </title> <booktitle> Proc. International Symposium on Shared Memory Multiprocessing, Information Processing Society of Japan, </booktitle> <address> Tokyo, Japan, </address> <month> Apr. </month> <year> 1991, </year> <pages> pp. 25-36. </pages>
Reference-contexts: poorly with the lack of knowledge about program inputs and with the way the sequential program was written; sometimes the analysis wasn't sophisticated enough to parallelize the most appropriate, complex loops; and sometimes the effective transformations were without the realm of what is considered to be compiler technology today (see <ref> [8] </ref> for a detailed treatment of compiler interactions).
Reference: [9] <author> Singh, J.P., and Hennessy, J.L. </author> <title> Parallelizing the Simulation of Ocean Eddy Currents. </title> <institution> Stanford University Tech. </institution> <type> Rep. </type> <institution> CSL-TR-89-388, Stanford University, Stanford, </institution> <address> CA, </address> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Some discussion of this application's interactions with a shared memory system can be found in <ref> [9] </ref>. 5.1 Vertical versus Horizontal Partitioning For a large part of this section, we refer to the chart in Figure 5 depicting the parallelizable phases in a time-step. Once again, we focus on the part of the time-step that precedes the first equation solution. <p> The algorithm is quite complicated, and is described in detail in [12, 11]; for our purposes, we only treat it skeletally to describe our parallelization. A more detailed description of the parallelization can be found in <ref> [9] </ref>. We can think of every column of the grid we are solving for as representing an unknown in the block tridiagonal system of equations.
Reference: [10] <author> Censier, L.M., and Feautrier, P. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-27, </volume> <month> 12 (Dec. </month> <year> 1978), </year> <pages> pp. 1112-1118. </pages>
Reference-contexts: Excluding synchronization variables and global sums, the only data that remain actively shared among processors in this application are the elements at interpartition boundaries of grids that are accessed in near-neighbour fashion. This has positive implications for message-passing machines as well as directory-based cache coherence protocols <ref> [10] </ref>, since almost all the sharing is among only a small number of processors. While the computation per processor is proportional to the area of its subdomain, communication is proportional to only the perimeter, so the computation-to-communication ratio can be arbitrarily increased by increasing the problem size.
Reference: [11] <author> Swartztrauber, P. and Sweet, R. </author> <title> Efficient FORTRAN Subprograms for the Solution of Elliptic Equations. </title> <type> NCAR Tech. </type> <institution> Note TN/IA-109, National Center for Atmospheric Research, Boulder, CO, </institution> <month> Jul. </month> <year> 1975. </year>
Reference-contexts: The algorithm is quite complicated, and is described in detail in <ref> [12, 11] </ref>; for our purposes, we only treat it skeletally to describe our parallelization. A more detailed description of the parallelization can be found in [9]. We can think of every column of the grid we are solving for as representing an unknown in the block tridiagonal system of equations.
Reference: [12] <author> Sweet, R. </author> <title> A Cyclic Reduction Method for the Solution of Block Tridiagonal Systems of Equations. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 14, </volume> <month> 4 (Sep. </month> <year> 1977), </year> <pages> pp. 706-720. </pages>
Reference-contexts: The algorithm is quite complicated, and is described in detail in <ref> [12, 11] </ref>; for our purposes, we only treat it skeletally to describe our parallelization. A more detailed description of the parallelization can be found in [9]. We can think of every column of the grid we are solving for as representing an unknown in the block tridiagonal system of equations.

References-found: 12

