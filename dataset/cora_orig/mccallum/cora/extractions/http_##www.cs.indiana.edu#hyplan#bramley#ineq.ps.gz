URL: http://www.cs.indiana.edu/hyplan/bramley/ineq.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/bramley.html
Root-URL: http://www.cs.indiana.edu
Title: SOLVING LINEAR INEQUALITIES IN A LEAST SQUARES SENSE  
Author: R. BRAMLEY AND B. WINNICKA 
Keyword: Key Words. iterative methods, linear inequalities, least squares, linear separability  
Note: AMS(MOS) subject classification. 65F10, 65F20, 65F30, 65K05  
Abstract: In 1980, Han [6] described a finitely terminating algorithm for solving a system Ax b of linear inequalities in a least squares sense. The algorithm uses a singular value decomposition of a submatrix of A on each iteration, making it impractical for all but the smallest problems. This paper shows that a modification of Han's algorithm allows the iterates to be computed using QR factorization with column pivoting, which significantly reduces the computational cost and allows efficient updating/downdating techniques to be used. The effectiveness of this modification is demonstrated, implementation details are given, and the behaviour of the algorithm discussed. Theoretical and numerical results are shown from the application of the algorithm to linear separability problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Bennett and O. Mangasarian, </author> <title> Robust linear programming discrimination of two linearly inseparable sets, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 23-34. </pages>
Reference-contexts: In <ref> [1] </ref>, Bennett and Mangasarian formulate this problem in terms of the L 1 norm: minimize 1 m X i i w + fl + 1 + 1 k X i j w fl + 1 + and then solve it as the linear programming problem: minimize over (w; fl; y; z) <p> So jflj &lt; 1, and the theorem holds with ff = (1 + fl)=(1 fl). It is easy to check condition (17) before beginning computations, and perturbing any entry of G will avoid the trivial solution. By contrast, in the L 1 norm minimizing method of <ref> [1] </ref>, only the objective function for the linear program needs scaling. How ever, that formulation only guarantees that nontrivial solutions exist, but it does not assure that a linear programming program will find such a nontrivial solution. <p> Each data point has 9 components, corresponding to experimental measurements. The second data set consists of 297 points, 137 from set A (corresponding to a negative diagnosis) and 160 from set B (corresponding to a positive diagnosis). We discarded data samples that had missing measurements. As in <ref> [1] </ref>, the data was divided randomly into a training group consisting of 2/3 of the data points, and a testing group consisting of the remaining 1/3 of the data. <p> For the second data set, 5 or 6 iterations were needed each time and an average of 95% of the flops were spent in finding the search direction. Table 3 shows the results both with and without the secondary minimization on fl, along with similar results from <ref> [1] </ref>.
Reference: [2] <author> R. Bramley and B. Winnicka, </author> <title> Solving linear inequalities in a least squares sense, </title> <journal> SIAM Journal of Scientific Computing, </journal> <note> (1994). In revision for special issue for 1994 Colorado Conference on Iterative Methods. </note>
Reference-contexts: Furthermore, methods for computing QR factorization with some form of pivoting on parallel machines allow implementation on modern high performance computers. 5. Numerical Characteristics of the Algorithm. This Section summarizes results of numerical testing, and further details can be found in <ref> [2] </ref>.
Reference: [3] <author> R. Detrano et al., </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease, </title> <journal> American Journal of Cardiology, </journal> <volume> 64 (1989), </volume> <pages> pp. 304-310. </pages>
Reference-contexts: Performance on Test Databases. The linear separability methods have been tested on the Wisconsin Breast Cancer and Cleveland Heart Disease Databases <ref> [3] </ref>. Here the goal is to provide a linear predictor that can be used to distinguish between benign and malignant tumors in the first data base, and patients at risk or not at risk of heart attack in the second data base.
Reference: [4] <author> G. Golub and C. V. Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <address> Baltimore, 2 ed., </address> <year> 1989. </year>
Reference-contexts: The observations may be inconsistent, and in this case a solution is sought that minimizes the norm of the residuals. More information about linear least squares problems and solution techniques can be found in <ref> [9, 14, 4] </ref>. <p> The vector G y f is the minimum norm, least squares solution to the problem of minimizing jjGyf jj, and can be computed by a QR factorization when G is full-rank, or by singular value or complete orthogonal decomposition otherwise. See Chapter 5 of <ref> [4] </ref> for details on computing these and other factorizations for linear least squares. Second, let I f1; 2; : : : ; mg be an index set. Then A I is the submatrix of A consisting of rows with indices in I.
Reference: [5] <author> G. Golub and V. Pereyra, </author> <title> Differentiation of pseudoinverse, separable nonlinear least squares problems and other tales, in Generalized Inverses and Applications, </title> <editor> M. Nashed, ed., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 303-323. </pages>
Reference-contexts: This is critical since, with one exception, the proof of finite termination in [6] involves only the quantity A I d, not d. Before addressing that exception, we recall a result of Golub and Pereyra <ref> [5] </ref>: Theorem 4.1. Let G be an m fi n matrix, and let GP = Q ^ R be the QR with column pivoting factorization of G.
Reference: [6] <author> S.-P. Han, </author> <title> Least-squares solution of linear inequalities, </title> <type> Tech. Rep. </type> <institution> TR-2141, Mathematics Research Center, University of Wisconsin-Madison, </institution> <year> 1980. </year>
Reference-contexts: In particular, the numerical determination of the active set consists of a test against zero, without the need to introduce machine or problem dependent tolerances. The only algorithm specifically designed for solving arbitrary systems of linear inequalities in a least squares sense was developed by S.-P. Han <ref> [6] </ref>. That algorithm requires finding the minimum norm least squares (equality) solution to systems A I x = b I , where A I is a submatrix of A consisting of some rows of A. <p> Section 2 of this paper defines notation and reviews some basic properties of least squares solutions for linear inequalities, most of which can be found in <ref> [6] </ref>. Section 3 outlines Han's algorithm and Section 4 presents and validates the minor change in the convergence proof that allows QR with column pivoting to be used. Section 5 gives implementation details and testing results. <p> Basics of systems of linear inequalities. This Section summarizes some fundamental properties of linear inequalities from Han's technical report, and proofs of the results can be found in <ref> [6] </ref>. Let A 2 &lt; mfin be an arbitrary real matrix, and let b 2 &lt; m be a given vector. No relation is assumed between m and n, and the matrix A can be rank-deficient, ill-conditioned, or even the zero matrix. <p> Note that (I A y the null space of A I , so that A I d = A I d svd . This is critical since, with one exception, the proof of finite termination in <ref> [6] </ref> involves only the quantity A I d, not d. Before addressing that exception, we recall a result of Golub and Pereyra [5]: Theorem 4.1. Let G be an m fi n matrix, and let GP = Q ^ R be the QR with column pivoting factorization of G. <p> Since QR factorization with column pivoting takes O (mn 2 ) work in the dense case, this would provide a polynomial upper bound on the algorithm. A linear programming problem can be stated as a system of inequalities <ref> [6] </ref>, so this algorithm would be another polynomial method for linear programming.
Reference: [7] <author> B. Hendrickson and R. Leland, </author> <title> The Chaco user's guide, </title> <type> tech. rep., </type> <institution> Sandia National Laboratory, </institution> <address> Albuquerque, N.M., </address> <year> 1994. </year> <note> email rwlelan@cs.sandia.gov. </note>
Reference-contexts: The results indicate that with the new implementation, this linear inequalities solution method is a worthwhile addition to a computational scientist's toolkit. Current work includes applying this method to the graph partitioning problem for parallel computing (see <ref> [7] </ref> for a survey of this problem and solution methods). This works in phases: given a graph corresponding to a physical mesh, first find the "deepest" set of nodes by a breadth first search from the boundary nodes.
Reference: [8] <author> S. Katznelson, </author> <title> An algorithm for solving nonlinear resistor networks, </title> <journal> Bell System Technical Journal, </journal> <volume> 44 (1965), </volume> <pages> pp. 1605-1620. </pages>
Reference-contexts: His arguments in particular show that of the two alternatives in Theorem 4.3, the second alternative (equation (6)) cannot occur. It is worthwhile to compare those results with related ones. As early 7 as 1965, Katznelson <ref> [8] </ref> established finite termination of an algorithm for solving piecewise linear systems of equations that arise in circuits.
Reference: [9] <author> C. L. Lawson and R. J. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: The observations may be inconsistent, and in this case a solution is sought that minimizes the norm of the residuals. More information about linear least squares problems and solution techniques can be found in <ref> [9, 14, 4] </ref>.
Reference: [10] <author> W. Li and J. Swetits, </author> <title> A Newton method for solving convex quadratic programs, </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 (1993), </volume> <pages> pp. 466-488. </pages>
Reference-contexts: It is worthwhile to compare those results with related ones. As early 7 as 1965, Katznelson [8] established finite termination of an algorithm for solving piecewise linear systems of equations that arise in circuits. More recently Li and Swetits <ref> [10] </ref> have established finite termination for solving systems of the form (x) = Q T [(Ax + b) + + (Cx + d)] = 0 (c.f. the gradient of f (x)).
Reference: [11] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference: [12] <author> R. Penrose, </author> <title> A generalized inverse for matrices, </title> <journal> Proc. Cambridge Phil. Soc., </journal> <volume> 51 (1955), </volume> <pages> pp. 406-413. </pages>
Reference-contexts: Note however that the Hessian of f fails to exist at points where a T i x = b i for any i = 3. Outline of Han's algorithm. Two notations are needed for the statement of Han's algorithm. First, G y denotes the pseudo-inverse of the matrix G <ref> [12] </ref>. In practice, all that is needed is the action of G y on a vector f , not the linear operator itself in explicit form.
Reference: [13] <author> G. Stewart, </author> <title> An iterative method for solving linear inequalities, </title> <type> Tech. Rep. </type> <institution> TR-1833, University of Maryland Computer Science Department, </institution> <year> 1987. </year>
Reference-contexts: Futhermore when the system is not consistent, linear programming can identify that case, but does not directly provide an "optimal" solution. Other methods developed for solving linear inequalities include an unusual algorithm by Stewart <ref> [13] </ref>, which defines a function that diverges in a direction that converges to a solution of the inequalities; if no solution exists, the function converges to a unique minimum.

References-found: 13

