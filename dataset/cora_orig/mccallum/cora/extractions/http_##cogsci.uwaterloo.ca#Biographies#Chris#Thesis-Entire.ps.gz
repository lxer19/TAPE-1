URL: http://cogsci.uwaterloo.ca/Biographies/Chris/Thesis-Entire.ps.gz
Refering-URL: http://cogsci.uwaterloo.ca/Biographies/chris.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Mind as a Dynamical System  
Author: by Chris Elias Chris Elias 
Degree: A thesis presented to the University of Waterloo in fulfillment of the thesis requirement for the degree of Master of Arts in Philosophy  
Date: 1995  
Address: Ontario, Canada, 1995  
Affiliation: Waterloo,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abraham, Abraham, </author> <title> and Shaw (1994). Dynamical Systems for Psychology. </title>
Reference-contexts: In effect, psychologists wish to understand what effect a person's experience has on their current mental condition <ref> (Abraham, Abraham, and Shaw, 1994) </ref>.
Reference: <author> Achinstein, P. </author> <year> (1968). </year> <booktitle> Concepts of science. </booktitle> <address> Baltimore, </address> <publisher> Johns Hopkins Press. </publisher>
Reference-contexts: correctly notes that "the notion of model is not dependent on prior development of a formal theory (in science)" (Hesse, 1972, p. 356), however, this does not put into question its logical priority. 11 Unfortunately, it is often true that what is called a model is also called a theory <ref> (Achinstein, 1968, p. 212) </ref>. But this does not mean that what is a model is a theory. A model describes, through resemblance and often for the purposes of empirical verification, the structures and mechanisms posited by a theory. <p> of this can be found in Achinstein (1968), with the claim: a theoretical model describes a type of object or system by attributing to it what might be called an inner structure, composition or mechanism, reference to which is intended to explain various properties exhibited by that object or system <ref> (Achinstein, 1968, p. 212) </ref>. Similarly, synonymous use of the terms model and picture when referring to theory can be found in some relevant literature (Ubbink, 1961, p. 179), though this usage is too imprecise for our purposes.
Reference: <author> Anderson, J. R. </author> <year> (1983). </year> <title> The architecture of cognition. </title> <address> Cambridge, MA, </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: Symbolicists believe that all cognition is performed by manipulation of such symbolic representations. Prime examples of employing this approach to the study of cognition are found in Anderson's set of Adaptive Control of Thought (Act) models which culminated with Act* (pronounced Act-star) <ref> (Anderson, 1983) </ref>, and Newell's Soar architecture (Newell, 1990). However, the late 1970s saw the resurgence of the examination of cognitive functioning in terms of neural-like, or parallel distributed, processing (PDP). This theory, in its purer form, postulates a distributed representation of concepts across a network of highly interconnected simple nodes.
Reference: <author> Barton, S. </author> <year> (1994). </year> <title> Chaos, self-organization, and psychology. </title> <booktitle> American Psychologist 49(1): </booktitle> <pages> 5-14. </pages>
Reference-contexts: A further criticism of this model can be directed at its predictive or correlative properties. Although the model accounts quite well for a number of observed properties, it does not correspond with the actual EEG patterns in the olfactory lobe <ref> (Barton, 1994, p. 10) </ref>. The consequences of this inaccuracy seem quite severe. <p> to their field ignore the differences between their field and the rigorous ones from which dynamical systems theory arose: One way that the distinction between fields is set aside is when authors use rigorous terminology from nonlinear dynamics to refer to psychological variables that are multidimensional and difficult to quantify <ref> (Barton, 1994, p. 12) </ref>. For example, some psychologists have equated the dynamical concept of chaos with overwhelming anxiety, others with creativity, and still others with destructiveness (Barton, 1994). <p> For example, some psychologists have equated the dynamical concept of chaos with overwhelming anxiety, others with creativity, and still others with destructiveness <ref> (Barton, 1994) </ref>. These applications of chaos are clearly more metaphorical than rigorous, and bear little resemblance to the definitions used in precise dynamical systems theory models. <p> There is no rigor added to their model simply because the chosen metaphor is mathematical. Barton duly notes that in the paper describing one such dynamical model of a Jungian hypothesis, Abraham et al. imply a level of measurement precision we don't have in clinical psychology <ref> (Barton, 1994, p. 12) </ref>. There is no explanation in these clinical psychological applications of dynamics systems theory to the phenomenology or intentionality of cognition. These models are simply metaphorical descriptions, they advance no new insights in clinical psychology. They do not reveal any details about what is being modeled. <p> discontinuity between model and reality is a common problem in investigating natural nonlinear systems: Not only are investigators rarely able to completely characterize all the variables that affect a complex system, but they must isolate a system well enough to cut through what Morrison (1991) called a sea of noise <ref> (Barton, 1994, p. 10) </ref>. When it is difficult to know the factors involved and to find the signal of interest in ambient noise, as is often the case, it is common practice for dynamicists to define lumped parameters.
Reference: <author> Beardsley, M. </author> <year> (1972). </year> <title> Metaphor. </title> <booktitle> The encyclopedia of philosophy. </booktitle> <address> New York, </address> <publisher> MacMillan Publishing Co. & The Free Press. </publisher> <pages> 284-289. </pages>
Reference-contexts: So, to differentiate between model and analogy in science, one can determine if the mapping of these important properties is explicit, leaving no room for interpretation; if so, one is dealing with a model. In this sense, a model can be thought of as a kind of controlled metaphor <ref> (Beardsley, 1972, p. 287) </ref> 1 . <p> So the metaphorical description may least misleadingly, perhaps, be considered as an aid to thought rather than a special mode of thinking" <ref> (Beardsley, 1972, p. 287) </ref>. 9 The atom is like the solar system - leaving room for the listener to fill in the details, and possibly to infer wrongly that the orbits of electrons and planets are similar a model would consist of a picture, physical prototype, or mathematical description in which
Reference: <author> Bechtel, W. </author> <title> (in press). Natural deduction in connectionist systems. </title> <journal> Synthese. </journal>
Reference: <author> Brooks, Rodney A. </author> <year> (1991). </year> <title> Intelligence without reason. </title> <institution> MIT AI Lab Memo 1293, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: briefly considered in section 2.2.5: It seems that we, as natural cognitive agents, manipulate representations; how is this possible if we do not represent things to ourselves? There has been moderate success dynamically modeling natural cognizers without any use of representation - but only cognizers with the sophistication of cockroaches <ref> (Brooks, 1991) </ref>.
Reference: <author> Cartwright, N. </author> <year> (1991). </year> <title> Fables and models I. </title> <booktitle> Proceedings of the Aristotelian Society Supplement 65: </booktitle> <pages> 55-68. </pages>
Reference: <author> Churchland, P. </author> <year> (1989). </year> <title> A neurocomputational perspective. </title> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: such models and theories in cognitive science in an attempt to generate criteria for evaluating dynamicist theory and its resulting models. 1.1 Distinguishing Metaphor, Analogy, Model and Theory As Churchland has noted, problems in the philosophy of mind have recently found themselves reconstructed as problems in the philosophy of science <ref> (Churchland, 1989) </ref>. The result, has been an infusion of both philosophy of science and philosophy of mind into the multidisciplinary field of cognitive science. <p> despite changes in head position, recognize subtle similarities among sonar returns, pronounce well-articulated speech from printed text, recognize three-dimensional shapes independently of the angle of illumination, parse sentences into grammatical types, recognize voiced phonemes, predict the folding of protein molecules, correctly recognize colors across changes in illumination, and so forth <ref> (Churchland, 1989, p. xv) </ref>. Some such successes have not been effectively handled by the symbolic approach 15 . However, there are also many symbolic successes which have not yet been effectively handled by connectionism. <p> If dynamicism is to be a nonempirical cognitive theory, it will encounter the same difficulties as folk psychology <ref> (Churchland, 1989) </ref> and cannot reasonably be considered in the same category as symbolicism and connectionism. However, this is clearly not the aim of dynamicists. Clearly, lack of empirical exemplars in such a relatively new approach is no reason to dismiss it.
Reference: <author> Churchland, P. S. </author> <year> (1993). </year> <title> Presidential address. </title> <journal> American Psycological Association, </journal> <note> Churchland, </note> <author> P. S. and T. </author> <title> Sejnowski (1992). The computational brain. </title> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference: <author> Donald, M. </author> <year> (1991). </year> <title> Origins of the modern mind: Three stages in the evolution of culture and cognition. </title> <publisher> Cambridge, Harvard University Press. </publisher>
Reference-contexts: The external transformation is also encoded into an internal transformation. Then the internal transformation is applied to the internal situation to obtain a new internal situation. Finally the new internal situation is decoded to an external situation. (Newell, 1990, p. 59). 30 are dedicated to general categories of processing <ref> (Donald, 1991, p. 46) </ref> it is not clear that these areas can be divided into the three categories that the symbolicists have chosen. What is even less likely is the idea of a high-level command language which would activate complex coordinated movements.
Reference: <author> Egeth, H. E. and D. </author> <month> Dagenbach </month> <year> (1991). </year> <title> Parallel versus serial processing in visual search: Further evidence from subadditive effects of visual quality. Journal of Experimental Psychology: </title> <booktitle> Human Perception and Performance 17: </booktitle> <pages> 551-560. </pages>
Reference: <author> Fodor, J. and Z. </author> <title> Pylyshyn (1988). Connectionism and cognitive architecture: A critical analysis. </title> <journal> Cognition 28: </journal> <pages> 3-71. </pages>
Reference-contexts: Giunti (1991) showed that the symbolicist Turing Machine is a dynamical system (van Gelder, 1993). So, it could be concluded that there is nothing to gain from introducing a separate dynamicist method of studying cognition. However, Turing Machines and connectionist networks have also been shown to be computationally equivalent <ref> (Fodor and Pylyshyn, 1988, p. 10) </ref> yet the approaches are vastly disparate in their methods, strengths, and philosophical commitments. Similarly, though Turing Machines are dynamical in the strictest mathematical sense they are nonetheless serial and discrete. <p> To be a truly dynamical system, there should be no representation in the cognitive model. And clearly, connectionists represent concepts in their networks 21 . Dynamicists understand the connectionist attempt to abstractly represent the world as a 21 It is generally agreed that both connectionists and symbolicists are representationalists <ref> (Fodor and Pylyshyn, 1988, p. 7) </ref>. 45 different guise of the symbolicist's misguided commitment to symbolic representation. Van Gelder concludes: it is the concept of representation which is insufficiently sophisticated (van Gelder, 1993, p. 6) for understanding cognition.
Reference: <author> Gleick, </author> <title> James (1987). Chaos: making a new science. </title> <address> New York: </address> <publisher> Viking. </publisher>
Reference-contexts: These facts make it rather curious that it is touted as a paradigmatically important dynamical 22 Which there are theoretically guaranteed to be given that the systems are chaotic <ref> (Gleick, 1987) </ref>. 51 systems model. The model's similarities with connectionism make it quite difficult to accept the assertion that this type of dynamical model is the seed of a new paradigm in cognitive modeling.
Reference: <author> Globus, G. G. </author> <year> (1992). </year> <title> Toward a noncomputational cognitive neuroscience. </title> <journal> Journal of Cognitive Neuroscience 4(4): </journal> <pages> 299-310. </pages> <note> 68 Goodwin, </note> <author> B. </author> <year> (1994). </year> <title> How the leopard changed its spots: The evolution of complexity. United States, </title> <publisher> Charles Scribners Sons. </publisher>
Reference-contexts: Commonly, discriminating these approaches relies on the dynamicist belief that connectionist networks are not adequate for reproducing the behavior of living neural nets which rely on ongoing chemical tuning of the input/output transfer function at the nodes, connection weights, network parameters, and connectivity <ref> (Globus, 1992, p. 299) </ref>. So, it seems if a model is complicated enough, the dynamicist would claim it is no longer connectionist, but dynamicist. It is uncertain how connectionists who posit such models (see Churchland and Sejnowski, 1992) would react to an insistence that they are not connectionists. <p> In realistic nets, however, it is not the representations that are changed; it is the self-organizing process that changes via chemical modulation. Indeed, it no longer makes sense to talk of representations <ref> (Globus, 1992, p. 302) </ref>. For dynamicists, a cognitive agent can best be described by a certain type of system of equations which display the same dynamical characteristics as that agent. <p> Examples of strange attractors, chaos, catastrophe, etc. are all found in connectionist networks, and such concepts have been used to analyze these networks. Critiques of connectionism from dynamicists do not seem to present any sort of united front. Some dynamicists note the lack of realism in some networks <ref> (Globus, 1992) </ref>. Others reject connectionism not because of a failure in principle but because of a failure of spirit (Thelen and Smith, 1994, p.41). 24 Still others reject connectionism as being too committed to symbolicist ideas: ideas like representation (van Gelder and Port, in press, p. 27). <p> The complexity of real networks does not represent a qualitatively distinct functioning, rather just the end-goal of current connectionist models. Thus, claims consonant with: simplified silicon nets can be thought of as computing but biologically realistic nets are non computational <ref> (Globus, 1992, p. 300) </ref> are severely misleading. The chemical modulation of neurotransmitter synthesis, release, transport, etc. is simply a more complicated process, not a qualitatively different method of functioning. As Globus (1992) later admits, connectionist networks severely stretch the concept of computation in the direction of dynamical systems theory.
Reference: <author> Harnard, S. </author> <year> (1992). </year> <title> Connecting object to symbol in modelling cognition. Connectionism in Context. </title> <publisher> London, Springer-Verlag. </publisher>
Reference: <author> Hawking, S. </author> <year> (1988). </year> <title> A brief history of time. </title> <publisher> London, Bantam Press. </publisher>
Reference-contexts: Finally, theoretical unity has been heralded as a main aim of scientific inquiry. Such a belief has propelled physicists to seek the elusive Grand Unified Theory (GUT) of forces <ref> (Hawking, 1988) </ref>.
Reference: <author> Hesse, M. </author> <year> (1972). </year> <title> Models and analogies in science. </title> <booktitle> The encyclopedia of philosophy. </booktitle> <address> New York, </address> <publisher> MacMillan Publishing Co. & The Free Press. </publisher> <pages> 354-359. </pages>
Reference-contexts: However, because the term metaphor is often used in a less rigorous sense (i.e. in a literary, aesthetic, or rhetoric context), analogy will most often be our term of choice, as we are examining these sorts of relations in a scientific context <ref> (as per Hesse, 1972) </ref>. In forming an analogy, one attempts to express similarities between situations in the world, say A and B (Holyoak and Thagard, 1995, p. 5). <p> airplane, ideal behavior, a description of behavior of a real system and an i m p l e m e n t a t i o n of a mathematically described system, respectively): The relation between model and thing modeled can be said generally to be a relation of analogy <ref> (Hesse, 1972, p. 355) </ref>. In science, however, a model is no longer simply a resemblance, but rather a precise description of the properties of the system being modeled. The more important properties of the source that are exactly demonstrated by the model, the better the model (see section 1.2.1). <p> In cases like the atom model, however, the representation is not one of exact proportions, but rather an explicit mapping from one physical domain to another <ref> (see Hesse, 1972) </ref>. On the other hand, computer and mathematical models have no similar sort of physical existence 3 . So, our distinction between physical and computer models comes down to one of the activities of the modeler. <p> Other views of the status of theories, such as those of Putnam and Newell, lend theories a stronger resistance to controversy and dispute (Putnam, 1981; Newell, 1990). 4 Hesse correctly notes that "the notion of model is not dependent on prior development of a formal theory (in science)" <ref> (Hesse, 1972, p. 356) </ref>, however, this does not put into question its logical priority. 11 Unfortunately, it is often true that what is called a model is also called a theory (Achinstein, 1968, p. 212). But this does not mean that what is a model is a theory. <p> Still, this criterion ties in with the additional criterion of explanation. Scientific explanation is generally agreed to have two main aims: claims Hesse, successful prediction of the behavior of things in experimentally defined conditions, and theoretical representation of the causal structure of the world from which this behavior follows <ref> (Hesse, 1972, p. 325) </ref>. Since explanation is often considered the purpose of all scientific inquiry (Hesse, 1972; Cartwright, 1991; Koertge, 1992; c.f. Le Poidevin, 1991), it deserves further attention. In cognitive science, as we have examined it so far, neither the theory nor the model can unilaterally account for explanation.
Reference: <author> Holyoak, K. and P. </author> <title> Thagard (1995). Mental leaps: Analogy in creative thought. </title> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: In forming an analogy, one attempts to express similarities between situations in the world, say A and B <ref> (Holyoak and Thagard, 1995, p. 5) </ref>. If the 7 analogy is good, these similarities will allow for knowledge of the better understood situation, say A, to be transferred effectively to the lesser understood situation, that is B. <p> There are certain details of B which will not be captured and others which may even be falsely implied. Nevertheless, analogy plays an extremely important role in the sciences. Scientific analogical thinking has often led researchers to a novel perspective resulting in valuable insights <ref> (Holyoak and Thagard, 1995, pp. 186-188) </ref>. Similarly, analogy in cognitive science is often central to a particular view of 8 cognition. For the symbolicist, the mind is a specific type of computer; a symbol manipulator.
Reference: <author> Kauffman, S. A. </author> <year> (1993). </year> <title> The origins of order: self-organization and selection in evolution. </title> <publisher> Oxford, Oxford University Press. </publisher>
Reference-contexts: In fact it seems the only viable way to discuss such large (i.e. 100 000 unit) networks is by appealing to the overall dynamics of the system and thoroughly apply dynamical systems concepts, descriptions and analysis <ref> (Kauffman, 1993, p. 210) </ref>.
Reference: <author> Koertge, N. </author> <year> (1992). </year> <title> Explanation and its problems. </title> <journal> British Journal of the Philosophy of Science 43: </journal> <pages> 85-98. </pages>
Reference-contexts: It attempts to explain how a system works, addressing the organization and relationships of the systems 12 elements. For Koertge, explanation of this sort is genuine only if a theory gives an account of the system's causal mechanisms <ref> (Koertge, 1992, p. 91) </ref>. However, he also notes that Woodward (1992) argues that many seemingly explanatory advances in science are not causal. So, for our purposes we will be non-restrictive in the criteria we place on an explanation, it may be causal or not. <p> There is no question that scientists place a high value on unified conceptual systems... Kitcher posits that the aim of science is not just knowledge, but unified knowledge. It is its unified character which makes science explanatory <ref> (Koertge, 1992, p. 93) </ref>. It is not necessary for us to ally ourselves with this particular stance. However, the importance of theoretical unity should be well taken.
Reference: <author> Kosslyn, S. </author> <year> (1980). </year> <title> Image and mind. </title> <publisher> Cambridge, Harvard University Press. </publisher>
Reference-contexts: It seems obvious that human cognizers use representation in their dealings with the world around them. For example, people seem to have the ability to rotate, and examine objects in their head. It seems they are manipulating a representation <ref> (Kosslyn, 1980) </ref>. More striking perhaps is the abundant use of auditory and visual symbols used by human cognizers everyday to communicate with one another. However, dynamicists are committed to not having subsystems manipulate representations or communicate with other subsystems by passing representations.
Reference: <author> Langer, Susanne K. </author> <year> (1957). </year> <title> Philosophy in a new key: A study in the symbolism of reason, </title> <booktitle> rite and art. </booktitle> <address> Cambridge, </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Le Poidevin, R. </author> <year> (1991). </year> <title> Fables and Models II. </title> <booktitle> Proceedings of the Aristotelian Society Supplement 65: </booktitle> <pages> 69-82. </pages>
Reference: <author> Luck, S. J. and S. A. </author> <month> Hillyard </month> <year> (1990). </year> <title> Electrophysiological evidence for parallel and serial processing during visual search. </title> <journal> Perception and Psychophysics 48: </journal> <pages> 603-617. </pages>
Reference: <author> Meade, A. J. and A. A. </author> <title> Fernandez (1994). Solution of nonlinear ordinary differential equations by feedforward neural networks. </title> <note> Mathematical and Computer Modelling To Appear: </note> <author> Miller, J. O. </author> <year> (1988). </year> <title> Discrete and continuous models of human information processing: Theoretical distincitons and empirical results. </title> <journal> Acta Psychologica 67: </journal> <pages> 191-257. </pages>
Reference-contexts: In this paper, Meade uses a feed forward artificial neural network to solve a system of three coupled nonlinear second and third order ordinary differential equations <ref> (Meade and Fernandez, 1994) </ref> . The techniques that he has developed are also applicable to partial differential equations.
Reference: <author> Molenaar, P. C. M. </author> <year> (1990). </year> <title> Neural netowrk simulation of a discrete model of continuous effects of irrelevant stimuli. </title> <journal> Acta Psychologica 74: </journal> <pages> 237-258. </pages>
Reference: <author> Morton, A. </author> <year> (1988). </year> <journal> The chaology of mind. </journal> <volume> 48(3): </volume> <pages> 135-142. </pages>
Reference-contexts: This application is another attempt to deal with the general problem of relating patterns of activity over time - as many psychological states surely are - to instantaneous states <ref> (Morton, 1988, p. 141) </ref>. In effect, psychologists wish to understand what effect a person's experience has on their current mental condition (Abraham, Abraham, and Shaw, 1994).
Reference: <author> Newell, A. </author> <year> (1990). </year> <title> Unified theories of cognition. </title> <address> Cambridge, MA, </address> <publisher> Harvard University Press. 69 Pinel, </publisher> <editor> J. P. </editor> <address> (1993). Biopsychology. </address> <publisher> Allyn & Bacon Inc. </publisher>
Reference-contexts: Symbolicists believe that all cognition is performed by manipulation of such symbolic representations. Prime examples of employing this approach to the study of cognition are found in Anderson's set of Adaptive Control of Thought (Act) models which culminated with Act* (pronounced Act-star) (Anderson, 1983), and Newell's Soar architecture <ref> (Newell, 1990) </ref>. However, the late 1970s saw the resurgence of the examination of cognitive functioning in terms of neural-like, or parallel distributed, processing (PDP). This theory, in its purer form, postulates a distributed representation of concepts across a network of highly interconnected simple nodes. <p> top to bottom - from behavior, through systems to networks to neurons and molecules: a unified science of the mind-brain (Churchland and 5 Sejnowski, 1992, p. 13); or similarly from a symbolicist: the hallmark of a unified theory is the range of central cognition and its surround that it addresses <ref> (Newell, 1990, p. 31) </ref>; or lastly from the dynamicist: a picture of cognitive processing in its entirety, from peripheral input systems to peripheral output systems and everything in between (van Gelder and Port, in press). <p> Thus, the theory is in some ways logically prior 4 , but is not completely independent from its models: Soar is a model of the human cognitive architecture - that is, it embodies the theory <ref> (Newell, 1990, p. 207) </ref>. In cognitive science, and on the Popperian view of scientific explanation, without any model there is no support for the acceptance of a theory. <p> Not surprisingly, in cognitive science the search for unified theories has also arisen: Psychology has arrived at the possibility of unified theories of cognition - theories that gain their power by positing a single system of mechanisms that operate together to produce the full range of human cognition <ref> (Newell, 1990, p. 1) </ref>. Interestingly, the unity criteria plays a large role in the connectionist rejection of classicism, the classicist rejection of connectionism, and the dynamicist claim to oust both of these views. <p> As of yet, no cognitive theories have come close to proving that they are ideal in this sense. However, that does not mean that the theories are to be flat out 13 rejected: Theories cumulate. They are refined and reformulated, corrected and expanded <ref> (Newell, 1990, p. 14) </ref>. This evident rejection of Popperian theoretical ideals in science is not uncommon amongst cognitive scientists. However, a theory which does not have any sort of empirical support will be short-lived in the field. Theories may be nonideal but they may not be unsupported. <p> in later chapters. 1.2 Applying Models It is imperative to make clear the importance of computational simulation in the validation of a cognitive theory: for a system as complex as the mind, it is difficult to have much faith in a theory without having it instantiated in some operational form <ref> (Newell, 1990, p. 194) </ref>. More often than not, it is the simulations and models of a theory which are critically examined by competing approaches. The reason is simply that simulations tend to objectify part of the body of knowledge on which a particular theory relies. <p> And possibly, the model can be used to predict the behavior of natural cognizers on newly designed tests. If a model provides 14 accurate predictions etc., the theory behind the model is verified for the particular areas of cognition addressed by the psychological tests <ref> (see Newell, 1990) </ref>. Of course, such a method seldom covers a wide variety of cognitive behaviors. As Churchland and Sejnowski point out, what goes into the model depends on what one is trying to explain (Churchland and Sejnowski, 1992, p. 136). <p> These latter criteria are far more subjective in nature and are overshadowed in importance by the former. However, they can be useful in understanding why theories occasionally outlast their usefulness <ref> (Newell, 1990, p. 14) </ref> and in finding merit in theories which might otherwise be cast aside. <p> Though Andersons (1983) Act* is classified by Newell as a unified theory of cognition, Newell believes that Soar will expand extensively on the range of behaviors exhibited by Act* <ref> (Newell, 1990, p. 29) </ref>. To quickly characterize Soar, it is best to turn to Newells own description (1990, p. ix, 39): Soar has its roots in thirty years of research on the nature of artificial intelligence (AI) and human cognition. <p> It uses a production system as the foundation of the architecture, thus being a kind of recognize-act system. It uses 8 The more controversial aspects of Newells theory, such as chunked learning and memory characterization <ref> (Newell, 1990) </ref>, will thus not be discussed here. 25 problem spaces everywhere to do all of its business, that is, to formulate all its tasks. Soar is undeniably an impressive model. It has been tested on many tasks, including: AI toy problems (e.g. <p> It has been tested on many tasks, including: AI toy problems (e.g. Blocks World, Missionaries, Tower of Hanoi, the Eight Puzzle), expert systems (e.g. medical diagnosis, computer system configurations), forms of parsing, version spaces (a scheme for learning concepts), syllogistic reasoning, algorithm design, and cryptarithmetic <ref> (Newell, 1990, p. 217) </ref>. All of these tasks were tackled, many quite successfully, relying on the symbolic approach. Symbolicists claim that humans have programs, indeed, that they have analogous programs (Newell, 1990, p. 248). <p> All of these tasks were tackled, many quite successfully, relying on the symbolic approach. Symbolicists claim that humans have programs, indeed, that they have analogous programs <ref> (Newell, 1990, p. 248) </ref>. However, such a claim cannot be simply interpreted as claiming that humans are just like the familiar serial digital computer. Rather, the symbolicist introduces the abstract characterization of knowledge-level systems and symbol systems (Newell, 1990, p. 51, 76). <p> Symbolicists claim that humans have programs, indeed, that they have analogous programs (Newell, 1990, p. 248). However, such a claim cannot be simply interpreted as claiming that humans are just like the familiar serial digital computer. Rather, the symbolicist introduces the abstract characterization of knowledge-level systems and symbol systems <ref> (Newell, 1990, p. 51, 76) </ref>. The former is a system which processes knowledge about the systems goals, actions, environment and the relations between these items. <p> The former is a system which processes knowledge about the systems goals, actions, environment and the relations between these items. Furthermore, the system has a single law of behavior which is: take actions to attain goals using all of the knowledge available to the system <ref> (Newell, 1990, p. 50) </ref>. A symbol system is a form of a universal computational system (e.g. a Turing Machine) which Newell defines in terms of general characteristics of memory, symbols, operations, interpretation and capacities (Newell, 1990, p. 77). <p> A symbol system is a form of a universal computational system (e.g. a Turing Machine) which Newell defines in terms of general characteristics of memory, symbols, operations, interpretation and capacities <ref> (Newell, 1990, p. 77) </ref>. The purpose of these symbol systems is to realize knowledge systems by implementing representation law so that the symbol structures encode the knowledge about the external world (Newell, 1990, pp. 78-79). <p> The purpose of these symbol systems is to realize knowledge systems by implementing representation law so that the symbol structures encode the knowledge about the external world <ref> (Newell, 1990, pp. 78-79) </ref>. So, through application of these definitions, natural cognizers are defined more precisely: humans are symbol systems that are at least modest approximations of knowledge systems (Newell, 1990, p. 113). Soar, of course, is attempting to achieve this same distinction. <p> So, through application of these definitions, natural cognizers are defined more precisely: humans are symbol systems that are at least modest approximations of knowledge systems <ref> (Newell, 1990, p. 113) </ref>. Soar, of course, is attempting to achieve this same distinction. <p> The reason cognition is cast in terms of knowledge-level systems is that this level can be described before anything about the internal workings of the system is determined <ref> (Newell, 1990, p. 50) </ref>. Thus symbolicists wish to implement generic knowledge-level descriptions of behavior and they feel the most appropriate way to accomplish this task is to discover the symbol-level 26 mechanisms that permit a close approximation to the knowledge level (Newell, 1990, p. 80). <p> Thus symbolicists wish to implement generic knowledge-level descriptions of behavior and they feel the most appropriate way to accomplish this task is to discover the symbol-level 26 mechanisms that permit a close approximation to the knowledge level <ref> (Newell, 1990, p. 80) </ref>. Obviously, such a description of cognition will likely conflict with the dynamicist assertion that cognition is embedded, non-representational and described by coupled nonlinear differential equations. Indeed, the conflict is not minor. <p> In an attack on the poor temporal nature of symbolic systems, van Gelder and Port claim that symbolicists leave time out of the picture (van Gelder and Port, in press, p. 2). However, Newell includes operation in real time as the third most important constraint that shapes the mind <ref> (Newell, 1990, p. 19) </ref>. Thus, it is not the case the symbolicists ignore time, but it may be the case that they have difficulty meeting temporal constraints. <p> Thus, it is not the case the symbolicists ignore time, but it may be the case that they have difficulty meeting temporal constraints. Newell uses neurological data to lend support to his assumption that any particular step in a cognitive algorithm operates on the time scale of approximately 10ms <ref> (Newell, 1990, p. 127) </ref>. However, application of this constraint seems rather contrived 9 and more importantly is inconsistent. <p> However, application of this constraint seems rather contrived 9 and more importantly is inconsistent. For instance, one Soar application employs a single production to encode whether or not a light is on <ref> (Newell, 1990, p. 275) </ref> and a second application uses a single production to encode: If the problem space is the base-level-space, and the state has a box with nothing on top, and the state has input that has not been examined, then make the comprehend operator acceptable, and note that the <p> The claim that Soar has somehow allowed rough predictions of human reaction time is very unconvincing. It is rather more likely that the modeler's analysis, experience with psychological results, and chosen constants allowed such rough predictions <ref> (Newell, 1990, pp. 274-282) </ref>. 27 examined (Newell, 1990, p. 167). It seems unrealistic that both of these productions should fire on the same order of magnitude, i.e. approximately 10ms. <p> The claim that Soar has somehow allowed rough predictions of human reaction time is very unconvincing. It is rather more likely that the modeler's analysis, experience with psychological results, and chosen constants allowed such rough predictions (Newell, 1990, pp. 274-282). 27 examined <ref> (Newell, 1990, p. 167) </ref>. It seems unrealistic that both of these productions should fire on the same order of magnitude, i.e. approximately 10ms. <p> On the other had, symbolicists attach qualitatively different explanations at different time scales. Thus, it is reasonable to ignore lower time scale in an explanation of a higher one - for the dynamicist, this is impossible <ref> (Newell, 1990, p. 154) </ref>. The power of dynamical systems theory to model real time phenomena is so impressive that van Gelder and Port exploit this property to support a major objection to symbolicism: Cognitive processes always unfold in real time. <p> Currently, the symbolicist cannot confidently assert how time in their model of cognitive processes relates to time in the natural cognizer. For, as Newell himself notes: minor changes in assumptions move the total time accounting in substantial ways that have strong consequences for which model fits the data <ref> (Newell, 1990, p. 294) </ref>. 2.2.3 Architecture An architecture provides a boundary, of sorts, which separates the structure of the system from its content. For Newell, this means that behavior is determined by variable content being processed according to the fixed processing structure, which is the architecture (Newell, 1990, p. 82). <p> For Newell, this means that behavior is determined by variable content being processed according to the fixed processing structure, which is the architecture <ref> (Newell, 1990, p. 82) </ref>. In contrast, the dynamicist would insist that both the structure and content are variable, and that it is wrong 28 to assume this fixedness in the face of a dynamical changing environment and agent. <p> Taken together, these levels combine to form the total cognitive system. Newell realizes that cognitive scientists need to describe this structure as well, for it is critical to a unified theory of cognition <ref> (Newell, 1990, p. 194) </ref>.. Nonetheless, Newell has admittedly focused on central architecture and points out that Soar has not yet dealt with any of the central phenomena of perception or motor behavior (Newell, 1990, p. 304). <p> Nonetheless, Newell has admittedly focused on central architecture and points out that Soar has not yet dealt with any of the central phenomena of perception or motor behavior <ref> (Newell, 1990, p. 304) </ref>. However, the symbolicist does not see this as a major drawback to a theory of cognition, because it is generally accepted that mind (is) the control system that guides the behaving organism in its complex interactions with the dynamic real world (Newell, 1990, p. 43). <p> However, the symbolicist does not see this as a major drawback to a theory of cognition, because it is generally accepted that mind (is) the control system that guides the behaving organism in its complex interactions with the dynamic real world <ref> (Newell, 1990, p. 43) </ref>. Thus, central cognition supervises the other aspects of cognition and seeing as it is at the top of the hierarchy, it is the most interesting (perhaps) place to begin an examination of cognitive behavior. <p> Perceptual Syst ems Motor Syst ems Central Cognition Total Cognitive System Environment For the symbolicist, there is a strict division between central architecture and the peripheral cognitive activities: so for perception, the arriving perceptual 29 elements and the encoding activity are outside central cognition, that is, inaccessible to it <ref> (Newell, 1990, p. 257) </ref>; or in the case of motor control, cognitive intention produces a command to release motor action (Newell, 1990, p. 259). <p> division between central architecture and the peripheral cognitive activities: so for perception, the arriving perceptual 29 elements and the encoding activity are outside central cognition, that is, inaccessible to it (Newell, 1990, p. 257); or in the case of motor control, cognitive intention produces a command to release motor action <ref> (Newell, 1990, p. 259) </ref>. Of course, these commands are not always conscious commands (e.g. the command to beat the heart), but they are symbols which are encoded by central cognition and then decoded and executed once they have been passed to the motor system. <p> Newell himself realizes the debate over the viability of this architecture is neither easily resolved: unfortunately, the nature of the command language to the motor system is exactly where obscurity is deepest... <ref> (Newell, 1990, p. 259) </ref>, nor trivial: interestingly, the major barrier to a more complete explanation is the poorly defined motor system in Soar (Newell, 1990, p. 301). Not surprisingly, these barriers are reiterated and elaborated by competing cognitive approaches in order to discredit symbolicism. <p> over the viability of this architecture is neither easily resolved: unfortunately, the nature of the command language to the motor system is exactly where obscurity is deepest... (Newell, 1990, p. 259), nor trivial: interestingly, the major barrier to a more complete explanation is the poorly defined motor system in Soar <ref> (Newell, 1990, p. 301) </ref>. Not surprisingly, these barriers are reiterated and elaborated by competing cognitive approaches in order to discredit symbolicism. <p> The external transformation is also encoded into an internal transformation. Then the internal transformation is applied to the internal situation to obtain a new internal situation. Finally the new internal situation is decoded to an external situation. <ref> (Newell, 1990, p. 59) </ref>. 30 are dedicated to general categories of processing (Donald, 1991, p. 46) it is not clear that these areas can be divided into the three categories that the symbolicists have chosen. <p> From the family of universal computers, Newell clearly identifies the type of computer deemed necessary to exhibit cognition; one that will provide a means to build representation systems <ref> (Newell, 1990, p. 68) </ref>. These representational systems are, for Newell, strictly symbol systems (Newell, 1990, p. 76). This conjecture has been labeled the Physical Symbol System Hypothesis by Newell and Simon (see section 2.2.1). <p> From the family of universal computers, Newell clearly identifies the type of computer deemed necessary to exhibit cognition; one that will provide a means to build representation systems (Newell, 1990, p. 68). These representational systems are, for Newell, strictly symbol systems <ref> (Newell, 1990, p. 76) </ref>. This conjecture has been labeled the Physical Symbol System Hypothesis by Newell and Simon (see section 2.2.1). This very strong position, has been attacked by the dynamicists in their attempt to validate their own approach to cognitive theorizing. <p> the processes which are out of reach of the computational model (van Gelder and Port, in press, p. 17) by virtue of its commitment to symbolic representational computation. 2.2.5 Representation By defining cognitive systems as necessarily representational, and then claiming symbolic could be defined to be essentially synonymous with representational <ref> (Newell, 1990, p. 72) </ref>, Newell has noted the close relationship between computational and representational commitments. From the assumption that any form of representation naturally denotes a symbolic representation, it is obvious why symbolicists in general rely on logic-like structures to represent knowledge (Newell, 1990, p. 54). <p> From the assumption that any form of representation naturally denotes a symbolic representation, it is obvious why symbolicists in general rely on logic-like structures to represent knowledge <ref> (Newell, 1990, p. 54) </ref>. Newell claims that what a logic lets us do is represent the knowledge of X as a finite set of expressions plus a process for generating the infinite set of other expressions that comprise X's total knowledge (Newell, 1990, p. 55). <p> Newell claims that what a logic lets us do is represent the knowledge of X as a finite set of expressions plus a process for generating the infinite set of other expressions that comprise X's total knowledge <ref> (Newell, 1990, p. 55) </ref>. Thus, in an implementation like Soar or Act* for that matter, all knowledge is represented in the form of a production system - a list of if-then statements. <p> Thus, in an implementation like Soar or Act* for that matter, all knowledge is represented in the form of a production system - a list of if-then statements. The dynamicist would note that it is important to realize that assuming the representational medium is a list structure <ref> (Newell, 1990, p. 61) </ref>, is not based on biological analysis; and cannot be supported by such analysis. <p> Thus, it is rather dubious that the symbolicist has empirical support for claiming: Putatively, all that the lower-level systems of the biological band do is support the computational mechanisms, which in turn operate to enforce the representational laws <ref> (Newell, 1990, p. 149) </ref>. Of course, such assumptions are based on 33 functional observations on the part of the symbolicist 11 - humans do, after all, process symbols (e.g. in our use of language, logic, mathematics, etc.). Dynamicists tend towards a different extreme. <p> Thus the system is influenced by the environment and the system dynamically self-organizes to reflect such influences. If our brain is coupled to the real world in a dynamical way, the Cartesian and symbolicist views of the brain as a self-contained representer <ref> (Newell, 1990) </ref> are inaccurate. However, it is not easy to convincingly deny that representation plays an important role in cognition. It seems obvious that human cognizers use representation in their dealings with the world around them. <p> not communicate with other modules by passing representations; they interact by being coupled (i.e., by simultaneously influencing each others behavior) (van Gelder and Port, in press, p. 9). 34 A simple, yet accurate, definition of connectionism is provided by Newell: connectionism is a commitment to a particular neural-like computational technology <ref> (Newell, 1990, p. 484) </ref>. In The Computational Brain, Churchland and Sejnowski note that their commitment to this computational technology results in the implicit hypothesis that emergent properties are high-level effects that depend on lower-level phenomena in some systematic way (Churchland and Sejnowski, 1992, p. 2). <p> motion perception, at least for the reason that other areas also have neurons that selectively respond to motion and where lesions result in deficits in motion perception (Churchland and Sejnowski, 1992, p. 317). 40 become organized in task-relevant ways just by performing tasks, without requiring programming by an intelligent agent <ref> (Newell, 1990, p. 485) </ref>. Though symbolicist systems like Soar can behave in a similar way, they do not yet do so continuously that aspect of cognition has been put off to [Soar's] future agenda (Newell, 1990, p. 488). <p> Though symbolicist systems like Soar can behave in a similar way, they do not yet do so continuously that aspect of cognition has been put off to [Soar's] future agenda <ref> (Newell, 1990, p. 488) </ref>. Natural cognizers seem to self-organize continuously (Thelen and Smith, 1994, p. 203), so it is a distinct advantage for connectionist models to be able to easily incorporate continuous organization. <p> The Churchland and Sejnowski (1992) book never once refers to such practices. Furthermore, Newell never even hints at such a devastating claim, on the contrary he notes: another cognitive power (of connectionist systems) that seems significant is finding the solution to very large systems of weak or soft constraints <ref> (Newell, 1990, p. 485) </ref>. A power which is unique to connectionism, due to the difference in their approaches to handling representation and computation. This attempt at discrediting the computational perspective of connectionism seems misguided and somewhat halfhearted.
Reference: <author> Popper, K. </author> <year> (1981). </year> <title> The rationality of scientific revolutions. </title> <publisher> Scientific Revolutions. </publisher> <address> New York, </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: In cognitive science, and on the Popperian view of scientific explanation, without any model there is no support for the acceptance of a theory. Thus, without a model, the statement which may be called a theory is not falsifiable (i.e. no elimination of error <ref> (Popper, 1981, p. 83) </ref>, and should be correctly termed a hypothesis. Of course, in practice theories are sometimes held without strong empirical support, as we shall see.
Reference: <author> Putnam, H. </author> <year> (1981). </year> <title> The corroboration of theories. </title> <publisher> Scientific Revolutions. </publisher> <address> New York, </address> <publisher> Oxford University Press. </publisher>
Reference: <author> Robertson, S. S., A. H. Cohen, G. Mayer-Kress. </author> <year> (1993). </year> <title> Behavioural Chaos: Beyond the Metaphor. A dynamic systems approach to development: Applications. </title> <publisher> Cambridge, MIT Press. </publisher> <pages> 120-150. </pages>
Reference-contexts: Even though dynamical concepts and theory are seductive, we may mistake translation for explanation <ref> (Robertson, Cohen et al., 1993, p. 119) </ref>. We cannot allow ourselves to accept new concepts which do not deepen our understanding of the system being modeled. <p> However, upon further investigation, the only conclusions that could be drawn were: We clearly know very little about the biological substrate of CM <ref> (Robertson, Cohen et al., 1993, p. 147) </ref>. In the end, there is no completed dynamicist model presented, though models which do not work are discounted. So, they have employed dynamicist models to constrain the solution, but not to provide new insights. <p> In their closing remarks, they note: We are therefore a long way from the goal of building a dynamical model of CM in which the state variables and parameters have a clear correspondence with psychobiological and environmental factors <ref> (Robertson, Cohen et al., 1993, p. 147) </ref>. In other words, a truly dynamicist model is still a future consideration.
Reference: <author> Schweicker, R. and G. J. </author> <title> Boggs (1984). Models of central capacity and concurrency. </title> <journal> Journal of Mathematical Psychology 28: </journal> <pages> 223-281. </pages>
Reference: <author> Simon, H. A. and A. </author> <title> Newell (1970). </title> <booktitle> Information-processing in computer and man. Perspectives on the computer revolution. </booktitle> <address> Englewood Cliffs, </address> <publisher> Prentice-Hall, Inc. </publisher>
Reference-contexts: Thus, the dynamicists are insisting that there is an inherent value in understanding cognition as dynamical instead of connectionist or symbolicist (van Gelder and Port, in press). Some critics will claim that a dynamical systems approach to cognition is simply not new <ref> (Simon and Newell, 1970, p. 273) </ref>. Giunti (1991) showed that the symbolicist Turing Machine is a dynamical system (van Gelder, 1993). So, it could be concluded that there is nothing to gain from introducing a separate dynamicist method of studying cognition. <p> They are not linked in the same way to their environment, and the types of processing and behavior exhibited is qualitatively different. Thus it is difficult, in any practical way, to see dynamicist and symbolicist models as truly equivalent <ref> (Simon and Newell, 1970) </ref>. However, Smolensky's (1988) claim that connectionism presents a dynamical systems approach to modeling cognition is different altogether. Connectionist nets are inherently coupled, nonlinear, parallel dynamical systems. These systems are self-organizing and evolve based on continuously varying input from their environment.
Reference: <author> Skarda, C. A. and W. J. </author> <title> Freeman (1987). How brains make chaos in order to make sense of the world. </title> <booktitle> Behavioral and Brain Sciences 10: </booktitle> <pages> 161-195. </pages>
Reference-contexts: Of these two approaches, connectionism has had a longer life span. Thus, significantly more connectionist models than dynamicist models have been developed. In many cases, however, the same model is claimed to support both dynamicist and connectionist interests <ref> (e.g. Skarda and Freeman, 1987) </ref>. Thus, a distinction between connectionism and dynamicism is often quite subtle. <p> This output was achieved by setting a number of feedback gains and distributed delays in accordance with our understanding of the anatomy and physiology of the larger system <ref> (Skarda and Freeman, 1987, p. 166) </ref> in the set of differential equations that had been chosen to model the olfactory bulb. It is important to note that the behavior of the system can be greatly affected by the choice of certain parameters, especially if the system is potentially chaotic. <p> Most tellingly, the authors themselves saw their paper and model as showing that the brain may indeed use computational mechanisms like those found in connectionist models <ref> (Skarda and Freeman, 1987, p. 161) </ref>. Furthermore, they realized that: Our model supports the line of research pursued by proponents of connectionist or parallel distributed processing (PDP) models in cognitive science (Skarda and Freeman, 1987, p. 170). Dynamicists, however, wish to rest a new cognitive paradigm on such a model. <p> Furthermore, they realized that: Our model supports the line of research pursued by proponents of connectionist or parallel distributed processing (PDP) models in cognitive science <ref> (Skarda and Freeman, 1987, p. 170) </ref>. Dynamicists, however, wish to rest a new cognitive paradigm on such a model. Ironically, the model itself looks very much like a connectionist network, with the less typical addition of inhibition and far more complex transfer functions at each node. <p> most rigorous of dynamical models, such as the Skarda and Freeman model previously discussed, extending dynamical concepts beyond the metaphorical proves difficult: Given this broad picture of the dynamics of this neural system we can sketch a metaphorical picture of its multiple stable states in terms of a phase portrait <ref> (Skarda and Freeman, 1987, p. 166) </ref>. Furthermore, in its psychological dimension our model is extremely limited, being competent to simulate only preattentive cognition (Skarda and Freeman, 1987, p. 170). <p> Furthermore, in its psychological dimension our model is extremely limited, being competent to simulate only preattentive cognition <ref> (Skarda and Freeman, 1987, p. 170) </ref>. The concepts of dynamical systems theory provide an interesting method of thinking about cognitive systems, but they have not yet been shown to be 55 successfully transferable to rigorous definitions of human behavior or cognition.
Reference: <author> Smolensky, P. </author> <year> (1988). </year> <title> On the proper treatment of connectionism. </title> <booktitle> Behavioral and Brain Sciences 11(1): </booktitle> <pages> 1-23. </pages>
Reference-contexts: There is seldom an attempt to simply substitute patterns of activation for symbols. The type of computations being done are so different that such a substitution would defeat the purpose of using connectionist networks in the first place <ref> (Smolensky, 1988) </ref>. Furthermore, symbolicist symbols 42 and connectionist patterns of activation behave very differently.
Reference: <author> Thagard, P. </author> <year> (1992). </year> <title> Conceptual revolutions. </title> <publisher> Princeton, Princeton University Press. </publisher>
Reference-contexts: One of the best known refutations of this position was given by Chomsky in his 1959 review of B. F. Skinner's book Verbal Behavior. Subsequently, behaviorism fell out of favor as it was further shown that the behaviorist approach was inadequate for explicating even basic animal learning <ref> (Thagard, 1992, p. 231) </ref>. The reasons for the behaviorist failure was its fundamental rejection of representation in natural cognizers. Dynamicists have forwarded a similar rejection of representation as important to cognition (see sections 2.1.3, 2.2.5, 2.3.5). Consequently, they fall prey to the same criticism forwarded over three decades ago. <p> Consequently, they fall prey to the same criticism forwarded over three decades ago. The early work of researchers like Johnson-Laird, Miller, Simon and Newell firmly established a general commitment to representation in cognitive science inquiries <ref> (Thagard, 1992, p. 233) </ref>. There have been no reasons given by dynamicists which would fundamentally disturb this commitment. Finally, it seems quite clear that the average human cognizer is continually manipulating representations.
Reference: <author> Thagard, P. </author> <title> (in press). Modelling conceptual revolutions. Dialogue. </title>
Reference: <author> Thelen, E. and L. B. </author> <title> Smith (1994). A dynamic systems approach to the development of cognition and action. </title> <publisher> Cambridge, MIT Press. </publisher>
Reference-contexts: Dynamicists have critically examined both symbolicism and connectionism and have decided to dismiss these theories of cognition and instead wish to propose a radical departure from current cognitive theory, one in which there are no structures and there are no rules <ref> (Thelen and Smith, 1994, p. xix) </ref>. The dynamicist view relies heavily on an area of mathematics referred to as dynamical systems theory. The concepts of dynamical systems theory are applied by dynamicists to a description of cognition. <p> Though symbolicist systems like Soar can behave in a similar way, they do not yet do so continuously that aspect of cognition has been put off to [Soar's] future agenda (Newell, 1990, p. 488). Natural cognizers seem to self-organize continuously <ref> (Thelen and Smith, 1994, p. 203) </ref>, so it is a distinct advantage for connectionist models to be able to easily incorporate continuous organization. As well, the dynamicist can rightfully claim that this ability is naturally within the scope of dynamicism for a connectionist network is a dynamical system. <p> Dynamicists propose to explain all of cognition - a cognitive theory cannot claim to be more unified. Though dynamicists to date have often limited the scope of their models to locomotor issues, their objective is unequivocally to explain all aspects of cognition <ref> (Thelen and Smith, 1994) </ref>. Clearly, the dynamical systems approach to cognition is intended as a unified theory, one which is expected to rival the existing connectionist and symbolicist theories of cognitive science (van Gelder and Port, in press). <p> Dynamicists tend to be quite succinct in presenting their opinion of what this relation should be and are not shy about their project to replace current cognitive approaches: we propose here a radical departure from current cognitive theory. <ref> (Thelen and Smith, 1994, p. xix) </ref>. The dynamicist wish to dissolve connectionism and symbolicism has given them reason to critically assess the theoretical commitments of both paradigms. A detailed comparison of the three accounts has been provided in chapter 2. <p> Critiques of connectionism from dynamicists do not seem to present any sort of united front. Some dynamicists note the lack of realism in some networks (Globus, 1992). Others reject connectionism not because of a failure in principle but because of a failure of spirit <ref> (Thelen and Smith, 1994, p.41) </ref>. 24 Still others reject connectionism as being too committed to symbolicist ideas: ideas like representation (van Gelder and Port, in press, p. 27). The lack of realism in networks is often intentional due to the limitations of current computational power.
Reference: <author> Ubbink, J. B. </author> <year> (1961). </year> <title> Model description and knowledge. The concept and the role of the model in mathematics and natural and social sciences. </title> <publisher> Holland, </publisher> <address> D. </address> <publisher> Reidel Publishing. </publisher> <editor> van Gelder, T. </editor> <year> (1993). </year> <note> What might cognition be if not computation? Cognitive Sciences Indiana University Research Report 75 : van Gelder, </note> <author> T. and R. </author> <title> Port (in press). Its about time: An overview of the dynamical approach to cognition. Mind as motion: Explorations in the dynamics of cognition. </title> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, a physical system/program mapping has a nonphysical target, namely the behaviors of particular equations or, more generally, functions. 10 physical reproduction of the system to provide the description. More precisely a functioning computer model can be viewed as an implementation of the mathematical model <ref> (Ubbink, 1961) </ref>. This type of computer modeling, commonly in the form of a program, is often referred to as simulation. Simulation is by far the most common method of modeling used in cognitive science. <p> Similarly, synonymous use of the terms model and picture when referring to theory can be found in some relevant literature <ref> (Ubbink, 1961, p. 179) </ref>, though this usage is too imprecise for our purposes.
References-found: 40

