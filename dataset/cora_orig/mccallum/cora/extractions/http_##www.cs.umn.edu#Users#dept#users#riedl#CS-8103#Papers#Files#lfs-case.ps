URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Files/lfs-case.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/riedl/CS-8103/Papers/Files/
Root-URL: http://www.cs.umn.edu
Title: Beating the I/O Bottleneck: A Case for Log-Structured File Systems  
Author: John Ousterhout Fred Douglis 
Note: The work described here was supported in part by the National Science Foundation under Presidential Young Investigator Award No. ECS-8351961 and Grant No. MIP-8715235.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California at Berkeley  
Abstract: CPU speeds are improving at a dramatic rate, while disk speeds are not. This technology shift suggests that many engineering and office applications may become so I/O-limited that they cannot benefit from further CPU improvements. This paper discusses several techniques for improving I/O performance, including caches, battery-backed-up caches, and cache logging. We then examine in particular detail an approach called log-structured file systems, where the file system's only representation on disk is in the form of an append-only log. Log-structured file systems po tentially provide order-of-magnitude improvements in write performance. When log-structured file systems are combined with arrays of small disks (which provide high bandwidth) and large main-memory file caches (which satisfy most read accesses), we believe it will be possible to achieve 1000-fold improvements in I/O performance over today's systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amdahl, G. </author> <title> ``Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities.'' </title> <booktitle> Proc. AFIPS 1967 Spring Joint Computer Conference, </booktitle> <address> Atlantic City, N.J., </address> <month> April </month> <year> 1967. </year>
Reference-contexts: This notion was first put forth by Gene Amdahl in the late 1960's <ref> [1] </ref>, and has since come to be known as ``Amdahl's Law.'' Recent technology trends suggest that disk input and output may become such a bottleneck in the near future. <p> Bursts of reads will mostly be satisfied in the cache; bursts of writes can be buffered in the cache, with the actual disk I/O per formed asynchronously without requiring the writing process to wait. At this point, two interesting issues emerge: <ref> [1] </ref> Disk traffic in the future will become more and more dominated by writes, and most of the data written will never be read back (it will live in the file cache until deleted or overwritten).
Reference: [2] <author> Finlayson, R., and Cheriton, D. </author> <title> ``Log Files: An Extended File Service Exploiting Write-Once Storage.'' </title> <booktitle> Proc. Eleventh Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987, </year> <pages> pp. 139-148. </pages>
Reference-contexts: The main reason for performing disk I/O is as a safety precaution in case the cache contents are lost. <ref> [2] </ref> Seeks limit disk performance. If files are only 3-4 kbytes long and two transfers are required per file, then only 3% of the raw disk bandwidth can actu ally be utilized. <p> These systems assumed infinite storage capacity or write-once media, so they did not address wrap around issues. Finlayson and Cheriton have recently implemented a system providing append-only log files and propose them as a general-purpose construct <ref> [2] </ref>. Finlay-son and Cheriton's system allows retrieval from the log with cost proportional to the logarithm of the log size (in comparison to the constant cost of the mechanism in Section 7.1).
Reference: [3] <author> Gait, J. </author> <title> ``The Optical File Cabinet: A Random-Access File System for Write-Once Optical Disks.'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 21, No. 6, </volume> <month> June </month> <year> 1988, </year> <pages> pp. 11-22. </pages>
Reference-contexts: In contrast, our motivation for logging is to achieve high performance with read-write media. At least three recent projects have used logging as part of a file system for write-once media: Swallow [15], CDFS [4], and the Optical File Cabinet <ref> [3] </ref>. The Swallow system also included a generational approach to handle media with different characteristics, and the CDFS and the Optical File Cabinet papers describe mechanisms for random-access retrieval from their logs. These systems assumed infinite storage capacity or write-once media, so they did not address wrap around issues.
Reference: [4] <author> Garfinkel, S., and Love, J. </author> <title> ``A File System for Write-Once Media.'' </title> <publisher> MIT Media Lab report, </publisher> <month> October </month> <year> 1986. </year>
Reference-contexts: In contrast, our motivation for logging is to achieve high performance with read-write media. At least three recent projects have used logging as part of a file system for write-once media: Swallow [15], CDFS <ref> [4] </ref>, and the Optical File Cabinet [3]. The Swallow system also included a generational approach to handle media with different characteristics, and the CDFS and the Optical File Cabinet papers describe mechanisms for random-access retrieval from their logs.
Reference: [5] <author> Hagmann, R. </author> <title> ``Reimplementing the Cedar File System Using Logging and Group Commit.'' </title> <booktitle> Proc. Eleventh Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987, </year> <pages> pp. 155-162. </pages>
Reference-contexts: UNIX-like model we have assumed, and they also assumed infinite storage capacity so they did not have to address log wrap-around issues. - 15 - Beating the I/O Bottleneck January 30, 1992 One project where logging has been used to improve performance is Hagmann's work on the Cedar File System <ref> [5] </ref>. He used logs for file maps in order to provide high reliability of map information without performance degradation. However, Hagmann used logs only for the map information, so his mechanisms did not reduce seeks for file data.
Reference: [6] <author> Howard, J., et al. </author> <title> ``Scale and Performance in a Distributed File System.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, No. 1, </volume> <month> February </month> <year> 1988, </year> <pages> pp. 51-81. </pages>
Reference: [7] <author> Kim, M. </author> <title> ``Synchronized Disk Interleaving.'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-35, No. 11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Many scientific applications and some engineering and database applications fall into this category. If sequential blocks of large files are spread evenly across the disks of an array (``striped''), then all the disks can be used simultaneously to read or write the files <ref> [7] </ref>. If the files are large enough, then the cost of seeks will be small compared to the cost of reading the data, and the file system's performance will scale with the size of the array.
Reference: [8] <author> Lazowska, E., et al. </author> <title> ``File Access Performance of Diskless Workstations.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 4, No. 3, </volume> <month> August </month> <year> 1986, </year> <pages> pp. 238-268. </pages>
Reference: [9] <author> McKusick, M., et al. </author> <title> ``A Fast File System for UNIX.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2, No. 3, </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: In the environment of the mid 1990's with 100 MIP workstations, each user might then generate 250-1000 kbytes/sec. of I/O traffic. The average file size in engineering/office environments is only 3-4 kbytes [11,13]. With today's file organizations (e.g. 4.3 BSD UNIX <ref> [9] </ref>), two disk transfers are required for each file access: one to read or write file header information, and another to read or write the file's data. Typical disks today can only make about 30-40 transfers per second. <p> Thus a random-access retrieval mechanism seems essen tial if a log-structured file system is to provide improved overall performance. In a traditional file system such as 4.3 BSD UNIX <ref> [9] </ref> there are two steps in locating a file. First, its textual name must be translated into an internal identifier for the file. Second, given the internal identifier, the blocks of the file must be located.
Reference: [10] <author> Nelson, M., Welch, B., and Ousterhout, J. </author> <title> ``Caching in the Sprite Network File System.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, No. 1, </volume> <month> Febru-ary </month> <year> 1988, </year> <pages> pp. 134-154. </pages>
Reference: [11] <author> Ousterhout, J., et al. </author> <title> ``A Trace-Driven Analysis of the UNIX 4.2 BSD File System.'' </title> <booktitle> Proc. Tenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: File servers of the future are likely to have file caches with hundreds of Mbytes; this should be enough to hold all the files used by groups of dozens of users over periods of days or weeks (see <ref> [11] </ref> for supporting evidence). As a result, almost all read requests should be satisfied in the file cache. Unfortunately, about 1/3 of file I/O (measured by either files accessed or bytes transferred) is writes [11], and most of this data must be written to disk. <p> the files used by groups of dozens of users over periods of days or weeks (see <ref> [11] </ref> for supporting evidence). As a result, almost all read requests should be satisfied in the file cache. Unfortunately, about 1/3 of file I/O (measured by either files accessed or bytes transferred) is writes [11], and most of this data must be written to disk. <p> One possible approach is to place newly-written data in the cache and delay writes to disk for a while; some of the newly-written data will be deleted before they are written to disk, and disk I/O requirements will be reduced correspondingly (for example, <ref> [11] </ref> measured that 15-20% of all newly-written bytes are deleted within 30 seconds; in recent measurements of our current system, we found that 40% or more of all new bytes lived less than 30 seconds and 90% lived less than a day).
Reference: [12] <author> Patterson, D., Gibson, G., and Katz, R. </author> <title> ``A Case for Redundant Arrays of Inexpensive Disks (RAID).'' </title> <booktitle> ACM SIGMOD 88, </booktitle> <address> Chicago, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: We are now seeing the advent of disk arrays, where large high-performance reliable disk systems are created by combining many small inexpensive disks <ref> [12] </ref>. By 1995, we predict that disk arrays with hundreds or even thousands of disks will be standard products. - 2 - Beating the I/O Bottleneck January 30, 1992 Disk arrays offer at least two potential performance advantages.
Reference: [13] <author> Satyanarayanan, M. </author> <title> ``A Study of File Sizes and Functional Lifetimes.'' </title> <booktitle> Proc. Eighth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1981, </year> <pages> pp. 96-108. </pages>
Reference: [14] <author> Schroeder, M., Gifford, D., and Needham, R. </author> <title> ``A Caching File System for a Programmer's Workstation.'' </title> <booktitle> Proc. Tenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1985, </year> <pages> pp. 25-34. </pages> - <note> 17 - Beating the I/O Bottleneck January 30, </note> <year> 1992 </year>
Reference: [15] <author> Svobodova, L. </author> <title> ``A Reliable-Object-Oriented Repository for a Distributed Computer System.'' </title> <booktitle> Proc. Eighth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1981, </year> <pages> pp. 47-58. </pages>
Reference-contexts: In contrast, our motivation for logging is to achieve high performance with read-write media. At least three recent projects have used logging as part of a file system for write-once media: Swallow <ref> [15] </ref>, CDFS [4], and the Optical File Cabinet [3]. The Swallow system also included a generational approach to handle media with different characteristics, and the CDFS and the Optical File Cabinet papers describe mechanisms for random-access retrieval from their logs.
Reference: [16] <author> Ungar, D. </author> <title> ``Generation Scavenging: A Non-Disruptive High Performance Storage Reclamation Algorithm.'' </title> <booktitle> Proc. Software Engineering Symposium on Practical Software Development Environments, </booktitle> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1984, </year> <pages> pp. 157-167. </pages>
Reference-contexts: For example, there might be a hierarchy of logs where information gets archived from log to log as the logs wrap. This approach produces a result similar to garbage col lectors based on generation scavenging <ref> [16] </ref>. It offers the possibility of integrating archival storage (such as optical disks or digital video-cassettes) into the system as the most senior level in the log hierarchy. One of the most important issues in handling log wrap-around is how to identify the files that are still live.
References-found: 16

