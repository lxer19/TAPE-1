URL: ftp://info.mcs.anl.gov/pub/ADOLC/PAPERS/juedes.ps.gz
Refering-URL: http://www.mcs.anl.gov/Projects/autodiff/AD_Tools/adolc.anl/adolc.html
Root-URL: http://www.mcs.anl.gov
Title: Chapter 1 Generalized Neural Networks, Computational Differentiation, and Evolution  
Author: David W. Juedes Karthik Balakrishnan 
Abstract: Backpropagation is a powerful and widely used procedure for training multilayer, feedforward artifical neural networks. The procedure can be seen as a special case of the reverse mode of computational differentiation. This connection between backpropagation and computational differentiation leads us to envision a scenario wherein neural networks can be trained by using gradient descent methods and computational differentiation tools like ADOL-C. The primary advantage offered by such an approach is the possibility of training networks consisting of heterogeneous functional units|a notion we refer to as generalized neural networks. This approach, in conjuction with evolutionary algorithms, can be used to produce near-optimal designs. This paper presents this approach in more detail and demonstrates its usefulness through simulation results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Balakrishnan and V. Honavar, </author> <title> Evolutionary design of neural architectures | a preliminary taxonomy and guide to literature, </title> <type> Tech. Rep. CS TR 95-01, </type> <institution> Department of Computer Science, Iowa State University, Ames, </institution> <address> IA - 50011, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: In this work we constrain the choice of activation functions to ones that are differentiable. This would enable such networks to be trained using gradient descent techniques like BP. Further, the functions should also have bounded output (e.g., in the interval <ref> [-1, 1] </ref>) to prevent the weights from becoming unbounded themselves. Functions such as sigmoid, sine, cosine, tanh, and gaussian are suitable candidates for GNNs. <p> Evolutionary approaches such as genetic algorithms (GAs), have been shown to produce near-optimal results in vast, complex, and multimodal search spaces [8, 5]. They are thus natural candidates for searching the space of network architectures <ref> [1] </ref>. These algorithms are models of processes that appear to be at work in biological evolution.
Reference: [2] <author> T. </author> <title> Cover, Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition, </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 14 (1965), </volume> <pages> pp. 326-334. </pages> <note> 14 Juedes and Balakrishnan </note>
Reference-contexts: Many researchers have argued this point, and there is considerable reason to believe that higher order networks may help realize mappings more efficiently and compactly than conventional ANNs <ref> [2] </ref>. In this paper, we examine neural networks that are inherently heterogeneous. These generalized neural networks differ from standard neural networks in the sense that they can incorporate a variety of activation functions. Here we show that these networks can compactly represent several well-studied problems. <p> This is a trivial example of the ability to produce compact networks by the wise choice of the activation function. It is well known in the neural network community that supplementing the inputs of an ANN with higher-order combinations of the inputs increases the capacity of the network <ref> [2] </ref> and the ability to learn geometrically invariant properties [4]. Product units [3], which compute a weighted product of their inputs rather than a weighted sum, fall into this category and have been shown to be of good use in a number of problems [10].
Reference: [3] <author> R. Durbin and D. Rumelhart, </author> <title> Product units: A computationally powerful and biologically plausible extension to backpropagation networks, </title> <booktitle> Neural Computation, 1 (1989), </booktitle> <pages> pp. 133-142. </pages>
Reference-contexts: It is well known in the neural network community that supplementing the inputs of an ANN with higher-order combinations of the inputs increases the capacity of the network [2] and the ability to learn geometrically invariant properties [4]. Product units <ref> [3] </ref>, which compute a weighted product of their inputs rather than a weighted sum, fall into this category and have been shown to be of good use in a number of problems [10]. With these in mind, we have been exploring ANNs that contain units incorporating (possibly) different activation functions.
Reference: [4] <author> C. Giles and T. Maxwell, </author> <title> Learning, invariance, and generalization in high-order neural networks, </title> <journal> Applied Optics, </journal> <volume> 26 (1987), </volume> <pages> pp. 4972-4978. </pages>
Reference-contexts: It is well known in the neural network community that supplementing the inputs of an ANN with higher-order combinations of the inputs increases the capacity of the network [2] and the ability to learn geometrically invariant properties <ref> [4] </ref>. Product units [3], which compute a weighted product of their inputs rather than a weighted sum, fall into this category and have been shown to be of good use in a number of problems [10].
Reference: [5] <author> D. E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachutsetts, </address> <year> 1989. </year>
Reference-contexts: For example, Saarinen et al. [18] show that backpropagation is essentially the reverse mode of computational differentiation. In a related context, Yoshida [21] uses computational differentiation and two-dimensional conjugate gradient search to train neural networks.) Finally, we address the issue of network architecture design by demonstrating how evolutionary algorithms <ref> [5, 8] </ref> and computational differentiation can be used to build near-minimal networks. 2 ANN and BP Conventional ANNs are highly interconnected networks of relatively simple computing elements. In the accepted parlance, the computing elements are referred to as neurons or units, while the interconnections are typically called weights. <p> Thus, if the problem on hand is characterized by a large search space with sparsely populated solutions | the random searches become highly ineffective. Evolutionary approaches such as genetic algorithms (GAs), have been shown to produce near-optimal results in vast, complex, and multimodal search spaces <ref> [8, 5] </ref>. They are thus natural candidates for searching the space of network architectures [1]. These algorithms are models of processes that appear to be at work in biological evolution.
Reference: [6] <author> A. Griewank, D. Juedes, and J. Utke, ADOL-C, </author> <title> A package for the automatic differentiation of algorithms written in C/C++, </title> <journal> ACM Trans. Math. Software, </journal> <note> (to appear). Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: In place of BP, we propose the use of computational differentiation. In this section we show how ADOL-C <ref> [6] </ref> can be used to train GNN. Consider an arbitrary GNN whose activation functions are composites of the standard functions like sigmoid, sine, cosine, and tanh. Such a network can be trained using ADOL-C by performing the following basic steps. (See [6] for more information and examples.) 1. <p> In this section we show how ADOL-C <ref> [6] </ref> can be used to train GNN. Consider an arbitrary GNN whose activation functions are composites of the standard functions like sigmoid, sine, cosine, and tanh. Such a network can be trained using ADOL-C by performing the following basic steps. (See [6] for more information and examples.) 1. Design a standard C function that evaluates the network and computes the error on a given input/output pair. 2. Convert each float or double variable in the C function to variable of the class adouble as defined in adouble.h. 3.
Reference: [7] <author> R. Hecht-Nielsen, </author> <title> Neurocomputing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction From pattern recognition to robotics, database retrieval to stock-market prediction, medical diagnosis to manufacturing and process control, artificial neural networks (ANNs) are finding widespread application <ref> [7] </ref>. Apart from their resemblance (though superficial) to the information-processing structures in the brain, their popularity can be attributed to a variety of factors|primarily their ability to elegantly capture input-output associations. Mathematically, ANNs perform a function, mapping the input space to the output space.
Reference: [8] <author> J. Holland, </author> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: For example, Saarinen et al. [18] show that backpropagation is essentially the reverse mode of computational differentiation. In a related context, Yoshida [21] uses computational differentiation and two-dimensional conjugate gradient search to train neural networks.) Finally, we address the issue of network architecture design by demonstrating how evolutionary algorithms <ref> [5, 8] </ref> and computational differentiation can be used to build near-minimal networks. 2 ANN and BP Conventional ANNs are highly interconnected networks of relatively simple computing elements. In the accepted parlance, the computing elements are referred to as neurons or units, while the interconnections are typically called weights. <p> Thus, if the problem on hand is characterized by a large search space with sparsely populated solutions | the random searches become highly ineffective. Evolutionary approaches such as genetic algorithms (GAs), have been shown to produce near-optimal results in vast, complex, and multimodal search spaces <ref> [8, 5] </ref>. They are thus natural candidates for searching the space of network architectures [1]. These algorithms are models of processes that appear to be at work in biological evolution.
Reference: [9] <author> N. Karmarkar and R. Karp, </author> <title> The differencing method of set partitioning, </title> <type> Tech. Rep. </type> <institution> UCB/CSD 82/113, University of California, Berkeley, Computer Science Division, </institution> <year> 1982. </year>
Reference-contexts: Such approaches use a procedure like simulated annealing (SA) or random restarts <ref> [9] </ref> to perform a global search for network architectures, which are then locally trained using BP. The effectiveness of the global searches in this context is bounded by the dimension of the search space and the density of the solution points.
Reference: [10] <author> L. Leerink, C. Giles, B. Horne, and M. Jabri, </author> <title> Learning with product units, </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <year> 1995, </year> <pages> pp. 537-544. </pages>
Reference-contexts: Product units [3], which compute a weighted product of their inputs rather than a weighted sum, fall into this category and have been shown to be of good use in a number of problems <ref> [10] </ref>. With these in mind, we have been exploring ANNs that contain units incorporating (possibly) different activation functions. We label such networks generalized neural networks (GNNs), more to differentiate them from conventional ANNs than to introduce any new nomenclature. <p> These biases need to be explored in more detail if the methods are to be used on real-world problems. (A systematic evaluation and comparision of these and other approaches is already underway [13].) As an alternative, Leerink et al., <ref> [10] </ref> and others have used combinations of global and local search techniques. Such approaches use a procedure like simulated annealing (SA) or random restarts [9] to perform a global search for network architectures, which are then locally trained using BP.
Reference: [11] <author> W. S. McCulloch and W. Pitts, </author> <title> A logical calculus of ideas immanent in nervous activity, </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 (1943), </volume> <pages> pp. 115-133. </pages>
Reference-contexts: These networks are assumed to be simplistic, organized structures, with well-defined input (sensory), hidden (associative), and output (response) layers, as shown in Figure 1. The era of artificial neural computing began with the seminal work of McCulloch and Pitts <ref> [11] </ref>, who observed that biological neurons behave in many ways like linear threshold units. Building on the work of McCulloch, Pitts, and others, Rosenblatt [14, 15, 16] defined a very simple learning device consisting of a single linear threshold unit built on top of additional hardware.
Reference: [12] <author> M. Minsky and S. Papert, </author> <title> Perceptrons, </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: More precisely, Rosenblatt showed that if there is a setting of the weights and the threshold of the perceptron that solves a given problem, the learning rule is guaranteed to discover it. This is the famous perceptron convergence theorem <ref> [12] </ref>. Unfortunately, Minsky and Papert [12] showed that perceptrons are computationally limited and cannot compute nonlinearly separable functions. For example, a perceptron cannot compute the XOR function. <p> More precisely, Rosenblatt showed that if there is a setting of the weights and the threshold of the perceptron that solves a given problem, the learning rule is guaranteed to discover it. This is the famous perceptron convergence theorem <ref> [12] </ref>. Unfortunately, Minsky and Papert [12] showed that perceptrons are computationally limited and cannot compute nonlinearly separable functions. For example, a perceptron cannot compute the XOR function. <p> Process of evolutionary design of neural architectures 6 Simulation Results The ability of GNNs to represent mappings through compact networks is most easily illustrated in single-node networks. Consider, for example, the parity problem. 2 While it is well known <ref> [12] </ref> that a network of a single sigmoidal or threshold unit cannot represent the parity problem, it is straightforward to design single-node sine or cosine networks to solve parity. (See, for example, Figure 5.) Moreover, it is relatively straightforward to prove that a network of a single sine gate can solve
Reference: [13] <author> R. Parekh, J. Yang, and V. Honavar, </author> <title> Constructive neural network learning algorithms for multi-category pattern classification, </title> <type> Tech. Rep. ISU CS TR 95-15a, </type> <institution> Iowa State University,, Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: However, the problem of determining good network architectures still remains an elusive issue. Some heuristic approaches in the literature allow one to incrementally construct networks by adding one or more units at a time. Popular examples of such constructive algorithms are tower, tiling, upstart, and cascade-correlation <ref> [13] </ref>. The first three algorithms typically are used for constructing multilayer perceptron networks, while cascade-correlation is used with networks of sigmoid units. <p> These biases need to be explored in more detail if the methods are to be used on real-world problems. (A systematic evaluation and comparision of these and other approaches is already underway <ref> [13] </ref>.) As an alternative, Leerink et al., [10] and others have used combinations of global and local search techniques. Such approaches use a procedure like simulated annealing (SA) or random restarts [9] to perform a global search for network architectures, which are then locally trained using BP.
Reference: [14] <author> F. Rosenblatt, </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain, </title> <journal> Psychological Review, </journal> <volume> 65 (1958), </volume> <pages> pp. </pages> <month> 386-408. </month> <title> [15] , Perceptron simulation experiments, </title> <booktitle> Proceedings of the IRE, 48 (1960), </booktitle> <pages> pp. 301-309. </pages> <booktitle> [16] , Principles of Neurodynamics, </booktitle> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: The era of artificial neural computing began with the seminal work of McCulloch and Pitts [11], who observed that biological neurons behave in many ways like linear threshold units. Building on the work of McCulloch, Pitts, and others, Rosenblatt <ref> [14, 15, 16] </ref> defined a very simple learning device consisting of a single linear threshold unit built on top of additional hardware. These simple networks, called perceptrons, could be trained via a simple learning rule that roughly corresponds to gradient descent.
Reference: [17] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> Learning internal representations by error-propagation, in Parallel Distributed Processing, </title> <editor> D. E. Rumelhart and R. J. McClelland, eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <note> ch. 8. </note>
Reference-contexts: An artificial neural network multilayer perceptrons can compute any Boolean function, there is no analogous training rule for such networks because the internal threshold units are not differentiable and thus not amenable to gradient descent. As noted by Rumelhart et al. <ref> [17] </ref>, the training problem for multilayer neural networks can be solved by replacing the nondifferentiable threshold function by a differentiable function that approximates the threshold in the limit, for example, the sigmoidal function| f (x) = 1 1+e kx . <p> Such networks can be trained via a gradient descent procedure by adjusting the weights of the interconnections based on their contribution to the total network error. Rumelhart et al. <ref> [17] </ref> give a simple and efficient technique known as backpropagation for computing the partial derivatives of total error in the network with respect to the interconnection weights. (Note: BP was independently discovered earlier by Werbos [20].) BP performs a gradient-descent in parameter space, minimizing an appropriate error function. <p> With a continuous model assumption, the weight-update rule that minimizes E can be easily derived (refer Rumelhart et al. <ref> [17] </ref> for details of the derivation) and can be shown to 4 Juedes and Balakrishnan be w ij = jffi j o i :(2) In this equation, ffi j refers to the instantaneous gradient given by the expression ffi j = (t j o j ):o 0 j if j is <p> Intuitively, large steps would help find a local minimum faster. However, if the steps are very large, the system could end up oscillating about a local minimum, maybe even never reaching the minimum. This is the step size problem. Rumelhart et al. <ref> [17] </ref> suggested a simple way out of this dilemma. They used a momentum term which factors in a fraction of the previous weight change into the current weight change. <p> hidden units rather than using a sigmoid function and one or more hidden units. 4 GNN and ADOL-C Training networks with arbitrary activation functions by using gradient descent methods presents a unique problem: How does one compute the partial derivatives in such a network? As formulated by Rumelhart et al. <ref> [17] </ref>, BP only applies to semi-linear functions, 1 and not arbitrary composite functions. In place of BP, we propose the use of computational differentiation. In this section we show how ADOL-C [6] can be used to train GNN. <p> This network topology was chosen because it is easily generalizable and because it is straightforward to train via the BP algorithm as given in Rumelhart et al. <ref> [17] </ref>. .......... .......... &lt;--- K hidden layers -----&gt; &lt;-------------N inputs-------------&gt; Fig. 3. A generic test network topology Since we expect both BP and ADOL-C to train these networks equally well, our simulation centered only on the time required to compute derivatives.
Reference: [18] <author> S. Saarinen, R. Bramley, and G. Cybenko, </author> <title> Neural networks, backpropagation, and automatic differentiation, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> pp. 31-42. </pages>
Reference-contexts: Moreover, we build on the connection between backpropagation and the reverse mode of computational differentiation to show that these networks can be trained quickly. (Others have made the connection between computational differentiation and neural networks. For example, Saarinen et al. <ref> [18] </ref> show that backpropagation is essentially the reverse mode of computational differentiation.
Reference: [19] <author> L. Uhr, </author> <title> Digital and analog microcircuit and sub-net structures for connectionist networks, in Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, </title> <editor> V. Honavar and L. Uhr, eds., </editor> <publisher> Academic Press, </publisher> <year> 1994, </year> <title> ch. </title> <booktitle> XV, </booktitle> <pages> pp. 341-370. </pages>
Reference-contexts: Generalized Neural Networks 5 GNNs can be expected to produce compact mappings by drawing on the relative strengths of the different activation functions made available to the units. This capability is in keeping with the thesis of Uhr in <ref> [19] </ref>, where a case is made for the development of subnetworks and specialized circuits, in order to build efficient ANNs.
Reference: [20] <author> P. Werbos, </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, </title> <type> PhD thesis, </type> <institution> Committee on Appl. Math., Harvard Univ., </institution> <address> Cambridge, Mass., </address> <month> November </month> <year> 1974. </year>
Reference-contexts: Rumelhart et al. [17] give a simple and efficient technique known as backpropagation for computing the partial derivatives of total error in the network with respect to the interconnection weights. (Note: BP was independently discovered earlier by Werbos <ref> [20] </ref>.) BP performs a gradient-descent in parameter space, minimizing an appropriate error function. Typically, the parameters are the weights and thresholds in the network.
Reference: [21] <author> T. Yoshida, </author> <title> Rapid learning method for multilayered neural networks using two dimensional conjugate gradient search, </title> <journal> Journal of Information Processing, </journal> <volume> 15 (1992), </volume> <pages> pp. 79-86. </pages>
Reference-contexts: For example, Saarinen et al. [18] show that backpropagation is essentially the reverse mode of computational differentiation. In a related context, Yoshida <ref> [21] </ref> uses computational differentiation and two-dimensional conjugate gradient search to train neural networks.) Finally, we address the issue of network architecture design by demonstrating how evolutionary algorithms [5, 8] and computational differentiation can be used to build near-minimal networks. 2 ANN and BP Conventional ANNs are highly interconnected networks of relatively
References-found: 19

