URL: file://ftp.irisa.fr/local/lande/dlm-scp95.ps.Z
Refering-URL: http://www.irisa.fr/lande/LeMetayer.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Lazy types and Program Analysis  
Author: Chris Hankin Daniel Le Metayer INRIA/IRISA, 
Date: September 5, 1994  
Address: LONDON SW7 2BZ, UK  35042 RENNES CEDEX, FRANCE  
Affiliation: Department of Computing, Imperial College,  Campus de Beaulieu,  
Abstract: Approaches to static analysis based on non-standard type systems have received considerable interest recently. Most work has concentrated on the relationship between such analyses and abstract interpretation. In this paper, we focus on the problem of producing efficient algorithms from such type-based analyses. The key idea is the notion of lazy types. We present the basic notions in the context of a higher-order strictness analysis of list-processing functions. We also sketch some recent work concerning a general framework for program analysis based on these ideas. We conclude with some experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. van Bakel, </author> <title> Complete restrictions of the intersection type discipline, </title> <journal> Theoretical Computer Science, </journal> <volume> 102(1) </volume> <pages> 135-163, </pages> <year> 1992. </year>
Reference-contexts: ` T tl (e) : t Taut-cons ` T cons (e 1 ; e 2 ) : t 4 As a first step to solve these problems, we introduce a slightly restricted language of strictness formulae T I (Fig. 3); this language is closely related to van Bakel's strict types <ref> [1] </ref>. Basically strict types do not allow intersections on the right hand side of an arrow. <p> First we show that the weakening rule can be removed from ` T without changing the set of derivable types provided we add a form of weakening in the Var and Fix rules. A similar property has been proved for other type systems including a form of weakening <ref> [1, 29] </ref>. This property addresses the first problem identified above; now weakenings are applied at specific (rather than arbitrary) points in the proof. <p> We take as an illustration the rules for conjunction, constants, and application: 6 f G G 1 G f 2 G t 8j 2 <ref> [1; m] </ref>; 9i 2 [1; n] i G j 8:( ` G e : ) ) G ` G e : ( 6= ( 0 ; e 0 )) ! G 0 ! 0 Conj ` G e : 1 ` G e : 2 Var [x 7! 1 ] ` <p> We take as an illustration the rules for conjunction, constants, and application: 6 f G G 1 G f 2 G t 8j 2 [1; m]; 9i 2 <ref> [1; n] </ref> i G j 8:( ` G e : ) ) G ` G e : ( 6= ( 0 ; e 0 )) ! G 0 ! 0 Conj ` G e : 1 ` G e : 2 Var [x 7! 1 ] ` G x : 2
Reference: [2] <author> P. N. Benton, </author> <title> Strictness logic and polymorphic invariance, </title> <booktitle> in Proceedings of the 2nd Int. Symposium on Logical Foundations of Computer Science, </booktitle> <publisher> LNCS 620, Springer Verlag, </publisher> <year> 1992. </year>
Reference: [3] <author> P. N. Benton, </author> <title> Strictness Properties of Lazy Algebraic Datatypes, </title> <booktitle> in Proceedings WSA'93, </booktitle> <publisher> LNCS 724, Springer Verlag, </publisher> <year> 1993. </year> <month> 23 </month>
Reference-contexts: As an aside this might also provide some insight for a simpler correctness proof of abstract graph reduction. Wadler's domain construction does not readily generalise to other recursive data types. Recently Benton <ref> [3] </ref> has shown how to construct an abstract domain from any algebraic data type. It should be straightforward to extend our system (and algorithm) to incorporate such domains.
Reference: [4] <author> G. L. Burn, </author> <title> Evaluation Transformers a model for the parallel evaluation of functional languages (extended abstract), </title> <booktitle> in Proceedings of the 1987 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> LNCS 274, Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: There have been a number of proposals to extend strictness analysis to recursively defined data structures <ref> [4, 28, 35, 36] </ref>. The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem [14, 21].
Reference: [5] <author> G. L. Burn, </author> <title> A Logical Framework for Program Analysis, </title> <booktitle> in Proceedings of the 1992 Glas-gow Functional Programming Workshop, </booktitle> <publisher> Springer Verlag Workshops in Computer Science, </publisher> <year> 1992. </year>
Reference-contexts: In each case there is usually a "natural" interpretation for the operators u, ) and norm which, together with interpretations for constants, gives a type structure (see below). If we use one of these standard structures, Burn, <ref> [5] </ref>, has shown that if the above implication holds for the type constants then it also holds for the derived types; this gives a "local" test to determine if a structure is a model. We choose here to illustrate type structures with the CPER (Complete Partial Equivalence Relations) model. <p> In our earlier work, [16], we extend a result of Burn's, <ref> [5] </ref>, and show that soundness of the constant rules ensures soundness of the logic. This gives a local correctness condition. In [16], there is an analogous result concerning soundness of the abstract machine.
Reference: [6] <author> G. Burn and D. Le Metayer, </author> <title> Proving the correctness of compiler optimisations based on strictness analysis, </title> <booktitle> in Proceedings 5th int. Symp. on Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 714, Springer Verlag, </publisher> <year> 1993. </year>
Reference: [7] <author> T.-R. Chuang and B. Goldberg, </author> <title> A syntactic approach to fixed point computation on finite domains, </title> <booktitle> in Proceedings of the 1992 ACM Conference on Lisp and Functional Programming, </booktitle> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy.
Reference: [8] <author> P. Cousot and R. Cousot, </author> <title> Static determination of dynamic properties of recursive procedures, </title> <editor> in E. J. Neuhold (ed.), </editor> <booktitle> Formal Description of Programming Concepts, </booktitle> <publisher> North-Holland, </publisher> <year> 1978. </year>
Reference-contexts: This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques [9, 14], chaotic iteration sequences <ref> [8, 34] </ref> (and the related minimal function graphs [25]), and frontiers-based algorithms [33, 21]. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. <p> Another technique for improving the computation of fixed points is called chaotic iteration. It was introduced in <ref> [8] </ref> and extended to higher-order functional programs in [34]. The chaotic iteration starts with an initial set of arguments and each step computes a new version of the abstract function for some needed arguments. Several choices can be made for the selection of these arguments.
Reference: [9] <author> P. Cousot and R. Cousot, </author> <title> Comparing the Galois connection and widening/narrowing approaches to abstract interpretation, </title> <editor> in M. Bruynooghe and M. Wirsing (eds), PLILP'92, </editor> <publisher> LNCS 631, Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: However the development of algorithms has not kept pace with the theoretical developments. This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques <ref> [9, 14] </ref>, chaotic iteration sequences [8, 34] (and the related minimal function graphs [25]), and frontiers-based algorithms [33, 21]. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. <p> In other words, we do not have to choose a particular domain before the analysis as is usually done for abstract interpretation (except when widening operators are used as in <ref> [9] </ref>). 2 A strictness logic for the analysis of lists We consider a strongly typed language, fl L , with terms defined by the following syntax: e = x j c j x:e j e 1 e 2 j fix (g:e) j cond (e 1 ; e 2 ; e 3
Reference: [10] <author> O. Danvy and J. Hatcliff, </author> <title> CPS transformation after strictness analysis, </title> <type> Technical Report, </type> <institution> Kansas State University, </institution> <note> to appear in ACM LOPLAS. </note>
Reference: [11] <author> M. van Eekelen, E. Goubault, C. Hankin and E. Nocker, </author> <title> Abstract reduction: a theory via abstract interpretation, </title> <editor> in R. Sleep et al (eds), </editor> <title> Term graph rewriting: theory and practice, </title> <publisher> John Wiley & Sons Ltd, </publisher> <year> 1992. </year>
Reference-contexts: Because of this parameterisable termination condition, it is difficult to formally qualify the power of abstract graph reduction. Another advantage of the lazy types approach is the fact that its correctness proof is much easier to establish (see <ref> [11] </ref> for an introduction to the complications involved by a formalisation of abstract graph reduction). Another technique for improving the computation of fixed points is called chaotic iteration. It was introduced in [8] and extended to higher-order functional programs in [34].
Reference: [12] <author> A. Ferguson and R. J. M. Hughes, </author> <title> Fast abstract interpretation using sequential algorithms, </title> <booktitle> in Proceedings WSA'93, </booktitle> <publisher> LNCS 724, Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Even so our algorithm performs much better (half a second for cat) than the frontiers based implementation; this is because it may not be necessary to compute total information about constituent higher-order functions. Ferguson and Hughes, <ref> [12] </ref>, report that the analysis of cat requires 5 seconds and the analysis of Ccat around 10 seconds. <p> We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. <p> The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. Some of these proposals trade a cheaper implementation against a loss of accuracy [26, 28]. In contrast, <ref> [12, 32] </ref> use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of [12] uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees. <p> Some of these proposals trade a cheaper implementation against a loss of accuracy [26, 28]. In contrast, [12, 32] use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of <ref> [12] </ref> uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees. In [32] the analysis is expressed as a form of reduction of abstract graphs. As in our work, the abstract domain is infinite and computation is done lazily.
Reference: [13] <author> S. Finne and G. Burn, </author> <title> Assessing the evaluation transformer model of reduction on the spineless G-machine, </title> <booktitle> in Proceedings of the 6th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> ACM Press, </publisher> <year> 1993, </year> <pages> pp. 331-341. </pages>
Reference: [14] <author> C. L. Hankin and L. S. Hunt, </author> <title> Approximate fixed points in abstract interpretation, </title> <editor> in B. Krieg-Bruckner (ed), </editor> <booktitle> Proceedings of the 4th European Symposium on Programming, </booktitle> <publisher> LNCS 582, Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: However the development of algorithms has not kept pace with the theoretical developments. This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques <ref> [9, 14] </ref>, chaotic iteration sequences [8, 34] (and the related minimal function graphs [25]), and frontiers-based algorithms [33, 21]. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. <p> The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem <ref> [14, 21] </ref>. The main feature of our approach is the notion of lazy types (or lazily evaluated types) which allows us to compute only the information required to answer a particular question about the strictness of a function.
Reference: [15] <author> C. L. Hankin and D. Le Metayer, </author> <title> Deriving algorithms from type inference systems: Application to strictness analysis, </title> <booktitle> in Proceedings of POPL'94, </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: The type inference system is shown in Fig. 2. is an environment mapping variables to formulae (i.e. strictness types). In the rule Cond-1, represents the standard type of e 2 (or e 3 ). This system is an extension of <ref> [15, 23] </ref> and the soundness and completeness proofs of the logic (with respect to traditional abstract interpretation) follow straightforwardly from [24]. <p> Definition 3.1 (Most General Types) M GT (; e) = f i 2 T S j ` T e : i g We show in <ref> [15] </ref> that the most general type of an expression is precisely the information returned by the standard abstract interpretation-based analysis. This suggests that abstract interpretation is sometimes inefficient just because it computes much more information than really required.
Reference: [16] <author> C. L. Hankin and D. Le Metayer, </author> <title> A type-based framework for program analysis, </title> <booktitle> Proceedings of the Static Analysis Symposium, </booktitle> <publisher> LNCS, Springer Verlag, </publisher> <year> 1994 </year> <month> (to appear). </month>
Reference-contexts: In <ref> [16] </ref> we propose a methodology for defining analyses based on these ideas. We just provide the main intuition here and we show how the framework can be specialised to PER models and binding time analysis. <p> In our earlier work, <ref> [16] </ref>, we extend a result of Burn's, [5], and show that soundness of the constant rules ensures soundness of the logic. This gives a local correctness condition. In [16], there is an analogous result concerning soundness of the abstract machine. <p> In our earlier work, <ref> [16] </ref>, we extend a result of Burn's, [5], and show that soundness of the constant rules ensures soundness of the logic. This gives a local correctness condition. In [16], there is an analogous result concerning soundness of the abstract machine. <p> We first summarise the list of tasks identified in <ref> [16] </ref> in order to set up a correct instance of the generic analysis: 1. Define the list of constants of the language. 2. Define the list of type constants. 3. Provide a type structure and an interpretation for type constants. Show that the structure yields a model. 4.
Reference: [17] <author> J. J. Hannan, </author> <title> Investigating a proof-theoretic meta-language, </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <type> DIKU Technical Report Nr 91/1, </type> <year> 1991. </year>
Reference: [18] <author> J. Hannan and D. Miller, </author> <title> From Operational Semantics to Abstract Machines, </title> <booktitle> Mathematical Structures in Computer Science, </booktitle> <volume> 2(4), </volume> <year> 1992. </year>
Reference-contexts: Rather than introducing a new algorithm and proving its correctness in a second stage, we derive the algorithm from the logic by a succession of refinements in the style of <ref> [18] </ref>. <p> behaviour of And and an axiom for the terminal case: M 4 C (S 1 and S 2 ) : S R M 4 nil R : nil R The end result is that we now have an inference system which is an Abstract Evaluation System in the terminology of <ref> [18] </ref>. This means that we can alternatively present it as a rewriting system describing a machine.
Reference: [19] <author> P. H. Hartel and K. G. Langendoen, </author> <title> Benchmarking implementations of lazy functional languages, </title> <booktitle> in Proceedings of the 6th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> ACM Press, </publisher> <year> 1993, </year> <pages> pp. 341-350. 24 </pages>
Reference: [20] <author> L. S. Hunt, </author> <title> Abstract Interpretation of Functional Languages: From Theory to Practice, </title> <type> PhD thesis, </type> <institution> Imperial College, </institution> <year> 1991. </year>
Reference-contexts: We report here on experimental results. We use as a testbed two versions of a function concatenating lists of lists, the second one being defined in terms of continuations. These examples were provided by S. Hunt to illustrate the limitations of the frontiers optimisation <ref> [20, 21] </ref>. foldr g nil b = b foldr g cons (x; xs) b = g x (foldr g xs b) append nil l = l append cons (x; xs) l = cons (x; (append xs l)) cat l = foldr append l nil Cfoldr g nil b C = C
Reference: [21] <author> L. S. Hunt and C. L. Hankin, </author> <title> Fixed Points and Frontiers: A New Perspective, </title> <journal> Journal of Functional Programming, </journal> <volume> 1(1), </volume> <year> 1991. </year>
Reference-contexts: This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques [9, 14], chaotic iteration sequences [8, 34] (and the related minimal function graphs [25]), and frontiers-based algorithms <ref> [33, 21] </ref>. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. In contrast to abstract interpretation, type inference systems are routinely implemented as part of production quality compilers. <p> The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem <ref> [14, 21] </ref>. The main feature of our approach is the notion of lazy types (or lazily evaluated types) which allows us to compute only the information required to answer a particular question about the strictness of a function. <p> (g)i &gt; fl hT rue; nil; nili 5.2 Higher-order and lists We consider the following functions: f oldr b g nil = b f oldr b g cons (x; xs) = g x (f oldr b g xs) cat l = f oldr nil append l which were introduced in <ref> [21] </ref> to demonstrate the inefficiency of traditional abstract interpretation. <p> We report here on experimental results. We use as a testbed two versions of a function concatenating lists of lists, the second one being defined in terms of continuations. These examples were provided by S. Hunt to illustrate the limitations of the frontiers optimisation <ref> [20, 21] </ref>. foldr g nil b = b foldr g cons (x; xs) b = g x (foldr g xs b) append nil l = l append cons (x; xs) l = cons (x; (append xs l)) cat l = foldr append l nil Cfoldr g nil b C = C <p> These results should be compared with other implementations. The contrast with frontiers based "optimisations" of abstract interpretation is striking: the analysis of <ref> [21] </ref> takes 30 minutes to process cat and does not terminate for examples involving Ccat. The basic reason is that abstract interpretation based analyses systematically compute all the properties satisfied by a function; when the function is higher-order this can involve a vast amount of information.
Reference: [22] <author> L. S. Hunt and D. Sands, </author> <title> Binding Time Analysis: A new PERspective, </title> <booktitle> in Proceedings of the ACM Symposium on Partial Evaluation and Semantics-based Program Manipulation, </booktitle> <year> 1991. </year>
Reference-contexts: The motivation for using CPERs is that certain properties which cannot be represented by Scott-closed sets can be represented by CPERs. Hunt and Sands have used CPERs in binding time analysis <ref> [22] </ref>. Let CPER (D) be the set of CPERs on D.
Reference: [23] <author> T. P. Jensen, </author> <title> Strictness Analysis in Logical Form, </title> <editor> in J. Hughes (ed), </editor> <booktitle> Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> LNCS 523, Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: One of the earliest examples is Kuo and Mishra's strictness analysis [26]. A natural question arises concerning the relationship between this approach and abstract interpretation. Kuo and Mishra's system is strictly weaker than the standard approaches based on abstract interpretation but Jensen <ref> [23] </ref> has shown how it can be extended to regain this equivalence. The logic is not immediately suggestive of an algorithm; this is mainly because of the weakening rule which may be applied at arbitrary points in a derivation. <p> The ordering on types is described in Fig. 1. Some occurrences of t are subscripted by a standard type because the set of contant types include in fact a collection of t and f <ref> [23] </ref> (one for each possible "arrow structure" of a standard type). These subscripts are often omitted because thay can be inferred from the context. We define = as the equivalence induced by 2 the ordering on types: = t , t and t . <p> The type inference system is shown in Fig. 2. is an environment mapping variables to formulae (i.e. strictness types). In the rule Cond-1, represents the standard type of e 2 (or e 3 ). This system is an extension of <ref> [15, 23] </ref> and the soundness and completeness proofs of the logic (with respect to traditional abstract interpretation) follow straightforwardly from [24]. <p> We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy.
Reference: [24] <author> T. P. Jensen, </author> <title> Abstract Interpretation in Logical Form, </title> <type> PhD thesis, </type> <institution> University of London, </institution> <year> 1992. </year> <note> Also available as DIKU Technical Report 93/11. </note>
Reference-contexts: In the rule Cond-1, represents the standard type of e 2 (or e 3 ). This system is an extension of [15, 23] and the soundness and completeness proofs of the logic (with respect to traditional abstract interpretation) follow straightforwardly from <ref> [24] </ref>. <p> It should be straightforward to extend our system (and algorithm) to incorporate such domains. Benton's construction leads to quite large domains; the size of the domains would make conventional abstract interpretation intractable and highlights the benefit of our approach which lazily explores the domain. In his thesis Jensen, <ref> [24] </ref>, has developed a more general logical treatment of recursive types. His approach involves two extensions to the logic; the first is to add disjunctions and the second extension involves adding modal operators for describing uniform properties of elements of recursive types.
Reference: [25] <author> N. D. Jones and A. Mycroft, </author> <title> Data-flow analysis of applicative programs using minimal function graphs, </title> <booktitle> in Proceedings of the ACM Conference on Principles of Programming Languages, </booktitle> <year> 1986. </year>
Reference-contexts: This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques [9, 14], chaotic iteration sequences [8, 34] (and the related minimal function graphs <ref> [25] </ref>), and frontiers-based algorithms [33, 21]. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. In contrast to abstract interpretation, type inference systems are routinely implemented as part of production quality compilers.
Reference: [26] <author> T.-M. Kuo and P. Mishra, </author> <title> Strictness analysis: a new perspective based on type inference, </title> <booktitle> in Proceedings of the 4th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: In contrast to abstract interpretation, type inference systems are routinely implemented as part of production quality compilers. This has led some researchers to develop program analyses based on non-standard type inference. One of the earliest examples is Kuo and Mishra's strictness analysis <ref> [26] </ref>. A natural question arises concerning the relationship between this approach and abstract interpretation. Kuo and Mishra's system is strictly weaker than the standard approaches based on abstract interpretation but Jensen [23] has shown how it can be extended to regain this equivalence. <p> The first one illustrates the iterative process involved in the treatment of recursion and the second one involves higher-order functions and lists. 5.1 Recursion The following function was used in <ref> [26] </ref> to demonstrate the limitations of a type system without conjunction. fix (g:(x:y:z:cond (eq z 0)(+ x y)(g y x ( z 1)))) We show how the lazy type algorithm is able to derive that this function is strict in its first argument, so has type T 1 = f ! <p> We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. <p> The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. Some of these proposals trade a cheaper implementation against a loss of accuracy <ref> [26, 28] </ref>. In contrast, [12, 32] use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of [12] uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees.
Reference: [27] <author> J. Launchbury, </author> <title> Strictness and binding time: two for the price of one, </title> <booktitle> in Proceedings of the ACM Conference on Programming Languages Design and Implementation, </booktitle> <year> 1991. </year>
Reference: [28] <author> A. Leung and P. Mishra, </author> <title> Reasoning about simple and exhaustive demand in higher-order lazy languages, </title> <booktitle> in Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> LNCS 523, Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: There have been a number of proposals to extend strictness analysis to recursively defined data structures <ref> [4, 28, 35, 36] </ref>. The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem [14, 21]. <p> We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. <p> The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. Some of these proposals trade a cheaper implementation against a loss of accuracy <ref> [26, 28] </ref>. In contrast, [12, 32] use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of [12] uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees.
Reference: [29] <author> J. C. Mitchell, </author> <title> Type inference with simple subtypes, </title> <journal> Journal of Functional Programming, </journal> <volume> 1(3), </volume> <year> 1991. </year>
Reference-contexts: First we show that the weakening rule can be removed from ` T without changing the set of derivable types provided we add a form of weakening in the Var and Fix rules. A similar property has been proved for other type systems including a form of weakening <ref> [1, 29] </ref>. This property addresses the first problem identified above; now weakenings are applied at specific (rather than arbitrary) points in the proof.
Reference: [30] <author> L. Mauborgne, </author> <title> Abstract interpretation using TDGs, </title> <booktitle> Proceedings of the Static Analysis Symposium, </booktitle> <publisher> LNCS, Springer Verlag, </publisher> <year> 1994 </year> <month> (to appear). </month>
Reference-contexts: Furthermore their algorithm requires a huge amount of memory to execute. The reason seems to be that their analyser is based on a coding of abstract functions in terms of concrete data structures which take a lot of space. The analyser described in <ref> [30] </ref> is an efficient implementation of abstract interpretation based on a representation of boolean functions as Typed Decision Graphs. It includes an implementation of the widening technique to accelerate fixed point iteration. The analysis of cat takes 4 seconds and the analysis of Ccat one hour.
Reference: [31] <author> A. Mycroft, </author> <title> Abstract Interpretation and Optimising Transformations for Applicative Programs, </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <month> December </month> <year> 1981. </year>
Reference-contexts: Abstract interpretation represents the strictness properties of a function by an abstract function defined on boolean domains <ref> [31] </ref>. For instance g abs t f = f means that g is undefined if its second argument is undefined. In terms of types, this property is represented by g : t ! f ! f . Notice that t and f are now (non-standard) types.
Reference: [32] <author> E. Nocker, </author> <title> Strictness analysis using abstract reduction, </title> <booktitle> in Proceedings of the 6th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: We have not yet identified worst case examples for our approach. 9 Conclusions The problem of designing efficient algorithms for strictness analysis has received much attention recently and one current trend seems to revert from the usual "extensional" approach to more "intensional" or syntactic techniques <ref> [26, 28, 23, 7, 12, 32] </ref>. The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. <p> The key observation underlying these works is that the choice of representing abstract functions by functions can be disastrous in terms of efficiency and is not always justified in terms of accuracy. Some of these proposals trade a cheaper implementation against a loss of accuracy [26, 28]. In contrast, <ref> [12, 32] </ref> use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of [12] uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees. <p> In contrast, [12, 32] use intensional representations of functions to build very efficient algorithms without sacrificing accuracy. The analysis of [12] uses concrete data structures; these are special kinds of Scott domains whose elements can be seen as syntax trees. In <ref> [32] </ref> the analysis is expressed as a form of reduction of abstract graphs. As in our work, the abstract domain is infinite and computation is done lazily. There are important differences however. Their derivation strategy is even more lazy than ours in the following sense. <p> These extra measures can take the form of arbitrary cuts in the derivation (using empirical resource consumption criteria) incurring a loss of accuracy. A neededness analysis called reduction path analysis is also proposed in <ref> [32] </ref> to allow termination of the computation without throwing away too much information. Because of this parameterisable termination condition, it is difficult to formally qualify the power of abstract graph reduction.
Reference: [33] <author> S. L. Peyton Jones and C. Clack, </author> <title> Finding Fixed Points in Abstract Interpretation, </title> <editor> in S. Abramsky and C. L. Hankin (eds), </editor> <title> Abstract Interpretation of Declarative Languages, </title> <publisher> Ellis Horwood, </publisher> <year> 1987. </year>
Reference-contexts: This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques [9, 14], chaotic iteration sequences [8, 34] (and the related minimal function graphs [25]), and frontiers-based algorithms <ref> [33, 21] </ref>. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. In contrast to abstract interpretation, type inference systems are routinely implemented as part of production quality compilers.
Reference: [34] <author> M. Rosendhal, </author> <title> Higher-order chaotic iteration sequences, </title> <booktitle> in Proceedings of the 5th Int. Symp. Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 714, Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: This is now a major barrier that is preventing the inclusion of the most advanced techniques in compilers. The most significant contributions for improving the efficiency of abstract interpretation include widening techniques [9, 14], chaotic iteration sequences <ref> [8, 34] </ref> (and the related minimal function graphs [25]), and frontiers-based algorithms [33, 21]. The latter has unacceptable performance for some commonly occurring higher-order programs. The first two are general approaches for accelerating convergence in fixed point computations. <p> Another technique for improving the computation of fixed points is called chaotic iteration. It was introduced in [8] and extended to higher-order functional programs in <ref> [34] </ref>. The chaotic iteration starts with an initial set of arguments and each step computes a new version of the abstract function for some needed arguments. Several choices can be made for the selection of these arguments. <p> The main departure of our algorithm is the lazy evaluation of types (as opposed to the eager evaluation of needed arguments in <ref> [34] </ref>). As an example the two algorithms exhibit different behaviours when applied to the following function: fix (f:(x:y:z:cond (eq y 0)(+ y z)(f x z (f x z y)))) Assume that we want to decide whether this expression has type f ! t ! t ! f . <p> Rephrased in terms of types, the chaotic iteration sequence described in <ref> [34] </ref> includes f ! t ! f ! f in the set of "needed" types. This type is not really required, it is called a spurious element in [34]. <p> Rephrased in terms of types, the chaotic iteration sequence described in <ref> [34] </ref> includes f ! t ! f ! f in the set of "needed" types. This type is not really required, it is called a spurious element in [34]. This element occurs because the chaotic iteration starts with the least abstract function in the domain (characterised by the type t ! t ! t ! f ). In contrast the lazy types algorithm returns False after the first iteration step.
Reference: [35] <author> P. Wadler, </author> <title> Strictness Analysis on Non-flat Domains, </title> <editor> in S. Abramsky and C. L. Hankin (eds), </editor> <title> Abstract Interpretation of Declarative Languages, </title> <publisher> Ellis Horwood, </publisher> <year> 1987. </year> <month> 25 </month>
Reference-contexts: There have been a number of proposals to extend strictness analysis to recursively defined data structures <ref> [4, 28, 35, 36] </ref>. The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem [14, 21]. <p> For example, the sum function from the previous section is translated as: sum (l) = fix (s:l:case (0; f; l)) where f x xs = x + (sum xs) The loss af accuracy that occurs without the case operator is discussed in <ref> [35] </ref>. Abstract interpretation represents the strictness properties of a function by an abstract function defined on boolean domains [31]. For instance g abs t f = f means that g is undefined if its second argument is undefined. <p> Let us now turn to the types used for the representation of properties of lists. As a first stage, we consider the extension of the boolean domain to Wadler's 4-point domain <ref> [35] </ref>. We show that this extension can be generalised to domains of unbounded depth later. <p> For example, it is not adequate for describing a property such as "this is a list containing lists whose one element is undefined". Following Wadler <ref> [35] </ref>, we can in fact generalise the definition of 4-point domain from the 2-point domain to domains of any depth. Let D 0 = ft; f g with f 0 t.
Reference: [36] <author> P. Wadler and J. Hughes, </author> <title> Projections for Strictness Analysis, </title> <booktitle> in Proceedings of the 1987 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <publisher> LNCS 274, Springer Verlag, </publisher> <year> 1987. </year> <month> 26 </month>
Reference-contexts: There have been a number of proposals to extend strictness analysis to recursively defined data structures <ref> [4, 28, 35, 36] </ref>. The abstract interpretation and the projections approaches have led to the construction of analyses based on rich domains which make them intractable even for some simple examples. Techniques striving for a better representation of the domains do not really solve the problem [14, 21].
References-found: 36

