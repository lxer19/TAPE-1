URL: http://www.isle.org/~langley/papers/bayes.aaai92.ps
Refering-URL: http://www.isle.org/~langley/analysis.html
Root-URL: 
Email: KThompsog@ptolemy.arc.nasa.gov  
Title: An Analysis of Bayesian Classifiers (1988), involves the formulation of average-case models for specific algorithms
Author: Pat Langley Wayne Iba Kevin Thompson fLangley, Iba, 
Keyword: Probabilistic Approaches to Induction  
Address: (M/S 269-2)  Moffett Field, CA 94035 USA  
Affiliation: AI Research Branch  NASA Ames Research Center  
Note: In Proceedings of the Tenth National Conference on Artificial Intelligence (1992). San Jose: AAAI Press.  A third approach, proposed by Cohen and Howe  of this technique, as does Hirschberg and Pazzani's (1991) work on inducing k-CNF concepts. By assuming information about the target concept, the num Also affiliated with RECOM Technologies Also affiliated with Sterling Software.  
Abstract: In this paper we present an average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains, and give experimental results on these domains as a check on our reasoning. One goal of research in machine learning is to discover principles that relate algorithms and domain characteristics to behavior. To this end, many researchers have carried out systematic experimentation with natural and artificial domains in search of empirical regularities (e.g., Kibler & Langley, 1988). Others have focused on theoretical analyses, often within the paradigm of probably approximately correct learning (e.g., Haus-sler, 1990). However, most experimental studies are based only on informal analyses of the learning task, whereas most formal analyses address the worst case, and thus bear little relation to empirical results. ber of attributes, and the class and attribute frequencies, they obtain predictions about the behavior of induction algorithms and used experiments to check their analyses. 1 However, their research does not focus on algorithms typically used by the experimental and practical sides of machine learning, and it is important that average-case analyses be extended to such methods. Recently, there has been growing interest in probabilistic approaches to inductive learning. For example, Fisher (1987) has described Cobweb, an incremental algorithm for conceptual clustering that draws heavily on Bayesian ideas, and the literature reports a number of systems that build on this work (e.g., Allen & Lang-ley, 1990; Iba & Gennari, 1991; Thompson & Langley, 1991). Cheeseman et al. (1988) have outlined Auto-Class, a nonincremental system that uses Bayesian methods to cluster instances into groups, and other researchers have focused on the induction of Bayesian inference networks (e.g., Cooper & Kerskovits, 1991). These recent Bayesian learning algorithms are complex and not easily amenable to analysis, but they share a common ancestor that is simpler and more tractable. This supervised algorithm, which we refer to simply as a Bayesian classifier, comes originally from work in pattern recognition (Duda & Hart, 1973). The method stores a probabilistic summary for each class; this summary contains the conditional probability of each attribute value given the class, as well as the probability (or base rate) of the class. This data structure approximates the representational power of a perceptron; it describes a single decision boundary through the instance space. When the algorithm encounters a new instance, it updates the probabilities stored with the specified class. Neither the order of training instances nor the occurrence of classification errors have any effect on this process. When given a test instance, the classifier uses an evaluation function (which we describe in detail later) to rank the alter 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J.A., & Langley, P. </author> <year> (1990). </year> <title> Integrating memory and search in planning. Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling, </title> <journal> and Control (pp. </journal> <pages> 301-312). </pages> <address> San Diego: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Angluin, D., & Laird, P. </author> <year> (1988). </year> <title> Learning from noisy examples. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 343-370. </pages>
Reference: <author> Buntine, W., & Caruana, R. </author> <year> (1991). </year> <title> Introduction to IND and recursive partitioning (Technical Report FIA-91-28). </title> <institution> Moffett Field, CA: NASA Ames Research Center, Artificial Intelligence Research Branch. </institution>
Reference-contexts: Moreover, earlier studies (e.g., Clark & Niblett, 1987) present evidence of the practicality of the algorithm. Table 1 presents additional experimental evidence for the utility of Bayesian classifiers. In this study we compare the method to IND's emulation of the C4 algorithm <ref> (Buntine & Caruana, 1991) </ref> and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection (Murphy & Aha, 1992), include the "small" soybean dataset, chess end games involving a king-rook-king-pawn confrontation, cases of lymphography diseases, and two biological datasets.
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 54-64). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 , 261-284. </pages>
Reference: <author> Cohen, P. R., & Howe, A. E. </author> <year> (1988). </year> <title> How evaluation guides AI research. </title> <journal> AI Magazine, </journal> <pages> 9 , 35-43. </pages>
Reference: <author> Cooper, G. F., & Herskovits, E. </author> <year> (1991). </year> <title> A Bayesian method for constructing Bayesian belief networks from databases. </title> <booktitle> Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (pp. </booktitle> <pages> 86-94). </pages> <address> Los Angeles: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 139-172. </pages>
Reference: <author> Haussler, D. </author> <year> (1990). </year> <title> Probably approximately correct learning. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1101-1108). </pages> <address> Boston: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Iba, W., & Gennari, J. H. </author> <year> (1991). </year> <title> Learning to recognize movements. </title> <editor> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Iba, W., & Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <address> Aberdeen: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The probability of the class after one has corrupted values is P 0 (C) = (1z)P (C)+z (1P (C)) = P (C)[12z]+z ; as we have noted elsewhere <ref> (Iba & Langley, 1992) </ref>. For an irrelevant attribute A j , the probability P (A j jC) is unaffected by class noise and remains equal to P (A j ), since the attribute is still independent of the class. However, the situation for relevant attributes is more complicated.
Reference: <author> Hirschberg, D. S., & Pazzani, M. J. </author> <year> (1991). </year> <title> Average-case analysis of a k-CNF learning algorithm (Technical Report 91-50). </title> <institution> Irvine: University of California, Department of Information & Computer Science. </institution>
Reference: <author> Kibler, D., & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 81-92). </pages> <address> Glasgow: </address> <publisher> Pittman. </publisher>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1992). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <institution> Irvine: University of California, Department of Information & Computer Science. </institution>
Reference-contexts: Table 1 presents additional experimental evidence for the utility of Bayesian classifiers. In this study we compare the method to IND's emulation of the C4 algorithm (Buntine & Caruana, 1991) and an algorithm that simply predicts the modal class. The five domains, from the UCI database collection <ref> (Murphy & Aha, 1992) </ref>, include the "small" soybean dataset, chess end games involving a king-rook-king-pawn confrontation, cases of lymphography diseases, and two biological datasets.
Reference: <author> Opper, M., & Haussler. D. </author> <year> (1991). </year> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory (pp. </booktitle> <pages> 75-87). </pages> <address> Santa Cruz: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 A related approach involves deriving the optimal learning algorithm under certain assumptions, and then implementing an approximation of that algorithm <ref> (e.g., Opper & Haussler, 1991) </ref>. Analysis of Bayesian Classifiers 224 Domain Bayes IND/C4 Freq.
Reference: <author> Pazzani, M. J., & Sarrett, W. </author> <year> (1990). </year> <title> Average-case analysis of conjunctive learning algorithms. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 339-347). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this case, equivalent levels lead to somewhat slower learning rates, as one would expect given that attribute noise can corrupt multiple values, whereas class noise affects only one. Finally, we can compare the behavior of the Bayesian classifier to that of Wholist <ref> (Pazzani & Sarrett, 1990) </ref>. One issue of interest is the number of training instances required to achieve some criterion level of accuracy.
Reference: <author> Thompson, K., & Langley, P. </author> <year> (1991). </year> <title> Concept formation in structured domains. </title> <editor> In D. H. Fisher, M. J. Paz-zani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Ma-teo: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 18

