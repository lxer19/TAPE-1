URL: http://www.csc.ncsu.edu/eos/service/ece/project/rtcomm/WWW/QOSinATM2.ps.Z
Refering-URL: http://www.csc.ncsu.edu/eos/service/ece/project/rtcomm/WWW/qos.html
Root-URL: http://www.csc.ncsu.edu
Title: An Approach towards End-to-end QoS with Statistical Multiplexing in ATM Networks  
Abstract: Sanjeev Rampal, Douglas S. Reeves, Ioannis Viniotis and Dharma P. Agrawal email: fsdrampal, reeves, candice, dpa@eos.ncsu.edug Technical Report TR 95/2 Center for Communications and Signal Processing North Carolina State University, Raleigh. Abstract We address the problem of providing quality-of-service (QoS) guarantees in a multiple hop packet/cell switched environment while providing high link utilization in the presence of bursty traffic. A scheme based on bandwidth and buffer reservations at the Virtual Path level is proposed for ATM networks. This approach enables us to provide accurate end-to-end QoS guarantees while achieving high utilization by employing statistical multiplexing and traffic shaping of bursty traffic sources. A simple round robin scheduler is proposed for realizing this approach and is shown to be implementable using standard ATM hardware viz. cell spacers. The problem of distributing the bandwidth and buffer space assigned to a VP over its multiple hops is addressed. We prove the optimality of the approach of allowing all the end-to-end loss to occur at the first hop under some conditions and show that its performance can be bounded with respect to the optimal in other conditions. This results in an equal amount of bandwidth to a VP at each hop and essentially no queueing after the first hop. Using simulations, the average case performance of this approach is also found to be good. Additional simulation results are presented to evaluate the proposed approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> C.M. Aras, J.F. Kurose, D.S. Reeves and H.G. Schulzrinne, </editor> <booktitle> "Real-time Communication in Packet-Switched Networks," Proc. of the IEEE, Special Issue on Real-Time Systems, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The problem of being able to predict the QoS that a connection (also referred to as a "call" or "virtual circuit" in this paper), will receive when admitted has proved to be a very difficult one <ref> [1] </ref>. Statistical modeling techniques for a single ATM node have been analyzed in a number of papers [13], [23]. <p> To ensure high utilization, we would like to have as large a multiplexing buffer as possible so that to ensure the end-to-end delay requirements are met, so we need a short cycle length. The WRR scheduler is characterized as having the delay-bandwidth coupling problem <ref> [1] </ref> since a VP assigned a lower bandwidth experiences higher network delay. * Cell Loss Probability Even when the bandwidth assignment to a VP is fixed (n j =T ), choice of T can affect cell loss since each VP sees the WRR server as an "On-Off " server with vacations.
Reference: [2] <author> J.J. Bae, T. Suda and R. Simha, </author> <title> "Analysis of a Finite Buffer Queue with Heterogeneous Markov Modulated Arrival Processes: A Study of the Effects of Traffic Burstiness on Individual Packet Loss," </title> <booktitle> in Proc. IEEE INFOCOM '92, </booktitle> <year> 1992, </year> <month> pp.219-230. </month>
Reference-contexts: Bandwidth partitioning is also more efficient when a class with more stringent cell loss constraint but lower arrival rate is multiplexed with a traffic class which can tolerate higher cell loss but has higher arrival rate. In <ref> [2] </ref>, Bae et al show that when multiplexing a heterogeneous set of sources, the allowable CLP may have to be set more stringent than the most stringent of the individual allowable cell losses in order to meet the QoS requirements of 5 Q3Q2Q1 Arrivals at Q1 Departures from Q1 Arrivals at
Reference: [3] <author> F. Bonomi, S. Montagna and R. Paglino, </author> <title> "A Further Look at Statistical Multiplexing in ATM Networks," </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 26(1993), </volume> <month> pp.119-138. </month>
Reference-contexts: In their studies on statistical multiplexing in ATM networks, Bonomi et al <ref> [3] </ref> recommend use of sharing when the individual source bandwidths are small compared to the link rate, but suggest the use of partitioning schemes such as time-division-multiplexing when individual source bandwidths are large compared to the link bandwidths.
Reference: [4] <author> I. Chlamtac, A. Farago, and T. Zhang, </author> <title> "How to establish and utilize virtual paths in ATM networks," </title> <booktitle> Proc. IEEE ICC'93, </booktitle> <address> Geneva, </address> <year> 1993, </year> <pages> pp. 1368-1372. </pages>
Reference-contexts: Several issues remain for further investigation. This approach requires very efficient design of the configuration and resource assignments to Virtual Paths, which is known to be a very difficult problem <ref> [4] </ref>. The Call Admission Control algorithm needs to be extended to VCs which traverse multiple VPs and we are investigating approaches for this currently. Finally, the implementation issues of such an approach 27 need to be looked at in full detail. 7 Acknowledgements The authors would like to thank Dr.
Reference: [5] <author> J.H.S. Chan and D.H.K. Tsang, </author> <title> "Bandwidth allocation of multiple QOS classes in ATM environment," </title> <booktitle> Proc IEEE Infocom '94, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: However, many studies on statistical multiplexing of multimedia traffic models have found that full sharing is not always the most efficient way to utilize link bandwidth. In <ref> [5] </ref>, Chan and Tsang examined the bandwidth allocation problem for multimedia traffic and found that bandwidth partitioning results in better overall efficiency when multiplexing traffic classes which vary sufficiently in their allowable cell loss (more than 4 orders of magnitude).
Reference: [6] <author> D.D. Clark, S. Shenker and L. Zhang, </author> <title> "Supporting real-time applications in an integrated services packet network: architecture and mechanism," </title> <booktitle> Proc. SIGCOMM '92, </booktitle> <address> Aug.1992, pp.14-26. </address>
Reference-contexts: We do not include any LANs which may be used to access the ATM network. Analysis of the complete end-to-end path from user to user is beyond the scope of this paper. 2 a network, which is used to determine its transmission priority at a downstream node <ref> [6] </ref>. The use of the EFCI (Explicit Forward Congestion Indication) feature in ATM has also been explored in spreading congestion information over different nodes [22].
Reference: [7] <author> R.L. Cruz, </author> <title> "A Calculus for Network Delay, Part II: Network Analysis," </title> <journal> IEEE Transactions on Information Theory, vol.37, </journal> <volume> no.1, </volume> <month> Jan </month> <year> 1991, </year> <month> pp.132-141. </month>
Reference-contexts: While all these schemes indicate improved network performance under certain simulation conditions, no schemes exist as yet which provide provable and predictable end-to-end QoS, along with high utilization for bursty traffic. Cruz <ref> [7] </ref> has shown that with simple work-conserving FIFO, as loss tolerance approaches 0, the downstream buffer requirement grows exponentially with the number of hops (H ) in the path. Equivalently, if we use smaller buffers, we may have to allocate more than the peak bandwidth to ensure zero loss. <p> TDM requires buffer space of the order of the number of sources at each hop). In contrast the buffer size required to ensure no cell loss increases exponentially with the number of hops in a path with a full sharing FIFO scheme <ref> [7] </ref>. * Bandwidth partitioning introduces a degree of fault tolerance (since a misbehaving source will directly affect only other sources within its own partition), and can be used to also enforce fairness for call level acceptance probabilities [29]. * Non-real-time traffic such as file transfers can always use unused slots from
Reference: [8] <author> A. DeSimone, </author> <title> "Generating burstiness in networks: A simulation study of correlation effects in networks of queues," </title> <journal> ACM Computer Communication Review, </journal> <volume> vol.21, </volume> <month> Jan. </month> <year> 1991, </year> <month> pp.24-31. </month>
Reference-contexts: In <ref> [8] </ref>, DeSimone shows that traffic smoothing at the network edge alone is not sufficient in protecting a network from congestion. In this paper, we propose the use of deterministic bandwidth reservations at the Virtual Path level in order to overcome some of these problems.
Reference: [9] <author> G. de Veciana, C. Courcoubetis, and J. Walrand, </author> <title> "Decoupling bandwidths for networks: A decomposition approach to resource management," </title> <booktitle> in Proc IEEE Infocom '94, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1994, </year> <month> pp.466-473. </month>
Reference-contexts: Statistical modeling techniques for a single ATM node have been analyzed in a number of papers [13], [23]. These include approximate formulas based on large buffer asymptotics ([10], <ref> [9] </ref>) and those based on large 1 In general several other parameters, such as second moments of delay and loss may also be specified as QoS metrics. <p> In [34] it is shown that the smoothing of a source is maximal when it is serviced in a uniform deterministic manner. Equivalently, in <ref> [9] </ref> De Veciana et al show that the bandwidth requirement at downstream nodes is minimized by serving a connection at precisely its effective rate (which is the minimum deterministic service rate required to meet a connection's QoS requirements). Similar results are presented in [20]. <p> In section 5, we present some simulations which also illustrate the smoothing of real-life sources. Considerable attention has been given recently to the "effective bandwidth" formulation [13], [10], <ref> [9] </ref>. A key property of this formulation is that the effective bandwidth of an aggregation of sources is simply the linear sum of the effective bandwidths of the individual sources.
Reference: [10] <author> A.I. Elwalid and D. Mitra, </author> <title> "Effective Bandwidth of General Markovian Traffic Sources and Admission Control of High Speed Networks," </title> <journal> IEEE/ACM Transactions on Networking, Vol.1, </journal> <volume> No.3, </volume> <month> June </month> <year> 1993, </year> <month> pp.329-343. </month>
Reference-contexts: In section 5, we present some simulations which also illustrate the smoothing of real-life sources. Considerable attention has been given recently to the "effective bandwidth" formulation [13], <ref> [10] </ref>, [9]. A key property of this formulation is that the effective bandwidth of an aggregation of sources is simply the linear sum of the effective bandwidths of the individual sources. <p> However in practice bandwidth allocation schemes are expected to be linear mainly for simplicity of implementation and due to convenience for other network management functions such as routing and resource management <ref> [10] </ref>, [23]. presence of a linear bandwidth allocation scheme. For simplicity we assume we have 4 sources with the same effective bandwidth. <p> This could be based on bandwidth tables computed o*ine, or on approximate formulas such as equivalent capacity <ref> [10] </ref> or any other scheme. F (i; j) results in the value TRUE if the CLP is acceptable, otherwise it results in the value FALSE. When a call i requests admission into VP V j the following call admission control procedure is executed. <p> Figures 12 and 13 show the per-source effective bandwidth requirement for a multiplexed set of voice sources and a multiplexed set of video sources, respectively. These curves are obtained by using the equivalent bandwidth approximation [13], <ref> [10] </ref>. The delay shown in the curves is the maximum queuing delay that would be encountered in the first hop of the VP onto which the sources are multiplexed. The standard On-Off model with exponentially distributed On and Off durations was used for the voice sources [35]. <p> For 20 voice sources and 200 msecs of delay, 80% utilization can be achieved. Typical end-to-end acceptable delay limits are up to 250-300 msecs. The equivalent capacity formulas are in fact conservative over-estimates and thus in practice even higher utilizations will be achieved <ref> [10] </ref>. Clearly, for both video and voice traffic a single level of multiplexing is able to achieve high utilization.
Reference: [11] <author> D. Ferrari and D.C. Verma, </author> <title> "A Scheme for Real-Time Channel Establishment in Wide-Area Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 8, NO. 4, </volume> <month> April </month> <year> 1990, </year> <month> pp.368-379. </month>
Reference-contexts: VP bandwidth guarantees are enforced using a deterministic scheduler. In this paper we investigate the use of a Weighted Round Robin (WRR) type scheduler (equivalent to a multi-rate time-division multiplexor). The WRR scheduler is very simple to implement and analyze. Many other deterministic schedulers, such as Earliest Due Date <ref> [11] </ref> Stop&Go [12] or Weighted Fair Queuing [26] could also be used. However, these are significantly more complicated than WRR, and with our method WRR is good enough to achieve high utilizations (as we will show). We discuss the use of other schedulers briefly towards the end of this section. <p> The delay-bandwidth coupling may be eliminated by using servers such as the EDD scheduler <ref> [11] </ref>. However, the EDD server suffers in the presence of heterogeneous traffic in addition to having high implementation complexity.
Reference: [12] <author> S.J. Golestaani, </author> <title> "A Framing Strategy for Congestion Management," </title> <journal> IEEE Journal on Selected Areas in Communications, Vol.9, </journal> <volume> No. 7, </volume> <month> Sept. </month> <year> 1991, </year> <note> pp.1064-1077. 28 </note>
Reference-contexts: However, these results normally involve some approximations and have not been extended to the multiple node (end-to-end) 2 case so far. On the other hand, flow control schemes that enable exact end-to-end QoS calculations have been proposed <ref> [12] </ref>, [26], but employ peak bandwidth allocation which results in poor link utilization. This is because standard multimedia traffic models have a peak cell rate (PCR) at least 3 to 4 times the sustainable cell rate (SCR) [29], [31]. multi-hop case. <p> Equivalently, if we use smaller buffers, we may have to allocate more than the peak bandwidth to ensure zero loss. In comparison, the use of a non-work-conserving scheduler can always guarantee zero losses, while requiring only O (H ) buffer requirements and peak bandwidth allocation <ref> [12] </ref>. Hence, The optimal cell service policy to achieve a given loss specification at the network level is a non-work-conserving policy. A hotly debated issue in the ATM community is the use of end-point versus hop-by-hop flow control. <p> In this paper we investigate the use of a Weighted Round Robin (WRR) type scheduler (equivalent to a multi-rate time-division multiplexor). The WRR scheduler is very simple to implement and analyze. Many other deterministic schedulers, such as Earliest Due Date [11] Stop&Go <ref> [12] </ref> or Weighted Fair Queuing [26] could also be used. However, these are significantly more complicated than WRR, and with our method WRR is good enough to achieve high utilizations (as we will show). We discuss the use of other schedulers briefly towards the end of this section. <p> In order to resolve the contradictory requirements on the value of T , a multi-level round robin may be employed. Such a strategy was proposed in <ref> [12] </ref> as the Stop&Go strategy. The same approach can now be employed at the VP level to tradeoff different requirements on the value of T . However the implementation complexity of such a scheme increases dramatically with respect to a one-level scheme in that case. <p> This scheduler does have its problems including the tradeoffs involved in choosing the WRR cycle length and the delay-bandwidth coupling problem. The problem with server cycle length may be reduced by using a multiple level round-robin server such as HRR [16] or Stop & Go <ref> [12] </ref>, however implementation complexity increases accordingly and an engineering decision may be required to determine the right approach. The delay-bandwidth coupling may be eliminated by using servers such as the EDD scheduler [11].
Reference: [13] <author> R. Guerin, H. Ahmadi and M. Naghshineh, </author> <title> "Equivalent Capacity and its Application to Bandwidth Allocation in High-Speed Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, Vol.9, </journal> <volume> No.7, </volume> <month> Sep </month> <year> 1991, </year> <pages> pp. 968-981. </pages>
Reference-contexts: Statistical modeling techniques for a single ATM node have been analyzed in a number of papers <ref> [13] </ref>, [23]. These include approximate formulas based on large buffer asymptotics ([10], [9]) and those based on large 1 In general several other parameters, such as second moments of delay and loss may also be specified as QoS metrics. <p> We limit ourselves to the CTD and CLP in this paper. 1 MUX T 2 T 2 1 2 3 (a) Increase in Peak Rate due to Multiplexing (b) Cell Loss Due to Work-conserving Server Node 1 Node 2 Node 3 C1 C3 numbers of sources [15], <ref> [13] </ref>). However, these results normally involve some approximations and have not been extended to the multiple node (end-to-end) 2 case so far. <p> Hence the loss of utilization of a partitioned scheme may be made quite small compared to a fully shared scheme if the partitioning is carried out at a sufficiently coarse level. To demonstrate this, we use an approximation based on the central limit theorem suggested in <ref> [13] </ref> for determining the bandwidth requirement of an aggregation of sources. The total bandwidth requirements for 200 voice sources based on the standard On-Off model [35] was determined with different partitioning granularities (Figure 5). <p> In section 5, we present some simulations which also illustrate the smoothing of real-life sources. Considerable attention has been given recently to the "effective bandwidth" formulation <ref> [13] </ref>, [10], [9]. A key property of this formulation is that the effective bandwidth of an aggregation of sources is simply the linear sum of the effective bandwidths of the individual sources. <p> However, we show that the bandwidth requirement using the MGF policy is still within an easily computable constant factor of the optimal assignment. We assume that the required service rate at hop 1 with the MGF policy is calculated using an effective bandwidth function <ref> [13] </ref> or o*ine computed bandwidth versus loss tables. For a given source, the effective bandwidth depends on the buffer size and the required loss rate. Let T B 1 (p) denote the effective bandwidth function as a function of the loss rate. <p> Figures 12 and 13 show the per-source effective bandwidth requirement for a multiplexed set of voice sources and a multiplexed set of video sources, respectively. These curves are obtained by using the equivalent bandwidth approximation <ref> [13] </ref>, [10]. The delay shown in the curves is the maximum queuing delay that would be encountered in the first hop of the VP onto which the sources are multiplexed. The standard On-Off model with exponentially distributed On and Off durations was used for the voice sources [35].
Reference: [14] <author> F. Guillemin and W. Monin, </author> <title> "Management of cell delay variation in ATM Networks, </title> " <booktitle> Proc IEEE Globecom '92, </booktitle> <address> Orlando, </address> <month> Dec. </month> <year> 1992, </year> <pages> pp. 128-132. </pages>
Reference-contexts: Switch Fabric Buffer for non real-time data Buffer for real-time data DeMUX Output Link Reemission Pointer Spacing Algorithm 3.4 Implementation of the WRR Scheme Using Cell Spacers It has been suggested that cell spacing at each NNI in an ATM network can be effectively employed to improve network link utilization <ref> [14] </ref>. The WRR scheme suggested above can easily be implemented using a cell spacing algorithm, so that the implementation complexity of the proposed scheme appears to be reasonable. A cell spacer is used to enforce rates and ensures against increase in jitter [14]. <p> be effectively employed to improve network link utilization <ref> [14] </ref>. The WRR scheme suggested above can easily be implemented using a cell spacing algorithm, so that the implementation complexity of the proposed scheme appears to be reasonable. A cell spacer is used to enforce rates and ensures against increase in jitter [14]. Hence a cell spacer can also be used for strict rate enforcement as is required in the proposed scheme. serving this output buffer according to strict non work conserving schedule, while utilizing free slots for data from the non real time buffer, the WRR scheduler can be implemented.
Reference: [15] <author> I. Hsu and J. Walrand, </author> <title> "Admission Control for ATM Networks," </title> <booktitle> Proc. IMA Workshop on Stochastic Networks, </booktitle> <address> Minneapolis, Minnesota, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: We limit ourselves to the CTD and CLP in this paper. 1 MUX T 2 T 2 1 2 3 (a) Increase in Peak Rate due to Multiplexing (b) Cell Loss Due to Work-conserving Server Node 1 Node 2 Node 3 C1 C3 numbers of sources <ref> [15] </ref>, [13]). However, these results normally involve some approximations and have not been extended to the multiple node (end-to-end) 2 case so far.
Reference: [16] <author> C.R. Kalmanek, H. Kanakia and S. Keshav, </author> <title> "Rate controlled servers for very high-speed networks," </title> <booktitle> Proc. IEEE Globecom '90, </booktitle> <address> San Diego, </address> <month> Dec </month> <year> 1990, </year> <month> pp.12-20. </month>
Reference-contexts: This scheduler does have its problems including the tradeoffs involved in choosing the WRR cycle length and the delay-bandwidth coupling problem. The problem with server cycle length may be reduced by using a multiple level round-robin server such as HRR <ref> [16] </ref> or Stop & Go [12], however implementation complexity increases accordingly and an engineering decision may be required to determine the right approach. The delay-bandwidth coupling may be eliminated by using servers such as the EDD scheduler [11].
Reference: [17] <author> J.F. Kurose, </author> <title> "Open issues and Challenges in Providing Quality of Service Guarantees in High-Speed Networks," </title> <journal> ACM Computer Communication Review, </journal> <month> Jan </month> <year> 1993, </year> <pages> pp. 6-15. </pages>
Reference-contexts: This is because standard multimedia traffic models have a peak cell rate (PCR) at least 3 to 4 times the sustainable cell rate (SCR) [29], [31]. multi-hop case. In Figure 1a (from <ref> [17] </ref>), connection 3's minimum inter-cell arrival time changes from 3 slots to 1 slot (thereby changing its PCR), because of the work-conserving multiplexor (Note: a server is work-conserving if it does not idle as long as there is a cell waiting for transmission).
Reference: [18] <author> W.C. Lau and S.Q. Li, </author> <title> "Traffic Analysis in Large-Scale High-Speed Integrated Networks: Validation of Nodal Decomposition Approach," </title> <booktitle> in Proc. IEEE INFOCOM '93,1993, </booktitle> <address> pp.1320-1329. </address>
Reference-contexts: Actually finding the optimal configuration may prove to be quite difficult as the simple examples in section 2 have shown. An approximation commonly made in practice is that traffic characteristics are unaltered as we move from one hop to the next <ref> [18] </ref>. Simulations under certain conditions indicated this assumption is approximately correct in a full sharing environment in which the bandwidths of individual sources are small compared to the link rate.
Reference: [19] <author> C.T. Lea and A. Alyatama, </author> <title> "Bandwidth Quantization in the Broadband ISDN," </title> <booktitle> Proc IEEE Infocom '92, </booktitle> <year> 1992, </year> <month> pp.21-29. </month>
Reference-contexts: When T is small, this unit bandwidth can be quite large, leading to possible loss of utilization due to quantization errors. Hence a large value of T allows better bandwidth allocation granularity. In their study of bandwidth quantization in BISDN <ref> [19] </ref>, Lea and Alyatama suggest that network performance will not get severely affected if the bandwidth assignments are quantized into 10 or more levels. * Cell Delay Variance (CDV) Finally, delay jitter or the difference between the maximum and minimum cell delays increases with the maximum delay i.e. with T so <p> This cycle length results in a bandwidth granularity of about 424 Kb/s which is about 1/365 times the link bandwidth so that allocation granularity appears to be sufficiently fine as per the study on bandwidth quantization in BISDN <ref> [19] </ref>. Finally, as we show in section 5, by using cell spacers at the first and last hops of every VP, we can ensure that the variance in delays over the end-to-end path as well as the cell loss probability becomes independent of the value of T .
Reference: [20] <author> S. Low and P. Varaiya, </author> <title> "Burstiness bounds for some burst reducing servers," </title> <booktitle> Proc. IEEE INFOCOM '93,San Francisco, </booktitle> <month> March </month> <year> 1993, </year> <month> pp.2-9. </month>
Reference-contexts: Equivalently, in [9] De Veciana et al show that the bandwidth requirement at downstream nodes is minimized by serving a connection at precisely its effective rate (which is the minimum deterministic service rate required to meet a connection's QoS requirements). Similar results are presented in <ref> [20] </ref>.
Reference: [21] <author> B. Maglaris, D. Anastassiou, P. Sen, G. Karlsson and J. </author> <title> Robbins,"Performance Models of Statistical Multiplexing in Packet Video Communications," </title> <journal> IEEE Transactions on Communications, vol.36, </journal> <volume> no.7, </volume> <month> July </month> <year> 1988, </year> <month> pp.834-844. </month>
Reference-contexts: The standard On-Off model with exponentially distributed On and Off durations was used for the voice sources [35]. The video model used was a well-known Markov-modulated fluid model with 11 states <ref> [21] </ref>. From this curve the multiplexing potential of both video and voice sources can be clearly seen. For example, for 20 voice sources and an allowable queuing delay of 100 msecs the per-source effective rate is about 16 Kb/s; this represents an average utilization of 70%.
Reference: [22] <author> B.A. Makrucki, </author> <title> "On the performance of submitting excess traffic to ATM networks," </title> <booktitle> Proc. IEEE Globecom '91, Phoenix, </booktitle> <month> Dec </month> <year> 1991, </year> <month> pp.281-288. </month>
Reference-contexts: The use of the EFCI (Explicit Forward Congestion Indication) feature in ATM has also been explored in spreading congestion information over different nodes <ref> [22] </ref>. While all these schemes indicate improved network performance under certain simulation conditions, no schemes exist as yet which provide provable and predictable end-to-end QoS, along with high utilization for bursty traffic.
Reference: [23] <author> N. M. Mitrou and D. E. Pendarakis, </author> <title> "Cell-level statistical multiplexing in ATM networks: Analysis, dimensioning and call acceptance control w.r.t. QOS criteria," in Queueing, Performance and Control in ATM, </title> <booktitle> Proc. Workshop at the 13th International Teletraffic Congress, </booktitle> <editor> J. W. Cohen and C. D. Pack, eds., </editor> <address> Copenhagen, </address> <publisher> North-Holland, </publisher> <month> June </month> <year> 1991, </year> <pages> pp. 7-12. </pages>
Reference-contexts: Statistical modeling techniques for a single ATM node have been analyzed in a number of papers [13], <ref> [23] </ref>. These include approximate formulas based on large buffer asymptotics ([10], [9]) and those based on large 1 In general several other parameters, such as second moments of delay and loss may also be specified as QoS metrics. <p> However in practice bandwidth allocation schemes are expected to be linear mainly for simplicity of implementation and due to convenience for other network management functions such as routing and resource management [10], <ref> [23] </ref>. presence of a linear bandwidth allocation scheme. For simplicity we assume we have 4 sources with the same effective bandwidth.
Reference: [24] <author> R. Nagrajan, J.F. Kurose and D. Towsley, </author> <title> "Local Allocation of End-to-end Quality-of-Service Resources in High-Speed Networks ," proc. IFIP Workshop on Performance Analysis of ATM Systems, </title> <address> Martinique, </address> <month> Jan </month> <year> 1993. </year>
Reference-contexts: We note that assigning different resources at different physical hops of a VP is equivalent to obtaining differing levels of QoS at each hop. Hence, the problem is equivalent to splitting the end-to-end QoS specifications into per-hop QoS specifications <ref> [24] </ref>. A policy which obtains the per-hop QoS specifications given the end-to-end QoS specification will be referred to as a QoS allocation policy.
Reference: [25] <author> R.O. Onvural and Y.C. Liu, </author> <title> "On the amount of bandwidth allocated to Virtual Paths in ATM Networks," </title> <booktitle> Proc IEEE Globecom '92, </booktitle> <year> 1992, </year> <month> pp.1460-1464. </month>
Reference-contexts: In this section, we will only concern ourselves with one end-to-end QoS metric and its per-hop allocations viz. the cell loss probability (CLP) or cell loss rate (CLR). A similar problem was also addressed by Onvural and Liu <ref> [25] </ref>. However, in their model, the link bandwidth was fully shared by all VPs unlike our reservations-based approach. A VP is essentially modeled as a tandem queue of H deterministic servers. There is no cross traffic and external cells enter only the first queue and depart from the last.
Reference: [26] <author> A.K. Parekh and R.G. Gallager, </author> <title> "A Generalized Processor Sharing Approach to Flow Control in Integrated Services Networks," </title> <booktitle> Proc, IEEE INFOCOM '93, </booktitle> <month> Mar. </month> <year> 1993, </year> <month> pp.521-530. </month>
Reference-contexts: However, these results normally involve some approximations and have not been extended to the multiple node (end-to-end) 2 case so far. On the other hand, flow control schemes that enable exact end-to-end QoS calculations have been proposed [12], <ref> [26] </ref>, but employ peak bandwidth allocation which results in poor link utilization. This is because standard multimedia traffic models have a peak cell rate (PCR) at least 3 to 4 times the sustainable cell rate (SCR) [29], [31]. multi-hop case. <p> In this paper we investigate the use of a Weighted Round Robin (WRR) type scheduler (equivalent to a multi-rate time-division multiplexor). The WRR scheduler is very simple to implement and analyze. Many other deterministic schedulers, such as Earliest Due Date [11] Stop&Go [12] or Weighted Fair Queuing <ref> [26] </ref> could also be used. However, these are significantly more complicated than WRR, and with our method WRR is good enough to achieve high utilizations (as we will show). We discuss the use of other schedulers briefly towards the end of this section.
Reference: [27] <author> C. Partridge, </author> <title> Gigabit Networking, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: This is because each source sees a buffer of the same size as before which is now served at a lower rate. Typically, QoS constraints seek to limit the maximum cell delay <ref> [27] </ref>. Hence as long as the maximum delay is met, both schemes can support a required QoS. We can ensure this by performing the partitioning at a sufficiently coarse level that the rates assigned to each separate buffer are sufficiently high and delay stays within the specified upper bound. <p> We will assume that the server cycle time T is larger than this delay. Note that typical switch fabric delays are of the order of a few microseconds at most <ref> [27] </ref>, while the server cycle time will be of the order of a few milliseconds as we shall show later. Using the above notation and assuming that d h s &lt; T , we state the following theorem.
Reference: [28] <author> G. Ramamurthy and R. S. Dighe, </author> <title> "A multidimensional framework for congestion control in B-ISDN," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> Vol. 9, </volume> <month> Dec. </month> <year> 1991, </year> <note> pp.1440-1451. 29 </note>
Reference-contexts: The ATM forum is moving towards rate-based flow control techniques as opposed to credit-based approaches. We note that in general, combinations of preventive and reactive congestion control mechanisms have been found to be effective in providing network level congestion control <ref> [28] </ref>. An approach has also been investigated in which the performance (delay) achieved at an upstream node is added to a packet as it traverses 2 We use the term "end-to-end" to refer to a multiple hop path contained solely within an all-ATM network.
Reference: [29] <author> S. Rampal, D.S. Reeves and D.P. Agrawal, </author> <title> "An Evaluation of Routing and Admission Control Algorithms for Real-Time Traffic in Packet-Switched Networks", </title> <booktitle> Proc. IFIP Conference on High Performance Networking (HPN '94), </booktitle> <address> Grenoble, </address> <month> June </month> <year> 1994, </year> <month> pp.77-92. </month>
Reference-contexts: This is because standard multimedia traffic models have a peak cell rate (PCR) at least 3 to 4 times the sustainable cell rate (SCR) <ref> [29] </ref>, [31]. multi-hop case. <p> the number of hops in a path with a full sharing FIFO scheme [7]. * Bandwidth partitioning introduces a degree of fault tolerance (since a misbehaving source will directly affect only other sources within its own partition), and can be used to also enforce fairness for call level acceptance probabilities <ref> [29] </ref>. * Non-real-time traffic such as file transfers can always use unused slots from real-time traffic and obtain full link utilization.
Reference: [30] <author> S. Rampal, D.S. Reeves and D.P. Agrawal, </author> <title> "End-to-end guaranteed QoS with statistical multiplexing in ATM networks," Modeling and Performance Evaluation of ATM Networks, D.D. </title> <editor> Kouvatsos (ed.), </editor> <publisher> Chapman and Hall, </publisher> <year> 1995. </year>
Reference-contexts: We add that in the proposed architecture, we allow for VCs to traverse more than one VP from origin to destination nodes so that in general a VC does not see a deterministic end-to-end pipe as its VP. This approach was first introduced in <ref> [30] </ref>. In this paper, a detailed description and analysis of this approach is presented. Section 2 introduces the approach of bandwidth reservations at the path level. we demonstrate the difficulty of the multi-hop QoS problem by presenting some important sample path properties.
Reference: [31] <author> S. Rampal, D.S. Reeves, </author> <title> "Routing algorithms for multimedia traffic," </title> <note> submitted for publication, available by anonymous ftp from ftp.csc.ncsu.edu:/pub/rtcomm. </note>
Reference-contexts: This is because standard multimedia traffic models have a peak cell rate (PCR) at least 3 to 4 times the sustainable cell rate (SCR) [29], <ref> [31] </ref>. multi-hop case. In Figure 1a (from [17]), connection 3's minimum inter-cell arrival time changes from 3 slots to 1 slot (thereby changing its PCR), because of the work-conserving multiplexor (Note: a server is work-conserving if it does not idle as long as there is a cell waiting for transmission). <p> Other advantages of such an approach include simplicity of implementation and the ability to integrate call-level GoS controls in terms of call acceptance probabilities and fair network access. Some form of path level bandwidth allocation will anyway be required in order to provide call-level GoS <ref> [31] </ref>. However, we note that in general the number of calls between a given origin destination pair may not be large enough to drastically eliminate the burstiness of the aggregate traffic of a VP. In this case, better statistical multiplexing potential at downstream nodes may need to be exploited. <p> The delay-bandwidth coupling may be eliminated by using servers such as the EDD scheduler [11]. However, the EDD server suffers in the presence of heterogeneous traffic in addition to having high implementation complexity. Rampal et al, ([29], <ref> [31] </ref>) present an evaluation of some of these schedulers. 4 The Multi-hop Bandwidth and Buffer Assignment Problem In the above sections, we have proposed an approach based on reservation of link bandwidths and buffer space for each VP.
Reference: [32] <author> S. Rampal, </author> <title> "Routing and End-to-end Quality-of-service Issues in Multimedia Networks, </title> " <type> Thesis Proposal, </type> <institution> Dept of Electrical and Computer Engg, NC State University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: However in case of a network of nodes with constant service times, total losses can increase if the service rate of a node is increased. Proof: A sample path proof of the first part is in <ref> [32] </ref>. The second statement can be simply illustrated using an example. <p> However in case of a network of nodes with constant service times, total losses can increase if the buffer size at a node is increased. Proof: A sample path proof of the first part can be found in <ref> [32] </ref>. The second part is again illustrated using an example. <p> As another example, the squared coefficient of variation of the inter-departure times from an M=D=1 queue can be found using Laplace transform techniques as C 2 D = 1 ae 2 <ref> [32] </ref>. Modeling ATM traffic as Markovian is inaccurate, however, this example serves to illustrate the phenomenon of traffic smoothing. At a load of 90%, C 2 D = 0:19. Typically a C 2 of less than 0:2 is considered a deterministic source in queueing systems. <p> Proof : The proof follows from the non work-conserving nature of the WRR server <ref> [32] </ref> 2. For simplicity, we currently assume that the server cycle is of the same length at each hop. In general, this need not be so. <p> V j which traverses H physical hops, can be easily calculated as D max = bB 1 j bB 1 H X T h prop (1) where C is the physical link capacity (assumed same for all hops), and T h prop is the propagation delay for the h'th hop <ref> [32] </ref>. The first two terms represent the maximum queueing delay at the first hop of the VP. <p> In any case, estimating effective bandwidth at downstream hops becomes difficult and more approximate because traffic characterization at downstream hops is very difficult. In general though, we may have very large downstream buffers because of which, it may make more sense to experience cell loss downstream. In <ref> [32] </ref>, we present a heuristic which assigns all the available cell loss to a downstream node and peak bandwidths at all hops preceding this hop.
Reference: [33] <author> S. Rampal, </author> <title> "Routing and End-to-end Quality-of-service Issues in Multimedia Networks," </title> <type> Thesis under preparation. </type>
Reference-contexts: C 1 C h ; h = 2; 3; : : : H ). Proof: The proof is in <ref> [33] </ref>. <p> Proof: The proof is in <ref> [33] </ref>. This is an important theorem since it indicates that end-to-end losses are minimized by having all the losses occur at hop 1 given the total buffer space and service rate at the last hop. <p> Further, the bandwidth requirement at any hop with the MGF policy is never more than the bandwidth requirement at the corresponding hop under any other multi hop resource allocation policy. Proof: The proof is in <ref> [33] </ref>. This theorem states that not only is the total end-to-end bandwidth requirement minimized by using the MGF policy, but also the bandwidth at each hop is also minimized. <p> Proof: The proof essentially exploits the monotonic relation between service rate and loss rate (Theorem 1) and is in <ref> [33] </ref>. As an example with R = 2 over a 4 hop path, the total VP bandwidth with the MGF policy is n omore than 1.6 times the optimal.
Reference: [34] <author> N. Shroff and M. Schwartz, </author> <title> "Video Modeling in ATM Networks using Deterministic Smoothing at the Source, </title> " <booktitle> in Proc. IEEE Infocom '94, </booktitle> <address> Toronto, </address> <month> June, </month> <year> 1994, </year> <month> pp.342-349. </month>
Reference-contexts: This is illustrated in Figure 1a where the PCR of connection 3 increased due to bandwidth sharing. Typically this occurs when a connection is multiplexed with a more bursty source. * Deterministic bandwidth partitioning results in minimal burstiness of the output process of a given source. In <ref> [34] </ref> it is shown that the smoothing of a source is maximal when it is serviced in a uniform deterministic manner.
Reference: [35] <author> K. Sriram, </author> <title> "Methodologies for bandwidth allocation, transmission scheduling, and congestion avoidance in broadband ATM networks," Computer Networks and ISDN Systems, </title> <booktitle> vol.26, 1993, pp.43-59. </booktitle> <pages> 30 </pages>
Reference-contexts: To demonstrate this, we use an approximation based on the central limit theorem suggested in [13] for determining the bandwidth requirement of an aggregation of sources. The total bandwidth requirements for 200 voice sources based on the standard On-Off model <ref> [35] </ref> was determined with different partitioning granularities (Figure 5). <p> We discuss the use of other schedulers briefly towards the end of this section. The idea of round-robin type service of different traffic classes for ATM has been suggested by others also (see Sriram's Dynamic Time Slice Scheme in particular <ref> [35] </ref>). 10 Input Ports Output Ports Non Blocking Switch Fabric However, to our knowledge the performance achievable by this approach has not been quantified so far, particularly for the multi-hop case. <p> The delay shown in the curves is the maximum queuing delay that would be encountered in the first hop of the VP onto which the sources are multiplexed. The standard On-Off model with exponentially distributed On and Off durations was used for the voice sources <ref> [35] </ref>. The video model used was a well-known Markov-modulated fluid model with 11 states [21]. From this curve the multiplexing potential of both video and voice sources can be clearly seen.
References-found: 35

