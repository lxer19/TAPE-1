URL: http://www.cs.berkeley.edu/~arvindk/papers/multipol.ps
Refering-URL: http://www.cs.berkeley.edu/~arvindk/
Root-URL: 
Email: fsoumen,deprit,ejim,jjones,arvindk,cpwen,yelickg@cs.berkeley.edu  
Title: Multipol: A Distributed Data Structure Library  
Author: Soumen Chakrabarti, Etienne Deprit, Eun-Jin Im, Jeff Jones, Arvind Krishnamurthy, Chih-Po Wen, and Katherine Yelick 
Affiliation: U.C. Berkeley, Computer Science Division  
Abstract: Applications with dynamic data structures, unpredictable computational costs, and irregular data access patterns require substantial effort to parallelize. Much of their programming complexity comes from the implementation of distributed data structures. We describe a library of such data structures, Multipol, which includes parallel versions of classic data structures such as trees, sets, lists, graphs, and queues. The library is built on a portable runtime layer that provides basic communication, synchronization, and caching. The data structures address the classic trade-off between locality and load balance through a combination of replication, partitioning, and dynamic caching. To tolerate remote communication latencies, some of the operations are split into a separate initiation and completion phase, allowing for computation and communication overlap at the library interface level. This leads to a form of relaxed consistency semantics for the data types. In this paper we give an overview of Multipol, discuss the performance trade-offs and interface issues, and describe some of the applications that motivated its development.
Abstract-found: 1
Intro-found: 1
Reference: [AH90] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering-a new definition. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1990. </year> <month> 12 </month>
Reference-contexts: Adve and Hill have designed a formalism and expressed these and other variants of shared access specification <ref> [AH90] </ref>. Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information [Wei88]. We have carried this idea even further in the Grobner basis program [CY93, CY94], the Oct-tree code, and the Phylogeny program. Portable parallel programming.
Reference: [AN93] <author> G.A. Alverson and D. Notkin. </author> <title> Program structuring for effective parallel portability. </title> <booktitle> IEEE Transac--tions on Parallel and Distributed Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information [Wei88]. We have carried this idea even further in the Grobner basis program [CY93, CY94], the Oct-tree code, and the Phylogeny program. Portable parallel programming. Alverson <ref> [AN93] </ref> introduced the notion of effectively portability and developed the runtime system Chameleon, which separates the abstract interface of the data structures from their implementation for portability. Crowl [CL94] developed the programming language Natasha, which supports "forall" like control abstractions and multiple implementations of the abstractions.
Reference: [And93] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. In this paper we give an overview of distributed data structures in general and Multipol in particular. <p> Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates <ref> [And93] </ref>, pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment. When tasks involve dynamic data structures, execution time and data access patterns cannot be predicted by the compiler.
Reference: [Bad91] <author> S. B. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: Another thread of work has been the development of application-specific distributed data structures, including irregular grids [BSS91], the LPARX library for writing adaptive mesh codes <ref> [Bad91] </ref>, B-trees [WW90], sets [Dal86] and oct-trees [WS93]. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. In contrast, we look at significantly more irregular, dynamic, and even non-deterministic problems. 11 Shared memory semantics.
Reference: [BBG + 93] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Maloney, and B. Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel system. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. In this paper we give an overview of distributed data structures in general and Multipol in particular. <p> Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather [FLR92] and pC++ <ref> [BBG + 93] </ref>. As language-based approaches, they encapsulate default policies for data and task assignment. When tasks involve dynamic data structures, execution time and data access patterns cannot be predicted by the compiler. Since no policy is likely to be universally applicable, user overrides are also permitted.
Reference: [BH86] <author> J. Barnes and P. Hut. </author> <title> A hierarchical o(nlogn) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <address> 324:446, </address> <year> 1986. </year>
Reference-contexts: The oct-tree is one example of such a structure. A three dimensional space is partitioned into eight equally sized cubes, and the process repeated on each sub-cube with more than one object, until each cube has at most one object <ref> [BH86] </ref>. The oct-tree is the hierarchy created from this decomposition. Event Graph: The event graph provides flow-controlled, in-order message delivery in a static network. The nodes of the event graph represent computational processes that send and receive messages along the edges.
Reference: [BRB90] <author> K.S. Brace, R.L. Rudell, and R.E. Bryant. </author> <title> Efficient implementation of a BDD package. </title> <booktitle> In 27th ACM/IEEE Design Automation Conference, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Hash Table: A common parallel programming paradigm requires that a set of processors cooperatively access a sparse address space. For boolean function manipulation, for example, this address space contains functions in the form of DAGs <ref> [BRB90] </ref>. For search problems, the processors generate sets of states in the space as they iterate over a subset of the space. The hash table provides a sparse address space distributed across the processor, with a well-behaved hash function providing a natural load balance.
Reference: [Bre94] <author> Eric Brewer. </author> <title> Portable High-Performance Supercomputing: High-Level Platform-Dependent Optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Crowl [CL94] developed the programming language Natasha, which supports "forall" like control abstractions and multiple implementations of the abstractions. The programmer explicitly selects the implementation using annotations in the source programs. Neither Alverson nor Crowl addressed the composition of multiple data structures. Brewer <ref> [Bre94] </ref> proposed a systematic approach to tuning parallel programs based on statistical sampling.
Reference: [BSS91] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory multiprocessors. </title> <journal> Concurrenty: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, these systems provide mechanisms but no general understanding of application performance with respect to data structure design and implementation, or how to address these issues in a new application. Another thread of work has been the development of application-specific distributed data structures, including irregular grids <ref> [BSS91] </ref>, the LPARX library for writing adaptive mesh codes [Bad91], B-trees [WW90], sets [Dal86] and oct-trees [WS93]. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. <p> thread of work has been the development of application-specific distributed data structures, including irregular grids <ref> [BSS91] </ref>, the LPARX library for writing adaptive mesh codes [Bad91], B-trees [WW90], sets [Dal86] and oct-trees [WS93]. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. In contrast, we look at significantly more irregular, dynamic, and even non-deterministic problems. 11 Shared memory semantics.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction We have implemented a number of irregular parallel applications, including a symbolic algebra system [CY94], a timing level circuit simulator [WY93], a biological cell simulation [Ste94], a solution to the phylogeny problem [Jon94], and a electromagnetics simulation kernel <ref> [CDG + 93] </ref>. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The neighbor relation is still important, but the connectivity pattern is irregular. In an electromagnetics application called EM3D, the mesh is irregular but static <ref> [CDG + 93] </ref>. In this case there are two set of mesh points, one with electric field values and one 2 with magnetic field values. The graph is bipartite, and computation proceeds by reading values from one set of vertices and updating the neighboring values in the other set. <p> A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D <ref> [CDG + 93] </ref>, cell simulation [Ste94], and n-body solvers, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones [Ste94] or from checking whether a possibly remote address is actually local <ref> [CDG + 93] </ref>. Message passing models in which objects cannot span processor boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [CGH] <author> Rohit Chandra, Anoop Gupta, and John L. Hennesy. </author> <title> Data locality and load balancing in COOL. </title> <note> submitted for publication. </note>
Reference-contexts: Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL <ref> [CGH] </ref>, Concurrent Aggregates [And93], pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment. When tasks involve dynamic data structures, execution time and data access patterns cannot be predicted by the compiler.
Reference: [CL94] <author> Lawrence A. Crowl and Thomas J. LeBlanc. </author> <title> Parallel programming with control abstraction. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1994. </year>
Reference-contexts: Portable parallel programming. Alverson [AN93] introduced the notion of effectively portability and developed the runtime system Chameleon, which separates the abstract interface of the data structures from their implementation for portability. Crowl <ref> [CL94] </ref> developed the programming language Natasha, which supports "forall" like control abstractions and multiple implementations of the abstractions. The programmer explicitly selects the implementation using annotations in the source programs. Neither Alverson nor Crowl addressed the composition of multiple data structures.
Reference: [CRY94] <author> Soumen Chakrabarti, Abhiram Ranade, and Katherine Yelick. </author> <title> Randomized load balancing for tree-structured computation. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The only other long-running operation is for termination detection, since this requires global agreement among the processors. There is an enormous literature on dynamic load balancing and we have two implementations: one that uses a simple randomized load balancing scheme <ref> [CRY94] </ref> and another that uses heuristics for locality along with round-robin task pushing. Both of these work well for the applications in which locality is not a major concern, including Grobner basis, the phylogeny problem, and a symmetric eigenvalue code [Sin].
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The scheduling policy used by one data structure can be changed without introducing anomalies, such as unexpected livelock or deadlock, into other parts of the program. The Multipol threads are designed for direct programming, in contrast to compiler-controlled threads such as TAM <ref> [CSS + 91] </ref>, in that Multipol provides more flexibility such as arbitrary size threads and custom schedulers. A set of macros can be used to facilitate programming. <p> Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine <ref> [CSS + 91] </ref>, and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment. When tasks involve dynamic data structures, execution time and data access patterns cannot be predicted by the compiler.
Reference: [CY93] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> On the correctness of a distributed memory Grobner basis computation. In Rewriting Techniques and Applications, </title> <address> Montreal, Canada, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of this set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93, Jon94] </ref>. 3.3 Communication Cost Reduction Some communication cannot be avoided, but its cost can be reduced by minimizing the number of messages (as opposed to the volume) and by using less expensive unacknowledged messages. <p> Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information [Wei88]. We have carried this idea even further in the Grobner basis program <ref> [CY93, CY94] </ref>, the Oct-tree code, and the Phylogeny program. Portable parallel programming. Alverson [AN93] introduced the notion of effectively portability and developed the runtime system Chameleon, which separates the abstract interface of the data structures from their implementation for portability.
Reference: [CY94] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Distributed data structures and algorithms for Grobner basis computation. </title> <booktitle> Lisp and Symbolic Computation, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction We have implemented a number of irregular parallel applications, including a symbolic algebra system <ref> [CY94] </ref>, a timing level circuit simulator [WY93], a biological cell simulation [Ste94], a solution to the phylogeny problem [Jon94], and a electromagnetics simulation kernel [CDG + 93]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information [Wei88]. We have carried this idea even further in the Grobner basis program <ref> [CY93, CY94] </ref>, the Oct-tree code, and the Phylogeny program. Portable parallel programming. Alverson [AN93] introduced the notion of effectively portability and developed the runtime system Chameleon, which separates the abstract interface of the data structures from their implementation for portability.
Reference: [Dal86] <author> William J. Dally. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, Pasadena, California, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Another thread of work has been the development of application-specific distributed data structures, including irregular grids [BSS91], the LPARX library for writing adaptive mesh codes [Bad91], B-trees [WW90], sets <ref> [Dal86] </ref> and oct-trees [WS93]. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. In contrast, we look at significantly more irregular, dynamic, and even non-deterministic problems. 11 Shared memory semantics.
Reference: [Fel82] <author> J. Felsenstein. </author> <title> Numerical methods for inferring evolutionary trees. Q. </title> <journal> Rev. Biol, </journal> <volume> 57 </volume> <pages> 379-404, </pages> <year> 1982. </year>
Reference-contexts: Each processor owns a local portion of the space, which it accesses efficiently, as well as a port into the global structure. Trie: Memoizing intermediate solutions may speed up many types of search problems. The phylogeny problem <ref> [Fel82] </ref> uses this technique for determining the evolutionary history for a set of species based on their characters. The kernel of this algorithm explores the space of all subsets of characters searching for incompatibilities with a proposed evolutionary history.
Reference: [FLR92] <author> J.A. Feldman, C.C. Lim, and T. Rauber. </author> <title> The shared-memory language psather on a distributed-memory multiprocessor. </title> <booktitle> In Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, CO, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. In this paper we give an overview of distributed data structures in general and Multipol in particular. <p> Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather <ref> [FLR92] </ref> and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment. When tasks involve dynamic data structures, execution time and data access patterns cannot be predicted by the compiler. Since no policy is likely to be universally applicable, user overrides are also permitted.
Reference: [GLL + 90] <author> Kaourosh Gharachorloo, Daniel Lenoski, James Laudon, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year>
Reference-contexts: Other potential uses are in Barnes-Hut type particle simulations. Critical to the use of the replicated list is understanding the commutativity and idempotence of the operations. While it is widely recognized that increased application control of shared memory operations improves performance <ref> [GLL + 90, LLG + 90] </ref>, we have found that control at the read/write level of abstraction is insufficient. 2.3 Scheduling Structures Task queue: Many task-parallel MIMD applications depend on some implementation of a distributed task pool to explore a task tree (or task DAG in the more general case).
Reference: [HW90] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <pages> pages 463-492, </pages> <month> July </month> <year> 1990. </year> <note> A preliminary version appeared in the proceedings of the 14th ACM Symposium on Principles of Programming Languages, </note> <year> 1987, </year> <title> under the title: Axioms for concurrent objects. </title>
Reference-contexts: Instead, long-running operations take a synchronization counter as an argument, which the caller can use to determine if the operation has completed. This leads to a relaxed consistency model for the data types, which is weaker than either sequential consistency [Lam79] or linearizability <ref> [HW90] </ref>. A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models.
Reference: [Jon94] <author> Jeff Jones. </author> <title> Parallelizing the phylogeny problem. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Computer Science Division, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: 1 Introduction We have implemented a number of irregular parallel applications, including a symbolic algebra system [CY94], a timing level circuit simulator [WY93], a biological cell simulation [Ste94], a solution to the phylogeny problem <ref> [Jon94] </ref>, and a electromagnetics simulation kernel [CDG + 93]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> In the phylogeny application and Grobner basis problem, not only are updates to the global set of results lazy, but each processor keeps partially completed cached copies of this set. This yields a correct, albeit different, execution than the sequential program <ref> [CY93, Jon94] </ref>. 3.3 Communication Cost Reduction Some communication cannot be avoided, but its cost can be reduced by minimizing the number of messages (as opposed to the volume) and by using less expensive unacknowledged messages.
Reference: [KC93] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert | Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Supercomputing'93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system <ref> [KC93] </ref>, the Chare kernel [SK91, KK93], and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment.
Reference: [KK93] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A Portable Concurrent Object Oriented System based on C++. </title> <type> Technical Report UIUCDCS-R-93-1796, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> March </month> <year> 1993. </year> <note> Also in OOPSLA'93. </note>
Reference-contexts: Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel <ref> [SK91, KK93] </ref>, and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment.
Reference: [KY94] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel spmd programs. </title> <booktitle> In Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Techniques such as pipelining remote operations and multithreading can be used to hide latency. Even on a machines like the CM-5 with relatively low communication latency, the benefits from message overlap are noticeable: message pipelining of simple remote read and write operations can save as much as 30% <ref> [KY94] </ref> and overlap of higher level operations in the Grobner basis application saves about 10%. On workstation networks with longer hardware latencies and expensive remote message handlers, the savings should be even higher. 4 The latency hiding techniques require the operations be nonblocking, or split-phase.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Instead, long-running operations take a synchronization counter as an argument, which the caller can use to determine if the operation has completed. This leads to a relaxed consistency model for the data types, which is weaker than either sequential consistency <ref> [Lam79] </ref> or linearizability [HW90]. A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. <p> The programmer's most intuitive notion of shard memory is sequential consistency, which specifies that accesses to shared memory are an arbitrary merge of accesses from all the processors, but each has atomic effect <ref> [Lam79] </ref>. Release consistency relaxes this by permitting limited reordering of the processor order of accesses, with hardware instructions to commit all outstanding writes when the program needs this property to proceed [LLG + 90].
Reference: [LKMS91] <author> S. Lin, E. Kuh, and M. Marek-Sadowska. SWEC: </author> <title> A stepwise equivalent conductance simulator for cmos vlsi circuits. </title> <booktitle> In Proc. of European Design Automation conference, </booktitle> <month> February </month> <year> 1991. </year>
Reference-contexts: The nodes of the event graph represent computational processes that send and receive messages along the edges. The messages are called events because their order with respect to the same edge should be preserved. The event graph is used in a discrete event timing simulator called SWEC <ref> [LKMS91] </ref>. Discrete event simulations like SWEC can be parallelized using either optimistic or conservative scheduling. We have parallelized SWEC using optimistic scheduling [WY93] and focus here on the conservative simulation. 2.2 Association Structures Association structures store a set of values without any relationship to a physical domain.
Reference: [LLG + 90] <author> Daniel Lenoski, James Laudon, Kaourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year>
Reference-contexts: Other potential uses are in Barnes-Hut type particle simulations. Critical to the use of the replicated list is understanding the commutativity and idempotence of the operations. While it is widely recognized that increased application control of shared memory operations improves performance <ref> [GLL + 90, LLG + 90] </ref>, we have found that control at the read/write level of abstraction is insufficient. 2.3 Scheduling Structures Task queue: Many task-parallel MIMD applications depend on some implementation of a distributed task pool to explore a task tree (or task DAG in the more general case). <p> Release consistency relaxes this by permitting limited reordering of the processor order of accesses, with hardware instructions to commit all outstanding writes when the program needs this property to proceed <ref> [LLG + 90] </ref>. Adve and Hill have designed a formalism and expressed these and other variants of shared access specification [AH90]. Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information [Wei88].
Reference: [Mad92] <author> Niel K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for maxwell's curl equations using non-orthogonal unstructured grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Bipartite Graph: Some algorithms gain efficiency and accuracy by using unstructured meshes, which place mesh points at physically significant locations, such as the face of a tetrahedron <ref> [Mad92] </ref>, and use more refined meshes in areas of greatest activity. The neighbor relation is still important, but the connectivity pattern is irregular. In an electromagnetics application called EM3D, the mesh is irregular but static [CDG + 93].
Reference: [RLW94] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In 21st International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The replicated list is built on top of a generic object layer, which implements a distributed shared memory by supporting variable size objects and a user-defined meaning of object consistency. It is the software generalization of shared memory protocols on Tempest <ref> [RLW94] </ref>. The replicated list has been used in the Grobner basis algorithm, and applies to Knuth-Bendix style theorem provers [YG92]. Other potential uses are in Barnes-Hut type particle simulations. Critical to the use of the replicated list is understanding the commutativity and idempotence of the operations.
Reference: [Sin] <author> Dhillon Inderjit Singh. </author> <title> The bisection eigenvalue algorithm. </title> <type> Unpublished manuscript. </type>
Reference-contexts: Both of these work well for the applications in which locality is not a major concern, including Grobner basis, the phylogeny problem, and a symmetric eigenvalue code <ref> [Sin] </ref>. However, more sophistication in task assignment will likely be required for other applications, that have partitioned data sets. 6 Related Work Several research efforts are related to ours: general libraries and systems software, application specific libraries, customized shared memory software, and systems for performance portability. Libraries and software systems.
Reference: [SK91] <author> Wei Shu and L.V. Kale. </author> <title> Chare kernel a runtime support system for parallel computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 198-211, </pages> <year> 1991. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. In this paper we give an overview of distributed data structures in general and Multipol in particular. <p> Libraries and software systems. Recent research has yielded a variety of runtime support like the Concert system [KC93], the Chare kernel <ref> [SK91, KK93] </ref>, and the compiler-controlled threaded abstract machine [CSS + 91], and programming languages like COOL [CGH], Concurrent Aggregates [And93], pSather [FLR92] and pC++ [BBG + 93]. As language-based approaches, they encapsulate default policies for data and task assignment.
Reference: [SL93] <author> Daniel J. Scales and Monica S. Lam. </author> <title> A flexible shared memory system for distributed memory machines. </title> <type> Unpublished manuscript, </type> <year> 1993. </year>
Reference-contexts: In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. Although several languages and runtime systems support the development of such data structures <ref> [And93, BBG + 93, FLR92, SK91, SL93] </ref>, there are no comprehensive data structure libraries, such as those that exist for uniprocessors. Multipol is such a library. In this paper we give an overview of distributed data structures in general and Multipol in particular.
Reference: [Ste94] <author> Stephen Steinberg. </author> <title> Parallelizing a cell simulation: Analysis, abstraction, and portability. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: 1 Introduction We have implemented a number of irregular parallel applications, including a symbolic algebra system [CY94], a timing level circuit simulator [WY93], a biological cell simulation <ref> [Ste94] </ref>, a solution to the phylogeny problem [Jon94], and a electromagnetics simulation kernel [CDG + 93]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> A operation completes sometime between the initiation and synchronization point, but no other ordering is guaranteed. Several applications can take advantage of relaxed consistency models. For bulk-synchronous problems such as EM3D [CDG + 93], cell simulation <ref> [Ste94] </ref>, and n-body solvers, data structure updates are delayed until the end of a computation phase, at which point all processors wait for all updates to complete. <p> Languages that support a global view of distributed data structures, for example, may incur costs from translating global indices into local ones <ref> [Ste94] </ref> or from checking whether a possibly remote address is actually local [CDG + 93]. Message passing models in which objects cannot span processor boundaries avoid these overheads, but lose the ability to form abstractions across processors.
Reference: [Wei88] <author> William E. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <type> Technical Report MIT/LCS/TM-367, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Adve and Hill have designed a formalism and expressed these and other variants of shared access specification [AH90]. Weihl's thesis makes a case for reordering accesses more aggressively than possible in hardware by using more application information <ref> [Wei88] </ref>. We have carried this idea even further in the Grobner basis program [CY93, CY94], the Oct-tree code, and the Phylogeny program. Portable parallel programming.
Reference: [WS93] <author> M.S. Warren and J.K. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 12-21, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Another thread of work has been the development of application-specific distributed data structures, including irregular grids [BSS91], the LPARX library for writing adaptive mesh codes [Bad91], B-trees [WW90], sets [Dal86] and oct-trees <ref> [WS93] </ref>. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. In contrast, we look at significantly more irregular, dynamic, and even non-deterministic problems. 11 Shared memory semantics.
Reference: [WW90] <author> William E. Weihl and Paul Wang. </author> <title> Multi-version memory: Software cache management for concurrent B-trees. </title> <booktitle> In Proceedings of the Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: Another thread of work has been the development of application-specific distributed data structures, including irregular grids [BSS91], the LPARX library for writing adaptive mesh codes [Bad91], B-trees <ref> [WW90] </ref>, sets [Dal86] and oct-trees [WS93]. Some early Fortran versions of irregular grid code can be improved by message aggregation using the PARTI library [BSS91]. In contrast, we look at significantly more irregular, dynamic, and even non-deterministic problems. 11 Shared memory semantics.
Reference: [WY93] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction We have implemented a number of irregular parallel applications, including a symbolic algebra system [CY94], a timing level circuit simulator <ref> [WY93] </ref>, a biological cell simulation [Ste94], a solution to the phylogeny problem [Jon94], and a electromagnetics simulation kernel [CDG + 93]. In all of these projects, the key parallelization task was the development and performance tuning of distributed data structures. <p> The event graph is used in a discrete event timing simulator called SWEC [LKMS91]. Discrete event simulations like SWEC can be parallelized using either optimistic or conservative scheduling. We have parallelized SWEC using optimistic scheduling <ref> [WY93] </ref> and focus here on the conservative simulation. 2.2 Association Structures Association structures store a set of values without any relationship to a physical domain. These are common in search problems and are often built as dictionaries that store key/value pairs. <p> These numbers are based on a simulation, where it is easier to control and measure the effects of individual optimizations. However, the data structure was motivated by a parallel implementation of SWEC using optimistic scheduling, which shows a speedup of 55 for a large circuit on a 64-node CM5 <ref> [WY93] </ref>. 5.5 Oct-Tree To provide an efficient implementation of an oct-tree, two important issues need to addressed: how to spread the oct-tree across processors to obtain good load balance without sacrificing locality, and how to decrease the cost of accessing remote nodes.
Reference: [YG92] <author> Katherine A. Yelick and Steven J. </author> <title> Garland. A parallel completion procedure for term rewriting systems. </title> <booktitle> In Conference on Automated Deduction, </booktitle> <address> Saratoga Springs, NY, </address> <year> 1992. </year> <title> 14 interchange approach based on bytes read per inter-particle interaction. </title> <type> 15 16 </type>
Reference-contexts: It is the software generalization of shared memory protocols on Tempest [RLW94]. The replicated list has been used in the Grobner basis algorithm, and applies to Knuth-Bendix style theorem provers <ref> [YG92] </ref>. Other potential uses are in Barnes-Hut type particle simulations. Critical to the use of the replicated list is understanding the commutativity and idempotence of the operations.
References-found: 39

