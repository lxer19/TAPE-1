URL: http://www.cs.utah.edu/~retrac/papers/tocs95.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Techniques for Reducing Consistency-Related Communication in Distributed Shared Memory Systems  
Author: John B. Carter John K. Bennett and Willy Zwaenepoel 
Note: Present address:  
Address: Houston, TX 77251-1892  Building, Salt Lake City, UT 84112  
Affiliation: Computer Systems Laboratory Rice University  Department of Computer Science, University of Utah, 3190 Merrill Engineering  
Abstract: Distributed shared memory (DSM) is an abstraction of shared memory on a distributed memory machine. Hardware DSM systems support this abstraction at the architecture level; software DSM systems support the abstraction within the runtime system. One of the key problems in building an efficient software DSM system is to reduce the amount of communication needed to keep the distributed memories consistent. In this paper we present four techniques for doing so: (1) software release consistency; (2) multiple consistency protocols; (3) write-shared protocols; and (4) an update-with-timeout mechanism. These techniques have been implemented in the Munin DSM system. We compare the performance of seven Munin application programs, first to their performance when implemented using message passing, and then to their performance when running on a conventional software DSM system that does not embody the above techniques. On a 16-processor cluster of workstations, Munin's performance is within 5% of message passing for four out of the seven applications. For the other three, performance is within 29% to 33%. Detailed analysis of two of these three applications indicates that the addition of a function shipping capability would bring their performance to within 7% of the message passing performance. Compared to a conventional DSM system, Munin achieves performance improvements ranging from a few to several hundred percent, depending on the application. This research was supported in part by the National Science Foundation under Grants CDA-8619893, CCR-9010351, CCR-9116343, by the IBM Corporation under Research Agreement No. 20170041, by the Texas Advanced Technology Program under Grants 003604014 and 003604012, and by a NASA Graduate Fellowship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Several designs for hardware distributed shared memory systems have been published recently, of which DASH [14], GalacticaNet [25], and APRIL <ref> [1] </ref> are representative. We have adopted from the DASH project [14] the concept of release consistency. The differences between DASH's implementation of release consistency and Munin's implementation of release consistency were explained in detail in Section 2.1. DASH uses a write-invalidate protocol for all consistency maintenance. <p> The GalacticaNet design includes a provision to time out updates to stale data, which is shown to have a significant effect on performance when there is a large number of processors. The APRIL machine addresses the problem of high latencies in distributed shared memory multiprocessors in a different way <ref> [1] </ref>. APRIL provides sequential consistency, but relies on extremely fast processor switching to overlap memory latency with computation. For APRIL to be successful at reducing the impact of read misses, there must be several threads ready to run on each processor.
Reference: [2] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The only way for processors to communicate is through explicit message passing. Distributed memory machines are easier to build, especially for large configurations, because unlike shared memory machines they do not require complex and expensive hardware cache controllers <ref> [2] </ref>. The shared memory programming model is, however, more attractive since most application programmers find it difficult to program machines using a message passing paradigm that requires them to explicitly partition data and manage communication.
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> Lazy release consistency reduces the number of messages required to maintain consistency, but the implementation is more expensive in terms of protocol and memory overhead [17]. A variety of systems have sought to present an object-oriented interface to shared memory. We describe the Orca <ref> [3] </ref> as an example of this approach. In general, the object-oriented nature allows the compiler and the runtime system to carry out a number of powerful optimizations, but the programs have to be written in the particular object model supported.
Reference: [4] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Multiple consistency protocols are used to keep memory consistent in accordance with the observation that no single consistency protocol is the best for all applications, or even for all data items in a single application <ref> [4, 11] </ref>. 3. Write-shared protocols address the problem of false sharing in DSM by allowing multiple processes to write concurrently into a shared page, with the updates being merged at the appropriate synchronization point, in accordance with the definition of release consistency. 4. <p> To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study <ref> [4] </ref> and others [11, 24] support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 . <p> Thus, the consistency protocol simply consists of replication on demand. A runtime error is generated if a thread attempts to write to read-only data. Migratory data is accessed multiple times by a single thread, including one or more writes, before another thread accesses the data <ref> [4, 24] </ref>. This access pattern is typical of shared data that is accessed only inside a critical section or via a work queue. <p> The consistency protocol for migratory data propagates the data to the next thread that accesses the data, provides the thread with read 1 The results of our original study <ref> [4] </ref> indicated that there were eight basic access patterns (private, write-once, migratory, write-many, producer-consumer, result, read-mostly, and synchronization), but experience has made it clear that several of the protocols were redundant. <p> Munin uses variables rather than pages as the basic unit of granularity because this better reflects the way data is used and reduces the amount of false sharing between unrelated variables <ref> [4] </ref>. Munin's strategies for maintaining the object directory are designed to reduce the number of messages required to maintain the distributed object directory. <p> In general, when a shared data item is not owned by the local node, the information in the local directory entry acts as a "hint" to reduce the overhead of performing consistency operations. 3.5 Synchronization Support Synchronization objects are accessed in a fundamentally different way than ordinary data <ref> [4] </ref>. Thus, Munin provides efficient implementations of locks, barriers, and condition variables that directly use V's communication primitives rather than synchronizing through shared memory. More elaborate synchronization mechanisms, such as monitors and atomic integers, can be built using these basic mechanisms. <p> A frequent situation in which this scheme works to particular advantage is when a thread attempts to reacquire a lock for which it was the last owner <ref> [4] </ref>. In this case, the thread finds the associated token to be available locally, and is thus able to acquire the lock immediately (without any message overhead). Similarly, if a small subset of threads continuously reuses the same lock, they communicate only with one another.
Reference: [5] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In COMPCON '93, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> On an orthogonal issue, Orca's consistency management uses an efficient, reliable, ordered broadcast protocol. For reasons of scalability, Munin does not rely on broadcast, although support for efficient multicast could improve the performance of some aspects of Munin. Midway <ref> [5] </ref> proposes a DSM system with entry consistency, a memory consistency model weaker than release consistency. The goal of Midway is to minimize communication costs by aggressively exploiting the relationship between shared variables and the synchronization objects that protect them.
Reference: [6] <author> J.B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation. A more detailed description of the Munin prototype appears elsewhere <ref> [6] </ref>. 3.1 Writing A Munin Program Munin programmers write parallel programs using threads, as they would on many shared memory multiprocessors. Synchronization is supported by library routines for the manipulation of locks, barriers and condition variables. All of the current applications were written in C.
Reference: [7] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: A distributed shared memory (DSM) system provides a shared memory programming model on a distributed memory machine. Hardware DSM systems, e.g., DASH [20], support this abstraction at the architecture level; software DSM systems, such as Ivy [21] and Munin <ref> [7] </ref>, support this abstraction within the runtime system. Software DSM systems consist of the same hardware as that found in a distributed memory machine, with the addition of a software layer, that provides the abstraction of a single shared memory. <p> Experience with release consistent memories indicates that because of the need to handle arbitrary thread preemption, most shared memory parallel programs are free of data races even when written assuming a sequentially consistent memory <ref> [7, 13] </ref>. More formally, the following constraints on the memory subsystem ensure release consistency: 1. Before an ordinary read or write is allowed to perform with respect to any other processor, all previous acquire accesses must be performed. 3 2. <p> and subsequent high-latency (idle) reloads, while at the same time retaining the superior scalability of an invalidation protocol by limiting the extent to which stale copies of particular pages are updated. 3 The Munin DSM Prototype The techniques described in Section 2 have been implemented in the Munin DSM system <ref> [7] </ref>. Munin was evaluated on a network of SUN-3/60 workstations running the V-System [9] connected via an isolated 10 megabit per second Ethernet. This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation.
Reference: [8] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year> <month> 32 </month>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21].
Reference: [9] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: These techniques have been incorporated in the Munin DSM system. Munin has been implemented on a network of SUN-3/60 workstations running the V-System <ref> [9] </ref>. <p> Munin was evaluated on a network of SUN-3/60 workstations running the V-System <ref> [9] </ref> connected via an isolated 10 megabit per second Ethernet. This section provides a brief overview of aspects of the implementation of Munin that are relevant to its evaluation. <p> A Munin system thread installs itself as the page fault handler for the Munin program. As a result, the underlying V kernel <ref> [9] </ref> forwards to this thread all memory exceptions. The Munin thread also interacts with the V kernel to communicate with the other Munin nodes over the network, and to manipulate the virtual memory system as part of maintaining the consistency of shared memory.
Reference: [10] <author> P. Dasgupta, R.C. Chen, S. Menon, M. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. LeBlanc Jr., W. Applebe, J.M. Bernabeu-Auban, P.W. Hutto, M.Y.A. Khalidi, and C.J. Wileknloh. </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <month> Winter </month> <year> 1990. </year>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [10, 12, 21] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> It is up to the programmer or the compiler to lay out the program data structures in the shared address space such that false sharing is reduced. The directory management scheme in our implementation is largely borrowed from Ivy's dynamic distributed manager scheme. Both Clouds <ref> [10] </ref> and Mirage [12] allow part of shared memory to be locked down at a particular processor. In Clouds, the programmer can request that a segment of shared memory be locked on a processor.
Reference: [11] <author> S.J. Eggers and R.H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-383, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Multiple consistency protocols are used to keep memory consistent in accordance with the observation that no single consistency protocol is the best for all applications, or even for all data items in a single application <ref> [4, 11] </ref>. 3. Write-shared protocols address the problem of false sharing in DSM by allowing multiple processes to write concurrently into a shared page, with the updates being merged at the appropriate synchronization point, in accordance with the definition of release consistency. 4. <p> To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [4] and others <ref> [11, 24] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 .
Reference: [12] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [10, 12, 21] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> It is up to the programmer or the compiler to lay out the program data structures in the shared address space such that false sharing is reduced. The directory management scheme in our implementation is largely borrowed from Ivy's dynamic distributed manager scheme. Both Clouds [10] and Mirage <ref> [12] </ref> allow part of shared memory to be locked down at a particular processor. In Clouds, the programmer can request that a segment of shared memory be locked on a processor.
Reference: [13] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluations of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Experience with release consistent memories indicates that because of the need to handle arbitrary thread preemption, most shared memory parallel programs are free of data races even when written assuming a sequentially consistent memory <ref> [7, 13] </ref>. More formally, the following constraints on the memory subsystem ensure release consistency: 1. Before an ordinary read or write is allowed to perform with respect to any other processor, all previous acquire accesses must be performed. 3 2.
Reference: [14] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Software release consistency is a software implementation of release consistency <ref> [14] </ref>, specifically aimed at reducing the number of messages required to maintain consistency in a software DSM system. Roughly speaking, release consistency requires memory to be consistent only at specific synchronization points. 2. <p> This requirement imposes severe restrictions on possible performance optimizations. Among the various relaxed memory models that have been developed, we chose the release consistency model developed as part of the DASH project <ref> [14] </ref>. Release consistency exploits the fact that programmers use synchronization to separate accesses to shared variables by different threads. The system then only needs to guarantee that memory is consistent at select synchronization points. This ability to allow temporary, but harmless, inconsistencies is what gives release consistency its power. <p> In general, if a program is free of data races, or, in other words, if there is synchronization between all conflicting shared memory accesses, then the program generates the same results on a release consistent memory system as it would on a sequentially consistent memory system <ref> [14] </ref>. Experience with release consistent memories indicates that because of the need to handle arbitrary thread preemption, most shared memory parallel programs are free of data races even when written assuming a sequentially consistent memory [7, 13]. More formally, the following constraints on the memory subsystem ensure release consistency: 1. <p> Because ordinary reads and writes can be buffered or pipelined, a release consistent memory can mask much of the communication required to keep shared data consistent. 2.1.1 Buffered Update versus Pipelined Invalidate Release Consistency The hardware implementation of release consistency in DASH <ref> [14] </ref> pipelines invalidation messages caused by writes to shared data. This implementation is primarily geared towards masking the latency of writes, rather than reducing the number of messages sent. <p> Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Several designs for hardware distributed shared memory systems have been published recently, of which DASH <ref> [14] </ref>, GalacticaNet [25], and APRIL [1] are representative. We have adopted from the DASH project [14] the concept of release consistency. The differences between DASH's implementation of release consistency and Munin's implementation of release consistency were explained in detail in Section 2.1. <p> is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Several designs for hardware distributed shared memory systems have been published recently, of which DASH <ref> [14] </ref>, GalacticaNet [25], and APRIL [1] are representative. We have adopted from the DASH project [14] the concept of release consistency. The differences between DASH's implementation of release consistency and Munin's implementation of release consistency were explained in detail in Section 2.1. DASH uses a write-invalidate protocol for all consistency maintenance.
Reference: [15] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The specific protocol varies from system to system. For instance, Ivy [21] supports a page-based write-invalidate protocol, while Emerald <ref> [15] </ref> uses object-oriented language support to handle shared 4 object invocations. Each of these systems, however, treats all shared data the same way.
Reference: [16] <author> A.R. Karlin, M.S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive snoopy caching. </title> <booktitle> In Proceedings of the 16th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 244-254, </pages> <year> 1986. </year>
Reference-contexts: This effect is one reason that existing commercial multiprocessors use invalidation-based protocols. We address this problem with a timeout algorithm similar to the competitive snoopy caching algorithm devised by Karlin <ref> [16] </ref>. The goal of the update timeout mechanism is to invalidate replicas of a cached variable that have not been accessed recently upon receipt of an update. Munin's update timeout mechanism is implemented as follows.
Reference: [17] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Lazy release consistency reduces the number of messages required to maintain consistency, but the implementation is more expensive in terms of protocol and memory overhead <ref> [17] </ref>. A variety of systems have sought to present an object-oriented interface to shared memory. We describe the Orca [3] as an example of this approach.
Reference: [18] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: This latter form of data can support spinlocks and message-passing fairly effectively. Our support for multiple protocols is more general, without added cost, and Munin's separate synchronization package removes the need to support data-driven memory. Lazy release consistency, as used in TreadMarks <ref> [18] </ref>, is an algorithm for implementing release consistency different from the one presented in this paper. Instead of updating every cached copy of a data item whenever the modifying thread performs a release operation, only the cached copies on the processor that next acquires the released lock are updated.
Reference: [19] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Related work is discussed in Section 9. We conclude in Section 10. 2 Techniques for Reducing Communication This section describes the four techniques employed by the Munin DSM system to reduce consistency-related communication. 2.1 Software Release Consistency Conventional DSM systems employ the sequential consistency model <ref> [19] </ref> as the basis for their consistency protocols. Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another read or write to shared data [21]. This requirement imposes severe restrictions on possible performance optimizations.
Reference: [20] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A distributed shared memory (DSM) system provides a shared memory programming model on a distributed memory machine. Hardware DSM systems, e.g., DASH <ref> [20] </ref>, support this abstraction at the architecture level; software DSM systems, such as Ivy [21] and Munin [7], support this abstraction within the runtime system.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: A distributed shared memory (DSM) system provides a shared memory programming model on a distributed memory machine. Hardware DSM systems, e.g., DASH [20], support this abstraction at the architecture level; software DSM systems, such as Ivy <ref> [21] </ref> and Munin [7], support this abstraction within the runtime system. Software DSM systems consist of the same hardware as that found in a distributed memory machine, with the addition of a software layer, that provides the abstraction of a single shared memory. <p> DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> This challenge can be best illustrated by considering how a conventional DSM system is implemented <ref> [21] </ref>. The global shared address space is divided in virtual memory pages. The local memory of each processor is used as a cache on the global shared address space. <p> If the access is a read, then the page becomes replicated in read-only mode. If the access is a write, then all other copies of the pages are invalidated. Throughout the rest of this paper, the term conventional DSM <ref> [21] </ref> refers to a DSM system that employs a page-based write-invalidate consistency protocol, such as the one just described. <p> Sequential consistency essentially requires that any update to shared data become visible to all other processors before the updating processor is allowed to issue another read or write to shared data <ref> [21] </ref>. This requirement imposes severe restrictions on possible performance optimizations. Among the various relaxed memory models that have been developed, we chose the release consistency model developed as part of the DASH project [14]. <p> An update protocol based on release consistency can, however, buffer writes, which reduces substantially the amount of communication required. 2.2 Multiple Consistency Protocols Most DSM systems employ a single protocol to maintain the consistency of all shared data. The specific protocol varies from system to system. For instance, Ivy <ref> [21] </ref> supports a page-based write-invalidate protocol, while Emerald [15] uses object-oriented language support to handle shared 4 object invocations. Each of these systems, however, treats all shared data the same way. <p> When a thread attempts to write to replicated data, a message is transmitted to invalidate all other copies of the data. The thread that generated the miss blocks until all invalidation messages are acknowledged. This single owner consistency protocol is typical of what existing DSM systems provide <ref> [10, 12, 21] </ref>, and is what we use exclusively to represent a conventional DSM system in our performance evaluation. Once read-only data has been initialized, no further updates occur. Thus, the consistency protocol simply consists of replication on demand. <p> We limit our discussion to those systems that are most related to the work presented in this paper. 9.1 Software DSMs Ivy was the first software DSM system <ref> [21] </ref>. It uses a single-writer, write-invalidate protocol for all data, with virtual memory pages as the units of consistency. This protocol is used as the baseline conventional protocol in our experiments.
Reference: [22] <author> R.G. Minnich and D.J. Farber. </author> <title> The Mether system: A distributed shared memory for SunOS 4.0. </title> <booktitle> In Proceedings of the Summer 1989 USENIX Conference, </booktitle> <pages> pages 51-60, </pages> <month> June </month> <year> 1989. </year> <month> 33 </month>
Reference-contexts: DSM systems combine the best features of shared memory and distributed memory machines. They support the convenient shared memory programming model on distributed memory hardware, which is more scalable and less expensive to build. However, although many DSM systems have been proposed and implemented (e.g., <ref> [3, 5, 8, 10, 12, 21, 22] </ref>), achieving good performance on DSM systems for a sizable class of applications has proven to be a major challenge. This challenge can be best illustrated by considering how a conventional DSM system is implemented [21]. <p> In both cases, the goal is to avoid extensive communication due to false sharing. The combination of software release consistency and write-shared protocols addresses the adverse effects of false sharing without introducing the delays caused by locking parts of shared memory to a processor. Mether <ref> [22] </ref> supports a number of special shared memory segments in fixed locations in the virtual address space of each machine in the system. In an attempt to support efficient memory-based spinlocks, Mether supports several different shared memory segments, each with different protocol characteristics.
Reference: [23] <author> A.C. Thekkath and H. Levy. </author> <title> Limits to low-latency communications on high-speed networks. </title> <journal> acm Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In particular, both processor and network speeds have improved by a factor of fifteen to twenty in the past four years. Interprocessor communication is still a high latency operation, but there are indications that latencies can be improved by an order of magnitude through careful protocol implementation <ref> [23] </ref>. At the same time, DRAM latencies are improving very slowly, so some form of cache will be present on essentially all future high-performance platforms. Finally, hardware DSM systems are becoming more common.
Reference: [24] <author> W.-D. Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: To understand how shared memory programs characteristically access shared data, we studied the access behavior of a suite of shared memory parallel programs. The results of this study [4] and others <ref> [11, 24] </ref> support the notion that using the flexibility of a software implementation to support multiple consistency protocols can improve the performance of DSM. They also suggest the types of access patterns that should be supported: conventional, read-only, migratory, write-shared, and synchronization 1 . <p> Thus, the consistency protocol simply consists of replication on demand. A runtime error is generated if a thread attempts to write to read-only data. Migratory data is accessed multiple times by a single thread, including one or more writes, before another thread accesses the data <ref> [4, 24] </ref>. This access pattern is typical of shared data that is accessed only inside a critical section or via a work queue.
Reference: [25] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <month> August </month> <year> 1992. </year> <month> 34 </month>
Reference-contexts: Thus, Midway is able to detect access violations without taking page faults, which eliminates the time spent handling interrupts. 9.2 Hardware DSMs Several designs for hardware distributed shared memory systems have been published recently, of which DASH [14], GalacticaNet <ref> [25] </ref>, and APRIL [1] are representative. We have adopted from the DASH project [14] the concept of release consistency. The differences between DASH's implementation of release consistency and Munin's implementation of release consistency were explained in detail in Section 2.1. DASH uses a write-invalidate protocol for all consistency maintenance. <p> DASH uses a write-invalidate protocol for all consistency maintenance. We instead use the flexibility of its software implementation to also attack the problem of read misses by using update protocols and migration when appropriate. The GalacticaNet system <ref> [25] </ref> also demonstrated that support for an update-based protocol that exploits the flexibility of a relaxed consistency protocol can improve performance by reducing the number of read misses and attendant processor stalls.
References-found: 25

