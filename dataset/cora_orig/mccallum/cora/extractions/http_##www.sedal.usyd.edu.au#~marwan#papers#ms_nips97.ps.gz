URL: http://www.sedal.usyd.edu.au/~marwan/papers/ms_nips97.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Title: Human Word Recognition Compared with Machine Reading  
Author: M. Schenkel C. Latimer M. Jabri 
Address: Sydney Sydney, NSW 2006 Australia  Sydney Sydney, NSW 2006 Australia  Sydney Sydney, NSW 2006 Australia  
Affiliation: Department of El. Eng. University of  Department of Psychology University of  Department of El. Eng. University of  
Abstract: We present a study which is concerned with word recognition rates for heavily degraded documents. We compare human with machine reading capabilities in a series of experiments, which explores the interaction of word/non-word recognition, word frequency and legality of non-words with degradation level. We also study the influence of character segmentation, and compare human performance with that of our artificial neural network model for reading. We found that the proposed computer model uses word context as efficiently as humans, but performs worse on the pure character recognition task.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Guyon, P. Albrecht, Y. Le Cun, J. Denker, and W. Hubbard. </author> <title> Design of a neural network character recognizer for a touch terminal. </title> <journal> Pattern Recognition, </journal> <volume> 24(2) </volume> <pages> 105-119, </pages> <year> 1991. </year>
Reference-contexts: We use a space displacement neural network (SDNN) which is a multi-layer feed-forward network with local connections and shared weights, the layers of which perform successively higher-level feature extraction. SDNN's are derived from Time Delay Neural Networks which have been succeshusfully used in speech recognition [2] and handwriting recognition <ref> [4, 1] </ref>. Thanks to its convolutional structure the computational complexity of the sliding window approach is kept tractable. Only about one eighth of the network connections are reevaluated for each new input window. The outputs of the SDNN are processed by a hidden Markov model (HMM).
Reference: [2] <author> K. J. Lang and G. E. Hinton. </author> <title> A Time Delay Neural Network architecture for speech recognition. </title> <type> Technical Report CMU-cs-88-152, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1988. </year>
Reference-contexts: We use a space displacement neural network (SDNN) which is a multi-layer feed-forward network with local connections and shared weights, the layers of which perform successively higher-level feature extraction. SDNN's are derived from Time Delay Neural Networks which have been succeshusfully used in speech recognition <ref> [2] </ref> and handwriting recognition [4, 1]. Thanks to its convolutional structure the computational complexity of the sliding window approach is kept tractable. Only about one eighth of the network connections are reevaluated for each new input window. The outputs of the SDNN are processed by a hidden Markov model (HMM).
Reference: [3] <author> V. I. Levenshtein. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions and reversals. Soviet Physics-Doklady, </journal> <volume> 10(8) </volume> <pages> 707-710, </pages> <year> 1966. </year>
Reference-contexts: The edit-distance measues how many edit operations (insertion, deletion and substitution) are necessary to convert a given input string into a target word <ref> [3, 5] </ref>. We now select all dictionary words that have the smallest edit-distance to the string recognized without dictionary. The such composed word list contains on average in the order of 10 words. The list length varies considerable depending mance. on the quality of the initial string.
Reference: [4] <author> O. Matan, C. J. C. Burges, Y. Le Cun, and J. Denker. </author> <title> Multi-digit recognition using a Space Dispacement Neural Network. </title> <editor> In J. E. Moody, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 488-495, </pages> <address> Denver, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We use a space displacement neural network (SDNN) which is a multi-layer feed-forward network with local connections and shared weights, the layers of which perform successively higher-level feature extraction. SDNN's are derived from Time Delay Neural Networks which have been succeshusfully used in speech recognition [2] and handwriting recognition <ref> [4, 1] </ref>. Thanks to its convolutional structure the computational complexity of the sliding window approach is kept tractable. Only about one eighth of the network connections are reevaluated for each new input window. The outputs of the SDNN are processed by a hidden Markov model (HMM).
Reference: [5] <author> T. Okuda, E. Tanaka, and K. Tamotsu. </author> <title> A method for the correction of garbled words based on the Levenshtein metric. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-25(2):172-177, </volume> <year> 1976. </year>
Reference-contexts: The edit-distance measues how many edit operations (insertion, deletion and substitution) are necessary to convert a given input string into a target word <ref> [3, 5] </ref>. We now select all dictionary words that have the smallest edit-distance to the string recognized without dictionary. The such composed word list contains on average in the order of 10 words. The list length varies considerable depending mance. on the quality of the initial string.
Reference: [6] <author> M. Schenkel, I. Guyon, and D. Henderson. </author> <title> On-line cursive script recognition using Time Delay Neural Networks and Hidden Markov Models. </title> <journal> Machine Vision and Applications, </journal> <volume> 8 </volume> <pages> 215-223, </pages> <year> 1995. </year>
Reference-contexts: Training and testing took about one hour. a viewing angle of 1 ffi . 4 Machine Reading For the machine reading tests, we used our integrated segmentation/recognition system, using a sliding window technique with a combination of a neural network and a hidden Markov model <ref> [6, 7] </ref>. In the following we describe the basic workings without going into too much detail on the specific algorithms. For more detailed description see [6]. A sliding window approach to word recognition performs no segmentation on the input data of the recognizer. <p> In the following we describe the basic workings without going into too much detail on the specific algorithms. For more detailed description see <ref> [6] </ref>. A sliding window approach to word recognition performs no segmentation on the input data of the recognizer. It consists basically of sweeping a window over the input word in small steps.
Reference: [7] <author> M. Schenkel and M. Jabri. </author> <title> Degraded printed document recognition using convolutional neural networks and hidden markov models. </title> <note> In accepted for the ACNN-97, </note> <year> 1997. </year>
Reference-contexts: Training and testing took about one hour. a viewing angle of 1 ffi . 4 Machine Reading For the machine reading tests, we used our integrated segmentation/recognition system, using a sliding window technique with a combination of a neural network and a hidden Markov model <ref> [6, 7] </ref>. In the following we describe the basic workings without going into too much detail on the specific algorithms. For more detailed description see [6]. A sliding window approach to word recognition performs no segmentation on the input data of the recognizer. <p> Normally additive costs are used instead of multiplicative probabilities. The HMM then selects the word causing the smallest costs. Our best architecture contains 4 convolutional layers with a total of 50,000 parameters <ref> [7] </ref>. 4.1 The Dictionary Model A natural way of including a dictionary in this process, is to restrict the solution space of the HMM to words given by the dictionary.
References-found: 7

