URL: http://dimacs.rutgers.edu/techps/1994/94-38.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1994.html
Root-URL: http://www.cs.rutgers.edu
Title: Scheduling Algorithms to Improve the Performance of Parallel Data Transfers 1  
Author: by Dannie Durand ; Ravi Jain David Tseytlin 
Date: April 26, 1994  
Address: 445 South Street Morristown, New Jersey 07960  
Affiliation: Bellcore  
Note: Distributed  ing Workshop, Cancun, Mexico,  2 Permanent Member 3 Permanent Member 4 Visitor DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell Laboratories and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 94-38 July 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal and J. S. Vitter. </author> <title> The Input/Output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <pages> pages 1116-1127, </pages> <month> Sep. </month> <year> 1988. </year> <month> - 19 </month> - 
Reference-contexts: These include both methods to improve the rate of I/O delivery to uniprocessor systems by introducing parallelism into the I/O subsystem, and methods of improving the I/O performance of multiprocessors. At the highest level, new theoretical models of parallel I/O systems are being developed <ref> [1, 33, 25, 32] </ref>, allowing the study of many fundamental algorithms in terms of their I/O complexity. At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29].
Reference: [2] <author> T. E. Anderson, S.S. Owicki, J. B. Saxe, and C. P. Thacker. </author> <title> High-Speed Switch Scheduling for Local-Area Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The algorithms are based on a simple bidding scheme similar to those used by <ref> [27, 2] </ref>: For all Clients: Color an incident edge chosen uniformly at random. <p> For this paper, we focus on the HDF heuristic. 4.4 Related Work Some related distributed communications algorithms have been presented by <ref> [27, 2, 15] </ref>. Anderson et al. [2] describe a matching-based scheduling algorithm for routing data cells from inputs to outputs in an ATM communications switch. The scale of their problem is quite different since their algorithm must be implemented in hardware and complete in real time. <p> For this paper, we focus on the HDF heuristic. 4.4 Related Work Some related distributed communications algorithms have been presented by [27, 2, 15]. Anderson et al. <ref> [2] </ref> describe a matching-based scheduling algorithm for routing data cells from inputs to outputs in an ATM communications switch. The scale of their problem is quite different since their algorithm must be implemented in hardware and complete in real time.
Reference: [3] <institution> Inside the TC2000, </institution> <year> 1990. </year>
Reference-contexts: As an example, slotted and retry protocols for sending messages have been used in the BBN TC2000 parallel computer <ref> [3] </ref>. The protocols are provided at a hardware level transparently to the user, and their relative performance for different workloads has been studied.
Reference: [4] <author> Claude Berge. </author> <title> Graphs. </title> <publisher> North Holland, </publisher> <year> 1985. </year>
Reference-contexts: Recall that the degree of a vertex is the number of edges incident upon it, and the degree of a graph, or graph degree, is the maximum degree of any vertex. It is well known <ref> [4] </ref> that colors are necessary and sufficient to edge-color a bipartite graph of degree . 4 A Distributed Scheduling Algorithm In this section, we present a parameterized class of distributed bipartite edge coloring algorithms to solve the data transfer scheduling problem described above.
Reference: [5] <author> L. Bianco, J. Blazewicz, P. Dell'Olmo P, and M. Drozdowski. </author> <title> Scheduling multiprocessor tasks on a dynamic configuration of dedicated processors. </title> <type> Technical Report R-92/045, </type> <institution> Institute of Computing Science, TU Poznan, </institution> <year> 1992. </year>
Reference-contexts: For example, the results of Blazewicz et al. assume that tasks do not require specific pre-assigned resource instances, which is not relevant for the I/O situation (see <ref> [5, 6, and references therein] </ref>). As another example, the ground-breaking work by Coffman et al. [8] on file transfer scheduling assumes transfers cannot be preempted once started, leading to NP-complete problems in general.
Reference: [6] <author> L. Bianco, J. Blazewicz, P. Dell'Olmo P, and M. Drozdowski. </author> <title> Scheduling preemptive multiprocessor tasks on dedicated processors. </title> <booktitle> Perf. Eval., </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: For example, the results of Blazewicz et al. assume that tasks do not require specific pre-assigned resource instances, which is not relevant for the I/O situation (see <ref> [5, 6, and references therein] </ref>). As another example, the ground-breaking work by Coffman et al. [8] on file transfer scheduling assumes transfers cannot be preempted once started, leading to NP-complete problems in general.
Reference: [7] <author> E. G. Coffman, Jr., </author> <title> editor. Computer and job-shop scheduling theory. </title> <publisher> John Wiley, </publisher> <year> 1976. </year>
Reference-contexts: For example, most previous work concerns job-shop and multiprocessor scheduling, in which tasks must acquire several resources but only one at a time (see <ref> [7, 16, 23, 26, 19] </ref> for surveys). Serial acquisition of multiple resource does not, in general, lead to optimal schedules; algorithms which simultaneously schedule multiple resources are required. The relatively little previous work on simultaneous resource scheduling does not apply directly to our problem.
Reference: [8] <author> E. G. Coffman, Jr., M. R. Garey, D. S. Johnson, and A. S. LaPaugh. </author> <title> Scheduling file transfers. </title> <journal> SIAM Journal of Computing, </journal> <volume> 3 </volume> <pages> 744-780, </pages> <year> 1985. </year>
Reference-contexts: For example, the results of Blazewicz et al. assume that tasks do not require specific pre-assigned resource instances, which is not relevant for the I/O situation (see [5, 6, and references therein]). As another example, the ground-breaking work by Coffman et al. <ref> [8] </ref> on file transfer scheduling assumes transfers cannot be preempted once started, leading to NP-complete problems in general. For I/O operations this assumption is not necessary, since in practice most I/O transfers are performed in terms of fixed-size blocks, and preemption is permitted at block boundaries. <p> It may be possible to extend our results for bipartite topologies to general graphs, as in the approach taken by [20]. Similarly, limitations in the bandwidth of the network can be considered, as well as variable-length transfers in which preemption is not permitted, as in <ref> [8] </ref>. Finally, we will consider this work in the context of disk striping. Some research has indicated that striping gives excellent speedups for some applications and is difficult to exploit for others.
Reference: [9] <author> P. F. Corbett, S. J. Baylor, and D. G. Feitelson. </author> <title> Overview of the Vesta Parallel File System. </title> <booktitle> In The 1993 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-17, </pages> <year> 1993. </year>
Reference-contexts: At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29]. Operating systems optimizations include layer integration and integrated buffer management to reduce copying costs, and research in file systems <ref> [9, 22] </ref>. At the lowest level, performance improvements are being achieved at the hardware and network level. Fine-grain parallelism at the disk level has been proposed through mechanisms such as disk striping, interleaving, RAID and RADD [28, 31].
Reference: [10] <author> Thomas H. Corman. </author> <title> Fast permuting on disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, 17:41 -57, </journal> <month> January </month> <year> 1993. </year>
Reference-contexts: Kotz and Cormen [21, 11] have studied these requirements. In this paper, we describe an approach to reducing the I/O bottleneck using scheduling techniques. In a single out-of-core application with regular data patterns, the programmer can design a specific schedule to optimize data movement as, for example, in <ref> [10] </ref>. However, in time-sharing systems or in dynamic applications with irregular data movement, more general scheduling techniques must be considered.
Reference: [11] <author> T. H. Cormen and D. Kotz. </author> <title> Integrating Theory and Practise in Parallel File Systems. </title> <booktitle> In Proceedings of the DAGS 93 Symposium on Parallel I/O and Databases, </booktitle> <pages> pages 64 - 74, </pages> <year> 1993. </year>
Reference-contexts: Finally, to support solutions to the I/O problem, new disk architectures must be sufficiently flexible and programmable that new I/O paradigms can be implemented and tested. Kotz and Cormen <ref> [21, 11] </ref> have studied these requirements. In this paper, we describe an approach to reducing the I/O bottleneck using scheduling techniques. In a single out-of-core application with regular data patterns, the programmer can design a specific schedule to optimize data movement as, for example, in [10].
Reference: [12] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved Parallel I/O via a Two-phase Run-time Access Strategy. </title> <booktitle> In The 1993 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year>
Reference-contexts: At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion <ref> [12] </ref> and compiler hints [29]. Operating systems optimizations include layer integration and integrated buffer management to reduce copying costs, and research in file systems [9, 22]. At the lowest level, performance improvements are being achieved at the hardware and network level. <p> Finally, we will consider this work in the context of disk striping. Some research has indicated that striping gives excellent speedups for some applications and is difficult to exploit for others. For example, del Rosario et al. <ref> [12] </ref> have shown that I/O performance is highly sensitive to the interaction of stripe size and in-core data layout. Hence, for those cases where disk striping is not effective, I/O scheduling may be promising.
Reference: [13] <author> P. J. Denning. </author> <title> Effects of scheduling on file memory operations. </title> <booktitle> In Proc. AFIPS Spring Joint Comp. Conf., </booktitle> <pages> pages 9-21, </pages> <year> 1967. </year>
Reference-contexts: However, in time-sharing systems or in dynamic applications with irregular data movement, more general scheduling techniques must be considered. One important innovation that addresses the - 2 - I/O bottleneck in sequential computer systems was to schedule I/O operations by reordering the requests in the queues at devices <ref> [13, 30, and references therein] </ref>. Explicit scheduling of I/O operations is also a potentially significant contributor to an integrated approach towards solving the I/O bottleneck in parallel computer systems.
Reference: [14] <author> M.D. Durand, T. Montaut, L. Kervella, and W. Jalby. </author> <title> Impact of Memory Contention on Dynamic Scheduling on NUMA Multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Much of the previous work on scheduling deals with tasks which each require only a single resource at any given time <ref> [19, 14] </ref>, and is not relevant for I/O operations which each require a pre-assigned set of multiple resources (e.g. a processor, channel, and disk) simultaneously in order to execute.
Reference: [15] <author> M. Gereb-Graus and T. Tsantilas. </author> <title> Efficient Optical Communication in Parallel Computers. </title> <booktitle> In 1992 Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 41-48, </pages> <year> 1992. </year> <month> - 20 </month> - 
Reference-contexts: For this paper, we focus on the HDF heuristic. 4.4 Related Work Some related distributed communications algorithms have been presented by <ref> [27, 2, 15] </ref>. Anderson et al. [2] describe a matching-based scheduling algorithm for routing data cells from inputs to outputs in an ATM communications switch. The scale of their problem is quite different since their algorithm must be implemented in hardware and complete in real time. <p> The scale of their problem is quite different since their algorithm must be implemented in hardware and complete in real time. Nevertheless, the approach is interesting and can offer some insight into the parallel data transfer problem. Gereb-Graus and Tsantilas <ref> [15] </ref> have presented some work on distributed communication algorithms for optical computers. In the optical communication parallel (OCP) computer model, if two or more messages arrive at the same port simultaneously, neither message is accepted. If a single message arrives at a server, the server sends back an acknowledgement. <p> Panconesi and Srinavasan show analytically that PS requires at most 1:6 + log 2+ffi n colors to edge-color the entire graph. 4.5 A Decentralized Algorithm Requiring Global Information Note that, unlike the algorithm of Gereb-Graus and Tsantilas <ref> [15] </ref> and the PS algorithm of Panconesi and Srinavasan [27] described in Section 4.4, our algorithms do not rely upon any global information about the graph degree in order to operate correctly. For the purposes of comparison, we implemented an algorithm which does use global information. <p> It would be interesting to determine experimentally the appropriate rate at which to reduce Ncolors for different types of graphs. For the graph sizes we studied, our results suggest that the choice of Ncolors = (i) is too large. The work of Gereb-Graus and Tsantilas <ref> [15] </ref> gives some intuition into the time required for unscheduled transfers. Recall that in their algorithm, described in Section 4.4, there is no scheduling stage.
Reference: [16] <author> Mario Gonzalez, Jr. </author> <title> Deterministic processor scheduling. </title> <journal> Computing Surveys, </journal> <volume> 9:173, </volume> <month> Sept. </month> <year> 1977. </year>
Reference-contexts: For example, most previous work concerns job-shop and multiprocessor scheduling, in which tasks must acquire several resources but only one at a time (see <ref> [7, 16, 23, 26, 19] </ref> for surveys). Serial acquisition of multiple resource does not, in general, lead to optimal schedules; algorithms which simultaneously schedule multiple resources are required. The relatively little previous work on simultaneous resource scheduling does not apply directly to our problem.
Reference: [17] <author> R. Jain, K. Somalwar, J. Werth, and J.C. Browne. </author> <title> Scheduling Parallel I/O Operations in Multiple Bus Systems. </title> <journal> Journal of Parallel and Distributed Computing, 16:352 -362, </journal> <month> December </month> <year> 1992. </year>
Reference-contexts: Previous simulation studies have shown that centralized, static scheduling algorithms can reduce the time required to complete a set of data transfers by up to 40% <ref> [17, 18] </ref>. Furthermore, scheduling becomes increasingly attractive as the I/O bottleneck becomes more severe: as processor speeds increase, the overhead for computing good schedules decreases while the importance of rapidly delivering data to the processors increases.
Reference: [18] <author> R. Jain, K. Somalwar, J. Werth, and J.C. Browne. </author> <title> Requirements and Heuristics for Scheduling Parallel I/O Operations. </title> <note> DRAFT; submitted to journal, </note> <year> 1993. </year>
Reference-contexts: Previous simulation studies have shown that centralized, static scheduling algorithms can reduce the time required to complete a set of data transfers by up to 40% <ref> [17, 18] </ref>. Furthermore, scheduling becomes increasingly attractive as the I/O bottleneck becomes more severe: as processor speeds increase, the overhead for computing good schedules decreases while the importance of rapidly delivering data to the processors increases. <p> For this paper, we focus on one heuristic which has proven to be very effective in centralized algorithms <ref> [18] </ref>. The heuristic we use is called Highest Degree First (HDF). With HDF, - 8 - clients continue to select an edge to color uniformly at random in line 7. However, when each client sends its proposed colors to the servers, it now includes its current degree as well.
Reference: [19] <author> Ravi Jain. </author> <title> Scheduling data transfers in parallel computers and communications systems. </title> <type> Technical Report TR-93-03, </type> <institution> Univ. Texas at Austin, Dept. of Comp. Sci., </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Much of the previous work on scheduling deals with tasks which each require only a single resource at any given time <ref> [19, 14] </ref>, and is not relevant for I/O operations which each require a pre-assigned set of multiple resources (e.g. a processor, channel, and disk) simultaneously in order to execute. <p> For example, most previous work concerns job-shop and multiprocessor scheduling, in which tasks must acquire several resources but only one at a time (see <ref> [7, 16, 23, 26, 19] </ref> for surveys). Serial acquisition of multiple resource does not, in general, lead to optimal schedules; algorithms which simultaneously schedule multiple resources are required. The relatively little previous work on simultaneous resource scheduling does not apply directly to our problem. <p> It has either considered very general resource requirements, leading to problems known to be NP-complete or requiring linear programming solutions of high time complexity, or made assumptions which are not relevant for scheduling parallel I/O operations (see <ref> [19] </ref> for a survey). For example, the results of Blazewicz et al. assume that tasks do not require specific pre-assigned resource instances, which is not relevant for the I/O situation (see [5, 6, and references therein]).
Reference: [20] <author> H. J. Karloff and D. B. Schmoys. </author> <title> Efficient Parallel Algorithms for Edge Coloring Problems. </title> <journal> Journal of Algorithms, </journal> <pages> pages 39 -52, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Important extensions to our work include the modeling of more complex I/O interconnection architectures, e.g. to model multimedia mail in a workstation cluster. It may be possible to extend our results for bipartite topologies to general graphs, as in the approach taken by <ref> [20] </ref>. Similarly, limitations in the bandwidth of the network can be considered, as well as variable-length transfers in which preemption is not permitted, as in [8]. Finally, we will consider this work in the context of disk striping.
Reference: [21] <author> D. Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proc. 2nd Intl. Conf. Par. Distrib. Info. Sys., </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Finally, to support solutions to the I/O problem, new disk architectures must be sufficiently flexible and programmable that new I/O paradigms can be implemented and tested. Kotz and Cormen <ref> [21, 11] </ref> have studied these requirements. In this paper, we describe an approach to reducing the I/O bottleneck using scheduling techniques. In a single out-of-core application with regular data patterns, the programmer can design a specific schedule to optimize data movement as, for example, in [10].
Reference: [22] <author> O. Kreiger and M. Stumm. </author> <title> HFS: A Flexible File System for large-scale Multiprocessors. </title> <booktitle> In Proceedings of the DAGS 93 Symposium on Parallel I/O and Databases, </booktitle> <pages> pages 6 - 14, </pages> <year> 1993. </year>
Reference-contexts: At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29]. Operating systems optimizations include layer integration and integrated buffer management to reduce copying costs, and research in file systems <ref> [9, 22] </ref>. At the lowest level, performance improvements are being achieved at the hardware and network level. Fine-grain parallelism at the disk level has been proposed through mechanisms such as disk striping, interleaving, RAID and RADD [28, 31].
Reference: [23] <author> E. L. Lawler, J. K. Lenstra, and A. H. G. Rinnooy Kan. </author> <title> Recent developments in deterministic sequencing and scheduling: A survey. In Deterministic and Stochastic Scheduling, </title> <address> pages 35-73. D. </address> <publisher> Reidel Publishing, </publisher> <year> 1982. </year>
Reference-contexts: For example, most previous work concerns job-shop and multiprocessor scheduling, in which tasks must acquire several resources but only one at a time (see <ref> [7, 16, 23, 26, 19] </ref> for surveys). Serial acquisition of multiple resource does not, in general, lead to optimal schedules; algorithms which simultaneously schedule multiple resources are required. The relatively little previous work on simultaneous resource scheduling does not apply directly to our problem.
Reference: [24] <author> M. Luby. </author> <title> Removing Randomness in Parallel Computation without a Processor Penalty. </title> <booktitle> In Proceedings of the IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 162-173, </pages> <year> 1988. </year>
Reference-contexts: When (i) &lt; t , PS switches to a distributed algorithm proposed by Luby <ref> [24] </ref>. Luby's algorithm, which is a vertex-coloring algorithm, is used to color the line graph of G, thus requiring up to 2 t 1 colors.
Reference: [25] <author> M. Nodine and J. S. Vitter. </author> <title> Paradigms for optimal sorting with multiple disks. </title> <booktitle> In Proc. 26th Hawaii Intl. Conf. Sys. Sci., </booktitle> <pages> page 50, </pages> <year> 1993. </year>
Reference-contexts: These include both methods to improve the rate of I/O delivery to uniprocessor systems by introducing parallelism into the I/O subsystem, and methods of improving the I/O performance of multiprocessors. At the highest level, new theoretical models of parallel I/O systems are being developed <ref> [1, 33, 25, 32] </ref>, allowing the study of many fundamental algorithms in terms of their I/O complexity. At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29].
Reference: [26] <author> Krishna Palem. </author> <title> On the complexity of precedence constrained scheduling. </title> <type> PhD thesis, </type> <institution> Univ. Texas at Austin, Dept. of Comp. Sci., </institution> <year> 1986. </year> <note> Available as Tech. Rept. TR-86-11. </note>
Reference-contexts: For example, most previous work concerns job-shop and multiprocessor scheduling, in which tasks must acquire several resources but only one at a time (see <ref> [7, 16, 23, 26, 19] </ref> for surveys). Serial acquisition of multiple resource does not, in general, lead to optimal schedules; algorithms which simultaneously schedule multiple resources are required. The relatively little previous work on simultaneous resource scheduling does not apply directly to our problem.
Reference: [27] <author> A. Panconesi and A Srinavasan. </author> <title> Fast Randomized Algorithms for Distributed Edge Coloring. </title> <booktitle> In Proceedings of the 1992 ACM Symposium on Parallel and Distributed Computing, </booktitle> <pages> pages 251-262, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The algorithms are based on a simple bidding scheme similar to those used by <ref> [27, 2] </ref>: For all Clients: Color an incident edge chosen uniformly at random. <p> For this paper, we focus on the HDF heuristic. 4.4 Related Work Some related distributed communications algorithms have been presented by <ref> [27, 2, 15] </ref>. Anderson et al. [2] describe a matching-based scheduling algorithm for routing data cells from inputs to outputs in an ATM communications switch. The scale of their problem is quite different since their algorithm must be implemented in hardware and complete in real time. <p> This approach does not incur the cost of scheduling, but may result in substantially longer communication times for certain architectures. This tradeoff will be discussed in detail in Section 5.3. Panconesi and Srinavasan <ref> [27] </ref> presented a distributed algorithm for edge-coloring on general graphs. They developed sophisticated mathematical tools which allow them to prove rigorous results concerning the complexity of their algorithm and the number of colors used. There are two major differences between our algorithm and the algorithm of Panconesi and Srinavasan [27], henceforth <p> Srinavasan <ref> [27] </ref> presented a distributed algorithm for edge-coloring on general graphs. They developed sophisticated mathematical tools which allow them to prove rigorous results concerning the complexity of their algorithm and the number of colors used. There are two major differences between our algorithm and the algorithm of Panconesi and Srinavasan [27], henceforth called PS. First, PS does not use heuristics or multiple passes to improve the matchings obtained in each phase. Second, as it stands, the algorithm is not suitable for implementation on a fully distributed system because it requires some global information. <p> Panconesi and Srinavasan show analytically that PS requires at most 1:6 + log 2+ffi n colors to edge-color the entire graph. 4.5 A Decentralized Algorithm Requiring Global Information Note that, unlike the algorithm of Gereb-Graus and Tsantilas [15] and the PS algorithm of Panconesi and Srinavasan <ref> [27] </ref> described in Section 4.4, our algorithms do not rely upon any global information about the graph degree in order to operate correctly. For the purposes of comparison, we implemented an algorithm which does use global information. <p> For Ncolors = 1, our algorithms yield schedule lengths of 1:05 -1:2 for HDF and 1:02 -1:09 for four passes, a substantial improvement over the algorithms Panconesi and Srinavasan <ref> [27] </ref>, whose schedule lengths have been shown to be ~ 1:6 theoretically. In our experiments, for the situations we studied, we saw that the modified algorithm mPS also produced schedule lengths of ~ 1:6. However, because PS uses Ncolors &gt; 1, it may generate schedules faster than our algorithms.
Reference: [28] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: At the lowest level, performance improvements are being achieved at the hardware and network level. Fine-grain parallelism at the disk level has been proposed through mechanisms such as disk striping, interleaving, RAID and RADD <ref> [28, 31] </ref>. Finally, to support solutions to the I/O problem, new disk architectures must be sufficiently flexible and programmable that new I/O paradigms can be implemented and tested. Kotz and Cormen [21, 11] have studied these requirements.
Reference: [29] <author> R. H. Patterson, G. A. Gibson, and M. Satyanarayanan. </author> <title> Informed Prefetching: Converting High Throughput to Low Latency. </title> <booktitle> In Proceedings of the DAGS 93 Symposium on Parallel I/O and Databases, </booktitle> <pages> pages 41 - 55, </pages> <year> 1993. </year> <month> - 21 </month> - 
Reference-contexts: At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints <ref> [29] </ref>. Operating systems optimizations include layer integration and integrated buffer management to reduce copying costs, and research in file systems [9, 22]. At the lowest level, performance improvements are being achieved at the hardware and network level.
Reference: [30] <author> A. Silberschatz and J. Peterson. </author> <title> Operating systems concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: However, in time-sharing systems or in dynamic applications with irregular data movement, more general scheduling techniques must be considered. One important innovation that addresses the - 2 - I/O bottleneck in sequential computer systems was to schedule I/O operations by reordering the requests in the queues at devices <ref> [13, 30, and references therein] </ref>. Explicit scheduling of I/O operations is also a potentially significant contributor to an integrated approach towards solving the I/O bottleneck in parallel computer systems.
Reference: [31] <author> M. Stonebraker and G. A. Schloss. </author> <title> Distributed RAID anew multiple copy algorithm. </title> <booktitle> In Proc. 6th Intl. Conf. Data Eng., </booktitle> <pages> pages 430-437, </pages> <year> 1990. </year>
Reference-contexts: At the lowest level, performance improvements are being achieved at the hardware and network level. Fine-grain parallelism at the disk level has been proposed through mechanisms such as disk striping, interleaving, RAID and RADD <ref> [28, 31] </ref>. Finally, to support solutions to the I/O problem, new disk architectures must be sufficiently flexible and programmable that new I/O paradigms can be implemented and tested. Kotz and Cormen [21, 11] have studied these requirements.
Reference: [32] <author> J. S. Vitter and M. H. Nodine. </author> <title> Large-scale sorting in uniform memory hierarchies. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 107-114, </pages> <address> Jan./Feb. </address> <year> 1993. </year>
Reference-contexts: These include both methods to improve the rate of I/O delivery to uniprocessor systems by introducing parallelism into the I/O subsystem, and methods of improving the I/O performance of multiprocessors. At the highest level, new theoretical models of parallel I/O systems are being developed <ref> [1, 33, 25, 32] </ref>, allowing the study of many fundamental algorithms in terms of their I/O complexity. At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29].
Reference: [33] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proc. ACM Symp. Theory of Comp., </booktitle> <year> 1990. </year>
Reference-contexts: These include both methods to improve the rate of I/O delivery to uniprocessor systems by introducing parallelism into the I/O subsystem, and methods of improving the I/O performance of multiprocessors. At the highest level, new theoretical models of parallel I/O systems are being developed <ref> [1, 33, 25, 32] </ref>, allowing the study of many fundamental algorithms in terms of their I/O complexity. At the next level, new language and compiler features are being developed to support I/O parallelism and optimizations, using data layout conversion [12] and compiler hints [29].
References-found: 33

