URL: ftp://ftp.cs.helsinki.fi/pub/Reports/by_Project/Cosco/MDL_Learning_of_Probabilistic_Neural_Networks_for_Discrete_Problem_Domains.ps.gz
Refering-URL: 
Root-URL: 
Title: MDL Learning of Probabilistic Neural Networks for Discrete Problem Domains  
Author: Henry Tirri and Petri Myllymaki 
Date: June 1994), 1493-1497.  
Note: Proceedings of the IEEE World Congress on Computational Intelligence (Orlando,  
Abstract: Given a problem, a case-based reasoning (CBR) system will search its case memory and use the stored cases to find the solution, possibly modifying retrieved cases to adapt to the required input specifications. In discrete domains CBR reasoning can be based on a rigorous Bayesian probability propagation algorithm. Such a Bayesian CBR system can be implemented as a probabilistic feedforward neural network with one of the layers representing the cases. In this paper we introduce a Minimum Description Length (MDL) based learning algorithm to obtain the proper network structure with the associated conditional probabilities. This algorithm together with the resulting neural network implementation provide a massively parallel architecture for solving the efficiency bottleneck in case-based reasoning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor and D. Freeman, </author> <title> AutoClass: A Bayesian Classification System. Pp. </title> <booktitle> 54-64 in Proc. of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, June 12-14, </address> <publisher> 1988 (Mor-gan Kaufmann). </publisher> <pages> 1496 </pages>
Reference: [2] <author> C. Chow and C. Liu, </author> <title> Approximating dis-crete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 462-467, </pages> <year> 1968. </year>
Reference-contexts: Hence we will now illustrate how such Bayesian tree structures can be learned, the resulting tree can then be trivially transformed into the respective neural network. Work on learning Bayesian networks has recently increased as the networks have become more widely applied. Of the research related to ours <ref> [2, 3, 14] </ref> the closest to the ideas applied here is that of Lam and Bacchus [8]. They suggest an MDL learning scheme for general, multiply connected Bayesian networks. However, their encoding scheme is biased towards sparse networks with few values.
Reference: [3] <author> G.F. Cooper and E. Herskovits, </author> <title> A Bayesian Method for Induction of Probabilistic Networks from Data. </title> <journal> Machine Learning, </journal> <volume> vol. </volume> <pages> 9 , pp. 309-347, </pages> <year> 1992. </year>
Reference-contexts: Hence we will now illustrate how such Bayesian tree structures can be learned, the resulting tree can then be trivially transformed into the respective neural network. Work on learning Bayesian networks has recently increased as the networks have become more widely applied. Of the research related to ours <ref> [2, 3, 14] </ref> the closest to the ideas applied here is that of Lam and Bacchus [8]. They suggest an MDL learning scheme for general, multiply connected Bayesian networks. However, their encoding scheme is biased towards sparse networks with few values.
Reference: [4] <author> H. Kitano, </author> <title> Challenges of Massive Parallelism. Pp. </title> <booktitle> 813-834 in Proc. of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, France, </address> <publisher> August 1993 (Morgan Kaufmann). </publisher>
Reference: [5] <author> J. Kolodner, </author> <title> Case-Based Reasoning. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference: [6] <author> I. Kononenko, </author> <title> Successive Naive Bayesian Classifier. </title> <journal> Informatica, </journal> <volume> vol. 17, </volume> <pages> pp. 167-174, </pages> <year> 1993. </year>
Reference: [7] <author> P. Langley, </author> <title> Induction of Recursive Bayesian Classifiers. </title> <note> Pp. 153-164 in P.B. </note> <editor> Brazdil (ed.), </editor> <booktitle> Proc. of ECML-93, European Conference on Machine Learning, </booktitle> <address> Vienna, Austria, April 5-7, </address> <publisher> 1993 (Springer-Verlag). </publisher>
Reference: [8] <author> W. Lam and F. Bacchus, </author> <title> Using Causal Information and Local Measures to Learn Bayesian Networks. Pp. </title> <editor> 243-250 in D. Heckerman and A. Mamdani (eds.), </editor> <booktitle> Proc. of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington, D.C., July 9-11, </address> <publisher> 1993 (Morgan Kaufmann). </publisher>
Reference-contexts: 1 One should observe that in noisy domains the best strategy is not necessary to store all the cases encountered ing analogous to the one presented in <ref> [8] </ref> (see Section 3). In Section 4 we will outline the learning algorithm, and Section 5 gives the conclusions. II. <p> Work on learning Bayesian networks has recently increased as the networks have become more widely applied. Of the research related to ours [2, 3, 14] the closest to the ideas applied here is that of Lam and Bacchus <ref> [8] </ref>. They suggest an MDL learning scheme for general, multiply connected Bayesian networks. However, their encoding scheme is biased towards sparse networks with few values. In our case it will give too high a penalty for introducing new values to the root node, i.e., for introducing new cases. <p> In our case it will give too high a penalty for introducing new values to the root node, i.e., for introducing new cases. This observation was also verified empirically by experimenting with the criteria presented in Section 2 of <ref> [8] </ref>. For our neural network structure their criteria will usually produce only one node at the "case layer" (layer 3 in Fig. 2). <p> Encoding the data using the tree Giving a usable criteria for calculating the length of the encoding of the data given a Bayesian tree is more difficult. In principle to represent the data as a binary string we can use standard coding techniques. Lam and Bacchus <ref> [8] </ref> suggest the use of Huffmann coding for the data encoding, although in fact the optimal coding would be achieved by arithmetic coding [15]. However, since we are only interesting in the length of the resulting code string, the actual choice of the encoding scheme is not of much importance.
Reference: [9] <author> P. Myllymaki and H. Tirri, </author> <title> Bayesian Case-Based Reasoning with Neural Networks. Pp. </title> <booktitle> 422-427 in Proc. of the IEEE International Conf. on Neural Networks, </booktitle> <address> San Francisco, </address> <publisher> March 1993 (IEEE Press). </publisher>
Reference-contexts: This simple structure allows us to adopt the probability propagation algorithm developed by Pearl [13] and implement it as a probabilistic feedforward network presented in Fig. 2. The details of the propagation algorithm and the neural network structure are presented in <ref> [9] </ref>. However, to construct such networks one needs a learning algorithm to determine the arc weights, i.e., the conditional probabilities P (A i = a ij jC = c k ) and the structure of the network.
Reference: [10] <author> P. Myllymaki and H. Tirri, </author> <title> Massively Parallel Case-Based Reasoning with Probabilistic Similarity Metrics. </title> <note> Pp. 48-53 in M.M. </note> <editor> Richter, S. Wess, K.-D. Althoff and F. Maurer (eds.), </editor> <booktitle> Proceedings of EWCBR-93, the First European Workshop on Case-Based Reasoning, </booktitle> <institution> University of Kaiserslautern, </institution> <month> 1-5 November, </month> <year> 1993. </year> <type> SEKI Report SR-93-12 (SFB 314), </type> <institution> University of Kaiserslautern. </institution>
Reference: [11] <author> R. </author> <title> Neapolitan, Probabilistic Reasoning in Expert Systems. </title> <publisher> Wiley Interscience, </publisher> <year> 1990. </year>
Reference: [12] <author> E. Parzen, </author> <title> On estimation of a probability distribution and mode. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 33, </volume> <pages> pp. 1065-1076, </pages> <year> 1962. </year>
Reference: [13] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In other words, the Bayesian network corresponding to this representation is a tree, where a 1494 single variable C is the root of the tree, and variables A i form the leaves (see Fig. 1). This simple structure allows us to adopt the probability propagation algorithm developed by Pearl <ref> [13] </ref> and implement it as a probabilistic feedforward network presented in Fig. 2. The details of the propagation algorithm and the neural network structure are presented in [9].
Reference: [14] <author> J. Pearl and T. Verma, </author> <title> A theory of inferred causation. </title> <note> Pp. 441-452 in J.A. </note> <editor> Allen, R. Fikes and E. Sandevall (eds.), </editor> <booktitle> Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <publisher> April 1991 (Morgan Kaufmann). </publisher>
Reference-contexts: Hence we will now illustrate how such Bayesian tree structures can be learned, the resulting tree can then be trivially transformed into the respective neural network. Work on learning Bayesian networks has recently increased as the networks have become more widely applied. Of the research related to ours <ref> [2, 3, 14] </ref> the closest to the ideas applied here is that of Lam and Bacchus [8]. They suggest an MDL learning scheme for general, multiply connected Bayesian networks. However, their encoding scheme is biased towards sparse networks with few values.
Reference: [15] <author> J. Rissanen, </author> <title> Generalized Kraft-Inequality and Arithmetic Coding. </title> <journal> IBM J. Res. Devel., </journal> <volume> vol. 20, </volume> <pages> pp. 198-203, </pages> <year> 1976. </year>
Reference-contexts: In principle to represent the data as a binary string we can use standard coding techniques. Lam and Bacchus [8] suggest the use of Huffmann coding for the data encoding, although in fact the optimal coding would be achieved by arithmetic coding <ref> [15] </ref>. However, since we are only interesting in the length of the resulting code string, the actual choice of the encoding scheme is not of much importance.
Reference: [16] <author> J. Rissanen, </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: MDL criteria for learning In an abstract setting, learning using MDL principle is based on the idea that the best network representing the data set is the network that minimizes the sum consisting the length of encoding of the network and the length of encoding the data given the network <ref> [16] </ref>. Thus applicability of the MDL principle depends on the proper encoding of both of these components. Finding such encodings for feed-forward neural network models has proven to be a non-trivial task (see e.g., discussions in [18]). <p> Since we are encoding the data using a Bayesian tree, the probability estimates q i are produced by the particular tree under consideration. It is well known that the shortest code length will be achieved, when the probability estimates q i are the true probabilities p i (see e.g., <ref> [16] </ref>). Therefore a good approximation for the length of the data coded with the Bayesian tree will be N i where the sum is over all the possible data vectors.
Reference: [17] <author> W.R. Robinson, </author> <title> Counting unlabeled acyclic digraphs. In C.H.C. Little (ed.), </title> <booktitle> Lecture notes in mathematics 622: Combinatorial mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: Although in the general case the number of possible Bayesian network structures grows super-exponentially with the number of nodes <ref> [17] </ref>, in our tree structure domain it is reasonable to assume that the number of cases does not exceed the size of the data set, so one would expect to evaluate only N different tree structures.
Reference: [18] <author> P. Smyth, </author> <title> On Stochastic Complexity and Admissible Models for Neural Network Classifiers. </title> <note> Pp. 818-824 in Lippmann, </note> <editor> Moody, Touretzky (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Thus applicability of the MDL principle depends on the proper encoding of both of these components. Finding such encodings for feed-forward neural network models has proven to be a non-trivial task (see e.g., discussions in <ref> [18] </ref>). Since in our case we have a tight correspondence between the Bayesian tree and the probabilistic network, we are able to make use of the simpler structure and develop the MDL criteria at the Bayesian tree structure level.
Reference: [19] <author> D.F. Specht, </author> <title> Probabilistic Neural Networks. </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. 109-118, </pages> <year> 1990. </year>
Reference: [20] <author> D.F. Specht, </author> <title> A General Regression Neural Network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, No. 6, </volume> <pages> pp. 568-576, </pages> <month> November </month> <year> 1991. </year> <month> 1497 </month>
References-found: 20

