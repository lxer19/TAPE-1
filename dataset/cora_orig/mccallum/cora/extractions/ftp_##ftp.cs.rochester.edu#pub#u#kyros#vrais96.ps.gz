URL: ftp://ftp.cs.rochester.edu/pub/u/kyros/vrais96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/kyros/publicat.htm
Root-URL: 
Email: kyros@cs.rochester.edu  vallino@cs.rochester.edu  
Title: Affine Object Representations for Calibration-Free Augmented Reality  
Author: Kiriakos N. Kutulakos James Vallino 
Address: Rochester, NY 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Note: To appear in: Proc. 1996 IEEE Virtual Reality Ann. Int. Symp. (VRAIS'96)  
Abstract: We describe the design and implementation of a video-based augmented reality system capable of overlaying three-dimensional graphical objects on live video of dynamic environments. The key feature of the system is that it is completely uncalibrated: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four feature points that are specified by the user at system initialization time and whose world coordinates are unknown. Our approach is based on the following observation: Given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by (1) tracking lines and fiducial points at frame rate, and (2) representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Azarbayejani, T. Starner, B. Horowitz, and A. Pentland. </author> <title> Visually controlled graphics. </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> 15(6) </volume> <pages> 602-605, </pages> <year> 1993. </year>
Reference-contexts: Affine Basis Tracking The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [1, 4, 22, 28] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [7-9]).
Reference: [2] <author> R. Azuma and G. Bishop. </author> <title> Improving static and dynamic registration in an optical see-through HMD. </title> <booktitle> In Proc. SIG-GRAPH'94, </booktitle> <pages> pages 197-204, </pages> <year> 1994. </year>
Reference-contexts: Furthermore, the elements of this matrix are simply the image x- and y-coordinates of feature points. This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter <ref> [2, 5, 32] </ref> to both track the feature points and compute the matrix. Second, the use of affine models leads to a simple through-the-lens method [16] for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects.
Reference: [3] <author> M. Bajura, H. Fuchs, and R. Ohbuchi. </author> <title> Merging virtual objects with the real world: Seeing ultrasound imagery within the patient. </title> <booktitle> In Proc. SIGGRAPH'92, </booktitle> <pages> pages 203-210, </pages> <year> 1992. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning <ref> [3, 17, 28] </ref> as well as developing three-dimensional user interfaces [14].
Reference: [4] <author> M. Bajura and U. Neumann. </author> <title> Dynamic registration correction in video-based augmented reality systems. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 15(5) </volume> <pages> 52-60, </pages> <year> 1995. </year>
Reference-contexts: These approaches track a small number of fiducial points in the user's 3D environment to either weaken the system's calibration requirements [22, 28] or to compensate for calibration errors in the system <ref> [4] </ref>. This paper describes the design and implementation of a novel video-based augmented reality system. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [4, 22, 28, 31] </ref>. To our knowledge, only two systems have been reported [22, 28] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. <p> Affine Basis Tracking The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [1, 4, 22, 28] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [7-9]). <p> The objects were then rotated together through the sequence of views in black squares. The accuracy of the image overlays is limited by radial distortions of the camera <ref> [4] </ref> and the affine approximation to perspective projection. Radial distortions are not currently taken into account. In order to assess the limitations resulting from the affine approximation to perspective we computed mis-registration errors as follows.
Reference: [5] <author> Y. Bar-Shalom and T. E. Fortmann. </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Furthermore, the elements of this matrix are simply the image x- and y-coordinates of feature points. This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter <ref> [2, 5, 32] </ref> to both track the feature points and compute the matrix. Second, the use of affine models leads to a simple through-the-lens method [16] for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects. <p> We use two independent constant acceleration Kalman filters <ref> [5] </ref> whose states consist of the first and second rows of the projection matrix 3fi4 , respectively, as well as their time derivatives. The filter's measurement equation is given by Eq. (9). Feature points and their affine coordinates are determined at system initialization time. <p> Upon initialization of the affine basis, the linear features defining the basis are tracked automatically. Line tracking runs on a single processor at rates between 30Hz and 60Hz for approximately 12 lines and provides updated Kalman filter estimates for the elements of the projection matrix <ref> [5] </ref>. Conceptually, the tracking subsystem can be thought of as an affine camera position tracker that returns the current affine projection matrix asynchronously upon request. For each new video frame, the rows of the projection matrix are used to build the view transformation matrix.
Reference: [6] <author> E. B. Barrett, M. H. Brill, N. N. Haag, and P. M. Pay-ton. </author> <title> Invariant linear methods in photogrammetry and model-matching. </title> <booktitle> In Geometric Invariance in Computer Vision, </booktitle> <pages> pages 277-292. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [6, 26] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [7] <author> A. Blake and M. Isard. </author> <title> 3D position, attitude and shape input using video tracking of hands and lips. </title> <booktitle> In ACM SIG-GRAPH'94, </booktitle> <pages> pages 185-192, </pages> <year> 1994. </year>
Reference: [8] <author> A. Blake and A. Yuille, </author> <title> editors. Active Vision. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [9] <author> C. M. Brown and D. Terzopoulos, </author> <title> editors. Real-Time Computer Vision. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference: [10] <author> R. Cipolla, P. A. Hadfield, and N. J. Hollinghurst. </author> <title> Uncalibrated stereo vision with pointing for a man-machine interface. </title> <booktitle> In Proc. IAPR Workshop on Machine Vision Applications, </booktitle> <year> 1994. </year>
Reference-contexts: This solves the placement problem for virtual objects. The entire process is shown in Figure 6. Affine object representations lead naturally to a through-the-lens method for constraining further the user degrees of freedom during the interactive placement of virtual objects. Such constraints have been used in vision-based pointing interfaces <ref> [10] </ref>.

Reference: [12] <author> O. D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Proc. </title> <booktitle> 2nd European Conf. on Computer Vision, </booktitle> <pages> pages 563-578, </pages> <year> 1992. </year>
Reference-contexts: The analysis presented in this paper can be directly generalized to account for the perspective projection model. In particular, Eq. (2) still holds when the object is represented in a projective frame of reference defined by 5 fiducial points <ref> [12] </ref>. The use of projective-invariant representations for re-projecting virtual objects under perspective projection is currently under investigation.
Reference: [13] <author> O. D. Faugeras. </author> <title> Three-Dimensional Computer Vision: A Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Interactive Object Placement Before virtual objects can be overlaid with images of a three-dimensional environment, the geometrical relationship between these objects and the environment must be established. Our approach for placing virtual objects in the 3D environment borrows from a few simple results in stereo vision <ref> [13] </ref>: given a point in space, its 3D location is uniquely determined by the point's projection in two images taken at different positions of the camera (Figure 5 (a)).
Reference: [14] <author> S. Feiner, B. MacIntyre, and D. Soligmann. </author> <title> Knowledge-based augmented reality. </title> <journal> Comm. of the ACM, </journal> <volume> 36(7) </volume> <pages> 53-62, </pages> <year> 1993. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning [3, 17, 28] as well as developing three-dimensional user interfaces <ref> [14] </ref>.
Reference: [15] <author> J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes. </author> <title> Computer Graphics Principles and Practice. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1990. </year>
Reference-contexts: First, we show that by representing virtual objects in an affine reference frame and by performing computer graphics operations such as projection and visible-surface determination directly on affine models, the entire video overlay process is described by a single 4fi4 homogeneous view transformation matrix <ref> [15] </ref>. Furthermore, the elements of this matrix are simply the image x- and y-coordinates of feature points. <p> Third, efficient execution of computer graphics operations on affine virtual objects is achieved by implementing affine projection computations directly on dedicated graphics hardware. 2. Geometrical Foundations Accurate projection of a virtual object requires knowing precisely the combined effect of the object-to-world, world-to-camera and camera-to-image transformations <ref> [15] </ref>. <p> One of the key aspects of affine object representations is that even though they are non-Euclidean, they nevertheless allow rendering operations such as z-buffering and clipping <ref> [15] </ref> to be performed accurately. This is because both depth order as well as the intersection of lines and planes is preserved under affine transformations. More specifically, z-buffering relies on the ability to order in depth two object points that project to the same pixel in the image.
Reference: [16] <author> M. Gleicher and A. Witkin. </author> <title> Through-the-lens camera control. </title> <booktitle> In Proc. SIGGRAPH'92, </booktitle> <pages> pages 331-340, </pages> <year> 1992. </year>
Reference-contexts: Second, the use of affine models leads to a simple through-the-lens method <ref> [16] </ref> for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects. Third, efficient execution of computer graphics operations on affine virtual objects is achieved by implementing affine projection computations directly on dedicated graphics hardware. 2.
Reference: [17] <editor> W. Grimson et al. </editor> <title> An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization. </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 430-436, </pages> <year> 1994. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning <ref> [3, 17, 28] </ref> as well as developing three-dimensional user interfaces [14]. <p> In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display. Furthermore, 1 initialization of virtual objects requires additional calibra-tion stages <ref> [17, 22] </ref>, and camera calibration must be performed whenever its intrinsic parameters (e.g., focal length) change. Our approach is motivated by recent approaches to video-based augmented reality that reduce the effects of calibration errors through real-time processing of the live video images viewed by the user [31].
Reference: [18] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> Affine structure from motion. </title> <journal> J. Opt. Soc. Am., </journal> <volume> A(2):377-385, </volume> <year> 1991. </year>
Reference-contexts: The only requirement is the ability to track across frames at least four features (points or lines) that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is based on the following observation, pointed out by Koenderink and van Doorn <ref> [18] </ref> and Ullman and Basri [29]: Given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. <p> Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction <ref> [18, 30] </ref> and recognition [20]. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied. <p> We use the following two properties of affine point representations <ref> [18, 23, 30] </ref> (Figure 3): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: " p p # " b 1 u m b 3 u
Reference: [19] <author> K. N. Kutulakos and J. Vallino. </author> <title> Affine object representations for calibration-free augmented reality. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Eq. (5) tells us that these coordinates define a 4 fi 4 homogeneous transformation T that corresponds to a rigid rotation since the object itself was rotated rigidly. By repeating this process three times, the user provides enough information to span the entire space of rotations. See <ref> [19] </ref> for more details. 8 Experimental System We have implemented a prototype augmented reality system consisting of a Silicon Graphics RealityEngine2 that handles all graphics operations using the OpenGL and Open-Inventor graphics libraries, and a tracking subsystem implemented in C that runs on a SUN SPARCserver2000.
Reference: [20] <author> Y. Lamdan, J. T. Schwartz, and H. J. Wolfson. </author> <title> Object recognition by affine invariant matching. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 335-344, </pages> <year> 1988. </year>
Reference-contexts: Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction [18, 30] and recognition <ref> [20] </ref>. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied.
Reference: [21] <author> P. F. McLauchlan, I. D. Reid, and D. W. Murray. </author> <title> Recursive affine structure and motion from image sequences. </title> <booktitle> In Proc. 3rd European Conf. on Computer Vision, </booktitle> <pages> pages 217-224, </pages> <year> 1994. </year>
Reference-contexts: In addition, fast rotational motions of the camera make tracking particularly difficult due to large point displacements across frames. Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account feature occlusions and re-appearances <ref> [21] </ref> and by using fiducials that can be efficiently identified and accurately localized in each frame [22, 28].
Reference: [22] <author> J. Mellor. </author> <title> Enhanced reality visualization in a surgical environment. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display. Furthermore, 1 initialization of virtual objects requires additional calibra-tion stages <ref> [17, 22] </ref>, and camera calibration must be performed whenever its intrinsic parameters (e.g., focal length) change. Our approach is motivated by recent approaches to video-based augmented reality that reduce the effects of calibration errors through real-time processing of the live video images viewed by the user [31]. <p> These approaches track a small number of fiducial points in the user's 3D environment to either weaken the system's calibration requirements <ref> [22, 28] </ref> or to compensate for calibration errors in the system [4]. This paper describes the design and implementation of a novel video-based augmented reality system. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [4, 22, 28, 31] </ref>. To our knowledge, only two systems have been reported [22, 28] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream [4, 22, 28, 31]. To our knowledge, only two systems have been reported <ref> [22, 28] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. The system of Mellor [22] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> To our knowledge, only two systems have been reported [22, 28] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. The system of Mellor <ref> [22] </ref> is capable of overlaying 3D medical data over live video of patients in a surgical environment. The system tracks circular features in an known 3D configuration to invert the object-to-image transformation using a linear method. <p> Affine Basis Tracking The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [1, 4, 22, 28] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [7-9]). <p> Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account feature occlusions and re-appearances [21] and by using fiducials that can be efficiently identified and accurately localized in each frame <ref> [22, 28] </ref>. <p> On a practical level, we are considering techniques for correcting radial distortion and are planning to use specialized fiducial trackers <ref> [22] </ref> to improve tracking accuracy and versatility. Acknowledgements The authors would like to thank Chris Brown for many helpful discussions and for his constant encouragement and support throughout the course of this work. The financial support of the National Science Foundation under Grant No.
Reference: [23] <editor> J. L. Mundy and A. Zisserman, editors. </editor> <booktitle> Geometric Invari-ance in Computer Vision. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: We use the following two properties of affine point representations <ref> [18, 23, 30] </ref> (Figure 3): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: " p p # " b 1 u m b 3 u
Reference: [24] <author> S. M. Seitz and C. R. Dyer. </author> <title> Complete scene structure from four point correspondences. </title> <booktitle> In Proc. 5th Int. Conf. on Computer Vision, </booktitle> <pages> pages 330-337, </pages> <year> 1995. </year>
Reference-contexts: In particular, if L ; R are the projection matrices associated with the first and second image, respectively, and L ; R are the corresponding optical axes defined by Eq. (6), the epipolar line can be parameterized by the set <ref> [24] </ref> By taking the epipolar constraint into account, we can ensure that the points specified by the user provide a physically valid placement of the virtual object.
Reference: [25] <author> L. S. Shapiro, A. Zisserman, and M. Brady. </author> <title> 3D motion recovery via affine epipolar geometry. </title> <journal> Int. J. Computer Vision, </journal> <volume> 16(2) </volume> <pages> 147-182, </pages> <year> 1995. </year>
Reference-contexts: The basic principles behind these representations are briefly reviewed next. We will assume in the following that the camera-to-image transformation can modeled using the weak perspective projection model <ref> [25] </ref> (Figure 2). 1 1 This assumption is not crucial, however. The analysis presented in this paper can be directly generalized to account for the perspective projection model. <p> In general, the correspondence induced by q L and q R may not define a physical point in space. Once p's projection is specified in one image, its projection in the second image must lie on a line satisfying the epipolar constraint <ref> [25] </ref>. This line is computed automatically and is used to constrain the user's selection of q R in the second image.
Reference: [26] <author> A. Shashua. </author> <title> A geometric invariant for visual recognition and 3D reconstruction from two perspective/orthographic views. </title> <booktitle> In Proc. IEEE Workshop on Qualitative Vision, </booktitle> <pages> pages 107-117, </pages> <year> 1993. </year>
Reference-contexts: Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [6, 26] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [27] <author> M. Tuceyran et al. </author> <title> Calibration requirements and procedures for a monitor-based augmented reality system. </title> <journal> IEEE Trans. Visualization and Computer Graphics, </journal> <volume> 1(3) </volume> <pages> 255-273, </pages> <year> 1995. </year>
Reference-contexts: At the heart of these issues is the ability to describe the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference. Typical approaches rely on 3D position tracking devices [31] and precise calibration <ref> [27] </ref> to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly (Figure 1). In practice, camera calibration and position tracking are prone to errors which accumulate in the augmented display.
Reference: [28] <author> M. Uenohara and T. Kanade. </author> <title> Vision-based object registration for real-time image overlay. </title> <booktitle> In Proc. CVRMED'95, </booktitle> <pages> pages 14-22, </pages> <year> 1995. </year>
Reference-contexts: Applications of this powerful visualization tool include overlaying clinical 3D data with live video of patients during surgical planning <ref> [3, 17, 28] </ref> as well as developing three-dimensional user interfaces [14]. <p> These approaches track a small number of fiducial points in the user's 3D environment to either weaken the system's calibration requirements <ref> [22, 28] </ref> or to compensate for calibration errors in the system [4]. This paper describes the design and implementation of a novel video-based augmented reality system. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream <ref> [4, 22, 28, 31] </ref>. To our knowledge, only two systems have been reported [22, 28] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. <p> Very little work has been published on augmented reality systems that reduce the effects of calibration errors through real-time processing of the live video stream [4, 22, 28, 31]. To our knowledge, only two systems have been reported <ref> [22, 28] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed [11] or perfectly calibrated. The system of Mellor [22] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> The most closely related work to our own is the work of Uenohara and Kanade <ref> [28] </ref>. Their system allows overlay of planar diagrams onto live video by tracking feature points in an unknown configuration that lie on the same plane as the diagram. Calibration is avoided by expressing diagram points as linear combinations of the feature points. <p> Affine Basis Tracking The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [1, 4, 22, 28] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [7-9]). <p> Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account feature occlusions and re-appearances [21] and by using fiducials that can be efficiently identified and accurately localized in each frame <ref> [22, 28] </ref>.
Reference: [29] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Trans. on Pattern Anal. and Mach. Intell., </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: Our approach is based on the following observation, pointed out by Koenderink and van Doorn [18] and Ullman and Basri <ref> [29] </ref>: Given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points.
Reference: [30] <author> D. Weinshall and C. Tomasi. </author> <title> Linear and incremental acquisition of invariant shape models from image sequences. </title> <booktitle> In Proc. 4th Int. Conf. on Computer Vision, </booktitle> <pages> pages 675-682, </pages> <year> 1993. </year>
Reference-contexts: Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction <ref> [18, 30] </ref> and recognition [20]. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied. <p> We use the following two properties of affine point representations <ref> [18, 23, 30] </ref> (Figure 3): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: " p p # " b 1 u m b 3 u

Reference: [32] <author> J.-R. Wu and M. Ouhyoung. </author> <title> A 3D tracking experiment on latency and its compensation methods in virtual environments. </title> <booktitle> In Proc. 8th ACM Symp. on User Interface Software and Technology, </booktitle> <pages> pages 41-49, </pages> <year> 1995. </year> <month> 12 </month>
Reference-contexts: Furthermore, the elements of this matrix are simply the image x- and y-coordinates of feature points. This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter <ref> [2, 5, 32] </ref> to both track the feature points and compute the matrix. Second, the use of affine models leads to a simple through-the-lens method [16] for interactively placing virtual objects within the user's 3D environment and for animating them relative to other physical or virtual objects.
References-found: 30

