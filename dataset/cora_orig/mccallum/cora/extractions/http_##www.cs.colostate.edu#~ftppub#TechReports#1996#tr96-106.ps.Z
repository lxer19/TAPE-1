URL: http://www.cs.colostate.edu/~ftppub/TechReports/1996/tr96-106.ps.Z
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Author: J. Ross Beveridge, Bruce A. Draper and Kris Siejko 
Keyword: Progress on Target and Terrain Recognition  
Note: This work was sponsored by the Advanced Research Projects Agency (ARPA) under grants DAAH04-93-G-422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office.  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Date: December 16, 1995  
Affiliation: Research at Colorado State University.  Computer Science Department Colorado State University  
Pubnum: Technical Report CS-96-106  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [Ant96a] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coreg-istering 3D Models, Range, and Optical Imagery Using Least-Median Squares Fitting. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A recent extension of this work to perform median filtering has dramatically improved the quality of the results <ref> [Ant96b, Ant96a] </ref>. 2 Coregistration Space Matching A match quality measure has been formalized to evaluate alternative coregistration estimates based upon fidelity of the target model to the sensor data. <p> The complete system has now been demonstrated on optical and range imagery collected at Fort Carson using high quality range and optical target model features generated using the system described above. Results of these improvements are reported in more detail elsewhere in these proceedings <ref> [Ant96a] </ref>. Here, allow us to summarize briefly what coregistration does, and show one result using the new median filtering capability. 4.5.1 Review and Median Filtering Example Coregistration addresses a fundamental problem arising when optical and range imagery is collected from separate co-located sensors.
Reference: [Ant96b] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coreg-istration of Range and Optical Images Using Coplanarity and Orientation Constraints. </title> <booktitle> In 1996 Conference on Computer Vision and Patter Recognition, </booktitle> <pages> page (submitted), </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: A recent extension of this work to perform median filtering has dramatically improved the quality of the results <ref> [Ant96b, Ant96a] </ref>. 2 Coregistration Space Matching A match quality measure has been formalized to evaluate alternative coregistration estimates based upon fidelity of the target model to the sensor data. <p> The motivation for this work and full mathematical development has been presented previously [SB94, BHP95]. Recently, median filtering has been included in the algorithm. In addition, a match error for ranking alternative correspondence mappings <ref> [Ant96b] </ref> has been developed. The complete system has now been demonstrated on optical and range imagery collected at Fort Carson using high quality range and optical target model features generated using the system described above. Results of these improvements are reported in more detail elsewhere in these proceedings [Ant96a].
Reference: [BDHR94] <author> Shashi Buluswar, Bruce A. Draper, Allen Hanson, and Ed-ward Riseman. </author> <title> Non-parametric Classification of Pixels Under Varying Outdoor Illumination. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1619-1626, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Color detection is relatively mature and has been integrated and demonstrated running on the Unmanned Ground Vehicle. The color detection effort is led by the University of Massachusetts. The general approach to target detection as laid out in <ref> [BDHR94] </ref> is based upon more general work on the use of learned multi variate decision trees in computer vision [DBU94]. <p> and the progress in each area is summarized in the following sections. 4 Progress on Key Components 4.1 Color Detection The essential elements of this work along with results on data collected by ourselves and Martin Marietta at Fort Carson [BPY94a] were reported in the previous Image Understanding Workshop Proceedings <ref> [BDHR94] </ref>. Since this initial description of the color detection work, the following has been accomplished: 1. An improved way of coalescing individual pixel de tections into ROIs has been implemented. 2. <p> The current system, using a single LUT, has been demonstrated to generalize across times of day, lighting conditions, weather and vehicles. Results demonstrating this on color imagery obtained from 35mm film have been previously reported <ref> [BDHR94, BHP95] </ref>, and more recent results obtained using the color CCD sensor on the UGV are summarized below. The current system does not generalize across sensors, but instead is trained to the specific response of a particular sensor.
Reference: [Bev93] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In principle, search in this combinatorial space of correspondence mappings could be conducted using local search in a manner analogous to that set for in Bev-eridge's dissertation <ref> [Bev93] </ref>. However, as a practical matter, the fine grained sampling of range points causes the combinatorics to explode. <p> A proof-of-concept horizon line matching experiment has been conducted using imagery collected by Lockheed-Martin at the UGV Demo C test site. The geometric matching system originally developed as part of Beveridge's thesis work is being used <ref> [Bev93] </ref> in this test and results are presented elsewhere in these proceedings [J. 96]. The results prove the feasibility of using local search matching as a tool to automate the orientation correction process in domains where horizon struc ture is distinctive.
Reference: [BHP94] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Techsystems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Figure 2b shows the projection of a 3D silhouette onto the image plane using the sensor calibration parameters <ref> [BHP94] </ref>. The pose estimate comes from the hypothesis generation stage. For each projected silhouette feature, a search is initiated in the image for the locally best corresponding line segment.
Reference: [BHP95] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Model-based fusion of flir, </title> <editor> color and ladar. In Paul S. Schenker and Gerard T. McKee, editors, </editor> <booktitle> Proceedings: Sensor Fusion and Networked Robotics VIII, Proc. SPIE 2589, </booktitle> <pages> pages 2 - 11, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: The current system, using a single LUT, has been demonstrated to generalize across times of day, lighting conditions, weather and vehicles. Results demonstrating this on color imagery obtained from 35mm film have been previously reported <ref> [BDHR94, BHP95] </ref>, and more recent results obtained using the color CCD sensor on the UGV are summarized below. The current system does not generalize across sensors, but instead is trained to the specific response of a particular sensor. <p> Gradient Response d. Weight 4.5 Range, Optical and Target Coregistration Anthony Schwickerath has developed a new least-median-squares algorithm for determining the best coregistration estimate based upon a set of corresponding sensor and target features. The motivation for this work and full mathematical development has been presented previously <ref> [SB94, BHP95] </ref>. Recently, median filtering has been included in the algorithm. In addition, a match error for ranking alternative correspondence mappings [Ant96b] has been developed.
Reference: [BHR86] <author> J. B. Burns, A. R. Hanson, and E. M. Riseman. </author> <title> Extracting straight lines. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(4):425 - 456, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: The final result is a list of 3D edges representing the silhouette of the target model for a given viewing angle. 4.4 Model-Driven Image Feature Extraction Bottom-up line extraction, such as performed by the Burns algorithm <ref> [BHR86] </ref>, is unreliable in imagery such as that shown in Figure 2b. To overcome the difficulty inherent in this imagery, a more model-driven approach is required. To accomplish this, we combine two ideas from the literature: model-driven edge detection [FL87, FL88] and directionally tuned gradient filters [Can86].
Reference: [BJLP92] <author> James Bevington, Randy Johnston, Joel Lee, and Richard Peters. </author> <title> A modular target recognition algorithm for ladar. </title> <booktitle> In Proc of the 2nd Automatic Target Recognizer Systems and Technology Conference, </booktitle> <pages> pages 91 - 104, </pages> <address> Fort Belvoir, VA, </address> <month> mar </month> <year> 1992. </year>
Reference-contexts: To provide this capability, a LADAR boundary template probing algorithm is being utilized which is itself an accomplished ATR algorithm <ref> [BJLP92] </ref>. This algorithm does the best it can to reduce the possibilities and then its top hypotheses are passed onto the final coregistration verification stage.
Reference: [BPY94a] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA data collection final report. </title> <type> Technical Report CS-94-118, </type> <institution> Computer Science Dept., Colorado State University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: of the components outlined above is a focus of research and the progress in each area is summarized in the following sections. 4 Progress on Key Components 4.1 Color Detection The essential elements of this work along with results on data collected by ourselves and Martin Marietta at Fort Carson <ref> [BPY94a] </ref> were reported in the previous Image Understanding Workshop Proceedings [BDHR94]. Since this initial description of the color detection work, the following has been accomplished: 1. An improved way of coalescing individual pixel de tections into ROIs has been implemented. 2.
Reference: [BPY94b] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: They have also been making enhancements, including a new visualization tool developed by Jim Steinborn which enables us to better understand the geometric structure of the probe templates. Preliminary tests of system have been performed on LADAR images from the Fort Carson data set <ref> [BPY94b] </ref>. Templates for the M113 and M60 generated at 52 meters were selected for all the tests. Templates generated at closer and farther ranges were tried as well, but the results changed little.
Reference: [BR95] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. Computer Vision and Image Understanding, </title> <note> 61(3):351 - 364, 1995. (short version in IEEE Second CAD-Based Vision Workshop). </note>
Reference-contexts: Constraints derive from known sensor, target and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches which attempt to fuse data based upon low-level queues only [EG92]. The roots of our approach lie in past alignment based object recognition research <ref> [Low85, HU90, BR95] </ref> which has demonstrated the value of algorithms which precisely vary 3D object to sensor alignment as part recognition. While this paradigm is dominant in many domains, it is surprisingly absent from work on ATR.
Reference: [BSS96] <author> J. Ross Beveridge, Mark R. Stevens, and N. A. Schwick-erath. </author> <title> Toward target verification through 3-d model-based sensor fusion. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page (Submitted), </note> <year> 1996. </year>
Reference-contexts: A local search operator in the space of coregistration mappings is then used to find better matches between target models and sensor features. This work is further described below, in <ref> [BSS96] </ref> and elsewhere in these proceedings [Mar96a].
Reference: [Can86] <author> John Canny. </author> <title> A computational approach to edge detection. </title> <journal> PAMI, </journal> <volume> 8(6) </volume> <pages> 679-698, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: To overcome the difficulty inherent in this imagery, a more model-driven approach is required. To accomplish this, we combine two ideas from the literature: model-driven edge detection [FL87, FL88] and directionally tuned gradient filters <ref> [Can86] </ref>. The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge. A gradient mask tuned to the specific expected orientation of the segment is used. <p> A gradient mask tuned to the the particular expected orientation of each silhouette edge is created by rotating the first derivative of a Gaussian. There are many precedents for tuned edge masks including Canny <ref> [Can86] </ref> and Torres [TP86] and their use for bottom-up edge detection [Shu94, FA91]. An example of such a filter, displayed as an image, is shown in Figure 2a. The placement of the silhouette edge is locally perturbed so as to maximize a function of the underlying tuned gradient response.
Reference: [DBU94] <author> B. Draper, C. E. Brodley, and P. Utgoff. </author> <title> Goal-directed Classification Using Linear Machine Decision Trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <note> 16(9):(to appear), </note> <month> September </month> <year> 1994. </year> <month> 9 </month>
Reference-contexts: The color detection effort is led by the University of Massachusetts. The general approach to target detection as laid out in [BDHR94] is based upon more general work on the use of learned multi variate decision trees in computer vision <ref> [DBU94] </ref>. Hypothesis Generation Given regions of interest generated by the color detection process, or any other detection algorithm, this second stage hypothesizes what type or types of vehicles may be present and at what positions and orientations relative to the sensors.
Reference: [EG92] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gon-zalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Constraints derive from known sensor, target and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches which attempt to fuse data based upon low-level queues only <ref> [EG92] </ref>. The roots of our approach lie in past alignment based object recognition research [Low85, HU90, BR95] which has demonstrated the value of algorithms which precisely vary 3D object to sensor alignment as part recognition.
Reference: [FA91] <author> William T. Freeman and Edward H. Adelson. </author> <title> The Design and Use of Steerable Filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-13(9):891-906, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: A gradient mask tuned to the the particular expected orientation of each silhouette edge is created by rotating the first derivative of a Gaussian. There are many precedents for tuned edge masks including Canny [Can86] and Torres [TP86] and their use for bottom-up edge detection <ref> [Shu94, FA91] </ref>. An example of such a filter, displayed as an image, is shown in Figure 2a. The placement of the silhouette edge is locally perturbed so as to maximize a function of the underlying tuned gradient response.
Reference: [FL87] <author> Pascal Fua and Yvan G. Leclerc. </author> <title> Finding Object Boundaries Using Guided Gradient Ascent. </title> <booktitle> In Proceedings: Image Understanding Workshop - 1987, </booktitle> <pages> pages 888-891. </pages> <publisher> DARPA, Morgan Kaufmann, </publisher> <month> February </month> <year> 1987. </year>
Reference-contexts: To overcome the difficulty inherent in this imagery, a more model-driven approach is required. To accomplish this, we combine two ideas from the literature: model-driven edge detection <ref> [FL87, FL88] </ref> and directionally tuned gradient filters [Can86]. The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge. A gradient mask tuned to the specific expected orientation of the segment is used.
Reference: [FL88] <author> Pascal Fua and Yvan G. Leclerc. </author> <title> Model Driven Edge Detection. </title> <booktitle> In Proceedings: Image Understanding Workshop - 1988, </booktitle> <pages> pages 1016-1020. </pages> <publisher> DARPA, Morgan Kaufmann, </publisher> <month> April </month> <year> 1988. </year>
Reference-contexts: To overcome the difficulty inherent in this imagery, a more model-driven approach is required. To accomplish this, we combine two ideas from the literature: model-driven edge detection <ref> [FL87, FL88] </ref> and directionally tuned gradient filters [Can86]. The quality of a straight line segment denoting an extended edge is defined to be a function of the gradient magnitude under that edge. A gradient mask tuned to the specific expected orientation of the segment is used.
Reference: [HU90] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Recognizing Solid Objects by Alignment with an Image. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: Constraints derive from known sensor, target and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches which attempt to fuse data based upon low-level queues only [EG92]. The roots of our approach lie in past alignment based object recognition research <ref> [Low85, HU90, BR95] </ref> which has demonstrated the value of algorithms which precisely vary 3D object to sensor alignment as part recognition. While this paradigm is dominant in many domains, it is surprisingly absent from work on ATR.
Reference: [J. 96] <author> J. Ross Beveridge and Christopher Graves and Christopher E. Lesher. </author> <title> Local Serach as a Tool for Horizon Line Matching. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A proof-of-concept horizon line matching experiment has been conducted using imagery collected by Lockheed-Martin at the UGV Demo C test site. The geometric matching system originally developed as part of Beveridge's thesis work is being used [Bev93] in this test and results are presented elsewhere in these proceedings <ref> [J. 96] </ref>. The results prove the feasibility of using local search matching as a tool to automate the orientation correction process in domains where horizon struc ture is distinctive.
Reference: [KH94] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust methods for estimating pose and a sensitivity analysis. </title> <booktitle> CVGIP:Image Understanding, </booktitle> <volume> 11, </volume> <year> 1994. </year>
Reference-contexts: Model-Driven Image Feature Extraction In optical imagery, bottom-up feature extraction is problematic at best. Consequently, a model-driven edge detection and line extraction process has been implemented which seeks locally optimal silhouette features in the optical imagery. Range, Optical and Target Coregistration Extending past work on 3D pose determination <ref> [KH94] </ref>, a new least-squares algorithm has been developed which determines the 3D pose of the target relative to a range and an optical sensor and simultaneously adjusts the registration mapping between the sensor image planes [SB94].
Reference: [Low85] <author> David G. Lowe. </author> <title> Perceptual Organization and Visual Recognition. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1985. </year>
Reference-contexts: Constraints derive from known sensor, target and scene geometry. This may be thought of as model-based sensor fusion, and contrasts with more traditional approaches which attempt to fuse data based upon low-level queues only [EG92]. The roots of our approach lie in past alignment based object recognition research <ref> [Low85, HU90, BR95] </ref> which has demonstrated the value of algorithms which precisely vary 3D object to sensor alignment as part recognition. While this paradigm is dominant in many domains, it is surprisingly absent from work on ATR.
Reference: [Mar96a] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A local search operator in the space of coregistration mappings is then used to find better matches between target models and sensor features. This work is further described below, in [BSS96] and elsewhere in these proceedings <ref> [Mar96a] </ref>. <p> He has also developed a fully automated system for extracting edge and surface information from these polygonal models. This later system is summarized here and more fully described elsewhere in these proceedings <ref> [Mar96a] </ref>. For optical imagery, 3D face boundaries likely to generate observable edges in imagery are extracted from the 3D target models. This is done on-line given an estimate of the target pose and lighting. <p> These 3D features naturally accommodate modest changes in viewing angle. If the expected pose estimate changes significantly, new features may be generated. The first version of the feature prediction system extracts only features associated with the silhouette. More recent work, described in more detail elsewhere in these proceedings <ref> [Mar96a] </ref>, extends this method to add significant internal structure as well. The silhouette prediction algorithm begins by assigning a unique color to each face in the target model. This color acts as an index into a hash table of 3D faces. <p> This work is being pursued as an extension of Mark Stevens's work on model prediction and model-driven feature extraction. A more detailed account of this work appears elsewhere in these proceedings <ref> [Mar96a] </ref>. What is presented here is an overview with an example. Early experiments with search in coregistration-space suggests an ability to correct quite large errors in the initial coregistration parameters.
Reference: [Mar96b] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Optical Linear Feature Detection Based on Model Pose. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A weighting value for each pixel is created (see Figure 2d) and the response for the silhouette line is the weighted sum of responses at each pixel. Additional details on this work appear elsewhere in these proceedings <ref> [Mar96b] </ref>. 5 a. Mask b. Silhouette Line c. Gradient Response d. Weight 4.5 Range, Optical and Target Coregistration Anthony Schwickerath has developed a new least-median-squares algorithm for determining the best coregistration estimate based upon a set of corresponding sensor and target features.
Reference: [Pin88] <author> Juan Pineda. </author> <title> A Parallel Algorithm for Polygon Rasteriza-tion. </title> <booktitle> In Proceedings of Siggraph '88, </booktitle> <pages> pages 17-20, </pages> <year> 1988. </year>
Reference-contexts: The placement of the silhouette edge is locally perturbed so as to maximize a function of the underlying tuned gradient response. To do this with subpixel accuracy, a commonly used graphics anti-aliasing technique known as Pineda Arithmetic <ref> [Pin88] </ref> is used to weight the contribution of individual pixels. A weighting value for each pixel is created (see Figure 2d) and the response for the silhouette line is the weighted sum of responses at each pixel. Additional details on this work appear elsewhere in these proceedings [Mar96b]. 5 a.
Reference: [SB94] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Range, Optical and Target Coregistration Extending past work on 3D pose determination [KH94], a new least-squares algorithm has been developed which determines the 3D pose of the target relative to a range and an optical sensor and simultaneously adjusts the registration mapping between the sensor image planes <ref> [SB94] </ref>. A recent extension of this work to perform median filtering has dramatically improved the quality of the results [Ant96b, Ant96a]. 2 Coregistration Space Matching A match quality measure has been formalized to evaluate alternative coregistration estimates based upon fidelity of the target model to the sensor data. <p> Gradient Response d. Weight 4.5 Range, Optical and Target Coregistration Anthony Schwickerath has developed a new least-median-squares algorithm for determining the best coregistration estimate based upon a set of corresponding sensor and target features. The motivation for this work and full mathematical development has been presented previously <ref> [SB94, BHP95] </ref>. Recently, median filtering has been included in the algorithm. In addition, a match error for ranking alternative correspondence mappings [Ant96b] has been developed.
Reference: [SBG95] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In BRL/CAD Symposium 95, </booktitle> <year> 1995. </year>
Reference-contexts: These detailed models are a tremendous asset. However, much work is required to transform these CSG models into features appropriate for matching to sensor data. Over the past year, Mark Stevens has developed a semi-automated system for transforming the CSG to a polygonal representation <ref> [SBG95, Ste95] </ref>. He has also developed a fully automated system for extracting edge and surface information from these polygonal models. This later system is summarized here and more fully described elsewhere in these proceedings [Mar96a].
Reference: [Shu94] <author> Alexander Shustorovich. </author> <title> Scale Specific and Robust Edge/Line Encoding with Linear Combinations of Gabor Wavelets. </title> <journal> Pattern Recognition, </journal> <volume> 27(5) </volume> <pages> 713-725, </pages> <year> 1994. </year>
Reference-contexts: A gradient mask tuned to the the particular expected orientation of each silhouette edge is created by rotating the first derivative of a Gaussian. There are many precedents for tuned edge masks including Canny [Can86] and Torres [TP86] and their use for bottom-up edge detection <ref> [Shu94, FA91] </ref>. An example of such a filter, displayed as an image, is shown in Figure 2a. The placement of the silhouette edge is locally perturbed so as to maximize a function of the underlying tuned gradient response.
Reference: [Ste95] <author> Mark R. Stevens. </author> <title> Obtaining 3D Silhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State University, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: These detailed models are a tremendous asset. However, much work is required to transform these CSG models into features appropriate for matching to sensor data. Over the past year, Mark Stevens has developed a semi-automated system for transforming the CSG to a polygonal representation <ref> [SBG95, Ste95] </ref>. He has also developed a fully automated system for extracting edge and surface information from these polygonal models. This later system is summarized here and more fully described elsewhere in these proceedings [Mar96a].
Reference: [TP86] <author> Vincent Torre and Tomaso A. Poggio. </author> <title> On Edge Detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(2):147-164, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: A gradient mask tuned to the the particular expected orientation of each silhouette edge is created by rotating the first derivative of a Gaussian. There are many precedents for tuned edge masks including Canny [Can86] and Torres <ref> [TP86] </ref> and their use for bottom-up edge detection [Shu94, FA91]. An example of such a filter, displayed as an image, is shown in Figure 2a. The placement of the silhouette edge is locally perturbed so as to maximize a function of the underlying tuned gradient response.
Reference: [U. 91] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: doubt be improved through more careful tuning to the Fort Carson data, this experiment demonstrates the system is capable of focusing attention upon a small set of aspect and target hypotheses. 4.3 Model Feature Prediction Highly detailed Constructive Solid Geometry (CSG) models of target vehicles are available in BRL-CAD format <ref> [U. 91] </ref>. These detailed models are a tremendous asset. However, much work is required to transform these CSG models into features appropriate for matching to sensor data. Over the past year, Mark Stevens has developed a semi-automated system for transforming the CSG to a polygonal representation [SBG95, Ste95].
Reference: [X. 94] <author> X. Y. Jiang and H. Bunke. </author> <title> Fast Segmentation of Range Images into Planar Regions by Scan Line Grouping. </title> <journal> Maching Vision and Applications, </journal> <volume> 7(2):115 - 122, </volume> <year> 1994. </year> <month> 10 </month>
Reference-contexts: This is trivial in the case of the target models, where bounded faces are already represented. For the range data, we are experimenting with a scan-line range seg mentation technique <ref> [X. 94] </ref>. 7 a. Initial Orientation b. Initial Color Pose c. Initial LADAR Pose d. Resulting Orientation e. Resulting Color Pose f.
References-found: 32

