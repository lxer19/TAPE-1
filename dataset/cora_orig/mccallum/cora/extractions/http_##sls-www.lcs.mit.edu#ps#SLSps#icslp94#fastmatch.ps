URL: http://sls-www.lcs.mit.edu/ps/SLSps/icslp94/fastmatch.ps
Refering-URL: http://www.sls.lcs.mit.edu/~dg/publications.html
Root-URL: 
Title: LEXICAL TREE BASED PRUNING FOR SEGMENT-BASED SPEECH RECOGNITION 1  
Author: Mike Phillips and David Goddeau 
Address: Cambridge, Massachusetts 02139, U.S.A.  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: The goal of a fast match for continuous speech recognition is to efficiently reduce the set of possible words which may start at any point in time. In order to save computation, the fast match must use less computation than the complete search, and in order to not reduce accuracy, the fast match must not prune away words which would have been correctly recognized by the complete search. The segmental approach employed in the MIT Summit speech recognition system allows the use of a fairly simple and efficient fast match for large vocabulary continuous speech recognition. In the Summit system, a segmental network is computed before the lexical search is performed. This network represents possible phonetic interpretations of the speech signal, and contains a vector of probabilities for all possible phones for each segment. The fact that this network is precomputed allows the use of an efficient lookahead to determine the possible words which may begin at any node in the network. During training the phones are collapsed into a smaller set of phone classes and the pronunciations of the words in the lexicon are represented as a tree in terms of these phone classes. This tree is pruned to a depth of N for a lookahead of N phones. During recognition, to find the words which may begin at any node in the segment network, the lexical tree is matched against the phonetic network to find all nodes for which the phone class probabilities exceed some threshold. Since the phone class lexical tree has many less nodes than the total number of pronunciation arcs in the full lexicon, this search can proceed very quickly. This paper will present the algorithm in more detail, discuss methods to select the phone classes, and explore the tradeoffs between length of the lookahead, phone class pruning threshold, computational savings, and accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Zue, V., Glass, J., Goddeau, D., Goodine, D., Hirschman, L., Phillips, M., Polfroni, J., and Seneff, S. </author> <title> "The MIT Atis System: February 1992 Progress Report," </title> <booktitle> Proc. DARPA Speech and NL Workshop, </booktitle> <pages> 84-88, </pages> <address> Harriman, NY, </address> <month> February </month> <year> 1992. </year>
Reference: [2] <author> Zue, et al., </author> <title> "The MIT SUMMIT Speech Recognition System: A Progress Report", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop,179-189,Philadelphia, </booktitle> <month> February </month> <year> 1989. </year>
Reference-contexts: INTRODUCTION Summit is a segment-based, speaker-independent, continuous speech recognition system <ref> [2] </ref>.
Reference: [3] <editor> Zue, et al., "galaxy..., </editor> <booktitle> these Proceedings. </booktitle>
Reference-contexts: RECOGNITION TASK AND SPEECH CORPUS The experiments in the following sections were all performed using a version of summit with an approximately 4000 word recognition vocabulary, developed for the galaxy system <ref> [3] </ref>. In order to run the experiments efficiently, the system used for these experiments used only context-independent models and a bigram language model. The acoustic models used in the experiments were trained on xxx utterances from the atis [4] and voy-ager corpora.
Reference: [4] <author> MADCOW, </author> <title> "Multi-Site Data Collection for a Spoken Language Corpora", </title> <booktitle> Fifth DARPA Speech and Natural Language Workshop,February,1992. </booktitle>
Reference-contexts: In order to run the experiments efficiently, the system used for these experiments used only context-independent models and a bigram language model. The acoustic models used in the experiments were trained on xxx utterances from the atis <ref> [4] </ref> and voy-ager corpora. The bigram was trained from a collection of sentences from the atis and voyager corpora, plus a small set of sentences from other domains. The bigram and acoustic models are constant throughout the experiments.
Reference: [5] <author> Zue V., Daly, N., Glass, J., Goodine, D., Leung, H., Phillips, M., and Polifroni, J., Seneff, S., and Soclof, M., </author> <title> "The Collection and Preliminary Analysis of a Spontaneous Speech Database", </title> <booktitle> DARPA Speech and Natural Language Workshop, </booktitle> <address> 160-167,October,1989. </address>
References-found: 5

