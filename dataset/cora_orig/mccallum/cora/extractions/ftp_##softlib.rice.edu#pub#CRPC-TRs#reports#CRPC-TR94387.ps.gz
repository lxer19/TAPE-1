URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94387.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: GMRES ON (NEARLY) SINGULAR SYSTEMSfl  
Author: PETER N. BROWNy AND HOMER F. WALKERz 
Keyword: Key words. gmres method, residual minimizing methods, Krylov subspace methods, iterative linear algebra methods, singular or ill-conditioned linear systems  
Note: AMS(MOS) subject classifications. 65F10  
Abstract: We consider the behavior of the gmres method for solving a linear system Ax = b when A is singular or nearly so, i.e., ill-conditioned. The (near) singularity of A may or may not affect the performance of gmres, depending on the nature of the system and the initial approximate solution. For singular A, we give conditions under which the gmres iterates converge safely to a least-squares solution or to the pseudo-inverse solution. These results also apply to any residual minimizing Krylov subspace method that is mathematically equivalent to gmres. A practical procedure is outlined for efficiently and reliably detecting singularity or ill-conditioning when it becomes a threat to the performance of gmres. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammar-ling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <note> LAPACK User's Guide, </note> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: A well-developed Fortran implementation of ice is provided by auxiliary routine xlaic1 of Lapack <ref> [1] </ref>, where x = s for single precision or x = d for double precision. This implementation was used in all of the numerical experiments reported in x4. 4. Numerical experiments. In this section, we discuss several numerical experiments that illustrate the theoretical and practical points brought out above.
Reference: [2] <author> Z. Bai, D. Hu, and L. Reichel, </author> <title> A Newton basis GMRES implementation, </title> <type> Tech. Rep. 91-03, </type> <institution> Department of Mathematics, University of Kentucky, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, the standard implementation of [16] and Householder variants in [18] determine ideally conditioned B k such that B T k B k = I k (in exact arithmetic). Other implementations in <ref> [2] </ref> and [19] generate B k that are usually well-conditioned, if not ideally conditioned.
Reference: [3] <author> V. A. Barker, </author> <title> Numerical solution of sparse singular systems of equations arising from ergodic Markov chain modeling, </title> <journal> Comm. Statist. Stochastic Models, </journal> <volume> 5 (1989), </volume> <pages> pp. 335-381. </pages>
Reference-contexts: In x4, we discuss several illustrative numerical experiments. Others have considered gmres and related methods on singular or ill-conditioned systems. It is noted in <ref> [3] </ref> and [15] that gmres can be used to solve singular homogeneous systems that arise in Markov chain modeling.
Reference: [4] <author> C. H. Bischof, </author> <title> Incremental condition estimation, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <pages> 11 (312-322), </pages> <address> p. </address> <year> 1990. </year>
Reference-contexts: However, if r 0 = e n , then the solution is reached at the first step with 2 (R 1 ) = n = n = 1. A very efficient means of monitoring the conditioning of R k is provided by incremental condition estimation (ice) <ref> [4] </ref>, [5]. This determines estimates of the largest and smallest singular values of each R k in O (k) arithmetic operations, given estimates of the largest and smallest singular values of R k1 . <p> The condition numbers of these factors can be estimated very efficiently using incremental condition estimation (ice) <ref> [4] </ref>, [5]. Acknowledgment. The authors are grateful to the referees and the editor for their helpful comments and suggestions, which significantly improved the paper.
Reference: [5] <author> C. H. Bischof and P. T. P. Tang, </author> <title> Robust incremental condition estimation, </title> <type> Tech. Rep. </type> <note> CS-91-133, LAPACK Working Note 33, </note> <institution> Computer Science Department, University of Tennessee, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: However, if r 0 = e n , then the solution is reached at the first step with 2 (R 1 ) = n = n = 1. A very efficient means of monitoring the conditioning of R k is provided by incremental condition estimation (ice) [4], <ref> [5] </ref>. This determines estimates of the largest and smallest singular values of each R k in O (k) arithmetic operations, given estimates of the largest and smallest singular values of R k1 . <p> The condition numbers of these factors can be estimated very efficiently using incremental condition estimation (ice) [4], <ref> [5] </ref>. Acknowledgment. The authors are grateful to the referees and the editor for their helpful comments and suggestions, which significantly improved the paper.
Reference: [6] <author> P. N. Brown, </author> <title> A theoretical comparison of the Arnoldi and GMRES algorithms, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <pages> 12 (58-78), </pages> <address> p. </address> <year> 1991. </year>
Reference-contexts: which occurs when dim A (K k ) &lt; dim K k , and degeneracy of K k , which occurs when dim K k &lt; k. (The latter kind of breakdown is sometimes referred to as "lucky" or "happy" breakdown, especially in the context of the Lanczos algorithm, cf. <ref> [6] </ref> and [16].) The definition is intended to focus on essential breakdown of the method, as opposed to breakdown associated with any particular implementation or ancillary algorithm used in it. <p> All computing was done in double precision Fortran on Sun Microsytems Sparc architectures. Experiment 4.1. This experiment, which involves a contrived problem, points up the danger of not monitoring the conditioning of AB k and terminating when excessive ill-conditioning appears. The matrix A is from the example in <ref> [6, x6] </ref>, A = B B 0 1 . . . . . . 1 0 C C : We assume that n is odd, in which case A is singular with N (A) = spanf (1; 0; 1; 0; ; 0; 1) T g: Since A is skew symmetric, the
Reference: [7] <author> R. W. Freund, </author> <title> A transpose-free quasi-minimal residual algorithm for non-Hermitian linear systems, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 425-448. </pages>
Reference-contexts: In [9], conditions are given for the convergence of general Krylov subspace methods on singular systems, and particular results are derived for the qmr [10] and tfqmr <ref> [7] </ref> methods (see x2 below), with applications to Markov chain modeling. Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in [12]; see also [13] and the references in [12] and [13] for more on deflation techniques for nearly singular systems.
Reference: [8] <author> R. W. Freund, G. H. Golub, and N. M. Nachtigal, </author> <title> Iterative solution of linear systems, </title> <booktitle> Acta Numerica 1992, 1 (1992), </booktitle> <pages> pp. 57-100. </pages>
Reference-contexts: The results in x2 apply not only to gmres but also to any mathematically equivalent method, i.e., any method that takes steps characterized by the residual minimizing property (1.2). (See <ref> [8, x2.4] </ref> for a discussion of mathematically equivalent methods.) Thus in x2, one can think of gmres as a 1 An x 2 IR n for which kb Axk 2 is minimal. 2 The least-squares solution x such that kxk 2 is minimal. 2 generic minimal residual method that characterizes corrections
Reference: [9] <author> R. W. Freund and M. Hochbruck, </author> <title> On the use of two QMR algorithms for solving singular systems and applications in Markov chain modeling, </title> <journal> J. Numer. Lin. Alg. Appl., </journal> <volume> 1 (1994), </volume> <pages> pp. 403-420. </pages>
Reference-contexts: In x4, we discuss several illustrative numerical experiments. Others have considered gmres and related methods on singular or ill-conditioned systems. It is noted in [3] and [15] that gmres can be used to solve singular homogeneous systems that arise in Markov chain modeling. In <ref> [9] </ref>, conditions are given for the convergence of general Krylov subspace methods on singular systems, and particular results are derived for the qmr [10] and tfqmr [7] methods (see x2 below), with applications to Markov chain modeling. <p> Then there cannot be breakdown through rank deficiency of the least-squares problem (1.2), and the theorem follows from Theorem 2.2. 7 Conditions that are essentially equivalent to those in Theorem 2.6 appear in <ref> [9] </ref>. The index of A, denoted index (A), is defined to be the smallest integer q such that A q and A q+1 have the same rank. <p> It is easily seen that index (A) = 1 if and only if A is singular and N (A) " R (A) = f0g. For a consistent system (1.1) with index (A) = 1, general conditions are given in <ref> [9] </ref> under which a Krylov subspace method is convergent. It is further shown in [9] that the qmr and tfqmr methods are convergent for such a system. If N (A) " R (A) = f0g and (1.1) is consistent, then 2 (A k ) satisfies (2.3). <p> For a consistent system (1.1) with index (A) = 1, general conditions are given in <ref> [9] </ref> under which a Krylov subspace method is convergent. It is further shown in [9] that the qmr and tfqmr methods are convergent for such a system. If N (A) " R (A) = f0g and (1.1) is consistent, then 2 (A k ) satisfies (2.3).
Reference: [10] <author> R. W. Freund and N. M. Nachtigal, </author> <title> QMR: a quasi-minimal residual method for non-Hermitian linear systems, </title> <journal> Numer. Math., </journal> <volume> 60 (1991), </volume> <pages> pp. 315-339. </pages>
Reference-contexts: It is noted in [3] and [15] that gmres can be used to solve singular homogeneous systems that arise in Markov chain modeling. In [9], conditions are given for the convergence of general Krylov subspace methods on singular systems, and particular results are derived for the qmr <ref> [10] </ref> and tfqmr [7] methods (see x2 below), with applications to Markov chain modeling. Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in [12]; see also [13] and the references in [12] and [13] for more on deflation techniques for nearly singular systems.
Reference: [11] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <note> second ed., </note> <year> 1989. </year>
Reference-contexts: A rigorous worst-case bound on k^r k r k k 2 =kr 0 k 2 would require u 2 (A k ) multiplied by a polynomial of low degree in n and k (see <ref> [11, Ch. 5] </ref>), but this is not necessary here.
Reference: [12] <author> J. C. Meza, </author> <title> A modification to the GMRES method for ill-conditioned linear systems, </title> <type> Tech. Rep. </type> <institution> SAND95-8220, Sandia National Laboratories, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in <ref> [12] </ref>; see also [13] and the references in [12] and [13] for more on deflation techniques for nearly singular systems. In [14], extensions of gmres are considered in which Krylov subspaces are augmented with approximate eigenvectors generated during previous iterations. <p> Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in <ref> [12] </ref>; see also [13] and the references in [12] and [13] for more on deflation techniques for nearly singular systems. In [14], extensions of gmres are considered in which Krylov subspaces are augmented with approximate eigenvectors generated during previous iterations. These extensions appear to be most effective when there are a few relatively small eigenvalues.
Reference: [13] <author> J. C. Meza and W. W. Symes, </author> <title> Deflated Krylov subspace methods for nearly singular linear systems, </title> <journal> J. Optimization Theory Appl., </journal> <volume> 72 (1992), </volume> <pages> pp. 441-457. </pages>
Reference-contexts: Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in [12]; see also <ref> [13] </ref> and the references in [12] and [13] for more on deflation techniques for nearly singular systems. In [14], extensions of gmres are considered in which Krylov subspaces are augmented with approximate eigenvectors generated during previous iterations. <p> Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in [12]; see also <ref> [13] </ref> and the references in [12] and [13] for more on deflation techniques for nearly singular systems. In [14], extensions of gmres are considered in which Krylov subspaces are augmented with approximate eigenvectors generated during previous iterations. These extensions appear to be most effective when there are a few relatively small eigenvalues.
Reference: [14] <author> R. B. Morgan, </author> <title> A restarted GMRES method augmented with eigenvectors, </title> <note> SIAM J. Matrix Anal. Appl., 16 (1995). To appear. </note>
Reference-contexts: Deflation-like modifications of gmres based on truncated singular value decomposition solutions have recently been considered in [12]; see also [13] and the references in [12] and [13] for more on deflation techniques for nearly singular systems. In <ref> [14] </ref>, extensions of gmres are considered in which Krylov subspaces are augmented with approximate eigenvectors generated during previous iterations. These extensions appear to be most effective when there are a few relatively small eigenvalues.
Reference: [15] <author> B. Philippe, Y. Saad, and W. J. Stewart, </author> <title> Numerical methods in Markov chain modeling, </title> <journal> Oper. Res., </journal> <volume> 40 (1992), </volume> <pages> pp. 1156-1179. </pages>
Reference-contexts: In x4, we discuss several illustrative numerical experiments. Others have considered gmres and related methods on singular or ill-conditioned systems. It is noted in [3] and <ref> [15] </ref> that gmres can be used to solve singular homogeneous systems that arise in Markov chain modeling.
Reference: [16] <author> Y. Saad and M. H. Schultz, </author> <title> GMRES: a generalized minimal residual method for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 856-869. </pages>
Reference-contexts: 1. Introduction. The generalized minimal residual (gmres) method of Saad and Schultz <ref> [16] </ref> is widely used for solving a general linear system Ax = b; A 2 IR nfin ;(1.1) and its behavior is well-understood when A is nonsingular. <p> when dim A (K k ) &lt; dim K k , and degeneracy of K k , which occurs when dim K k &lt; k. (The latter kind of breakdown is sometimes referred to as "lucky" or "happy" breakdown, especially in the context of the Lanczos algorithm, cf. [6] and <ref> [16] </ref>.) The definition is intended to focus on essential breakdown of the method, as opposed to breakdown associated with any particular implementation or ancillary algorithm used in it. <p> For perspective, we recall that Proposition 2, p. 865, of <ref> [16] </ref> ensures that, if A is nonsingular, then gmres does not break down until the solution of (1.1) has been found. Breakdown in [16, Prop. 2, p. 865] is associated specifically with breakdown of the Arnoldi process used in the gmres implementation in [16], but the statement remains true with our <p> For perspective, we recall that Proposition 2, p. 865, of [16] ensures that, if A is nonsingular, then gmres does not break down until the solution of (1.1) has been found. Breakdown in <ref> [16, Prop. 2, p. 865] </ref> is associated specifically with breakdown of the Arnoldi process used in the gmres implementation in [16], but the statement remains true with our definition (see x2 below). In contrast to the nonsingular case, anything may happen when A is singular. <p> recall that Proposition 2, p. 865, of <ref> [16] </ref> ensures that, if A is nonsingular, then gmres does not break down until the solution of (1.1) has been found. Breakdown in [16, Prop. 2, p. 865] is associated specifically with breakdown of the Arnoldi process used in the gmres implementation in [16], but the statement remains true with our definition (see x2 below). In contrast to the nonsingular case, anything may happen when A is singular. <p> Conversely, breakdown through degeneracy of the Krylov subspace occurs if and only if (1.1) is consistent and the solution has been found. Also, these results imply the result in <ref> [16, Prop. 2, p. 865] </ref> cited earlier: If A is nonsingular, then gmres does not break down until the solution of (1.1) has been found. Indeed, if A is nonsingular, then gmres cannot break down through rank deficiency of (1.2), and the second alternative must hold. <p> Sound gmres implementations are designed so that, as much as possible, each B k is well-conditioned regardless of the conditioning of A. For example, the standard implementation of <ref> [16] </ref> and Householder variants in [18] determine ideally conditioned B k such that B T k B k = I k (in exact arithmetic). Other implementations in [2] and [19] generate B k that are usually well-conditioned, if not ideally conditioned. <p> It is reasonable to assume that this factorization is determined using one or more stable factorization techniques. For example, the implementations of <ref> [16] </ref> and [18] first use modified Gram-Schmidt or, respectively, Householder transformations to produce AB k = B k+1 H k , where H k 2 IR (k+1)fik is upper Hessenberg, and then use plane rotations J 1 , : : : , J k to obtain A k B k = <p> rotations J 1 , : : : , J k to obtain A k B k = Q k R k with Q k = B k+1 J T 1 : : : J T In general, each Q k may be only implicitly specified, as in the implementations of <ref> [16] </ref> and [18], but each R k is always produced explicitly. Then, since the conditioning of AB k is determined by that of R k , it suffices to monitor the conditioning of R k and terminate the iterations if excessive ill-conditioning or singularity appears. <p> In the important case in which B T k B k = I k , as in the implementations of <ref> [16] </ref> and [18], we have 2 (R k ) = 2 (AB k ) = 2 (A k ) 2 (A), where A k = Aj K k as above. <p> This implementation was used in all of the numerical experiments reported in x4. 4. Numerical experiments. In this section, we discuss several numerical experiments that illustrate the theoretical and practical points brought out above. A standard modified Gram-Schmidt gmres implementation, as originally outlined in <ref> [16] </ref>, was used in all experiments. Recall that with this implementation, the basis matrix B k is ideally conditioned, with B T k B k = I k .
Reference: [17] <author> P. N. Swarztrauber and R. A. Sweet, </author> <title> Efficient fortran subprograms for the solution of elliptic partial differential equations, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 (1979), </volume> <pages> pp. 352-364. </pages>
Reference-contexts: We took 10 d = 10 and preconditioned the discretized problems on the right with a fast Poisson solver from Fishpack <ref> [17] </ref>. This preconditioner is very effective for this fairly small value of d. We took tol = 10 10 in order to see how gmres behaved with a tight stopping tolerance. We also stopped the iterations when the condition number estimate became greater than 1=(50u) 10 14 .
Reference: [18] <author> H. F. Walker, </author> <title> Implementation of the GMRES method using Householder transformations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 152-163. </pages>
Reference-contexts: Sound gmres implementations are designed so that, as much as possible, each B k is well-conditioned regardless of the conditioning of A. For example, the standard implementation of [16] and Householder variants in <ref> [18] </ref> determine ideally conditioned B k such that B T k B k = I k (in exact arithmetic). Other implementations in [2] and [19] generate B k that are usually well-conditioned, if not ideally conditioned. <p> It is reasonable to assume that this factorization is determined using one or more stable factorization techniques. For example, the implementations of [16] and <ref> [18] </ref> first use modified Gram-Schmidt or, respectively, Householder transformations to produce AB k = B k+1 H k , where H k 2 IR (k+1)fik is upper Hessenberg, and then use plane rotations J 1 , : : : , J k to obtain A k B k = Q k <p> 1 , : : : , J k to obtain A k B k = Q k R k with Q k = B k+1 J T 1 : : : J T In general, each Q k may be only implicitly specified, as in the implementations of [16] and <ref> [18] </ref>, but each R k is always produced explicitly. Then, since the conditioning of AB k is determined by that of R k , it suffices to monitor the conditioning of R k and terminate the iterations if excessive ill-conditioning or singularity appears. <p> In the important case in which B T k B k = I k , as in the implementations of [16] and <ref> [18] </ref>, we have 2 (R k ) = 2 (AB k ) = 2 (A k ) 2 (A), where A k = Aj K k as above.
Reference: [19] <author> H. F. Walker and L. Zhou, </author> <title> A simpler GMRES, </title> <journal> J. Numer. Lin. Alg. Appl., </journal> <volume> 1 (1994), </volume> <pages> pp. 571-581. 14 </pages>
Reference-contexts: For example, the standard implementation of [16] and Householder variants in [18] determine ideally conditioned B k such that B T k B k = I k (in exact arithmetic). Other implementations in [2] and <ref> [19] </ref> generate B k that are usually well-conditioned, if not ideally conditioned. In any event, in well-constructed gmres implementations, the conditioning of B k does not suffer directly from ill-conditioning of A; furthermore, any ill-conditioning of B k seems likely to be reflected in ill-conditioning of AB k .
References-found: 19

