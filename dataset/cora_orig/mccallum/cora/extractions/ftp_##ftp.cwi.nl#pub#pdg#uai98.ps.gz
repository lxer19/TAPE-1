URL: ftp://ftp.cwi.nl/pub/pdg/uai98.ps.gz
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 26,  
Title: Minimum Encoding Approaches for Predictive Modeling  
Author: Peter Grunwald Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri 
Date: July 1998).  
Note: To appear in the Proceedings of the Fourteenth International Conference on Uncertainty in  
Web: http://www.cwi.nl/~pdg/  http://www.cs.Helsinki.FI/research/cosco/  
Address: P.O.Box 94079 NL-1090 GB Amsterdam, The Netherlands  P.O.Box  FIN-00014 University of Helsinki, Finland  (Madison, WI,  
Affiliation: CWI Dept. of Algorithms and Architectures  Complex Systems Computation Group (CoSCo)  Department of Computer Science  Artificial Intelligence  
Abstract: We analyze differences between two information-theoretically motivated approaches to statistical inference and model selection: the Minimum Description Length (MDL) principle, and the Minimum Message Length (MML) principle. Based on this analysis, we present two revised versions of MML: a pointwise estimator which gives the MML-optimal single parameter model, and a volumewise estimator which gives the MML-optimal region in the parameter space. Our empirical results suggest that with small data sets, the MDL approach yields more accurate predictions than the MML estimators. The empirical results also demonstrate that the revised MML estimators introduced here perform better than the original MML estimator suggested by Wallace and Freeman.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baxter, R., & Oliver, J. </author> <year> (1994). </year> <title> MDL and MML: Similarities and differences (Tech. </title> <type> Rep. No. 207). </type> <institution> Department of Computer Science, Monash University. </institution>
Reference-contexts: Both approaches are based on the idea that the more we are able to compress a given set of data, the more we have learned about the domain the data was collected from. Nevertheless, as discussed in <ref> (Baxter & Oliver, 1994) </ref>, there are subtle differences between these two approaches in both the underlying philosophy and the proposed formal criteria. We stress that this paper concerns the general MDL principle, not the original MDL model selection criterion (Rissanen, 1978).
Reference: <author> Berger, J. </author> <year> (1985). </year> <title> Statistical decision theory and Bayesian analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Hence we can use d 2 = 12=I n ( 0 ), where I n ( 0 ) = E fi (@ 2 log f (x n j)=@ 2 ) which coincides with the Fisher (expected) information for n observations <ref> (Berger, 1985) </ref>. The advantage of using I n ( 0 ) is that now the optimal d is independent of the observed data and becomes a function of 0 only. <p> for predicting x i+1 on the basis of x i = (x 1 ; : : : ; x i ) is f (x i+1 jx i ) / f (x i+1 j)P ()d; (19) where the prior distribution P () is chosen to be the so-called Jeffrey's prior () <ref> (Berger, 1985) </ref>, () / jI ()j: (20) It is now interesting to see that our revised volumewise MML estimator 00 leads to the following predictive distribution: f (x i+1 jx i ) = f (x i+1 j (x i )); (21) where (x i ) is set equal to the maximum
Reference: <author> Cooper, G., & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> Cover, T., & Thomas, J. </author> <year> (1991). </year> <title> Elements of information theory. </title> <address> New York, NY: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: We denote by B fl the set of all finite binary strings. By a (prefix) code C we mean a one-one function from a countable set A to B fl , where the mapping is such that the Kraft inequality <ref> (Cover & Thomas, 1991) </ref> holds: X 2 L C (x) 1: (1) Here L C (x) is the length (number of bits) of C (x), the encoding of x.
Reference: <author> Friedman, N., Geiger, D., & Goldszmidt, M. </author> <year> (1997). </year> <title> Bayesian network classifiers. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <pages> 131-163. </pages>
Reference-contexts: In the first set of experiments, we computed the cross-validated 0/1-scores for each of the four methods by using 5-fold crossvalidation (following the testing scheme used in <ref> (Friedman et al., 1997) </ref>).
Reference: <author> Heckerman, D. </author> <year> (1996). </year> <title> A tutorial on learning with Bayesian networks (Tech. Rep. No. MSR-TR-95-06). One Microsoft Way, </title> <address> Redmond, WA 98052: </address> <institution> Microsoft Research, Advanced Technology Division. </institution>
Reference-contexts: For being able to perform these experiments, we now instantiate the above listed different predictive distributions for a model family of practical importance, the family of Bayesian networks (see, e.g., <ref> (Heckerman, 1996) </ref>). A Bayesian network is a representation of a probability distribution over a set of (in our case) discrete variables, consisting of an acyclic directed graph, where the nodes correspond to domain variables X 1 ; : : : ; X m . <p> For this reason, in this set of experiments we determined the subjective prior h by using the equivalent sample size (ESS) priors, which have a clear interpretation from a subjective Bayesian point of view <ref> (Heckerman, 1996) </ref>. Experiments with different ESS subjective priors seemed to produce similar results. In the experiments reported here, the equivalent sample sizes where chosen to be the smallest possible numbers with which the above mentioned technical difficulty did not occur. 3 "http://www.ics.uci.edu/~mlearn/".
Reference: <author> Heckerman, D., Geiger, D., & Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 (3), </volume> <pages> 197-243. </pages>
Reference: <author> Kontkanen, P., Myllymaki, P., Silander, T., Tirri, H., & Grunwald, P. </author> <year> (1997). </year> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the sixth international workshop on artificial intelligence and statistics (pp. </booktitle> <pages> 311-318). </pages> <address> Ft. Lauderdale, Florida. </address>
Reference: <author> Kontkanen, P., Myllymaki, P., Silander, T., Tirri, H., & Grunwald, P. </author> <year> (1998). </year> <title> Bayesian and information-theoretic priors for Bayesian network parameters. </title> <editor> In C. Nedellec & C. Rouveirol (Eds.), </editor> <booktitle> Machine learning: ECML-98, proceedings of the 10th European conference (pp. </booktitle> <pages> 89-94). </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Our revised pointwise MML estimator described in Section 3.2 leads to using the subjective prior h (fi) as the prior P (fi). The revised volumewise MML estimator described in Section 3.2 suggests that the prior should be defined by P (fi) = h (fi)(fi). As shown in <ref> (Kontkanen et al., 1998) </ref>, the Jeffrey's prior distribution (fi) can in the above Bayesian net work model family case be computed by (fi) / i=1 q i =1 q i ) 2 l=1 q i l ) 1 where P i q i stands for the probability P (pa i =
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 445-471. </pages>
Reference-contexts: 1 INTRODUCTION Two related but distinct approaches to statistical inference and model selection are the Minimum Description Length (MDL) principle <ref> (Rissanen, 1978, 1987, 1996) </ref>, and the Minimum Message Length (MML) principle (Wallace & Boulton, 1968; Wallace & Freeman, 1987). <p> Nevertheless, as discussed in (Baxter & Oliver, 1994), there are subtle differences between these two approaches in both the underlying philosophy and the proposed formal criteria. We stress that this paper concerns the general MDL principle, not the original MDL model selection criterion <ref> (Rissanen, 1978) </ref>. The latter takes the same form as the Bayesian BIC criterion (Schwarz, 1978), which has led some people to believe that `MDL=BIC' (see the discussion in (Rissanen, 1996)). The instantiation of MDL we discuss here is not directly related to BIC. <p> One obvious possibility for this is to use a two-part code as with the MML approach; the resulting two-part code MDL estimator is discussed in <ref> (Rissanen, 1978, 1989) </ref>. However, it is relatively easy to see that the two-part code (2) is redundant: every data sequence x n can be encoded using every ^ 2 Q for which f (x n j ^ ) &gt; 0.
Reference: <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49 (3), </volume> <pages> 223-239 and 252-265. </pages>
Reference-contexts: By encoding continuous outcomes to an arbitrary but finite and fixed precision, we may also regard each density function f (xj) as a code C f with codelengths L C f (x) = log f (xj) (see <ref> (Rissanen, 1987) </ref> for details). We call C f the code corresponding to f . Similarly, for each code C for the set A, we may regard P C (defined by P C (x) = 2 L C (x) ) as a (possibly subaddi-tive) probability distribution over A.
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic complexity in statistical inquiry. </title> <address> New Jersey: </address> <publisher> World Scientific Publishing Company. </publisher>
Reference: <author> Rissanen, J. </author> <year> (1996). </year> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42 (1), </volume> <pages> 40-47. </pages>
Reference-contexts: We stress that this paper concerns the general MDL principle, not the original MDL model selection criterion (Rissanen, 1978). The latter takes the same form as the Bayesian BIC criterion (Schwarz, 1978), which has led some people to believe that `MDL=BIC' (see the discussion in <ref> (Rissanen, 1996) </ref>). The instantiation of MDL we discuss here is not directly related to BIC. Recently (Rissanen, 1996), the MDL approach has been refined to incorporate effects on the description length of the data that are due to local geometrical properties of the hypothesis space. <p> The latter takes the same form as the Bayesian BIC criterion (Schwarz, 1978), which has led some people to believe that `MDL=BIC' (see the discussion in <ref> (Rissanen, 1996) </ref>). The instantiation of MDL we discuss here is not directly related to BIC. Recently (Rissanen, 1996), the MDL approach has been refined to incorporate effects on the description length of the data that are due to local geometrical properties of the hypothesis space. Wallace and Freeman have already taken these properties into account in their paper on MML estimators (Wallace & Freeman, 1987). <p> In the present paper we investigate this claim, and show that it does not hold: though superficially similar, the refinement of MDL proposed in <ref> (Rissanen, 1996) </ref> is quite different from the MML approach proposed in (Wallace & Freeman, 1987). <p> In Section 4, we show how the two-part code MDL, the form of MDL that most resembles MML, can be refined by the considerations presented in <ref> (Rissanen, 1996) </ref>. In Section 5 we discuss how to construct different predictive distributions based on the MML and MDL estimators considered. <p> To obtain this interval, let us adapt the line of reasoning used in <ref> (Rissanen, 1996) </ref>, and look at the MML two-part code in another manner. We partition the parameter space into a set of adjacent regions R 1 ; : : : ; R M , each of width w, where w is such that M t N . <p> Though C 1;2 will always use the particular ^ 2 Q for which the total description length is minimized, codewords are `reserved' for many other ways of encoding x n . Until recently, it has not been clear how to remove this redundancy in a principled manner. In <ref> (Rissanen, 1996) </ref>, this problem was finally solved. Following Rissanen, for simplicity we assume that Q contains a finite number of parameters. As in Section 3, we can thus write Q = Q N = f ^ 1 ; : : : ^ N g. <p> In general, P x n 2D i f (x n j ^ i ) &lt; 1, which means that the revised two-part code has a strictly shorter codelength than the original one. It was shown in <ref> (Rissanen, 1996) </ref> that the normalization trick described above can be optimally exploited (for large N ) if, for every 2 , the density of parameter values in Q N in the neighborhood of is proportional to p jI ()j, where jI ()j is the determinant of the Fisher information matrix (18). <p> Rissanen chooses the second option, but explicitly mentions that the first one is possible too <ref> (Rissanen, 1996, page 43) </ref>. We now see the reason for the confusion mentioned in the introduction: although the optimal width between adjacent parameter values as determined in (Wallace & Freeman, 1987) is also proportional to 1= p this same width was chosen for a very different reason.
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference-contexts: We stress that this paper concerns the general MDL principle, not the original MDL model selection criterion (Rissanen, 1978). The latter takes the same form as the Bayesian BIC criterion <ref> (Schwarz, 1978) </ref>, which has led some people to believe that `MDL=BIC' (see the discussion in (Rissanen, 1996)). The instantiation of MDL we discuss here is not directly related to BIC.
Reference: <author> Wallace, C., & Boulton, D. </author> <year> (1968). </year> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11, </volume> <pages> 185-194. </pages>
Reference: <author> Wallace, C., & Freeman, P. </author> <year> (1987). </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49 (3), </volume> <pages> 240-265. 10 </pages>
Reference-contexts: Recently (Rissanen, 1996), the MDL approach has been refined to incorporate effects on the description length of the data that are due to local geometrical properties of the hypothesis space. Wallace and Freeman have already taken these properties into account in their paper on MML estimators <ref> (Wallace & Freeman, 1987) </ref>. It has been informally claimed by several people at several conferences that a large part of Ris-sanen's 1996 work is already implicit in (Wallace & Freeman, 1987). <p> Wallace and Freeman have already taken these properties into account in their paper on MML estimators <ref> (Wallace & Freeman, 1987) </ref>. It has been informally claimed by several people at several conferences that a large part of Ris-sanen's 1996 work is already implicit in (Wallace & Freeman, 1987). In the present paper we investigate this claim, and show that it does not hold: though superficially similar, the refinement of MDL proposed in (Rissanen, 1996) is quite different from the MML approach proposed in (Wallace & Freeman, 1987). <p> a large part of Ris-sanen's 1996 work is already implicit in <ref> (Wallace & Freeman, 1987) </ref>. In the present paper we investigate this claim, and show that it does not hold: though superficially similar, the refinement of MDL proposed in (Rissanen, 1996) is quite different from the MML approach proposed in (Wallace & Freeman, 1987). <p> Our analysis of the reasons for this difference shows that there is a notable weakness in the derivation of MML estimators presented in <ref> (Wallace & Freeman, 1987) </ref>. By removing this weakness, we arrive at two revised versions of MML. <p> In the theoretical part of the paper, in Section 2 we first briefly review the MDL and MML principles, and discuss their basic differences and similarities. In Section 3, we review in detail how the MML estimators were derived in <ref> (Wallace & Freeman, 1987) </ref>, and point out an important oversight in the derivation. Based on this analysis, we present two `revised' versions of MML: a pointwise estimator which gives the MML--optimal single parameter value , and a volumewise estimator which gives the MML-optimal region in the parameter space. <p> C 1 and the estimator ^ : X n ! minimizing the sum X r (x)[L C 1 ( ^ (x n )) log f (x n j ^ (x n ))]: (3) The estimator ^ that is optimal in the above sense is called the strict MML (SMML) estimator <ref> (Wallace & Freeman, 1987) </ref>. <p> To be sure, priors do arise in MDL modeling, but they are merely used as technical tools and not as representing prior knowledge about the problem at hand. 3 MML ESTIMATORS 3.1 MMLWF ESTIMATOR We now consider the original derivation presented in <ref> (Wallace & Freeman, 1987) </ref>, and call the resulting MML estimator the MMLWF estimator. We concentrate first on the case of a model class M containing models depending on a single parameter (hence R 1 ). <p> For this, one must know the precision d that was used to encode ^ . Since the optimal precision depends on x n , it is not constant and hence it seems that it must be made part of the code too. However, in <ref> (Wallace & Freeman, 1987) </ref> it was shown that the minimum of the expected message length reached for the optimal precision d 2 = 12=I (x n ; 0 ) is very broad with respect to d. <p> This means that there is only one set of possible truncated estimates which can be constructed without reference to the data. We can thus construct a code for the estimate which does not need a precision preamble (for more details we refer to <ref> (Wallace & Freeman, 1987) </ref>). <p> Rissanen chooses the second option, but explicitly mentions that the first one is possible too (Rissanen, 1996, page 43). We now see the reason for the confusion mentioned in the introduction: although the optimal width between adjacent parameter values as determined in <ref> (Wallace & Freeman, 1987) </ref> is also proportional to 1= p this same width was chosen for a very different reason.
References-found: 16

