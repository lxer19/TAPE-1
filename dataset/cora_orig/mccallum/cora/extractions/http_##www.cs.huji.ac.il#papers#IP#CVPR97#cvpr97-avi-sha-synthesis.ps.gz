URL: http://www.cs.huji.ac.il/papers/IP/CVPR97/cvpr97-avi-sha-synthesis.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/CVPR97/index.html
Root-URL: http://www.cs.huji.ac.il
Email: favidan,shashuag@cs.huji.ac.il  
Title: Novel View Synthesis in Tensor Space  
Author: Shai Avidan Amnon Shashua 
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science The Hebrew University  
Abstract: We present a new method for synthesizing novel views of a 3D scene from few model images in full correspondence. The core of this work is the derivation of a tensorial operator that describes the transformation from a given tensor of three views to a novel tensor of a new configuration of three views. By repeated application of the operator on a seed tensor with a sequence of desired virtual camera positions we obtain a chain of warping functions (tensors) from the set of model images to create the desired virtual views. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Adelson and J. Bergen. </author> <title> The plenoptic function and the el ements of early vision. </title> <editor> In M. Landy and J. A. Movshon, ed itors, </editor> <title> Computational Models of Visual Processing. </title> <publisher> The MIT Press, </publisher> <address> Cambridgem, Mass., </address> <year> 1991. </year> <note> Chapter 1. </note>
Reference-contexts: Instead of flow-field interpolation among the model images, it is possible to interpolate directly over the plenoptic function <ref> [1] </ref> a function which represents the amount of light emitted at each point in space as a function of direction.
Reference: [2] <author> S. avidan, T. Evgeniou, A. Shashua, and T. Poggio. </author> <title> Image based view synthesis. </title> <type> Technical report, </type> <institution> A.I. </institution> <note> Memo No. 1603, </note> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: We are currently working on modeling non-rigid as well as rigid transformation under this framework <ref> [2] </ref>.
Reference: [3] <author> S. Avidan and A. Shashua. </author> <title> Unifying two-view and three view geometry. </title> <booktitle> In ARPA, Image Understanding Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: As shown in <ref> [3] </ref>, the rank of fl jk i is 2 whereas the rank of the tensor of three distinct views is 4 but otherwise all other properties remain and, in particular, ff jk i can serve as the first tensor that starts the synthesis process described above [3]. <p> As shown in <ref> [3] </ref>, the rank of fl jk i is 2 whereas the rank of the tensor of three distinct views is 4 but otherwise all other properties remain and, in particular, ff jk i can serve as the first tensor that starts the synthesis process described above [3].
Reference: [4] <author> E. Barretta, P. Payton, and G. Gheen. </author> <title> Robust algebraic in variant methods with applications in geometry and imaging. </title> <booktitle> In Proceedings of the SPIE on Remote Sensing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: This process is referred to as re-projection in the literature. There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results (see <ref> [4, 24, 27] </ref>). We have described so far the implementation of the re-projection paradigm via the trilinear equations. In other words, given two model views and a tensor, the third view is uniquely determined and can be synthesized by means of a warping function applied to the two model images.
Reference: [5] <author> T. Beier and S. Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH, </booktitle> <year> 1992. </year>
Reference-contexts: The work in this area can be roughly divided into three classes: (i) image interpolation, (ii) Off-line (Mosaic-based) synthesis, and (iii) On-line synthesis. The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing <ref> [5] </ref>, direct interpolation from image-flows (multi-dimensional morphing) [6, 21], image interpolation using 3D models instead of image-flow [7], and physically correct image interpolation [23, 31].
Reference: [6] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based im age analysis and synthesis. </title> <type> Technical report, </type> <institution> A.I. </institution> <note> Memo No. 1431, </note> <institution> Artificial Intelligence Laboratory, Massachusetts Insti tute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing [5], direct interpolation from image-flows (multi-dimensional morphing) <ref> [6, 21] </ref>, image interpolation using 3D models instead of image-flow [7], and physically correct image interpolation [23, 31].
Reference: [7] <author> S. Chen and L. Williams. </author> <title> View interpolation for image syn thesis. </title> <booktitle> In SIGGRAPH, </booktitle> <year> 1993. </year>
Reference-contexts: The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing [5], direct interpolation from image-flows (multi-dimensional morphing) [6, 21], image interpolation using 3D models instead of image-flow <ref> [7] </ref>, and physically correct image interpolation [23, 31]. All but the last two references do not guarantee to produce physically cor-rect images, and all cannot extrapolate from the set of input images that is, create novel viewing positions that are outside of the viewing cone of the model images.
Reference: [8] <author> S. E. Chen. </author> <title> QuickTimeVR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH, </booktitle> <year> 1995. </year>
Reference-contexts: The simplest stitching occurs when the camera motion includes only rotation in which case the transformation between the views is parametric and does not include any 3D shape (the transformation being a 2D projective transformation, a homography). This was cleverly done by <ref> [13, 8] </ref> in what is known as QuickTime VR. Szeliski and Kang [29] create high-resolution mosaics from low-resolution video streams, and Peleg and Herman [20] relax the fixed camera constraint by introducing the projection manifold.
Reference: [9] <author> O. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Proceedings of the European Conference on Computer Vision, </title> <year> 1992. </year>
Reference-contexts: The Seed Tensor of Two views Given two acquired images we can construct a special tensor composed of the elements of the fundamental matrix <ref> [9] </ref> that can serve as a seed tensor that starts the chain of tensors, as follows.
Reference: [10] <author> O. Faugeras and B. Mourrain. </author> <title> On the geometry and algebra of the point and line correspondences between N images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [11] <author> O. Faugeras and L. Robert. </author> <title> What can two images tell us about a third one? In Proceedings of the European Confer ence on Computer Vision, 1994. (a) (b) (c) (d) the images are taken from the generated movie. They should be seen left to right, top to bottom. (a) (b) (c) (d) (e) rest are taken from the virtual movie. </title>
Reference-contexts: Non-singular Configurations: to rely on warping functions that are free from singularities under camera motion. For example, the use of the fundamental matrix, or concatenation of fundamental matrices, for deriving a warping function based on epipolar line intersection (cf. <ref> [11] </ref>) is undesirable on this account due to singularities that arise when the camera centers are collinear. Driving Mode: the specification of the virtual camera position should be intuitively simple for the user.
Reference: [12] <author> S. Gortler, R. Grzeszczuk, R. Szeliski, and M. Cohen. </author> <booktitle> The lumigraph. In SIGGRAPH, </booktitle> <pages> pages 4354, </pages> <year> 1996. </year>
Reference-contexts: Instead of flow-field interpolation among the model images, it is possible to interpolate directly over the plenoptic function [1] a function which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. [18] and Gortler et al. <ref> [12] </ref> interpolate between a dense set of several thousands of example images to reconstruct a reduced plenoptic function (under an occlusion-free world assumption). Hence, they considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [13] <author> N. Greene. </author> <title> Creating raster omnimax images from multiple perspective views using the elliptical weighted average fil ter. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 6(6):2127, </volume> <year> 1986. </year>
Reference-contexts: The simplest stitching occurs when the camera motion includes only rotation in which case the transformation between the views is parametric and does not include any 3D shape (the transformation being a 2D projective transformation, a homography). This was cleverly done by <ref> [13, 8] </ref> in what is known as QuickTime VR. Szeliski and Kang [29] create high-resolution mosaics from low-resolution video streams, and Peleg and Herman [20] relax the fixed camera constraint by introducing the projection manifold.
Reference: [14] <author> W. </author> <title> Grimson. Why stereo vision is not always about 3D re construction. </title> <type> Technical report, </type> <institution> A.I. </institution> <note> Memo No. 1435, </note> <institution> Artifi cial Intelligence Laboratory, Massachusetts Institute of Tech nology, </institution> <year> 1993. </year>
Reference-contexts: Depth maps are easily provided for synthetic environments, whereas for real scenes the process is fragile especially under small base-line situations that arise due to the requirement of dense correspondence between the model images/mosaics <ref> [14] </ref>. The challenges facing an optimal on-line synthesis approach are therefore: Implicit Scene Modeling: to reduce as much as possible the computational steps from the input correspondence field among the model images to useful algebraic structures that would suffice for generating new views.
Reference: [15] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [16] <author> A. </author> <title> Heyden. Reconstruction from image sequences by means of relative depths. </title> <booktitle> In Proceedings of the International Con ference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [17] <author> S. Laveau and O. Faugeras. </author> <title> 3-d scene representation as a collection of images. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: Laveau and Faugeras <ref> [17] </ref> were the first to use the epipolar constraint for view synthesis, allowing them to extrapolate, as well as interpolate, between the example images. <p> Driving Mode: the specification of the virtual camera position should be intuitively simple for the user. For example, rotation and translation of the camera from its current position is prevalent among most 3D viewers. None of the existing approaches for on-line synthesis satisfy all three requirements. For example, <ref> [17] </ref> satisfy the first requirement at the cost of complicating the driving mode by specifying control points; using depth maps provide an intuitive driving mode and lack of singularities but does not satisfy the implicit scene modeling requirement. <p> In other words, the synthesis process can start with two model views and their fundamental matrix, as in <ref> [17] </ref>, but the later steps follow the trilinear tensor machinery which ensures lack of singular configurations and provide a natural driving mode thus satisfying the three requirements described in Section 1. 4. Experimental Results The tensor-based rendering method was implemented on synthetic and real image images.
Reference: [18] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In SIG GRAPH, </booktitle> <pages> pages 3142, </pages> <year> 1996. </year>
Reference-contexts: Instead of flow-field interpolation among the model images, it is possible to interpolate directly over the plenoptic function [1] a function which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. <ref> [18] </ref> and Gortler et al. [12] interpolate between a dense set of several thousands of example images to reconstruct a reduced plenoptic function (under an occlusion-free world assumption). Hence, they considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [19] <author> L. McMillan and G. Bishop. </author> <title> Plenoptic modeling: An image based rendering system. </title> <booktitle> In SIGGRAPH, </booktitle> <year> 1995. </year>
Reference-contexts: The singular camera motions can be relaxed by using the depth map of the environment. McMillan and Bishop <ref> [19] </ref> use a full depth map (3D reconstruction of the camera motion and the environment) together with the epipolar constraint to provide a direct connection between the virtual camera motion and the reprojection engine.
Reference: [20] <author> S. Peleg and J. Herman. </author> <title> Panoramic mosaic by manifold pro jection. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <year> 1997. </year>
Reference-contexts: This was cleverly done by [13, 8] in what is known as QuickTime VR. Szeliski and Kang [29] create high-resolution mosaics from low-resolution video streams, and Peleg and Herman <ref> [20] </ref> relax the fixed camera constraint by introducing the projection manifold. A drawback of this approaches is that one cannot correctly simulate translational camera motion from the set of model images.
Reference: [21] <author> T. Poggio and R. Brunelli. </author> <title> A novel approach to graphics. </title> <type> Technical report, </type> <institution> A.I. </institution> <note> Memo No. 1354, </note> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing [5], direct interpolation from image-flows (multi-dimensional morphing) <ref> [6, 21] </ref>, image interpolation using 3D models instead of image-flow [7], and physically correct image interpolation [23, 31].
Reference: [22] <author> B. Rousso, S. Avidan, A. Shashua, and S. Peleg. </author> <title> Robust re covery of camera rotation from three frames. </title> <booktitle> In Proceedings of IEEE Conference on Computer Vision and Pattern Recog nition, </booktitle> <year> 1996. </year>
Reference: [23] <author> S. M. Seitz and C. R. Dyer. </author> <title> Physically-valid view synthesis by image interpolation. </title> <booktitle> IEEE Workshop on Representation of Visual Scenes, </booktitle> <year> 1995. </year>
Reference-contexts: The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing [5], direct interpolation from image-flows (multi-dimensional morphing) [6, 21], image interpolation using 3D models instead of image-flow [7], and physically correct image interpolation <ref> [23, 31] </ref>. All but the last two references do not guarantee to produce physically cor-rect images, and all cannot extrapolate from the set of input images that is, create novel viewing positions that are outside of the viewing cone of the model images.
Reference: [24] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(8):779789, </volume> <year> 1995. </year>
Reference-contexts: We propose a new view-synthesis method that makes use of the recent development of multi-linear matching constraints, known as trilinearities, that were first introduced in <ref> [24] </ref>. The trilinearities provide a general (not subject to singular camera configurations) warping function from model fl An on-line demo can be found on the World-Wide-Web in the following address http://www.cs.huji.ac.il/labs/vision/demos/demo.html. <p> Since every corresponding triplet p; p 0 ; p 00 contributes four linearly independent equations, then seven corresponding points across the three views uniquely determine (up to scale) the tensor ff jk i . These constraints first became prominent in <ref> [24] </ref> and the underlying theory has been studied intensively in [28, 15, 25, 10, 30, 16, 26]. <p> This process is referred to as re-projection in the literature. There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results (see <ref> [4, 24, 27] </ref>). We have described so far the implementation of the re-projection paradigm via the trilinear equations. In other words, given two model views and a tensor, the third view is uniquely determined and can be synthesized by means of a warping function applied to the two model images. <p> Going back to the tensor of views &lt; 1; 2; 3 &gt; described in eqn. (1), we note that the homographies A and B correspond to an arbitrary plane, and the choice of the plane does not change the tensor coefficients <ref> [24] </ref>.
Reference: [25] <author> A. Shashua and P. Anandan. </author> <title> The generalized trilinear con straints and the uncertainty tensor. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <year> 1996. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [26] <author> A. Shashua and S. Avidan. </author> <title> The rank4 constraint in multi ple view geometry. </title> <booktitle> In Proceedings of the European Confer ence on Computer Vision, </booktitle> <year> 1996. </year> <note> Also in CIS report #9520, </note> <month> November </month> <year> 1995, </year> <institution> Technion. </institution>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [27] <author> A. Shashua and S. Maybank. </author> <title> Degenerate n point config urations of three views: Do critical surfaces exist? Techni cal report, </title> <type> Technical report, </type> <institution> Hebrew University of Jerusalem, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This process is referred to as re-projection in the literature. There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results (see <ref> [4, 24, 27] </ref>). We have described so far the implementation of the re-projection paradigm via the trilinear equations. In other words, given two model views and a tensor, the third view is uniquely determined and can be synthesized by means of a warping function applied to the two model images.
Reference: [28] <author> A. Shashua and M. Werman. </author> <title> On the trilinear tensor of three perspective views and its underlying geometry. </title> <booktitle> In Proceed ings of the International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [29] <author> R. Szeliski and S. Kang. </author> <title> Direct methods for visual scene reconstruction. </title> <booktitle> IEEE Workshop on Representation of Visual Scenes, </booktitle> <year> 1995. </year>
Reference-contexts: This was cleverly done by [13, 8] in what is known as QuickTime VR. Szeliski and Kang <ref> [29] </ref> create high-resolution mosaics from low-resolution video streams, and Peleg and Herman [20] relax the fixed camera constraint by introducing the projection manifold. A drawback of this approaches is that one cannot correctly simulate translational camera motion from the set of model images.
Reference: [30] <author> B. Triggs. </author> <title> Matching constraints and the joint image. </title> <booktitle> In Pro ceedings of the International Conference on Computer Vi sion, </booktitle> <year> 1995. </year>
Reference-contexts: These constraints first became prominent in [24] and the underlying theory has been studied intensively in <ref> [28, 15, 25, 10, 30, 16, 26] </ref>.
Reference: [31] <author> T. Werner, R. Hersch, and V. Hlavac. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Proceedings of the In ternational Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: The first class, image interpolation, is designed to create in-between images among two or more model images. This includes image morphing [5], direct interpolation from image-flows (multi-dimensional morphing) [6, 21], image interpolation using 3D models instead of image-flow [7], and physically correct image interpolation <ref> [23, 31] </ref>. All but the last two references do not guarantee to produce physically cor-rect images, and all cannot extrapolate from the set of input images that is, create novel viewing positions that are outside of the viewing cone of the model images.
References-found: 31

