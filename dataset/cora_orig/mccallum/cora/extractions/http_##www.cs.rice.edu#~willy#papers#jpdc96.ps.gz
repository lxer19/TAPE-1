URL: http://www.cs.rice.edu/~willy/papers/jpdc96.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Email: e-mail: fhhl, sandhya, alc, willyg@cs.rice.edu  
Phone: Tel: (713) 285-5402  
Title: Quantifying the Performance Differences Between PVM and TreadMarks  
Author: Honghui Lu Sandhya Dwarkadas, Alan L. Cox, and Willy Zwaenepoel 
Note: This research was supported in part by NSF NYI Award CCR-9457770, NSF CISE postdoctoral fellowship Award CDA-9310073, NSF Grants CCR-9116343 and BIR-9408503, and by the Texas Advanced Technology Program under Grant 003604012.  
Address: 6100 S. Main St. Houston, TX 77005-1892  
Affiliation: Department of Electrical and Computer Engineering  Department of Computer Science Rice University  
Abstract: We compare two systems for parallel programming on networks of workstations: Parallel Virtual Machine (PVM), a message passing system, and TreadMarks, a software distributed shared memory (DSM) system. We present results for eight applications that were implemented using both systems. The programs are Water and Barnes-Hut from the SPLASH benchmark suite; 3-D FFT, Integer Sort (IS) and Embarrassingly Parallel (EP) from the NAS benchmarks; ILINK, a widely used genetic linkage analysis program; and Successive Over-Relaxation (SOR) and Traveling Salesman (TSP). Two different input data sets were used for five of the applications. We use two execution environments. The first is an 155Mbps ATM network with eight Sparc-20 model 61 workstations; the second is an eight processor IBM SP/2. The differences in speedup between TreadMarks and PVM are dependent on the application, and, only to much a lesser extent, on the platform and the data set used. In particular, the TreadMarks speedup for six of the eight applications is within 15% of that achieved with PVM. For one application, the difference in speedup is between 15% and 30%, and for one application, the difference is around 50%. More important than the actual differences in speedups, we investigate the causes behind these differences. The cost of sending and receiving messages on current networks of workstations is very high, and previous work has identified communication costs as the primary source of overhead in software DSM implementations. The observed performance differences between PVM and Tread-Marks are therefore primarily a result of differences in the amount of communication between the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This partial order is known as hb1 <ref> [1] </ref>. Vector timestamps are used to represent the partial order. When a processor executes an acquire, it sends its current timestamp in the acquire message. The previous releaser then piggybacks on its response the set of write notices that have timestamps greater than the timestamp in the acquire message.
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [28]; 3-D FFT, Integer Sort (IS), and Embarrassingly Parallel (EP) from the NAS benchmarks <ref> [2] </ref>; ILINK, a widely used genetic linkage analysis program [8]; and Successive Over-Relaxation (SOR), and Traveling Salesman Problem (TSP). Two different input sets were used for five of the applications. <p> Max bandwidth with copying 7.2 MB/sec. 21.0 MB/sec. Table 1 Characteristics of the Experimental Platforms 4.2 Applications We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [28]; 3-D FFT, IS, and EP from the NAS benchmarks <ref> [2] </ref>; ILINK, a widely used genetic linkage analysis program [8]; and SOR, and TSP. The execution times for the sequential programs, without any calls to PVM or TreadMarks, are shown in Table 2. This table also shows the problem sizes used for each application. <p> In terms of performance, we again focus on the performance of the applications on the SPARC/ATM platform, as the results for the IBM SP/2 are qualitatively the same. 5.1 EP The Embarrassingly Parallel program comes from the NAS benchmark suite <ref> [2] </ref>. EP generates pairs of Gaussian random deviates and tabulates the number of pairs in successive square annuli. In the parallel version, the only communication is summing up a ten-integer list at the end of the program. In TreadMarks, updates to the shared list are protected by a lock. <p> Consequently, the performance gap between TreadMarks and PVM shrinks from 4% to zero, and both of them have a speedup of 7.53. 5.3 Integer Sort Integer Sort (IS) <ref> [2] </ref> from the NAS benchmarks requires ranking an unsorted sequence of keys using bucket sort. The parallel version of IS divides up the keys among the processors. First, each processor counts its own keys, and writes the result in a private array of buckets. <p> With reduced false sharing, TreadMarks sends 71% less messages and 29% less data at 8 processors. TreadMarks speedup is increased to 4.56, only 2% slower than PVM. 5.7 3-D FFT 3-D FFT, from the NAS <ref> [2] </ref> benchmark suite, numerically solves a partial differential equation using three dimensional forward and inverse FFT's. Assume the input array A is n 1 fi n 2 fi n 3 , organized in row-major order.
Reference: [3] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In the current implementation of TreadMarks, diff accumulation occurs for migratory data. 7 Migratory data is shared sequentially by a set of processors <ref> [3, 29] </ref>. Each processor has exclusive read and write access for a time. Accesses to migratory data are protected by locks in TreadMarks. Each time a processor accesses migratory data, it must see all the preceding modifications.
Reference: [4] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [30, 4, 16, 21] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations. <p> With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate [16] version of release consistency (RC) [10] and a multiple-writer protocol <ref> [4] </ref> to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model. <p> Diff squashing only reduces the data totals, which is a second order effect in TreadMarks overheads. As a result, it is profitable only for IS, where diffs completely overlap. 6 Related Work Our study distinguishes itself from most related work by being, with the exception of Carter et al. <ref> [4] </ref>, the first study to compare message passing to software distributed shared memory, implemented on top of message passing. <p> We are thus evaluating the cost of layering shared memory in software on top of message passing, in contrast to the studies that evaluate message passing and shared memory as two architectural models implemented in hardware. In contrast to the work on Munin <ref> [4] </ref>, we use lazy rather than eager release consistency. It has been demonstrated that lazy release consistency leads to lower communication requirements and better performance [15]. Furthermore, our study is done on common Unix platforms and using a well-known message passing system.
Reference: [5] <author> S. Chandra, J.R. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared memory programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Furthermore, our study is done on common Unix platforms and using a well-known message passing system. Among the architectural studies comparing message passing and shared memory, we cite two recent articles, namely Chandra et al. <ref> [5] </ref> and Klaiber and Levy [17]. Both of these are simulation studies, while our results are derived from measurements of an implementation. Chandra et al. [5] compares four applications, running either with a user-space message passing or with a full-map 23 invalidate shared memory coherence protocol. <p> Among the architectural studies comparing message passing and shared memory, we cite two recent articles, namely Chandra et al. <ref> [5] </ref> and Klaiber and Levy [17]. Both of these are simulation studies, while our results are derived from measurements of an implementation. Chandra et al. [5] compares four applications, running either with a user-space message passing or with a full-map 23 invalidate shared memory coherence protocol. All other simulation parameters, such processor and network characteristics and number of processors, are kept the same.
Reference: [6] <author> R. W. Cottingham Jr., R. M. Idury, and A. A. Schaffer. </author> <title> Faster sequential genetic linkage computations. </title> <journal> American Journal of Human Genetics, </journal> <volume> 53 </volume> <pages> 252-263, </pages> <year> 1993. </year>
Reference-contexts: The number of messages sent in TreadMarks drops to twice that in PVM. Consequently, TreadMarks' speedup increases from 4.72 to 4.99, less than 2% lower than the speedup of 5.12 obtained in PVM. 5.8 ILINK ILINK <ref> [6, 20] </ref> is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. The input to ILINK consists of several family trees. The program traverses the family trees and visits each nuclear family. The main data structure in ILINK is a pool of genarrays.
Reference: [7] <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Various compiler techniques can also be used to remedy some of the deficiencies of shared memory recognized in this study. For instance, Eggers and Jeremiassen [13] discuss compiler transformations to reduce the effect of false sharing, and Dwarkadas et al. <ref> [7] </ref> evaluate compiler support for communication aggregation, merging data and synchronization, and reduction of coherence overhead.
Reference: [8] <author> S. Dwarkadas, A.A. Schaffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [28]; 3-D FFT, Integer Sort (IS), and Embarrassingly Parallel (EP) from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program <ref> [8] </ref>; and Successive Over-Relaxation (SOR), and Traveling Salesman Problem (TSP). Two different input sets were used for five of the applications. We ran these programs on eight Sparc-20 model 61 workstations, connected by a 155Mbits per second ATM network, and on an eight processor IBM SP/2. <p> Table 1 Characteristics of the Experimental Platforms 4.2 Applications We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite [28]; 3-D FFT, IS, and EP from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program <ref> [8] </ref>; and SOR, and TSP. The execution times for the sequential programs, without any calls to PVM or TreadMarks, are shown in Table 2. This table also shows the problem sizes used for each application. On the IBM SP/2 we were able to run some applications with larger data sizes. <p> The computation either updates a parent's genarray conditioned on the spouse and all children, or updates one child conditioned on both parents and all the other siblings. We use the parallel algorithm described in Dwarkadas et al. <ref> [8] </ref>. Updates to each individual's genarray are parallelized. A master processor assigns the non-zero elements in the parent's genarray to all processors in a round robin fashion.
Reference: [9] <author> G.A. Geist and V.S. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice and Experience, </journal> <pages> pages 293-311, </pages> <month> June </month> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on this platform is message passing, using libraries such as PVM <ref> [9] </ref>, TCGMSG [11] and Express [25]. A message passing standard MPI [24] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer. <p> Since sending messages between workstations is expensive, this extra communication can hurt performance. Much work has been done in the past decade to improve the performance of DSM systems. In this paper, we compare a state-of-the-art DSM system, TreadMarks [16], with the most commonly used message passing system, PVM <ref> [9] </ref>. Our goals are to assess the differences in programmability and performance between DSM and message passing systems and to precisely determine the remaining causes of the lower performance of DSM systems. <p> Section 4 gives an overview of the experimental results. Section 5 discusses the performance of the different applications. Section 6 discusses related work. Section 7 concludes the paper. 2 PVM Versus TreadMarks 2.1 PVM PVM <ref> [9] </ref>, standing for Parallel Virtual Machine, is a message passing system originally developed at Oak Ridge National Laboratory. With PVM, the user data must be packed before being dispatched. The pack either copies user data into a send buffer, or keeps pointers to user data.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate [16] version of release consistency (RC) <ref> [10] </ref> and a multiple-writer protocol [4] to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model.
Reference: [11] <author> R.J. Harrison. </author> <title> Portable tools and applications for parallel computers. </title> <journal> In International Journal of Quantum Chemistry, </journal> <volume> volume 40, </volume> <pages> pages 847-863, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on this platform is message passing, using libraries such as PVM [9], TCGMSG <ref> [11] </ref> and Express [25]. A message passing standard MPI [24] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer.
Reference: [12] <author> J. T. Hecht, Y. Wang, B. Connor, S. H. Blanton, and S. P. Daiger. Non-syndromic cleft lip and palate: </author> <title> No evidence of linkage to hla or factor 13a. </title> <journal> American Journal of Human Genetics, </journal> <volume> 52 </volume> <pages> 1230-1233, </pages> <year> 1993. </year>
Reference-contexts: The diffing mechanism in TreadMarks automatically achieves the same effect. Since only the non-zero elements are modified during each nuclear family update, the diffs transmitted to the master only contain the non-zero elements. We used the CLP data set <ref> [12] </ref>, with an allele product 2 fi 4 fi 4 fi 4. The sequential program runs for 1473 seconds. At 8 processors, TreadMarks achieves a speedup of 5.57, which is 93% of the 5.99 obtained by PVM.
Reference: [13] <author> T.E. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the 5th ACM Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Various compiler techniques can also be used to remedy some of the deficiencies of shared memory recognized in this study. For instance, Eggers and Jeremiassen <ref> [13] </ref> discuss compiler transformations to reduce the effect of false sharing, and Dwarkadas et al. [7] evaluate compiler support for communication aggregation, merging data and synchronization, and reduction of coherence overhead.
Reference: [14] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The merge is accomplished through the use of diffs. A diff is a runlength encoding of the modifications made to a page, generated by comparing the page to a copy saved prior to the modifications. TreadMarks implements a lazy invalidate version of RC <ref> [14] </ref>. A lazy implementation delays the propagation of consistency information until the time of an acquire. Furthermore, the releaser notifies the acquirer of which pages have been modified, causing the acquirer to invalidate its local copies of these pages.
Reference: [15] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> An evaluation of software-based re lease consistent protocols. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 29 </volume> <pages> 126-141, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: The relative differences for the other applications remain the same as on the SPARC/ATM platform. Communication costs have been identified as the primary source of overhead in software DSM implementations. In an earlier study of the performance of TreadMarks <ref> [15] </ref>, execution times were broken down into various components. Memory management and consistency overhead were shown to account for 3% or less of execution time for all applications. <p> This cost includes the overhead of memory protection operations, page faults as a result of memory protection violations, twinning and diffing, and the maintenance of timestamps and write notices. Earlier work <ref> [15] </ref> has demonstrated that in current networking environments this cost is relatively small compared to the communication overhead. We therefore concentrate on the differences in communication, and refer the reader to our earlier paper [15] for a detailed account of consistency overhead. 3 Methodology We tried to quantify how much each <p> Earlier work <ref> [15] </ref> has demonstrated that in current networking environments this cost is relatively small compared to the communication overhead. We therefore concentrate on the differences in communication, and refer the reader to our earlier paper [15] for a detailed account of consistency overhead. 3 Methodology We tried to quantify how much each of the aforementioned factors contributed to TreadMarks' performance. Three of them are assessed lack of bulk transfer, false sharing, and diff accumulation. <p> In contrast to the work on Munin [4], we use lazy rather than eager release consistency. It has been demonstrated that lazy release consistency leads to lower communication requirements and better performance <ref> [15] </ref>. Furthermore, our study is done on common Unix platforms and using a well-known message passing system. Among the architectural studies comparing message passing and shared memory, we cite two recent articles, namely Chandra et al. [5] and Klaiber and Levy [17].
Reference: [16] <author> P. Keleher, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [30, 4, 16, 21] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations. <p> Since sending messages between workstations is expensive, this extra communication can hurt performance. Much work has been done in the past decade to improve the performance of DSM systems. In this paper, we compare a state-of-the-art DSM system, TreadMarks <ref> [16] </ref>, with the most commonly used message passing system, PVM [9]. Our goals are to assess the differences in programmability and performance between DSM and message passing systems and to precisely determine the remaining causes of the lower performance of DSM systems. <p> Nonblocking receive can be called multiple times to check for the presence of the same message, while performing other work between calls. When there is no more useful work to do, the blocking receive can be called for the same message. 2.2 TreadMarks TreadMarks <ref> [16] </ref> is a software DSM system built at Rice University. It is an efficient user-level DSM system that runs on commonly available Unix systems. We use TreadMarks version 1.0.1 in 5 our experiments. 2.2.1 TreadMarks Interface TreadMarks provides primitives similar to those used in hardware shared memory machines. <p> They have the same syntax as conventional memory allocation calls. With TreadMarks, it is imperative to use explicit synchronization, as data is moved from processor to processor only in response to synchronization calls (see Section 2.2.2). 2.2.2 TreadMarks Implementation TreadMarks uses a lazy invalidate <ref> [16] </ref> version of release consistency (RC) [10] and a multiple-writer protocol [4] to reduce the amount of communication involved in implementing the shared memory abstraction. The virtual memory hardware is used to detect accesses to shared memory. RC is a relaxed memory consistency model.
Reference: [17] <author> A.C Klaiber and H.M. Levy. </author> <title> A comparison of message passing and shared memory architec tures for data parallel languages. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-106, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Furthermore, our study is done on common Unix platforms and using a well-known message passing system. Among the architectural studies comparing message passing and shared memory, we cite two recent articles, namely Chandra et al. [5] and Klaiber and Levy <ref> [17] </ref>. Both of these are simulation studies, while our results are derived from measurements of an implementation. Chandra et al. [5] compares four applications, running either with a user-space message passing or with a full-map 23 invalidate shared memory coherence protocol. <p> For these applications, the software overhead of the message passing layers compensates for the extra communication in the shared memory programs. For their fourth application, extra communication causes shared memory to perform about 50% worse than message passing. Klaiber and Levy <ref> [17] </ref> compare the communication requirements of data-parallel programs on message passing and shared memory machines. We focus instead on execution times, and use the communication requirements as a means to explain the differences in execution times.
Reference: [18] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the 1993 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The results may therefore be influenced by differences in the quality of the code generated by the two compilers. Having recognized the advantages and drawbacks of shared memory and message passing, several groups have recently proposed machine designs that integrate both architectural models <ref> [18, 19, 26] </ref>. Various compiler techniques can also be used to remedy some of the deficiencies of shared memory recognized in this study.
Reference: [19] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The results may therefore be influenced by differences in the quality of the code generated by the two compilers. Having recognized the advantages and drawbacks of shared memory and message passing, several groups have recently proposed machine designs that integrate both architectural models <ref> [18, 19, 26] </ref>. Various compiler techniques can also be used to remedy some of the deficiencies of shared memory recognized in this study.
Reference: [20] <author> G. M. Lathrop, J. M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for multilocus linkage analysis in humans. </title> <booktitle> Proceedings of National Academy of Science, USA, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The number of messages sent in TreadMarks drops to twice that in PVM. Consequently, TreadMarks' speedup increases from 4.72 to 4.99, less than 2% lower than the speedup of 5.12 obtained in PVM. 5.8 ILINK ILINK <ref> [6, 20] </ref> is a widely used genetic linkage analysis program that locates specific disease genes on chromosomes. The input to ILINK consists of several family trees. The program traverses the family trees and visits each nuclear family. The main data structure in ILINK is a pool of genarrays.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year> <month> 26 </month>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [30, 4, 16, 21] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations.
Reference: [22] <author> H. Lu, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> Message passing versus distributed shared memory on networks of workstations. </title> <booktitle> In Proceedings SuperComputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This paper extends the results presented by Lu et al. <ref> [22] </ref>, and quantifies the effect of the last three factors by measuring the performance gain when each factor is eliminated.
Reference: [23] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in message passing and shared memory implementations of a standard cell router. </title> <booktitle> In 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 88-96, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Finally, there have a variety of papers comparing implementations of individual applications in shared memory and message passing, including, e.g., hierarchical N-body simulation [27] and VLSI cell routing <ref> [23] </ref>. 7 Conclusions This paper presents two contributions. First, our results show that, on a large variety of programs, the performance of a well optimized DSM system is comparable to that of a message passing system.
Reference: [24] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <note> version 1.0, </note> <month> May </month> <year> 1994. </year>
Reference-contexts: Currently, the prevailing programming model for parallel computing on this platform is message passing, using libraries such as PVM [9], TCGMSG [11] and Express [25]. A message passing standard MPI <ref> [24] </ref> has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer.
Reference: [25] <institution> Parasoft Corporation, Pasadena, CA. </institution> <note> Express user's guide, version 3.2.5, </note> <year> 1992. </year>
Reference-contexts: Processors in workstation clusters do not share physical memory, so all interprocessor communication must be performed by sending messages over the network. Currently, the prevailing programming model for parallel computing on this platform is message passing, using libraries such as PVM [9], TCGMSG [11] and Express <ref> [25] </ref>. A message passing standard MPI [24] has also been developed. With the message passing paradigm, the distributed nature of the memory system is fully exposed to the application programmer.
Reference: [26] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The results may therefore be influenced by differences in the quality of the code generated by the two compilers. Having recognized the advantages and drawbacks of shared memory and message passing, several groups have recently proposed machine designs that integrate both architectural models <ref> [18, 19, 26] </ref>. Various compiler techniques can also be used to remedy some of the deficiencies of shared memory recognized in this study.
Reference: [27] <author> J.P. Singh, J.L. Hennessy, and A. Gupta. </author> <title> Implications of hierarchical n-body methods for multiprocessor architectures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(2) </volume> <pages> 141-202, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The last 5 iterations are timed in order to exclude any cold start effects. The sequential program runs for 122 seconds. At 8 processors, PVM and TreadMarks achieve speedups of 4.64 and 4.01 respectively. The low computation to communication ratio and the need for fine-grained communication <ref> [27] </ref> contribute to the poor speedups on both TreadMarks and PVM. TreadMarks sends 513 times more messages than PVM at 8 processors. This is the result of both false sharing and multiple diff requests. <p> Finally, there have a variety of papers comparing implementations of individual applications in shared memory and message passing, including, e.g., hierarchical N-body simulation <ref> [27] </ref> and VLSI cell routing [23]. 7 Conclusions This paper presents two contributions. First, our results show that, on a large variety of programs, the performance of a well optimized DSM system is comparable to that of a message passing system.
Reference: [28] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 2-12, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Our goals are to assess the differences in programmability and performance between DSM and message passing systems and to precisely determine the remaining causes of the lower performance of DSM systems. We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite <ref> [28] </ref>; 3-D FFT, Integer Sort (IS), and Embarrassingly Parallel (EP) from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program [8]; and Successive Over-Relaxation (SOR), and Traveling Salesman Problem (TSP). Two different input sets were used for five of the applications. <p> Max bandwidth with copying 7.2 MB/sec. 21.0 MB/sec. Table 1 Characteristics of the Experimental Platforms 4.2 Applications We ported eight parallel programs to both TreadMarks and PVM: Water and Barnes-Hut from the SPLASH benchmark suite <ref> [28] </ref>; 3-D FFT, IS, and EP from the NAS benchmarks [2]; ILINK, a widely used genetic linkage analysis program [8]; and SOR, and TSP. The execution times for the sequential programs, without any calls to PVM or TreadMarks, are shown in Table 2. <p> Diff accumulation accounts for 11% of the data sent in TreadMarks, or 340 kilobytes, but it contributes little to TreadMarks performance. With the high speed networks we use, message size is a secondary factor in deciding communication cost compared with number of messages. 5.5 Water Water from the SPLASH <ref> [28] </ref> benchmark suite is a molecular dynamics simulation. The main data structure in Water is a one-dimensional array of records, in which each record represents a molecule. It contains the molecule's center of mass, and for each of the atoms, the computed forces, the displacements and their first six derivatives. <p> only constitutes 8% of the messages sent in TreadMarks, and has little effect on TreadMarks' performance. 18 Although diff accumulation is responsible for 47% of the total data sent under TreadMarks, the performance is hardly affected because of the high computation to communication ratio. 5.6 Barnes-Hut Barnes-Hut from the SPLASH <ref> [28] </ref> benchmark suite is an N-body simulation using the hierarchical Barnes-Hut Method. A tree-structured hierarchical representation of physical space is used. Each leaf of the tree represents a body, and each internal node of the tree represents a "cell", a collection of bodies in close physical proximity.
Reference: [29] <author> W.-D. Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In the current implementation of TreadMarks, diff accumulation occurs for migratory data. 7 Migratory data is shared sequentially by a set of processors <ref> [3, 29] </ref>. Each processor has exclusive read and write access for a time. Accesses to migratory data are protected by locks in TreadMarks. Each time a processor accesses migratory data, it must see all the preceding modifications.
Reference: [30] <author> M.J. Zekauskas, </author> <title> W.A. Sawdon, and B.N. Bershad. Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: The programmer needs to keep in mind where the data is, decide when to communicate with other processors, whom to communicate with, and what to communicate, making it hard to program in message passing, especially for applications with complex data structures. Software distributed shared memory (DSM) systems (e.g., <ref> [30, 4, 16, 21] </ref>) provide a shared memory abstraction on top of the native message passing facilities. An application can be written as if it were executing on a shared memory multiprocessor, accessing shared data with ordinary read and write operations.
References-found: 30

