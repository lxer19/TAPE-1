URL: http://www.cs.monash.edu.au/~rohan/PAPERS/lp.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rohan@cs.monash.edu.au  
Phone: fax: +61-3-9905-5146  
Title: The Likelihood Principle and MML Estimators  
Author: Rohan A. Baxter 
Keyword: MML, Likelihood Principle, Minimum Encoding Length Estimators, Bayesian Point Estimation. Area of Interest: Minimum Encoding Length Inference Methods.  
Address: 3168, Australia  
Affiliation: Department of Computer Science Monash University ph: +61-3-9905-5721 Clayton, Vic.  
Abstract: The minimum message length (MML) family of estimators are discrete Bayesian point estimators which violate the Likelihood Principle. The nature of the breach is examined and examples are given. MML proponents have claimed that the violation is an `innocent' one. This claim is examined. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: MML approximations, other than Equation (3), have been developed where the assumptions stated above do not hold [7]. 1.5 The Likelihood Principle The LP states that an estimator should be dependent only on the observed data, rather than data not seen <ref> [1, 2] </ref>. The LP has been viewed as desirable using examples where an estimate, ^ , changes when x has not changed, but X has. <p> We will briefly mention one other possible, more limited, interpretation. In this interpretation, MML estimators do not breach the LP. Berger has warned against too broad an application of the LP <ref> [1, page 29] </ref>: A second point is that the Likelihood Principle does not say that all information about is contained in f (xj), just that all experimental information is. There may well be other information relevant to the statistical analysis, such as prior in formation or considerations of loss. <p> The likelihood function is [2, page 248]: f bin (xj; n) = n ! The negative binomial model stops the experiment after x successes, so n varies, and the likelihood function is: f negbin (nj; x) = x 1 x (1 ) nx (6) Following Berger <ref> [1, page 503] </ref> (see also [2, page 248-249]): The binomial and negative binomial experiments can be interpreted as sequential experiments involving independent Bernoulli (B (1; )) trials; the binomial experiment would arise from using the fixed sample size stopping rule which stops after 12 observations, while the negative binomial experiment would <p> A classic example is estimation of the mean and variance of a Gaussian distribution. The data is hypothetically obtained from an experiment carried out using a measuring device <ref> [1] </ref>. The experimenter is imagined to have measuring devices with differing ranges (e.g. voltmeters ). The question is then whether the different data spaces, corresponding to the different measuring devices, will affect the MML estimates. We assume that the prior is unaffected by a change in data space. <p> Estimators adhering to the LP will have the same estimate, whether voltmeter A or B was used. In our example (not based on voltmeters), we now consider the following data spaces.The first, X A , has the usual range x 2 <ref> [1; 1] </ref>. The second, X B , has the range x 2 [L; U ]. 3.1 MML Estimators We now show that restricting the data space, X, does not change the MML estimate.
Reference: [2] <author> J.M. Bernardo and A.F.M. Smith. </author> <title> Bayesian Theory. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: MML approximations, other than Equation (3), have been developed where the assumptions stated above do not hold [7]. 1.5 The Likelihood Principle The LP states that an estimator should be dependent only on the observed data, rather than data not seen <ref> [1, 2] </ref>. The LP has been viewed as desirable using examples where an estimate, ^ , changes when x has not changed, but X has. <p> The likelihood function is <ref> [2, page 248] </ref>: f bin (xj; n) = n ! The negative binomial model stops the experiment after x successes, so n varies, and the likelihood function is: f negbin (nj; x) = x 1 x (1 ) nx (6) Following Berger [1, page 503] (see also [2, page 248-249]): The <p> likelihood function is [2, page 248]: f bin (xj; n) = n ! The negative binomial model stops the experiment after x successes, so n varies, and the likelihood function is: f negbin (nj; x) = x 1 x (1 ) nx (6) Following Berger [1, page 503] (see also <ref> [2, page 248-249] </ref>): The binomial and negative binomial experiments can be interpreted as sequential experiments involving independent Bernoulli (B (1; )) trials; the binomial experiment would arise from using the fixed sample size stopping rule which stops after 12 observations, while the negative binomial experiment would arise from the stopping rule
Reference: [3] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> N.J., </address> <year> 1989. </year>
Reference-contexts: This is always greater than the optimal one-part code length, the Stochas--tic Complexity (SC), I sc = P <ref> [3] </ref>. Note that SC differs from SMML because it involves choosing a fi k from a set of parameter spaces, fi 1 ; :::; fi K .
Reference: [4] <author> C.S. Wallace. </author> <title> False oracles and SMML estimators. </title> <type> Technical Report 89/128, </type> <institution> Dept. of Computer Science,Monash University, </institution> <month> June </month> <year> 1989. </year> <note> (Also appears in these ISIS proceedings). </note>
Reference-contexts: the data using the estimate. 1.2 SMML Estimators The Strict MML (SMML) estimator ^ = m (x) has a range fi fl = f ^ i ; i = 1; 2; :::g = f ^ : ^ = m (x); x 2 Xg which is a countable subset of fi <ref> [6, 4] </ref>. Define X i = x : m (x) = ^ i and the prior probability of an estimate, q i = q ( ^ i ) = P X i r (x) for all i. <p> For simplicity, we shall reduce the problem to estimating the mean only, however the argument will still hold for both parameters. We consider a data space, x 2 [L; U ]. Following Wallace <ref> [4] </ref>, we note that the range of m (x) in this problem is a set of equally-spaced values of ^ , except at the very edges of the range of X. Wallace calculates the spacing to be ffi = p .
Reference: [5] <author> C.S. Wallace. </author> <note> Information theory lecture notes (chapter 2). </note> <institution> Dept. of Computer Science, Monash University, Clayton 3168, Australia, </institution> <year> 1993. </year>
Reference-contexts: The SMML mapping m () is difficult to work with. A more convenient, but sub-optimal (hence I fsmml I smml ), approach is to partition the parameter space, fi, into regions <ref> [6, 5] </ref>. Each region j is represented by an estimate ^ j . The prior probability of ^ j is q j = R 2region j h ()d. <p> Each region j is represented by an estimate ^ j . The prior probability of ^ j is q j = R 2region j h ()d. FSMML minimizes the following expected message length expression <ref> [5] </ref>: I fsmml = X Z 2region j h ()[ log (q ( ^ j )) X f (xj) log (f (xj ^ j ))]d (2) Equation (2) can be used to search for an optimal FSMML estimator.
Reference: [6] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> J. R. Statist. Soc B, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: In the final section, we draw some conclusions on the `innocence' or otherwise of MML estimators breaching the LP. 1.1 Bayesian Framework and MML MML is a general method of statistical point estimation and inference <ref> [6] </ref>. MML estimators apply to the following Bayesian framework. <p> the data using the estimate. 1.2 SMML Estimators The Strict MML (SMML) estimator ^ = m (x) has a range fi fl = f ^ i ; i = 1; 2; :::g = f ^ : ^ = m (x); x 2 Xg which is a countable subset of fi <ref> [6, 4] </ref>. Define X i = x : m (x) = ^ i and the prior probability of an estimate, q i = q ( ^ i ) = P X i r (x) for all i. <p> The SMML mapping m () is difficult to work with. A more convenient, but sub-optimal (hence I fsmml I smml ), approach is to partition the parameter space, fi, into regions <ref> [6, 5] </ref>. Each region j is represented by an estimate ^ j . The prior probability of ^ j is q j = R 2region j h ()d. <p> The MML estimate is found by minimizing <ref> [6] </ref>: h ( ^ ) 1 log f (xj ^ ) + 2 where jF ()j is the determinant of the expected Fisher Information Matrix and d is a constant describing the efficiency of the optimal quantizing lattice in d dimensions. <p> This is the case in the examples that follow in the next two sections. 1.6 An `innocent little' Violation of the LP? The original proponents of the SMML and MML estimators, Wallace and Freeman <ref> [6, page 264] </ref> have commented: The resulting attractive features [of MML]... seem sufficient for an innocent little violation of the likelihood principle. Attractive features of MML estimators include invariance, generality and consistency. The argument is that these are a sufficient trade-off for the downside of breaching the LP.
Reference: [7] <author> C.S. Wallace and P.R. Freeman. </author> <title> Single factor analysis by MML estimation. </title> <journal> J. R. Statist. Soc. B., </journal> <volume> 54(1) </volume> <pages> 185-209, </pages> <year> 1992. </year>
Reference-contexts: The MML approximation makes this approach to minimum encoding inference practical for vector-valued parameters. MML approximations, other than Equation (3), have been developed where the assumptions stated above do not hold <ref> [7] </ref>. 1.5 The Likelihood Principle The LP states that an estimator should be dependent only on the observed data, rather than data not seen [1, 2]. The LP has been viewed as desirable using examples where an estimate, ^ , changes when x has not changed, but X has.
References-found: 7

