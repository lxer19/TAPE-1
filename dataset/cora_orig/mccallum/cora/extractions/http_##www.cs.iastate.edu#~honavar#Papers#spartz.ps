URL: http://www.cs.iastate.edu/~honavar/Papers/spartz.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: AN EMPIRICAL ANALYSIS OF THE EXPECTED SOURCE VALUES RULE  
Author: Richard Spartz Vasant Honavar 
Address: Ames, IA 50010  
Affiliation: Department of Computer Science Iowa State University  
Abstract: Despite its notoriously slow learning time, backpropagation (BP) is one of the most widely used neural network training algorithms. Two major reasons for this slow convergence are the step size problem nd the flat spot problem [Fahlman, 1988]. In [Samad, 1991] a simple modification, the expected source values (ESV) rule, is proposed for speeding up the BP algorithm. We have extended the ESV rule by coupling it with a flat-spot removal strategy presented in [Fahlman, 1988], as well as incorporating a momentum term to combat the step size problem. The resulting rule has shown dramatically improved learning time over standard BP, measured in training epochs. Two versions of the ESV modification are mentioned in [Samad, 1991], on-demand and up-front. but simulation results are given mostly for the on-demand case. Our results indicate that the up-front version works somewhat better than the on-demand version in terms of learning speed. We have also analyzed the interactions between the three modifications as they are used in various combinations. 
Abstract-found: 1
Intro-found: 1
Reference: [Balakrishnan & Honavar, 1992] <author> Balakrishnan, K., & V. Honavar. </author> <year> 1992. </year> <title> "Improving Convergence of Backpropagation by Handling Flat-spots in the Output Layer." </title> <booktitle> In Proceedings of the Second International Conference on Artificial Neural Networks. </booktitle> <address> Brighton, U.K. </address>
Reference-contexts: We will call this the sigmoid-prime offset (SO) rule. An alternative solution to the flat spots problem has been explored in <ref> [Balakrishnan & Honavar, 1992] </ref>. 3.3 The Expected Source Values Modification Equation (3) decreases the sum-squared error after each update, driving the error toward zero. Let us look at the calculation of ffi j .
Reference: [Fahlman, 1988] <author> Fahlman, Scott E. </author> <year> 1988. </year> <title> "An Empirical Study of Learning Speed in Back-Propagation Networks." </title> <type> Technical Report CMU-CS-88-162. </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Although mathematically sound, the learning rule suffers from sometimes excrutiatingly slow convergence. Two factors contributing to the slow convergence of BP are the step size problem and flat spots in the sigmoid function where the derivative is near zero <ref> [Fahlman, 1988] </ref>. In section 3 we define these problems and give two modifications to BP to help alleviate them. Section 3 also presents the expected source values (ESV) modification. <p> This may cause the unit to be "stuck" in the flat spot for several iterations. One solution to the flat spots problem is to add on offset to the sigmoid-prime function to ensure that it does not approach zero <ref> [Fahlman, 1988] </ref>. The resulting weight update rule is the same as equation (3), but we use o 0 k = o k (1o k )+os in equation (4) to calculate the error ffi k for any unit k, where os = 0.0 or 0.1. <p> We chose these data sets because they are popular benchmarks in connectionist research <ref> [Rumelhart et al., 1986, Fahlman, 1988, Yang & Honavar, 1991] </ref>, thus giving many previous results to compare our results against. <p> For each tuple of , ff, and fi, the network was trained until it correctly classified a given percentage of the training patterns (success) or a specified number of training epochs were performed (failure). We used two different metrics of correct classification: Threshold and margin metric: <ref> [Fahlman, 1988] </ref>, the network is said to have correctly classified a training pattern if the actual network output is within a specified threshold for each of the components of the output vector; we used a threshold of 0.25.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E. G. E. Hinton, and R. J. Williams. </author> <year> 1986. </year> <title> "Learning Internal Representations by Error Propagation." </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> D. E. Rumelhart and J. L. McClelland, eds. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: 1 INTRODUCTION The generalized delta rule <ref> [Rumelhart et al., 1986] </ref>, also known as the back-propagation (BP) learning algorithm, is one of the most popular multilayer neural network training algorithms. Although mathematically sound, the learning rule suffers from sometimes excrutiatingly slow convergence. <p> Section 4 explains the data sets and experimental method used to evaluate the rules. Section 5 presents our simulation results and section 6 gives the conclusions drawn from this study, as well as some possible directions for future work. 2 THE BACK PROPAGATION ALGORITHM The BP algorithm <ref> [Rumelhart et al., 1986] </ref>, is a supervised learning algorithm for feedforward neural networks. A feedforward neural network consists of two or more layers of processing units: the input layer, the output layer, and possibly intermediate layers called hidden layers. <p> See <ref> [Rumelhart et al., 1986] </ref> for the derivation of the above formulas. 2.3 Training Algorithm The BP learning algorithm proceeds as follows. Each component of the input vector is mapped to the activation of the corresponding input unit. <p> If the step size is set too large, however, the system error tends to oscillate, never reaching a minimum. A simple way to help alleviate the step size problem is to use a momentum term <ref> [Rumelhart et al., 1986] </ref>. The mo mentum term adds a fraction of the previous weight change to the current weight adjustment value. <p> We chose these data sets because they are popular benchmarks in connectionist research <ref> [Rumelhart et al., 1986, Fahlman, 1988, Yang & Honavar, 1991] </ref>, thus giving many previous results to compare our results against.
Reference: [Samad, 1991] <author> Samad, T. </author> <year> 1991. </year> <title> "Back Propagation With Expected Source Values." </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 4: </volume> <pages> 615-618. </pages>
Reference-contexts: This leads us to the following modified BP rule <ref> [Samad, 1991] </ref>: w ji = (o i + fiffi i )ffi j (6) where fi is a constant. The new rule reverts back to the original rule when fi equals zero or when i is an input unit (ffi i = 0.0). <p> For any application of the BP rule, we recommend the use of all three modifications. The computational overhead is small, and the gain in learning speed outweighs this small overhead. We need to further understand the ESV modification. It is not clear from the intuitive argument given in <ref> [Samad, 1991] </ref> why we get the improvements that were observed. Further empirical and theoretical investigation is needed to determine whether the rule gives better performance simply due to roughly and dynamically adjusting the step size, or if it is approximating a more useful weight update value.
Reference: [Yang & Honavar, 1991] <author> Yang, Jihoon, and V. Honavar. </author> <title> "Experiments with the Cascade-Correlation Algorithm." </title> <booktitle> In Proceedings of the 4th UNB Artificial Intelligence Symposium. </booktitle> <address> Fredericton, NB, Canada. </address>
Reference-contexts: We chose these data sets because they are popular benchmarks in connectionist research <ref> [Rumelhart et al., 1986, Fahlman, 1988, Yang & Honavar, 1991] </ref>, thus giving many previous results to compare our results against. <p> Iris Classification: 22-4-3, This data set is a real classification problem. It consists of 150 examples of iris plant samples which need to be classified into one of three species. This is the same data set as used in <ref> [Yang & Honavar, 1991] </ref>. Soybean Classification: 208-23-17, This data set is also a real classification problem. It consists of 289 examples of soybean plants with one of 17 diseases. This is the same data set as used in [Yang & Honavar, 1991]. 4.2 Training Criteria In order to address the goals <p> This is the same data set as used in <ref> [Yang & Honavar, 1991] </ref>. Soybean Classification: 208-23-17, This data set is also a real classification problem. It consists of 289 examples of soybean plants with one of 17 diseases. This is the same data set as used in [Yang & Honavar, 1991]. 4.2 Training Criteria In order to address the goals of this study, we need to test the learning rule on all six data sets for a range of param Data Set Rule fi Ave. <p> Maximum output metric: <ref> [Yang & Honavar, 1991] </ref>, the network is said to have correctly classified a training pattern if the output unit with maximum activation corresponds to the position of the maximum value in the target pattern.
References-found: 5

