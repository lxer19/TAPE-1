URL: http://www.isi.edu/~rdv/netstation/nap-research/nap-research/nap-research-labelled.ps
Refering-URL: http://www.isi.edu/~rdv/netstation/nap-research/index.html
Root-URL: http://www.isi.edu
Email: rdv@ISI.Edu  
Title: on Network Attached Peripherals  change. Not yet for public distribution.  
Author: Rodney Doyle Van Meter III 
Date: January 19, 1996  
Address: Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Note: A Brief Survey of Current Work  Note: This is a preliminary version of in-progress, unreviewed and incomplete work. Data, conclusions and verbiage may all  
Abstract: An extended abstract of this paper appeared in ACM Operating Systems Review, Jan. 1996 Abstract Work on network-attached peripherals (NAPs) can be divided into essentially three areas device interfaces and protocols, multimedia use and mass storage use. This paper is an extended abstract reviewing some of the current work and provides references and WWW pointers to many of the projects. The impact of this technological advance on operating systems is discussed. The primary purpose of this paper is to broaden understanding of the advantages and pitfalls of NAPs and encourage further research in the design and use of network-attached peripherals and NAP-capable systems. This paper 1 and an extended abstract are available on the web or from the author. 2 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> File server manages HPC networks. Parallel and Distributed Technology, 2(4):90, </institution> <year> 1994. </year> <title> Maximum Strategy product announcement in New Products section. </title>
Reference-contexts: The obvious example is a special-purpose network node that provide NFS (Network File System)[58, 13] services only no general-purpose computing facilities. Examples include the Parity Systems Etherstore [48], Auspex NS7000 3 , Network Appliance 4 [31] and the Maximum Strategy proFILE XL RAID array <ref> [1] </ref>. However, the high-level 3 http://www.auspex.com/ 4 http://www.netapp.com/ protocol spoken by these is NFS, which provides file-oriented service, an operating system dependent interface. Thus, they would qualify as file servers rather than network-attached peripherals. <p> There may also be additional information on this topic in the comp.arch.storage Frequently Asked Questions (FAQ) 37 . * Maximum Strategy 38 makes high-speed supercomputer RAID arrays that speak either IPI-3 block protocol or NFS over HiPPI, ATM or Fi bre Channel <ref> [1] </ref>. * HiPPI tape drives available include the Datat-ape ID-1 and Sony ID-1 with a Triplex interface.
Reference: [2] <author> J. F. Adam, H. H. Houh, M. Ismert, and D. L. </author> <title> Tennenhouse. </title> <journal> Media-intensive data communications in a "desk-area" network. IEEE Communications, </journal> <pages> pages 60-67, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Capture of video to disk and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT <ref> [43, 32, 2] </ref>. * The Desk Area Network work is being done at Cambridge [8, 40, 29].
Reference: [3] <author> ANSI. </author> <title> High-performance parallel interface - framing protocol (HIPPI-FP). </title> <note> Technical Report X3T9.3/89-013 rev 4.2, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: Most of these network technologies are intended to be used primarily in homogeneous environments, although some level of interoperation is supported. For example, both the HiPPI framing protocol <ref> [3] </ref> (which defines the packet format) and ATM have defined mappings on top of a Fibre Channel physical connection. Thus, care must be taken when referring to the higher-level protocols to distinguish them from the physical interconnects. Figure 1 shows some possible methods of using an IPI-3 upper-level protocol.
Reference: [4] <author> ANSI. </author> <title> High-performance parallel interface - mechanical, electrical and signalling protocol specification (HIPPI-PH). </title> <note> Technical Report X3T9.3/88-023 rev 8.1, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: A parallel copper connection can be switched over computer-room distances, or used as a simple channel. Often used in conjunction with an ethernet for a back channel or control network, as HiPPI interfaces are unidirectional <ref> [4, 55] </ref>. HiPPI can also carry TCP/IP network as a high-speed LAN. There is some discussion now of improving transfer rates to 1 GB/s with a growth path to 10 GB/s. IBM's Serial Storage Architecture (SSA) is a relatively new interface.
Reference: [5] <author> ANSI. </author> <title> Information systems fibre channel protocol for SCSI (FCP). </title> <type> Technical Report TR X3T10-993D R010, </type> <institution> ANSI, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: A disadvantage of this is the potentially long latency to begin a transfer. Others use IPI-3 or a particular SCSI protocol (such as the Fibre Channel Protocol, FCP <ref> [5] </ref> as the transport protocol to insure the fidelity and ordering of data as it is sent through the network. This is especially important for transfers that exceed the network maximum transfer unit (MTU).
Reference: [6] <author> ANSI. </author> <title> Information technology - SCSI-3 architecture model. </title> <type> Technical Report TR X3T10-994D R17, </type> <institution> ANSI, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: These are all block-oriented protocols. IPI-3 provides transport up through application layers in the ISO networking model. SCSI relies on the lower-level network interfaces to provide some of these services. SCSI grew into what is known as SCSI-3 <ref> [6] </ref> partially as a result of the desire to use SCSI for NAPs. It has been suggested [66] that certain aspects of the SCSI model have shortcomings from a networking point of view. <p> Access rights were assumed, and the semantics of, for example, the SCSI RESERVE command <ref> [6] </ref> are of the shared/exclusive read/write variety.
Reference: [7] <author> ANSI. </author> <title> Information technology - SCSI-3 primary commands. </title> <type> Technical Report TR X3T10-995D R6, </type> <institution> ANSI, </institution> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Thus, the protocols used at the devices themselves have been drawn mostly from SCSI and IPI-3. Historically IPI-3 has been preferred, but SCSI is becoming increasingly common. SCSI as a command protocol (SPC <ref> [7] </ref>, the SCSI Primary Commands, are common to all SCSI devices, plus each device type has a type-specific set such as the SCSI Block Commands for disk drives) runs over SSA, Fibre Channel and Serial Bus. IPI-3 is commonly used over HiPPI, and runs over Fibre Channel as well.
Reference: [8] <author> P. Barham, M. Hayter, D. McAuley, and I. Pratt. </author> <title> Devices on the desk area network. </title> <journal> J. Selected Areas in Communications, </journal> <volume> 13(4) </volume> <pages> 722-732, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT [43, 32, 2]. * The Desk Area Network work is being done at Cambridge <ref> [8, 40, 29] </ref>.
Reference: [9] <author> L. Berdahl. </author> <title> Parallel transport protocol proposal. </title> <institution> Lawrence Livermore National Labs, </institution> <note> January 3, 1995. Draft. ftp://svr4.nersc.gov/pub/Pio-1-3-95.ps. </note>
Reference-contexts: In addition to supporting parallel file systems (see section 6 below) via concurrent transfers from separate devices to separate compute nodes, it is possible to transfer data in parallel between two endpoints, if two or more paths between the nodes exist [64]. The Parallel Transport Protocol proposal <ref> [9] </ref> provides a means for specifying logically concurrent transfers between groups of NAPs and mapping data from N sources to M sinks. Jain et al propose graph coloring as a means for optimizing the use of sources and sinks in concurrent transfers [36]. <p> This is quite probably the most advanced work on network attached peripherals, and has been tackling the important issues of security and parallel transfers. The Parallel Transport Protocol proposal <ref> [9] </ref> is related to HPSS. There is some related work in the Sequoia 2000 27 project on the network aspects 28 of mass storage and especially operating system I/O. SIOF 29 , the Scalable I/O Facility is targetting I/O for massively parallel supercomputers. SIOF will also use HPSS.
Reference: [10] <author> R. Bordawekar, A. Choudhary, and J. M. del Rosario. </author> <title> An experimental performance evaluation of touchstone delta concurrent file system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Also relevant is the work on operating systems for distributed-memory multicomputers [63], such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance <ref> [10] </ref> and file system design [46, 20, 17, 19] have been done. Some of this work includes, for example, distributed file block layout and synchronization mechanisms that may prove useful for NAP file systems.
Reference: [11] <author> U. M. Borghoff. </author> <title> Design of optimal distributed file systems: A framework for research. </title> <journal> ACM Operating Systems Review, </journal> <volume> 26(4) </volume> <pages> 30-61, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: As a distributed system, the existing body of research on issues such as resource control and deadlock, naming, caching, etc. is all relevant. Especially important is the work on distributed file systems <ref> [41, 67, 47, 24, 11, 67] </ref>. An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients.
Reference: [12] <author> L.-F. Cabrera and D. D. E. Long. Swift: </author> <title> Using distributed disk striping to provide high i/o data rates. </title> <journal> Computing Systems, </journal> <volume> 4(4) </volume> <pages> 405-436, </pages> <year> 1991. </year>
Reference-contexts: In this case, the server manages the data and initiates transfers, but need not be in the data path, a canonical example of third-party transfers and the uses of network-attached peripherals. The Swift distributed RAID array <ref> [12, 44] </ref> was the first project to propose striping of data across multiple network connections as an alternative to striping on local disks. Their approach involves creating transfer plans to support the striping. The TickerTAIP distributed RAID array [14] is composed of network-attached disks.
Reference: [13] <author> B. Callaghan, B. Pawlowski, and P. Staubach. </author> <title> NFS version 3 protocol specification, </title> <month> June </month> <year> 1995. </year>
Reference: [14] <author> P. Cao, S. B. Lim, S. Venkataraman, and J. Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proc. 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 52-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Jain et al propose graph coloring as a means for optimizing the use of sources and sinks in concurrent transfers [36]. It is also possible (Zebra) to utilize multiple servers for a single client, similar to a parallel file system or distributed RAID array. The TickerTAIP <ref> [14] </ref> distributed array transfers to and from the network-attached disks in parallel. 4 NAP Multimedia Research Several research projects concerning using network-attached peripherals in multimedia workstations are ongoing in various universities. <p> The Swift distributed RAID array [12, 44] was the first project to propose striping of data across multiple network connections as an alternative to striping on local disks. Their approach involves creating transfer plans to support the striping. The TickerTAIP distributed RAID array <ref> [14] </ref> is composed of network-attached disks. It represents important work in calculation and management of distributed parity, especially for small writes. As covered in section 3.2, Cray has implemented a Shared File System for a HiPPI RAID array.
Reference: [15] <editor> S. Coleman, editor. </editor> <booktitle> Twelfth IEEE Symposium on Mass Storage Systems, </booktitle> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: The key conferences on this topic are hosted by the IEEE Computer Society <ref> [15] </ref> and NASA's Goddard Space Flight Center [38]. NAPs in mass storage are used in hierarchical storage management (HSM) systems, as well as with channel extenders for remote copying of data.
Reference: [16] <author> S. S. Coleman and R. W. Watson. </author> <title> The emerging paradigm shift in storage system architectures. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <pages> pages 607-620, </pages> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: NAPs have been used in hierarchical storage systems for a number of years (primarily HiPPI disk arrays), but the increasing speed and sophistication of both the peripherals and the HSM software has truly brought NAPs to the forefront recently. See Coleman and Watson <ref> [16] </ref> for a good introduction to HSM (and good references on the history of network-attached peripherals). The SSSWG 24 is the IEEE's Storage Systems Standards Working Group.
Reference: [17] <author> P. F. Corbett, S. J. Baylor, and D. G. Feitel-son. </author> <title> Overview of the vesta parallel file system. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 7-14, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Also relevant is the work on operating systems for distributed-memory multicomputers [63], such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance [10] and file system design <ref> [46, 20, 17, 19] </ref> have been done. Some of this work includes, for example, distributed file block layout and synchronization mechanisms that may prove useful for NAP file systems. A key resource for research in this area is David Kotz's excellent page on parallel I/O 35 .
Reference: [18] <author> R. Cummings. </author> <title> System architectures using fibre channel. </title> <booktitle> In Coleman [15], </booktitle> <pages> pages 251-256. </pages>
Reference: [19] <author> E. P. DeBenedictis and J. M. del Rosario. </author> <title> Modular scalable i/o. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 17 </volume> <pages> 122-128, </pages> <year> 1993. </year>
Reference-contexts: Also relevant is the work on operating systems for distributed-memory multicomputers [63], such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance [10] and file system design <ref> [46, 20, 17, 19] </ref> have been done. Some of this work includes, for example, distributed file block layout and synchronization mechanisms that may prove useful for NAP file systems. A key resource for research in this area is David Kotz's excellent page on parallel I/O 35 .
Reference: [20] <author> P. C. Dibble and M. L. Scott. </author> <title> Beyond striping: The Bridge multiprocessor file system. </title> <journal> Computer Architecture News, </journal> <volume> 19(5), </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Also relevant is the work on operating systems for distributed-memory multicomputers [63], such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance [10] and file system design <ref> [46, 20, 17, 19] </ref> have been done. Some of this work includes, for example, distributed file block layout and synchronization mechanisms that may prove useful for NAP file systems. A key resource for research in this area is David Kotz's excellent page on parallel I/O 35 .
Reference: [21] <author> A. L. Drapeau, K. W. Shirrif, J. H. Hartman, E. L. Miller, S. Seshan, R. H. Katz, K. Lutz, D. A. Patterson, E. K. Lee, P. H. Chen, and G. A. Gibson. </author> <title> RAID-II: a high-bandwidth network file server. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 234-244, </pages> <year> 1994. </year>
Reference-contexts: The HSC, CDC, and MaxStrat controllers are clearly block servers, and the Auspex clearly is a file server; the RAID-II is more of a hybrid system. The RAID-II system developed at UC Berkeley <ref> [21] </ref> blurs the distinction between network-attached RAID array and file server.
Reference: [22] <author> R. Felderman, A. DeSchon, D. Cohen, and G. Finn. </author> <title> ATOMIC: A high speed local communication architecture. </title> <journal> J. High Speed Networks, </journal> <volume> 3(1) </volume> <pages> 1-29, </pages> <year> 1994. </year>
Reference-contexts: Variants of ATM [54] networks are in use for the Viewstation and Desk Area Network research. 9 http://www.microp.com/SSA.html 10 http://www.ssaia.org/ 11 http://www1.cern.ch/HSI/fcs/fcs.html 12 http://www.amdahl.com/ext/CARP/FCA/FCA.html 13 http://www.amdahl.com/ext/CARP/SBCON/SBCON.html 14 http://firewire.org/ 3 Our own Netstation re-search is using the ATOMIC 15 high-speed switched local area network <ref> [22] </ref>, originating in an interconnect technology for massively parallel computers and being commercialized as Myrinet 16 . The Digital VAXcluster CI and star coupler [39] and the UltraNet are currently in only limited use, due to the aging of the technology.
Reference: [23] <author> G. Finn. </author> <title> An integration of network communication with workstation architecture. </title> <journal> ACM Computer Communication Review, </journal> <month> Oct. </month> <year> 1991. </year> <note> Available online at ftp://venera.isi.edu/atomic-doc/ATOMIC.Netstation.ps. </note>
Reference-contexts: Capture of video to disk and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI <ref> [23, 66] </ref>. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT [43, 32, 2]. * The Desk Area Network work is being done at Cambridge [8, 40, 29].
Reference: [24] <author> M. Fridrich and W. </author> <title> Older. Helix: The architecture of the XMS distributed file system. </title> <journal> IEEE Software, </journal> <pages> pages 21-29, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: As a distributed system, the existing body of research on issues such as resource control and deadlock, naming, caching, etc. is all relevant. Especially important is the work on distributed file systems <ref> [41, 67, 47, 24, 11, 67] </ref>. An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients.
Reference: [25] <author> G. Gibson. </author> <title> Secure distributed and parallel file systems based on network-attached autonomous disk drives. </title> <type> White paper, </type> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: They have proposed more object-oriented semantics for the network node, rearchitecting the file system by moving the file system/device level boundary to take advantage of the strengths of NAPs. One possibility is to have the disk drive store objects, reached via the triple &lt;objectid,offset,length&gt; <ref> [25] </ref>, reminiscent of but more advanced than &lt;count,key,data&gt; mainframe disk drives.
Reference: [26] <author> R. Gopalakrishnan and A. D. Bovopolous. </author> <title> A protocol processing architecture for networked multimedia computers. </title> <journal> ACM Operating Systems Review, </journal> <volume> 27(3) </volume> <pages> 19-33, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Some of this work has now been commercialized by Nemesys 23 . * Symphony <ref> [26] </ref> is concerned with intra-node hardware and software architectures to support real-time network protocols. * At Los Alamos National Laboratory, an experimental system that drives a HiPPI frame buffer from a farm of Alpha workstations has been built.[62] 5 NAPs in Mass Storage "Mass storage" in the context of this section
Reference: [27] <author> J. H. Hartman and J. K. Ousterhout. </author> <title> The zebra striped network file system. </title> <journal> ACM Trans. Com-put. Syst., </journal> <volume> 13(3) </volume> <pages> 274-310, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Such protection is inadequate; the drive must confirm that the request is from the system kernel or file manager process (or a party authorized by them). A cryptographic exchange to confirm the identity of requestor may be necessary. The Zebra striped network file system <ref> [27] </ref> achieves its level of protection by allowing writes to the storage server to append to the log without authentication. However, the written blocks do not become part of the visible file system until the file metadata has been updated by the file manager process, which performs appropriate permission checks.
Reference: [28] <author> A. Hawes. </author> <title> Serial storage architecture: A low-cost, high-speed serial connection for disk subsystems. </title> <type> Technical report, </type> <institution> IBM, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Most of the other interconnects under discussion here, including Myrinet, HiPPI, Fibre Channel fabrics and ATM, are switched networks that allow multiple full-bandwidth transfers to execute concurrently. SSA even in its loop form supports some spatial reuse <ref> [28] </ref>. Transfers that do not pass through the same loop node can run independently.
Reference: [29] <author> M. Hayter and D. McAuley. </author> <title> The desk area network. </title> <journal> ACM Operating Systems Review, </journal> <volume> 25(4) </volume> <pages> 14-21, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT [43, 32, 2]. * The Desk Area Network work is being done at Cambridge <ref> [8, 40, 29] </ref>.
Reference: [30] <author> R. G. Herrtwich. </author> <title> Summary of the second international workshop on network adn operating system support for digital audio and video. </title> <journal> ACM Operating Systems Review, </journal> <volume> 26(2) </volume> <pages> 32-59, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: There is a large body of work on network and operating systems support for multimedia, notably the workshops on digital audio and video <ref> [30] </ref>. Also see papers such as [57] . These have not focussed specifically on NAPs, but the principles are important. 36 http://www.pegasus.esprit.ec.org/papers/pegpapers.html 10 7 Existing Network Attached Peripherals Numerous network-attached peripherals using IPI-3 over HiPPI already exist and are in production use, primarily in supercomputing environments.
Reference: [31] <author> D. Hitz. </author> <title> An NFS file server appliance. </title> <type> Technical Report 3001 Rev. </type> <institution> B, Network Appliance, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: The obvious example is a special-purpose network node that provide NFS (Network File System)[58, 13] services only no general-purpose computing facilities. Examples include the Parity Systems Etherstore [48], Auspex NS7000 3 , Network Appliance 4 <ref> [31] </ref> and the Maximum Strategy proFILE XL RAID array [1]. However, the high-level 3 http://www.auspex.com/ 4 http://www.netapp.com/ protocol spoken by these is NFS, which provides file-oriented service, an operating system dependent interface. Thus, they would qualify as file servers rather than network-attached peripherals.
Reference: [32] <author> H. H. Houh, J. F. Adam, M. Ismert, C. J. Lind-blad, and D. L. Tennenhouse. </author> <title> The vunet desk area network: Architecture, implementation and experience. </title> <journal> J. Selected Areas in Communications, </journal> <volume> 13 </volume> <pages> 710-721, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Capture of video to disk and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT <ref> [43, 32, 2] </ref>. * The Desk Area Network work is being done at Cambridge [8, 40, 29].
Reference: [33] <author> R. Hyer, R. Ruef, and R. W. Watson. </author> <title> High-performance data transfers using network-attached peripherals at the national storage laboratory. </title> <booktitle> In Coleman [15], </booktitle> <pages> pages 275-284. </pages>
Reference-contexts: Data does not have to transit the host's bus or be copied by the host; it transfers directly from the disk drive to the tape drive through the network. Hyer et all <ref> [33] </ref> discuss the hazards of getting a NAP to cooperate with an existing system, pointing out flaws in IPI-3 that make it unsuitable for third-party use, especially the lack of an authentication mechanism.
Reference: [34] <editor> IEEE. </editor> <booktitle> Proc. Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <month> Sept. </month> <year> 1995. </year> <month> 12 </month>
Reference: [35] <author> IEEE P1244. </author> <title> Reference Model for Open Storage Systems Interconnection Mass Storage System Reference Model Version 5, </title> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: the endpoints and providing them with means of recognizing communication from the other endpoint (e.g. using cryptographic methods to authenticate the source of the RPC and data). 3.5 Continuous Media Services One goal, especially for the multimedia-oriented systems, is to provide real-time delivery of data, such 20 In the OSSI <ref> [35] </ref>, the mover is the lowest-level entity controlling the device, responsible for moving data. In a bus-attached system, the mover may be the device driver. For a NAP, however, part or all of the mover's functionality might be implemented at the NAP itself. 7 as graphics data. <p> See Coleman and Watson [16] for a good introduction to HSM (and good references on the history of network-attached peripherals). The SSSWG 24 is the IEEE's Storage Systems Standards Working Group. The SSSWG's Open Storage Systems Interconnection reference model <ref> [35] </ref> defines a structure for a set of standards relating to mass storage, and (implicitly) incorporates NAPs. The High Performance Storage System 25 (HPSS)[68] being de 21 http://www.isi.edu/div7/netstation/netstation home.html 22 http://www.tns.lcs.mit.edu/tns-www-home.html 23 http://www.nemesys.uk/ 24 http://www.arl.mil/IEEE/ssswg.html 25 http://www.ccs.ornl.gov/HPSS/HPSS.html 8 veloped at the National Storage Lab 26 uses NAPs.
Reference: [36] <author> R. Jain, K. Somalwar, J. Werth, and J. Browne. </author> <title> Scheduling parallel i/o operations in multiple bus systems. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 352-362, </pages> <year> 1992. </year>
Reference-contexts: The Parallel Transport Protocol proposal [9] provides a means for specifying logically concurrent transfers between groups of NAPs and mapping data from N sources to M sinks. Jain et al propose graph coloring as a means for optimizing the use of sources and sinks in concurrent transfers <ref> [36] </ref>. It is also possible (Zebra) to utilize multiple servers for a single client, similar to a parallel file system or distributed RAID array.
Reference: [37] <author> R. H. Katz. </author> <title> High-performance network and channel based storage. </title> <journal> Proc. IEEE, </journal> <volume> 90(8) </volume> <pages> 1238-1261, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: These structures may be raw partitions, swap space, database partitions, Unix-like file systems (FFS, log-structured, journalled, striped) or file systems with nothing in common with Unix-like file systems (VMS, PCs, mainframes). Katz <ref> [37] </ref> distinguishes the two as block servers and file servers. Some storage subsystems may in fact provide both sorts of interfaces, file and block, allowing the system to be configured flexibly. <p> There are also channel extenders such as the ChannelHIway 32 from Essential Communications for HiPPI and others for SCSI, which do not copy a disk but do allows devices to be used over significant distances. Katz <ref> [37] </ref> compares different hardware approaches to the problem of networked storage. He discusses the distinction between "block servers" (NAPs) and "file servers". The DEC VAXcluster HSC, Control Data disk array controller, Auspex NS5000, Maximum Strategy HiPPI-2 array controller, and Berke-ley's own RAID-II are covered in detail.
Reference: [38] <editor> B. Kobler and P. Hariharan, editors. </editor> <booktitle> Third NASA Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: The key conferences on this topic are hosted by the IEEE Computer Society [15] and NASA's Goddard Space Flight Center <ref> [38] </ref>. NAPs in mass storage are used in hierarchical storage management (HSM) systems, as well as with channel extenders for remote copying of data.
Reference: [39] <author> N. P. Kronenberg, H. M. Levy, and W. D. Strecker. Vaxclusters: </author> <title> A closely-coupled distributed system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The Digital VAXcluster CI and star coupler <ref> [39] </ref> and the UltraNet are currently in only limited use, due to the aging of the technology. Solflower Computer's Storage Crossbar [56] is a newer technology with some similarities to the VAXcluster. These all suffer from the drawback of being non-standard interconnects. <p> Using a custom file system, also known as Shareable File System (SFS), that links into the kernel at the vnode, access to the disks is coordinated to prevent metadata corruption. Although details of the implementation are proprietary, in principle it seems to have some similarity to VAXClusters <ref> [39] </ref>, providing buffering and multiple device control, and optionally acting as a processor-to-processor communications path.
Reference: [40] <author> I. M. Leslie, D. McAuley, and S. J. Mullen-der. </author> <title> Pegasus operating system support for distributed multimedia systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 25(1) </volume> <pages> 69-78, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT [43, 32, 2]. * The Desk Area Network work is being done at Cambridge <ref> [8, 40, 29] </ref>. <p> The operating system for the Cambridge DAN work is known as Pegasus <ref> [40] </ref>. Pegasus 36 is intended to support transfers to and from multimedia peripherals at appropriate data rates. The system has one large, shared address space for all nodes and processes, similar to shared-virtual-memory multicom-puters such as the KSR-1.
Reference: [41] <author> E. Levy and A. Silberschatz. </author> <title> Distributed file systems: Concepts and examples. </title> <journal> ACM Comput. Surv., </journal> <volume> 22(4) </volume> <pages> 322-374, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: As a distributed system, the existing body of research on issues such as resource control and deadlock, naming, caching, etc. is all relevant. Especially important is the work on distributed file systems <ref> [41, 67, 47, 24, 11, 67] </ref>. An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients.
Reference: [42] <author> A. Liebl. </author> <title> Authentication in distributed systems: A bibliography. </title> <journal> ACM Operating Systems Review, </journal> <volume> 27(4) </volume> <pages> 31-41, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: One difficulty is that some of these are hard to do efficiently, and above all a network peripheral is useless if it isn't fast. The concerns of security can be divided into several parts, well-known to programmers of distributed systems <ref> [42] </ref>, but not common issues for peripherals: * authentication of authority to execute a given command * authentication of source of data and command status * integrity of data * privacy of data Another important element in the security of the data on the disk drive in a system is that
Reference: [43] <author> C. J. Lindblad, D. J. Wetherall, W. F. Sta-sior, J. F. Adam, H. H. Houh, M. Ismert, D. R. Bacher, B. M. Philips, and D. L. Tennenhouse. </author> <title> Viewstation applications: Implications for network traffic. </title> <journal> J. Selected Areas in Communications, </journal> <volume> 13 </volume> <pages> 768-778, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Capture of video to disk and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI [23, 66]. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT <ref> [43, 32, 2] </ref>. * The Desk Area Network work is being done at Cambridge [8, 40, 29].
Reference: [44] <author> D. D. E. Long, B. R. Montague, and L.-F. Cabr-era. Swift/RAID: </author> <title> A distributed RAID system. </title> <journal> Computing Systems, </journal> <volume> 7(3) </volume> <pages> 333-359, </pages> <year> 1994. </year>
Reference-contexts: In this case, the server manages the data and initiates transfers, but need not be in the data path, a canonical example of third-party transfers and the uses of network-attached peripherals. The Swift distributed RAID array <ref> [12, 44] </ref> was the first project to propose striping of data across multiple network connections as an alternative to striping on local disks. Their approach involves creating transfer plans to support the striping. The TickerTAIP distributed RAID array [14] is composed of network-attached disks.
Reference: [45] <author> K. C. Matthews. </author> <title> Implementing a shared file system on a HIPPI disk array. </title> <booktitle> In IEEE [34], </booktitle> <pages> pages 77-88. </pages>
Reference-contexts: The key point is that the protocol presented should be low-level enough to allow the host operating system to define any structure it desires without paying a significant penalty in overhead for unused functionality. Maximum Strategy, as part of Cray's Shared File System effort <ref> [45] </ref>, has augmented their Gen 5 storage array, which uses the IPI-3 command set, to include support for semaphores at the device. Their first two implementations used a separate semaphore server, at first a custom hardware device and later a dedicated Sun SPARC.
Reference: [46] <author> E. L. Miller and R. H. Katz. </author> <title> Rama: A file system for massively-parallel computers. </title> <booktitle> In Cole-man [15], </booktitle> <pages> pages 163-168. </pages>
Reference-contexts: Also relevant is the work on operating systems for distributed-memory multicomputers [63], such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance [10] and file system design <ref> [46, 20, 17, 19] </ref> have been done. Some of this work includes, for example, distributed file block layout and synchronization mechanisms that may prove useful for NAP file systems. A key resource for research in this area is David Kotz's excellent page on parallel I/O 35 .
Reference: [47] <author> M. N. Nelson, Y. A. Khalidi, and P. W. Madany. </author> <title> The spring file system. </title> <type> Technical Report SMLI TR-93-10, </type> <institution> Sun Microsystems Laboratories, Inc., </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Most distributed file systems, such as NFS, are built on an RPC interface built on datagram network services such as UDP [52]. The file system semantics may be either stateless (NFS) or stateful (Sprite [?], Spring <ref> [47] </ref>) Some NAPs, such as the 2nd-generation LLNL RAID array, use TCP [51] to set up a connection for each transfer as it is initiated. A disadvantage of this is the potentially long latency to begin a transfer. <p> As a distributed system, the existing body of research on issues such as resource control and deadlock, naming, caching, etc. is all relevant. Especially important is the work on distributed file systems <ref> [41, 67, 47, 24, 11, 67] </ref>. An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients.
Reference: [48] <institution> Parity Systems. Etherstore product info, </institution> <year> 1995. </year>
Reference-contexts: The obvious example is a special-purpose network node that provide NFS (Network File System)[58, 13] services only no general-purpose computing facilities. Examples include the Parity Systems Etherstore <ref> [48] </ref>, Auspex NS7000 3 , Network Appliance 4 [31] and the Maximum Strategy proFILE XL RAID array [1]. However, the high-level 3 http://www.auspex.com/ 4 http://www.netapp.com/ protocol spoken by these is NFS, which provides file-oriented service, an operating system dependent interface.
Reference: [49] <author> J. Pasquale and E. Anderson. </author> <title> Container shipping: Operating system support for i/o-intensive applications. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 84-93, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: This is the approach taken for SCSI disks on a Fibre Channel or SSA network. Research into significant changes in the I/O paradigm presented to applications programmers, such as the work on containers <ref> [49] </ref>, is beginning to address ways of making the system efficient. Containers separates the actions of causing an I/O to occur and mapping the resulting data into the process' address space.
Reference: [50] <author> R. Pike, K. Thompson, and H. Trickey. </author> <title> Plan 9 from bell labs. </title> <booktitle> In Proc. Summer 1990 UKUUG Conf., </booktitle> <pages> pages 1-9, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients. This may make distributed systems such as Amoeba [60] or Plan 9 <ref> [50] </ref> more efficient and more easily location-transparent. The boundaries for which nodes are and are not technically part of "my" system become less clear, as well.
Reference: [51] <author> J. Postel. </author> <title> DoD standard transmission control protocol, </title> <month> Jan. </month> <year> 1980. </year> <note> RFC 761. </note>
Reference-contexts: Most distributed file systems, such as NFS, are built on an RPC interface built on datagram network services such as UDP [52]. The file system semantics may be either stateless (NFS) or stateful (Sprite [?], Spring [47]) Some NAPs, such as the 2nd-generation LLNL RAID array, use TCP <ref> [51] </ref> to set up a connection for each transfer as it is initiated. A disadvantage of this is the potentially long latency to begin a transfer.
Reference: [52] <author> J. Postel. </author> <title> User datagram protocol, </title> <month> Aug. </month> <year> 1980. </year> <note> RFC 768. </note>
Reference-contexts: Most distributed file systems, such as NFS, are built on an RPC interface built on datagram network services such as UDP <ref> [52] </ref>. The file system semantics may be either stateless (NFS) or stateful (Sprite [?], Spring [47]) Some NAPs, such as the 2nd-generation LLNL RAID array, use TCP [51] to set up a connection for each transfer as it is initiated.
Reference: [53] <author> M. W. Sachs, A. Leff, and D. Sevigny. </author> <title> LAN and I/O convergence: A survey of the issues. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 24-32, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Excellent WWW sources of information include CERN's High Speed Interconnect 6 page and LLNL's Standards page 7 . See also Sachs et al <ref> [53] </ref> for a summary of the network-related issues facing interconnect developers that did not affect channel developers.
Reference: [54] <author> K.-Y. Siu and R. Jain. </author> <title> A brief overview of ATM: Protocol layers, LAN emulation and traffic management. </title> <journal> ACM SIG Communications, </journal> <volume> 25(2) </volume> <pages> 6-20, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Serial Bus, known as P1394 14 or as FireWire, comes originally from Apple [61]. It behaves as a system bus, using the IEEE 1212 Control and Status Register low-level address assignments or SCSI as a high-level protocol. Variants of ATM <ref> [54] </ref> networks are in use for the Viewstation and Desk Area Network research. 9 http://www.microp.com/SSA.html 10 http://www.ssaia.org/ 11 http://www1.cern.ch/HSI/fcs/fcs.html 12 http://www.amdahl.com/ext/CARP/FCA/FCA.html 13 http://www.amdahl.com/ext/CARP/SBCON/SBCON.html 14 http://firewire.org/ 3 Our own Netstation re-search is using the ATOMIC 15 high-speed switched local area network [22], originating in an interconnect technology for massively parallel computers and
Reference: [55] <author> N. P. Smith. </author> <title> HIPPI and beyond. </title> <booktitle> Supercomputing Review, </booktitle> <pages> pages 69-79, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: A parallel copper connection can be switched over computer-room distances, or used as a simple channel. Often used in conjunction with an ethernet for a back channel or control network, as HiPPI interfaces are unidirectional <ref> [4, 55] </ref>. HiPPI can also carry TCP/IP network as a high-speed LAN. There is some discussion now of improving transfer rates to 1 GB/s with a growth path to 10 GB/s. IBM's Serial Storage Architecture (SSA) is a relatively new interface.
Reference: [56] <author> Solflower Computer. </author> <title> A storage crossbar for unix workstations, </title> <month> Jan. </month> <year> 1995. </year> <note> white paper. </note>
Reference-contexts: The Digital VAXcluster CI and star coupler [39] and the UltraNet are currently in only limited use, due to the aging of the technology. Solflower Computer's Storage Crossbar <ref> [56] </ref> is a newer technology with some similarities to the VAXcluster. These all suffer from the drawback of being non-standard interconnects. SSA and P1394 arguably do not qualify as network-attached peripheral interconnects, since they do not carry general-purpose network traffic and are oriented toward a single-host environment. <p> They have achieved read rates through the file system, which involves setting shared-read semaphores at the semaphore server, of 12 to 84 megabytes per second, as transfer size varies from 64KB to 16 MB 33 . The Solflower Computer Storage Crossbar <ref> [56] </ref> provides direct high-bandwidth access to SCSI disks to up to sixteen Sun workstations. Using a custom file system, also known as Shareable File System (SFS), that links into the kernel at the vnode, access to the disks is coordinated to prevent metadata corruption.
Reference: [57] <author> R. Steinmetz. </author> <title> Analyzing the multimedia operating system. </title> <journal> IEEE Multimedia, </journal> <volume> 2(1) </volume> <pages> 68-84, </pages> <year> 1995. </year>
Reference-contexts: In a bus-attached system, the mover may be the device driver. For a NAP, however, part or all of the mover's functionality might be implemented at the NAP itself. 7 as graphics data. While research is being conducted on such topics within non-NAP systems <ref> [57] </ref> and for networked systems, I know of no work specifically related to using NAPs for real-time services. The Fibre Channel community has considered isochronous classes of service but the work is low priority. The ATM standard supports isochronous transfers. <p> There is a large body of work on network and operating systems support for multimedia, notably the workshops on digital audio and video [30]. Also see papers such as <ref> [57] </ref> . These have not focussed specifically on NAPs, but the principles are important. 36 http://www.pegasus.esprit.ec.org/papers/pegpapers.html 10 7 Existing Network Attached Peripherals Numerous network-attached peripherals using IPI-3 over HiPPI already exist and are in production use, primarily in supercomputing environments.
Reference: [58] <author> Sun Microsystems Inc. NFS: </author> <title> Network file system protocol specification, </title> <year> 1989. </year>
Reference: [59] <author> A. S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> 2 edition, </address> <year> 1988. </year>
Reference-contexts: Using HiPPI-FP over Fibre Channel is only likely in the event of a heterogeneous interconnect involving both. The network structure for these interconnects tends, rather than directly following the ISO 7-layer model <ref> [59] </ref>, to have a flatter structure. The upper-level protocols often are involved in packet framing and flow control, which may complicate use of heterogeneous interconnects.
Reference: [60] <author> A. S. Tanenbaum, R. van Renesse, H. van Stavaren, G. J. Sharp, S. J. Mullender, J. Jansen, and G. van Rossum. </author> <title> Experiences with the amoeba distributed operating system. </title> <journal> Com-mun. ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients. This may make distributed systems such as Amoeba <ref> [60] </ref> or Plan 9 [50] more efficient and more easily location-transparent. The boundaries for which nodes are and are not technically part of "my" system become less clear, as well.
Reference: [61] <author> M. Teener. </author> <title> A bus on a diet the serial bus alternative. </title> <booktitle> In Proc. IEEE CompCon 1992, </booktitle> <month> Feb. </month> <year> 1992. </year> <note> An updated version can be obtained at ftp://ftp.apple.com/pub/standards/p1394. 13 </note>
Reference-contexts: They are trying to take advantage of the Fibre Channel work as well. However, I believe ESCON is treated primarily as a channel. Serial Bus, known as P1394 14 or as FireWire, comes originally from Apple <ref> [61] </ref>. It behaves as a system bus, using the IEEE 1212 Control and Status Register low-level address assignments or SCSI as a high-level protocol.
Reference: [62] <author> D. Tolmie. </author> <title> An experimental workstation farm with ATM interconnections and an hdtv frame buffer. email communication from det@lanl.gov, </title> <year> 1994. </year>
Reference: [63] <author> A. R. Trapathi and N. M. Karnik. </author> <title> Trends in multiprocessor and distributed operating system design. </title> <journal> J. Supercomputing, </journal> 9(1/2):23-50, 1995. 
Reference-contexts: This may make distributed systems such as Amoeba [60] or Plan 9 [50] more efficient and more easily location-transparent. The boundaries for which nodes are and are not technically part of "my" system become less clear, as well. Also relevant is the work on operating systems for distributed-memory multicomputers <ref> [63] </ref>, such as the Intel Touchstone Delta/Paragon 34 family, which has some nodes dedicated to I/O and others to computation, and the IBM SP and Cray T3D machines. Numerous studies on I/O performance [10] and file system design [46, 20, 17, 19] have been done.
Reference: [64] <author> C. B. S. Traw and J. M. Smith. </author> <title> Striping within the network subsystem. </title> <journal> IEEE Network, </journal> <pages> pages 22-32, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: In addition to supporting parallel file systems (see section 6 below) via concurrent transfers from separate devices to separate compute nodes, it is possible to transfer data in parallel between two endpoints, if two or more paths between the nodes exist <ref> [64] </ref>. The Parallel Transport Protocol proposal [9] provides a means for specifying logically concurrent transfers between groups of NAPs and mapping data from N sources to M sinks. Jain et al propose graph coloring as a means for optimizing the use of sources and sinks in concurrent transfers [36].
Reference: [65] <author> H. Truested. </author> <title> Fibre channel arbitrated loop direct attach SCSI profile (private loop) version 2.0. </title> <type> Technical report, </type> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The current choice for implementing security is to put the SCSI peripherals either on a network that is physically isolated, or as private loop ports on an arbitrated loop <ref> [65] </ref> (AL) that contains no "dangerous" untrusted ports. In figure 2, an arbitrated loop is connected to a fabric. The dotted arrows indicate access that is allowed. The SCSI Initiator external to the loop can access the device configured as public loop, but not the device configured as private loop.
Reference: [66] <author> R. Van Meter. Nxs: </author> <title> X on a network-attached frame buffer. </title> <address> Oct. </address> <year> 1995. </year> <note> in preparation. </note>
Reference-contexts: SCSI relies on the lower-level network interfaces to provide some of these services. SCSI grew into what is known as SCSI-3 [6] partially as a result of the desire to use SCSI for NAPs. It has been suggested <ref> [66] </ref> that certain aspects of the SCSI model have shortcomings from a networking point of view. <p> Capture of video to disk and playback from disk are similar. * Netstation 21 - Greg Finn's group at ISI <ref> [23, 66] </ref>. * The ViewStation work is being done by David Tennenhouse's Telemedia, Networks, and Sys tems Group 22 at MIT [43, 32, 2]. * The Desk Area Network work is being done at Cambridge [8, 40, 29].
Reference: [67] <author> R. Y. Wang and T. E. Anderson. XFS: </author> <title> A wide area mass storage file system. </title> <note> Available at http://cs-tr.cs.berkeley.edu/TR/UCB:CSD-93-783 or http://now.berkeley.edu/, Dec. </note> <year> 1993. </year>
Reference-contexts: As a distributed system, the existing body of research on issues such as resource control and deadlock, naming, caching, etc. is all relevant. Especially important is the work on distributed file systems <ref> [41, 67, 47, 24, 11, 67] </ref>. An important realization is that the resources are truly distributed. A disk drive that "belongs" to no processor may contain a file system that is shared by multiple clients, necessitating a new synchronization policy between clients.
Reference: [68] <author> R. W. Watson and R. A. Coyne. </author> <title> The parallel i/o architecture of the high-performance storage system (HPSS). </title> <booktitle> In IEEE [34], </booktitle> <pages> pages 27-44. 14 </pages>
References-found: 68

