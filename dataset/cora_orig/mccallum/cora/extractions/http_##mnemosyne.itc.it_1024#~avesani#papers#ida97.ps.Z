URL: http://mnemosyne.itc.it:1024/~avesani/papers/ida97.ps.Z
Refering-URL: http://mnemosyne.itc.it:1024/avesani/html/carica.html
Root-URL: 
Email: ricci@irst.itc.it  
Phone: Phone: ++39-461-314334 Fax: ++39-461-302040  
Title: CBET: a Case Base Exploration Tool  
Author: Paolo Avesani, Anna Perini and Francesco Ricci Francesco Ricci 
Note: Send correspondence to:  
Address: Via Sommarive 38050 Povo (TN) Italy  Via Sommarive 38050 Povo (TN) Italy  
Affiliation: Istituto per la Ricerca Scientifica e Tecnologica  Istituto per la Ricerca Scientifica e Tecnologica  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aamodt and E. </author> <title> Plaza. Case-based reasoning: foundational issues, methodological variations, and system approaches. </title> <journal> AI Communications, </journal> <volume> 7(1) </volume> <pages> 39-59, </pages> <year> 1994. </year>
Reference-contexts: Case-Based Reasoning is a general problem solving methodology that searches for a solution of a current problem by first retrieving a similar problem stored in a memory of cases and then adapting the solution found to the new problem <ref> [1] </ref>. When a forest fire is detected CHARADE retrieves similar emergencies from the memory and builds a plan through an interactive process where the user make strategical choices and the system checks and propagates domain constraints. <p> The result of a QBE is a new case base and can be further queried by QBE or NN, or can be used for other studies supported by CBET. Nearest Neighbor is a common algorithm in Pattern Recognition [4] and Case-Based Reasoning <ref> [1] </ref>. Given a set of cases X = fx 1 ; : : : ; x n g and a probe case y, NN searches in X for the k most similar cases to y.
Reference: [2] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: The major focus of the system is retrieval for learning, and we have addressed this problem by using both CBR and KDD techniques. So for example retrieval based on a Nearest Neighbor algorithm, a standard techniques in CBR <ref> [2] </ref>, can be supported by a selection of relevant features, performed with statistical and information theory algorithms [14]. CBET supports the user from the definition of the data structures, to the modification and maintenance of the case base.
Reference: [3] <author> L. K. Branting. </author> <title> Techniques for the retrieval of structured cases. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Case-Based Reasoning, </booktitle> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: Moreover a tuning step, based on knowledge elicitation techniques, is necessary to optimize retrieval performance. Case bases store knowledge combining numerical and symbolic data and often they adopt case representations based on complex structures (graph, semantic net) <ref> [3] </ref>. In our domain, for example, graphics is largely used to describe both the assessment of the emergency situation and the reaction plan. A set of icons displaced on a map of the territory illustrate the deployment of the resources and the activity they are engaged to.
Reference: [4] <author> B. V. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: The result of a QBE is a new case base and can be further queried by QBE or NN, or can be used for other studies supported by CBET. Nearest Neighbor is a common algorithm in Pattern Recognition <ref> [4] </ref> and Case-Based Reasoning [1]. Given a set of cases X = fx 1 ; : : : ; x n g and a probe case y, NN searches in X for the k most similar cases to y.
Reference: [5] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery in databases. </title> <journal> AI Magazine, </journal> <pages> pages 37-54, </pages> <month> fall </month> <year> 1996. </year>
Reference-contexts: For that reason we moved from a strict CBR paradigm to a KDD system <ref> [5] </ref>. A case base is a rich source of information that often cannot be completely exploited in CBR systems because they do not focus on knowledge acquisition.
Reference: [6] <author> D. H.Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: That yields two issues: first, defining a similarity measure between cases; second, a partitioning criteria to divide a set of examples into two clusters. In CBET we have borrowed two methods: COBWEB <ref> [6] </ref> and RPCL (Rival Penalized Competitive Learning) [15], they are based on unsupervised and supervised learning techniques respectively [13]. 8 COBWEB organizes the examples into a hierarchy using four kind of operators: incorporate when an examples fits well into an existing cluster; create when an examples has very different characteristics from
Reference: [7] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 121-129, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We describe these techniques in the next section. 3.2 Feature Selection and Weighting Cases are described by a set of numeric and symbolic features. A central problem in Machine Learning relates to the identification of those features that provide predictive or descriptive value for a given target feature <ref> [7] </ref>. In CBET feature selection and more in general feature weighting [14] has two types of applications. First, in retrieval, when features are to be balanced in computing the similarity function. <p> Second, when data are to be visualized we must identify the most relevant dimensions along which to plot data. 6 Feature selection an weighting algorithms can be divided into two general classes: filter models and wrapper models <ref> [7] </ref>. Filter models selects relevant features before applying the chosen induction algorithm. Conversely wrapper models search for a good subset using the induction algorithm itself as part of the evaluation function.
Reference: [8] <author> A. Perini and F. Ricci. </author> <title> An interactive planning architecture. </title> <editor> In M. Ghallab and A. Milani, editors, </editor> <booktitle> New directions in AI Planning, </booktitle> <pages> pages 273-283. </pages> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction The KDD system described in this paper grows from the results of CHARADE, a decision support system for controlling environmental emergencies. A demonstrator based on the CHARADE platform was developed for managing forest fires emergencies <ref> [8] </ref>. A major component of the CHARADE demonstrator is a Case-Based Reasoning system for planning first interventions to forest fires.
Reference: [9] <author> J. R. Quinlan. C4.5: </author> <title> Programs for machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The info of the distribution of the f T values is defined as: Info (f T ) = k X p (j) log 2 (p (j)) (1) where p (j) is the probability of value j among the possible values of f T <ref> [9] </ref>. <p> In TDIDT test on continuous features are usually performed by selecting a threshold and dividing the cases into those whose value of feature f d is greater of the threshold and those whose value is less than the threshold <ref> [9] </ref>. We have followed a different approach, we discretize the feature f d into a finite number of values and then we apply the above definitions. 2 A first index of the relevance of feature f d in describing feature f T is given by the Information Gain [9]: Gain (f <p> the threshold <ref> [9] </ref>. We have followed a different approach, we discretize the feature f d into a finite number of values and then we apply the above definitions. 2 A first index of the relevance of feature f d in describing feature f T is given by the Information Gain [9]: Gain (f T ; f d ) = (1 p (f d =?))(Inf o (f T ) Entropy (f d ) (3) where p (f d =?) is the probability that feature f d has unknown value.
Reference: [10] <author> B. D. Ripley. </author> <title> Pattern recognition and neural networks. </title> <address> Cambridge U.P., </address> <year> 1996. </year>
Reference-contexts: The Index is defined as: CH (f T ; f d ) = (T raceB=(g 1))=(T raceW=(n g)) (5) Where T race is the trace of the matrix (the sum of the diagonal elements); W and B are within-class covariance matrix and the between-classes covariance matrix respectively <ref> [10] </ref>; n is the number of cases and g is the number of different values of f d In case of f T is numeric more classical statistical indices of correlation are used. We describe here only the use made in CBET of the Mean Squared Error.
Reference: [11] <author> E. Rissland and J. Daniels. </author> <title> Using cbr to drive ir. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 400-407, </pages> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: Case-base retrieval consists of looking for a set of cases that are in the case-base and are similar to a given partially described case (probe). Case-base retrieval relates to Information Retrieval and DBMS querying. In fact, new approaches are trying to integrate CBR and Information retrieval <ref> [11] </ref>. CBET provides both ways to case retrieval: query by example (QBE) and nearest neighbor (NN). Moreover the two can be coupled, for example, retrieving a NN in the output of a QBE. QBE is one traditional approache to query a data base.
Reference: [12] <author> D. B. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 293-301, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: CBET uses also another index for evaluating features, namely the Calinski-Harabasz. This index has been used in clustering for evaluating how well the classes (different values of f T ), regarded as clusters, are separated in a dataset <ref> [12] </ref>. In our application, clusters are defined by grouping cases having equal values of the feature f d (continuous features are dealt with discretization).
Reference: [13] <author> S. M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Let us assume first that f T is discrete and finite (symbolic features), in this situation we have borrowed some ideas from the theory of top down induction of decision trees (TDIDT) for classification <ref> [13] </ref>. A decision tree recursively selects a feature and partitions the data set into many subsets as the number of possible values of the chosen (discrete) feature. A central problem in TDIDT is therefore selecting features in such a way that the associated partition maximize a given "purity" objective function. <p> In CBET we have borrowed two methods: COBWEB [6] and RPCL (Rival Penalized Competitive Learning) [15], they are based on unsupervised and supervised learning techniques respectively <ref> [13] </ref>. 8 COBWEB organizes the examples into a hierarchy using four kind of operators: incorporate when an examples fits well into an existing cluster; create when an examples has very different characteristics from any existing example in the current cluster; merge when the hierarchy is overly branched and combining two clusters
Reference: [14] <author> D. Wettschereck, T. Mohri, and D. W. Aha. </author> <title> A review and comparative evaluation of feature weighting methods for lazy learning algorithms. </title> <journal> AI Review Journal, </journal> <note> 1997. to be published. </note>
Reference-contexts: So for example retrieval based on a Nearest Neighbor algorithm, a standard techniques in CBR [2], can be supported by a selection of relevant features, performed with statistical and information theory algorithms <ref> [14] </ref>. CBET supports the user from the definition of the data structures, to the modification and maintenance of the case base. <p> The contribution to the total distance given by the i-th feature value distance is weighted by the positive constant w i . There is a vast literature on feature weighting <ref> [14] </ref> and a number of different distance functions can be defined by varying the weights. <p> A central problem in Machine Learning relates to the identification of those features that provide predictive or descriptive value for a given target feature [7]. In CBET feature selection and more in general feature weighting <ref> [14] </ref> has two types of applications. First, in retrieval, when features are to be balanced in computing the similarity function.
Reference: [15] <author> L. Xu, A. Krzyzak, and E. Oja. </author> <title> Rival penalized competitive learning for cluster analysis, RBF net, and curve detection. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 636-649, </pages> <year> 1993. </year> <month> 10 </month>
Reference-contexts: That yields two issues: first, defining a similarity measure between cases; second, a partitioning criteria to divide a set of examples into two clusters. In CBET we have borrowed two methods: COBWEB [6] and RPCL (Rival Penalized Competitive Learning) <ref> [15] </ref>, they are based on unsupervised and supervised learning techniques respectively [13]. 8 COBWEB organizes the examples into a hierarchy using four kind of operators: incorporate when an examples fits well into an existing cluster; create when an examples has very different characteristics from any existing example in the current cluster;
References-found: 15

