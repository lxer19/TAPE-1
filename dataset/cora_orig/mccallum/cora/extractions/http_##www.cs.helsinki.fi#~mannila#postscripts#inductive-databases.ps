URL: http://www.cs.helsinki.fi/~mannila/postscripts/inductive-databases.ps
Refering-URL: http://www.cs.helsinki.fi/~mannila/data-mining-publications.html
Root-URL: 
Email: Heikki.Mannila@cs.helsinki.fi  
Title: Inductive Databases and Condensed Representations for Data Mining Extended abstract  
Author: Heikki Mannila 
Web: http://www.cs.helsinki.fi/~mannila  
Address: P.O. Box 26 (Teollisuuskatu 23), FIN-00014 Helsinki, Finland  
Affiliation: University of Helsinki, Department of Computer Science  
Note: To appear in the Proceedings of the International Logic Programming Symposium, Jan Maluszynski (ed.), MIT Press, 1997.  
Abstract: Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. It can be argued that several data mining tasks consist of locating interesting sentences from a given logic that are true in the database. Then the task of the user/analyst is to is to query this set, the theory of the database. This view gives rise to the concept of of inductive databases, i.e., databases that in addition to the data contain also inductive generalizations about the data. We describe a rough framework for inductive databases, and consider also condensed representations, data structures that make it possible to answer queries about the inductive database approximately correctly and reasonably efficiently. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <editor> In P. Buneman and S. Jajodia, editors, </editor> <booktitle> Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD'93), </booktitle> <pages> pages 207 - 216, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: This section can be considered as a brief introduction to a typical data mining task. Given a schema R = fA 1 ; : : : ; A p g of attributes with domain f0; 1g, and a relation r over R, an association rule <ref> [1] </ref> about r is an expression of the form X ) B, where X R and B 2 R n X. <p> Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., <ref> [1, 2, 8, 20, 21] </ref>. A typical approach [2] is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307 - 328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., <ref> [1, 2, 8, 20, 21] </ref>. A typical approach [2] is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs. <p> How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., [1, 2, 8, 20, 21]. A typical approach <ref> [2] </ref> is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [3] <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1058 - 1053, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [3, 4, 10, 11, 12] </ref>; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic.
Reference: [4] <author> L. De Raedt and S. Dzeroski. </author> <title> First-order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70:375 - 392, </volume> <year> 1994. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [3, 4, 10, 11, 12] </ref>; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic.
Reference: [5] <author> U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, pages 1 -34. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Discovering knowledge from data should therefore be seen as a process containing several steps: understanding the domain, preparing the data set, discovering patterns (data mining), postprocessing of discovered patterns, and putting the results into use. See <ref> [5] </ref> for a slightly different process model and an excellent discussion.
Reference: [6] <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: There is a suspicion that there might be nuggets of useful information hiding in the masses of unanalyzed or underanalyzed data, and therefore semiautomatic methods for locating interesting information from data would be useful. Data mining has in the 1990's emerged as visible research and development area; see <ref> [6] </ref> for a recent overview of the area.
Reference: [7] <author> J. Gray, S. Chaudhuri, A. Bosworth, A. Layman, D. Reichart, M. Ven-katrao, F. Pellow, and H. Pirahesh. </author> <title> Data Cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals. Data Mining and Knowledge Discovery, </title> <address> 1(1):29 - 53, </address> <year> 1997. </year>
Reference-contexts: The approximation can be fairly accurate. The data cube <ref> [7] </ref> can also be viewed as a condensed representation for a class of queries. One can also consider a "frequent" variation of the data cube, i.e., a cube where the computation is halted for any cell whose frequency falls below a given threshold.
Reference: [8] <author> J. Han and Y. Fu. </author> <title> Discovery of multiple-level association rules from large databases. </title> <booktitle> In Proceedings of the 21st International Conference on 8 Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 420 - 431, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., <ref> [1, 2, 8, 20, 21] </ref>. A typical approach [2] is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [9] <author> M. Jaeger, H. Mannila, and E. Weydert. </author> <title> Data mining as selective theory extraction in probabilistic logic. </title> <editor> In R. Ng, editor, </editor> <booktitle> SIGMOD'96 Data Mining Workshop, </booktitle> <institution> The University of British Columbia, Department of Computer Science, </institution> <type> TR 96-08, </type> <pages> pages 41-46, </pages> <year> 1996. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [3, 4, 10, 11, 12]; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in <ref> [9] </ref>, where we argue that the logic L should be a slice of a probabilistic first-order logic. In such querying of rules, the user often wants to cross the boundary between data and rules several times.
Reference: [10] <author> J.-U. Kietz and S. Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 335 - 359. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [3, 4, 10, 11, 12] </ref>; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic.
Reference: [11] <author> W. Kloesgen. </author> <title> Efficient discovery of interesting statements in databases. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 4(1):53 - 69, </volume> <year> 1995. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [3, 4, 10, 11, 12] </ref>; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic.
Reference: [12] <author> H. Mannila and K.-J. Raiha. </author> <title> Design by example: An application of Arm-strong relations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(2):126 - 141, </volume> <year> 1986. </year>
Reference-contexts: This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning <ref> [3, 4, 10, 11, 12] </ref>; some theoretical results can be found in [15]. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic.
Reference: [13] <author> H. Mannila and H. Toivonen. </author> <title> Discovering generalized episodes using minimal occurrences. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 146 - 151, </pages> <address> Portland, Oregon, Aug. 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The basic ideas of the frequent set finding algorithm are fairly widely applicable. For example, they can be used to find repeated episodes in sequences of events <ref> [16, 13] </ref>. 3 Data mining as querying the theory of the data base The goal of knowledge discovery is to obtain useful knowledge from large collections of data.
Reference: [14] <author> H. Mannila and H. Toivonen. </author> <title> Multiple uses of frequent sets and condensed representations. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 189 - 194, </pages> <address> Portland, Oregon, Aug. 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Association rules are a simple formalism and they produce nice results for binary data. The information about the frequent sets can actually be used to approximate fairly accurately the confidences and supports of a far wider set of rules, including negation and disjunction <ref> [14] </ref>. The basic restriction for the use of association rules is that the relation should be sparse in the sense that there are no frequent sets that contain more than about 15 attributes. <p> Another, less obvious example is given by the collection of frequent sets of a 0-1 valued relation <ref> [14] </ref>: the collection of frequent sets can be used to give approximate answers to arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The approximation can be fairly accurate.
Reference: [15] <author> H. Mannila and H. Toivonen. </author> <title> Levelwise search and borders of theories in knowledge discovery. Data Mining and Knowledge Discovery, </title> <type> 1(3), </type> <year> 1997. </year>
Reference-contexts: Their running time is approximately O (N F ), where N = np is the size of the input and F is the sum of the sizes of the sets in the candidate collection C during the 3 operation of the algorithm <ref> [15] </ref>. This is nearly linear, and the algorithms seem to scale nicely to tens of millions of examples. Typically the only case when they fail is when the output is too large, i.e., there are too many frequent sets. <p> This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in machine learning [3, 4, 10, 11, 12]; some theoretical results can be found in <ref> [15] </ref>. A suggested logical formalism for this approach is given in [9], where we argue that the logic L should be a slice of a probabilistic first-order logic. In such querying of rules, the user often wants to cross the boundary between data and rules several times.
Reference: [16] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Discovery of frequent episodes in event sequences. Data Mining and Knowledge Discovery, </title> <type> 1(3), </type> <year> 1997. </year>
Reference-contexts: The basic ideas of the frequent set finding algorithm are fairly widely applicable. For example, they can be used to find repeated episodes in sequences of events <ref> [16, 13] </ref>. 3 Data mining as querying the theory of the data base The goal of knowledge discovery is to obtain useful knowledge from large collections of data.
Reference: [17] <author> A. Moore and M. Lee. </author> <title> Cached sufficient statistics for efficient machine learning with large datasets. </title> <type> Technical Report CMU-RI-TR-97-27, </type> <institution> Robotics Institute, Carnegie-Mellon University, </institution> <year> 1997. </year>
Reference-contexts: Whether this approach is generally useful is still open. Condensed representations have connections to maximum entropy approaches, as well as to some recent ideas in machine learning <ref> [17, 18] </ref>. 6 Conclusions We have presented a draft framework for inductive databases, based on the ideas that parts of data mining can be viewed as querying the theory of a database, and that the querying of rules and data have to be mixed in the data mining process.
Reference: [18] <author> A. W. Moore, J. Schneider, and K. Deng. </author> <title> Efficient locally weighted polynomial regression predictions. </title> <booktitle> In Proceedings of the 1997 International Machine Learning Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Whether this approach is generally useful is still open. Condensed representations have connections to maximum entropy approaches, as well as to some recent ideas in machine learning <ref> [17, 18] </ref>. 6 Conclusions We have presented a draft framework for inductive databases, based on the ideas that parts of data mining can be viewed as querying the theory of a database, and that the querying of rules and data have to be mixed in the data mining process.
Reference: [19] <author> K. Mulmuley. </author> <title> Computational Geometry: An Introduction Through Randomized Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1993. </year> <month> 9 </month>
Reference-contexts: One can also consider a "frequent" variation of the data cube, i.e., a cube where the computation is halted for any cell whose frequency falls below a given threshold. Similarly, in computational geometry the notion of an "-approximation <ref> [19] </ref> is closely related. Developing condensed representations for various classes of patterns seems a promising way of implementing inductive databases and more generally improving the effectiveness of data mining algorithms. Whether this approach is generally useful is still open.
Reference: [20] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An efficient algorithm for mining association rules in large databases. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases (VLDB'95), </booktitle> <pages> pages 432 - 444, </pages> <address> Zurich, Swizerland, </address> <year> 1995. </year>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., <ref> [1, 2, 8, 20, 21] </ref>. A typical approach [2] is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [21] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB'96), </booktitle> <pages> pages 134 - 145, </pages> <address> Mumbay, India, Sept. 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Namely, for each frequent set X and each B 2 X verify whether the rule X n fBg ) B has sufficiently high confidence. How can one find all frequent sets X? This can be done in a multitude of ways; see, e.g., <ref> [1, 2, 8, 20, 21] </ref>. A typical approach [2] is to use the fact that all subsets of a frequent set are also frequent: first find all frequent sets of size 1 by reading the data once and recording the number of times each attribute A occurs.
Reference: [22] <author> J. D. Ullman. </author> <title> A system for managing query flocks. </title> <note> Available from http://www-db.stanford.edu/~ullman/pub/flocks.html, 1997. 10 </note>
Reference-contexts: Looking at the search for association rules, the frequency of a collection of attribute sets can be evaluated very efficiently in one pass through the data. Similar, but less ad hoc, methods should be possible for more complex situations; see <ref> [22] </ref> for some interesting ideas. The second problem is to evaluate queries from a query class without looking at the whole data.
References-found: 22

