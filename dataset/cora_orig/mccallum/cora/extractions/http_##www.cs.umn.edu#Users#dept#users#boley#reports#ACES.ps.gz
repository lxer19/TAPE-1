URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/ACES.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Methods for Large Sparse Eigenvalue Problems from Waveguide Analysis  
Author: Chang Peng and Daniel Boley 
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: We discuss several techniques for finding leading eigenvalues and eigenvectors for large sparse matrices. The techniques are demonstrated on a scalar Helmholtz equation derived from a model semiconductor rib waveguide problem. We compare the simple inverse iteration approach with more sophisticated methods, including minimum degree reordering, Arnoldi and Lanczos methods. We then propose a new Arnoldi method designed particularly for the constrained generalized eigenvalue problem, a formulation arising naturally from the scalar waveguide problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. L. Boley. </author> <title> Krylov space methods on state-space control models. Circ., </title> <booktitle> Syst. & Signal Proc., </booktitle> <year> 1992. </year> <note> to appear. </note>
Reference-contexts: While the Arnoldi method reduces the large matrix A into a small upper Hessenberg matrix H m , the Lanczos method, however, reduces the matrix into a small tridiagonal matrix, and solve the tridiagonal matrix for eigen-values as the approximation <ref> [1] </ref>. In the Lanczos method the matrix V m will not be orthonormal.
Reference: [2] <author> Bernice M. Dillon and Jon P. Webb, </author> <title> "A comparison of Formulations for the Vector Finite Element Analysis of Waveguides", </title> <journal> IEEE Trans. Microwave Theory Tech., </journal> <volume> vol. 42, </volume> <pages> pp. 308-316, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The combined equations are then discretized by finite differences yielding a large sparse ordinary matrix eigenvalue problem Ax = x [9]. Discretizing the vector fields equations using finite element formulations leads to a large sparse generalized eigenvalue problem <ref> [2] </ref>: Ax = Bx (1) where the matrix A nfin is real and non-singular, B nfin is real, symmetric, and positive definite. Solving problem (1) constitutes the largest part of the computational effort.
Reference: [3] <author> Alan George and Joseph W. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1981. </year>
Reference-contexts: Thus often line 9 is skipped, or performed only once during several passes through the main loop. If the matrix A is banded, we can reorder A using the minimum degree ordering before we do the LU decomposition <ref> [3] </ref>. This is a heuristic algorithm that reorders the equations so that (hopefully) the resulting L and U factors are more sparse, i.e. have fewer non-zero elements. Then the total number of flops needed by the algorithm will be considerably reduced. <p> Then the total number of flops needed by the algorithm will be considerably reduced. For the details of minimum degree reordering, please refer to <ref> [3] </ref>. For these experiments, we have taken advantage of MATLAB's built-in functions for various reordering algorithms including the minimum degree reordering. Algorithm 2.2 Inverse Iteration with Minimum Degree Reordering: 1. Choose a random unit vector x 0 and a shift oe; 2.
Reference: [4] <author> Gene H. Golub and Charles F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> John Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: We can see that if oe is much closer to 1 than to 2 , the convergence can be very fast. Each iteration requires that a system of linear equations of the form (A oeI)x new = x old be solved, often by a variant of LU Factorization <ref> [4] </ref>. If oe is updated using Rayleigh quotients during the iteration, then the iteration converges quadratically in general, and cubically if the matrix is symmetric [4], but changing oe requires that (A oeI) be re-factored from scratch in each iteration at great expense. <p> requires that a system of linear equations of the form (A oeI)x new = x old be solved, often by a variant of LU Factorization <ref> [4] </ref>. If oe is updated using Rayleigh quotients during the iteration, then the iteration converges quadratically in general, and cubically if the matrix is symmetric [4], but changing oe requires that (A oeI) be re-factored from scratch in each iteration at great expense. So often the shift is fixed, allowing the factoring to be done just once, where the shift is estimated in advance by physical considerations. <p> To see the potential advantage that could be gained just from minimum degree ordering if pivoting is turned off, we computed the flop count for the Cholesky factorization on a shifted matrix A (like LU Decomposition without pivoting for symmetric positive definite matrices <ref> [4] </ref>). <p> We here propose an efficient process that avoids forming A 0 . Since what the inverse iteration needs is the matrix-vector product x = A 01 x 0 . We express A 01 in terms of A 1 , whose LU decomposition is already available. By the Sherman-Morrison formula <ref> [4] </ref> and using A 1 u = 1 u, we have A 01 = A 1 + flA 1 u (1 flv T A 1 u) 1 v T A 1 fl uv T A 1 = (I + fiuv T )A 1 (4) where fi = fl fl . <p> Combining this into Algorithm 2.1 and 2.2 is the inverse iteration with deflation. We would like to point out here that this deflation technique can be cascaded multiple time, or generalized to the block version, e.g. deflation of multiple eigenvalues at the same time, by using the Sherman-Morrison-Woodbury formula <ref> [4] </ref>. There are various choices for the vector v. The Hotelling's deflation chooses v as the left eigenvector w, e.g. w T A = w T or A T w = w. The advantage of this choice is that A 0 will have the same eigenvectors as A. <p> When the matrix is symmetric, the the Arnoldi and Lanczos methods reduce to the same algorithm, and the resulting algorithm is known by the Kaniel-Paige theory to have very nice convergence properties, at least for the extreme eigenvalues <ref> [4] </ref>. When the matrix is nonsymmetric, the Arnoldi method is more straightforward to implement and has somewhat better convergence properties, but requires more space and cost per iteration than the Lanczos method.
Reference: [5] <author> Zine-Eddine Abid, Klein L. Johnson, and Anand Gopinath, </author> <title> "Analysis of Dielectric Guides by Vector Transverse Magnetic Field Finite Elements", </title> <journal> J. of Lightwave Tech., </journal> <volume> vol. 11, </volume> <pages> pp. 1545-1549. </pages>
Reference-contexts: In addition, it is not entirely a simple matter to compute the eigenvalues of the nonsymmetric tridiagonal matrix resulting from the Lanczos method. Deflation can be incorporated into both Arnoldi and Lanczos methods. 4 Constrained Raleigh Quotient Maximization Problems In the transverse magnetic field formulation using finite element method <ref> [5] </ref>, the following problem is encountered: Find (; x) s.t. = max C T x=0; (Ax; x) (12) where A is symmetric, B is symmetric positive definite, and C has full column rank.
Reference: [6] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1980. </year>
Reference: [7] <author> Y. Saad, </author> <title> "Variations on Arnoldi's method for computing eigenelements of large unsymmetric matrices", </title> <journal> Linear Algebra Appl., </journal> <volume> 34 </volume> <pages> 269-295, </pages> <year> 1980. </year>
Reference-contexts: Here, the modified Gram-Schmidt orthogonaliza-tion process can be used. Each Av j is generated, it is orthogonalized against all v i with i &lt; j. It is now known that this reduced problem can indeed give good approximation to large modulus eigenvalues <ref> [7] </ref>. Below is the algorithm. Algorithm 3.1 Arnoldi Algorithm: 1. Choose an random unit vector v 1 ; 2. Iterate: for j = 1; 2; :::; m do 3. Compute w j = Av j ; 4.
Reference: [8] <author> Y. Saad, </author> <title> Numerical Methods for Large Eigenvalue Problems, </title> <journal> pp. </journal> <pages> 153-156. </pages> <publisher> Manchester University Press, </publisher> <year> 1992. </year>
Reference-contexts: are 2; 013; 624 and 3; 685; 377, respectively, representing a savings of 45% over the unreordered version! If we have already obtained the leading eigenvalue and need to find the next one closest to the known eigenvalue, we can use the so called def lation technique, described as follows <ref> [8] </ref>. Suppose and u are the known eigenpair of A, and v is a vector such that v T u = 1. <p> If the iteration converges, then the upper triangular matrix R k will tend to be the leading k fi k part of the Schur canonical form of (A oeI) 1 <ref> [8] </ref>, that is the diagonal elements of R k will be close to the first k eigenvalues of (A oeI) 1 in order of magnitude. So the eigenvalues nearest to oe are obtained. The matrix Q k will be close to the corresponding Schur vectors.
Reference: [9] <author> M.S. Stern, </author> <title> "Semivectorial Polarised finite difference method for Optical Waveguides with Arbitrary Index Profiles", </title> <booktitle> IEE PROCEEDINGS, vol .135, Pt.J, No.1, </booktitle> <pages> pp. 56-63, </pages> <month> Feb. </month> <title> Figure1. Matrix Non-Zeroes showing original matrix and LU factors combined. Upper figures show original order, lower figures show minimum degree ordering. nz is the total number of nonzeroes. </title>
Reference-contexts: 1 Problem Formulation In the waveguide analysis (see e.g. <ref> [9] </ref>), the analysis of the propagation of the electric and magnetic fields in waveguides, based on the use of Maxwell's equations, often lead to scalar Helmholtz equations of the general form r 2 T E x + k 2 E x = fi 2 E x , where E x denotes <p> When this Helmholtz equation is solved for the x coordinate only, one must impose internal continuity conditions across the boundaries between materials of different dielectric constants. The combined equations are then discretized by finite differences yielding a large sparse ordinary matrix eigenvalue problem Ax = x <ref> [9] </ref>. Discretizing the vector fields equations using finite element formulations leads to a large sparse generalized eigenvalue problem [2]: Ax = Bx (1) where the matrix A nfin is real and non-singular, B nfin is real, symmetric, and positive definite. <p> As a model test case, we derived the matrix A from the scalar Helmholtz equation, combined with suitable internal continuity conditions, applied to a typical semiconductor rib waveguide. See <ref> [9] </ref> for the details. In our examples, we used a matrix of modest size (1800 fi 1800) with a zero structure shown in Figure 1.
References-found: 9

