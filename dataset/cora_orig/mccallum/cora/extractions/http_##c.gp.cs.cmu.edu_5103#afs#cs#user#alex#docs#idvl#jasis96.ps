URL: http://c.gp.cs.cmu.edu:5103/afs/cs/user/alex/docs/idvl/jasis96.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/alex/www/HomePage.html
Root-URL: http://www.cs.cmu.edu
Title: Speech Recognition for a Digital Video Library  
Author: Michael J. Witbrock and Alexander G. Hauptmann 
Keyword: video browsing, information retrieval interfaces, speech recognition, News-On-Demand, multimedia indexing and search, Informedia, artificial intelligence, automatic text summarization, video summarization, digital library  
Abstract: The standard method for making the full content of audio and video material searchable and is to annotate it with human-generated meta-data that describes the content in a way that the search can understand, as is done in the creation of multimedia CDROMs. However, for the huge amounts of data that could usefully be included in digital video and audio libraries, the cost of producing this meta-data is prohibitive. In the Informedia Digital Video Library, the production of the meta-data supporting the library interface is automated using techniques derived from artificial intelligence (AI) research. By applying speech recognition together with natural language processing, information retrieval and image analysis, an interface has been produced that helps users locate the information they want and navigate or browse the digital video library more effectively. Specific interface components include automatic titles, filmstrips, video skims, word location marking and representative frames for shots. Both the user interface and the information retrieval engine within Informedia are designed for use with automatically derived meta-data, much of which depends on speech recognition for its production. Some experimental information retrieval results will be given supporting a basic premise of the Informedia project: that speech recognition generated transcripts can make multimedia material searchable. The Informedia project emphasizes the integration of speech recognition, image processing, natural language processing and information retrieval to compensate for deficiencies in these individual technologies. 
Abstract-found: 1
Intro-found: 1
Reference: [Brown95] <author> Brown, M. G., Foote, J. T., Jones, G. J. F., Sprck Jones, K., and Young, S. J. </author> <title> Automatic Content-based Retrieval of Broadcast News, </title> <booktitle> Proceedings of ACM Multimedia. </booktitle> <address> San Francisco: </address> <publisher> ACM, </publisher> <month> November, </month> <year> 1995, </year> <pages> pp. 35-43. </pages>
Reference-contexts: Despite errors in recognition and word prefix and suffix mismatches, the system performed reasonably well, since these errors scatter evenly over all documents allowing the consistently high search scores of well-matching correct segments to dominate the retrieval. Another news processing system that included video materials was the MEDUSA system <ref> [Brown95] </ref>. The MEDUSA news broadcast application could digitize and record news video and teletext transcriptions, which are equivalent to closed-captions. Instead of segmenting the news into stories, the system used overlapping windows of adjacent text lines for indexing and retrieval.
Reference: [Christel94a] <author> Christel, M., Stevens, S., & Wactlar, H. </author> <title> Informedia Digital Video Library, </title> <booktitle> Proceedings of the Second ACM International Conference on Multimedia, Video Program. </booktitle> <address> New York: </address> <publisher> ACM, </publisher> <month> October, </month> <year> 1994, </year> <pages> pp. 480-481. </pages>
Reference: [Christel94b] <author> Christel, M., Kanade, T., Mauldin, M., Reddy, R., Sirbu, M., Stevens, S., and Wactlar, H., </author> <title> Informedia Digital Video Library, </title> <journal> Communications of the ACM, </journal> <volume> 38 (4), </volume> <month> April </month> <year> 1994, </year> <pages> pp. 57-58. </pages>
Reference: [CNN-AT-WORK95] <institution> Cable News Network/Intel CNN at Work - Live News on your Networked PC Product Information. </institution> <note> http://www.intel.com/comm-net/cnn_work/index.html. </note>
Reference-contexts: Users can store headlines together with video clips and retrieve them later. However, this retrieval depends entirely on the separately transmitted, manually created, text headlines and the service does not include news sources other than CNN. In addition, CNN-AT-WORK does not feature an integrated multimodal query interface <ref> [CNN-AT-WORK95] </ref>. Preliminary investigation into the use of speech recognition for analysis of a news story was carried out by Schuble and Wechsler [Schuble95]. Since they lacked a powerful speech recognizer, their approach used a phonetic engine that transformed the spoken content of the news stories into possibly erroneous phoneme strings.
Reference: [Flickner95] <author> Flickner, M., Sawhney, H., Niblack, W., Ashley, J., Huang, Q., Dom, B., Gorkani, M., Hafner, J., Lee, D., Petkovic, D., Steele, D., and Yanker, P. </author> <title> Query by Image and Video Content: </title> <booktitle> The QBIC System. IEEE Computer, </booktitle> <month> September </month> <year> 1995, </year> <pages> pp. 23-31 17 </pages>
Reference-contexts: The Informedia project, in as much as it involves the indexing of non-textual data, also bears similarities to projects such as QBIC <ref> [Flickner95] </ref>, which applies both automatic image characterization and hand-annotation to images, and supports retrieval using image similarity. One of the more interesting features of the QBIC system is that it allows query by demonstration, with the user sketching the features desired in the retrieved image.
Reference: [Hauptmann95] <author> Hauptmann, A. G., Witbrock, M. J., Rudnicky, A. I., and Reed, S., </author> <title> Speech for Multimedia Information Retrieval, </title> <booktitle> UIST-95, Proceedings of User Interface Software Technology, </booktitle> <year> 1995, </year> <note> in press </note>
Reference: [Hauptmann95b] <author> Hauptmann, A. G. and Smith, M. A., </author> <title> Text, Speech and Vision for Video Segmentation: the Informedia Project. </title> <booktitle> AAAI Fall Symposium on Computational Models for Integrating Language and Vision, </booktitle> <address> Boston MA Nov 10-12 1995., </address> <pages> pp. 90-95. </pages>
Reference-contexts: Firstly, color histograms are computed from the video frames, and shot changes are hypothesized where the differences between successive histograms peak. Secondly, camera or scene motion is inferred from the motion information stored in the MPEG data stream and extracted using Lucas-Kanade optical flow analysis <ref> [Hauptmann95b] </ref>. Since actual shot changes involve the complete replacement of one scene by another, they are characterized by random apparent motion vectors. Consistent motion vectors across a hypothesized scene break can therefore allow that hypothesis to be rejected. 5.
Reference: [Hauptmann96] <author> Hauptmann, A.G. and Witbrock, M.J., </author> <title> Informedia News on Demand: Multimedia Information Acquisition and Retrieval, </title> <editor> in Maybury, M. T., Ed, </editor> <booktitle> Intelligent Multimedia Information Retrieval, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <note> 1996 (In Press). </note>
Reference: [Hwang94] <author> Hwang, M., Rosenfeld, R., Thayer, E., Mosur, R., Chase, L., Weide, R., Huang, X., and Alleva, F., </author> <title> Improving Speech Recognition Performance via PhoneDependent VQ Codebooks and Adaptive Language Models in SPHINX-II. </title> <journal> ICASSP-94, </journal> <volume> vol. I, </volume> <pages> pp. 549-552. </pages> <address> [Informedia95] http://www.informedia.cs.cmu.edu/ </address>
Reference: [James96] <author> James D. A., </author> <title> System for Unrestricted Topic Retrieval from Radio News Broadcasts. </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <address> Atlanta, GA, USA, </address> <month> May </month> <year> 1996, </year> <pages> pp. 279-282. </pages>
Reference-contexts: The prompts are also converted into phoneme based tokens in the same way. Table 4 gives precision and recall for word based and phonetic representations of both human generated and machine generated transcriptions. Combined word and phoneme based retrieval is also evaluated, as originally proposed by James <ref> [James96] </ref>. Figures are given for two information retrieval engines. The first observation one can make from these data is that the speech recognition vocabulary is crucial to IR performance.
Reference: [Jones96] <author> Jones, G.J.F., Foote, J.T., Sprck Jones, K., and Young, S.J., </author> <title> Retrieving Spoken Documents by Combining Multiple Index Sources, </title> <booktitle> SIGIR-96 Proceedings of the 1996 ACM SIGIR Conference, </booktitle> <address> Zrich. </address>
Reference-contexts: The focus of MEDUSA was in the system architecture and the information retrieval component. No image processing and no speech recognition were performed. The system did, however serve as the substrate for a later series of speech recognition and information retrieval experiments <ref> [Jones96] </ref>. Using a speech recognition system to extract words from spoken messages, these experiments evaluated information retrieval using a combination of word spotting based on rapid scanning of word lattices and whole word retrieval, in a video mail retrieval task. <p> Different corpus sizes were also compared. The most substantial body of previous work has been done at Cambridge University in the United Kingdom. Jones et al. <ref> [Jones96] </ref> used a specially constructed test set of 50 queries and 300 voice mail messages, from 15 speakers, constructed to have on average 10.8 highly relevant documents per query. They measured precision at rank 5, 10, 15 and 20 and also reported the average precision. <p> Recall is defined as the number of correct hits returned to the user divided by the number of hits a perfect retrieval should have returned. Recall and precision are thus computed based on the retrieved set, the relevant set, and their intersection. <ref> [Jones96] </ref> reported results for precision at 5, 10, 15 and 20 items retrieved and in order to allow some comparison, we will use average precision and recall over those four ranks. <p> They reported reasonable success in retrieving relevant documents. Similarly, Jones et al, <ref> [Jones96] </ref> used a combination of whole word transcription and phoneme lattices to improve on the retrieval effectiveness of a system using either alone. <p> This is consistent with the findings of Jones et al. <ref> [Jones96] </ref> on a voice mail retrieval task. In contrast to Jones et al, the current experiments do not use word spotting or phoneme lattices to augment the vocabulary of the recognition system. Instead of word spotting, a large set of phoneme strings of various lengths is indexed and searched. <p> The challenge here is to reliably identify the prosodically marked words and to favor them as relevant terms. While the results presented here, as well as those from Schuble and Wechsler [Schuble95] and Jones et al <ref> [Jones96] </ref> show some success in information retrieval from spoken documents, much remains to be done. In the near term, the combination of whole-word recognition and phoneme recognition shows clear promise.
Reference: [Li96] <author> Li, W., Gauch, S., Gauch, J., and Pua, K.M., </author> <title> VISION: A Digital Video Library, Digital Libraries 96: </title> <booktitle> 1 st ACM International Conference on Research and Development in Digital Libraries, </booktitle> <address> Bethesda MD, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: It is also distinguished by its stated concentration on the use of preexisting, mature, domain independent indexing technologies <ref> [Li96] </ref>. The Broadcast News Navigator (BNN) system [Maybury96][Mani96] has concentrated on the automatic segmentation of stories from news broadcasts using discourse structure.
Reference: [Mani96] <author> Mani, I., House, D., Maybury, M. and Green, M. </author> <year> 1996. </year> <title> Towards Content-Based Browsing of Broadcast News Video, </title> <editor> in Maybury, M. T. (editor), </editor> <booktitle> Intelligent Multimedia Information Retrieval. </booktitle>
Reference: [Maybury96] <author> Maybury, M., Merlino, A., and Rayson, J., </author> <month> submitted </month> <year> 1996. </year> <title> Segmentation, Content Extraction and Visualization of Broadcast News Video using Multistream Analysis, </title> <booktitle> in Proceedings of the ACM International Conference on Multimedia, </booktitle> <address> Boston, MA. </address>
Reference: [Ogle95] <author> Ogle, V. and Stonebraker, M. Chabot: </author> <title> Retrieval from a Relational Database of Images, </title> <journal> IEEE Computer, </journal> <volume> Vol. 28, No 9, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: A similar effort, which also encompasses some video material, is the Photobook system [Pentland94]. Photobook employs relatively sophisticated statistical characterizations of selected image features, such as faces, shapes and textures, to support accurate retrieval by image similarity. A final example of an image retrieval system is Chabot <ref> [Ogle95] </ref>, a part of the Berkeley digital library project. This system includes an element of cross-modal operation, allowing users to search simultaneously in preexisting annotations and color content characterizations of a large set of landscape images.
Reference: [Pentland94] <author> Pentland, A, Picard, R., Sclaroff, S., Photobook: </author> <title> Tools for Content-Base Manipulation of Image Databases . SPIE Conference on Storage and Retrieval of Image and Video Databases II, </title> <type> (SPIE paper 2185-05) Feb 6-10, </type> <address> 1994, San Jose CA, pp34-47 </address>
Reference-contexts: One of the more interesting features of the QBIC system is that it allows query by demonstration, with the user sketching the features desired in the retrieved image. A similar effort, which also encompasses some video material, is the Photobook system <ref> [Pentland94] </ref>. Photobook employs relatively sophisticated statistical characterizations of selected image features, such as faces, shapes and textures, to support accurate retrieval by image similarity. A final example of an image retrieval system is Chabot [Ogle95], a part of the Berkeley digital library project.
Reference: [Rudnicky95] <author> Rudnicky, A., </author> <title> Language Modeling with Limited Domain Data, </title> <booktitle> Proceeding of the 1995 ARPA Workshop on Spoken Language Technology, </booktitle> <publisher> in press. </publisher>
Reference-contexts: The language model is based on trigram (word triple) probabilities estimated from The Wall Street Journal (WSJ) and other North American business news sources, collected between 1987 and 1994 <ref> [Rudnicky95] </ref>. When trigram counts are too small, the model backs off to bigrams and then to simple corpus frequencies. This language model does not exactly match the characteristics of contemporary broadcast news, and efforts are underway to improve it.
Reference: [CMU-Speech95] <institution> URL: </institution> <note> http://www.speech.cs.cmu.edu/speech/ </note>
Reference-contexts: Although a strictly phonetic transcription of the data used in these experiments has not yet been generated, an approximation is achieved by converting story transcripts into a phonetic representation by looking up the words in the transcript in the large phonetic dictionary used by Sphinx-II <ref> [CMU-Speech95] </ref>. All substrings of between three and six phonemes in length are generated from these transcriptions and used as the lexical tokens for building the inverted index and retrieval. The prompts are also converted into phoneme based tokens in the same way.
Reference: [CMU-Speech96] <institution> URL: </institution> <note> http://www.speech.cs.cmu.edu/cgi-bin/cmudict </note>
Reference: [Salton71] <editor> Salton, G., Ed, </editor> <title> The SMART Retrieval System, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1971. </year>
Reference-contexts: Because each corpus has significantly different characteristics, one should resist the temptation to compare precision values. 9 METHOD OF EVALUATION The standard metrics for retrieval effectiveness in the information retrieval literature are precision and recall <ref> [Salton71] </ref>. Precision is defined as the number of correct (relevant) hits returned by the system divided by the number of total hits returned to the user.
Reference: [Schuble95] <author> Schuble, P. and Wechsler, M. </author> <title> First Experiences with a System for Content Based Retrieval of Information from Speech Recordings, </title> <booktitle> IJCAI-95 Workshop on Intelligent Multimedia Information Retrieval, </booktitle> <editor> Maybury, M. T., (chair), </editor> <booktitle> working notes, </booktitle> <pages> pp. 59 - 69, </pages> <month> August, </month> <year> 1995. </year>
Reference-contexts: In addition, CNN-AT-WORK does not feature an integrated multimodal query interface [CNN-AT-WORK95]. Preliminary investigation into the use of speech recognition for analysis of a news story was carried out by Schuble and Wechsler <ref> [Schuble95] </ref>. Since they lacked a powerful speech recognizer, their approach used a phonetic engine that transformed the spoken content of the news stories into possibly erroneous phoneme strings. The query was also transformed into a phoneme string and the database searched for the best approximate match. <p> This indicates that speech recognition generated transcripts are less focused on the correct topic and are more likely to be displaced by other apparently relevant stories from the distractor set. Experiment 4: Phonetic transcription compared with large vocabulary recognition In previous work, Schuble and Wechsler <ref> [Schuble95] </ref> performed experiments in which they used automatic phonetic transcriptions, as opposed to the whole word speech recognition transcripts described above, for information retrieval in a small radio news corpus. They reported reasonable success in retrieving relevant documents. <p> Words from SR 44.11 7.89 Words from Text less OOVs for SR 38.36 17.20 Phonemes from Text 17.74 9.75 Phonemes from SR 25.05 12.02 Text words + Text Phonemes interpolated 9.15 2.18 SR words + SR Phonemes interpolated 20.06 6.87 Experimental Summary The experiments confirm findings by Wechsler and Schuble <ref> [Schuble95] </ref>, that phoneme-based recognition using phoneme strings of different lengths can be used for effective information retrieval. <p> The challenge here is to reliably identify the prosodically marked words and to favor them as relevant terms. While the results presented here, as well as those from Schuble and Wechsler <ref> [Schuble95] </ref> and Jones et al [Jones96] show some success in information retrieval from spoken documents, much remains to be done. In the near term, the combination of whole-word recognition and phoneme recognition shows clear promise.
Reference: [Wactlar96] <author> Wactlar, H. D., Kanade, T., Smith, M. A. and Stevens, </author> <title> S.M. Intelligent Access to Digital Video: Informedia Project. </title> <booktitle> IEEE Computer, </booktitle> <month> 29(5) May </month> <year> 1996, </year> <pages> pp. 46-52. </pages>
Reference-contexts: Video summaries in the form of skims are created for each story at several different levels of detail. Like title generation, skim generation, as proposed in <ref> [Wactlar96] </ref>, also uses extractive text summarization, but in this case the extracted text representing the important concepts should be more evenly spread throughout the story. Skims are produced at a variety of lengths, covering 10.8 to 26 percent of the original story.
Reference: [Witten94] <author> Witten, I.H., Moffat, A., and Bell, </author> <title> T.C., Managing Gigabytes : Compressing and Indexing Documents and Images, </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Where possible, it is compared with actual precision and recall figures calculated with respect to human-generated relevance judgments. The baseline system uses a search engine based on TF and stop words (also referred to as coordinate matching by <ref> [Witten94] </ref>). The initial comparison used the 602-story corpus (set 2) to evaluate retrieval effectiveness for the closed-caption transcripts, the manual transcripts and the speech recognition transcripts in the set.
Reference: [Woods96] <author> Woods, Bill, </author> <title> Conceptually Indexed Video: Enhanced Storage and Retrieval . http://www.sun.com/960201/cover/video.html </title>
Reference-contexts: These latter experiments have not yet been extended to news or other broadcast data. 3 Other projects that seek to index and retrieve from video news sources include the Conceptually Indexed Video project at Sun <ref> [Woods96] </ref>, which is attempting to build conceptual taxonomies of query terms to improve the quality of returned stories, and the VISION system at the University of Kansas which, while similar in aim to Informedia, is concentrating on the problems of compressing video data and delivering it over the Internet.
Reference: [Zhang95] <author> Zhang, H., Low, C., and Smoliar, S. </author> <title> Video parsing and indexing of compressed data, </title> <booktitle> Multimedia Tools and Applications 1 (March 1995), </booktitle> <pages> pp. 89-111. </pages>
References-found: 25

