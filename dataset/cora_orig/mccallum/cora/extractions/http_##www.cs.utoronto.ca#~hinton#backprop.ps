URL: http://www.cs.utoronto.ca/~hinton/backprop.ps
Refering-URL: http://www.cs.toronto.edu/~hinton/backprop.html
Root-URL: 
Title: Experiments on Learning by Back Propagation  
Author: David C. Plaut Steven J. Nowlan Geoffrey E. Hinton 
Note: This research was supported by contract N00014-86-K-00167 from the Office of Naval Research, an R.K. Mellon Fellowship to David Plaut, and a scholarship from the Natural Science and Engineering Research Council of Canada to Steven Nowlan.  
Date: June 1986  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie-Mellon University  
Pubnum: Technical Report CMU-CS-86-126  
Abstract-found: 0
Intro-found: 1
Reference: [Ackley et al. 85] <author> David H. Ackley and Geoffrey E. Hinton and Terrance J. </author> <note> Sejnowski. </note>
Reference-contexts: Motivation for such study may be found in work which has investigated gain effects in other types of networks. The continuous valued units used in the back propagation networks can be related to the stochastic units used in Boltzmann Machines <ref> [Hinton and Sejnowski 83, Ackley et al. 85] </ref>. The sigmoid function used to determine the output value of the continuous unit is the same function used to determine the probability distribution of the state of a binary-valued Boltzmann unit. <p> It has been shown that simulated annealing is a good method to improve the ability of networks of stochastic units to settle on a globally optimal solution <ref> [Kirkpatrick et al. 83, Ackley et al. 85] </ref>. Since gain in iterative networks plays a role analogous to the inverse of temperature in Boltzmann Machines, allowing the system to vary the gain as it settles may also improve the convergence of iterative networks. <p> Two different tasks were used to investigate the effects of gain variation. The first task was the 4-1-4 encoder problem, very similar to the 4-2-4 encoder described in <ref> [Ackley et al. 85] </ref>. The 4-2-4 network has three layers, with 4 units in each of the input and output layers and 2 units in the hidden layer. Each hidden unit is connected to all of the input units and all of the output units.
References-found: 1

