URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P667.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: A R G O N N E N A T I O NA L L
Author: by William Gropp Ewing Lusk 
Date: June 1997  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> J. J. Dongarra, G. A. Geist, R. J. Manchek, and P. M. Papadopoulos. </author> <title> Adding context and static groups into PVM. </title> <note> http://www.epm.ornl.gov/pvm/context.ps, July 1995. </note>
Reference-contexts: Where these details differ from the corresponding details in PVM, the goal-oriented approach can elucidate the sources of the differences. In addition to differences in explicit goals, we will also note a few differences 1 We treat the Oak Ridge version of PVM as represented by <ref> [5, 1] </ref> as the PVM speci fication. MPI is represented by the MPI-2 specification. more attributable to the origin of the two systems. <p> Combining operations is a classic way to solve race conditions, and this solution is used in many places in MPI. Eliminating race conditions makes many operations in MPI are collective. Note that the PVM 3.4 pvm newcontext <ref> [1] </ref> presents a race condition in the delivery of the new context value to other processes; MPI solves this problem by making context creation collective over all processes that will use the context.
Reference: 2. <author> A. J. Ferrari and V. S. Sunderam. TPVM: </author> <title> Distributed concurrent computing with lightweight processes. </title> <booktitle> In IEEE, editor, Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> August 2-4, </pages> <address> 1995, Washington, DC, USA, </address> <pages> pages 211-218, </pages> <address> 1109 Spring Street, Suite 300, Silver Spring, MD 20910, USA, 1995. </address> <publisher> IEEE Computer Society Press. IEEE catalog no. 95TB8075. </publisher>
Reference-contexts: minor reason for the relatively large number of functions in MPI. - MPI would support heterogeneous computing (by providing the MPI Datatype object, it allows implementations to be heterogeneous), although it would not require that all implementations be heterogeneous. 2 There is a project to join threads with PVM (TPVM <ref> [2] </ref>), but this is more a lightweight process model than a fully threaded model and, as such, does not of fer as rich a programming model as a fully thread-safe model would. - MPI would require well-defined behavior (no race conditions or avoidable implementation-specific behavior).
Reference: 3. <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Applications, </journal> 8(3/4):159-416, 1994. 
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM [4] and MPI <ref> [3] </ref> are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published [9, 8, 6, 11]. We consider it worthwhile to do so again for two reasons.
Reference: 4. <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine: A Users' Guide and Tutorial for Networked Parallel Computing. Scientific and engineering computation. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM <ref> [4] </ref> and MPI [3] are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published [9, 8, 6, 11]. We consider it worthwhile to do so again for two reasons.
Reference: 5. <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 Users Guide and Reference manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> May 94. </month>
Reference-contexts: Where these details differ from the corresponding details in PVM, the goal-oriented approach can elucidate the sources of the differences. In addition to differences in explicit goals, we will also note a few differences 1 We treat the Oak Ridge version of PVM as represented by <ref> [5, 1] </ref> as the PVM speci fication. MPI is represented by the MPI-2 specification. more attributable to the origin of the two systems.
Reference: 6. <author> G. A. Geist, J. A. Kohl, and P. M. Papadopoulos. </author> <title> PVM and MPI: A comparison of features. </title> <journal> Calculateurs Paralleles, </journal> <volume> 8(2), </volume> <year> 1996. </year>
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM [4] and MPI [3] are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published <ref> [9, 8, 6, 11] </ref>. We consider it worthwhile to do so again for two reasons.
Reference: 7. <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The degree to which users want such programs to work was shown by the public reaction to the MPI 1 draft that did not provide a buffered send; the MPI Forum added the buffered send to satisfy this need. See <ref> [7, 12] </ref> for a more detailed introduction to MPI's handling of buffering. It is worth noting that the Unix socket interface provides a solution much like the MPI nonblocking operations, though somewhat less convenient for the user.
Reference: 8. <author> J. C. Hardwick. </author> <title> Porting a vector library: a comparison of MPI, Paris, CMMD and PVM. </title> <booktitle> In IEEE, editor, Proceedings of the 1994 Scalable Parallel Libraries Conference: </booktitle> <month> October 12-14, </month> <year> 1994, </year> <institution> Mississippi State University, </institution> <address> Mississippi, </address> <pages> pages 68-77, </pages> <address> 1109 Spring Street, Suite 300, Silver Spring, MD 20910, USA, 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM [4] and MPI [3] are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published <ref> [9, 8, 6, 11] </ref>. We consider it worthwhile to do so again for two reasons.
Reference: 9. <author> R. Hempel. </author> <title> The status of the MPI message-passing standard and its relation to PVM. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 1156 </volume> <pages> 14-21, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM [4] and MPI [3] are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published <ref> [9, 8, 6, 11] </ref>. We consider it worthwhile to do so again for two reasons.
Reference: 10. <author> Steven A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: These facilities are fully integrated with MPI's other functions. In PVM's case, while there are some projects like PIOUS <ref> [10] </ref>, there is no integrated parallel I/O capability. This reflects the differences in the orientation of the two systems: many of the parallel I/O functions are collective and are best defined in terms of static groups, such as MPI defines.
Reference: 11. <author> William Saphir. Devil's advocate: </author> <title> Reasons not to use PVM, </title> <month> May </month> <year> 1994. </year> <title> PVM User Group Meeting. </title>
Reference-contexts: 1 Introduction Although they came into existence in quite different ways, PVM [4] and MPI [3] are both specifications 1 for libraries that can be used for parallel computing. It is is natural to compare them. Indeed, many useful comparisons have been published <ref> [9, 8, 6, 11] </ref>. We consider it worthwhile to do so again for two reasons.
Reference: 12. <author> Marc Snir, Steve Otto, Steven Huss-Lederman, David Walker, and Jack Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The degree to which users want such programs to work was shown by the public reaction to the MPI 1 draft that did not provide a buffered send; the MPI Forum added the buffered send to satisfy this need. See <ref> [7, 12] </ref> for a more detailed introduction to MPI's handling of buffering. It is worth noting that the Unix socket interface provides a solution much like the MPI nonblocking operations, though somewhat less convenient for the user.
Reference: 13. <author> Web page: </author> <title> Introduction to the totalview debugger. http://www.dolphinics.com/tw/tv/totalview.html. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: MPI, as a standard, has no such object, but specific MPI implementations can and do provide similar services; for example, the MPICH implementation of MPI provides a process startup hook used by the Totalview <ref> [13] </ref> debugger. The MPI standard does not specify how implementations are to provide this service; as a standard, it should not.
References-found: 13

