URL: http://www.research.att.com/~mkearns/papers/pruning.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: mkearns@research.att.com  mansour@math.tau.ac.il  
Title: A Fast, Bottom-Up Decision Tree Pruning Algorithm with Near-Optimal Generalization  
Author: Michael Kearns Yishay Mansour 
Keyword: Decision Trees, Pruning, Theoretical Analysis, Model Selection, Uniform Convergence  
Date: February 27, 1998  
Affiliation: AT&T Labs  Tel Aviv University  
Abstract: In this work, we present a new bottom-up algorithm for decision tree pruning that is very efficient (requiring only a single pass through the given tree), and prove a strong performance guarantee for the generalization error of the resulting pruned tree. We work in the typical setting in which the given tree T may have been derived from the given training sample S, and thus may badly overfit S. In this setting, we give bounds on the amount of additional generalization error that our pruning suffers compared to the optimal pruning of T . More generally, our results show that if there is a pruning of T with small error, and whose size is small compared to jSj, then our algorithm will find a pruning whose error is not much larger. This style of result has been called an index of resolvability result by Barron and Cover in the context of density estimation. A novel feature of our algorithm is its locality | the decision to prune a subtree is based entirely on properties of that subtree and the sample reaching it. To analyze our algorithm, we develop tools of local uniform convergence, a generalization of the standard notion that may prove useful in other settings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew R. Barron and Thomas M. </author> <title> Cover. Minimum Complexity Density Estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 37, No. 4, </volume> <pages> pages 1034 - 1054, </pages> <year> 1991. </year>
Reference-contexts: Thus, if there is a relatively small subtree of T with small error, our algorithm enjoys a strong performance guarantee. This type of guarantee is fairly common in the model selection literature, and is sometimes referred to as an index of resolvability guarantee <ref> [1] </ref>. (It is also similar to the types of results stated in the literature on combining "experts" [4], although the interest there is not in generalization error but on-line prediction.) Our algorithm is a simple, bottom-up algorithm that performs a single pass over the tree T ; hence its running time <p> Then our algorithm will replace T fl v by this best leaf if and only if ^* v (T fl where ffi 2 <ref> [0; 1] </ref> is a confidence parameter. The exact choice of ff (m v ; s v ; ` v ; ffi) will depend on the setting, but in all cases can be thought of as a penalty for the complexity of the subtree T fl v . <p> In such a case, we would rather claim that our error is close to that of T 0 , with a penalty that goes only like p s 0 =m. This was the index of resolvability criterion for model selection first examined for density estimation by Barron and Cover <ref> [1] </ref>, and we now generalize our main result to this setting. Theorem 6 Let S be a random sample of size m drawn according an unknown target function and input distribution.
Reference: [2] <author> Marco Bohanec and Ivan Bratko. </author> <title> Trading Accuracy for simplicity in Decision Trees. </title> <journal> Machine Learning, </journal> <volume> Vol. 15, </volume> <pages> pages 223 - 250, </pages> <year> 1994. </year>
Reference-contexts: The use of dynamic programming for pruning was already suggested in the original book on CART [3] in order to minimize a weighted sum of the observed error and the size of the pruning. Bohanec and Bratko <ref> [2] </ref> showed that it is possible to compute in quadratic time the subtree of a given tree that minimizes the training error while obeying a specified size bound. <p> In the hold-out setting, a good algorithm is one that chooses the tree in prunings (T ) that minimizes the error on S (which can be computed in polynomial time via a dynamic programming approach <ref> [2] </ref>), and fairly general performance guarantees can be shown [6] that necessarily weaken as the hold-out set becomes a smaller fraction of the original data sample. 3 Description of the Pruning Algorithm We begin with a detailed description of the pruning algorithm, which is given the random sample S and a
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen, C.J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: In particular, we allow for the possibility that T was in fact constructed from S, perhaps by a standard greedy, top-down process as employed in the growth phases of the C4.5 and CART algorithms <ref> [8, 3] </ref>. Our interest here is in how one should best use the data S a second time to find a good subtree of T . Note that in the setting we imagine, T itself may badly overfit the data. <p> However, in the on-line prediction model of learning, their result is quite strong. Here we study the typical batch model in which we may not assume independence of our tree and data set. The use of dynamic programming for pruning was already suggested in the original book on CART <ref> [3] </ref> in order to minimize a weighted sum of the observed error and the size of the pruning. Bohanec and Bratko [2] showed that it is possible to compute in quadratic time the subtree of a given tree that minimizes the training error while obeying a specified size bound.
Reference: [4] <author> David P. Helmbold and Robert E. Schapire. </author> <title> Predicting Nearly as Well as the Best Pruning of a Decision Tree. </title> <booktitle> Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <publisher> ACM Press, </publisher> <pages> pages 61 - 68, </pages> <year> 1995. </year>
Reference-contexts: This type of guarantee is fairly common in the model selection literature, and is sometimes referred to as an index of resolvability guarantee [1]. (It is also similar to the types of results stated in the literature on combining "experts" <ref> [4] </ref>, although the interest there is not in generalization error but on-line prediction.) Our algorithm is a simple, bottom-up algorithm that performs a single pass over the tree T ; hence its running time is linear in size (T ). <p> First of all, our pruning algorithm is closely related to one proposed by Mansour [7], who emphasized the locality property and gave primarily experimental results, but was not able to bound the generalization error of the resulting pruned tree. Helmbold and Schapire <ref> [4] </ref> gave an efficient algorithm for predicting nearly as well as the best pruning of a given tree. However, this algorithm differs from ours in a number of important ways.
Reference: [5] <author> Michael Kearns and Yishay Mansour. </author> <title> On the Boosting Ability of Top-Down Decision Tree Learning Algorithms. </title> <booktitle> Proceedings of the 28th Annual ACM Symposium on the Theory of Computing, </booktitle> <publisher> ACM Press, </publisher> <pages> pages 459-468, </pages> <year> 1996. </year>
Reference-contexts: Combined with earlier results proving non-trivial performance guarantees for the common greedy, top-down growth heuristics in the model of boosting <ref> [5] </ref>, it is fair to say that there is now a solid theoretical basis for both the top-down and bottom-up passes of many standard decision tree learning algorithms. 2 Framework and Preliminaries We consider decision trees over an input domain X .
Reference: [6] <author> M. Kearns, Y. Mansour, A. Ng, D. Ron. </author> <title> An Experimental and Theoretical Comparison of Model Selection Methods. </title> <journal> Machine Learning, </journal> <volume> 27(1) </volume> <pages> 7-50, </pages> <year> 1997. </year>
Reference-contexts: There is a trade-off that renders the two scenarios incomparable in general <ref> [6] </ref>: by using a hold-out set for the pruning phase, we gain the independence of the sample from the given tree T , but at the price of having "wasted" some potentially valuable data for the training (construction) of T ; whereas in our setting, we waste no data, but cannot <p> In the hold-out setting, a good algorithm is one that chooses the tree in prunings (T ) that minimizes the error on S (which can be computed in polynomial time via a dynamic programming approach [2]), and fairly general performance guarantees can be shown <ref> [6] </ref> that necessarily weaken as the hold-out set becomes a smaller fraction of the original data sample. 3 Description of the Pruning Algorithm We begin with a detailed description of the pruning algorithm, which is given the random sample S and a tree T = T (S) as input.
Reference: [7] <author> Yishay Mansour. </author> <title> Pessimistic Decision Tree Pruning Based on Tree Size. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pages 195 - 201, </pages> <year> 1997. </year> <month> 15 </month>
Reference-contexts: There are a number of previous efforts related to our results, which we only have space to discuss briefly here; more detailed comparisons will be given in the full paper. First of all, our pruning algorithm is closely related to one proposed by Mansour <ref> [7] </ref>, who emphasized the locality property and gave primarily experimental results, but was not able to bound the generalization error of the resulting pruned tree. Helmbold and Schapire [4] gave an efficient algorithm for predicting nearly as well as the best pruning of a given tree.
Reference: [8] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In particular, we allow for the possibility that T was in fact constructed from S, perhaps by a standard greedy, top-down process as employed in the growth phases of the C4.5 and CART algorithms <ref> [8, 3] </ref>. Our interest here is in how one should best use the data S a second time to find a good subtree of T . Note that in the setting we imagine, T itself may badly overfit the data. <p> However, this algorithm would be considerably less efficient than the one we shall present. Finally, our ideas are certainly influenced by the many single-pass, bottom-up pruning heuristics in wide use in experimental machine learning, including that used by C4.5 <ref> [8] </ref>. While we do not know how to prove strong error guarantees for these heuristics, our current results provide some justification for them, and suggest specific modifications that yield fast, practical and principled methods for pruning with proven error guarantees. <p> Thus, any non-empty tree in prunings (T ) shares the same root as T , and can be "superimposed" on T . In particular, we are not allowing "surgical" operations such as the replacement of an internal node by its left or right subtree <ref> [8] </ref>. Nevertheless, the class prunings (T ) contains an exponential number of subtrees of T , and our goal will be find a tree in prunings (T ) with close to the smallest generalization error.
Reference: [9] <author> V.N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the Uniform Convergence of Relative Frequencies of Events to thier Probabilities. Theory of Probability and its Applications, </title> <publisher> XVI(2):264-280,1971. </publisher>
Reference-contexts: Proof:(Sketch) The proof closely follows the "two-sample trick" proof for the classical VC theorem <ref> [9] </ref>, with an important variation. Intuitively, we introduce a "nested two-sample trick", since we need to apply the idea twice | once for C, and again for H. As in the classical proof, we define two events, but now they are "local" events.
Reference: [10] <author> V.N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Bohanec and Bratko [2] showed that it is possible to compute in quadratic time the subtree of a given tree that minimizes the training error while obeying a specified size bound. By combining this observation with the ideas of structural risk minimization <ref> [10] </ref>, it is possible to derive a polynomial-time algorithm for our setting with error guarantees quite similar to those we will give for our algorithm. However, this algorithm would be considerably less efficient than the one we shall present. <p> How good is this? Since we assume that T itself (and therefore, all subtrees of T ) may have been constructed from the sample S, standard model selection analyses <ref> [10] </ref> indicate that * opt may be larger than the error of the best decision tree approximation to the target function by an amount growing like q s opt =m. (Recall that * opt is only the error of the optimal subtree of T | there may be other trees which
References-found: 10

