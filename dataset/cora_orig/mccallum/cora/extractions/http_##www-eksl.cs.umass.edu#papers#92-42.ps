URL: http://www-eksl.cs.umass.edu/papers/92-42.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Title: Analyzing Failure Recovery to Improve Planner Design  
Author: Adele E. Howe 
Note: This research was supported by DARPA-AFOSR contract F49620-89-C-00113, by the National Science Foundation under an Issues in Real-Time Computing grant, CDA-8922572, and by a grant from the Texas Instruments Corporation.  
Address: Amherst, Massachusetts 01003  
Affiliation: Computer Science  Experimental Knowledge Systems Laboratory Department of Computer Science University of Massachusetts  
Pubnum: Technical Report 92-42  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Atkinson, Mark James, Harry Porta, and Richard Doyle. </author> <title> Autonomous task level control of a robot. </title> <booktitle> In Proceedings of ROBEXS 86, Second Annual Workshop on Robotics and Expert Systems, </booktitle> <pages> pages 117-122, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Plans fail for perfectly good reasons: the environment changes unpredictably, sensors return flaky data [10], and effectors do not work as expected [6]. During planner development, plans fail for not so good reasons: the effects of actions are not adequately specified <ref> [1] </ref>, apparently unrelated actions interact [13], and the domain model is incomplete and incorrect [2]. Planners should not cause their own failures, but figuring out what went wrong and preventing it later is not easy. Failures tell us what went wrong, but not why.
Reference: [2] <author> R.T. Chien and S. Weissman. </author> <title> Planning and execution in incompletely specified environments. </title> <booktitle> In Proceedings of the Fourth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 160-174, </pages> <address> Tiblisi, Georgia, USSR, </address> <year> 1975. </year>
Reference-contexts: During planner development, plans fail for not so good reasons: the effects of actions are not adequately specified [1], apparently unrelated actions interact [13], and the domain model is incomplete and incorrect <ref> [2] </ref>. Planners should not cause their own failures, but figuring out what went wrong and preventing it later is not easy. Failures tell us what went wrong, but not why.
Reference: [3] <author> Paul R. Cohen, Michael Greenberg, David M. Hart, and Adele E. Howe. </author> <title> Trial by fire: Understanding the design requirements for agents in complex environments. </title> <journal> AI Magazine, </journal> <volume> 10(3), </volume> <month> Fall </month> <year> 1989. </year> <month> 8 </month>
Reference-contexts: Failure recovery analysis of these experiments should explain why a well-justified modification to the planner produced such havoc. The Phoenix system is a simulator of forest fire fighting in Yellowstone National Park and an agent architecture <ref> [3] </ref>. A single agent, the fireboss, coordinates the efforts of field agents who build fireline to contain the spread of the fire. Its spread is influenced by weather and terrain, but even when these factors remain constant, the fire's spread is unpredictable.
Reference: [4] <author> Kristian J. Hammond. </author> <title> Explaining and repairing plans that fail. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 109-114, </pages> <address> Milan, Italy, </address> <year> 1987. </year> <booktitle> International Joint Council on Artificial Intelligence. </booktitle>
Reference-contexts: Most approaches to debugging planners are knowledge intensive. Sussman's hacker [13] detects, classifies and repairs bugs in blocks world plans, but it requires considerable knowledge about its domain. Hammond's chef <ref> [4] </ref> backchains from failure to the states that caused it, applying causal rules that describe the effects of actions. Simmons's gordius [12] debugs faulty plans by regressing desired effects through a causal dependency structure constructed during plan generation from a causal model of the domain. <p> The remaining steps have been tailored to Phoenix, but conceptually could be expanded for other planners. These three steps are based on explaining failures by matching patterns to explanations and modifications (as in the "retrieve-and-apply" approach [11]). The previous application of the "retrieve-and-apply" approach to debugging other planners (e.g., <ref> [13, 4] </ref>) suggests that generalizing FRA involves expanding its model of the planner the set of suggestive structures and explanations to include ones appropriate for other planners. Beyond the need to expand the underlying knowledge, FRA will need to be extended in other ways as well.
Reference: [5] <author> David M. Hart, Paul R. Cohen, and Scott D. Anderson. </author> <title> Envelopes as a vehicle for improving the efficiency of plan execution. </title> <editor> In Katia P. Sycara, editor, </editor> <booktitle> Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 71-76, </pages> <address> Palo Alto, Ca., November 1990. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Failure F vit is detected when plan monitoring indicates that progress against the fire has been insufficient and not enough time remains to complete the plan. F vit is detected by an envelope action (a structure for comparing expected to actual progress <ref> [5] </ref>) called indirect-attack-envelope (A env ). Identifying Structures that Lead to Failure: The plan library is searched for plan structures that govern the interaction between the actions of the dependency.
Reference: [6] <author> Philip J. Hayes. </author> <title> A representation for robot plans. </title> <booktitle> In Proceedings of the Fourth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 181-188, </pages> <address> Tiblisi, Georgia, USSR, </address> <year> 1975. </year> <booktitle> International Joint Council on Artificial Intelligence. </booktitle>
Reference-contexts: 1 Introduction Plans fail for perfectly good reasons: the environment changes unpredictably, sensors return flaky data [10], and effectors do not work as expected <ref> [6] </ref>. During planner development, plans fail for not so good reasons: the effects of actions are not adequately specified [1], apparently unrelated actions interact [13], and the domain model is incomplete and incorrect [2].
Reference: [7] <author> Adele E. Howe. </author> <title> Accepting the Inevitable: The Role of Failure Recovery in the Design of Planners. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer Science, </institution> <address> Amherst, MA, </address> <year> 1992. </year> <month> Forthcoming. </month>
Reference-contexts: This paper presents a procedure, called failure recovery analysis (FRA), for analyzing execution traces of failure recovery to discover when and how the planner's actions may be causing failures <ref> [7] </ref>. Most approaches to debugging planners are knowledge intensive. Sussman's hacker [13] detects, classifies and repairs bugs in blocks world plans, but it requires considerable knowledge about its domain. Hammond's chef [4] backchains from failure to the states that caused it, applying causal rules that describe the effects of actions.
Reference: [8] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Failure recovery: A model and experiments. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 801-808, </pages> <address> Anaheim, CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: In FRA, the designer decides how best to repair the failures. 1.1 The Planner and its Environment Previous experiments and analyses of failure recovery in the Phoenix system (introduced below) showed that changing how the planner recovers from failures changed the type and frequency of failures encountered <ref> [8] </ref>. In these experiments, seemingly minor changes to the design of Phoenix's failure recovery component, such as adding two new failure recovery actions with limited applicability, had unexpected consequences. Failure recovery analysis of these experiments should explain why a well-justified modification to the planner produced such havoc.
Reference: [9] <author> Subbarao Kambhampati. </author> <title> A theory of plan modification. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence, </booktitle> <pages> pages 176-182, </pages> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Hammond's chef [4] backchains from failure to the states that caused it, applying causal rules that describe the effects of actions. Simmons's gordius [12] debugs faulty plans by regressing desired effects through a causal dependency structure constructed during plan generation from a causal model of the domain. Kambhampati's approach <ref> [9] </ref> requires the planner to generate validation structures, explanations of correctness for the plan. His theory of plan modification compares the validation structure to the planning situation, detects inconsistencies, and uses the validation structure to guide the repair of the plan.
Reference: [10] <author> M.H. Lee, D.P. Barnes, and N.W. Hardy. </author> <title> Knowledge based error recovery in industrial robots. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 824-826, </pages> <address> Karlsruhe, West Germany, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction Plans fail for perfectly good reasons: the environment changes unpredictably, sensors return flaky data <ref> [10] </ref>, and effectors do not work as expected [6]. During planner development, plans fail for not so good reasons: the effects of actions are not adequately specified [1], apparently unrelated actions interact [13], and the domain model is incomplete and incorrect [2].
Reference: [11] <author> Christopher Owens. </author> <title> Representing abstract plan failures. </title> <booktitle> In Proceedings of the Twelfth Cognitive Science Conference, </booktitle> <pages> pages 277-284, </pages> <address> Boston, MA, 1990. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: The remaining steps have been tailored to Phoenix, but conceptually could be expanded for other planners. These three steps are based on explaining failures by matching patterns to explanations and modifications (as in the "retrieve-and-apply" approach <ref> [11] </ref>). The previous application of the "retrieve-and-apply" approach to debugging other planners (e.g., [13, 4]) suggests that generalizing FRA involves expanding its model of the planner the set of suggestive structures and explanations to include ones appropriate for other planners.
Reference: [12] <author> Reid G. Simmons. </author> <title> A theory of debugging plans and interpretations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 94-99, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Sussman's hacker [13] detects, classifies and repairs bugs in blocks world plans, but it requires considerable knowledge about its domain. Hammond's chef [4] backchains from failure to the states that caused it, applying causal rules that describe the effects of actions. Simmons's gordius <ref> [12] </ref> debugs faulty plans by regressing desired effects through a causal dependency structure constructed during plan generation from a causal model of the domain. Kambhampati's approach [9] requires the planner to generate validation structures, explanations of correctness for the plan.
Reference: [13] <author> Gerald A. Sussman. </author> <title> A computational model of skill acquisition. </title> <type> Technical Report Memo no. </type> <institution> AI-TR-297, MIT AI Lab, </institution> <year> 1973. </year> <month> 9 </month>
Reference-contexts: 1 Introduction Plans fail for perfectly good reasons: the environment changes unpredictably, sensors return flaky data [10], and effectors do not work as expected [6]. During planner development, plans fail for not so good reasons: the effects of actions are not adequately specified [1], apparently unrelated actions interact <ref> [13] </ref>, and the domain model is incomplete and incorrect [2]. Planners should not cause their own failures, but figuring out what went wrong and preventing it later is not easy. Failures tell us what went wrong, but not why. <p> This paper presents a procedure, called failure recovery analysis (FRA), for analyzing execution traces of failure recovery to discover when and how the planner's actions may be causing failures [7]. Most approaches to debugging planners are knowledge intensive. Sussman's hacker <ref> [13] </ref> detects, classifies and repairs bugs in blocks world plans, but it requires considerable knowledge about its domain. Hammond's chef [4] backchains from failure to the states that caused it, applying causal rules that describe the effects of actions. <p> The remaining steps have been tailored to Phoenix, but conceptually could be expanded for other planners. These three steps are based on explaining failures by matching patterns to explanations and modifications (as in the "retrieve-and-apply" approach [11]). The previous application of the "retrieve-and-apply" approach to debugging other planners (e.g., <ref> [13, 4] </ref>) suggests that generalizing FRA involves expanding its model of the planner the set of suggestive structures and explanations to include ones appropriate for other planners. Beyond the need to expand the underlying knowledge, FRA will need to be extended in other ways as well.
References-found: 13

