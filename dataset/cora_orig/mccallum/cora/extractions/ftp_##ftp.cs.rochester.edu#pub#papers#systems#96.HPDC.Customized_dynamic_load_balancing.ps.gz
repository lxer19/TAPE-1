URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/96.HPDC.Customized_dynamic_load_balancing.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/zaki/papers.html
Root-URL: 
Email: fzaki,wei,srinig@cs.rochester.edu  
Title: Customized Dynamic Load Balancing for a Network of Workstations  
Author: Mohammed Javeed Zaki, Wei Li, Srinivasan Parthasarathy 
Address: Rochester, Rochester NY 14627  
Affiliation: Computer Science Department, University of  
Abstract: Load balancing involves assigning to each processor, work proportional to its performance, minimizing the execution time of the program. Although static load balancing can solve many problems (e.g., those caused by processor heterogeneity and non-uniform loops) for most regular applications, the transient external load due to multiple-users on a network of workstations necessitates a dynamic approach to load balancing. In this paper we examine the behavior of global vs local, and centralized vs distributed, load balancing strategies. We show that different schemes are best for different applications under varying program and system parameters. Therefore, customized load balancing schemes become essential for good performance. We present a hybrid compile-time and run-time modeling and decision process which selects (customizes) the best scheme, along with automatic generation of parallel code with calls to a runtime library for load balancing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> J.N.C. Arabe et. al. Dome: </editor> <title> parallel programming in a heterogeneous multi-user environment. </title> <type> CMU-CS-95-137 30786, </type> <institution> Carnegie Mellon Univ, </institution> <month> Apr </month> <year> 1995. </year>
Reference-contexts: For example, in [9], a global distributed scheme is presented, and load balancing involves periodic information exchanges. Dome <ref> [1] </ref> implements a global central and a local distributed scheme, and the load balancing involves periodic exchanges. Siegell [13] also presented a global centralized scheme, with periodic information exchanges. The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing.
Reference: [2] <author> R. Blumofe et al. </author> <title> Scheduling large-scale parallel computations on NOW. </title> <booktitle> 3rd HPDC, </booktitle> <month> Apr </month> <year> 1994. </year>
Reference-contexts: Siegell [13] also presented a global centralized scheme, with periodic information exchanges. The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing. In Phish <ref> [2] </ref>, a local distributed receiver-initiated scheme is described, where the processor requesting more tasks, chooses a processor at random from which to steal more work. CHARM [12] implements a local distributed receiver-initiated scheme.
Reference: [3] <author> M. Cierniak, W. Li, and M. J. Zaki. </author> <title> Loop scheduling for heterogeneity. </title> <booktitle> In 4th HPDC, </booktitle> <month> Aug </month> <year> 1995. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account [7]. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in <ref> [3] </ref>. 2.2 Dynamic Scheduling Predicting the Future: A common approach taken for load balancing on a workstation network is to predict future performance based on past information. For example, in [9], a global distributed scheme is presented, and load balancing involves periodic information exchanges. <p> The first loop nest is uniform, with n (n + 1)=2 iterations. The work per iteration is linear in the array size ( 2n + 4). The second loop nest has triangular work per iteration. We transform this into a uniform loop using the bitonic scheduling technique <ref> [3] </ref>, i.e., by combining iterations i and n (n + 1)=2 i + 1 into one iteration. The number of iterations for loop 2 is now given as n (n + 1)=4. The work is also linear in the array size.
Reference: [4] <author> L. Kipp (ed.). </author> <title> Perfect Benchmarks Doc, </title> <type> Suite 1. </type> <institution> CSRD, Univ. of Illinois, Urbana-Champaign, </institution> <month> Oct </month> <year> 1993. </year>
Reference-contexts: PVM [5], a message passing software system, was used to parallelize the applications. The applications we consider are given below: * Matrix Multiplication (MXM): Multiplication of two matrices. * TRFD: TRFD is part of the Perfect Benchmark application suite <ref> [4] </ref>. It simulates the computational aspects of two-electron integral transformations. We used a modified version of TRFD, which was enhanced to exploit the paral-lelism. * Adjoint Convolution (AC): Convolution of two n 2 length vectors. Results for AC are not presented due to lack of space.
Reference: [5] <author> A. Geist et al. </author> <title> PVM 3 user's guide & ref. manual. </title> <institution> ORNL/TM-12187, Oak Ridge Nat. Lab, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The compiler, however, generates code to handle this at run-time. The compiler can also help to generate symbolic cost functions for the iteration and communication cost. 5.2 Code Generation For the source-to-source code translation from a sequential program to a parallel program using PVM <ref> [5] </ref> for message passing, with DLB library calls, we use the Stanford University Intermediate Format (SUIF) [15] compiler. <p> We then present our modeling results for the applications. All the experiments were performed on a network of homogeneous Sun (Sparc LX) workstations, interconnected via an Ethernet LAN. External load was simulated within our programs as described in section 4. PVM <ref> [5] </ref>, a message passing software system, was used to parallelize the applications. The applications we consider are given below: * Matrix Multiplication (MXM): Multiplication of two matrices. * TRFD: TRFD is part of the Perfect Benchmark application suite [4]. It simulates the computational aspects of two-electron integral transformations.
Reference: [6] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-16, </pages> <month> Oct </month> <year> 1985. </year>
Reference-contexts: Once the processors have finished their assigned portion, more work is obtained from this queue. The simplest approach in this model is self-scheduling [14], where each processor is allocated only one iteration at a time. In fixed-size chunking <ref> [6] </ref>, each processor is allocated K iterations, while in guided self-scheduling [11] each processor is assigned 1=P -th of the remaining iterations, where P is the number of processors. Affinity scheduling [8] also takes processor affinity into account.
Reference: [7] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <journal> ACM Trans on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 353-375, </pages> <month> Nov </month> <year> 1993. </year>
Reference-contexts: For UMA (Uniform Memory Access) parallel machines, usually loop iterations can be scheduled in block or cyclic fashion. For NUMA (Non-Uniform Memory Access) parallel machines, loop scheduling has to take data distribution into account <ref> [7] </ref>. Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in [3]. 2.2 Dynamic Scheduling Predicting the Future: A common approach taken for load balancing on a workstation network is to predict future performance based on past information.
Reference: [8] <author> E.P. Markatos and T.J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Trans Parallel Dist Sys, </journal> <volume> 5(4), </volume> <month> Apr </month> <year> 1994. </year>
Reference-contexts: In fixed-size chunking [6], each processor is allocated K iterations, while in guided self-scheduling [11] each processor is assigned 1=P -th of the remaining iterations, where P is the number of processors. Affinity scheduling <ref> [8] </ref> also takes processor affinity into account.
Reference: [9] <author> N. Nedeljkovic and M. J. Quinn. </author> <title> Data-parallel programming on a network of heterogeneous workstations. </title> <booktitle> 1st HPDC, </booktitle> <month> Sep </month> <year> 1992. </year>
Reference-contexts: Static scheduling algorithms for heterogeneous programs, processors, and network were proposed in [3]. 2.2 Dynamic Scheduling Predicting the Future: A common approach taken for load balancing on a workstation network is to predict future performance based on past information. For example, in <ref> [9] </ref>, a global distributed scheme is presented, and load balancing involves periodic information exchanges. Dome [1] implements a global central and a local distributed scheme, and the load balancing involves periodic exchanges. Siegell [13] also presented a global centralized scheme, with periodic information exchanges.
Reference: [10] <author> H. Nishikawa and P. Steenkiste. </author> <title> A general architecture for load balancing in a distributed-memory environment. </title> <booktitle> Intl. Conf. Dist. Computing, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In the local schemes, instead of random selection of a processor from which to request more work, work is exchanged among all the neighbors (the number of neighbors is selected statically). These strategies are explained in more detail in the next section. In <ref> [10] </ref>, an approach was presented, where a user specifies homogeneous load balancers for different tasks within a heterogeneous application. They also present a global load balancer that handles the interactions among the different homogeneous load balancers.
Reference: [11] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Trans on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> Dec </month> <year> 1987. </year>
Reference-contexts: Once the processors have finished their assigned portion, more work is obtained from this queue. The simplest approach in this model is self-scheduling [14], where each processor is allocated only one iteration at a time. In fixed-size chunking [6], each processor is allocated K iterations, while in guided self-scheduling <ref> [11] </ref> each processor is assigned 1=P -th of the remaining iterations, where P is the number of processors. Affinity scheduling [8] also takes processor affinity into account.
Reference: [12] <author> V. Saletore et al. </author> <title> Parallel computations on the charm heterogeneous workstn. cluster. </title> <booktitle> 3rd HPDC, </booktitle> <month> Apr </month> <year> 1994. </year>
Reference-contexts: The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing. In Phish [2], a local distributed receiver-initiated scheme is described, where the processor requesting more tasks, chooses a processor at random from which to steal more work. CHARM <ref> [12] </ref> implements a local distributed receiver-initiated scheme. The information exchanged is the Forecasted Finish Time (FFT), i.e., the time for the processor to finish the remaining work. If the FFT falls below a threshold, the node requests a neighbor with higher FFT for more work.
Reference: [13] <author> B.S. Siegell. </author> <title> Automatic generation of parallel programs with dynamic load balancing for a network of workstations. </title> <type> CMU-CS-95-168 30880, </type> <institution> Carnegie Mel-lon Univ. - Sch. of Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: For example, in [9], a global distributed scheme is presented, and load balancing involves periodic information exchanges. Dome [1] implements a global central and a local distributed scheme, and the load balancing involves periodic exchanges. Siegell <ref> [13] </ref> also presented a global centralized scheme, with periodic information exchanges. The main contribution of this paper was the methodology for automatic generation of parallel programs with dynamic load balancing.
Reference: [14] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In ICPP, </booktitle> <month> Aug </month> <year> 1986. </year>
Reference-contexts: These fall under the task queue model, where there is a logically central task queue of loop iterations. Once the processors have finished their assigned portion, more work is obtained from this queue. The simplest approach in this model is self-scheduling <ref> [14] </ref>, where each processor is allocated only one iteration at a time. In fixed-size chunking [6], each processor is allocated K iterations, while in guided self-scheduling [11] each processor is assigned 1=P -th of the remaining iterations, where P is the number of processors.
Reference: [15] <author> R. Wilson et. al. </author> <title> An overview of the suif compiler system. </title> <type> Unpublished manuscript, </type> <institution> Stanford Univ. </institution>
Reference-contexts: The compiler can also help to generate symbolic cost functions for the iteration and communication cost. 5.2 Code Generation For the source-to-source code translation from a sequential program to a parallel program using PVM [5] for message passing, with DLB library calls, we use the Stanford University Intermediate Format (SUIF) <ref> [15] </ref> compiler. The input to the compiler consists of the sequential version of the code, with annotations to indicate the data decomposition for the shared arrays, and to indicate the loops which have to be load balanced.
Reference: [16] <author> M. Zaki et al. </author> <title> Performance impact of processor and memory heterogeneity in a network of machines. </title> <booktitle> 4th Heterogeneous Computing Wkshp, </booktitle> <month> Apr </month> <year> 1995. </year>
Reference-contexts: This number is specified by the user, and is denoted as P . * Processor Speeds: This specifies the ratio of a processor's performance w.r.t a base processor. Since this ratio is application specific <ref> [16] </ref>, we can obtain this by a profiling run. We may also try to predict this at compile-time.
Reference: [17] <author> M. Zaki et al. </author> <title> Customized dynamic load balancing for NOW. </title> <institution> TR602, Univ. of Rochester, </institution> <month> Dec </month> <year> 1995. </year>
Reference-contexts: We used a modified version of TRFD, which was enhanced to exploit the paral-lelism. * Adjoint Convolution (AC): Convolution of two n 2 length vectors. Results for AC are not presented due to lack of space. We refer the reader to <ref> [17] </ref> for more details. 6.1 Network Characterization The network characterization is done off-line. We measure the latency and bandwidth for the network, and we obtain models for the different types of communication patterns among the processors (e.g. all-to-one, one-to-all, all-to-all).
References-found: 17

