URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-tr95/nl-tr95-letter.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers.html
Root-URL: http://www.neci.nj.nec.com
Email: lawrence@elec.uq.oz.au, fgiles,sandiwayg@research.nj.nec.com  
Title: On the Applicability of Neural Network and Machine Learning Methodologies to Natural Language Processing  
Author: Steve Lawrence C. Lee Giles Sandiway Fong 
Note: Also with  Also with the  
Address: 4 Independence Way Princeton, NJ 08540  College Park, MD 20742  4072, Australia.  College Park, MD 20742.  
Affiliation: NEC Research Institute  Institute for Advanced Computer Studies University of Maryland  Electrical and Computer Engineering, University of Queensland, St. Lucia Qld  Institute for Advanced Computer Studies, University of Maryland,  
Pubnum: Technical Report UMIACS-TR-95-64 and CS-TR-3479  
Abstract: We examine the inductive inference of a complex grammar specifically, we consider the task of training a model to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. We investigate the following models: feed-forward neural networks, Fransconi-Gori-Soda and Back-Tsoi locally recurrent networks, Elman, Narendra & Parthasarathy, and Williams & Zipser recurrent networks, Euclidean and edit-distance nearest-neighbors, simulated annealing, and decision trees. The feed-forward neural networks and non-neural network machine learning models are included primarily for comparison. We address the question: How can a neural network, with its distributed nature and gradient descent based iterative calculations, possess linguistic capability which is traditionally handled with symbolic computation and recursive processes? Initial simulations with all models were only partially successful by using a large temporal window as input. Models trained in this fashion did not learn the grammar to a significant degree. Attempts at training recurrent networks with small temporal input windows failed until we implemented several techniques aimed at improving the convergence of the gradient descent training algorithms. We discuss the theory and present an empirical study of a variety of models and learning algorithms which highlights behaviour not present when attempting to learn a simpler grammar. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert B. Allen. </author> <title> Sequential connectionist networks for answering simple questions about a microworld. </title> <booktitle> In 5th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 489495, </pages> <year> 1983. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [52, 1, 12, 24, 51] </ref>. Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems.
Reference: [2] <author> A.D. </author> <title> Back. New Techniques for Nonlinear System Identification: A Rapprochement Between Neural Networks and Linear Systems. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, University of Queensland, </institution> <year> 1992. </year>
Reference-contexts: We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 3. Back-Tsoi FIR [3]. An FIR filter and gain term is included in every synapse. Definition 1 <ref> [2] </ref> An FIR MLP with L layers excluding the input layer (0; 1; :::; L), FIR filters of order n b , and N 0 ; N 1 ; :::; N L neurons per layer, is defined as: y l i k (t) (3) k (t) = i=0 ki (t) j=0
Reference: [3] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modeling. </title> <booktitle> Neural Computation, </booktitle> <address> 3(3):375385, </address> <year> 1991. </year>
Reference-contexts: Fransconi-Gori-Soda define local-output and local-activation versions of the architecture where the feedback is taken from the respective points. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 3. Back-Tsoi FIR <ref> [3] </ref>. An FIR filter and gain term is included in every synapse.
Reference: [4] <author> E.B. Baum and F. Wilczek. </author> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D.Z. An-derson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 5261, </pages> <address> New York, </address> <year> 1988. </year> <journal> American Institute of Physics. </journal>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6. Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( <ref> [4, 28, 50, 26, 27] </ref>) and has a natural interpretation in terms of learning probabilities [35].
Reference: [5] <author> N.A. Chomsky. </author> <title> Three models for the description of language. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-2:113124, </volume> <year> 1956. </year>
Reference-contexts: PropN ! John j Mary V ! chase j feed j see... In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite-state-automata (FSA). However, it has been firmly established <ref> [5] </ref> that the syntactic structures of natural language cannot be parsimoniously described by regular languages.
Reference: [6] <author> N.A. Chomsky. </author> <title> Lectures on Government and Binding. </title> <publisher> Foris Publications, </publisher> <year> 1981. </year>
Reference-contexts: In the light of such examples and the fact that such contrasts crop up not just in English but in other languages (for example, the stubborn contrast also holds in Dutch), some linguists (chiefly Chomsky <ref> [6] </ref>) have hypothesized that it is only reasonable that such knowledge is only partially acquired: the lack of variation found across speakers, and indeed, languages for certain classes of data suggests that there exists a fixed component of the language system.
Reference: [7] <author> N.A. Chomsky. </author> <title> Knowledge of Language: Its Nature, Origin, and Use. </title> <type> Prager, </type> <year> 1986. </year>
Reference-contexts: and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in a formal system <ref> [7] </ref>? Here, we will provide a couple of examples of the kind of knowledge native speakers often take for granted.
Reference: [8] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1(3):372381, </address> <year> 1989. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51].
Reference: [9] <author> J. P. Crutchfield and K. Young. </author> <title> Computation at the onset of chaos. </title> <editor> In W. Zurek, editor, </editor> <title> Complexity, Entropy and the Physics of Information. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent [48]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Pollack [42] points out that Crutchfield and Young <ref> [9] </ref> have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars.
Reference: [10] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Moody and Darkin have proposed search then converge learning rate schedules of the form <ref> [10, 11] </ref>: j (t) = 1 + t (6) where j (t) is the learning rate at time t, j 0 is the initial learning rate, and o is a constant.
Reference: [11] <author> C. Darken and J.E. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Neural Information Processing Systems 4, </booktitle> <pages> pages 10091016. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Moody and Darkin have proposed search then converge learning rate schedules of the form <ref> [10, 11] </ref>: j (t) = 1 + t (6) where j (t) is the learning rate at time t, j 0 is the initial learning rate, and o is a constant.
Reference: [12] <author> J.L. Elman. </author> <title> Structured representations and connectionist models. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 1725, </pages> <year> 1984. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [52, 1, 12, 24, 51] </ref>. Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems.
Reference: [13] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <booktitle> Cognitive Science, </booktitle> <address> 14:179211, </address> <year> 1990. </year>
Reference-contexts: Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in [39]. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in <ref> [13, 14] </ref>. 8 where y l k is the output of neuron k in layer l, N l is the number of neurons in layer l, w l ki is the weight connecting neuron k in layer l to neuron i in layer l 1, y l 0 = 1 (bias),
Reference: [14] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <address> 7(2/3):195226, </address> <year> 1991. </year>
Reference-contexts: In this paper we consider replacing the inference algorithm with a neural network or a machine learning methodology. Our grammar is that of the English language. The simple grammar used by Elman <ref> [14] </ref> shown in figure 1 contains some of the structures in the complete English grammar: eg. agreement, verb argument structure, interactions with relative clauses, and recursion. S ! NP VP . NP ! PropN j N j N RC VP ! V (NP) N ! boy j girl j cat... <p> Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions <ref> [14] </ref>, and that they are Turing equivalent [48]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Pollack [42] points out that Crutchfield and Young [9] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. <p> Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in [39]. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in <ref> [13, 14] </ref>. 8 where y l k is the output of neuron k in layer l, N l is the number of neurons in layer l, w l ki is the weight connecting neuron k in layer l to neuron i in layer l 1, y l 0 = 1 (bias), <p> Dev. NMSE Std. Dev. Elman 0.387 0.023 0.405 0.14 W & Z 0.650 0.022 0.835 0.13 Table 5. Training set NMSE comparison for logistic and tanh sigmoid activation functions. initial training constrains later training in a useful way <ref> [14] </ref>. Results of four simulations per case comparing the use of sectioning with standard training on our problem are shown in table 6. The use of sectioning has consistently decreased performance.
Reference: [15] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):120130, </address> <year> 1992. </year>
Reference-contexts: Multi-layer perceptron. The output of a neuron is computed using 8 y l 0 N l1 X w l i A (2) 2. Frasconi-Gori-Soda locally recurrent networks. The Fransconi-Gori-Soda network has a locally recurrent globally feedforward architecture which includes a feedback connection around each hidden layer node <ref> [15] </ref>. Fransconi-Gori-Soda define local-output and local-activation versions of the architecture where the feedback is taken from the respective points. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 3. Back-Tsoi FIR [3].
Reference: [16] <author> K.S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1982. </year>
Reference-contexts: 1 Motivation 1.1 Formal Grammars and Grammatical Inference We give a brief introduction to formal grammars, grammatical inference, and natural language; for a thorough introduction we recommend Harrison [25] and Fu <ref> [16] </ref>. Briefly, a grammar G is a four tuple fN; T; P; Sg, where N and T are sets of terminals and nonterminals comprising the alphabet of the grammar, P is a set of production rules, and S is the start symbol. <p> with the procedures that can be used to infer the syntactic or production rules of an unknown grammar G based on a finite set of strings I from L (G), the language generated by G, and possibly also on a finite set of strings from the complement of L (G) <ref> [16] </ref>. In this paper we consider replacing the inference algorithm with a neural network or a machine learning methodology. Our grammar is that of the English language.
Reference: [17] <author> M. Gasser and C. Lee. </author> <title> Networks that learn phonology. </title> <type> Technical report, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1990. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology <ref> [17, 22, 54, 55] </ref>, morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [18] <author> C. Lee Giles, D. Chen, C.B. Miller, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Second-order recurrent neural networks for grammatical inference. </title> <booktitle> In 1991 IEEE INNS International Joint Conference on Neural Networks Seattle, </booktitle> <volume> volume II, </volume> <pages> pages 273281, </pages> <address> Piscataway, NJ, 1991. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51].
Reference: [19] <author> C. Lee Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):393405, </address> <year> 1992. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51]. <p> The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. <ref> [56, 19] </ref> on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent [48].
Reference: [20] <author> C. Lee Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317324, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 21 </pages>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51].
Reference: [21] <author> C. Lee Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen. </author> <title> Higher order recurrent networks & grammatical inference. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 380387, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51].
Reference: [22] <author> M. Hare. </author> <title> The role of similarity in Hungarian vowel harmony: A connectionist account. </title> <type> Technical Report CRL 9004, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology <ref> [17, 22, 54, 55] </ref>, morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [23] <author> M. Hare, D. Corina, and G.W. Cottrell. </author> <title> Connectionist perspective on prosodic structure. </title> <type> Technical Report CRL Newsletter Volume 3 Number 2, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology <ref> [23, 37, 47] </ref> and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [24] <author> Catherine L. Harris and J.L. Elman. </author> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 635642, </pages> <year> 1984. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [52, 1, 12, 24, 51] </ref>. Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems.
Reference: [25] <author> M.H. Harrison. </author> <title> Introduction to Formal Language Theory. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1978. </year>
Reference-contexts: 1 Motivation 1.1 Formal Grammars and Grammatical Inference We give a brief introduction to formal grammars, grammatical inference, and natural language; for a thorough introduction we recommend Harrison <ref> [25] </ref> and Fu [16]. Briefly, a grammar G is a four tuple fN; T; P; Sg, where N and T are sets of terminals and nonterminals comprising the alphabet of the grammar, P is a set of production rules, and S is the start symbol.
Reference: [26] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Training set NMSE comparison for batch versus stochastic update. 4. Weight initialisation. Random weights are initialised with the goal of ensuring that the sigmoids do not start out in saturation but are not very small (corresponding to a flat part of the error surface) <ref> [26] </ref>). In addition, several sets of random weights are tested and the set which provides the best performance on the training data is chosen 14 . <p> No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6. Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( <ref> [4, 28, 50, 26, 27] </ref>) and has a natural interpretation in terms of learning probabilities [35]. <p> All inputs were within the range zero to one. All target outputs were either 0.1 or 0.9. Bias inputs were used. The best of 20 random weight sets was chosen based on training set performance. Weights were initialised as shown in Haykin <ref> [26] </ref>. The logistic output activation function was used. The quadratic cost function was used.
Reference: [27] <author> J.A. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6. Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( <ref> [4, 28, 50, 26, 27] </ref>) and has a natural interpretation in terms of learning probabilities [35].
Reference: [28] <author> J. </author> <title> Hopfield. Learning algorithms and probability distributions in feed-forward and feed-back networks. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <address> 84:84298433, </address> <year> 1987. </year>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6. Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( <ref> [4, 28, 50, 26, 27] </ref>) and has a natural interpretation in terms of learning probabilities [35].
Reference: [29] <author> B. G. Horne and C. Lee Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Additionally, analysis of the data suggests that 100% correct classification on the training data with only two word inputs would not be possible without learning significant aspects of the grammar. Another comparison of recurrent neural network architectures, that of Giles and Horne <ref> [29] </ref>, compared various networks on randomly generated 6 and 64-state finite memory machines.
Reference: [30] <author> E.B. Hunt, J. Marin, and P.T. Stone. </author> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1966. </year>
Reference-contexts: CLS <ref> [30] </ref> used a heuristic lookahead method to construct decision trees. ID3 [44] extended CLS by using information content in the heuristic function.
Reference: [31] <author> L. Ingber. </author> <title> Very fast simulated re-annealing. </title> <booktitle> Mathematical Computer Modelling, </booktitle> <address> 12:967973, </address> <year> 1989. </year>
Reference-contexts: simulations with 10 hidden units where the trend across architectures was again similar but the performances were lower again (the Elman network attains roughly 88% correct classification on the training data and 65% on the English test data). 16 We have used the adaptive simulated annealing code by Lester Ingber <ref> [31, 32] </ref>. 17 The package used for simulated annealing by Lester Ingber includes speedups to the basic algorithm but even when we used these the algorithm did not converge. 12 TRAIN large small window window MLP 100 55 BT-FIR 100 56 Elman 100 100 W&Z 94 92 TEST large small window
Reference: [32] <author> L. Ingber. </author> <title> Adaptive simulated annealing (ASA). </title> <type> Technical report, </type> <institution> Lester Ingber Research, </institution> <address> McLean, VA, </address> <year> 1993. </year>
Reference-contexts: simulations with 10 hidden units where the trend across architectures was again similar but the performances were lower again (the Elman network attains roughly 88% correct classification on the training data and 65% on the English test data). 16 We have used the adaptive simulated annealing code by Lester Ingber <ref> [31, 32] </ref>. 17 The package used for simulated annealing by Lester Ingber includes speedups to the basic algorithm but even when we used these the algorithm did not converge. 12 TRAIN large small window window MLP 100 55 BT-FIR 100 56 Elman 100 100 W&Z 94 92 TEST large small window
Reference: [33] <author> A. K. Joshi. </author> <title> Tree adjoining grammars: how much context-sensitivity is required to provide reasonable structural descriptions? In L. </title> <editor> Karttunen D. R. Dowty and A. M. Zwicky, editors, </editor> <booktitle> Natural Language Parsing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars. Several modern computational linguistic grammatical theories fall in this class <ref> [33, 43] </ref>. 1.3 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in
Reference: [34] <author> Joseph B. Kruskal. </author> <title> An overview of sequence comparison. </title> <editor> In David Sankoff and Joseph B. Kruskal, editors, </editor> <title> Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: The following equations are used iteratively to calculate the distances ending in the distance between the two complete sequences. i and j range from 0 to the length of the respective sequences and the superscripts denote sequences of the corresponding length. For more details see <ref> [34] </ref>. d (a i ; b j ) = min &gt; &lt; d (a i1 ; b j ) + w (a i ; 0) deletion of a i d (a i1 ; b j1 ) + w (a i ; b j ) b j replaces a i d (a
Reference: [35] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( [4, 28, 50, 26, 27]) and has a natural interpretation in terms of learning probabilities <ref> [35] </ref>.
Reference: [36] <author> H. Lasnik and J. Uriagereka. </author> <title> A Course in GB Syntax: Lectures on Binding and Empty Categories. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: vs. innate components assumed by Chomsky, to produce the same judgements as native speakers on the sharply grammatical/ungrammatical pairs of the sort discussed in the next section. 2 Data Our primary data consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka <ref> [36] </ref>. Most of these examples are organized into minimal pairs like the example I am eager for John to win/*I am eager John to win that we have seen above.
Reference: [37] <author> B. MacWhinney, J. Leinbach, R. Taraban, and J. McDonald. </author> <title> Language learning: cues or rules? Journal of Memory and Language, </title> <address> 28:255277, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology <ref> [23, 37, 47] </ref> and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [38] <author> R. Miikkulainen and M. Dyer. </author> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In D. S. Touretzky, G. E. Hinton, and T. J. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 188195, </pages> <address> Los Altos, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment <ref> [38, 51] </ref>. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [39] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(1):427, </volume> <year> 1990. </year>
Reference-contexts: Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in <ref> [39] </ref>. 5. Elman.
Reference: [40] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside re-estimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th annual meeting of the ACL, </booktitle> <pages> pages 128135, </pages> <address> Newark, </address> <year> 1992. </year>
Reference-contexts: The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 <ref> [40] </ref>. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. <p> Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently impractical for anything except relatively small grammars <ref> [40] </ref>. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53].
Reference: [41] <author> D. M. Pesetsky. </author> <title> Paths and Categories. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1982. </year>
Reference: [42] <author> J.B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <booktitle> Machine Learning, </booktitle> <address> 7:227252, </address> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Certain phenomena (eg. center embedding) are more compactly described by context-free grammars which are recognised by push-down automata, while others (eg. crossed-serial dependencies and agreement) are better described by context-sensitive grammars which are recognised by linear bounded automata <ref> [42] </ref>. 1.2 Representational Power Natural language has traditionally been handled using symbolic computation and recursive processes. The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. <p> Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent [48]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Pollack <ref> [42] </ref> points out that Crutchfield and Young [9] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars.
Reference: [43] <author> C. Pollard. </author> <title> Generalised context-free grammars, head grammars and natural language. </title> <type> PhD thesis, </type> <institution> Department of Linguistics, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1984. </year>
Reference-contexts: They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars. Several modern computational linguistic grammatical theories fall in this class <ref> [33, 43] </ref>. 1.3 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in
Reference: [44] <author> J.R. Quinlan. </author> <title> Discovering rules from large collections of examples: a case study. </title> <editor> In D. Michie, editor, </editor> <booktitle> Expert Systems in the Microelectronic Age. </booktitle> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1979. </year>
Reference-contexts: CLS [30] used a heuristic lookahead method to construct decision trees. ID3 <ref> [44] </ref> extended CLS by using information content in the heuristic function.
Reference: [45] <author> Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: CLS [30] used a heuristic lookahead method to construct decision trees. ID3 [44] extended CLS by using information content in the heuristic function. We tested the C4.5 algorithm by Ross Quinlan <ref> [45] </ref>, which is an industrial strength version of ID3 designed to handle noise. 5 For an output range of 0 to 1. 6 Sequences of length zero up to the actual sequence length are considered.
Reference: [46] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Theoretically, the Williams & Zipser network is the most powerful in terms of representational ability, yet the Elman network provides better performance. Investigation shows that this is due to the more complex error surface of the Williams & Zipser architecture. This result is supported by the parsimony principle <ref> [46] </ref>. Results indicate that a global minimum is never found for the task and algorithms described here, however, we note that the local minima which are found consistently possess performance which is similar within each architecture.
Reference: [47] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> volume 1, 2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology <ref> [23, 37, 47] </ref> and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [48] <author> H.T. Siegelmann, E.D. Sontag, and C. Lee Giles. </author> <title> The complexity of language recognition by neural networks. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Algorithms, Software, Architecture Information Processing 92, </booktitle> <volume> Vol 1, </volume> <pages> pages 329335. </pages> <publisher> Elsevier Science, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent <ref> [48] </ref>. The recurrent neural networks investigated in this paper constitute complex, dynamical systems. Pollack [42] points out that Crutchfield and Young [9] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling.
Reference: [49] <author> P. Simard, M.B. Ottaway, and D.H. Ballard. </author> <title> Analysis of recurrent backpropagation. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 103112, </pages> <address> San Mateo, 1989. (Pittsburg 1988), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Training set NMSE comparison for logistic and tanh sigmoid activation functions. 11 8 Simulated Annealing Previous work has shown the use of simulated annealing for finding the parameters of a recurrent network model to improve performance <ref> [49] </ref>. For comparison with the gradient descent based algorithms we have investigated using simulated annealing to train exactly the same Elman network as has been successfully trained to 100% correct training set classification using backpropagation through time (details in a later section). <p> In comparison, the successful Elman models obtain an NMSE of approximately 0.1. We have not found the use of simulated annealing to improve performance, as Simard et. al. <ref> [49] </ref> have. Their problem was the parity problem with only four hidden units. 9 Results Our results are based on multiple training/test set partitions and multiple random seeds. We have also used a set of Japanese control data.
Reference: [50] <author> S.A. Solla, E. Levin, and M. Fleisher. </author> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2:625639, </volume> <year> 1988. </year>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6. Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ( <ref> [4, 28, 50, 26, 27] </ref>) and has a natural interpretation in terms of learning probabilities [35].
Reference: [51] <author> M. F. St. John and J.L. McClelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46:546, </volume> <year> 1990. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [52, 1, 12, 24, 51] </ref>. Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. <p> The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment <ref> [38, 51] </ref>. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [52] <author> Andreas Stolcke. </author> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Technical Report TR-90-015, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [8, 21, 18, 19, 20, 59]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [52, 1, 12, 24, 51] </ref>. Neural network models have been shown to be able to account for 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems.
Reference: [53] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105108, </pages> <address> Ann Arbor, MI, </address> <year> 1982. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages <ref> [53] </ref>. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent [48]. The recurrent neural networks investigated in this paper constitute complex, dynamical systems.
Reference: [54] <author> D. S. Touretzky. </author> <title> Rules and maps in connectionist symbol processing. </title> <type> Technical Report CMU-CS-89-158, </type> <institution> Carnegie Mellon University: Department of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology <ref> [17, 22, 54, 55] </ref>, morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [55] <author> D. S. Touretzky. </author> <title> Towards a connectionist phonology: The many maps approach to sequence manipulation. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 188195, </pages> <year> 1989. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology <ref> [17, 22, 54, 55] </ref>, morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. [56, 19] on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex.
Reference: [56] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309316, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 a variety of phenomena in phonology [17, 22, 54, 55], morphology [23, 37, 47] and role assignment [38, 51]. Induction of simpler grammars has been addressed often - eg. <ref> [56, 19] </ref> on learning Tomita languages [53]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [14], and that they are Turing equivalent [48].
Reference: [57] <author> R.J. Williams and J. Peng. </author> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajectories. </title> <booktitle> Neural Computation, </booktitle> <address> 2(4):490501, </address> <year> 1990. </year>
Reference-contexts: We expect the feedforward and locally recurrent architectures to encounter difficulty performing the task and include them primarily as control cases. 7 Gradient Descent Learning We have used backpropagation through time 9 <ref> [57] </ref> to train the globally recurrent networks 10 , standard backpropagation for the multi-layer perceptron, and the gradient descent algorithms described by the authors for the locally recurrent networks. The error surface of a multilayer network is non-convex, non-quadratic, and often has large dimensionality.
Reference: [58] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1(2):270280, </address> <year> 1989. </year>
Reference-contexts: Williams and Zipser. A fully recurrent network where all non-input nodes are connected to all other nodes as described in <ref> [58] </ref>. <p> using learning rates large enough to help escape local minima, particularly in the case of the Williams & Zipser network. 9 Backpropagation through time extends backpropagation to include temporal aspects and arbitrary connection topologies by considering an equivalent feedforward network created by unfolding the recurrent network in time. 10 Real-time <ref> [58] </ref> recurrent learning was also tested but did not show any significant convergence for our problem. 11 Without modifying the standard gradient descent algorithms we were only able to train networks which operated on a large temporal input window.
Reference: [59] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 5(6):976990, </address> <year> 1993. </year> <month> 23 </month>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [8, 21, 18, 19, 20, 59] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [52, 1, 12, 24, 51].
References-found: 59

