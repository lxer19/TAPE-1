URL: http://www.cs.iastate.edu/~parekh/papers/distal.ps
Refering-URL: http://www.cs.iastate.edu/~parekh/publications.html
Root-URL: http://www.cs.iastate.edu
Title: DistAl: An Inter-pattern Distance-based Constructive Learning Algorithm  
Author: Jihoon Yang, Rajesh Parekh and Vasant Honavar 
Keyword: Constructive neural network learning algorithm, inter-pattern distance  
Address: 226 Atanasoff Hall, Iowa State University, Ames, IA 50011. U.S.A.  
Affiliation: AI Research Group, Department of Computer Science  
Abstract: Multi-layer networks of threshold logic units offer an attractive framework for the design of pattern classification systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl constructs a single hidden layer of spherical threshold neurons. Each neuron is designed to exclude a cluster of training patterns belonging to the same class. The weights and thresholds of the hidden neurons are determined directly by comparing the inter-pattern distances of the training patterns. This offers a significant advantage over other constructive learning algorithms that use an iterative (and often time consuming) weight modification strategy to train individual neurons. The individual clusters (represented by the hidden neurons) are combined by a single output layer of threshold neurons. The speed of DistAl makes it a good candidate for datamining and knowledge acquisition from very large datasets. Results of experiments on several artificial and real-world datasets show that DistAl compares favorably with other neural network learning algorithms for pattern classification. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Gallant, </author> <title> Neural Network Learning and Expert Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [2] <author> C-H. Chen, R. Parekh, J. Yang, K. Balakrishnan, and V. Honavar, </author> <title> "Analysis of decision boundaries generated by constructive neural network learning algorithms," </title> <booktitle> in Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> 1995, </booktitle> <volume> vol. 1, </volume> <pages> pp. 628-635. </pages>
Reference: [3] <author> R. Parekh, J. Yang, and V. Honavar, </author> <title> "Constructive neural network learning algorithms for multi-category real-valued pattern classification," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR97-06, Department of Computer Science, Iowa State University, </institution> <year> 1997. </year>
Reference: [4] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing: Explorations into the Microstructure of Cognition, </title> <booktitle> vol. 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference: [5] <author> F. Rosenblatt, </author> <title> "The perceptron: A probabilistic model for information storage and organization in the brain," </title> <journal> Psychological Review, </journal> <volume> vol. 65, </volume> <pages> pp. 386-408, </pages> <year> 1958. </year>
Reference: [6] <author> N. Nilsson, </author> <title> The Mathematical Foundations of Learning Machines, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1965. </year>
Reference: [7] <author> W. Krauth and M. Mezard, </author> <title> "Learning algorithms with optimal stability in neural networks," </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> vol. 20, </volume> <pages> pp. </pages> <address> L745-L752, </address> <year> 1987. </year>
Reference: [8] <author> J. Anlauf and M. Biehl, </author> <title> "Properties of an adaptive perceptron algorithm," </title> <booktitle> in Parallel Processing in Neural Systems and Computers, </booktitle> <pages> pp. 153-156. </pages> <year> 1990. </year>
Reference: [9] <author> M. Frean, </author> <title> "A thermal perceptron learning rule," </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 946-957, </pages> <year> 1992. </year>
Reference: [10] <author> H. Poulard, </author> <title> "Barycentric correction procedure: A fast method of learning threshold units," </title> <booktitle> in Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> 1995, </booktitle> <volume> vol. 1, </volume> <pages> pp. 710-713. </pages>
Reference: [11] <author> B. Raffin and M. Gordon, </author> <title> "Learning and generalization with minimerror, a temperature-dependent learning algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 1206-1224, </pages> <year> 1995. </year>
Reference: [12] <author> R. Reed, </author> <title> "Pruning algorithms | a survey," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 5, </volume> <pages> pp. 740-747, </pages> <year> 1993. </year>
Reference: [13] <author> R. Parekh, J. Yang, and V. Honavar, </author> <title> "Pruning strategies for constructive neural network learning algorithms," </title> <booktitle> in Proceedings of the IEEE/INNS International Conference on Neural Networks, </booktitle> <address> ICNN'97, </address> <year> 1997, </year> <pages> pp. 1960-1965. </pages>
Reference: [14] <author> V. Honavar, </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks, </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference: [15] <author> V. Honavar and L Uhr, </author> <title> "Generative learning structures for generalized connectionist networks," </title> <journal> Information Sciences, </journal> <volume> vol. 70, no. </volume> <pages> 1-2, pp. 75-108, </pages> <year> 1993. </year>
Reference: [16] <author> R. Parekh and V. Honavar, </author> <title> "Constructive theory refinement in knowledge based neural networks," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Anchorage, Alaska, </address> <year> 1998, </year> <note> To appear. </note>
Reference-contexts: It would be interesting to explore variants of DistAl that can avoid the need for maintaining the entire inter-pattern distance matrix during learning. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [46] and for knowledge-based theory refinement <ref> [16] </ref>, [47]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [16]. <p> Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [46] and for knowledge-based theory refinement <ref> [16] </ref>, [47]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [16].
Reference: [17] <author> J. Nadal, </author> <title> "Study of a growth algorithm for a feedforward network," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 55-59, </pages> <year> 1989. </year>
Reference-contexts: Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower <ref> [17] </ref> 3.5 4 4.5 E. Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns. Proof: See [44] for the detailed proof. III.
Reference: [18] <author> S. Gallant, </author> <title> "Perceptron based learning algorithms," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference: [19] <author> M. Mezard and J. Nadal, </author> <title> "Learning feed-forward networks: The tiling algorithm," </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> vol. 22, </volume> <pages> pp. 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling <ref> [19] </ref> 7 8 9 Tower [17] 3.5 4 4.5 E. Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns. Proof: See [44] for the detailed proof. III.
Reference: [20] <author> M. Frean, </author> <title> "The upstart algorithm: A method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 198-209, </pages> <year> 1990. </year>
Reference-contexts: A `-' indicates that the result is not reported in the corresponding reference. Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart <ref> [20] </ref> 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E.
Reference: [21] <author> N. Burgess, </author> <title> "A constructive algorithm that converges for real-valued input patterns," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 59-66, </pages> <year> 1994. </year>
Reference-contexts: TABLE I Comparison of the network size generated by different algorithms for the parity datasets. A `-' indicates that the result is not reported in the corresponding reference. Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade <ref> [21] </ref> 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E.
Reference: [22] <author> M. Marchand, M. Golea, and P. Rujan, </author> <title> "A convergence theorem for sequential learning in two-layer perceptrons," </title> <journal> Europhysics Letters, </journal> <volume> vol. 11, no. 6, </volume> <pages> pp. 487-492, </pages> <year> 1990. </year>
Reference-contexts: Then, they are fully connected to M output TLUs (1 for each output class) in an output layer. The representation of the patterns at the hidden layer is linearly separable <ref> [22] </ref>. Thus, an iterative perceptron learning rule can be used to train the output weights. However, the output weights can be directly set as follows: The weights between output and hidden neurons are chosen such that each hidden neuron overwhelms the effect of the hidden neurons generated later. <p> Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential <ref> [22] </ref> 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E. Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns. Proof: See [44] for the detailed proof.
Reference: [23] <author> J. Yang and V. Honavar, </author> <title> "Feature subset selection using a genetic algorithm," in Feature Extraction, Construction and Selection A Data Mining Perspective. </title> <publisher> Kluwer Academic, </publisher> <address> NY, </address> <year> 1998, </year> <note> To appear. </note>
Reference-contexts: We have applied DistAl to the task of feature subset selection (i.e., obtaining the optimal feature subset from the entire set of input attributes) using genetic algorithms <ref> [23] </ref>. DistAl worked fairly well (both in terms of speed and generalization) on the feature subset selection task. A potential disadvantage of DistAl is its need for maintaining the inter-pattern distance matrix during learning. The memory needed to store this matrix grows quadrati-cally with the size of the training set.
Reference: [24] <author> D. Aha, D. Kibler, and M. Albert, </author> <title> "Instance-based learning algorithms," </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 37-66, </pages> <year> 1991. </year>
Reference: [25] <author> P. Turney, </author> <title> "Theoretical analyses of cross-validation error and voting in instance-based learning," </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <pages> pp. 331-360, </pages> <year> 1994. </year>
Reference: [26] <author> P. Domingos, </author> <title> "Rule induction and instance-based learning: A unified approach," </title> <booktitle> in Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <year> 1995. </year>
Reference: [27] <author> T. Cover and P. Hart, </author> <title> "Nearest neighbor pattern classification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 13, </volume> <pages> pp. 21-27, </pages> <year> 1967. </year>
Reference: [28] <author> E. Diday, </author> <title> "Recent progress in distance and similarity measures in pattern recognition," </title> <booktitle> in Proceedings of the Second International Joint Conference on Pattern Recognition, </booktitle> <year> 1974, </year> <pages> pp. 534-539. </pages>
Reference: [29] <author> B. Dasarathy, </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techiniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference: [30] <author> C. Stanfill and D. Waltz, </author> <title> "Toward memory-based reasoning," </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, no. 12, </volume> <pages> pp. 1213-1228, </pages> <year> 1986. </year>
Reference: [31] <author> J. Kolodner, </author> <title> Case-Based Reasoning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1993. </year>
Reference: [32] <author> G. Carpenter and S. Grossberg, </author> <title> Pattern Recognition by Self-Organizing Neural Networks, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference: [33] <author> A. Tversky, </author> <title> "Features of similiarity," </title> <journal> Psychological Review, </journal> <volume> vol. 84, </volume> <pages> pp. 327-352, </pages> <year> 1977. </year>
Reference: [34] <author> G. Salton and M. McGill, </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [35] <author> D. Broomhead and D. Lowe, </author> <title> "Multivariable functional interpolation and adaptive networks," </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 321-355, </pages> <year> 1988. </year>
Reference: [36] <author> F. Girosi, M. Jones, and T. Poggio, </author> <title> "Regularization theory and neural networks architectures," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 219-269, </pages> <year> 1995. </year>
Reference: [37] <author> P. Langley, </author> <title> Elements of Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference: [38] <author> T. Mitchell, </author> <title> Machine Learning, </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1997. </year>
Reference: [39] <author> R. Duda and P. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [40] <author> D. Wilson and T. Martinez, </author> <title> "Improved heterogeneous distance functions," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 6, </volume> <pages> pp. 1-34, </pages> <year> 1997. </year>
Reference-contexts: A thorough comparison of our algorithm with all the existing pattern classification algorithms is beyond the scope of our project. In what follows we compare the performance of DistAl with the results for the k nearest-neighbor algorithm reported in <ref> [40] </ref>. As we can see from Table II, DistAl gave comparable results to other algorithms in most datasets (except Vowel) despite its fast execution. In case of Vowel dataset, the nearest neighbor algorithm [40] reports a even higher accuracy than DistAl. 1 1 The best results reported in the literature [45] <p> follows we compare the performance of DistAl with the results for the k nearest-neighbor algorithm reported in <ref> [40] </ref>. As we can see from Table II, DistAl gave comparable results to other algorithms in most datasets (except Vowel) despite its fast execution. In case of Vowel dataset, the nearest neighbor algorithm [40] reports a even higher accuracy than DistAl. 1 1 The best results reported in the literature [45] is 56% for Vowel dataset. TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40]. <p> the nearest neighbor algorithm <ref> [40] </ref> reports a even higher accuracy than DistAl. 1 1 The best results reported in the literature [45] is 56% for Vowel dataset. TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40].
Reference: [41] <author> H. Andersen and A. Tsoi, </author> <title> "A constructive algorithm for the training of a multilayer perceptron based on the genetic algorithm," </title> <journal> Complex Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 249-268, </pages> <year> 1993. </year>
Reference-contexts: Fig. 2. Execution of the DistAl algorithm for the XOR prob lem. TABLE I Comparison of the network size generated by different algorithms for the parity datasets. A `-' indicates that the result is not reported in the corresponding reference. Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP <ref> [41] </ref> 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E.
Reference: [42] <author> S. Fahlman and C. Lebiere, </author> <title> "The cascade correlation learning algorithm," in Neural Information Systems 2, </title> <editor> D. Touretzky, </editor> <publisher> Ed., </publisher> <pages> pp. 524-532. </pages> <address> Morgan-Kauffman, </address> <year> 1990. </year>
Reference-contexts: TABLE I Comparison of the network size generated by different algorithms for the parity datasets. A `-' indicates that the result is not reported in the corresponding reference. Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation <ref> [42] </ref> 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm [43] 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E.
Reference: [43] <author> M. Golea and M. Marchand, </author> <title> "A growth algorithm for neural network decision trees," </title> <journal> Europhysics Letters, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 205-210, </pages> <year> 1990. </year>
Reference-contexts: A `-' indicates that the result is not reported in the corresponding reference. Algorithm P7 P8 P9 DistAl 5 5 6 GA-MLP [41] 9 15 - Perceptron cascade [21] 3 4 4 Cascade correlation [42] 4-5 5-6 - Upstart [20] 6 7 8 Growth algorithm <ref> [43] </ref> 7 8 9 Sequential [22] 7 8 9 Tiling [19] 7 8 9 Tower [17] 3.5 4 4.5 E. Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns.
Reference: [44] <author> J. Yang, R. Parekh, and V. Honavar, </author> <title> "DistAl: An inter-pattern distance-based constructive learning algorithm," </title> <type> Tech. Rep. </type> <institution> ISU-CS-TR 97-05, Iowa State University, </institution> <year> 1997. </year>
Reference-contexts: Convergence Proof Theorem: DistAl guarantees to converge to 100% training accuracy with a finite number of hidden neurons for a dataset with a finite number of non-contradictory patterns. Proof: See <ref> [44] </ref> for the detailed proof. III. Experimental Evaluation of DistAl Experiments were performed on artificial datasets (viz. parity) and several real-world datasets available from the machine learning data repository at the University of Cal-ifornia at Irvine [45]. The experiment was run once for each distance metric. <p> The network was allowed to train until it achieved 100% classification accuracy on the training set. The best generalization (during the process of training) was then reported. For further details on the experimental set up and the datasets see <ref> [44] </ref>. A thorough comparison of our algorithm with all the existing pattern classification algorithms is beyond the scope of our project. In what follows we compare the performance of DistAl with the results for the k nearest-neighbor algorithm reported in [40].
Reference: [45] <author> P. Murphy and D. Aha, </author> <title> "Repository of machine learning databases," </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: Proof: See [44] for the detailed proof. III. Experimental Evaluation of DistAl Experiments were performed on artificial datasets (viz. parity) and several real-world datasets available from the machine learning data repository at the University of Cal-ifornia at Irvine <ref> [45] </ref>. The experiment was run once for each distance metric. In the case of the parity datasets the entire pattern set was used for training. Table I presents the size of the network generated by several algorithms for parity problems. <p> As we can see from Table II, DistAl gave comparable results to other algorithms in most datasets (except Vowel) despite its fast execution. In case of Vowel dataset, the nearest neighbor algorithm [40] reports a even higher accuracy than DistAl. 1 1 The best results reported in the literature <ref> [45] </ref> is 56% for Vowel dataset. TABLE II Comparison of generalization accuracy between various algorithms. DistAl is the results of our approach and NN is the best results in [40].
Reference: [46] <author> S. Thrun, </author> <title> "Lifelong learning: A case study," </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-208, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: It would be interesting to explore variants of DistAl that can avoid the need for maintaining the entire inter-pattern distance matrix during learning. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning <ref> [46] </ref> and for knowledge-based theory refinement [16], [47]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [16].
Reference: [47] <author> J. W. Shavlik, </author> <title> "A framework for combining symbolic and neural learning," in Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: It would be interesting to explore variants of DistAl that can avoid the need for maintaining the entire inter-pattern distance matrix during learning. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [46] and for knowledge-based theory refinement [16], <ref> [47] </ref>. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [16].
References-found: 47

