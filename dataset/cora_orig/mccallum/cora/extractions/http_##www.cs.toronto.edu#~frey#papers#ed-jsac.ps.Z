URL: http://www.cs.toronto.edu/~frey/papers/ed-jsac.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/papers/ed-jsac.abs.html
Root-URL: http://www.cs.toronto.edu
Title: Early-Detection and Trellis Splicing: Reduced-Complexity Iterative Decoding  
Author: Brendan J. Frey and Frank R. Kschischang, Member, IEEE 
Date: 2, FEBRUARY 1998 153  
Note: IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 16, NO.  
Abstract: The excellent bit error rate performance of new iterative decoding algorithms (e.g., turbodecoding) is achieved at the expense of a com-putationally burdensome decoding procedure. In this paper, we present a method called early-detection that can be used to reduce the computational complexity of a variety of iterative decoders. Using a confidence criterion, some information symbols, state variables and codeword symbols are detected early on during decoding. In this way, the computational complexity of further processing is reduced with a controllable increase in BER. We present an easily implemented instance of this algorithm, called trellis splicing, that can be used with turbodecoding. For a simulated system of this type, we obtain a reduction in computational complexity of up to a factor of four, relative to a turbodecoder that obtains the same increase in BER by performing fewer iterations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Berrou, A. Glavieux, and P. Thitimajshima, </author> <title> Near Shannon limit error-correcting coding and decoding: Turbo codes, </title> <booktitle> in Proceedings of the IEEE International Conference on Communications, </booktitle> <year> 1993. </year>
Reference: [2] <author> G. Battail, C. Berrou, and A. Glavieux, </author> <title> Pseudo-random recursive convolutional coding for near-capacity performance, </title> <booktitle> in Proceedings of GLOBECOM'93, </booktitle> <year> 1993. </year>
Reference: [3] <author> J. Lodge, R. Young, P. Hoeher, and J. Hagenauer, </author> <title> Separable MAP `filters' for the decoding of product and concatenated codes, </title> <booktitle> in Proceedings of IEEE International Conference on Communications, </booktitle> <year> 1993, </year> <pages> pp. 1740 1745. </pages>
Reference: [4] <author> J. D. Anderson, </author> <title> The TURBO-coding scheme, </title> <booktitle> in Proceedings of IEEE International Symposium on Information Theory, </booktitle> <year> 1994. </year>
Reference: [5] <author> J. E. M. Nillson and R. Kotter, </author> <title> Iterative decoding of product code constructions, </title> <booktitle> in Proceedings of the International Symposium on Information Theory and its Applications, </booktitle> <year> 1994. </year>
Reference: [6] <author> P. Jung and M. Nasshan, </author> <title> Dependence of the error performance of turbo-codes on the interleaver structure in short frame transmission systems, </title> <journal> Electronics Letters, </journal> <volume> vol. 30, </volume> <pages> pp. 287288, </pages> <year> 1994. </year>
Reference: [7] <author> D. Divsalar and F. Pollara, </author> <title> Turbo-codes for PCS applications, </title> <booktitle> in Proceedings of the International Conference on Communications, </booktitle> <year> 1995, </year> <pages> pp. 5459. </pages>
Reference: [8] <author> N. Wiberg, H.-A. Loeliger, and R. Kotter, </author> <title> Codes and iterative decoding on general graphs, </title> <journal> European Transactions on Telecommunications, </journal> <volume> vol. 6, </volume> <pages> pp. 513525, </pages> <month> September/October </month> <year> 1995. </year>
Reference: [9] <author> J. Hagenauer, E. Offer, and L. Papke, </author> <title> Iterative decoding of binary block and convolutional codes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, no. 2, </volume> <pages> pp. 429445, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Higher order criteria, such as the change in ^ L U k (u k ), would produce too many erroneous early-detections. Although the relative entropy from one iteration to the next was successfully used in <ref> [9] </ref> as a block-oriented termination criterion, the same rule would not work at the more refined symbol-oriented level of early-detection. <p> A plot of the log-probability ratio versus iteration number, for the correct value of each of 25 information bits from 25 randomly selected blocks in which the log-probability ratio dropped below -10.0 during decoding. der using Bayes' rule <ref> [9] </ref>. In fact, this procedure has been well-established for over a decade as the probability propagation algorithm for processing Bayesian networks used in the area of artificial intelligence [12,14,20]. <p> This algorithm computes the a posteriori information bit probabilities using the channel output and a priori information bit probabilities. The forward-backward algorithm can be viewed simply as a combination of probabilistic flows [29] computed in the forward direction and in the backward direction. Alternatively, a soft-output Viterbi algorithm (SOVA) <ref> [9] </ref> can be used. Here, we consider early-detection for information symbols only. As discussed earlier, early-detection of a single information symbol reduces the complexity of both constituent codes. Consider the simple two-state trellis shown in Fig. 5a. <p> Although there are various useful techniques and approximations for decreasing this cost <ref> [9, 13] </ref>, such as the SOVA [9], we will define it as our basic computational unit, and refer to it as a trellis section operation. <p> Although there are various useful techniques and approximations for decreasing this cost [9, 13], such as the SOVA <ref> [9] </ref>, we will define it as our basic computational unit, and refer to it as a trellis section operation.
Reference: [10] <author> S. Benedetto and G. Montorsi, Unveiling turbo-codes: </author> <title> Some results on parallel concatenated coding schemes, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 409428, </pages> <month> March </month> <year> 1996. </year>
Reference: [11] <author> D. J. C. MacKay and R. M. Neal, </author> <title> Near Shannon limit performance of low density parity check codes, </title> <journal> Electronics Letters, </journal> <volume> vol. 32, no. 18, </volume> <pages> pp. 16451646, </pages> <month> August </month> <year> 1996, </year> <title> Due to editing errors, </title> <journal> reprinted in Electronics Letters, </journal> <volume> vol. 33, </volume> <month> March </month> <year> 1997, </year> <month> 457458. </month>
Reference-contexts: Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes [14, 15], product codes [14, 15], and Gallager's low-density parity-check codes <ref> [11, 14] </ref>. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> Also, higher-order criteria increase the computational overhead of early-detection. III. REDUCTION IN COMPLEXITY DUE TO EARLY-DETECTION As pointed out by Wiberg [22] and MacKay and Neal <ref> [11] </ref>, the key ideas of iterative decoding using soft decisions were present in Gallager's work in the early 1960's on low-density parity-check codes [23].
Reference: [12] <author> B. J. Frey and F. R. Kschischang, </author> <title> Probability propagation and iterative decoding, </title> <booktitle> in Proceedings of the 34 th Allerton Conference, </booktitle> <year> 1996, </year> <note> Available at http://www.cs.utoronto.ca/frey. </note>
Reference-contexts: In Sec. III, we describe the computational consequences for subsequent decoding in a variety of iterative decoders, when an information symbol, codeword symbol or state variable is detected early. By viewing iterative decoding as message passing in the Bayesian network that describes a code <ref> [12, 1416, 20] </ref>, we are able to obtain an approximate general formula for the computational complexity of a single iteration of decoding. In Secs. IV and V we describe the trellis splicing algorithm and present results for thresholded early-detection of information symbols in a turbocode system.
Reference: [13] <author> S. Benedetto, D. Divsalar, G. Montorsi, and F. Pollara, </author> <title> Soft-ouput decoding algorithms in iterative decoding of parallel concatenated convolutional codes, </title> <note> Submitted to IEEE International Conference on Communications, </note> <year> 1996. </year>
Reference-contexts: Although there are various useful techniques and approximations for decreasing this cost <ref> [9, 13] </ref>, such as the SOVA [9], we will define it as our basic computational unit, and refer to it as a trellis section operation.
Reference: [14] <author> B. J. Frey, </author> <title> Graphical Models for Machine Learning and Digital Communication, </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1998. </year> <note> See http://www.cs.utoronto.ca/frey. </note>
Reference-contexts: Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes <ref> [14, 15] </ref>, product codes [14, 15], and Gallager's low-density parity-check codes [11, 14]. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes <ref> [14, 15] </ref>, product codes [14, 15], and Gallager's low-density parity-check codes [11, 14]. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes [14, 15], product codes [14, 15], and Gallager's low-density parity-check codes <ref> [11, 14] </ref>. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> For an overview of how graphical models such as Bayesian networks can be used to describe codes and to derive iterative decoders, see the article by Kschis-chang and Frey in this issue [15]. See <ref> [14] </ref> for a comprehensive book on this subject. For an in-depth derivation showing that turbodecoding is in fact probability propagation in a Bayesian network, see the article by McEliece, et al. in this issue [16]. <p> Also, there may be a useful way to adapt the threshold during decoding. In general, it seems possible to make a wide variety of adjustments to the general graph-based iterative decoding algorithms. For example, see <ref> [14] </ref> for a concurrent turbodecoding method that speeds up decoding by a factor of 80. We feel that investigating the complexity/performance tradeoffs for these algorithms will continue to be a rich and potentially rewarding research area.
Reference: [15] <author> F. R. Kschischang and B. J. Frey, </author> <title> Iterative decoding of compound codes by probability propagation in graphical models, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 16, no. 2, </volume> <month> February </month> <year> 1998, </year> <note> available at http://www.cs.utoronto.ca/frey. </note>
Reference-contexts: Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes <ref> [14, 15] </ref>, product codes [14, 15], and Gallager's low-density parity-check codes [11, 14]. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> Our method can be applied in conjunction with a variety of iterative decoding algorithms, including the decoders for turbocodes [1416], serially-concatenated convolutional codes <ref> [14, 15] </ref>, product codes [14, 15], and Gallager's low-density parity-check codes [11, 14]. In Sec. II we trace the soft decision reliabilities of many information bits during turbodecoding. <p> For an overview of how graphical models such as Bayesian networks can be used to describe codes and to derive iterative decoders, see the article by Kschis-chang and Frey in this issue <ref> [15] </ref>. See [14] for a comprehensive book on this subject. For an in-depth derivation showing that turbodecoding is in fact probability propagation in a Bayesian network, see the article by McEliece, et al. in this issue [16]. <p> For a given code, this framework indicates which variables ought to be given priority for early-detection, in order to save the most computations. Using the symbolic and graphical notation introduced in the companion article in this issue <ref> [15] </ref>, the Bayesian networks for a variety of codes are shown in the first column of pictures in Fig. 4. (The channel output variables are not shown their likelihoods are to be included as bias effects on the state variables, codeword bits, and information bits (where applicable) during decoding.) Each vertex <p> Z i jA i (z i ja i ) 6= 0, and let jA i j be the number of parents for Z i . (If Z i has no parents, let jA i j = 1.) Then, the computational complexity of each iteration of iterative decoding usually scales as <ref> [15] </ref> O = i=1 For example, if the constituent convolutional code for the tur-bocode described above has memory , then jP S 1;i jS 1;i1 ;U i j = 2 +1 and so the state variable s 1;i contributes a complexity of jP S 1;i jS 1;i1 ;U i j 2
Reference: [16] <author> R. J. McEliece, D. J. C. MacKay, and J. F. Cheng, </author> <title> Turbo-decoding as an instance of Pearl's `belief propagation' algorithm, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 16, no. 2, </volume> <month> February </month> <year> 1998. </year>
Reference-contexts: See [14] for a comprehensive book on this subject. For an in-depth derivation showing that turbodecoding is in fact probability propagation in a Bayesian network, see the article by McEliece, et al. in this issue <ref> [16] </ref>. In this section, we show how early-detection reduces decoding complexity by using an approximate general formula for the computational complexity of a single iteration of decoding. This formula is approximately valid for the iterative decoders for tur-bocodes, serially-concatenated convolutional codes and product codes.
Reference: [17] <author> L. Lee, </author> <title> Concatenated coding systems employing a unit-memory convolutional code and a byte-oriented decoding algorithm, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 25, no. 10, </volume> <pages> pp. 10641074, </pages> <month> October </month> <year> 1977. </year> <journal> 160 IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, </journal> <volume> VOL. 16, NO. 2, </volume> <month> FEBRUARY </month> <year> 1998 </year>
Reference: [18] <author> O. M. Collins, </author> <title> Determinate state convolutional codes, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 41, no. 12, </volume> <pages> pp. 17851794, </pages> <month> December </month> <year> 1993. </year>
Reference: [19] <author> J. Hagenauer, E. Offer, and L. Papke, </author> <title> Improving the standard coding system for deep space missions, </title> <booktitle> in Proceedings of IEEE International Conference on Communications, </booktitle> <year> 1993, </year> <pages> pp. 10921097. </pages>
Reference: [20] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems, </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo CA., </address> <year> 1988. </year>
Reference-contexts: In Sec. III, we describe the computational consequences for subsequent decoding in a variety of iterative decoders, when an information symbol, codeword symbol or state variable is detected early. By viewing iterative decoding as message passing in the Bayesian network that describes a code <ref> [12, 1416, 20] </ref>, we are able to obtain an approximate general formula for the computational complexity of a single iteration of decoding. In Secs. IV and V we describe the trellis splicing algorithm and present results for thresholded early-detection of information symbols in a turbocode system.
Reference: [21] <author> C. Berrou and A. Glavieux, </author> <title> Near optimum error correcting coding and decoding: </title> <journal> Turbo-codes, IEEE Transactions on Communications, </journal> <volume> vol. 44, </volume> <pages> pp. 12611271, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: To speed up decoding, our forward-backward algorithm was implemented using a linear interpolation approximation to the function log (1 + exp ()). Also, our decoder did not weight the extrinsic information by the reliability variances as was originally suggested by Berrou et al. <ref> [21] </ref>. (We found that this weighting operation is not necessary at BER greater than 10 6 .) Fig. 2 shows a plot of the log-probability ratio versus iteration number for the correct value of a randomly positioned information bit in each of the 100 blocks.
Reference: [22] <author> N. Wiberg, </author> <title> Codes and Decoding on General Graphs, </title> <institution> Department of Electrical Engineering, Linkoping University, Linkoping Sweden, </institution> <year> 1996, </year> <note> Doctoral dissertation. </note>
Reference-contexts: Also, higher-order criteria increase the computational overhead of early-detection. III. REDUCTION IN COMPLEXITY DUE TO EARLY-DETECTION As pointed out by Wiberg <ref> [22] </ref> and MacKay and Neal [11], the key ideas of iterative decoding using soft decisions were present in Gallager's work in the early 1960's on low-density parity-check codes [23].
Reference: [23] <author> R. G. Gallager, </author> <title> Low-Density Parity-Check Codes, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA., </address> <year> 1963. </year>
Reference-contexts: Also, higher-order criteria increase the computational overhead of early-detection. III. REDUCTION IN COMPLEXITY DUE TO EARLY-DETECTION As pointed out by Wiberg [22] and MacKay and Neal [11], the key ideas of iterative decoding using soft decisions were present in Gallager's work in the early 1960's on low-density parity-check codes <ref> [23] </ref>. Given the channel output, an iterative decoder processes each constituent code (one at a time or in parallel) while using soft decisions made by previous iterations to bias the results.
Reference: [24] <author> J. F. Cheng, </author> <title> Iterative Decoding, </title> <institution> Department of Electrical Engineering, California Institute of Technology, Pasadena California, </institution> <year> 1997, </year> <note> Doctoral dissertation. </note>
Reference: [25] <author> J. Pearl, </author> <title> Fusion, propagation, and structuring in belief networks, </title> <journal> Artificial Intelligence, </journal> <volume> vol. 29, </volume> <pages> pp. 241288, </pages> <year> 1986. </year>
Reference-contexts: The channel outputs for the systematic bits are Y 0;i . The probability propagation algorithm for estimating P Z i jO (z i jo) for an arbitrary observed subset O of Z in a Bayesian network was introduced in <ref> [25] </ref> and [26]. It turns out that the soft-decision iterative decoders for turbocodes, serially-concatenated convolutional codes, product codes, and low-density parity-check codes can be viewed as the application of this algorithm to Bayesian networks like the ones shown in the first column of pictures in Fig. 4.
Reference: [26] <author> S. L. Lauritzen and D. J. Spiegelhalter, </author> <title> Local computations with probabilities on graphical structures and their application to expert systems, </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 50, </volume> <pages> pp. 157224, </pages> <year> 1988. </year>
Reference-contexts: The channel outputs for the systematic bits are Y 0;i . The probability propagation algorithm for estimating P Z i jO (z i jo) for an arbitrary observed subset O of Z in a Bayesian network was introduced in [25] and <ref> [26] </ref>. It turns out that the soft-decision iterative decoders for turbocodes, serially-concatenated convolutional codes, product codes, and low-density parity-check codes can be viewed as the application of this algorithm to Bayesian networks like the ones shown in the first column of pictures in Fig. 4.
Reference: [27] <author> L. E. Baum and T. Petrie, </author> <title> Statistical inference for probabilistic functions of finite state markov chains, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 37, </volume> <pages> pp. 15591563, </pages> <year> 1966. </year>
Reference-contexts: For turbocodes, the Bayesian network consists of two or more chains that are processed using a special case of the probability propagation algorithm, called the forward-backward (a.k.a. BCJR) algorithm <ref> [27, 28] </ref>. This algorithm computes the a posteriori information bit probabilities using the channel output and a priori information bit probabilities. The forward-backward algorithm can be viewed simply as a combination of probabilistic flows [29] computed in the forward direction and in the backward direction.
Reference: [28] <author> L. R. Bahl, J. Cocke, F. Jelinek, and J. Raviv, </author> <title> Optimal decoding of linear codes for minimizing symbol error rate, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 284287, </pages> <month> March </month> <year> 1974. </year>
Reference-contexts: For turbocodes, the Bayesian network consists of two or more chains that are processed using a special case of the probability propagation algorithm, called the forward-backward (a.k.a. BCJR) algorithm <ref> [27, 28] </ref>. This algorithm computes the a posteriori information bit probabilities using the channel output and a priori information bit probabilities. The forward-backward algorithm can be viewed simply as a combination of probabilistic flows [29] computed in the forward direction and in the backward direction.

References-found: 28

