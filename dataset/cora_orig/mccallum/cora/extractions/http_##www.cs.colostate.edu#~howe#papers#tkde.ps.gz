URL: http://www.cs.colostate.edu/~howe/papers/tkde.ps.gz
Refering-URL: http://www.cs.colostate.edu/~howe/pubs.html
Root-URL: 
Email: email: howe@cs.colostate.edu  
Phone: telephone:  
Title: Improving the Reliability of Artificial Intelligence Planning Systems by Analyzing their Failure Recovery  
Author: Adele E. Howe 
Keyword: Index Terms: Artificial Intelligence, Planning, Failure Recovery, Reliability, Debugging  
Date: 31, 1994  
Note: March  
Address: Fort Collins, CO 80523  303-491-7589  
Affiliation: Computer Science Department Colorado State University  
Abstract: As planning technology improves, Artificial Intelligence planners are being embedded in increasingly complicated environments: ones that are particularly challenging even for human experts. Consequently, failure is becoming both increasingly likely for these systems (due to the difficult and dynamic nature of the new environments) and increasingly important to address (due to the systems' potential use on real world applications). This paper describes the development of a failure recovery component for a planner in a complex simulated environment and a procedure (called Failure Recovery Analysis) for assisting programmers in debugging that planner. The failure recovery design is iteratively enhanced and evaluated in a series of experiments. Failure Recovery Analysis is described and demonstrated on an example from the Phoenix planner. The primary advantage of these approaches over existing approaches is that they are based on only a weak model of the planner and its environment, which makes them most suitable when the planner is being developed. By integrating them, failure recovery and Failure Recovery Analysis improve the reliability of the planner by repairing failures during execution and identifying failures due to bugs in the planner and failure recovery itself. fl This research was supported by a DARPA-AFOSR contract F49620-89-C-00113, the National Science Foundation under an Issues in Real-Time Computing grant, CDA-8922572, and a grant from the Office of Naval Research under the University Research Initiative N00014-86-K-0764. The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. This research was conducted as part of my PhD thesis research at the University of Massachusetts. I would like to thank my thesis advisor, Paul Cohen, and my thesis committee for their advice, guidance and supervision of this research. I also wish to thank the anonymous reviewers for their comments, which helped clarify much of the presentation in the paper. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jose A. Ambros-Ingerson and Sam Steel. </author> <title> Integrating planning, execution and monitoring. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 83-88, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Formal theories of planning and replanning have been proposed to guide plan modification (e.g., [12,18]). Failure recovery can be treated as a normal part of the planning process. Lesser's Functionally Accurate, Cooperative (FA/C) paradigm for distributed problem solving [13] and Ambros-Ingerson et al.'s ipem <ref> [1] </ref> treat failure recovery as just another planning or problem solving task. Both recovery through formal analysis and as part of planning require that the planner employs a strong model of what to do in any situation, including failures.
Reference: [2] <author> Rodney A. Brooks. </author> <title> Symbolic error analysis and robot planning. </title> <journal> International Journal of Robotics Research, </journal> <volume> 1(4) </volume> <pages> 29-68, </pages> <month> Winter </month> <year> 1982. </year>
Reference: [3] <author> Carol A. Broverman. </author> <title> Constructive Interpretation of Human-Generated Exceptions During Plan Execution. </title> <type> PhD thesis, </type> <institution> COINS Dept, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> Febru-ary </month> <year> 1991. </year>
Reference-contexts: Thus, they all assume that the analysis mechanism possesses a complete model of the domain and/or planner. Alternatively, the debugger could solicit outside help or information by asking a human user or by inferring obtained from failure recovery execution. Broverman <ref> [3] </ref> views failure recovery as an opportunity for knowledge acquisition and requests assistance from the human user of the planner to augment the system's model.
Reference: [4] <author> Paul R. Cohen, Michael Greenberg, David M. Hart, and Adele E. Howe. </author> <title> Trial by fire: Understanding the design requirements for agents in complex environments. </title> <journal> AI Magazine, </journal> <volume> 10(3), </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: the target environment and are repaired using failure recovery methods stored in the plan knowledge base; during the outer loop, plans are analyzed to understand the observed behaviors and are repaired to avoid detrimental behaviors. 1.2 The Target Planner and its Environment This research was developed using the Phoenix system <ref> [4] </ref> 1 . The Phoenix system consists of a simulated environment, a set of agents that operate in that environment and an experimental interface for running experiments and collecting data. The simulated environment is forest fire fighting in Yellowstone National Park. <p> The map in the upper part of the display shows Yellowstone National Park north of Yellowstone Lake and prior to the famous fires. Features such as wind speed and direction are shown in the window in 1 The basic Phoenix planner as described in <ref> [4] </ref> included no failure recovery component; the research described in this paper augments the original system. 3 the upper left, and geographic features such as rivers, roads and terrain types are shown as light lines or grey shaded areas.
Reference: [5] <author> Fernando J. Corbato. </author> <title> On building systems that will fail. </title> <journal> Communications of the ACM, </journal> <volume> 34(9) </volume> <pages> 72-81, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The set of suggestive structures and explanations needs to be enhanced, especially when FRA is applied to another planner and environment. Additionally, the analysis of dependencies will be expanded to include other features over longer periods of time. 5 Conclusion Certain software systems, so called ambitious systems <ref> [5] </ref>, are prone to failure. These include systems being developed for novel or unfamiliar tasks, systems in unpredictable environments, or systems with organizational complexity.
Reference: [6] <author> Maria Gini. </author> <title> Automatic error detection and recovery. </title> <institution> Computer Science Dept. 88-48, University of Minnesota, Minneapolis, MN, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Backward recovery requires that actions can be undone and that the system has full con-trol over its environment; these requirements preclude backward recovery for many robotics and planning systems. Forward recovery transforms the failure state to a correct state by repairing the failure <ref> [6] </ref>. Approaches to forward recovery differ on how they decide on the appropriate transformation or repair: through formal analysis, normal planning, or heuristics. Formal analysis approaches decide how to repair failures by referencing a complete model of failures and their causes.
Reference: [7] <author> Kristian John Hammond. </author> <title> Case-Based Planning: An Integrated Theory of Planning, Learning and Memory. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> October </month> <year> 1986. </year>
Reference-contexts: One enhancement to the basic approach is to keep track of what changes, made during the debugging of particular plans, were felicitous. Hammond's chef <ref> [7] </ref> backchains from failure to the states that caused it, applying causal rules that describe the effects of actions and map the 14 information to canonical failures and repair strategies. <p> Suggestive structures are similar in purpose to Thematic Organization Packages (or TOPs) in Chef <ref> [7] </ref>; however, while TOPs encompass the diagnosis of the failure, the interactions between the steps and states of the plan and the strategies for repair, suggestive structures identify only the interactions between the steps. Consequently, suggestive structures can be combined to form different explanations of failures and indicate different repairs.
Reference: [8] <author> Steve Hanks and R. James Firby. </author> <title> Issues and architectures for planning and execution. </title> <editor> In Katia P. Sycara, editor, </editor> <booktitle> Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 71-76, </pages> <address> Palo Alto, Ca., November 1990. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <pages> 27 </pages>
Reference-contexts: All of these methods make structural changes to plans in progress and are applicable to nearly all failures. These recovery methods, or ones very like them, have been used in other recovery systems. WATA is similar to the "retry" method described in <ref> [8] </ref>. RV and RA are Phoenix specific forms of sipe's Reinstantiate [30]. SA is similar to "try a different action" in switch [22] and action substitution used for debugging in gordius [24].
Reference: [9] <author> Adele E. Howe. </author> <title> Analyzing failure recovery to improve planner design. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 387-393, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: As a consequence of 6 This section expands on the presentation of FRA originally published in the short paper <ref> [9] </ref> by providing more detail on FRA, evaluating aspects of the procedure and relating it to improving planner reliability. 15 these differences, FRA is most appropriate when one cannot model behavior off-line (a model is unavailable or flawed) and when the bugs are intermittent and may involve subtle interactions.
Reference: [10] <author> Adele E. Howe. </author> <title> Accepting the Inevitable: The Role of Failure Recovery in the Design of Planners. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Department of Computer Science, </institution> <address> Amherst, MA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: This paper describes an integrated approach to dealing with both types of failure, which uses feedback from failure recovery to help debug plan failures and improve failure recovery. This approach was developed as part of my thesis research <ref> [10] </ref> on improving the reliability of the Phoenix planner. 1.1 Approaches to Improving Planner Reliability In general, software failures have been addressed in two ways: automated failure recovery and debugging. The first involves designing the software to detect and repair its own failures.
Reference: [11] <author> Adele E. Howe and Paul R. Cohen. </author> <title> Failure recovery: A model and experiments. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 801-808, </pages> <address> Anaheim, CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: The model is based on three assumptions about the independence of the parameters: 2 The cost model and experiments testing improvements were described originally, but in less detail in <ref> [11] </ref>. 8 1. The cost of each method C (M m ) is independent of the situation S i . Because the recovery methods are designed to be domain-independent, intuition suggests that their costs may be independent of when they are used. 2.
Reference: [12] <author> Subbarao Kambhampati and James A. Hendler. </author> <title> A validation-structure-based theory of plan modification and reuse. </title> <journal> Artificial Intelligence Journal, </journal> <pages> 55(2-3), </pages> <year> 1992. </year>
Reference: [13] <author> Victor R. Lesser. </author> <title> A retrospective view of FA/C distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21(6):13471362, </volume> <month> November/December </month> <year> 1991. </year>
Reference-contexts: Formal theories of planning and replanning have been proposed to guide plan modification (e.g., [12,18]). Failure recovery can be treated as a normal part of the planning process. Lesser's Functionally Accurate, Cooperative (FA/C) paradigm for distributed problem solving <ref> [13] </ref> and Ambros-Ingerson et al.'s ipem [1] treat failure recovery as just another planning or problem solving task. Both recovery through formal analysis and as part of planning require that the planner employs a strong model of what to do in any situation, including failures.
Reference: [14] <author> Nancy G. Leveson. </author> <title> Software safety: Why, what, and how. </title> <journal> Computing Surveys, </journal> <volume> 18(2) </volume> <pages> 125-163, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Most of the planning literature has addressed planning time failure, while literature about robotics and more general software tends to address execution time failure. The two basic approaches to failure recovery are backward and forward recovery <ref> [14] </ref>. In backward recovery, the system is returned to some previous correct state and resumes execution from 4 there. Backward recovery requires that actions can be undone and that the system has full con-trol over its environment; these requirements preclude backward recovery for many robotics and planning systems.
Reference: [15] <author> D.M. Lyons, R. Vijaykumar, and S.T. Venkataraman. </author> <title> A representation for error detection and recovery in robot plans. </title> <booktitle> In Proceedings of SPIE Symposium on Intelligent Control and Adaptive Systems, </booktitle> <pages> pages 14-25, </pages> <address> Philadelphia, </address> <month> November </month> <year> 1989. </year>
Reference: [16] <author> David P. Miller. </author> <title> Execution monitoring for a mobile robot system. </title> <booktitle> In Proceedings of SPIE Symposium on Intelligent Control and Adaptive Systems, </booktitle> <pages> pages 36-43, </pages> <address> Philadelphia, PA, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Failure is increasingly likely because of the difficult and dynamic nature of the new environments; failure is increasingly important to address because of the systems' potential use on applications such as scheduling manufacturing production lines [26] and Hubble space telescope time [17], and controlling robots <ref> [16] </ref>. AI planners determine a course of action; it may be the next action to be taken or may be a long sequence of actions. Plan failures may be caused by actions not having their intended effects, by unexpected environmental changes or by inadequacies in the planner itself.
Reference: [17] <author> Steven Minton, Mark D. Johnston, Andrew B. Philips, and Philip Laird. </author> <title> Solving large-scale constraint satisfaction and scheduling problems using a heuristic repair method. </title> <booktitle> In Proceedings 28 of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 17-24, </pages> <address> Anaheim, CA, </address> <year> 1991. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Failure is increasingly likely because of the difficult and dynamic nature of the new environments; failure is increasingly important to address because of the systems' potential use on applications such as scheduling manufacturing production lines [26] and Hubble space telescope time <ref> [17] </ref>, and controlling robots [16]. AI planners determine a course of action; it may be the next action to be taken or may be a long sequence of actions.
Reference: [18] <author> Leora Morgenstern. Replanning. </author> <booktitle> In Proceedings of the DARPA Knowledge-Based Planning Workshop, </booktitle> <pages> pages 5-1 - 5-10, </pages> <address> Austin, TX, </address> <month> December </month> <year> 1987. </year>
Reference: [19] <author> N. Hari Narayanan and N. Viswanadham. </author> <title> A methodology for knowledge acquisition and reasoning in failure analysis of systems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-17(2):274-288, </volume> <month> March/April </month> <year> 1987. </year>
Reference: [20] <author> S.Y. Nof, O.Z. Maimon, and R. G. Wilhelm. </author> <title> Experiments for planning error-recovery program in robotic work. </title> <booktitle> In Proceedings of the 1987 ASME International Computers in Engineering Comference, </booktitle> <pages> pages 253-262, </pages> <address> NY,NY, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: Many different approaches have been proposed for failure recovery; most depend either on the domain or on the planner design. However, the literature offers few suggestions about how to design failure recovery for a new planner or domain. For constructing domain-dependent recovery programs, Nof et al. <ref> [20] </ref> propose a four-step framework: analyze the task, develop alternative 5 recovery strategies, determine a selection strategy, and update based on experience with the sys-tem.
Reference: [21] <author> Christopher Owens. </author> <title> Representing abstract plan failures. </title> <booktitle> In Proceedings of the Twelfth Cognitive Science Conference, </booktitle> <pages> pages 277-284, </pages> <address> Boston, MA, 1990. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: Both recovery through formal analysis and as part of planning require that the planner employs a strong model of what to do in any situation, including failures. Heuristic approaches allow for gaps in knowledge and apply recovery methods to repair the failure. Typically, heuristic approaches operate by "retrieve-and-apply" <ref> [21] </ref> which maps observed failures to suggested responses. The most comprehensive and commonly used domain-independent strategy is replanning (e.g., [15,30]), which involves restating the planning problem and re-initiating planning.
Reference: [22] <author> Harry J. Porta. </author> <title> Dynamic replanning. </title> <booktitle> In Proceedings of ROBEXS 86: Second Annual Workshop on Robotics and Expert Systems, </booktitle> <pages> pages 109-115, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: These recovery methods, or ones very like them, have been used in other recovery systems. WATA is similar to the "retry" method described in [8]. RV and RA are Phoenix specific forms of sipe's Reinstantiate [30]. SA is similar to "try a different action" in switch <ref> [22] </ref> and action substitution used for debugging in gordius [24]. The two replan methods, RT and RP, are constrained forms 7 of the more general replanning done in many failure recovery systems.
Reference: [23] <author> Reid Simmons. </author> <title> Monitoring and error recovery for autonomous walking. </title> <booktitle> In Proc. IEEE International Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 1407-1412, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Similarly, Simmons advocates starting the system with basic competence at its task (i.e., no failure recovery) and then adding execution monitoring and failure recovery methods as needed <ref> [23] </ref>.
Reference: [24] <author> Reid G. Simmons. </author> <title> A theory of debugging plans and interpretations. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 94-99, </pages> <address> Minneapolis, Minnesota, </address> <year> 1988. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: WATA is similar to the "retry" method described in [8]. RV and RA are Phoenix specific forms of sipe's Reinstantiate [30]. SA is similar to "try a different action" in switch [22] and action substitution used for debugging in gordius <ref> [24] </ref>. The two replan methods, RT and RP, are constrained forms 7 of the more general replanning done in many failure recovery systems. <p> One of the first approaches to debugging plans was Sussman's hacker [28] which detects, classifies and repairs bugs in Blocksworld plans, using considerable knowledge about its domain. Simmons' gordius <ref> [24] </ref> debugs faulty plans by regressing desired effects through a causal dependency structure constructed during plan generation from a causal model of the domain. One enhancement to the basic approach is to keep track of what changes, made during the debugging of particular plans, were felicitous.
Reference: [25] <author> Herbert A. Simon and Joseph B. Kadane. </author> <title> Optimal problem-solving search: All-or-none solutions. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 6 </volume> <pages> 235-247, </pages> <year> 1975. </year>
Reference-contexts: The cost model can be used to guide the selection of recovery methods to minimize total cost. Simon and Kadane <ref> [25] </ref> showed that, for problems of the type described by equation 1, the expected cost of executing a sequence is minimized by the strategy of trying the methods in decreasing order of P (M m jS i ) which intuitively means "select the method that is most likely to succeed with
Reference: [26] <author> Stephen F. Smith, Peng Si Ow, Nicola Muscettola, Jean-Yves Potvin, and Dirk C. Matthys. OPIS: </author> <title> an integrated framework for generating and revising factory schedules. </title> <editor> In Katia P. </editor> <booktitle> 29 Sycara, editor, Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 497-507. </pages> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: Failure is increasingly likely because of the difficult and dynamic nature of the new environments; failure is increasingly important to address because of the systems' potential use on applications such as scheduling manufacturing production lines <ref> [26] </ref> and Hubble space telescope time [17], and controlling robots [16]. AI planners determine a course of action; it may be the next action to be taken or may be a long sequence of actions.
Reference: [27] <author> Sankaran Srinivas. </author> <title> Error Recovery in Robot Systems. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <year> 1977. </year>
Reference: [28] <author> Gerald A. Sussman. </author> <title> A computational model of skill acquisition. </title> <type> Technical Report Memo no. </type> <institution> AI-TR-297, MIT AI Lab, </institution> <year> 1973. </year>
Reference-contexts: In general, the approach to debugging the knowledge base has been to debug the plans themselves, using knowledge intensive models of the domain, as the plans are being constructed for particular situations. One of the first approaches to debugging plans was Sussman's hacker <ref> [28] </ref> which detects, classifies and repairs bugs in Blocksworld plans, using considerable knowledge about its domain. Simmons' gordius [24] debugs faulty plans by regressing desired effects through a causal dependency structure constructed during plan generation from a causal model of the domain.
Reference: [29] <author> Katia Sycara. </author> <title> Using case-based reasoning for plan adaptation and repair. </title> <booktitle> In Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <pages> pages 425-434. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year>
Reference: [30] <author> David E. Wilkins. </author> <title> Recovering from execution errors in SIPE. </title> <type> Technical Report 346, </type> <institution> Artificial Intelligence Center, Computer Science and Technology Center, SRI International, </institution> <year> 1985. </year>
Reference-contexts: Similarly, Simmons advocates starting the system with basic competence at its task (i.e., no failure recovery) and then adding execution monitoring and failure recovery methods as needed [23]. Wilkins <ref> [30] </ref> advocates combining both domain-independent and domain-specific methods; the system can try the more efficient domain-specific methods when they are available, but fall back on the domain-independent, when necessary. 2.1 Designing Failure Recovery for Phoenix Most of the previously mentioned approaches to failure recovery classify the failure and select from a <p> These recovery methods, or ones very like them, have been used in other recovery systems. WATA is similar to the "retry" method described in [8]. RV and RA are Phoenix specific forms of sipe's Reinstantiate <ref> [30] </ref>. SA is similar to "try a different action" in switch [22] and action substitution used for debugging in gordius [24]. The two replan methods, RT and RP, are constrained forms 7 of the more general replanning done in many failure recovery systems.
Reference: [31] <author> Roland Zito-Wolf and Richard Alterman. Ad-hoc, </author> <title> fail-safe plan learning. </title> <booktitle> In Proceedings of the Twelfth Cognitive Science Conference, </booktitle> <pages> pages 908-913, </pages> <address> Boston, MA, </address> <month> July 24-28 </month> <year> 1990. </year> <month> 30 </month>
Reference-contexts: Broverman [3] views failure recovery as an opportunity for knowledge acquisition and requests assistance from the human user of the planner to augment the system's model. Zito-Wolf and Alterman adapt plans when overly general plans fail <ref> [31] </ref>; these adaptations repair failures and then are used to augment the plan stored in long-term memory with choice points and alternative actions.
References-found: 31

