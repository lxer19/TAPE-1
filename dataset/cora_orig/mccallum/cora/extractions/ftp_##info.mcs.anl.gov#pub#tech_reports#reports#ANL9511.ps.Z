URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/ANL9511.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: Distribution Category:  PETSc 2.0 Users Manual Revision 2.0.16  
Author: Mathematics and by Satish Balay William Gropp Lois Curfman McInnes Barry Smith 
Date: February 1997  
Note: This manual is intended for use with PETSc 2.0.16.  This work was supported by the Mathematical, Information, and Comptuational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Web: ANL-95/11  
Address: 9700 South Cass Avenue Argonne, IL 60439  
Affiliation: Computer Science (UC-405) ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Tcl/Tk World Wide Web page. </institution> <note> http://www.sunlabs.com/research/tcl/, August 1996. </note>
Reference-contexts: ltog [row]; 36 /* boundary points */ if (i == 0 || j == 0 || i == mx-1 || j == my-1 ) - ierr = MatSetValues (jac,1,&grow,1,&grow,&one,INSERT_VALUES); CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v <ref> [1] </ref> = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ <p> boundary points */ if (i == 0 || j == 0 || i == mx-1 || j == my-1 ) - ierr = MatSetValues (jac,1,&grow,1,&grow,&one,INSERT_VALUES); CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v <ref> [1] </ref> = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble <p> have very different numbers of nonzeros, one should attempt to indicate (nearly) the exact number of elements intended for the various rows with the optional array, nzz of length m, where m is the number of rows, for example, int nnz [m]; nnz [0] = &lt;nonzeros in row 0&gt; nnz <ref> [1] </ref> = &lt;nonzeros in row 1&gt; .... nnz [m-1] = &lt;nonzeros in row m-1&gt; In this case, the assembly process will require no additional memory allocations if the nnz estimates are correct. <p> Otherwise, the routine is implementation dependent. You MUST call VecRestoreArray () when you no longer need access to the array. */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); ierr = VecGetArray (f,&ff); CHKERRQ (ierr); /* Compute function */ ff <ref> [1] </ref> = xx [0]*xx [1] + xx [1]*xx [1] - 6.0; Restore vectors */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); ierr = VecRestoreArray (f,&ff); CHKERRQ (ierr); return 0; - /* FormJacobian Evaluates Jacobian matrix. 62 Input Parameters: . snes the SNES context . x input vector . dummy optional user-defined context <p> Otherwise, the routine is implementation dependent. You MUST call VecRestoreArray () when you no longer need access to the array. */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); ierr = VecGetArray (f,&ff); CHKERRQ (ierr); /* Compute function */ ff <ref> [1] </ref> = xx [0]*xx [1] + xx [1]*xx [1] - 6.0; Restore vectors */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); ierr = VecRestoreArray (f,&ff); CHKERRQ (ierr); return 0; - /* FormJacobian Evaluates Jacobian matrix. 62 Input Parameters: . snes the SNES context . x input vector . dummy optional user-defined context (not used here) Output <p> Otherwise, the routine is implementation dependent. You MUST call VecRestoreArray () when you no longer need access to the array. */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); ierr = VecGetArray (f,&ff); CHKERRQ (ierr); /* Compute function */ ff <ref> [1] </ref> = xx [0]*xx [1] + xx [1]*xx [1] - 6.0; Restore vectors */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); ierr = VecRestoreArray (f,&ff); CHKERRQ (ierr); return 0; - /* FormJacobian Evaluates Jacobian matrix. 62 Input Parameters: . snes the SNES context . x input vector . dummy optional user-defined context (not used here) Output Parameters: . jac - <p> Since this is such a small problem, we set all entries for the matrix at once. */ A [2] = xx <ref> [1] </ref>; A [3] = xx [0] + 2.0*xx [1]; ierr = MatSetValues (*jac,2,idx,2,idx,A,INSERT_VALUES); CHKERRQ (ierr); *flag = SAME_NONZERO_PATTERN; /* Restore vector */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); /* Assemble matrix */ ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyEnd (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); return 0; - To create a SNES <p> Since this is such a small problem, we set all entries for the matrix at once. */ A [2] = xx <ref> [1] </ref>; A [3] = xx [0] + 2.0*xx [1]; ierr = MatSetValues (*jac,2,idx,2,idx,A,INSERT_VALUES); CHKERRQ (ierr); *flag = SAME_NONZERO_PATTERN; /* Restore vector */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); /* Assemble matrix */ ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyEnd (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); return 0; - To create a SNES solver, one must first call SNESCreate () and <p> Also form a different preconditioning matrix that has only the diagonal elements. */ 72 i = 0; A [0] = 1.0; ierr = MatSetValues (*jac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); for ( i=1; i&lt;n-1; i++ ) - A [0] = d; A <ref> [1] </ref> = -2.0*d + 2.0*xx [i]; A [2] = d; ierr = MatSetValues (*jac,1,&i,3,j,A,INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [1],INSERT_VALUES); CHKERRQ (ierr); - ierr = MatSetValues (*jac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyBegin (*prejac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr <p> As discussed in Section 12.1, these options enable the user to set particular solvers, data structures, profiling options, and so on at runtime, thereby facilitating the customization and comparison of various algorithms and storage schemes. 14.1 Getting Started PETScView and PETScOpts use the Tcl and the Tk Toolkit <ref> [1] </ref>. Therefore, in order to use the PETSc utility programs, the Tcl and Tk packages must be installed on the user's local system. See the following WWW site for information about Tcl/Tk: http://www.sunlabs.com/research/tcl/. <p> For PETSc to control memory allocation, the user should merely set diagv=PETSC NULL. A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = <ref> [2, 1, 0, -3] </ref>. <p> A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = [2, 1, 0, -3]. The diagonals need not be listed in any particular order, so that diag = <ref> [-3, 0, 1, 2] </ref> or diag = [0, 2, -3, 1] would also be valid values for the diag array. a00 0 0 a03 0 0 a20 a21 a22 0 0 a25 0 0 a42 a43 a44 0 138 15.5.10 Parallel Block Diagonal Sparse Matrices The parallel block diagonal matrices are <p> A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = [2, 1, 0, -3]. The diagonals need not be listed in any particular order, so that diag = [-3, 0, 1, 2] or diag = <ref> [0, 2, -3, 1] </ref> would also be valid values for the diag array. a00 0 0 a03 0 0 a20 a21 a22 0 0 a25 0 0 a42 a43 a44 0 138 15.5.10 Parallel Block Diagonal Sparse Matrices The parallel block diagonal matrices are partitioned by rows across the processors, so
Reference: [2] <author> Peter N. Brown and Youcef Saad. </author> <title> Hybrid Krylov methods for nonlinear systems of equations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 </volume> <pages> 450-481, </pages> <year> 1990. </year>
Reference-contexts: || j == 0 || i == mx-1 || j == my-1 ) - ierr = MatSetValues (jac,1,&grow,1,&grow,&one,INSERT_VALUES); CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v <ref> [2] </ref> = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), <p> == my-1 ) - ierr = MatSetValues (jac,1,&grow,1,&grow,&one,INSERT_VALUES); CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v <ref> [2] </ref> = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), MatAssemblyEnd (). <p> B optionally different preconditioning matrix . flag flag indicating matrix structure */ int FormJacobian (SNES snes,Vec x,Mat *jac,Mat *B,MatStructure *flag,void *dummy) - Scalar *xx, A [4]; int ierr, idx <ref> [2] </ref> = -0,1-; /* Get pointer to vector data */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); /* Compute Jacobian entries and insert into matrix. Since this is such a small problem, we set all entries for the matrix at once. */ A [2] = xx [1]; A [3] = xx [0] <p> - Scalar *xx, A [4]; int ierr, idx <ref> [2] </ref> = -0,1-; /* Get pointer to vector data */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); /* Compute Jacobian entries and insert into matrix. Since this is such a small problem, we set all entries for the matrix at once. */ A [2] = xx [1]; A [3] = xx [0] + 2.0*xx [1]; ierr = MatSetValues (*jac,2,idx,2,idx,A,INSERT_VALUES); CHKERRQ (ierr); *flag = SAME_NONZERO_PATTERN; /* Restore vector */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); /* Assemble matrix */ ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyEnd (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); return 0; - To <p> The user can create a matrix-free context for use within SNES with the routine ierr = SNESDefaultMatrixFreeMatCreate (SNES snes,Vec x, Mat *mat); This routine creates the data structures needed for the matrix-vector products that arise within Krylov space iterative methods <ref> [2] </ref> by employing the matrix type MATSHELL, discussed in Section 3.3. The default SNES matrix-free approximations can also be invoked with the command -snes mf. <p> h 68 where h is computed via h = e rel fl u T a=jjajj 2 ifju 0 aj &gt; u min fl jjajj 1 = e rel fl u min fl sign (u T a) fl jjajj 1 =jjajj 2 otherwise: This approach is taken from Brown and Saad <ref> [2] </ref>. These parameters can also be set from the options database with -snes_mf_err &lt;err&gt; -snes_mf_umin &lt;umin&gt; Note that setting these parameter appropriately is crucial for achieving fast convergence with matrix-free Newton-Krylov methods. We include an example in Figure 14 that explicitly uses a matrix-free approach. <p> preconditioning matrix that has only the diagonal elements. */ 72 i = 0; A [0] = 1.0; ierr = MatSetValues (*jac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); for ( i=1; i&lt;n-1; i++ ) - A [0] = d; A [1] = -2.0*d + 2.0*xx [i]; A <ref> [2] </ref> = d; ierr = MatSetValues (*jac,1,&i,3,j,A,INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [1],INSERT_VALUES); CHKERRQ (ierr); - ierr = MatSetValues (*jac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatSetValues (*prejac,1,&i,1,&i,&A [0],INSERT_VALUES); CHKERRQ (ierr); ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyBegin (*prejac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyEnd (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = <p> For PETSc to control memory allocation, the user should merely set diagv=PETSC NULL. A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = <ref> [2, 1, 0, -3] </ref>. <p> A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = [2, 1, 0, -3]. The diagonals need not be listed in any particular order, so that diag = <ref> [-3, 0, 1, 2] </ref> or diag = [0, 2, -3, 1] would also be valid values for the diag array. a00 0 0 a03 0 0 a20 a21 a22 0 0 a25 0 0 a42 a43 a44 0 138 15.5.10 Parallel Block Diagonal Sparse Matrices The parallel block diagonal matrices are <p> A simple example of this storage format is illustrated below for block size nb=1. Here nd = 4 and diag = [2, 1, 0, -3]. The diagonals need not be listed in any particular order, so that diag = [-3, 0, 1, 2] or diag = <ref> [0, 2, -3, 1] </ref> would also be valid values for the diag array. a00 0 0 a03 0 0 a20 a21 a22 0 0 a25 0 0 a42 a43 a44 0 138 15.5.10 Parallel Block Diagonal Sparse Matrices The parallel block diagonal matrices are partitioned by rows across the processors, so
Reference: [3] <author> J. E. Dennis Jr. and Robert B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: ierr = MatSetValues (jac,1,&grow,1,&grow,&one,INSERT_VALUES); CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v <ref> [3] </ref> = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), MatAssemblyEnd (). <p> CHKERRQ (ierr); continue; - /* interior grid points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v <ref> [3] </ref> = -hydhx; col [3] = ltog [row + 1]; v [4] = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), MatAssemblyEnd (). <p> Since this is such a small problem, we set all entries for the matrix at once. */ A [2] = xx [1]; A <ref> [3] </ref> = xx [0] + 2.0*xx [1]; ierr = MatSetValues (*jac,2,idx,2,idx,A,INSERT_VALUES); CHKERRQ (ierr); *flag = SAME_NONZERO_PATTERN; /* Restore vector */ ierr = VecRestoreArray (x,&xx); CHKERRQ (ierr); /* Assemble matrix */ ierr = MatAssemblyBegin (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); ierr = MatAssemblyEnd (*jac,MAT_FINAL_ASSEMBLY); CHKERRQ (ierr); return 0; - To create a SNES solver, one <p> By default, this technique employs cubic backtracking <ref> [3] </ref>. An alternative line search routine can be set with the command ierr = SNESSetLineSearch (SNES snes, int (*ls)(SNES,Vec,Vec,Vec,Vec,double,double*,double*)); 65 Other line search methods provided by PETSc are SNESNoLineSearch () and SNESQuadraticLineSearch (), which can be set with the option -snes line search [basic,quadratic,cubic]. <p> A - Jacobian matrix . B different preconditioning matrix . flag flag indicating matrix structure */ int FormJacobian (SNES snes,Vec x,Mat *jac,Mat *prejac,MatStructure *flag, void *dummy) - Scalar *xx, A <ref> [3] </ref>, d; int i, n, j [3], ierr; ierr = VecGetArray (x,&xx); CHKERRQ (ierr); ierr = VecGetSize (x,&n); CHKERRQ (ierr); d = (double)(n - 1); d = d*d; /* Form Jacobian. <p> A - Jacobian matrix . B different preconditioning matrix . flag flag indicating matrix structure */ int FormJacobian (SNES snes,Vec x,Mat *jac,Mat *prejac,MatStructure *flag, void *dummy) - Scalar *xx, A <ref> [3] </ref>, d; int i, n, j [3], ierr; ierr = VecGetArray (x,&xx); CHKERRQ (ierr); ierr = VecGetSize (x,&n); CHKERRQ (ierr); d = (double)(n - 1); d = d*d; /* Form Jacobian.
Reference: [4] <author> S. Eisenstat. </author> <title> Efficient implementation of a class of CG methods. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 2 </volume> <pages> 1-4, </pages> <year> 1981. </year>
Reference-contexts: points */ v [0] = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v <ref> [4] </ref> = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), MatAssemblyEnd (). <p> = -hxdhy; col [0] = ltog [row - gxm]; v [1] = -hydhx; col [1] = ltog [row - 1]; v [2] = two*(hydhx + hxdhy) - sc*lambda*exp (x [row]); col [2] = grow; v [3] = -hydhx; col [3] = ltog [row + 1]; v <ref> [4] </ref> = -hxdhy; col [4] = ltog [row + gxm]; ierr = MatSetValues (jac,1,&grow,5,col,v,INSERT_VALUES); CHKERRQ (ierr); - /* Assemble matrix, using the 2-step process: MatAssemblyBegin (), MatAssemblyEnd (). <p> These variants can also be set with the options -pc sor omega &lt;omega&gt;, -pc sor its &lt;its&gt;, -pc sor backward, -pc sor symmetric, -pc sor local forward, -pc sor local backward, and -pc sor local symmetric. The Eisenstat trick <ref> [4] </ref> for SSOR preconditioning can be employed with the method PCEISENSTAT (-pc type eisenstat). By using both left and right preconditioning of the linear system, this variant of SSOR requires about half of the floating-point operations for conventional SSOR. <p> B optionally different preconditioning matrix . flag flag indicating matrix structure */ int FormJacobian (SNES snes,Vec x,Mat *jac,Mat *B,MatStructure *flag,void *dummy) - Scalar *xx, A <ref> [4] </ref>; int ierr, idx [2] = -0,1-; /* Get pointer to vector data */ ierr = VecGetArray (x,&xx); CHKERRQ (ierr); /* Compute Jacobian entries and insert into matrix.
Reference: [5] <author> S. C. Eisenstat and H. F. Walker. </author> <title> Choosing the forcing terms in an inexact Newton method. </title> <institution> Preprint Utah State University Math. Stat. Dept. Res. </institution> <type> Report 6/94/75, Logan, </type> <address> UT, </address> <year> 1994. </year> <note> (to appear in SIAM J. Sci. Comput.). </note>
Reference-contexts: FormJacobian (SNES snes,Vec X,Mat *J,Mat *B,MatStructure *flag,void *ptr) - AppCtx *user = (AppCtx *) ptr; /* user-defined application context */ Mat jac = *J; /* Jacobian matrix */ Vec localX = user-&gt;localX; /* local vector */ int *ltog; /* local-to-global mapping */ int ierr, i, j, row, mx, my, col <ref> [5] </ref>; int nloc, xs, ys, xm, ym, gxs, gys, gxm, gym, grow; Scalar two = 2.0, one = 1.0, lambda, v [5], hx, hy, hxdhy, hydhx, sc, *x; mx = user-&gt;mx; my = user-&gt;my; lambda = user-&gt;param; hx = one/(double)(mx-1); hy = one/(double)(my-1); sc = hx*hy; hxdhy = hx/hy; hydhx = <p> jac = *J; /* Jacobian matrix */ Vec localX = user-&gt;localX; /* local vector */ int *ltog; /* local-to-global mapping */ int ierr, i, j, row, mx, my, col <ref> [5] </ref>; int nloc, xs, ys, xm, ym, gxs, gys, gxm, gym, grow; Scalar two = 2.0, one = 1.0, lambda, v [5], hx, hy, hxdhy, hydhx, sc, *x; mx = user-&gt;mx; my = user-&gt;my; lambda = user-&gt;param; hx = one/(double)(mx-1); hy = one/(double)(my-1); sc = hx*hy; hxdhy = hx/hy; hydhx = hy/hx; /* Scatter ghost points to local vector, using the 2-step process DAGlobalToLocalBegin (), DAGlobalToLocalEnd (). <p> By default a constant relative convergence tolerance is used for solving the subsidiary linear systems within the Newton-like methods of SNES. When solving a system of nonlinear equations, one can instead employ the techniques of Eisenstat and Walker <ref> [5] </ref> to compute j k at each step of the nonlinear solver by using the option -snes ksp ew conv .
Reference: [6] <author> R. Freund, G. H. Golub, and N. Nachtigal. </author> <title> Iterative Solution of Linear Systems, </title> <address> pages 57-100. </address> <publisher> Acta Numerica. Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In addition, particular solution techniques and their associated options can be selected at runtime. The combination of a Krylov subspace method and a preconditioner is at the center of most modern numerical codes for the iterative solution of linear systems. See, for example, <ref> [6] </ref> for an overview of the theory of such methods. SLES creates a simplified interface to the lower-level KSP and PC modules within the PETSc package.
Reference: [7] <author> Roland W. Freund. </author> <title> A transpose-free quasi-minimal residual algorithm for non-Hermitian linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 14 </volume> <pages> 470-482, </pages> <year> 1993. </year>
Reference-contexts: Options Default Database Convergence Method KSPType Name Monitor y Richardson KSPRICHARDSON richardson true Chebychev KSPCHEBYCHEV chebychev true Conjugate Gradient [10] KSPCG cg true Generalized Minimal Residual [15] KSPGMRES gmres precond BiCGSTAB [19] KSPBCGS bcgs precond Conjugate Gradient Squared [17] KSPCGS cgs precond Transpose-Free Quasi-Minimal Residual (1) <ref> [7] </ref> KSPTFQMR tfqmr precond Transpose-Free Quasi-Minimal Residual (2) KSPTCQMR tcqmr precond Conjugate Residual KSPCR cr precond Least Squares Method KSPLSQR lsqr precond Shell for no KSP method KSPPREONLY preonly precond y true denotes true residual norm, precond denotes preconditioned residual norm 4.3.2 Convergence Tests The default convergence test, KSPDefaultConverged (), is
Reference: [8] <author> William Gropp, Ewing Lusk, Nathan Doss, and Anthony Skjellum. </author> <note> MPICH home page. http://www.mcs.anl.gov/mpi/mpich/index.html, December 1996. </note>
Reference-contexts: Also, Section 10.4 gives information on restricting event logging. 10.1.4 Using -log mpe with Upshot/Nupshot It is also possible to use the Upshot (or Nupshot) package [9] to visualize PETSc events. This package comes with the MPE software, which is part of the MPICH <ref> [8] </ref> implementation of MPI. The option -log_mpe [logfile] creates a logfile of events appropriate for viewing with Upshot. The user can either use the default logging file, mpe.log, or specify an optional name via logfile.
Reference: [9] <author> Virginia Herrarte and Ewing Lusk. </author> <title> Studying parallel program behavior with Upshot. </title> <type> Technical Report ANL-91/15, </type> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Chapter 14 provides details regarding this Tk/Tcl tool, which provides a high-level view of the interrelationships among various code modules. Also, Section 10.4 gives information on restricting event logging. 10.1.4 Using -log mpe with Upshot/Nupshot It is also possible to use the Upshot (or Nupshot) package <ref> [9] </ref> to visualize PETSc events. This package comes with the MPE software, which is part of the MPICH [8] implementation of MPI. The option -log_mpe [logfile] creates a logfile of events appropriate for viewing with Upshot.
Reference: [10] <author> Magnus R. Hestenes and Eduard Steifel. </author> <title> Methods of conjugate gradients for solving linear systems. </title> <journal> J. Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 409-436, </pages> <year> 1952. </year>
Reference-contexts: All methods use left preconditioning by default. Options Default Database Convergence Method KSPType Name Monitor y Richardson KSPRICHARDSON richardson true Chebychev KSPCHEBYCHEV chebychev true Conjugate Gradient <ref> [10] </ref> KSPCG cg true Generalized Minimal Residual [15] KSPGMRES gmres precond BiCGSTAB [19] KSPBCGS bcgs precond Conjugate Gradient Squared [17] KSPCGS cgs precond Transpose-Free Quasi-Minimal Residual (1) [7] KSPTFQMR tfqmr precond Transpose-Free Quasi-Minimal Residual (2) KSPTCQMR tcqmr precond Conjugate Residual KSPCR cr precond Least Squares Method KSPLSQR lsqr precond Shell for
Reference: [11] <author> Mark T. Jones and Paul E. Plassmann. BlockSolve v1.1: </author> <title> Scalable library software for the parallel solution of sparse linear systems. </title> <type> Technical Report ANL-92/46, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: By alleviating the considerable overhead for dynamic memory allocation, such tuning can significantly enhance performance. We support incomplete factorization preconditioners for several matrix types for the uniprocessor case. In addition, for the parallel case we provide an interface to the ILU and ICC preconditioners of BlockSolve95 <ref> [11] </ref>. BlockSolve95 is available by anonymous ftp at info.mcs.anl.gov in the directory pub/BlockSolve95; for further information see the WWW address: http://www.mcs.anl.gov/blocksolve95/index.html. PETSc enables users to employ the preconditioners within BlockSolve95 by using the BlockSolve95 matrix format MATMPIROWBS and invoking either the PCILU or PCICC method within the linear solvers.
Reference: [12] <author> Jorge J. More, Danny C. Sorenson, Burton S. Garbow, and Kenneth E. Hillstrom. </author> <title> The MINPACK project. </title> <editor> In Wayne R. Cowell, editor, </editor> <booktitle> Sources and Development of Mathematical Software, </booktitle> <pages> pages 88-111, </pages> <year> 1984. </year>
Reference-contexts: Users may write their own customized line search codes by modeling them after one of the defaults provided by PETSc. 5.2.2 Trust Region Methods The most basic trust region method in SNES for solving systems of nonlinear equations, SNES EQ NTR (-snes type tr), is taken from the MINPACK project <ref> [12] </ref>. Several parameters can be set to control the variation of the trust region size during the solution process. In particular, the user can control the initial trust region radius, computed by = 0 kF 0 k 2 ; by setting 0 via the option -snes trust region delta0 &lt;delta0&gt;. <p> Note that the user must always preset the nonzero structure in the matrix regardless of which coloring routine is used. For sequential matrices PETSc provides three matrix coloring routines from the MINPACK package <ref> [12] </ref>. These may be accessed with the command line options -mat_coloring sl, id, or lf Alternatively, one can set a coloring type of COLORING SL, COLORING ID, or COLORING LF when calling MatGetColoring ().
Reference: [13] <author> Jorge J. More and David Thuente. </author> <title> Line search algorithms with guaranteed sufficient decrease. </title> <type> Technical Report MCS-P330-1092, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: The method SNES UM NLS (-snes type umls) provides a line search Newton method for solving unconstrained minimization problems. The default line search algorithm is taken from More and Thuente <ref> [13] </ref>. Again, the user can set a variety of parameters to control the line search; one should run a SNES program with the option -help for details.
Reference: [14] <author> MPI: </author> <title> A message-passing interface standard. </title> <journal> International J. Supercomputing Applications, </journal> <volume> 8(3/4), </volume> <year> 1994. </year>
Reference-contexts: Then the command ":tag FunctionName" will cause VI to find the file and line number where a desired PETSc function is defined. 12.8 Parallel Communication When used in a message-passing environment, all communication within PETSc is done through MPI, the Message Passing Interface standard <ref> [14] </ref>.
Reference: [15] <author> Youcef Saad and Martin H. Schultz. </author> <title> GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: All methods use left preconditioning by default. Options Default Database Convergence Method KSPType Name Monitor y Richardson KSPRICHARDSON richardson true Chebychev KSPCHEBYCHEV chebychev true Conjugate Gradient [10] KSPCG cg true Generalized Minimal Residual <ref> [15] </ref> KSPGMRES gmres precond BiCGSTAB [19] KSPBCGS bcgs precond Conjugate Gradient Squared [17] KSPCGS cgs precond Transpose-Free Quasi-Minimal Residual (1) [7] KSPTFQMR tfqmr precond Transpose-Free Quasi-Minimal Residual (2) KSPTCQMR tcqmr precond Conjugate Residual KSPCR cr precond Least Squares Method KSPLSQR lsqr precond Shell for no KSP method KSPPREONLY preonly precond y
Reference: [16] <author> Barry F. Smith, Petter Bjorstad, and William Gropp. </author> <title> Domain Decomposition: Parallel Multilevel Methods for Elliptic Partial Differential Equations. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year> <month> 182 </month>
Reference-contexts: The command ierr = MGSetType (PC pc,MGType mode); indicates which form of multigrid to apply <ref> [16] </ref>. For standard V or W-cycle multigrids, one sets the mode to be MGMULTIPLICATIVE; for the additive form (which in certain cases reduces to the BPX method, or additive multilevel Schwarz, or multilevel diagonal scaling), one uses MGADDITIVE as the mode.
Reference: [17] <author> Peter Sonneveld. </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: All methods use left preconditioning by default. Options Default Database Convergence Method KSPType Name Monitor y Richardson KSPRICHARDSON richardson true Chebychev KSPCHEBYCHEV chebychev true Conjugate Gradient [10] KSPCG cg true Generalized Minimal Residual [15] KSPGMRES gmres precond BiCGSTAB [19] KSPBCGS bcgs precond Conjugate Gradient Squared <ref> [17] </ref> KSPCGS cgs precond Transpose-Free Quasi-Minimal Residual (1) [7] KSPTFQMR tfqmr precond Transpose-Free Quasi-Minimal Residual (2) KSPTCQMR tcqmr precond Conjugate Residual KSPCR cr precond Least Squares Method KSPLSQR lsqr precond Shell for no KSP method KSPPREONLY preonly precond y true denotes true residual norm, precond denotes preconditioned residual norm 4.3.2 Convergence
Reference: [18] <author> Trond Steihaug. </author> <title> The conjugate gradient method and trust regions in large scale optimization. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 626-637, </pages> <year> 1983. </year>
Reference-contexts: The default trust region method for unconstrained minimization, SNES UM NTR (-snes type umtr), is based on the work of Steihaug <ref> [18] </ref>. This method uses the preconditioned conjugate gradient method via the KSP solver KSPQCG to determine the approximate minimizer of the resulting quadratic at each nonlinear iteration.

References-found: 18

