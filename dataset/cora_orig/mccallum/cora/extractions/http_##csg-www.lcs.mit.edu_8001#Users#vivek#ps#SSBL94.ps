URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/SSBL94.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Title: An Optimal Asynchronous Scheduling Algorithm for Software Cache Consistency  
Author: Barbara Simons Vivek Sarkar Mauricio Breternitz, Jr. Michael Lai 
Abstract: We present a linear time algorithm for scheduling iterations of a loop that has no loop-carried dependences. The algorithm is optimal in the sense that any p consecutive iterations in the schedule can be executed simultaneously without any possibility of false sharing, where p is the number of processors, and the algorithm uses at most two wait synchronizations per iteration. Our algorithm is asynchronous in the sense that it allows the loop iterations to be scheduled greedily so that no processor will ever be kept idle if it could be safely executing an iteration. This property makes our algorithm superior to a "barrier" type algorithm, in which barrier synchronizations are used instead of pairwise signal-wait synchronizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> January 1993. IEEE Computer, "New Products" section, </institution> <note> page 105. </note>
Reference-contexts: algorithm, Section 5 discusses related work, and Section 6 contains our conclusions and an outline of future work. 2 Program and Architecture Models 2.1 Processor Architecture The target architecture for this study is a symmetric multiprocessor with local and global memories, as in the RP3 [5], Cedar [14], and POWER/4 <ref> [1] </ref> systems. The physical address space on each processor is divided in two sections, corresponding to the global (shared) and local memories. Each processor has equal access to global memory, via a non-blocking network. This organization is illustrated in Figure 1. Each processor has separate data and instruction caches.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubi-atowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Santa Teresa Laboratory, 555 Bailey Avenue, San Jose, California 95141. y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa Teresa Laboratory). with extensions for directory-based systems <ref> [15, 2] </ref>, and diff-based protocols for the False Sharing problem [9, 6, 18]. These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution.
Reference: [3] <author> A. Aiken and A. Nicolau. </author> <title> Optimal Loop Paral-lelization. </title> <institution> Dept. of Computer Science - Cornell Tech. </institution> <type> Report TR 88-905, </type> <month> March </month> <year> 1988". </year>
Reference-contexts: An algorithm to perform optimal software pipelin-ing, directed at dealing inter-iteration data dependencies for VLIW and superscalar architectures is presented in <ref> [3] </ref>. It works by performing repeated loop unrolling and compaction until a stable pipelining pattern emerges. Another software pipelining algorithm described in [12] attempts to overlap execution of loop iterations that have multiple inter-iteration dependen cies and distinct flow of control.
Reference: [4] <author> D. Bernstein and I. Gertner. </author> <title> Scheduling Expressions on a Pipelined Processor with a Maximal Delay of One Cycle. </title> <journal> TOPLAS, </journal> <volume> 11(1) </volume> <pages> 57-66, </pages> <month> Jan-uary </month> <year> 1989. </year>
Reference-contexts: In such cases the flush, prefetch, compute instructions can be further optimized by the use of instruction scheduling algorithms such as those given in <ref> [4] </ref> and [21]. <p> Resource utilization are addressed in ad-hoc fashion by using heuristic estimates of resource utilization, without concerns of optimality. Algorithms for instruction scheduling with pipelines or latencies are given in <ref> [4] </ref> and [21]. Surveys of pipeline scheduling can be found in [19] and [22]. Good solution to false sharing and stale data problems also provide performance boost for cache-coherent multiprocessors.
Reference: [5] <author> W. C. Brantley, K. P. McAuliffe, and J. Weiss. </author> <title> RP3 Processor-Memory Element. </title> <booktitle> Proc. 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 782-789, </pages> <year> 1985. </year>
Reference-contexts: issues involving implementation of the algorithm, Section 5 discusses related work, and Section 6 contains our conclusions and an outline of future work. 2 Program and Architecture Models 2.1 Processor Architecture The target architecture for this study is a symmetric multiprocessor with local and global memories, as in the RP3 <ref> [5] </ref>, Cedar [14], and POWER/4 [1] systems. The physical address space on each processor is divided in two sections, corresponding to the global (shared) and local memories. Each processor has equal access to global memory, via a non-blocking network. This organization is illustrated in Figure 1.
Reference: [6] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa Teresa Laboratory). with extensions for directory-based systems [15, 2], and diff-based protocols for the False Sharing problem <ref> [9, 6, 18] </ref>. These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem [26, 10, 11].
Reference: [7] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multilevel Caches. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> De-cember </month> <year> 1978. </year>
Reference-contexts: A mechanism is required to ensure that the processor obtains a current copy of the data block, in stead of accessing the old copy. There have been several hardware and software solutions proposed for the Stale Data and False Sharing problems, e.g. the invalidate-on-write and update-on-write protocols <ref> [7] </ref> for the Stale Data problem fl IBM Santa Teresa Laboratory, 555 Bailey Avenue, San Jose, California 95141. y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa
Reference: [8] <author> Joichi Cheong and A. Veidenbaum. </author> <title> Performance Evaluation of a Multiprocessor Cache Coherence Algorithm. </title> <type> Technical report, </type> <institution> U.of I., </institution> <note> Novem-ber 1986. CSRD Report No. 621-Technical Report submitted for publication to the 14th International Symposium on Computer Architecture, 6/2,5/87-Pittsburgh, PA. </note>
Reference-contexts: For architectures that allow both write-back and write-through caching, the compiler is required to identify those regions of shared memory that must be accessed in write-through mode. This is the approach adopted in <ref> [8] </ref>. Furthermore, shared data is invalidated from the cache immediately after the fork of a parallel region. The work of [10] allows write-back caches. It seems to be directed mostly at scalar variables and attempts to optimize invalidations by placing those immediately prior to read accesses.
Reference: [9] <author> Myrias Corporation. </author> <title> System Overview. </title> <address> Edmon-ton, Alberta, </address> <year> 1990. </year>
Reference-contexts: y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa Teresa Laboratory). with extensions for directory-based systems [15, 2], and diff-based protocols for the False Sharing problem <ref> [9, 6, 18] </ref>. These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem [26, 10, 11].
Reference: [10] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches (Extended Abstract). </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988. </year> <note> Also available as CSRD Rpt. No. 728 from U. </note> <institution> of Ill.-Center for Supercomputing Research and Development. </institution>
Reference-contexts: These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem <ref> [26, 10, 11] </ref>. Hardware solutions to the False Sharing problem enforce mutual exclusion on all parallel writes to the same cache line. <p> This is the approach adopted in [8]. Furthermore, shared data is invalidated from the cache immediately after the fork of a parallel region. The work of <ref> [10] </ref> allows write-back caches. It seems to be directed mostly at scalar variables and attempts to optimize invalidations by placing those immediately prior to read accesses. Particularly noteworthy is the work of [11], which extends vectorization algorithms.
Reference: [11] <author> Ervan Darnell, John M. Mellor-Crummey, and Ken Kennedy. </author> <title> Automatic Software Cache Coherence through Vectorization. </title> <booktitle> In Proceedings of the ACM 1992 International Conference on Supercomputing, </booktitle> <pages> pages 129-138, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem <ref> [26, 10, 11] </ref>. Hardware solutions to the False Sharing problem enforce mutual exclusion on all parallel writes to the same cache line. <p> The work of [10] allows write-back caches. It seems to be directed mostly at scalar variables and attempts to optimize invalidations by placing those immediately prior to read accesses. Particularly noteworthy is the work of <ref> [11] </ref>, which extends vectorization algorithms. In a nutshell, their algorithm works by placing cache flush and invalidate instructions, and using vectorization analysis to combine such operations to handle data in "vectors" of whole cache lines at the time.
Reference: [12] <author> K. Ebcioglu and T. Nakatani. </author> <title> A New Compilation Technique for Parallelizing Loops with Unpredictable Branches on a VLIW Architecture. </title> <booktitle> In 2nd Workshop on Parallel Compilation, </booktitle> <address> Urbana, Illinois, </address> <year> 1989. </year>
Reference-contexts: An algorithm to perform optimal software pipelin-ing, directed at dealing inter-iteration data dependencies for VLIW and superscalar architectures is presented in [3]. It works by performing repeated loop unrolling and compaction until a stable pipelining pattern emerges. Another software pipelining algorithm described in <ref> [12] </ref> attempts to overlap execution of loop iterations that have multiple inter-iteration dependen cies and distinct flow of control. Resource utilization are addressed in ad-hoc fashion by using heuristic estimates of resource utilization, without concerns of optimality.
Reference: [13] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness. </title> <booktitle> Lecture Notes in Computer Science, (589), 1991. Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, California, USA, </address> <month> August </month> <year> 1991. </year> <title> Edited by U. </title> <editor> Banerjee, D. Gelernter, A. Nicolau, D. </editor> <address> Padua. </address>
Reference-contexts: Efficient cache management is an important task of an optimizing compiler. Many different techniques have been proposed to help manage cache usage efficiently, some of the important ones being loop interchange, loop strip-mining, loop blocking (or tiling), and data prefetching <ref> [13, 23, 27] </ref>. We observe that cache line flushing/invalidation can both benefit from and enhance the applicability of these other optimization techniques.
Reference: [14] <author> D. Gajski, D. Kuck, D. Lawrie, and A. Sameh. </author> <title> CEDAR A Large Scale Multiprocessor. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: implementation of the algorithm, Section 5 discusses related work, and Section 6 contains our conclusions and an outline of future work. 2 Program and Architecture Models 2.1 Processor Architecture The target architecture for this study is a symmetric multiprocessor with local and global memories, as in the RP3 [5], Cedar <ref> [14] </ref>, and POWER/4 [1] systems. The physical address space on each processor is divided in two sections, corresponding to the global (shared) and local memories. Each processor has equal access to global memory, via a non-blocking network. This organization is illustrated in Figure 1.
Reference: [15] <author> K. Gharachorloo, D. Lenoski, J. Lanudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Santa Teresa Laboratory, 555 Bailey Avenue, San Jose, California 95141. y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa Teresa Laboratory). with extensions for directory-based systems <ref> [15, 2] </ref>, and diff-based protocols for the False Sharing problem [9, 6, 18]. These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. <p> Similarly, the other processors do not access shared data during execution of a serial construct by the master processor. Therefore, execution of parallel fork-join constructs may adopt a protocol similar to release consistency <ref> [15] </ref> for the global memory. Release consistency is a relaxed memory consistency protocol, introduced in order to reduce the impact of memory access latency in distributed shared memory systems and achieve higher performance. According to this protocol, memory is required to be consistent only at specific synchronization points. Parallel loops.
Reference: [16] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Shared-memory multiprocessor systems have the potential to deliver usable and scalable parallel computing. Perhaps the biggest obstacle to realizing this potential is the cache consistency problem <ref> [16] </ref>, the problem of ensuring that processor caches and global memory have a consistent view of shared data at synchronization points. For a shared-memory multiprocessor system, the cache consistency problem really consists of two orthogonal subproblems: 1. <p> The O.S. ensures that only one processor has ownership of a page at a given time. 4. Implemented in hardware: This is the most common approach with symmetric multiprocessors. Hardware mechanisms for cache consistency include snooping and directory protocols <ref> [16] </ref>. The first approach has the potential to yield the greatest performance, at a very high cost in program development and debugging time.
Reference: [17] <author> IBM. </author> <title> AIX Version 3.3 for RISC System/6000: Assembler Language Reference. </title> <type> Technical report, </type> <institution> International Business Machines, </institution> <month> January </month> <year> 1992. </year> <note> Pub. No. SC23-2197-01. </note>
Reference-contexts: Caches are managed in a write-back policy. Hits are serviced locally. On a cache miss, the processor fetches the entire cache line from memory. The processor is equipped with a number of cache control instructions, including &lt;flush cache line at address&gt; <ref> [17] </ref>, which writes the contents of a cache line back to memory, if modified, and invalidates the cache line. This is a user-level instruction. Of particular notice is the absence of a provision for non-cacheable data. In some systems, this feature is used to avoid caching of shared data. <p> We observe that cache line flushing/invalidation can both benefit from and enhance the applicability of these other optimization techniques. Many processors such as the RS/6000 <ref> [17] </ref> have an instruction of the form &lt;flush cache line at address&gt;, which causes the data residing on the cache line containing address to be stored back into memory.
Reference: [18] <author> Alan H. Karp and Vivek Sarkar. </author> <title> Data Merging for Shared-Memory Multiprocessors. </title> <booktitle> Proc. Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1993. </year> <note> (to appear). Also available as Technical Report 03.454, </note> <institution> Santa Teresa Laboratory, IBM Corporation, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: y IBM Advanced Workstations Division, 11400 Burnet Road, Austin, Texas 78758. z SunPro, M/S MTV12-40, 2550 Garcia Avenue, Mountain View, California 94043. (This work was done while the author was at IBM Santa Teresa Laboratory). with extensions for directory-based systems [15, 2], and diff-based protocols for the False Sharing problem <ref> [9, 6, 18] </ref>. These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem [26, 10, 11].
Reference: [19] <author> Eugene Lawler, Jan Karel Lenstra, Charles Mar-tel, Barbara Simons, and Larry Stockmeyer. </author> <title> Pipeline Scheduling: </title> <type> A Survey . Technical report, </type> <institution> IBM Research Division, </institution> <month> August </month> <year> 1987. </year> <type> Technical Report RJ 5738. </type>
Reference-contexts: Specifically, the problem of optimally scheduling a collection of prefetch, compute, and flush instructions becomes a variable length pipeline scheduling problem with assigned processors <ref> [22, 19] </ref>. 5 Related Work Most of the previous approaches for solving the problems of stale data and false sharing are hardware-based, as exemplified by snoopy cache and directory schemes. As already mentioned, the overhead of maintaining consistency in hardware can be high, both in execution time and system cost. <p> Resource utilization are addressed in ad-hoc fashion by using heuristic estimates of resource utilization, without concerns of optimality. Algorithms for instruction scheduling with pipelines or latencies are given in [4] and [21]. Surveys of pipeline scheduling can be found in <ref> [19] </ref> and [22]. Good solution to false sharing and stale data problems also provide performance boost for cache-coherent multiprocessors.
Reference: [20] <author> Jr. Mauricio Breternitz, Michael Lai, Vivek Sarkar, and Barbara Simons. </author> <title> Compiler Solutions for the Stale-Data and False-Sharing problems. </title> <type> Technical report, </type> <institution> IBM TR 03.466, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: If two iterations in a subschedule of length p or less are within d units of each other, we say that they are in conflict. There is no 3-safe schedule for less than 6 = 3 fl 2 iterations. All proofs have been omitted because of space constraints; <ref> [20] </ref> contains the proofs. Lemma 3.1 Let n be the number of iterations of L, and assume d = 2. If n &lt; 2p, there is no p-safe schedule for L.
Reference: [21] <author> Krishna Palem and Barbara Simons. </author> <title> Scheduling Time-Critical Instructions on RISC Machines. </title> <booktitle> Proceedings of the ACM 17th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 270-280, </pages> <month> Jan </month> <year> 1990. </year> <note> To appear in TOPLAS, </note> <month> Sept </month> <year> 1993. </year>
Reference-contexts: In such cases the flush, prefetch, compute instructions can be further optimized by the use of instruction scheduling algorithms such as those given in [4] and <ref> [21] </ref>. <p> Resource utilization are addressed in ad-hoc fashion by using heuristic estimates of resource utilization, without concerns of optimality. Algorithms for instruction scheduling with pipelines or latencies are given in [4] and <ref> [21] </ref>. Surveys of pipeline scheduling can be found in [19] and [22]. Good solution to false sharing and stale data problems also provide performance boost for cache-coherent multiprocessors.
Reference: [22] <author> Krishna Palem and Barbara Simons. </author> <title> Instruction Scheduling for Compilers . Technical report, </title> <institution> IBM Research Division, </institution> <month> December </month> <year> 1991. </year> <type> Technical Report RJ 8535. </type>
Reference-contexts: Specifically, the problem of optimally scheduling a collection of prefetch, compute, and flush instructions becomes a variable length pipeline scheduling problem with assigned processors <ref> [22, 19] </ref>. 5 Related Work Most of the previous approaches for solving the problems of stale data and false sharing are hardware-based, as exemplified by snoopy cache and directory schemes. As already mentioned, the overhead of maintaining consistency in hardware can be high, both in execution time and system cost. <p> Resource utilization are addressed in ad-hoc fashion by using heuristic estimates of resource utilization, without concerns of optimality. Algorithms for instruction scheduling with pipelines or latencies are given in [4] and [21]. Surveys of pipeline scheduling can be found in [19] and <ref> [22] </ref>. Good solution to false sharing and stale data problems also provide performance boost for cache-coherent multiprocessors.
Reference: [23] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <institution> Rice COMP TR89-93. </institution>
Reference-contexts: Efficient cache management is an important task of an optimizing compiler. Many different techniques have been proposed to help manage cache usage efficiently, some of the important ones being loop interchange, loop strip-mining, loop blocking (or tiling), and data prefetching <ref> [13, 23, 27] </ref>. We observe that cache line flushing/invalidation can both benefit from and enhance the applicability of these other optimization techniques.
Reference: [24] <author> A. C. Shaw. </author> <title> The Logical Design of Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: The key observation is that global memory is required to be consistent only at two points in the execution of a parallel construct: immediately prior to start of the execution of a parallel section (fork), and right at the end of the parallel construct (join) <ref> [24] </ref>. During the execution of a parallel construct, there is no need to maintain a consistent view of global memory for all processors.
Reference: [25] <author> J. Torrellas, A Gupta, and J Hennessy. </author> <title> Characterizing the caching and synchronization performance of a multiprocessor operating system. </title> <booktitle> Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 162-74, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Algorithms for instruction scheduling with pipelines or latencies are given in [4] and [21]. Surveys of pipeline scheduling can be found in [19] and [22]. Good solution to false sharing and stale data problems also provide performance boost for cache-coherent multiprocessors. This claim is confirmed in <ref> [25] </ref>, which improved performance by reducing the amount of cache misses due to false sharing for a number of applications. 6 Conclusions and Future Work We have analyzed the false sharing and stale data problems, and presented compiler-based solutions.
Reference: [26] <author> Alex Veidenbaum. </author> <title> A Compiler-Assisted Cache Coherence Solution for Multiprocessors. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pages 1029-1036, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: These protocols provide automatic support for cache consistency without requiring assistance from the programmer or compiler for correct parallel execution. There have also been some compiler-assisted solutions proposed in past work for the cache consistency problem <ref> [26, 10, 11] </ref>. Hardware solutions to the False Sharing problem enforce mutual exclusion on all parallel writes to the same cache line.
Reference: [27] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimization Algorithm. </title> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Efficient cache management is an important task of an optimizing compiler. Many different techniques have been proposed to help manage cache usage efficiently, some of the important ones being loop interchange, loop strip-mining, loop blocking (or tiling), and data prefetching <ref> [13, 23, 27] </ref>. We observe that cache line flushing/invalidation can both benefit from and enhance the applicability of these other optimization techniques.
Reference: [28] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, London and The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <booktitle> In the series, Research Monographs in Parallel and Distributed Computing. </booktitle>
Reference-contexts: According to this protocol, memory is required to be consistent only at specific synchronization points. Parallel loops. Loops that have no loop-carried dependences among iterations are an obvious source of parallelism <ref> [28] </ref>. If there is no false sharing, the iterations can be executed in parallel on separate processors.
References-found: 28

