URL: http://www.bath.ac.uk/Departments/Eng/wsc2/papers/kovacs.ps.gz
Refering-URL: http://www.bath.ac.uk/Departments/Eng/wsc2/ind_paper/p_kovacs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: T.Kovacs@cs.bham.ac.uk  
Phone: Phone: +44 121 414 3736 Fax: +44 121 414  
Title: XCS Classifier System Reliably Evolves Accurate, Complete, and Minimal Representations for Boolean Functions more complex
Author: Tim Kovacs 
Keyword: XCS, Classifier Systems, Generalization Problem, Structural Credit Assignment Problem, Reinforcement Learning.  
Note: A  is also presented and compared to condensation.  
Address: Birmingham U.K. B15 2TT  4281  
Affiliation: School of Computer Science, University of Birmingham  
Abstract: Wilson's recent XCS classifier system forms complete mappings of the payoff environment in the reinforcement learning tradition thanks to its accuracy based fitness. According to Wilson's Generalization Hypothesis, XCS has a tendency towards generalization. With the XCS Optimality Hypothesis, I suggest that XCS systems can evolve optimal populations (representations); populations which accurately map all input/action pairs to payoff predictions using the smallest possible set of non-overlapping classifiers. The ability of XCS to evolve optimal populations for boolean multiplexer problems is demonstrated using condensation, a technique in which evolutionary search is suspended by setting the crossover and mutation rates to zero. Condensation is automatically triggered by self-monitoring of performance statistics, and the entire learning process is terminated by autotermination. Combined, these techniques allow a classifier system to evolve optimal representations of boolean functions without any form of supervision. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Watkins, C., </author> <year> 1989, </year> <title> Learning from Delayed Rewards, </title> <type> PhD Thesis, </type> <institution> Cambridge University, </institution> <address> United Kingdom. </address>
Reference-contexts: A complete map is one which has an estimated payoff for each input/action pair. Many approaches to learning such mappings, and to producing accurate generalizations within them have been used by reinforcement learning systems. For example, the popular tabular Q Learning technique <ref> [1] </ref> exhaustively enumerates condition/action pairs and maintains a payoff estimate for each. As a result it suffers from poor scalability, although modifications to allow generalization have been introduced (see for example [2, 3]). <p> Wilson showed that learning in ZCS has strong similarities to the reinforcement learning technique Q Learning <ref> [1] </ref>. 3 * [M] The match set. This is the set of classifiers which match the current input to the system. * [A] The action set. XCS forms a match set, then selects an action from among those advocated by the classifiers in the match set.
Reference: [2] <author> Lin, L. J., </author> <year> 1992, </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference-contexts: For example, the popular tabular Q Learning technique [1] exhaustively enumerates condition/action pairs and maintains a payoff estimate for each. As a result it suffers from poor scalability, although modifications to allow generalization have been introduced (see for example <ref> [2, 3] </ref>). Classifier systems (CS) are rule based learning systems which are able to generalize over their inputs, and thus have the potential to scale well, but they have not traditionally constructed complete payoff maps.
Reference: [3] <author> Munos, R., and Patinel, J., </author> <year> 1994, </year> <title> Reinforcement learning with dynamic covering of state-action space: Partitioning Q-Learning, </title> <booktitle> in From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94), </booktitle> <pages> pp. 354-363. </pages>
Reference-contexts: For example, the popular tabular Q Learning technique [1] exhaustively enumerates condition/action pairs and maintains a payoff estimate for each. As a result it suffers from poor scalability, although modifications to allow generalization have been introduced (see for example <ref> [2, 3] </ref>). Classifier systems (CS) are rule based learning systems which are able to generalize over their inputs, and thus have the potential to scale well, but they have not traditionally constructed complete payoff maps.
Reference: [4] <author> Wilson, S. W., </author> <year> 1995, </year> <title> Classifier Fitness Based on Accuracy, </title> <booktitle> Evolutionary Computation, </booktitle> <pages> 3. </pages>
Reference-contexts: Classifier systems (CS) are rule based learning systems which are able to generalize over their inputs, and thus have the potential to scale well, but they have not traditionally constructed complete payoff maps. The current work is based on Wilson's XCS system <ref> [4, 5] </ref>, a new type of classifier system which, 1 Or, using alternative terminology, a stimulus/response/reward interaction. 1 unlike traditional CS, does construct complete payoff maps thanks to its shift to accuracy based fitness. According to Wilson's XCS Generalization Hypothesis, XCS has a natural tendency towards accurate generalization. <p> XCS is a type of classifier system introduced in <ref> [4] </ref> and extended in [5] (additional detail and analysis of the system are available in [8]). <p> As a result XCS tends to form "complete and accurate mappings X fi A ) P from inputs and actions to payoff predictions . . . which can make payoff-maximising action-selection straightforward" <ref> [4] </ref>. <p> I have briefly evaluated macroclassifiers in [8]. 1.2.3 Deletion Techniques <ref> [4] </ref> describes two techniques for calculating the probability of a classifier being selected for deletion in the GA. I have compared these techniques and found that although the second technique results in smaller population sizes, it is highly detrimental to the development of members of the optimal population. <p> I will refer herein to this modified form of the second technique as deletion technique 3. 1.2.4 Terminology and Notation The following terminology and notation was drawn from <ref> [4] </ref> whenever possible. * Classifiers are written: &lt;condition&gt;:&lt;action&gt; ) &lt;payoff prediction&gt; E.g. 111### : 0 ) 100 would be interpreted as: if the input string begins with 111, then action 0 should be taken, and a payoff of 100 units will be expected. 4 ZCS is intended as a minimalist classifier <p> 1.0 &lt; high &gt; C 1 0 1 100 0.0 1.0 &lt; high &gt; D 1 1 1 100 0.0 1.0 &lt; high &gt; E ## 0 0 0.0 1.0 &lt; high &gt; (The accuracy of A is 0.0 because its prediction error exceeds a threshold called the accuracy criterion <ref> [4, 8] </ref>.) We can describe each classifier as one of the following: * Overgeneral An overgeneral classifier matches too many inputs, specifically, some of the condition/action pairs it refers to pay off at different rates. <p> Because these two types of maximally general classifier are equally general and accurate, the generalization mechanism must distinguish the optimal population by considering the solution as a combination of interacting classifiers, not just a group of independent classifiers. 5 " represents the prediction error parameter of a classifier. 6 See <ref> [4] </ref> or [8] for a longer introduction to the multiplexer function. 5 2 Obtaining Optimal Populations by Condensation Wilson [4] discusses the use of condensation to reduce the size of the population once the system appears to have learned a given problem. <p> optimal population by considering the solution as a combination of interacting classifiers, not just a group of independent classifiers. 5 " represents the prediction error parameter of a classifier. 6 See <ref> [4] </ref> or [8] for a longer introduction to the multiplexer function. 5 2 Obtaining Optimal Populations by Condensation Wilson [4] discusses the use of condensation to reduce the size of the population once the system appears to have learned a given problem. Condensation consists of running the system with the mutation and crossover rates set to zero. <p> Once a classifier reaches zero numerosity it is removed from the system. Over several thousand cycles the result is a condensation of the population to its fittest members. Although condensation was applied to an XCS system in <ref> [4] </ref>, optimal populations were not obtained as condensation was applied too soon. Because no new classifier conditions are generated once condensation begins, the optimal population must already exist within the general population at this point.
Reference: [5] <author> Wilson, S. W., </author> <year> 1996, </year> <title> Generalization in XCS, Unpublished contribution to ICML '96 Workshop on Evolutionary Computing and Machine Learning. </title>
Reference-contexts: Classifier systems (CS) are rule based learning systems which are able to generalize over their inputs, and thus have the potential to scale well, but they have not traditionally constructed complete payoff maps. The current work is based on Wilson's XCS system <ref> [4, 5] </ref>, a new type of classifier system which, 1 Or, using alternative terminology, a stimulus/response/reward interaction. 1 unlike traditional CS, does construct complete payoff maps thanks to its shift to accuracy based fitness. According to Wilson's XCS Generalization Hypothesis, XCS has a natural tendency towards accurate generalization. <p> XCS is a type of classifier system introduced in [4] and extended in <ref> [5] </ref> (additional detail and analysis of the system are available in [8]).
Reference: [6] <author> Holland, J., H., </author> <year> 1975, </year> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor. </address>
Reference-contexts: At the same time, an optimal population is less diverse and may, depending on the problem, make a less useful base for further genetic search. 1.2 Overview of XCS Classifier systems are a form of domain independent rule-based machine learning system introduced by John Holland (see <ref> [6, 7] </ref>). Classifier systems use a Genetic Algorithm (GA) to generate condition/action rules or classifiers which are evaluated during interaction with the problem environment. Classifiers typically use strings of characters composed from the ternary alphabet f0, 1, #g to represent conditions and actions.
Reference: [7] <author> Holland, J., H., </author> <year> 1986, </year> <title> Machine Learning, an Artificial Intelligence Approach. Volume II, chapter Escaping Brittleness: The possibilities of General-Purpose Learning Algorithms Applied to Parallel Rule-Based Systems, </title> <journal> pp. </journal> <pages> 593-623, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: At the same time, an optimal population is less diverse and may, depending on the problem, make a less useful base for further genetic search. 1.2 Overview of XCS Classifier systems are a form of domain independent rule-based machine learning system introduced by John Holland (see <ref> [6, 7] </ref>). Classifier systems use a Genetic Algorithm (GA) to generate condition/action rules or classifiers which are evaluated during interaction with the problem environment. Classifiers typically use strings of characters composed from the ternary alphabet f0, 1, #g to represent conditions and actions.
Reference: [8] <author> Kovacs, T., </author> <year> 1996, </year> <title> Evolving Optimal Populations with XCS Classifier Systems, </title> <institution> Technical Report CSR-96-17 and CSRP-96-17, School of Computer Science, University of Birmingham, </institution> <address> United Kingdom, </address> <note> available from http://www/system/tech-reports/tr.html </note>
Reference-contexts: XCS is a type of classifier system introduced in [4] and extended in [5] (additional detail and analysis of the system are available in <ref> [8] </ref>). <p> XCS has many features in common with Wilson's earlier ZCS work [9]. 4 For an extended review and analysis of XCS the reader is referred to <ref> [8] </ref>, which addresses many of the subjects of this paper in more detail, and includes overviews of classifier systems, reinforcement learning, payoff environments, multiplexer problems, representational difficulties with classifier systems and other subjects. 1.2.1 Accuracy-Based Fitness In traditional CS, classifier strength plays a double role: it is used as a predictor <p> I have briefly evaluated macroclassifiers in <ref> [8] </ref>. 1.2.3 Deletion Techniques [4] describes two techniques for calculating the probability of a classifier being selected for deletion in the GA. <p> 1.0 &lt; high &gt; C 1 0 1 100 0.0 1.0 &lt; high &gt; D 1 1 1 100 0.0 1.0 &lt; high &gt; E ## 0 0 0.0 1.0 &lt; high &gt; (The accuracy of A is 0.0 because its prediction error exceeds a threshold called the accuracy criterion <ref> [4, 8] </ref>.) We can describe each classifier as one of the following: * Overgeneral An overgeneral classifier matches too many inputs, specifically, some of the condition/action pairs it refers to pay off at different rates. <p> two types of maximally general classifier are equally general and accurate, the generalization mechanism must distinguish the optimal population by considering the solution as a combination of interacting classifiers, not just a group of independent classifiers. 5 " represents the prediction error parameter of a classifier. 6 See [4] or <ref> [8] </ref> for a longer introduction to the multiplexer function. 5 2 Obtaining Optimal Populations by Condensation Wilson [4] discusses the use of condensation to reduce the size of the population once the system appears to have learned a given problem. <p> I investigated the use of several statistics for estimating the completion of [O] in <ref> [8] </ref> but none was very accurate, and all involved the use of a user-selected delay between the trigger condition being met and the commencement of condensation. <p> E.g. 000000 : 0 ) 100 and 000001 : 0 ) 100 can be combined into (and replaced by) 000000# : 0 ) 100. This is not always possible due to limitations in the representational capacity of the ternary alphabet currently used in XCS classifier conditions (see [10] and <ref> [8] </ref> section 5.3.
Reference: [9] <author> Wilson, S. W., </author> <year> 1994, </year> <title> ZCS: A zeroth level classifier system, </title> <booktitle> Evolutionary Computation, </booktitle> <pages> 2. </pages>
Reference-contexts: XCS has many features in common with Wilson's earlier ZCS work <ref> [9] </ref>. 4 For an extended review and analysis of XCS the reader is referred to [8], which addresses many of the subjects of this paper in more detail, and includes overviews of classifier systems, reinforcement learning, payoff environments, multiplexer problems, representational difficulties with classifier systems and other subjects. 1.2.1 Accuracy-Based Fitness
Reference: [10] <author> Schuurmans, D., and Schaeffer, J., </author> <year> 1989, </year> <title> Representational Difficulties with Classifier Systems, </title> <booktitle> in Proceedings Third International Conference on Genetic Algorithms, </booktitle> <pages> pp. </pages> <month> 328-333. </month> <title> 9 Although condensation might scale much better if a more accurate, problem-independent means of estimating the appropriate time to begin condensation could be found. </title> <type> 10 </type>
Reference-contexts: E.g. 000000 : 0 ) 100 and 000001 : 0 ) 100 can be combined into (and replaced by) 000000# : 0 ) 100. This is not always possible due to limitations in the representational capacity of the ternary alphabet currently used in XCS classifier conditions (see <ref> [10] </ref> and [8] section 5.3.
References-found: 10

