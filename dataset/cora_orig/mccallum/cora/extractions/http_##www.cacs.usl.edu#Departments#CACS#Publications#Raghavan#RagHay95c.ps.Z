URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/RagHay95c.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Email: e-mail: deogun@cse.unl.edu  e-mail: raghavan@cacs.usl.edu e-mail: hsr@swamp.cacs.usl.edu  
Title: Exploiting Upper Approximation in the Rough Set Methodology  
Author: Jitender S. Deogun Vijay V. Raghavan and Hayri Sever 
Keyword: Rough sets, feature selection, adaptive classifier.  
Address: Lincoln, NE 68588, USA  Lafayette, LA 70504, USA  
Affiliation: The Department of Computer Science University of Nebraska  The Center for Advanced Computer Studies University of Southwestern Louisiana  
Abstract: In this paper, we investigate enhancements to an upper classifier a decision algorithm generated by an upper classification method, which is one of the classification methods in rough set theory. Specifically, we consider two enhancements. First, we present a stepwise backward feature selection algorithm to preprocess a given set of features. This is important because rough classification methods are incapable of removing superfluous features. We prove that the stepwise backward selection algorithm finds a small subset of relevant features that are ideally sufficient and necessary to define target concepts with respect to a given threshold. This threshold value indicates an acceptable degradation in the quality of an upper classifier. Second, to make an upper classifier adaptive, we associate it with some kind of frequency information, which we call incremental information. An extended decision table is used to represent an adaptive upper classifier. It is also used for interpreting an upper classifier either deterministically or nondeterministically. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bollmann, P., and Cherniavsky, V. S. </author> <year> 1981. </year> <title> Measurement-theoretical investigation of the mz-metric. </title> <editor> In Oddy, R. N.; Robertson, S. E.; van Rus-bergen, C. J.; and Williams, R. W., eds., </editor> <booktitle> Information Retrieval Research. </booktitle> <address> Boston: </address> <publisher> Butterworths. </publisher> <pages> 256-267. </pages>
Reference-contexts: as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics (Devicver & Kittler 1982; Miller 1990); entropy or classification accuracy in pattern recognition and machine learning (Pal & Chakraborty 1986; Duda & Hart 1973; Fayyad & Irani 1992); classification quality, based on variations of MZ metric, in information retrieval systems <ref> (Bollmann & Cherniavsky 1981) </ref>. In such procedures, irrelevant features are either eliminated or assigned small coefficients. Instead of adapting near-optimal solutions for feature selection problem from other disciplines, we take advantage of the fact that the quality of an upper classifier worsens as the feature set is pruned down.
Reference: <author> Chang, C. Y. </author> <year> 1973. </year> <title> Dynamic programming as applied to feature subset selection in a pattern recognition system. </title> <journal> IEEE Trans. Syst., Man, Cybern. SMC-3:166-171. </journal>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and time complexity of J is low. Greedy approaches like stepwise backward/forward techniques (James 1985; Modrzejewski 1993), and dynamic programming <ref> (Chang 1973) </ref> are some of the efficient search techniques applied with some feature selection criterion.
Reference: <author> Deogun, J. S.; Raghavan, V. V.; and Sever, H. </author> <year> 1994. </year> <title> Rough set based classification methods and extended decision tables. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <pages> 302-309. </pages>
Reference: <author> Devicver, R. A., and Kittler, J. </author> <year> 1982. </year> <title> Pattern Recog-nation: A statistical approach. </title> <publisher> London: Prentice Hall. </publisher>
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1992. </year> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> 104-110. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> James, M. </author> <year> 1985. </year> <title> Classification Algorithms. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Kira, K., and Rendell, L. </author> <year> 1992. </year> <title> The feature selection problem: Tradational methods and a new algorithm. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> 129-134. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: The importance of feature selection in a broader sense is due to the potential it offers for speeding up the processes of both concept learning and object classification, reducing the cost of classification (e.g., eliminating redundant tests in medical diagnosis), and improving the quality of classification <ref> (Kira & Rendell 1992) </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J ); where: l is the number of features, and J is the computational effort required to evaluate each subset. <p> 1. 64.3% 78.4% 0,1 77.8% 3. 55.5% 100.0% 0,1,2,3,4 66.7% 5. 86.1% 100.0% 0,1,4 50.0% 7. 90.0% 93.5% 0,1,3,4 33.3% 9. 100% 65.3% 0,2,3 91.43% 11. 99.5% 99.7% 2,4,9,10,13 72.27% Table 3: Comparison of classification accuracies of UC and UC+SBS on real data sets. computed as suggested in Relief algorithm <ref> (Kira & Rendell 1992) </ref>; that is, the difference between two nonquantitative values is one if they are different and zero otherwise, and the difference between two quantitative values is normalized into the interval [0,1].
Reference: <author> Matheus, C. J.; Chan, P. K.; and Piatetsky-Shapiro, G. </author> <year> 1993. </year> <title> Systems for knowledge discovery in databases. </title> <journal> IEEE Trans. on Knowledge and Data Engineering 5(6) </journal> <pages> 903-912. </pages>
Reference-contexts: We use upper classifiers and decision tables to address some problematic aspects of handling very large, redundant, incomplete, noisy, and dynamic data <ref> (Matheus, Chan, & Piatetsky-Shapiro 1993) </ref>. The dynamic characteristic of the data requires to design an incremental method and separate the summary and the result of a method from one to another.
Reference: <author> Miller, A. J. </author> <year> 1990. </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Modrzejewski, M. </author> <year> 1993. </year> <title> Feature selection using rough sets theory. </title> <editor> In Brazdil, P. B., ed., </editor> <booktitle> Machine Learning: Proceedings of ECML-93. </booktitle> <publisher> Springer-Verlag. </publisher> <pages> 213-226. </pages>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <title> UCI repository of machine learning databases. For information contact m-lrepository@ics.uci.edu. </title>
Reference: <author> Narendra, P. M., and Fukunaga, K. </author> <year> 1977. </year> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computers c-26(9):917-922. </journal>
Reference: <author> Pal, S. K., and Chakraborty, B. </author> <year> 1986. </year> <title> Fuzzy set theoretic measure for automatic feature evaluation. </title> <journal> IEEE Trans. Syst., Man, Cybern. SMC-16(5):754-760. </journal>
Reference: <author> Pawlak, Z. </author> <year> 1986. </year> <title> On learning- a rough set approach. </title> <booktitle> In Lecture Notes, </booktitle> <volume> volume 208. </volume> <publisher> Springer Verlag. </publisher> <pages> 197-227. </pages>
Reference: <author> Raghavan, V. V., and Sever, H. </author> <year> 1995. </year> <title> The state of rough sets for database mining applications. </title> <editor> In Lin, T. Y., ed., </editor> <booktitle> Proceedings of 23rd Computer Science Conference Workshop on Rough Sets and Database Mining, </booktitle> <pages> 1-11. </pages>
Reference-contexts: The theory of rough sets in either algebraic or probabilistic approximation spaces has been used for a number of real life applications; namely, in medicine, phar macology, industry, engineering, control systems, so-cial sciences, switching circuits, image processing, etc., <ref> (Raghavan & Sever 1995) </ref>. In this article we consider classification methods only in algebraic approximation spaces, which do not require any preliminary or additional information about data as opposed to rough sets in probabilistic approximation spaces.
Reference: <author> Salzberg, S. </author> <year> 1992. </year> <title> Improving classification methods via feature selection. </title> <type> Technical Report JHU-92/12, </type> <institution> Johns Hopkins University, Department of Computer Science. </institution> <note> (revised April 1993). </note>
Reference-contexts: The threshold value indicates how much degradation one is willing to allow in the quality of an upper classifier. Controlled threshold value is inspired by Salzberg's CSS algorithm <ref> (Salzberg 1992) </ref>. Even though our feature selection algorithm is developed as a pre-processing stage for rough classifiers, it can certainly be integrated to any other data analysis technique.
Reference: <author> Sever, H. </author> <year> 1995. </year> <title> Knowledge Structuring for Database Mining and Text Retrieval Using Past Optimal Queries. </title> <type> Ph.D. Dissertation, </type> <institution> The University of Southwestern Louisiana. </institution>
Reference-contexts: The theory of rough sets in either algebraic or probabilistic approximation spaces has been used for a number of real life applications; namely, in medicine, phar macology, industry, engineering, control systems, so-cial sciences, switching circuits, image processing, etc., <ref> (Raghavan & Sever 1995) </ref>. In this article we consider classification methods only in algebraic approximation spaces, which do not require any preliminary or additional information about data as opposed to rough sets in probabilistic approximation spaces. <p> Let l be the size of CON , p be the size of U= g CON ; and m be the number of objects in U: The computational effort to evaluate the quality of an upper classifier in a given S is equal to O ' u (S) (lmp) <ref> (Sever 1995) </ref>. Then it is easy to see that the time complexity of SBS algorithm is bounded by O (l 2 O ' u (S) (lmp)); which is a polynomial time algorithm with the degree of three in l and of one in m and p.
Reference: <author> Slowinski, R., and Stefanowiski, J. </author> <year> 1995. </year> <title> Rough classification with valued closeness relation. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Knowledge Discovery. </booktitle>
Reference: <author> Ziarko, W. </author> <year> 1991. </year> <title> The discovery, analysis, and representation of data dependencies in databases. </title> <editor> In Piatetsky-Shapiro, G., and Frawley, W. J., eds., </editor> <booktitle> Knowledge Discovery in Databases. </booktitle> <address> Cambridge, MA: </address> <publisher> AAAI/MIT. </publisher>
Reference-contexts: The dynamic characteristic of the data requires to design an incremental method and separate the summary and the result of a method from one to another. For example, in the context of rough classification, the strength of a possible decision rule <ref> (Ziarko 1991) </ref> is a part of the summary of the decision algorithm.
References-found: 20

