URL: http://www.cs.princeton.edu/shrimp/Papers/ipps97ULC.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_9.html
Root-URL: http://www.cs.princeton.edu
Email: fsnd,yuqun,felteng@cs.princeton.edu  
Title: Reducing Waiting Costs in User-Level Communication  
Author: Stefanos N. Damianakis, Yuqun Chen, Edward W. Felten 
Address: Princeton, NJ 08544 USA  
Affiliation: Department of Computer Science Princeton University  
Abstract: This paper describes a mechanism for reducing the cost of waiting for messages in architectures that allow user-level communication libraries. We reduce waiting costs in two ways: by reducing the cost of servicing interrupts, and by carefully controlling when the system uses interrupts and when it uses polling. We have implemented our mechanism on the SHRIMP multi-computer and integrated it with our user-level sockets library. Experiments show that a hybrid spin-then-block strategy offers good performance in a wide variety of situations, and that speeding up the interrupt path significantly improves performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: 1. Introduction Many network interfaces can place incoming data directly in user memory <ref> [1, 7, 3, 2] </ref>. This capability enables the construction of very efficient network software since the network interface can deliver a burst of packets without any software intervention. On such an architecture, communication can be handled entirely in a user-level library. <p> This requires both a policy for when to poll and when to block, and a mechanism for efficient blocking. This paper considers the questions of which receive policy and which mechanism to use. We present an implementation on the prototype SHRIMP multicomputer <ref> [1, 7] </ref>, and the results of experiments using our user-level sockets library [5], for micro-benchmarks, larger benchmarks, and for a distributed file system. Our results show that a hybrid spin/block policy is best in a wide range of situations, and that reducing the interrupt service overhead significantly improves performance. 2. <p> Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn [3], and Telegraphos [12]. In addition, other architectures, such as Myrinet [2] and FLASH [10], are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer <ref> [1, 7] </ref> consists largely of off-the-shelf components. The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card [1]. <p> The SHRIMP Architecture The SHRIMP multicomputer [1, 7] consists largely of off-the-shelf components. The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card <ref> [1] </ref>. The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) [6]. This mechanism supports various message-passing packages and applications effectively, and delivers excellent performance [7]. The prototype system consists of four interconnected nodes. <p> The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours. Several experimental network interfaces allow a sending process to deposit data directly into the memory of a receiving process <ref> [1, 3, 7, 8, 12] </ref>. Though these systems differ in several important aspects, they all provide the basic functionality to support efficient user-level communication, so they all face the problem we are trying to solve.
Reference: [2] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: 1. Introduction Many network interfaces can place incoming data directly in user memory <ref> [1, 7, 3, 2] </ref>. This capability enables the construction of very efficient network software since the network interface can deliver a burst of packets without any software intervention. On such an architecture, communication can be handled entirely in a user-level library. <p> Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn [3], and Telegraphos [12]. In addition, other architectures, such as Myrinet <ref> [2] </ref> and FLASH [10], are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer [1, 7] consists largely of off-the-shelf components. The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card [1]. <p> Control information at the head of each message specifies the address of a user-level routine that is responsible for extracting the message from the network. This approach often requires servicing an interrupt for each message received. The Myrinet <ref> [2] </ref> network interface has a programmable network coprocessor which implements packet-switched message passing. The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours.
Reference: [3] <author> G. Buzzard, D. Jacobson, S. Marovich, and J. Wilkes. Hamlyn: </author> <title> A high-performance network interface with sender-based memory management. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> Aug. </month> <year> 1995. </year> <note> Also available as HP Laboratories technical report HPL-95-86, </note> <month> July, </month> <year> 1995. </year>
Reference-contexts: 1. Introduction Many network interfaces can place incoming data directly in user memory <ref> [1, 7, 3, 2] </ref>. This capability enables the construction of very efficient network software since the network interface can deliver a burst of packets without any software intervention. On such an architecture, communication can be handled entirely in a user-level library. <p> Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn <ref> [3] </ref>, and Telegraphos [12]. In addition, other architectures, such as Myrinet [2] and FLASH [10], are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer [1, 7] consists largely of off-the-shelf components. <p> The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours. Several experimental network interfaces allow a sending process to deposit data directly into the memory of a receiving process <ref> [1, 3, 7, 8, 12] </ref>. Though these systems differ in several important aspects, they all provide the basic functionality to support efficient user-level communication, so they all face the problem we are trying to solve.
Reference: [4] <author> S. Damianakis, Y. Chen, and E. W. Felten. </author> <title> Reducing waiting costs in user-level communication. </title> <type> Technical Report TR-525-96, </type> <institution> Dept. of Computer Science, Prince-ton University, </institution> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Otherwise, it proceeds to service the interrupt normally. Together, the three pieces implement an efficient mechanism by which a user-level process may wait for one of a set of flags to be changed by the arrival of a message. Our technical report <ref> [4] </ref> contains a more detailed explaination for each piece, as well as a justification of the correctness of this code. 5. Experiments This section describes the performance measurements we obtained using our mechanism. All tests were performed on a prototype four-node SHRIMP multicomputer. 5.1. <p> We ran ttcp on top of our user-level communication library implementing the standard sockets API [5]. ttcp's results are similar to the micro-benchmarks'; they are omitted because of space limitations, but are presented in out technical report <ref> [4] </ref>. 5.2. Distributed File System Our next set of experiments uses a distributed file system [14] implemented on top of our user-level sockets library. The distributed file system runs one process on each node; each process runs several user-level threads internally, and the processes communicate via our user-level sockets library.
Reference: [5] <author> S. Damianakis, C. Dubnicki, and E. W. Felten. </author> <title> Stream sockets on shrimp. </title> <type> Technical Report TR-513-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: This paper considers the questions of which receive policy and which mechanism to use. We present an implementation on the prototype SHRIMP multicomputer [1, 7], and the results of experiments using our user-level sockets library <ref> [5] </ref>, for micro-benchmarks, larger benchmarks, and for a distributed file system. Our results show that a hybrid spin/block policy is best in a wide range of situations, and that reducing the interrupt service overhead significantly improves performance. 2. <p> We ran ttcp on top of our user-level communication library implementing the standard sockets API <ref> [5] </ref>. ttcp's results are similar to the micro-benchmarks'; they are omitted because of space limitations, but are presented in out technical report [4]. 5.2. Distributed File System Our next set of experiments uses a distributed file system [14] implemented on top of our user-level sockets library.
Reference: [6] <author> C. Dubnicki, L. Iftode, E. W. Felten, and K. Li. </author> <title> Software support for virtual memory-mapped communication. </title> <booktitle> In Proceedings of the IEEE 8th International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card [1]. The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) <ref> [6] </ref>. This mechanism supports various message-passing packages and applications effectively, and delivers excellent performance [7]. The prototype system consists of four interconnected nodes. Each node is a DEC 560ST PC with a 60 MHz Pentium CPU. <p> Micro-Benchmarks Our first set of experiments uses a simple client/server program which ping-pongs one word between two processes, running on two different SHRIMP nodes. The ping-pong program was written using the Shrimp Base Library (SBL) API, the lowest level communication API available on SHRIMP <ref> [6] </ref>. In these experiments we inserted a fixed delay (the "message delay") before the send operation in the ping-pong program. This simulates a case where some amount of computation is required before replying to each arriving message.
Reference: [7] <author> E. W. Felten, R. Alpert, A. Bilas, M. A. Blumrich, D. W. Clark, S. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early experience with message-passing on the shrimp multicomputer. </title> <booktitle> In Proceedings of 23th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: 1. Introduction Many network interfaces can place incoming data directly in user memory <ref> [1, 7, 3, 2] </ref>. This capability enables the construction of very efficient network software since the network interface can deliver a burst of packets without any software intervention. On such an architecture, communication can be handled entirely in a user-level library. <p> This requires both a policy for when to poll and when to block, and a mechanism for efficient blocking. This paper considers the questions of which receive policy and which mechanism to use. We present an implementation on the prototype SHRIMP multicomputer <ref> [1, 7] </ref>, and the results of experiments using our user-level sockets library [5], for micro-benchmarks, larger benchmarks, and for a distributed file system. Our results show that a hybrid spin/block policy is best in a wide range of situations, and that reducing the interrupt service overhead significantly improves performance. 2. <p> Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn [3], and Telegraphos [12]. In addition, other architectures, such as Myrinet [2] and FLASH [10], are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer <ref> [1, 7] </ref> consists largely of off-the-shelf components. The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card [1]. <p> The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication (VMMC) [6]. This mechanism supports various message-passing packages and applications effectively, and delivers excellent performance <ref> [7] </ref>. The prototype system consists of four interconnected nodes. Each node is a DEC 560ST PC with a 60 MHz Pentium CPU. The routing network is an Intel backplane consisting of a two-dimensional mesh of Intel Mesh Routing Chips (iMRCs), and is the same network used in the Paragon multicomputer. <p> Further, the receiver can control the interrupts by enabling or disabling them for any page that has been mapped to receive incoming data. 2.2. SHRIMP Communication Software We have built several communication libraries for SHRIMP, implementing various standard communication interfaces. All of these libraries have a common structure <ref> [7] </ref> based on pairwise connections between processes. A connection generally consists of some buffers for transmitting the contents of user messages, and some buffers for transmitting control information. To pass a message, the sender first chooses a remote data buffer into which to place the message contents. <p> The receiver then sends some control information back to the sender, to signal that data buffer space can be re-used. The task of looking for incoming control information motivated the work described in this paper. Our experience in building communication libraries <ref> [7] </ref> for SHRIMP shows that changes in control data (flags) indicate the arrival of new data. As a consequence, our implementation provides support for waiting (via spinning, blocking or a combination) for any of a set of flag values to be changed. 3. <p> The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours. Several experimental network interfaces allow a sending process to deposit data directly into the memory of a receiving process <ref> [1, 3, 7, 8, 12] </ref>. Though these systems differ in several important aspects, they all provide the basic functionality to support efficient user-level communication, so they all face the problem we are trying to solve.
Reference: [8] <author> R. B. Gillett. </author> <title> Memory channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours. Several experimental network interfaces allow a sending process to deposit data directly into the memory of a receiving process <ref> [1, 3, 7, 8, 12] </ref>. Though these systems differ in several important aspects, they all provide the basic functionality to support efficient user-level communication, so they all face the problem we are trying to solve.
Reference: [9] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Emprical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: This tradeoff is much like the one between spinning and blocking in the context of multiprocessor locks. Ousterhout [13] describes spin-blocking for locks in the Medusa system. Karlin et al. <ref> [9] </ref> study the performance impact for several algorithms. The general discussion in this section is based on Karlin's work, though many of the implementation tradeoffs in Karlin's case are different than in ours. <p> A simple hybrid spin-block policy works fairly well: spin for time T block and then block if the message has not yet arrived. This policy does well in the two limiting cases of very small and very large arrival delay. Karlin et al. show <ref> [9] </ref> by a simple argument that this policy is 2-competitive: its cost is never more than twice that of the o*ine optimal policy, regardless of the sequence of arrival delays. 3.3.
Reference: [10] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-moni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The stanford flash multiprocessor. </title> <booktitle> In Proceedings of 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn [3], and Telegraphos [12]. In addition, other architectures, such as Myrinet [2] and FLASH <ref> [10] </ref>, are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer [1, 7] consists largely of off-the-shelf components. The nodes are Pentium PCs running the Linux operating system, which communicate through a routing network via a custom designed network interface card [1].
Reference: [11] <author> O. Maquelin, G. R. Gao, H. H. Hum, K. B. Theobald, and X.-M. Tia. </author> <title> Polling watchdog: Combining polling and interrupts for efficient message handling. </title> <booktitle> In Proceedings of 23th International Symposium on Computer Architecture, </booktitle> <pages> pages 179-188, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: None of them has yet provided a solution, but our solution strategy should work on all of them. The polling watchdog <ref> [11] </ref> is a technique that tries to minimize the number of interrupts by delaying interrupting the receiving process in case it reads the incoming data in time and avoids needing to be interrupted.
Reference: [12] <author> E. P. Markatos and M. G. H. Katevenis. Telegraphos: </author> <title> High-performance networking for parallel processing on workstation clusters. </title> <booktitle> In Proceedings of IEEE 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 144-153, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Some existing network interfaces that directly sup-port these assumptions are SHRIMP, Hamlyn [3], and Telegraphos <ref> [12] </ref>. In addition, other architectures, such as Myrinet [2] and FLASH [10], are sufficiently programmable to satisfy the assumptions. 2.1. The SHRIMP Architecture The SHRIMP multicomputer [1, 7] consists largely of off-the-shelf components. <p> The coprocessor gives the network interface flexibility to implement a variety of receive-side interfaces, but the Myrinet system as shipped does not support any policies as aggressive as ours. Several experimental network interfaces allow a sending process to deposit data directly into the memory of a receiving process <ref> [1, 3, 7, 8, 12] </ref>. Though these systems differ in several important aspects, they all provide the basic functionality to support efficient user-level communication, so they all face the problem we are trying to solve.
Reference: [13] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: This tradeoff is much like the one between spinning and blocking in the context of multiprocessor locks. Ousterhout <ref> [13] </ref> describes spin-blocking for locks in the Medusa system. Karlin et al. [9] study the performance impact for several algorithms. The general discussion in this section is based on Karlin's work, though many of the implementation tradeoffs in Karlin's case are different than in ours.
Reference: [14] <author> R. A. Shillner and E. W. Felten. </author> <title> Simplifying distributed file systems using a shared logical disk. </title> <type> Technical Report TR-524-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: Distributed File System Our next set of experiments uses a distributed file system <ref> [14] </ref> implemented on top of our user-level sockets library. The distributed file system runs one process on each node; each process runs several user-level threads internally, and the processes communicate via our user-level sockets library.
Reference: [15] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In contrast, the spin-block strategy has the tendency to co-schedule communicating processes. This property allows spin-block to avoid, or quickly escape thrashing cycle. This results in excellent performance for spin-block with both hog and nice processes. 6. Related Work Active messages <ref> [15] </ref> invoke a receiver-side handler whenever a message arrives. Control information at the head of each message specifies the address of a user-level routine that is responsible for extracting the message from the network. This approach often requires servicing an interrupt for each message received.
References-found: 15

