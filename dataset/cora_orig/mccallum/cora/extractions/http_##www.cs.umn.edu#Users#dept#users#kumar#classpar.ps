URL: http://www.cs.umn.edu/Users/dept/users/kumar/classpar.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: Parallel Formulations of Inductive Classification Learning Algorithm  
Author: Eui-Hong (Sam) Han, Anurag Srivastava, Vipin Kumar 
Keyword: Data mining, parallel processing, classification, decision trees.  
Date: 96-040 (May 28, 1996; Revised on June 14, 1996)  
Address: 4-192 EECS Bldg., 200 Union St. SE  Minneapolis, MN 55455, USA  
Affiliation: Dept. of Computer Information Sciences  University of Minnesota  
Pubnum: Technical Report  
Abstract: One of the important problems in data mining [SAD + 93] is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition given data into predefined classes. For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is N P - hard. All of the existing algorithms, like C4:5 [Qui93], CDP [AIS93] and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O(AN logN ) to O(AN (logN ) 2 ) with N training data items and A attributes. These algorithms are fast enough for application domains where N is relatively small. However, in the data mining domain where millions of records and a large number of attributes are involved, the execution time of these algorithms can become prohibitive, particularly in interactive applications. In this paper, we present parallel formulations of classification-rule-learning algorithm based on induction. We describe two basic parallel formulation, one is based on Synchronous Tree Construction Approach and the other is based on Partitioned Tree Construction Approach. We discuss the advantages and disadvantages of using these methods and propose a hybrid method that employs the good features of these methods. We also provide the analysis of the cost of computation and communication of the proposed hybrid method. 
Abstract-found: 1
Intro-found: 1
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engg., </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 [Qui93], CDP <ref> [AIS93] </ref> and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 [Qui93], CDP <ref> [AIS93] </ref> and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g.
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, the entropy is calculated based on either information theory [Qui93] or Gini Index <ref> [BFOS84] </ref>. One attribute with the most entropy gain is selected as a test for the node expansion. The C4:5 algorithm generates a classification-decision tree for the given training data set by recursive partitioning of the data. The decision tree is grown using depth-first strategy.
Reference: [Hon94] <author> S.J. Hong. </author> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report RC 19664, </type> <institution> IBM Research Division, </institution> <year> 1994. </year>
Reference-contexts: In some cases, S might be really big and thus the communication cost can be unacceptably high. There are two possible solutions to this problem. One method is to group continuous attribute values together to reduce the size of S. <ref> [Hon94] </ref> proposes a way to perform this mechanism as a preprocessing step of classification task. The other approach is to send the class distribution information of only the best possible continuous attribute values. One way to estimate the best possible candidate is using the local class distribution information.
Reference: [KGGK94] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis. </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: This can be done in three steps. In the first step, every processor collects the class distribution information of the local data. In the second step, the processors exchange the local class distribution information using global reduction <ref> [KGGK94] </ref>. Finally, each processor can simultaneously compute entropy gains of the attributes and find the best attribute for splitting the root node. There are two approaches for further progress. In Synchronous Tree Construction Approach, the entire set of processors synchronously expand one node of the decision tree at a time. <p> Exchange the class distribution information with all other processors and add up the class distribution information to get a complete distribution of all the attributes. This can be done via a global reduction operation <ref> [KGGK94] </ref>. 4. Calculate the entropy gains of each attribute and select the best attribute for child node expansion. 5. Based on the attribute values, create child nodes and partition the data according to the values of the attribute chosen. 6. <p> attributes in the database C Number of classes in the database M Average number of distinct values in the discrete attributes S Average number of distinct values in the continuous attributes L Level of a decision tree t c Unit computation time t s Start up time of communication latency <ref> [KGGK94] </ref> t w Per-word transfer time of communication latency [KGGK94] Table 4: Symbols used in the analysis. When communication costs become prohibitive the partitioning of the tree is done so that separate groups of processors are created which work on some part of the decision tree. <p> the database M Average number of distinct values in the discrete attributes S Average number of distinct values in the continuous attributes L Level of a decision tree t c Unit computation time t s Start up time of communication latency <ref> [KGGK94] </ref> t w Per-word transfer time of communication latency [KGGK94] Table 4: Symbols used in the analysis. When communication costs become prohibitive the partitioning of the tree is done so that separate groups of processors are created which work on some part of the decision tree. <p> We provide a separate analysis for two different data attribute types, discrete and continuous. The analysis of the mixture of discrete and continuous attributes could be done easily based on the analysis provided here. The detailed study of the communication patterns used in this analysis can be found in <ref> [KGGK94] </ref>. Table 4 describes the symbols used in this section. 5.1 Case with Discrete Attributes Only Here we give a detailed analysis of splitting for the case when only discrete attributes are present. At each leaf of a level, there are A attribute tables that need to be communicated. <p> Refer to <ref> [KGGK94] </ref> section 3.7 for details. 12 Note that in the worst case almost all of the time might be spent by this sender might be waiting on the receiver which is receiving data from other senders.
Reference: [KK92] <author> G. Karypis and V. Kumar. </author> <title> Unstructured tree search on SIMD parallel computers. </title> <type> Technical Report TR-92-021, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1992. </year> <month> 16 </month>
Reference: [MAR96] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology, </booktitle> <address> Avignon, France, </address> <year> 1996. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 [Qui93], CDP [AIS93] and SLIQ <ref> [MAR96] </ref>, use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. These algorithms are fast enough for application domains where N is relatively small. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 [Qui93], CDP [AIS93] and SLIQ <ref> [MAR96] </ref>, use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> We propose feasible solutions to this problem in two cases, namely one based on C4:5 [Qui93] and the other based on SLIQ <ref> [MAR96] </ref>, and provide analyses. In C4:5, all the data items that belong to one node is determined first, then these data item is sorted each time for different attributes.
Reference: [Qui93] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively. In fact, the computational complexity of finding an optimal classification decision tree is NP hard. All of the existing algorithms, like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use local heuristics to handle the computational complexity. The computational complexity of these algorithms ranges from O (AN logN) to O (AN (logN ) 2 ) with N training data items and A attributes. <p> Section 3 describes the two basic parallel formulation. The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> The hybrid method is described in Section 4 and its analysis is shown in Section 5. Section 6 presents our conclusions. 2 Sequential Classification Rule Learning Algorithms Most of the existing induction-based algorithms like C4:5 <ref> [Qui93] </ref>, CDP [AIS93] and SLIQ [MAR96], use Hunt's method [Qui93] as the basic algorithm. Here is a recursive description of Hunt's method for constructing a decision tree from a set T of training cases with classes denoted fC 1 ; C 2 ; : : : ; C k g. <p> Play overcast 83 78 false Play overcast 64 65 true Play overcast 81 75 false Play rain 71 80 true Don't Play rain 65 70 true Don't Play rain 75 80 false Play rain 68 80 false Play rain 70 96 false Play Table 1: A small training data set <ref> [Qui93] </ref> 2 Attribute Value Class Play Don't Play sunny 2 3 overcast 4 0 rain 3 2 Table 2: Class Distribution Information of Attribute Outlook Attribute Value Binary Test Class Play Don't Play 65 1 0 70 3 1 75 4 1 78 5 1 80 7 2 85 7 3 <p> For a continuous attribute, binary test involving all the distinct values of the attribute is considered. Table 3 shows the class distribution information of data attribute Humidity. Once the class distribution information of all the attributes are gathered, the entropy is calculated based on either information theory <ref> [Qui93] </ref> or Gini Index [BFOS84]. One attribute with the most entropy gain is selected as a test for the node expansion. The C4:5 algorithm generates a classification-decision tree for the given training data set by recursive partitioning of the data. The decision tree is grown using depth-first strategy. <p> The size of class distribution information of all the continuous attributes in the frontier of the decision tree could be too large to send in single communication buffer. We propose feasible solutions to this problem in two cases, namely one based on C4:5 <ref> [Qui93] </ref> and the other based on SLIQ [MAR96], and provide analyses. In C4:5, all the data items that belong to one node is determined first, then these data item is sorted each time for different attributes.
Reference: [SAD + 93] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> DBMS research at a crossroads: The vienna update. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <pages> pages 688-692, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year> <month> 17 </month>
Reference-contexts: 1 Introduction One of the important problems in data mining <ref> [SAD + 93] </ref> is the classification-rule learning. The classification-rule learning involves finding rules or decision trees that partition given data into predefined classes. For any realistic problem domain of the classification-rule learning, the set of possible decision trees is too large to be searched exhaustively.
References-found: 8

