URL: ftp://ftp.cs.umd.edu/pub/sel/papers/isern-96-01.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/SoftEng/ESEG/papers/papers.html
Root-URL: 
Email: email: [lanubile|visaggio]@seldi.uniba.it  
Title: Assessing Defect Detection Methods for Software Requirements Inspections Through External Replication  
Author: Filippo Lanubile*, and Giuseppe Visaggio 
Address: Via Orabona 4, 70126 Bari, Italy  
Affiliation: University of Bari, Dipartimento di Informatica  
Abstract: This paper presents the external replication of a controlled experiment which compared three defect detection techniques (Ad Hoc, Checklist, and Defect-based Scenario) for software requirements inspections, and evaluated the benefits of collection meetings after individual reviews. The results of our replication were partially different from those of the original experiment. Unlike the original experiment, we did not find any empirical evidence of better performance when using scenarios. To explain these negative findings we provide a list of hypotheses. On the other hand, the replication confirmed one result of the original experiment: the defect detection rate is not improved by the collection meetings. The external replication was made possible by the existence of an experimental kit provided by the original investigators. We discuss what difficulties we encountered in applying the package to our environment, having different cultures and skills. We also discovered some critical problems in the original experiment which can be considered threats to its internal validity. Using our results, experience and suggestions, other researchers will be able to improve the original experimental design before attempting further replications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Basili, V. R., Caldiera, G., and Rombach, H. D., </author> <year> 1994. </year> <title> Goal Question Metric paradigm. Encyclopedia of Software Engineering. </title> <editor> Marciniak J. J. (ed.), </editor> <address> Ney York, NY: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In the following we describe similarities and differences between our replication and the original experiment, and discuss the problems encountered during the preparation, execution and analysis of the experiment. 2.1 Goals and Hypotheses The main goal of the original experiment can be defined using a Goal/Question/Metric (GQM) template <ref> (Basili, 1994) </ref>: 4 Analyze the detection techniques for SRS inspections for the purpose of assessment with respect to the number and type of defects uncovered from the point of view of the researcher We established hypotheses in order to confirm the results of the original experiment.
Reference: <author> Fagan, M. E., </author> <year> 1976. </year> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15:3, </volume> <pages> 182-211. </pages>
Reference-contexts: 1. Introduction Software inspection is the best known way of detecting defects in software requirements specifications (SRS). Initially created for source code <ref> (Fagan, 1976) </ref>, software inspections have been extended for the intermediate products of the earlier phases of the software life cycle, such as design and requirements specifications (Humphrey, 1989). The basic software inspection process is made up of three steps: detection, collection, and repair. <p> Lecture references (IEEE, 1984), (Heninger, 1980), <ref> (Fagan, 1976) </ref> were found in the experimental kit. However, the style for SCR tabular notation in (Heninger, 1980) was partially different from that used in the experimental kit. Thus, we adapted our lectures to the style effectively used for the three SRSs.
Reference: <author> Heninger, K. L., </author> <year> 1980. </year> <title> Specifying software requirements for complex systems: new techniques and their application. </title> <journal> IEEE Trans. Soft. Eng., SE-6: </journal> <volume> 1, </volume> <pages> 2-13. </pages>
Reference-contexts: All the three SRSs adhered to the IEEE format (IEEE, 1984), with an overview written in natural language and the detailed sections specified using the SCR tabular notation <ref> (Heninger, 1980) </ref>. A complete list of the defects for WLMS and CRUISE, but not for the training SRS, was included in the experimental kit. Defects appeared both in the overview and detailed sections of the SRSs. <p> Lecture references (IEEE, 1984), <ref> (Heninger, 1980) </ref>, (Fagan, 1976) were found in the experimental kit. However, the style for SCR tabular notation in (Heninger, 1980) was partially different from that used in the experimental kit. Thus, we adapted our lectures to the style effectively used for the three SRSs. <p> Lecture references (IEEE, 1984), <ref> (Heninger, 1980) </ref>, (Fagan, 1976) were found in the experimental kit. However, the style for SCR tabular notation in (Heninger, 1980) was partially different from that used in the experimental kit. Thus, we adapted our lectures to the style effectively used for the three SRSs. After these lectures, we created teams and randomly assigned teams to detection techniques, and subjects to team roles (moderator, recorder, and reader).
Reference: <author> Humphrey, W. S., </author> <year> 1989. </year> <title> Managing the Software Process. </title> <address> New York: </address> <publisher> Addison-Wesley. </publisher> <editor> IEEE Std.830-1984. </editor> <title> IEEE Guide to Software Requirements Specification. </title> <journal> Soft. Eng. Tech. Comm. of the IEEE Computer Society. </journal>
Reference-contexts: 1. Introduction Software inspection is the best known way of detecting defects in software requirements specifications (SRS). Initially created for source code (Fagan, 1976), software inspections have been extended for the intermediate products of the earlier phases of the software life cycle, such as design and requirements specifications <ref> (Humphrey, 1989) </ref>. The basic software inspection process is made up of three steps: detection, collection, and repair. In the detection step, each member of the inspection team individually reviews the document.
Reference: <author> Judd, C. M., Smith, E. R., and Kidder, L. H., </author> <year> 1991. </year> <title> Research Methods in Social Relations, </title> <note> 6th edition. </note>
Reference-contexts: However, since it is not possible to draw final conclusions from a single experiment, we conducted a replication of the experiment of Porter, Votta, and Basili. A comprehensive definition of replication is in <ref> (Judd, 1991) </ref>: Replication means that other researchers in other settings with different samples attempt to reproduce the research as closely as possible. If the results of the replication are consistent with the original research, we have increased confidence in the hypothesis that the original study supported.
Reference: <editor> Orlando: </editor> <publisher> Holt Rinehart and Winston, Inc. </publisher>
Reference: <author> Parnas, D. L., and Weiss, D. M., </author> <year> 1985. </year> <title> Active design reviews: </title> <booktitle> principles and practices. Proc. 8th Int. Conf. Soft. Eng., </booktitle> <pages> 215-222. </pages>
Reference-contexts: Another dimension to characterize these approaches is the reviewers responsibility; with both Ad Hoc and Checklist approaches, reviewers have general (i.e. they look for all kinds of defects) and identical (i.e. no division of work within a team) responsibilities. Parnas and Weiss <ref> (Parnas, 1985) </ref> criticized the overlapping of responsibilities and proposed active design reviews, a new form of inspection process where the reviewers had specific responsibilities defined by their use of different checklists.
Reference: <author> Porter, A. A., Votta, L. G., and Basili, V. R., </author> <year> 1995. </year> <title> Comparing detection methods for software requirements inspections: a replicated experiment. </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> 21:6, </volume> <pages> 563-575. </pages>
Reference-contexts: Based on the same concern about the separation of responsibilities, the Scenario approach has been proposed <ref> (Porter, 1995) </ref> as a systematic defect detection technique with a specific and distinct responsibility for each reviewer. Defect-based scenarios are a set of procedures for detecting particular classes of defects. <p> Each reviewer follows a scenario for a specific defect class and a team combines distinct scenarios. Running a controlled experiment, Porter, Votta, and Basili <ref> (Porter, 1995) </ref> found that: the defect detection rate increased about 35% using a Scenario approach as compared to Ad Hoc and Checklist; Ad Hoc and Checklist techniques were equivalent with respect to the performance of reviewers; reviewers using scenarios found more defects in their target defect class as compared to reviewers <p> Our classification of the defects was identical to the original. Although classifying was a time-consuming activity, this experience can testify to the independence of the defect taxonomy from subjective evaluations. See the appendices in <ref> (Porter, 1995) </ref> for the taxonomy, the checklist and the three scenarios.
Reference: <author> Weinberg, G. M., and Schulman, E. L., </author> <year> 1974. </year> <title> Goals and performance in computer programming. </title> <booktitle> Human Factors, 16:1, </booktitle> <pages> 70-77. </pages>
Reference-contexts: Time limit was too short. Analyzing human performance in programming activities, Weinberg and Schulman concluded that unreasonably short deadlines would result in erroneous programs, and warned experimenters against mixing the results of subjects who have easily finished with those of subjects who were pressed for time <ref> (Weinberg, 1974) </ref>. We do not know if this happened to the 16 original experiment. In our case, if we discard all the data from subjects pressed for the deadline there will not be enough observations to analyze. 5.
References-found: 9

