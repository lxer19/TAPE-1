URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.mlc93.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/craven.mlc93.ps.abstract.html
Root-URL: 
Email: email: fcraven, shavlikg@cs.wisc.edu  
Title: Learning Symbolic Rules Using Artificial Neural Networks  
Author: Mark W. Craven and Jude W. Shavlik 
Affiliation: Computer Sciences Department University of Wisconsin  
Date: 1993  
Address: San Mateo, CA,  1210 West Dayton St. Madison, WI 53706  
Note: Appears in Machine Learning: Proceedings of the Tenth International Conference, P. E. Utgoff (editor), Morgan Kaufmann,  
Abstract: A distinct advantage of symbolic learning algorithms over artificial neural networks is that typically the concept representations they form are more easily understood by humans. One approach to understanding the representations formed by neural networks is to extract symbolic rules from trained networks. In this paper we describe and investigate an approach for extracting rules from networks that uses (1) the NofM extraction algorithm, and (2) the network training method of soft weight-sharing. Previously, the NofM algorithm had been successfully applied only to knowledge-based neural networks. Our experiments demonstrate that our extracted rules generalize better than rules learned using the C4.5 system. In addition to being accurate, our extracted rules are also reasonably comprehensible.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atlas, L., Cole, R., Connor, J., El-Sharkawi, M., Marks II, R. J., Muthusamy, Y., & Barnard, E. </author> <year> (1989). </year> <title> Performance comparisons between backpropagation networks and classification trees on three real-world applications. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Fisher, D. H. & McKusick, K. B. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> In Proc. of the 11th IJCAI, </booktitle> <pages> (pp. 788-793), </pages> <address> Detroit, MI. </address>
Reference: <author> Fu, L. M. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proc. of the 9th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Anaheim, CA. </address>
Reference: <author> Kramer, A. H. & Sangiovanni-Vincentelli, A. </author> <year> (1989). </year> <title> Efficient parallel learning algorithms for neural networks. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 1). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: After the number of hidden units is selected for each network, a similar cross-validation procedure is used to determine the parameter for soft weight-sharing. We use a conjugate-gradient learning algorithm <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref> to train the weights and the Gaussian parameters of the networks. Each hidden and output unit has five local Gaussians which act on the weights feeding into the unit. Decision trees are induced, and rules extracted from them, using Quinlan's (1993) C4.5 system.
Reference: <author> LeCun, Y., Denker, J. S., & Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Our proposed approach involves partitioning the space of hidden unit activations and then searching for rules that explain particular regions of this space. A second area that we plan to investigate in future research is to employ a weight-pruning method, such as Optimal Brain Damage <ref> (LeCun et al., 1990) </ref>, during learning. The effectiveness of the NofM algorithm is partly due to the weight pruning that it performs during its clustering step.
Reference: <author> McMillan, C., Mozer, M., & Smolensky, P. </author> <year> (1991). </year> <title> The connectionist scientist game: Rule extraction and refinement in a neural network. </title> <booktitle> In Proc. of the 13th Conf. of the Cognitive Science Society, </booktitle> <address> Chicago, IL. </address> <publisher> Erlbaum. </publisher>
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> In Proc. of the 11th IJCAI, </booktitle> <pages> (pp. 775-780), </pages> <address> Detroit, MI. </address> <note> (A longer version appears in Machine Learning, 6). </note>
Reference: <author> Nowlan, S. J. & Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference: <author> Ourston, D. & Mooney, R. J. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proc. of the 8th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> (pp. 815-820), </pages> <address> Boston, MA. </address>
Reference: <author> Pazzani, M. & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Pomerleau, D. A. </author> <year> (1991). </year> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 88-97. </pages>
Reference-contexts: 1 INTRODUCTION Artificial neural networks (ANNs) have been successfully applied to real-world problems as varied as steering a motor vehicle <ref> (Pomerleau, 1991) </ref> and learning to pronounce English text (Sejnowski & Rosen-berg, 1987).
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: Although their method was motivated by the desire for better generalization, we explore it here as a means for facilitating rule extraction. In the spirit of the minimum-description-length principle <ref> (Rissanen, 1978) </ref>, soft weight-sharing uses a cost function that penalizes network complexity. Thus, during training the network tries to find an optimal tradeoff between data-misfit (i.e., the error rate on the training examples) and complexity.
Reference: <author> Saito, K. & Nakano, R. </author> <year> (1988). </year> <title> Medical diagnostic expert system based on PDP model. </title> <booktitle> In Proc. of the IEEE International Conf. on Neural Networks, </booktitle> <pages> (pp. 255-262), </pages> <address> San Diego, CA. </address> <publisher> IEEE. </publisher>
Reference: <author> Sejnowski, T. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: 1 INTRODUCTION Artificial neural networks (ANNs) have been successfully applied to real-world problems as varied as steering a motor vehicle (Pomerleau, 1991) and learning to pronounce English text <ref> (Sejnowski & Rosen-berg, 1987) </ref>. In addition to these practical successes, several empirical studies have concluded that neural networks provide performance comparable to, and in some cases, better than common symbolic learning algorithms (Atlas et al., 1989; Fisher & McKusick, 1989; Mooney et al., 1989).
Reference: <author> Towell, G. G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin - Madison. </institution>
Reference-contexts: In this paper we describe and investigate an approach for extracting symbolic rules from trained neural networks. Our approach uses the NofM algorithm <ref> (Towell & Shavlik, 1991) </ref> to extract rules from networks that have been trained using Nowlan and Hinton's (1992) method of soft weight-sharing. Although soft weight-sharing was designed as a technique for improving generalization in neural networks, we explore it here as a means for facilitating rule extraction. <p> Experimental evidence indicates that the weights tend to be fairly well clustered after training as well <ref> (Towell, 1991) </ref>. The applicability of the NofM method might seem to be limited to knowledge-based networks, since in con The dotted ovals illustrate how the weights have been grouped into clusters.
Reference: <author> Towell, G. G. & Shavlik, J. W. </author> <year> (1991). </year> <title> Interpretation of artificial neural networks: Mapping knowledge-based neural networks into rules. </title> <editor> In Moody J., Han-son S. and Lippman, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> (A longer version will appear in Machine Learning). </note>
Reference-contexts: In this paper we describe and investigate an approach for extracting symbolic rules from trained neural networks. Our approach uses the NofM algorithm <ref> (Towell & Shavlik, 1991) </ref> to extract rules from networks that have been trained using Nowlan and Hinton's (1992) method of soft weight-sharing. Although soft weight-sharing was designed as a technique for improving generalization in neural networks, we explore it here as a means for facilitating rule extraction. <p> Experimental evidence indicates that the weights tend to be fairly well clustered after training as well <ref> (Towell, 1991) </ref>. The applicability of the NofM method might seem to be limited to knowledge-based networks, since in con The dotted ovals illustrate how the weights have been grouped into clusters.
Reference: <author> Towell, G. G., Shavlik, J. W., & Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. of the 8th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Previously, Towell (1991) reported that the NofM algorithm failed to extract accurate rules from conventional networks. We use two problem domains to investigate the effectiveness of our approach. The first domain involves recognizing promoters in DNA <ref> (Towell et al., 1990) </ref>. Promoters are short nucleotide sequences that occur before genes and serve as binding sites for the protein RNA polymerase during gene transcription. Identifying promoters is an important step in locating genes in DNA sequences. <p> For knowledge-based neural networks, this is a reasonable assumption since the weights are clustered before training. For example, using the Kbann algorithm <ref> (Towell et al., 1990) </ref> to map a set of symbolic rules into a knowledge-based network, the weights that are specified by the domain theory have values of approximately 4 and -4, whereas the rest of the weights have values near 0. <p> A second observation is that the rules focus on what are known by biologists to be the most significant regions of the DNA sequence. In particular, a domain theory developed by Michiel Noordewier <ref> (Towell et al., 1990) </ref> identifies the -14 to -7 and the -37 to -31 regions as containing the most important features of a promoter. These are termed the contact regions. The rules extracted from all of the hidden units specify antecedents primarily in these areas.
References-found: 18

