URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-13.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: REGULARIZED LINEAR PROGRAMS WITH EQUILIBRIUM CONSTRAINTS  
Author: O. L. MANGASARIAN 
Keyword: Key words. Linear programs with equilibrium constraints, regularization, exact penalty, concave minimization.  
Note: AMS subject classifications. 90C05, 90C30  
Abstract: We consider an arbitrary linear program with equilibrium constraints (LPEC) that may possibly be infeasible or have an unbounded objective function. We regularize the LPEC by perturbing it in a minimal way so that the regularized problem is solvable. We show that such regularization leads to a problem that is guaranteed to have a solution which is an exact solution to the original LPEC if that problem is solvable, otherwise it is a residual-minimizing approximate solution to the original LPEC. We propose a finite successive linearization algorithm for the regularized problem that terminates at point satisfying the minimum principle necessary optimality condition for the problem. 1. Introduction. We consider the following linear program with equilibrium 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. S. Bradley, O. L. Mangasarian, and J. B. Rosen. </author> <title> Parsimonious least norm approximation. </title> <type> Technical Report 97-03, </type> <institution> Computer Sciences Department, University of Wisconsin, Madi-son, Wisconsin, </institution> <month> March </month> <year> 1997. </year> <note> Computational Optimization and Applications, to appear. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps.Z. </note>
Reference-contexts: A smoothing of the penalty problem (3.7) objective function using a smoothing of the plus-function proposed in [4, 5] can also be used and can be shown to lead to an exact solution the penalty problem (3.7) for a finite value of the penalty parameter as was done in <ref> [1] </ref>. The encouraging computational results of [10] on a special case of the proposed algorithm on an NP-hard problem is a possible indicator of the possible effectiveness of the proposed algorithm for solving the regularized LPEC. 8 O. L. MANGASARIAN
Reference: [2] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: For special classes of important problems that arise in machine learning and data mining <ref> [11, 2, 13] </ref> where P (x; ff) is concave in x and S is a polyhedral set without straight lines going to infinity in both directions, such as problem (3.8) below, the point x of Theorem 2.1 is any repeated vertex solution of min P (x; ff i ); fff i <p> L. MANGASARIAN repeated vertex, a stationary repeated vertex can be easily obtained by successive linearization algorithms such as Algorithm 4.1 below. Such stationary vertices turn out to be very useful in machine learning and data mining applications <ref> [11, 2, 13] </ref>. In a related weaker result, Tikhonov and Arsenin [19, Theorem 1, p 101] establish asymptotic convergence of an inexact quadratic penalty function solution for the very special case of projecting a point on a possibly empty solution set of a system of linear equations.
Reference: [3] <author> E. J. Bredensteiner and K. P. Bennett. </author> <title> Feature minimization within decision trees. </title> <note> Department of Mathematical Sciences Math Report No. 218, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1995. </year> <title> Computational Optimizations and Applications. </title> <note> to appear. </note>
Reference-contexts: This is a special case of a mathematical program with equilibrium constraints [7] that has important applications in machine learning <ref> [9, 11, 3] </ref>. Fukushima and Pang [6] were the first to address the feasibility issue for a mathematical program with equilibrium constraints (MPEC) similar to those of our LPEC (1.1) and point out that it is a difficult problem in general. <p> 2 ) 0 = (u; v; r 1 ; r 2 ) 2 arg min 8 : fi fi fi A T u + N T v r 1 c; (u; v; r 1 ; r 2 ) 0 = (3.5) We note that for LPECs arising in machine learning <ref> [9, 11, 3] </ref> there is no need for this preliminary regularization in as much as the underlying linear programs there are feasible and solvable.
Reference: [4] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 71(1) </volume> <pages> 51-69, </pages> <year> 1995. </year>
Reference-contexts: A finite successive linearization algorithm is proposed for solving the regularized problem that terminates at a stationary point. A smoothing of the penalty problem (3.7) objective function using a smoothing of the plus-function proposed in <ref> [4, 5] </ref> can also be used and can be shown to lead to an exact solution the penalty problem (3.7) for a finite value of the penalty parameter as was done in [1].
Reference: [5] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> A class of smoothing functions for nonlinear and mixed complementarity problems. </title> <journal> Computational Optimization and Applications, </journal> <volume> 5(2) </volume> <pages> 97-138, </pages> <year> 1996. </year>
Reference-contexts: A finite successive linearization algorithm is proposed for solving the regularized problem that terminates at a stationary point. A smoothing of the penalty problem (3.7) objective function using a smoothing of the plus-function proposed in <ref> [4, 5] </ref> can also be used and can be shown to lead to an exact solution the penalty problem (3.7) for a finite value of the penalty parameter as was done in [1].
Reference: [6] <author> M. Fukushima and J.-S. Pang. </author> <title> Some feasibility issues in mathematical programs with equilibrium constraints. </title> <type> Technical report, </type> <institution> Department of Mathematics & Physics, Johns Hopkins University, </institution> <month> January </month> <year> 1997. </year> <note> SIAM Journal on Optimization, to appear. </note>
Reference-contexts: This is a special case of a mathematical program with equilibrium constraints [7] that has important applications in machine learning [9, 11, 3]. Fukushima and Pang <ref> [6] </ref> were the first to address the feasibility issue for a mathematical program with equilibrium constraints (MPEC) similar to those of our LPEC (1.1) and point out that it is a difficult problem in general. <p> Note that this preliminary regularization, which renders the underlying linear program solvable, does in no way ensure that the LPEC is solvable. It was pointed out in <ref> [6] </ref> that such feasibility of the underlying linear program does not ensure feasibility of the LPEC constraints. In fact one of the main objectives of this work is the ability to handle LPECs that may not have a solution.
Reference: [7] <author> Z.-Q. Luo, J.-S. Pang, and D. Ralph. </author> <title> Mathematical Programs with Equilibrium Constraints. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1996. </year>
Reference-contexts: This is a special case of a mathematical program with equilibrium constraints <ref> [7] </ref> that has important applications in machine learning [9, 11, 3]. Fukushima and Pang [6] were the first to address the feasibility issue for a mathematical program with equilibrium constraints (MPEC) similar to those of our LPEC (1.1) and point out that it is a difficult problem in general.
Reference: [8] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 43-54, </pages> <year> 1992. </year>
Reference-contexts: of LPEC (1.1) and constitutes a local error bound for the general linear complementarity problem, and a global error bound for M 2 R 0 , the class of matrices M for which 0 is the unique solution to the homogeneous linear complementarity problem: 0 y ? M y 0 <ref> [8, 16] </ref>. We turn our attention now to solution methods for the regularized problem. 4. Successive Linearization Algorithm. We propose here a finite successive linearization algorithm introduced in [14] that utilizes a supergradient of the piecewise linear concave objective function of the penalty problem (3.7) as follows. Algorithm 4.1.
Reference: [9] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: This is a special case of a mathematical program with equilibrium constraints [7] that has important applications in machine learning <ref> [9, 11, 3] </ref>. Fukushima and Pang [6] were the first to address the feasibility issue for a mathematical program with equilibrium constraints (MPEC) similar to those of our LPEC (1.1) and point out that it is a difficult problem in general. <p> In the present work we wish to address this case as well as the more general problem of an unsolvable LPEC via an exact penalty regularization approach. We note that exact penalty approaches have been proposed for solvable LPECs in <ref> [9] </ref> and for solvable MPECs in [15]. We briefly outline the contents of the paper now. In Section 2 we give a general exact penalty result which shows that a fixed solution of a penalty problem, for an increasing sequence of penalty parameters tending to infinity, minimizes the penalty term. <p> 2 ) 0 = (u; v; r 1 ; r 2 ) 2 arg min 8 : fi fi fi A T u + N T v r 1 c; (u; v; r 1 ; r 2 ) 0 = (3.5) We note that for LPECs arising in machine learning <ref> [9, 11, 3] </ref> there is no need for this preliminary regularization in as much as the underlying linear programs there are feasible and solvable.
Reference: [10] <author> O. L. Mangasarian. </author> <title> The linear complementarity problem as a separable bilinear program. </title> <journal> Journal of Global Optimization, </journal> <volume> 6 </volume> <pages> 153-161, </pages> <year> 1995. </year>
Reference-contexts: A similar supergradient-based algorithm has been proposed for a general linear complementarity problem [14], and successfully used on the NP-complete knapsack feasibility problem <ref> [10] </ref>. 1.1. Notation and Background. A word about our notation and background material. All vectors will be column vectors unless transposed to a row vector by a prime superscript T . <p> The sequence terminates at a point (x i ; y that satisfies the minimum principle necessary optimality condition for (3.7): c T (xx i )+ff (@ x (x i ) T (xx i ; y i )) 0; 8 (x; y) 2 S: We note that the bilinear algorithm of <ref> [10] </ref> for solving the knapsack feasibility problem as a linear complementarity problem can be interpreted as a special case of Algorithm 4.1 with a fixed j = 0. That bilinear algorithm solved 80 consecutive instances of the knapsack LCP ranging in size between 10 and 3000 without failure. <p> The encouraging computational results of <ref> [10] </ref> on a special case of the proposed algorithm on an NP-hard problem is a possible indicator of the possible effectiveness of the proposed algorithm for solving the regularized LPEC. 8 O. L. MANGASARIAN
Reference: [11] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Heidelberg, </address> <year> 1996. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: This is a special case of a mathematical program with equilibrium constraints [7] that has important applications in machine learning <ref> [9, 11, 3] </ref>. Fukushima and Pang [6] were the first to address the feasibility issue for a mathematical program with equilibrium constraints (MPEC) similar to those of our LPEC (1.1) and point out that it is a difficult problem in general. <p> For special classes of important problems that arise in machine learning and data mining <ref> [11, 2, 13] </ref> where P (x; ff) is concave in x and S is a polyhedral set without straight lines going to infinity in both directions, such as problem (3.8) below, the point x of Theorem 2.1 is any repeated vertex solution of min P (x; ff i ); fff i <p> L. MANGASARIAN repeated vertex, a stationary repeated vertex can be easily obtained by successive linearization algorithms such as Algorithm 4.1 below. Such stationary vertices turn out to be very useful in machine learning and data mining applications <ref> [11, 2, 13] </ref>. In a related weaker result, Tikhonov and Arsenin [19, Theorem 1, p 101] establish asymptotic convergence of an inexact quadratic penalty function solution for the very special case of projecting a point on a possibly empty solution set of a system of linear equations. <p> 2 ) 0 = (u; v; r 1 ; r 2 ) 2 arg min 8 : fi fi fi A T u + N T v r 1 c; (u; v; r 1 ; r 2 ) 0 = (3.5) We note that for LPECs arising in machine learning <ref> [9, 11, 3] </ref> there is no need for this preliminary regularization in as much as the underlying linear programs there are feasible and solvable.
Reference: [12] <author> O. L. Mangasarian. </author> <title> The ill-posed linear complementarity problem. </title> <editor> In Michael Ferris and Jong-Shi Pang, editors, </editor> <booktitle> Complementarity and Variational Problems, </booktitle> <pages> pages 226-233. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-15.ps.Z. </note>
Reference-contexts: This is is so because the feasible region of the LPEC (1.1) is a subset of the feasible region of the underlying linear program, and both problems have the same objective function. This preliminary regularization can be achieved <ref> [12, Theorem 2.2] </ref> by appropriately modifying the underlying linear program by solving: min kz (z Hz h) + k 1 ;(3.1) which is a regularization of the linear complementarity problem associated with the linear program underlying LPEC (1.1): 0 z ? Hz + h 0:(3.2) Here H = 6 4 0 <p> q 7 5 ; z = 6 4 y v 7 5 :(3.3) We note that because of the skew symmetry of H, the regularization problem (3.1) can be rewritten as a linear program and its solution can be used to generate a solvable underlying linear program for LPEC (1.1) <ref> [12, Equation (9)] </ref>. For further details about this regularization we refer the interested reader to [12, Theorem 2.2]. <p> For further details about this regularization we refer the interested reader to <ref> [12, Theorem 2.2] </ref>.
Reference: [13] <author> O. L. Mangasarian. </author> <title> Mathematical programming in data mining. Data Mining and Knowledge Discovery, </title> <booktitle> 1(2) </booktitle> <pages> 183-201, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-05.ps.Z. </note>
Reference-contexts: For special classes of important problems that arise in machine learning and data mining <ref> [11, 2, 13] </ref> where P (x; ff) is concave in x and S is a polyhedral set without straight lines going to infinity in both directions, such as problem (3.8) below, the point x of Theorem 2.1 is any repeated vertex solution of min P (x; ff i ); fff i <p> L. MANGASARIAN repeated vertex, a stationary repeated vertex can be easily obtained by successive linearization algorithms such as Algorithm 4.1 below. Such stationary vertices turn out to be very useful in machine learning and data mining applications <ref> [11, 2, 13] </ref>. In a related weaker result, Tikhonov and Arsenin [19, Theorem 1, p 101] establish asymptotic convergence of an inexact quadratic penalty function solution for the very special case of projecting a point on a possibly empty solution set of a system of linear equations.
Reference: [14] <author> O. L. Mangasarian. </author> <title> Solution of general linear complementarity problems via nondifferentiable concave minimization. </title> <journal> Acta Mathematica Vietnamica, </journal> <volume> 22(1) </volume> <pages> 199-205, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-10.ps.Z. </note>
Reference-contexts: In Section 4 we propose a supergradient-based successive linearization algorithm, that terminates in a finite number of steps at a stationary point, for solving the regularized penalty problem for the LPEC. A similar supergradient-based algorithm has been proposed for a general linear complementarity problem <ref> [14] </ref>, and successfully used on the NP-complete knapsack feasibility problem [10]. 1.1. Notation and Background. A word about our notation and background material. All vectors will be column vectors unless transposed to a row vector by a prime superscript T . <p> We turn our attention now to solution methods for the regularized problem. 4. Successive Linearization Algorithm. We propose here a finite successive linearization algorithm introduced in <ref> [14] </ref> that utilizes a supergradient of the piecewise linear concave objective function of the penalty problem (3.7) as follows. Algorithm 4.1. Successive Linearization Algorithm Choose ff &gt; 0. Start with an arbitrary (x 0 ; y 0 ) 2 R n+m . <p> By using <ref> [14, Theorem 3] </ref> finite termination of the above algorithm can be estab lished as follows. REGULARIZED LPEC 7 Theorem 4.2. Finite Termination The Successive Linearization Algorithm 4.1 generates a finite sequence of points with strictly decreasing objective function values for the penalty problem (3.7).
Reference: [15] <author> O. L. Mangasarian and J.-S. Pang. </author> <title> Exact penalty functions for mathematical programs with linear complementarity constraints. </title> <journal> Optimization, </journal> <volume> 42 </volume> <pages> 1-8, </pages> <year> 1997. </year> <note> Available from: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-06.ps.Z. </note>
Reference-contexts: In the present work we wish to address this case as well as the more general problem of an unsolvable LPEC via an exact penalty regularization approach. We note that exact penalty approaches have been proposed for solvable LPECs in [9] and for solvable MPECs in <ref> [15] </ref>. We briefly outline the contents of the paper now. In Section 2 we give a general exact penalty result which shows that a fixed solution of a penalty problem, for an increasing sequence of penalty parameters tending to infinity, minimizes the penalty term.
Reference: [16] <author> O. L. Mangasarian and J. Ren. </author> <title> New improved error bounds for the linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 241-255, </pages> <year> 1994. </year>
Reference-contexts: of LPEC (1.1) and constitutes a local error bound for the general linear complementarity problem, and a global error bound for M 2 R 0 , the class of matrices M for which 0 is the unique solution to the homogeneous linear complementarity problem: 0 y ? M y 0 <ref> [8, 16] </ref>. We turn our attention now to solution methods for the regularized problem. 4. Successive Linearization Algorithm. We propose here a finite successive linearization algorithm introduced in [14] that utilizes a supergradient of the piecewise linear concave objective function of the penalty problem (3.7) as follows. Algorithm 4.1.
Reference: [17] <author> K. G. Murty. </author> <title> Linear Programming. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: They show convergence to a projection of the point onto the set of solutions of the normal equations of the linear system when the linear system is infeasible. In contrast for example, Part (iii) of the above theorem shows how the Big M Method of linear programming <ref> [17, pp 81-82] </ref> can terminate at an optimal solution over the set of infeasibility minimizers when a linear program is infeasible. We turn our attention now to LPECs that may possibly be infeasible or whose objective may be unbounded. 3. LPEC Regularization.
Reference: [18] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: Proof (i) Since the concave objective function of the penalty problem (3.7) is bounded below on the nonempty polyhedral region that contains no lines that go to infinity in both directions, it must by <ref> [18, Corollary 32.3.4] </ref> have a vertex solution. (ii) Since S has a finite number of vertices, then there exists a sequence of positive numbers fff i g 1 i=0 " 1 such that: x 2 arg vertex min x2S c T x+d T y+ffe T minfy; N x+M yqg; 8ff 2
Reference: [19] <author> A. N. Tikhonov and V. Y. Arsenin. </author> <title> Solutions of Ill-Posed Problems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: L. MANGASARIAN repeated vertex, a stationary repeated vertex can be easily obtained by successive linearization algorithms such as Algorithm 4.1 below. Such stationary vertices turn out to be very useful in machine learning and data mining applications [11, 2, 13]. In a related weaker result, Tikhonov and Arsenin <ref> [19, Theorem 1, p 101] </ref> establish asymptotic convergence of an inexact quadratic penalty function solution for the very special case of projecting a point on a possibly empty solution set of a system of linear equations.
References-found: 19

