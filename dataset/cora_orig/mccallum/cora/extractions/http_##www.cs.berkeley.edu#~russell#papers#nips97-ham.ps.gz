URL: http://www.cs.berkeley.edu/~russell/papers/nips97-ham.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: fparr,russellg@cs.berkeley.edu  
Title: Reinforcement Learning with Hierarchies of Machines  
Author: Ron Parr and Stuart Russell 
Keyword: Category: reinforcement learning. Preference: plenary.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division, UC  
Abstract: We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and behavior-based or teleo-reactive approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Benson and N. Nilsson. </author> <title> Reacting, planning and learning in an autonomous agent. </title> <editor> In K. Furukawa, D. Michie, and S. Muggleton, editors, </editor> <booktitle> Machine Intelligence 14. </booktitle> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: We will use the following notation: An MDP is a 4-tuple, (S, A, T, R) where S is a set of states, A is a set of actions, T is a transition model mapping S fi A fi S into probabilities in <ref> [0, 1] </ref>, and R is a reward function mapping S fi A fi S into real-valued rewards. Algorithms for solving MDPs can return a policy that maps from S to A, a real-valued value function V on states, or a real-valued Q-function on state-action pairs. <p> The design of hierarchically organized, layered controllers was popularized by Brooks [3]. His designs use a somewhat different means of passing control, but our analysis and theorems apply equally well to his machine description language. The teleo-reactive agent designs of Benson and Nilsson <ref> [1] </ref> are even closer to our HAM language. Both of these approaches assume that the agent is completely specified, albeit self-modifiable.
Reference: [2] <author> D. C. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: Moreover, state aggregation in this form is hard to apply in many cases where natural bases for aggregation such as physical location can cluster together states with sharply varying values. Other work on environment decomposition includes that of Dean and Lin [6] and Bertsekas and Tsitsiklis <ref> [2] </ref>, who showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. In reinforcement learning, Dayan and Hinton [5] have proposed feudal RL which specifies an explicit subgoal structure, with fixed values for each subgoal achieved, in order to achieve a hierarchical decomposition of the state space. <p> Moreover, the HAM structure provides a natural decomposition of the HAM-induced model, making it amenable to the divide-and-conquer approaches of [6] and <ref> [2] </ref>. There are opportunities for generalization across all levels of the HAM paradigm. Value function approximation can be used for the HAM induced model and inductive learning methods can be used to produce HAMs or to generalize their effects upon different regions of the state space.
Reference: [3] <author> Rodney A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2 </volume> <pages> 14-23, </pages> <year> 1986. </year>
Reference-contexts: Lin's somewhat informal scheme [9] also allows agents to treat entire policies as single actions. Both of these approaches can be seen as special cases of our framework. The design of hierarchically organized, layered controllers was popularized by Brooks <ref> [3] </ref>. His designs use a somewhat different means of passing control, but our analysis and theorems apply equally well to his machine description language. The teleo-reactive agent designs of Benson and Nilsson [1] are even closer to our HAM language.
Reference: [4] <author> K. W. Currie and A. Tate. O-Plan: </author> <title> the Open Planning Architecture. </title> <journal> Artificial Intelligence, </journal> <volume> 52(1) </volume> <pages> 49-86, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Although it can yield suboptimal policies, top-down hierarchical control often reduces the complexity of decision making from exponential to linear in the size of the problem. For example, hierarchical task network (HTN) planners can generate solutions containing tens of thousands of steps <ref> [4] </ref>, whereas flat planners can manage only tens of steps. HTN planners are successful because they use a plan library that describes the decomposition of high-level activities into lower-level activities.
Reference: [5] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Neural Information Processing Systems 5, </booktitle> <pages> pages 361-368, </pages> <address> San Mateo, California, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other work on environment decomposition includes that of Dean and Lin [6] and Bertsekas and Tsitsiklis [2], who showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. In reinforcement learning, Dayan and Hinton <ref> [5] </ref> have proposed feudal RL which specifies an explicit subgoal structure, with fixed values for each subgoal achieved, in order to achieve a hierarchical decomposition of the state space.
Reference: [6] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, state aggregation in this form is hard to apply in many cases where natural bases for aggregation such as physical location can cluster together states with sharply varying values. Other work on environment decomposition includes that of Dean and Lin <ref> [6] </ref> and Bertsekas and Tsitsiklis [2], who showed that some MDPs are loosely coupled and hence amenable to divide-and-conquer algorithms. <p> Moreover, the HAM structure provides a natural decomposition of the HAM-induced model, making it amenable to the divide-and-conquer approaches of <ref> [6] </ref> and [2]. There are opportunities for generalization across all levels of the HAM paradigm. Value function approximation can be used for the HAM induced model and inductive learning methods can be used to produce HAMs or to generalize their effects upon different regions of the state space.
Reference: [7] <author> Yung-Jen Hsu. </author> <title> Synthesizing efficient agents from partial programs. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, ISMIS '91, Proceedings, </booktitle> <pages> pages 142-151, </pages> <address> Charlotte, North Carolina, </address> <month> October </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: The teleo-reactive agent designs of Benson and Nilsson [1] are even closer to our HAM language. Both of these approaches assume that the agent is completely specified, albeit self-modifiable. The idea of partial behavior descriptions can be traced at least to Hsu's partial programs <ref> [7] </ref>, which were used with a deterministic logical planner. 7 Conclusions and future work We have presented Hierarchies of Abstract Machines (HAMs) as a principled means of constraining the set of policies that are considered for a Markov decision process and we have demonstrated the efficacy of this approach in a
Reference: [8] <author> T. Jaakkola, M.I. Jordan, and S.P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201, </pages> <year> 1994. </year>
Reference-contexts: Proof sketch We note that the expected reinforcement signal from the HAMQ-learning rules is the same as the expected reinforcement signal that would be received if the agent were acting directly in the transformed model of Theorem 1 above. Thus, Theorem 1 of <ref> [8] </ref> can be applied to prove the convergence of the HAMQ-learning agent, provided that we enforce suitable constraints on the exploration strategy and the update parameter decay rate. We ran some experiments to measure the performance of HAMQ-learning on our sample problem.
Reference: [9] <author> Long-Ji Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: Behavioral decomposition seems to avoid the problem of creating non-Markov processes that plagues state-based approaches. Sutton [13] proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy. Lin's somewhat informal scheme <ref> [9] </ref> also allows agents to treat entire policies as single actions. Both of these approaches can be seen as special cases of our framework. The design of hierarchically organized, layered controllers was popularized by Brooks [3].
Reference: [10] <author> Martin L. Puterman. </author> <title> Markov decision processes: Discrete stochastic dynamic programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: We conclude with a discussion of related approaches and ongoing work. 2 Markov Decision Processes We assume the reader is familiar with the basic concepts of MDPs (see, e.g., <ref> [10] </ref>).
Reference: [11] <author> S. P. Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 323-340, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This seems natural in some domains, but it is not clear how the subgoal values should be related to the overall reward function. Singh <ref> [11] </ref> focuses on the simultaneous learning of policies to achieve subgoals and an ordering on these subgoals that achieves a higher level goal. Subgoal structure is encoded in a reward function that forces the agent to solve the subgoals in the proper sequence.
Reference: [12] <author> Satinder P. Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 361-368, </pages> <address> Cambridge, Massachusetts, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: As yet, no systematic understanding exists of the space of possible approaches, and various approaches may apply in various contexts. We can, however, identify two common threads: environment decomposition and behavior decomposition. State aggregation (see, e.g., <ref> [12] </ref>) is an example of environment decomposition in which similar states are clustered together and assigned the same value. This approach effectively reduces the size of the state space.
Reference: [13] <author> R. Sutton. </author> <title> Temporal abstraction in reinforcement learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, July 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Subgoal structure is encoded in a reward function that forces the agent to solve the subgoals in the proper sequence. Behavioral decomposition seems to avoid the problem of creating non-Markov processes that plagues state-based approaches. Sutton <ref> [13] </ref> proposes temporal abstractions, which concatenate sequences of state transitions together to permit reasoning about temporally extended events, and which can thereby form a behavioral hierarchy. Lin's somewhat informal scheme [9] also allows agents to treat entire policies as single actions.
References-found: 13

