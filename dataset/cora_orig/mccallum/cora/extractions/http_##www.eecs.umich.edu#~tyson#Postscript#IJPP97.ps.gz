URL: http://www.eecs.umich.edu/~tyson/Postscript/IJPP97.ps.gz
Refering-URL: http://www.eecs.umich.edu/~tyson/publications.html
Root-URL: http://www.cs.umich.edu
Email: (matthewj@cs.ucdavis.edu) (arp@tosca.colorado.edu)  
Title: d d Managing Data Caches using Selective Cache Line Replacement  
Author: Gary Tyson Matthew Farrens Andrew R. Pleszkun John Matthews 
Address: Riverside, CA Davis, CA 95616  (tyson@cs.ucr.edu) (farrens@cs.ucdavis.edu) Boulder, CO 80309-0425  
Affiliation: Department of Computer Science Computer Science Department Department of Electrical University of California, Riverside University of California, Davis and Computer Engineering  University of Colorado-Boulder  
Abstract: As processor performance continues to improve, more demands are being placed on the performance of the memory system. The caches employed in current processor designs are very similar to those described in early cache studies. In this paper, a detailed characterization of data cache behavior for individual load instructions is given. It will be shown that by selectively allocating cache lines according the characteristics of individual load instructions, overall performance can be improved for both the data cache and the memory system. This approach can improve some aspects of memory performance by as much as 60 percent on existing executables. 
Abstract-found: 1
Intro-found: 1
Reference: [ASWR93] <author> S. G. Abraham, R. A. Sugumar, D. Windheiser, B. R. Rau and R. Gupta, </author> <title> ``Predictability of Load/Store Instruction Latencies'', </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <address> Austin, Texas (December 1-3, </address> <year> 1993), </year> <pages> pp. 139-152. </pages>
Reference-contexts: By keeping these recently replaced lines in close proximity to the data cache, subsequent references to them will not experience the full main memory miss penalty. Among the most intriguing software approaches to reducing the miss penalty is a study by Abraham, et. al. <ref> [ASWR93] </ref> in which they observe that a very small number of load instructions are responsible for causing a disproportionate percentage of cache misses. <p> One could potentially reduce the miss rate of the data cache by simply not caching those data references that lead to a high miss rate. As Abraham, et el. - 4 - d d <ref> [ASWR93] </ref> point out, a large percentage of the data misses are caused by a very small number of instructions. <p> In addition, if we do not cache the data associated with high-miss-rate instructions, memory bandwidth requirements could be reduced since these references would only request a single word from memory, instead of an entire cache line. Since the study by Abraham, et el. <ref> [ASWR93] </ref> did not look at an extensive set of benchmark programs, we began by performing experiments similar to theirs in which we measured the miss rate associated with individual load and store instructions for a more extensive set of programs. <p> Thus, a cache lookup for an item is unaffected by whether it is marked C/NA or not --- only the allocation on a miss is affected. We looked at both static (similar to <ref> [ASWR93] </ref>) and dynamic approaches to identifying and marking these C/NA instructions. 4.1. Static Method We began by modeling a simple strategy in which all load instructions that do not meet a threshold for cache hit rate are marked C/NA. <p> Furthermore, the 75% threshold also relates to the memory bandwidth requirements for a cache line replacement (32 bytes) and a 64-bit load reference (8 bytes), and is the same value settled on by <ref> [ASWR93] </ref>. 4.1.1. Cache Hit Rate and Memory Bandwidth Utilization Table 3 shows the change in cache hit rate and required memory bandwidth after the poorest performing instructions were marked C/NA.
Reference: [Bela66] <author> L. A. Belady, </author> <title> ``A Study of Replacement Algorithms for a Virtual-Storage Computer'', </title> <journal> IBM Systems Journal, </journal> <volume> vol. 5, no. 2 (1966), </volume> <pages> pp. 282-288. </pages>
Reference-contexts: In order to calculate the optimal performance of this scheme, we ran experiments in which we assumed full knowledge of the future data reference pattern. (We essentially used Belady's optimal page replacement algorithm <ref> [Bela66] </ref> on cache lines instead of virtual pages). For standard n-way replacement schemes, we used this information to decide which of the n elements to replace. (Since these are standard replacement schemes, one of the elements did have to be removed). We called this the Must Replace (MR) scheme.
Reference: [CaGr94] <author> B. Calder and D. Grunwald, </author> <title> ``Fast and Accurate Instruction Fetch and Branch Prediction'', </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago, Illinois (April 18-21, </address> <year> 1994), </year> <pages> pp. 2-11. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [CaPo] <author> D. Callahan and A. Porterfield, </author> <title> ``Data Cache Performance and Supercomputer Applications'', </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 564-572. </pages>
Reference-contexts: Especially in processor designs with on-chip caches, access time to off-chip memory (measured in processor clock cycles) has increased dramatically. A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced.
Reference: [CHYP94] <author> P. Chang, E. Hao, T. Yeh and Y. Patt, </author> <title> ``Branch Classification: A New Mechanism for Improving Branch Predictor Performance'', </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <address> San Jose, Ca. </address> <month> (November 30 - December 2, </month> <year> 1994), </year> <pages> pp. 22-31. </pages>
Reference-contexts: To develop a more transparent scheme, we looked to previous work done in branch prediction. In <ref> [CHYP94] </ref>, Chang and Patt use a counter based scheme to choose the best performing scheme among different branch predictors. We use this same approach to determine whether the reference pattern for a load instruction is being captured by the cache.
Reference: [ChBa95] <author> T. Chen and J. Baer, </author> <title> ``Effective Hardware Based Data Prefetching for High-Performance Processors'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 44, no. </volume> <month> 5 (May </month> <year> 1995), </year> <pages> pp. 609-623. </pages>
Reference-contexts: Especially in processor designs with on-chip caches, access time to off-chip memory (measured in processor clock cycles) has increased dramatically. A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced. <p> An example of hardware-based prefetching is the work by Chen and Baer <ref> [ChBa95] </ref>. In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched.
Reference: [EKPP93] <author> P. G. Emma, J. W. Knight, J. H. Pomerene, T. R. Puzak and R. N. Rechtschaffen, </author> <title> ``Cache Miss Facility with Stored Sequences for Data Fetching'', </title> <type> U.S. Patent 5,233,702(Issued: </type> <month> August 3, </month> <year> 1993). </year> - <title> 26 - d d </title>
Reference-contexts: An example of hardware-based prefetching is the work by Chen and Baer [ChBa95]. In this paper the authors propose keeping a history of the strides of data references, and using that information to make predictions as to what should be prefetched. IBM uses a similar hardware approach <ref> [EKPP93] </ref> in which they associate previous miss behavior with a load instruction and use that information to do prefetching. A somewhat different hardware approach to reducing the miss penalty is put forth by [Joup90].
Reference: [FiFr92] <author> J. A. Fisher and S. M. Freudenberger, </author> <title> ``Predicting Conditional Branch Directions from Previous Runs of a Program'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 85-95. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [HePa90] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> (1990). </year>
Reference-contexts: From <ref> [HePa90] </ref>: Memory stall clock cycles = Reads Read Miss Rate Read Miss Penalty + Writes Write Miss Rate Write Miss Penalty This equation shows that in order to minimize the average access time, the hit rate should be maximized (thereby minimizing the miss rate) while simultaneously minimizing the miss penalty. 2.1. <p> Reducing Cache Misses (Miss Rate) Cache misses can be categorized into three types of misses: Compulsory, Capacity and Conflict <ref> [HePa90] </ref>. 1 Compulsory misses are those misses that are initially experienced when a cache is being filled (often called cold start misses), and are very difficult to eliminate. A capacity miss occurs in a cache when more active data items exist than the cache can encompass.
Reference: [Joup90] <author> N. Jouppi, </author> <title> ``Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers'', </title> <booktitle> Proceedings of the Seventeenth Annual International Symposium on Computer Architecture, </booktitle> <volume> vol. 18, no. </volume> <month> 2 (May </month> <year> 1990), </year> <pages> pp. 364-373. </pages>
Reference-contexts: IBM uses a similar hardware approach [EKPP93] in which they associate previous miss behavior with a load instruction and use that information to do prefetching. A somewhat different hardware approach to reducing the miss penalty is put forth by <ref> [Joup90] </ref>. The author makes the observation that a cache or a degree of associativity that is too small will lead to a substantial number of conflict (or capacity) misses, and that there is a good chance that the line that is selected for replacement will be needed again soon.
Reference: [KlLe91] <author> A. C. Klaiber and H. M. Levy, </author> <title> ``An Architecture for Software-Controlled Data Prefetching'', </title> <booktitle> Proceedings of the Eighteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Toronto, Canada (May 27-30, </address> <year> 1991), </year> <pages> pp. 43-53. </pages>
Reference-contexts: Especially in processor designs with on-chip caches, access time to off-chip memory (measured in processor clock cycles) has increased dramatically. A number of studies have proposed techniques (either compiler-based or hardware-based) that reduce the miss penalty of the cache by performing some type of data prefetch <ref> [CaPo, ChBa95, KlLe91] </ref>. If data items are prefetched during idle data cache cycles, references to a prefetched item will find it already in the cache and thus will not cause a miss and the associated miss penalty will not be experienced.
Reference: [MuQF91] <author> J. M. Mulder, N. T. Quach and M. J. Flynn, </author> <title> ``An Area Model for On-Chip Memories and its Application'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 26, no. </volume> <month> 2 (February </month> <year> 1991), </year> <pages> pp. 98-105. </pages>
Reference-contexts: Several studies have investigated the relationship between cache access time, cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found, for example, that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [PaS92] <author> S. Pan, K. So and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation'', </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA (October 12-15, </address> <year> 1992), </year> <pages> pp. 76-84. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [Smit81] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies'', </title> <booktitle> Proceedings of the Eighth Annual International Symposium on Computer Architecture, </booktitle> <address> Minneapolis, Minnesota (May 1981), </address> <pages> pp. 135-148. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [SrEu94] <author> A. Srivastava and A. Eustace, </author> <title> ``ATOM: A System for Building Customized Program Analysis Tools'', </title> <booktitle> Proceedings of the ACM SIGPLAN Notices 1994 Conference on Programming Languages and Implementations(June 1994), </booktitle> <pages> pp. 196-205. </pages>
Reference-contexts: Using the ATOM program trace facilities <ref> [SrEu94] </ref> and the SPEC92 suite of benchmarks, such statistics were relatively straight-forward to gather. Each program in the SPEC 92 suite was instrumented in order to track the data cache hit rate associated with each unique data address.
Reference: [WaRP92] <author> T. Wada, S. Rajan and S. A. Przybylski, </author> <title> ``An Analytical Access Time Model for On-Chip Cache Memories'', </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 27, no. 8 - 27 - d d (August 1992), </volume> <pages> pp. 1147-1156. </pages>
Reference-contexts: Several studies have investigated the relationship between cache access time, cache size and cache associativity <ref> [MuQF91, WaRP92] </ref>. These studies carefully parameterized a hardware model of the components of the cache (such as data array, tag array, compare logic, bus delays, etc.) and found, for example, that going from a direct mapped to 2-way associative cache substantially increases the access time to the cache.
Reference: [YeP91] <author> T. Yeh and Y. Patt, </author> <title> ``Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <address> Albuquerque, New Mexico (November 18-20, </address> <year> 1991), </year> <pages> pp. 51-61. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [YeP92] <author> T. Yeh and Y. Patt, </author> <title> ``Alternative Implementations of Two-Level Adaptive Training Branch Prediction'', </title> <booktitle> Proceedings of the Nineteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia (May 19-21, </address> <year> 1992), </year> <pages> pp. 124-134. </pages>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
Reference: [YeP93] <author> T. Yeh and Y. Patt, </author> <title> ``A Comparison of Dynamic Branch Predictors that use Two Levels of Branch History'', </title> <booktitle> Proceedings of the Twentieth Annual International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA (May 16-19, </address> <year> 1993), </year> <pages> pp. 257-266. </pages> <address> d d </address>
Reference-contexts: Our goal is to develop a scheme that will provide the same performance enhancement transparently. In order to select which items should be marked C/NA, we turn to the body of work on branch prediction strategies. There has been a great amount written about branch prediction strategies recently <ref> [CaGr94, FiFr92, PaS92, Smit81, YeP91, YeP92, YeP93] </ref>.
References-found: 19

