URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr92/tr92-002.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr92-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: A Distributed Data-balanced Dictionary Based on the B-link Tree  
Author: by Theodore Johnson Adrian Colbrook 
Keyword: Concurrent dictionary data structures, Message passing multiprocessor systems, Balanced search trees, B-link trees, Replica coherency.  
Address: Gainesville, FL 32611 Cambridge, MA 02139  1992  Cambridge, Massachusetts 02139  
Affiliation: University of Florida MIT Laboratory for Computer Science  MIT Laboratory for Computer Science  c Massachusetts Institute of Technology  Massachusetts Institute of Technology Laboratory for Computer Science  
Pubnum: Technical Report MIT/LCS/TR-530  
Email: ted@squall.cis.ufl.edu colbrook@concerto.lcs.mit.edu  
Date: February 1992  
Abstract: Many concurrent dictionary data structures have been proposed, but usually in the context of shared memory multiprocessors. In this paper, we present an algorithm for a concurrent distributed B-tree that can be implemented on message passing parallel computers. Our distributed B-tree (the dB-tree) replicates the interior nodes in order to improve parallelism and reduce message passing. We show how the dB-tree algorithm can be used to build an efficient, highly parallel, data-balanced distributed dictionary, the dE-tree. Adrian Colbrook was supported in part by the National Science Foundation under grant CCR-8716884, by the Defense Advanced Research Projects Agency (DARPA) under Contract N00014-89-J-1988, by an equipment grant from Digital Equipment Corporation and by a Science and Engineering Research Council Postdoctoral Fellowship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.L. Lehman and S.B. Yao. </author> <title> Efficient locking for concurrent operations on B-trees. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(4) </volume> <pages> 650-670, </pages> <year> 1981. </year>
Reference-contexts: 1 Introduction We introduce a new balanced search tree algorithm for distributed memory architectures. The search tree uses the B-link tree <ref> [1] </ref> as a base, and distributes ownership of the nodes among the processors that maintain the tree. We also replicate the non-leaf nodes, to improve parallelism. <p> To prevent this kind of process overtaking many algorithms have their operations use lock coupling to block independent operations [11, 22]. An operation traverses the tree by obtaining the appropriate lock on the child before releasing the lock it holds on the parent. B-link trees <ref> [1, 12, 13] </ref> eliminate the need for lock coupling. If the wrong node is reached at any stage the operation is able to recover. This reduces the number of locks that must be held concurrently and increases throughput. <p> In Section 2 we describe the dB-tree. In Section 3 we show how the dE-tree can be built from the dB-tree. Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree <ref> [1, 12, 13] </ref>. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms [20, 21, 29]. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. <p> C C sibling sibling B B A parent parent parent Complete the split Half-split Initially The B-link tree in a shared memory multiprocessor does not easily support the deletion of nodes. 6 2 THE DB-TREE, A CONCURRENT DISTRIBUTED B-TREE Lehman and Yao <ref> [1] </ref> recommend that nodes are never deleted. Sagiv [12] describes garbage collection algorithms for underfull nodes. Lanin and Shasha [13], and Wang [17] describe an algorithm that leaves stubs in place of deleted nodes.
Reference: [2] <author> R. H. Thomas. </author> <title> A majority consensus approach to concurrency control for multiple copy databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <year> 1979. </year>
Reference-contexts: The search tree uses the B-link tree [1] as a base, and distributes ownership of the nodes among the processors that maintain the tree. We also replicate the non-leaf nodes, to improve parallelism. If we apply a read-one, write-all consistency maintenance rule <ref> [2] </ref>, then reading a node becomes cheaper and writing to a node becomes more expensive as we increase the degree of replication. <p> We can achieve the atomic updates by requiring that modifying suboperations lock every copy of the modified node before performing the update and block all reads and updates on the node, by using one of the well-known algorithms for managing replicated data <ref> [2, 30] </ref>. However, we can maintain our replicated nodes with far less synchronization and overhead. First, observe that it is not necessary to distribute the contents of the node on every modification, it is only necessary to distribute the modification itself.
Reference: [3] <author> R. Bayer. </author> <title> Symmetric Binary B-trees: Data Structure and Maintenance Algorithms. </title> <journal> Acta Informat-ica, </journal> <volume> 1 </volume> <pages> 290-306, </pages> <year> 1972. </year>
Reference-contexts: A number of useful computations can be implemented in terms of dictionary abstract data types, including symbol tables, priority queues and pattern matching systems. The B-tree was originally introduced by Bayer <ref> [3] </ref>. The B-tree algorithms for sequential applications were designed to minimize the response time for a single query and the sequential algorithm for a single search operation on a balanced B-tree has logarithmic complexity.
Reference: [4] <author> M.J. Quinn. </author> <title> Designing efficient algorithms for parallel computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year> <note> REFERENCES 17 </note>
Reference-contexts: The improvement in the response time that may be achieved by a parallel algorithm for a single search can at best be logarithmic in the number of processors used <ref> [4] </ref>. Therefore, for parallel systems a more important concern is increasing system throughput for a series of search, insertion and deletion operations executing in parallel.
Reference: [5] <author> R. Bayer and M. Schkolnick. </author> <title> Concurrency of Operations on B-trees. </title> <journal> Acta Informatica, </journal> <volume> 9 </volume> <pages> 1-22, </pages> <year> 1977. </year>
Reference: [6] <author> R.E. Miller. </author> <title> Multiple access to B-trees. </title> <booktitle> In Proceedings of the 1978 Conference on Information Sciences and Systems, </booktitle> <pages> pages 400-408, </pages> <institution> Johns Hopkins University, Baltimore, MD, </institution> <year> 1978. </year>
Reference: [7] <author> C. Ellis. </author> <title> Concurrent search and insertions in 2-3 trees. </title> <journal> Acta Informatica, </journal> <volume> 14(1) </volume> <pages> 63-86, </pages> <year> 1980. </year>
Reference: [8] <author> Y. S. Kwong and D. Wood. </author> <title> A New Method for Concurrency in B-trees. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-8(3):211-222, </volume> <month> May </month> <year> 1982. </year>
Reference: [9] <author> G. Lausen. </author> <title> Integrated concurrency control in shared B-trees. </title> <journal> Computing, </journal> <volume> 33 </volume> <pages> 13-26, </pages> <year> 1984. </year>
Reference: [10] <author> D. Shasha and N. Goodman. </author> <title> Concurrent search tree algorithms. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 53-90, </pages> <year> 1988. </year>
Reference: [11] <author> Y. Mond and Y. Raz. </author> <title> Concurrency control in B+-trees using preparatory operations. </title> <booktitle> In Proceedings of the 11th International Conference on Very Large Data Bases, </booktitle> <pages> pages 331-334, </pages> <year> 1985. </year>
Reference-contexts: The second process can then update the node in such a fashion as to cause the first process to lock the wrong node when it eventually acquires the lock. To prevent this kind of process overtaking many algorithms have their operations use lock coupling to block independent operations <ref> [11, 22] </ref>. An operation traverses the tree by obtaining the appropriate lock on the child before releasing the lock it holds on the parent. B-link trees [1, 12, 13] eliminate the need for lock coupling. If the wrong node is reached at any stage the operation is able to recover.
Reference: [12] <author> Y. Sagiv. </author> <title> Concurrent Operations on B-Trees with Overtaking. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(2) </volume> <pages> 275-296, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: To prevent this kind of process overtaking many algorithms have their operations use lock coupling to block independent operations [11, 22]. An operation traverses the tree by obtaining the appropriate lock on the child before releasing the lock it holds on the parent. B-link trees <ref> [1, 12, 13] </ref> eliminate the need for lock coupling. If the wrong node is reached at any stage the operation is able to recover. This reduces the number of locks that must be held concurrently and increases throughput. <p> In Section 2 we describe the dB-tree. In Section 3 we show how the dE-tree can be built from the dB-tree. Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree <ref> [1, 12, 13] </ref>. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms [20, 21, 29]. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. <p> The B-link tree algorithms use the additional information stored in the nodes to let an operation recover if it misnavigates in the tree due to out-of-date information. In the concurrent B-link tree algorithm described by Sagiv <ref> [12] </ref>, insert operations place no more than one lock on the data structure at a time. Search operations start by placing an R (read) lock on the root and then searching the root to determine the next node to access. <p> Sagiv <ref> [12] </ref> describes garbage collection algorithms for underfull nodes. Lanin and Shasha [13], and Wang [17] describe an algorithm that leaves stubs in place of deleted nodes. We show how nodes can safely be removed from the tree due to the lack of shared memory.
Reference: [13] <author> V. Lanin and D. Shasha. </author> <title> A Symmetric Concurrent B-Tree Algorithm. </title> <booktitle> In 1986 Proceedings Fall Joint Computer Conference, </booktitle> <pages> pages 380-386, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: To prevent this kind of process overtaking many algorithms have their operations use lock coupling to block independent operations [11, 22]. An operation traverses the tree by obtaining the appropriate lock on the child before releasing the lock it holds on the parent. B-link trees <ref> [1, 12, 13] </ref> eliminate the need for lock coupling. If the wrong node is reached at any stage the operation is able to recover. This reduces the number of locks that must be held concurrently and increases throughput. <p> In Section 2 we describe the dB-tree. In Section 3 we show how the dE-tree can be built from the dB-tree. Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree <ref> [1, 12, 13] </ref>. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms [20, 21, 29]. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. <p> Sagiv [12] describes garbage collection algorithms for underfull nodes. Lanin and Shasha <ref> [13] </ref>, and Wang [17] describe an algorithm that leaves stubs in place of deleted nodes. We show how nodes can safely be removed from the tree due to the lack of shared memory. Our distributed B-tree algorithm builds on the concurrent B-link algorithms.
Reference: [14] <author> D. Shasha, V. Lanin, and J. Schmidt. </author> <title> An Analytical Model for the Performace of Concurrent B Tree Algorithms. Ultracomputer note 124, </title> <address> New York University, </address> <month> July </month> <year> 1987. </year>
Reference: [15] <author> A. Biliris. </author> <title> Operation specific locking in B-trees. </title> <booktitle> In Symposiun on the Principles of Database Systems, </booktitle> <pages> pages 159-169. </pages> <publisher> ACM SIGACT-SIGART-SIGMOD, </publisher> <year> 1987. </year>
Reference: [16] <author> C. Mohan and F. Levine. ARIES/IM: </author> <title> An efficient and high concurrency index management method using write-ahead logging. </title> <type> Research Report RJ 6864, </type> <institution> IBM, </institution> <year> 1989. </year>
Reference: [17] <author> P. Wang. </author> <title> An in-depth analysis of concurrent B-tree algorithms. </title> <type> Master's thesis, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Similarly, for message-passing architectures, the processor on which a node resides will receive messages from every processor trying to access the node. Resource contention is again most serious for the higher levels in the search tree. Node replication <ref> [17] </ref> reduces contention but requires a coherence protocol to maintain consistency. We present two algorithms for managing replicated copies of tree nodes. Associated with the contention issue is the problem of process overtaking. <p> If the wrong node is reached at any stage the operation is able to recover. This reduces the number of locks that must be held concurrently and increases throughput. We use the B-link tree as a base for the dB-tree. 1.2 Previous Work Wang and Weihl <ref> [17, 18] </ref> have proposed that parallel B-trees be stored using Multi-version Memory, a special cache coherence algorithm for linked data structures. Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging [23, 24] in transaction systems). <p> Sagiv [12] describes garbage collection algorithms for underfull nodes. Lanin and Shasha [13], and Wang <ref> [17] </ref> describe an algorithm that leaves stubs in place of deleted nodes. We show how nodes can safely be removed from the tree due to the lack of shared memory. Our distributed B-tree algorithm builds on the concurrent B-link algorithms. An example dB-tree (distributed B-tree) is shown in figure 2. <p> Both use the idea of a birth processor, the processor which is responsible for managing the replication of the object. The first algorithm <ref> [17] </ref> is simply to send all modifying suboperations on a node to the birth processor, and the birth processor decides on a serial order in which the operations are to be performed. <p> gives up its copy of the node, it must elect a new birth processor, and distribute the decision to all owners on the node (a synchronizing operation). 2.4 Parent Pointers Requiring every node to contain a pointer to its parent simplifies the task of finding parents and splitting the root <ref> [28, 17] </ref>. Parent pointers can be maintained in the following manner: When a node splits, some of the node's children are transferred to the new sibling.
Reference: [18] <author> W.E. Weihl and P. Wang. </author> <title> Multi-version memory: Software cache management for concurrent B-trees. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 650-655, </pages> <year> 1990. </year>
Reference-contexts: If the wrong node is reached at any stage the operation is able to recover. This reduces the number of locks that must be held concurrently and increases throughput. We use the B-link tree as a base for the dB-tree. 1.2 Previous Work Wang and Weihl <ref> [17, 18] </ref> have proposed that parallel B-trees be stored using Multi-version Memory, a special cache coherence algorithm for linked data structures. Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging [23, 24] in transaction systems).
Reference: [19] <author> V. Lanin, D. Shasha, and J. Schmidt. </author> <title> An analytical model for the performance of concurrent B-tree algorithms. </title> <type> Technical report, </type> <institution> Ultracomputer Laboratory, </institution> <address> New York University, </address> <year> 1987. </year> <note> 18 REFERENCES </note>
Reference-contexts: All concurrent search tree algorithms share the problem of managing contention. Concurrency control is required to ensure that two or more independent processes accessing a B-tree do not interfere with each other. A common approach is to associate a read/write lock with every node in the search tree <ref> [19] </ref>. This causes data contention as writers block incoming writers and readers, and readers block incoming writers. The contention is severe when it occurs at higher levels in the search tree, particularly at the root (often termed a root bottleneck [20, 21]). Similar problems are caused by resource contention.
Reference: [20] <author> T. Johnson and D. Shasha. </author> <title> A Framework for the Performance Analysis of Concurrent B-tree Algorithms. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Principles of Database Systems, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: This causes data contention as writers block incoming writers and readers, and readers block incoming writers. The contention is severe when it occurs at higher levels in the search tree, particularly at the root (often termed a root bottleneck <ref> [20, 21] </ref>). Similar problems are caused by resource contention. In a shared-memory architecture all of the processes trying to access the same tree node will access the same memory module on the machine. <p> Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree [1, 12, 13]. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms <ref> [20, 21, 29] </ref>. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. In a B-link tree, every node contains a pointer to its right neighbor.
Reference: [21] <author> T. Johnson. </author> <title> The Performance of Concurrent Data Structure Algorithms. </title> <type> PhD thesis, </type> <institution> NYU Dept. of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: This causes data contention as writers block incoming writers and readers, and readers block incoming writers. The contention is severe when it occurs at higher levels in the search tree, particularly at the root (often termed a root bottleneck <ref> [20, 21] </ref>). Similar problems are caused by resource contention. In a shared-memory architecture all of the processes trying to access the same tree node will access the same memory module on the machine. <p> Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree [1, 12, 13]. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms <ref> [20, 21, 29] </ref>. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. In a B-link tree, every node contains a pointer to its right neighbor.
Reference: [22] <author> L.J. Guibas and R. Sedgewick. </author> <title> A dichromatic framework for balancing trees. </title> <booktitle> In Proceedings of the 19th Annual IEEE Computer Society Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 8-21, </pages> <year> 1978. </year>
Reference-contexts: The second process can then update the node in such a fashion as to cause the first process to lock the wrong node when it eventually acquires the lock. To prevent this kind of process overtaking many algorithms have their operations use lock coupling to block independent operations <ref> [11, 22] </ref>. An operation traverses the tree by obtaining the appropriate lock on the child before releasing the lock it holds on the parent. B-link trees [1, 12, 13] eliminate the need for lock coupling. If the wrong node is reached at any stage the operation is able to recover.
Reference: [23] <author> W.E. Weihl. </author> <title> The impact or recovery on concurrency control. </title> <type> Technical Report MIT/LCS/TM-382b, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging <ref> [23, 24] </ref> in transaction systems). Our algorithm permits concurrent updates on replicated nodes (analogous to transition logging [23, 24]). Ellis [25] has proposed algorithms for a distributed hash table. The directories of the table are replicated among several sites, and the data buckets are distributed among several sites. <p> Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging <ref> [23, 24] </ref> in transaction systems). Our algorithm permits concurrent updates on replicated nodes (analogous to transition logging [23, 24]). Ellis [25] has proposed algorithms for a distributed hash table. The directories of the table are replicated among several sites, and the data buckets are distributed among several sites.
Reference: [24] <author> P.A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency control and recovery in database systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging <ref> [23, 24] </ref> in transaction systems). Our algorithm permits concurrent updates on replicated nodes (analogous to transition logging [23, 24]). Ellis [25] has proposed algorithms for a distributed hash table. The directories of the table are replicated among several sites, and the data buckets are distributed among several sites. <p> Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging <ref> [23, 24] </ref> in transaction systems). Our algorithm permits concurrent updates on replicated nodes (analogous to transition logging [23, 24]). Ellis [25] has proposed algorithms for a distributed hash table. The directories of the table are replicated among several sites, and the data buckets are distributed among several sites.
Reference: [25] <author> C.S. Ellis. </author> <title> Distributed data structures: A case study. </title> <journal> IEEE Transactions on Computing, </journal> <volume> C-34(12):1178-1185, </volume> <year> 1985. </year>
Reference-contexts: Multi-version Memory permits only a single update to occur on a replicated node at any point in time (analogous to value logging [23, 24] in transaction systems). Our algorithm permits concurrent updates on replicated nodes (analogous to transition logging [23, 24]). Ellis <ref> [25] </ref> has proposed algorithms for a distributed hash table. The directories of the table are replicated among several sites, and the data buckets are distributed among several sites. <p> Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute [31, 32, 33, 34, 35], but in the context of a transaction processing system. Our algorithms are similar to those described by Ellis <ref> [25] </ref> to maintain the replicated directories of the distributed hash tables in so that it is not always necessary that all 2.2 Integrating Concurrency Control with Replica Coherency 11 suboperations are performed in the same order at all nodes. For example, insert suboperations may be performed out-of order. <p> Data-balancing is necessary in order to distribute the request load, prevent processors from being required to devote a disproportionate share of their memory resources to the dictionary, and prevent out-of-storage errors when storage is available on other processors. Ellis' algorithm <ref> [25] </ref> performs data-balancing whenever a processor runs out of storage.
Reference: [26] <author> K.Z Gilon and D. Peleg. </author> <title> Compact deterministic distributed dictionaries. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 81-94. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: The distribution of updates can be limited in a distributed B-tree because it is a multilevel index structure. Also, the B-link tree is a very flexible data structure in which more sophisticated operations, such as range queries, can be easily implemented. Peleg <ref> [26, 27] </ref> has proposed several structures for implementing a distributed dictionary. The concern of these papers is the message complexity of access and data balancing. However, the issues of efficiency and concurrent access are not addressed. <p> Ellis' algorithm [25] performs data-balancing whenever a processor runs out of storage. Peleg <ref> [26, 27] </ref> has studied the issue of data-balancing in distributed dictionaries from a complexity point of view, requiring that no processor store more than O (M=N ) keys, where M is the number of keys and N is the number of processors.
Reference: [27] <author> D. Peleg. </author> <title> Distributed data structures: A complexity oriented view. </title> <booktitle> In Fourth Int's Workshop on Distributed Algorithms, </booktitle> <pages> pages 71-89, </pages> <address> Bari, Italy, </address> <year> 1990. </year>
Reference-contexts: The distribution of updates can be limited in a distributed B-tree because it is a multilevel index structure. Also, the B-link tree is a very flexible data structure in which more sophisticated operations, such as range queries, can be easily implemented. Peleg <ref> [26, 27] </ref> has proposed several structures for implementing a distributed dictionary. The concern of these papers is the message complexity of access and data balancing. However, the issues of efficiency and concurrent access are not addressed. <p> Ellis' algorithm [25] performs data-balancing whenever a processor runs out of storage. Peleg <ref> [26, 27] </ref> has studied the issue of data-balancing in distributed dictionaries from a complexity point of view, requiring that no processor store more than O (M=N ) keys, where M is the number of keys and N is the number of processors.
Reference: [28] <author> A. Colbrook, E.A. Brewer, C.N. Dellarocas, and W.E. Weihl. </author> <title> An algorithm for concurrent search trees. </title> <booktitle> In Proceedings of the 20th International Conference on Parallel Processing, </booktitle> <pages> pages III138-III141, </pages> <year> 1991. </year>
Reference-contexts: Peleg [26, 27] has proposed several structures for implementing a distributed dictionary. The concern of these papers is the message complexity of access and data balancing. However, the issues of efficiency and concurrent access are not addressed. Colbrook et al. <ref> [28] </ref> have proposed a pipelined distributed B-tree, where each level of the tree is maintained by a different processor. The parallelism that can be obtained from this implementation is limited by the number of levels in the tree, and the distributed tree is not data-balanced. <p> gives up its copy of the node, it must elect a new birth processor, and distribute the decision to all owners on the node (a synchronizing operation). 2.4 Parent Pointers Requiring every node to contain a pointer to its parent simplifies the task of finding parents and splitting the root <ref> [28, 17] </ref>. Parent pointers can be maintained in the following manner: When a node splits, some of the node's children are transferred to the new sibling.
Reference: [29] <author> V. Srinivasan and M. Carey. </author> <title> Performance of B-tree concurrency control algorithms. </title> <type> Technical Report Computer Sciences Technical Report 999, </type> <institution> University of Wisconson-Madison, </institution> <year> 1991. </year>
Reference-contexts: Finally, conclusions are drawn in Section 4. 2 The dB-tree, a Concurrent Distributed B-tree As a base for our distributed B-tree, we use the concurrent B-link tree [1, 12, 13]. The B-link tree algorithms have been found to have the highest performance among all existing concurrent B-tree algorithms <ref> [20, 21, 29] </ref>. Restructuring operations on B-link trees are performed one node at a time, so that the algorithms can be easily translated to a distributed environment. In a B-link tree, every node contains a pointer to its right neighbor.
Reference: [30] <author> D.K. Gifford. </author> <title> Weighted voting for replicated data. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Operating System Principles, </booktitle> <pages> pages 150-150. </pages> <publisher> ACM, </publisher> <year> 1979. </year>
Reference-contexts: We can achieve the atomic updates by requiring that modifying suboperations lock every copy of the modified node before performing the update and block all reads and updates on the node, by using one of the well-known algorithms for managing replicated data <ref> [2, 30] </ref>. However, we can maintain our replicated nodes with far less synchronization and overhead. First, observe that it is not necessary to distribute the contents of the node on every modification, it is only necessary to distribute the modification itself.
Reference: [31] <author> M. Herlihy. </author> <title> A quorum-consensus replication method for abstract data types. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(1) </volume> <pages> 32-53, </pages> <year> 1986. </year>
Reference-contexts: Second, the tree is never left in an incorrect state, so that search suboperations do not need to be blocked. Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute <ref> [31, 32, 33, 34, 35] </ref>, but in the context of a transaction processing system.
Reference: [32] <author> T.P. Ng. </author> <title> Using histories to implement atomic objects. </title> <journal> ACM Transactions of Computer Systems, </journal> <volume> 7(4) </volume> <pages> 360-393, </pages> <year> 1989. </year> <note> REFERENCES 19 </note>
Reference-contexts: Second, the tree is never left in an incorrect state, so that search suboperations do not need to be blocked. Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute <ref> [31, 32, 33, 34, 35] </ref>, but in the context of a transaction processing system.
Reference: [33] <author> W.E. Weihl and B. Liskov. </author> <title> Implementation of resilient, atomic data objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(2) </volume> <pages> 244-269, </pages> <year> 1985. </year>
Reference-contexts: Second, the tree is never left in an incorrect state, so that search suboperations do not need to be blocked. Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute <ref> [31, 32, 33, 34, 35] </ref>, but in the context of a transaction processing system.
Reference: [34] <author> P.M. Schwartz and A.Z. Spector. </author> <title> Synchronizing abstract data types. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 223-250, </pages> <year> 1984. </year>
Reference-contexts: Second, the tree is never left in an incorrect state, so that search suboperations do not need to be blocked. Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute <ref> [31, 32, 33, 34, 35] </ref>, but in the context of a transaction processing system.
Reference: [35] <author> W.E. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <year> 1988. </year>
Reference-contexts: Second, the tree is never left in an incorrect state, so that search suboperations do not need to be blocked. Third, many of the modifying suboperations commute. Several authors have studied the issue of concurrency control on abstract data types when some operations may commute <ref> [31, 32, 33, 34, 35] </ref>, but in the context of a transaction processing system.
Reference: [36] <author> K. Donovan. </author> <title> Performance of shared memory in a parallel computer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2) </volume> <pages> 253-256, </pages> <year> 1991. </year>
Reference-contexts: The problem with the simple approach is that the processors will not be data-balanced. Even if each insert is equally likely to be directed to each processor, the processor with the most keys will hold considerably more keys than the average <ref> [36] </ref>. Data-balancing is necessary in order to distribute the request load, prevent processors from being required to devote a disproportionate share of their memory resources to the dictionary, and prevent out-of-storage errors when storage is available on other processors.
Reference: [37] <author> Christos G. Cassandras, James F. Kurose, and Don Towsley. </author> <title> Resource contention management in parallel systems. </title> <institution> Radc-tr-89-48, Rome Air Development Center (RADC), </institution> <month> April </month> <year> 1989. </year>
Reference: [38] <author> F.C.H. Lin and R.M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13:32-38, </volume> <year> 1987. </year>
Reference: [39] <author> R.A. Finkel, M.H. Solomon, and M. Horowitz. </author> <title> Distributed algorithms for global structuring. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <pages> pages 455-460, </pages> <year> 1979. </year>
Reference: [40] <author> R.M. Bryant and R.A. Finkel. </author> <title> A stable distributed scheduling algorithm. </title> <booktitle> In Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <pages> pages 314-323, </pages> <year> 1981. </year>
Reference: [41] <author> L.M. Ni, C.W. Xu, and T.B. Genfreau. </author> <title> A distributed drafting algorithm for load balancing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11:32-38, </volume> <year> 1985. </year>
Reference: [42] <author> K. Hwang et al. </author> <title> A unix-based local computer network with load balancing. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 55-64, </pages> <year> 1982. </year>
Reference: [43] <author> E. Lumer and B.A. Huberman. </author> <title> Dynamics of resource allocation in distributed systems. </title> <note> Preprint (submitted to IEEE Transactions on SMC), </note> <institution> Xerox Palo Alto Research Center, </institution> <month> March </month> <year> 1990. </year>
References-found: 43

