URL: http://www-plateau.cs.berkeley.edu/ftp/pub/multimedia/papers/intelsc-mpeg-encode.ps.gz
Refering-URL: http://www-plateau.cs.berkeley.edu/ftp/pub/multimedia/papers/
Root-URL: 
Title: A Parallel Implementation of an MPEG1 Encoder: Faster Than Real-Time!  
Author: Ke Shen Lawrence A. Rowe and Edward J. Delp 
Address: West Lafayette, Indiana  Berkeley, California  
Affiliation: Computer Vision and Image Processing Laboratory School of Electrical Engineering Purdue University  Computer Science Division Department of Electrical Engineering and Computer Science University of California  
Abstract: In this paper we present an implementation of an MPEG1 encoder on the Intel Touchstone Delta and Intel Paragon parallel computers. We describe the unique aspects of mapping the algorithm onto the parallel machines and present several versions of the algorithms. We will show that I/O contention can be a bottleneck relative to performance. We will also describe how the Touchstone Delta and Paragon can be used to compress video sequences faster than real-time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> MPEG (Moving Pictures Expert Group), </editor> <title> Final Text for ISO/IEC 11172, Information technology | Coding of moving pictures and associated audio for digital storage media at up ot about 1.5 Mbit/s. </title> <address> ISO/IEC, </address> <year> 1993. </year>
Reference-contexts: 1. INTRODUCTION The Moving Pictures Experts Group (MPEG) has developed a standard, known as MPEG1, for the compression of digital video signals (and associated audio) at a data rate of 1.5 Mbits/s <ref> [1, 2] </ref>. The major features of the MPEG1 standard are that it utilizes discrete cosine transform (DCT) coding to remove spatial redundancy in a video frame (similar to JPEG [3]) and motion compensated prediction to remove temporal redundancy between video frames. <p> MPEG1 AND THE BERKELEY ENCODER 2.1 An Overview of MPEG1 The MPEG1 standard defines the syntax of the bit stream of a compressed digital video sequence <ref> [1, 2] </ref>. In other words, the decoder is defined and the implementation of the encoder is open to individual designs. MPEG divides the compressed video frames into three types: I pictures, P pictures and B pictures (Figure 1). from the reference frames. <p> However, there is usually little or no difference in quality if the original frames are used as reference frames <ref> [1] </ref>, while the use of original frames increases the execution speed because the decoded frames need not be computed. In the Berkeley Encoder, the Decode Server can be omitted if the original frames are used as reference frames.
Reference: [2] <author> D. Le Gall, </author> <title> "MPEG: A video compression standard for multimedia applications," </title> <journal> Communications of the ACM, </journal> <volume> vol. 34, no. 4, </volume> <pages> pp. 46-58, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: 1. INTRODUCTION The Moving Pictures Experts Group (MPEG) has developed a standard, known as MPEG1, for the compression of digital video signals (and associated audio) at a data rate of 1.5 Mbits/s <ref> [1, 2] </ref>. The major features of the MPEG1 standard are that it utilizes discrete cosine transform (DCT) coding to remove spatial redundancy in a video frame (similar to JPEG [3]) and motion compensated prediction to remove temporal redundancy between video frames. <p> MPEG1 AND THE BERKELEY ENCODER 2.1 An Overview of MPEG1 The MPEG1 standard defines the syntax of the bit stream of a compressed digital video sequence <ref> [1, 2] </ref>. In other words, the decoder is defined and the implementation of the encoder is open to individual designs. MPEG divides the compressed video frames into three types: I pictures, P pictures and B pictures (Figure 1). from the reference frames. <p> The first picture in a GOP must be an I picture (a GOP can contain one or more I pictures). The existence of GOPs facilitates the implementation of features such as random access, fast forward, or fast and normal reverse playback <ref> [2] </ref>. The image color space used in the MPEG1 is the YC r C b space, in which Y represents the luminance component and C r and C b represent the chrominance components, or the color difference [13].
Reference: [3] <author> W. B. Pennebaker and J. L. Mitchell, </author> <title> JPEG Still Image Data Compression Standard. Library of Congress Cataloging-in-Publication Data, </title> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1993. </year>
Reference-contexts: The major features of the MPEG1 standard are that it utilizes discrete cosine transform (DCT) coding to remove spatial redundancy in a video frame (similar to JPEG <ref> [3] </ref>) and motion compensated prediction to remove temporal redundancy between video frames. Motion compensated prediction is computationally intensive, however it provides a lower data rate than using just intraframe approaches for a fixed image quality.
Reference: [4] <author> K. L. Gong, </author> <title> "Parallel MPEG-1 video encoding," </title> <type> MS thesis, </type> <institution> Department of EECS, University of Cali-fornia at Berkeley, </institution> <month> May </month> <year> 1994. </year> <note> Issued as Technical Report 811. </note>
Reference-contexts: This immediately indicates that MPEG will require some effort at real-time implementation. Several approaches have been taken to the implementation of the MPEG1 standard. A software MPEG1 encoder has been developed at the University of California at Berkeley <ref> [4] </ref> that can compress video at a rate of 1.2 frames per second for images with spatial resolution of 352x288 (CIF) on a Sun SPARCstation 10. <p> One of the methods that can be used to increase the execution speed is to distribute the compression task to a group or cluster of workstations interconnected via a LAN or the Internet. The Berkeley MPEG Encoder can be used in this manner <ref> [4] </ref>. For example, on 6 Sun workstations connected by an ethernet a typical execution rate of 4.7 frames per second can be obtained. We will discuss the Berkeley Encoder in more This work was supported in part by the Advanced Research Projects Agency under contract DABT63-92-C-0022. <p> This is the approach used in the MVP and the VCP [7, 8]. The other approach is to parallelize in the temporal dimension. Sections of the video sequence can be assigned to different processors and compressed in parallel. The Berkeley Encoder has adopted the latter approach <ref> [4] </ref>, which is also the method we used on the Delta and Paragon. The Berkeley Encoder is able to run on several interconnected workstations by distributing the video data via a network file system (NFS) and passing messages through TCP/IP sockets [4]. <p> The Berkeley Encoder has adopted the latter approach <ref> [4] </ref>, which is also the method we used on the Delta and Paragon. The Berkeley Encoder is able to run on several interconnected workstations by distributing the video data via a network file system (NFS) and passing messages through TCP/IP sockets [4]. Figure 4 shows the processing architecture used when encoding a video sequence in parallel. Since only MPEG encoded frames are available to the decoder, the decoder has to use decoded frames as reference frames to predict the next frame.
Reference: [5] <author> Y. Yu and D. Anastassiou, </author> <title> "Software implementation of MPEG-II video encoding using socket programming in LAN," </title> <booktitle> Proceedings of SPIE Digital Video Compression on Personal Computers: Algorithms and Technologies, </booktitle> <volume> vol. 2187, </volume> <month> February, </month> <pages> 7-8 1994, </pages> <address> San Jose, California, </address> <pages> pp. 229-240. </pages>
Reference-contexts: The computing resources of the Concurrent Supercomputing Consortium at the California Institute of Technology were used for some of the experiments presented in this paper. detail in Section 2. An software-based parallel MPEG2 video encoder has also been reported <ref> [5] </ref>. However neither of these software encoders have reported real-time performance (30 frames per second). Another approach to increase the computational speed is by hardware acceleration. One example of hardware implementation is the Texas Instruments' Multimedia Video Processor (MVP) [6].
Reference: [6] <author> R. J. Gove, </author> <title> "The MVP: a highly-integrated video compression chip," </title> <booktitle> Proceedings of IEEE Data Compression Conference, </booktitle> <address> March, 28-31 1994, Snowbird, Utah, </address> <pages> pp. 215-224. </pages>
Reference-contexts: An software-based parallel MPEG2 video encoder has also been reported [5]. However neither of these software encoders have reported real-time performance (30 frames per second). Another approach to increase the computational speed is by hardware acceleration. One example of hardware implementation is the Texas Instruments' Multimedia Video Processor (MVP) <ref> [6] </ref>. The MVP is a single chip MIMD multiprocessor with crossbar shared memory. Real-time compression of CIF (320x240) video sequences has been reported using the MVP [7]. Another example is C-Cube's VideoRISC Compression Processor (VCP) [8]. Nine VCPs are required to encode CCIR-601 images using MPEG2 in real-time [9].
Reference: [7] <author> W. Lee, R. J. Gove, and Y. Kim, </author> <title> "Real-time MPEG video compression using the MVP," </title> <booktitle> Proceedings of IEEE Data Compression Workshop, </booktitle> <address> April 1994, Snowbird, Utah. </address>
Reference-contexts: One example of hardware implementation is the Texas Instruments' Multimedia Video Processor (MVP) [6]. The MVP is a single chip MIMD multiprocessor with crossbar shared memory. Real-time compression of CIF (320x240) video sequences has been reported using the MVP <ref> [7] </ref>. Another example is C-Cube's VideoRISC Compression Processor (VCP) [8]. Nine VCPs are required to encode CCIR-601 images using MPEG2 in real-time [9]. In both examples, hardware parallel approaches are used to obtain real-time performance [10]. Real-time performance is necessary for many applications such as cable television and video conferencing. <p> One approach is to parallelize the encoding operation in the spatial directions, which is similar to most parallel image processing algorithms. The existence of macroblocks or slices in MPEG1 facilitates this type of parallelism. This is the approach used in the MVP and the VCP <ref> [7, 8] </ref>. The other approach is to parallelize in the temporal dimension. Sections of the video sequence can be assigned to different processors and compressed in parallel. The Berkeley Encoder has adopted the latter approach [4], which is also the method we used on the Delta and Paragon.
Reference: [8] <author> P. Wayner, </author> <title> "Digital video goes real-time," </title> <journal> Byte, </journal> <volume> vol. 19, no. 1, </volume> <pages> pp. 107-112, </pages> <month> January </month> <year> 1994. </year> <title> [9] "C-Cube Microsystems product catalog," </title> <booktitle> Spring, </booktitle> <year> 1994. </year>
Reference-contexts: One example of hardware implementation is the Texas Instruments' Multimedia Video Processor (MVP) [6]. The MVP is a single chip MIMD multiprocessor with crossbar shared memory. Real-time compression of CIF (320x240) video sequences has been reported using the MVP [7]. Another example is C-Cube's VideoRISC Compression Processor (VCP) <ref> [8] </ref>. Nine VCPs are required to encode CCIR-601 images using MPEG2 in real-time [9]. In both examples, hardware parallel approaches are used to obtain real-time performance [10]. Real-time performance is necessary for many applications such as cable television and video conferencing. However, some applications may require "faster-than-real-time" performance. <p> One approach is to parallelize the encoding operation in the spatial directions, which is similar to most parallel image processing algorithms. The existence of macroblocks or slices in MPEG1 facilitates this type of parallelism. This is the approach used in the MVP and the VCP <ref> [7, 8] </ref>. The other approach is to parallelize in the temporal dimension. Sections of the video sequence can be assigned to different processors and compressed in parallel. The Berkeley Encoder has adopted the latter approach [4], which is also the method we used on the Delta and Paragon.
Reference: [10] <author> K. Shen, G. W. Cook, L. H. Jamieson, and E. J. Delp, </author> <title> "An overview of parallel processing approaches to image compression," </title> <booktitle> Proceedings of the SPIE Conference on Image and Video Compression, </booktitle> <volume> vol. </volume> <pages> 2186, </pages> <address> February 1994, San Jose, California, </address> <pages> pp. 197-208. </pages>
Reference-contexts: Real-time compression of CIF (320x240) video sequences has been reported using the MVP [7]. Another example is C-Cube's VideoRISC Compression Processor (VCP) [8]. Nine VCPs are required to encode CCIR-601 images using MPEG2 in real-time [9]. In both examples, hardware parallel approaches are used to obtain real-time performance <ref> [10] </ref>. Real-time performance is necessary for many applications such as cable television and video conferencing. However, some applications may require "faster-than-real-time" performance. For example, suppose one wanted to convert an analog video library that contains 10,000 movie titles into a digital library.
Reference: [11] <institution> Intel Supercomputer Systems Division, Intel Corporation, Intel Paragon XP/S Technical Summary, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: In this paper we explore the use of high performance parallel computers to compress video sequences. We have implemented a largely modified version of the Berkeley MPEG1 encoder on the Intel Touchstone Delta and Paragon parallel computers <ref> [11, 12] </ref>. 2. MPEG1 AND THE BERKELEY ENCODER 2.1 An Overview of MPEG1 The MPEG1 standard defines the syntax of the bit stream of a compressed digital video sequence [1, 2]. In other words, the decoder is defined and the implementation of the encoder is open to individual designs. <p> The file system distributes file blocks to all available disks using algorithms for reading and writing that allow several PEs to use the concurrent disks simultaneously [12]. The Intel Paragon XP/S supercomputer is a distributed memory parallel computer <ref> [11] </ref>. It has a similar architecture to that of the Delta and uses i860 XP processors which operate at 50 MHz and have peak floating point performance of 75 Mflops double-precision or 100 Mflops single-precision (Figure 6). The mesh is connected to Parallel File Systems (PFS). <p> In the CFS or PFS, I/O nodes and the mesh networks of the Touchstone Delta and the Paragon are optimized for operations such as opening a file (parallel mode), reading/writing a large amount of data from/to the file and then closing the file <ref> [12, 11] </ref>. In the Berkeley Encoder each file contains a single original frame at the input and a single compressed frame at the output. Hence, a large number of files have to be opened, read/written or closed simultaneously.
Reference: [12] <institution> Intel Supercomputer Systems Division, Intel Corporation, </institution> <note> A Touchstone DELTA System Description, February 26 1991. </note>
Reference-contexts: In this paper we explore the use of high performance parallel computers to compress video sequences. We have implemented a largely modified version of the Berkeley MPEG1 encoder on the Intel Touchstone Delta and Paragon parallel computers <ref> [11, 12] </ref>. 2. MPEG1 AND THE BERKELEY ENCODER 2.1 An Overview of MPEG1 The MPEG1 standard defines the syntax of the bit stream of a compressed digital video sequence [1, 2]. In other words, the decoder is defined and the implementation of the encoder is open to individual designs. <p> PARALLEL IMPLEMENTATION 3.1 Parallel Systems We have implemented a modified version of the Berkeley MPEG1 Encoder on the Intel Touchstone Delta and Intel Paragon. The Intel Touchstone Delta consists of 512 (16x32) Numerical Nodes (Figure 5) <ref> [12] </ref>. Each node is an Intel i860 processor which operates at 40 MHz with 16 Mbytes of memory and has a peak speed of 60 Mflops double-precision or 80 Mflops single-precision. The overall peak speed of the Touchstone Delta is 32 Gflops. <p> The CFS consists of 64 disks which are served by 32 I/O nodes, each of which serves two disks. The file system distributes file blocks to all available disks using algorithms for reading and writing that allow several PEs to use the concurrent disks simultaneously <ref> [12] </ref>. The Intel Paragon XP/S supercomputer is a distributed memory parallel computer [11]. It has a similar architecture to that of the Delta and uses i860 XP processors which operate at 50 MHz and have peak floating point performance of 75 Mflops double-precision or 100 Mflops single-precision (Figure 6). <p> In the CFS or PFS, I/O nodes and the mesh networks of the Touchstone Delta and the Paragon are optimized for operations such as opening a file (parallel mode), reading/writing a large amount of data from/to the file and then closing the file <ref> [12, 11] </ref>. In the Berkeley Encoder each file contains a single original frame at the input and a single compressed frame at the output. Hence, a large number of files have to be opened, read/written or closed simultaneously.
Reference: [13] <author> K. Jack, </author> <title> Video Demystified: A Handbook for the Digital Engineer. </title> <address> California: </address> <publisher> HighText Publications, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The image color space used in the MPEG1 is the YC r C b space, in which Y represents the luminance component and C r and C b represent the chrominance components, or the color difference <ref> [13] </ref>. The pictures are subsampled to the 4:1:1 format, i.e. the chrominance components are subsampled with respect to the luminance component by half in both vertical and horizontal directions [13]. <p> in which Y represents the luminance component and C r and C b represent the chrominance components, or the color difference <ref> [13] </ref>. The pictures are subsampled to the 4:1:1 format, i.e. the chrominance components are subsampled with respect to the luminance component by half in both vertical and horizontal directions [13]. A video frame is divided into non-overlapping blocks of pixels, known as macro-blocks, each of which contains 16x16 pixels of the luminance image, 8x8 pixels of the C r chrominance image and 8x8 pixels of the C b chrominance image. An 8x8 pixel array is known as a block. <p> The video source is stored in the YUV color space, which can be transformed to the YC r C b color space, with the chrominance components, U and V, subsampled to 4:1:1 format <ref> [13] </ref>. In our experiments, we set the quantizer scales to be 8 for I pictures, 10 for P pictures and 25 for B pictures and the frame pattern to be IBBPBB. The logarithmic search scheme was used on half pixel displacements with the search range set to 10 pixels.
Reference: [14] <author> I. Pitas, ed., </author> <title> Parallel Algorithms for Digital Image Processing, Computer Vision and Neural Networks. </title> <address> Chichester, England: </address> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: we have used in this paper are the one at the Concurrent Supercomputing Consortium at Caltech which has 513 numeric node and the one at Purdue University which has 140 numeric nodes. 3.2 Parallel Implementation The parallel algorithms we developed use the Single Program Multiple Data (SPMD) model of parallelism <ref> [14] </ref>. We started by directly mapping the Berkeley Encoder to the Delta and Paragon.
References-found: 13

