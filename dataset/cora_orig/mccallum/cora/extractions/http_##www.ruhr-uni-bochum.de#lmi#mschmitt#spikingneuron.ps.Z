URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/spikingneuron.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: E-mail: fmaass,mschmittg@igi.tu-graz.ac.at,  
Title: On the Complexity of Learning for a Spiking Neuron (Extended Abstract) restricted spiking neuron that
Author: Wolfgang Maass and Michael Schmitt 
Web: http://www.cis.tu-graz.ac.at/igi/  
Address: Graz, Klosterwiesgasse 32/2, A-8010 Graz, Austria.  
Affiliation: Computer Science, Technische Universitat  
Note: Address: Institute for Theoretical  
Abstract: We introduce a simple model of a spiking neuron that, in addition to the weights that model the plasticity of synaptic strength, also has variable transmission delays between neurons as programmable parameters. For coding of input and output values two modes are taken into account: binary coding for the Boolean and analog coding for the real-valued domain. We investigate the complexity of learning for a single spiking neuron within the framework of PAC-learnability. With regard to sample complexity, we prove that the VC-dimension is fi(n log n) and, hence, strictly larger than that of a threshold gate. In particular, the lower bound holds for binary coding and even if all weights are kept fixed. The upper bound is valid for the case of analog coding if weights and delays are programmable. With regard to computational complexity, we show that there is no polynomial-time PAC-learning algorithm, unless RP = NP, for a quite The results demonstrate that temporal coding has a surprisingly large impact on the complexity of learning for single neurons. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agmon-Snir and I. Segev. </author> <title> Signal delay and input synchronization in passive dendritic structures. </title> <journal> Journal of Neurophysiology, </journal> <volume> 70 </volume> <pages> 2066-2085, </pages> <year> 1993. </year>
Reference-contexts: For analog coding we assume that a i encodes a real number t i 2 <ref> [0; 1] </ref> by firing at time t i . The output value of v is the time t v when it fires (or t v T for a suitable constant T if one wants to scale the real-valued output of v into a specific range such as [0; 1]). <p> t i 2 <ref> [0; 1] </ref> by firing at time t i . The output value of v is the time t v when it fires (or t v T for a suitable constant T if one wants to scale the real-valued output of v into a specific range such as [0; 1]). In case that v does not fire, we assume that this encodes some fixed analog output t 0 (e.g. t 0 = 0). We will consider both types of coding in this article. <p> One well-known mechanism is the selection of a few synapses out of an initially very large set of synapses between two neurons. Some other biological mechanisms for changing the effective delay between two neurons are discussed in <ref> [1, 8] </ref>. Theorem 2.2 The VC-dimension of a spiking neuron with n variable delays as programmable parameters is (n log n). This holds even if the inputs are restricted to binary values and all weights are kept fixed.
Reference: [2] <author> M. Anthony and N. Biggs. </author> <title> Computational Learning Theory. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, </address> <year> 1992. </year>
Reference-contexts: the threshold were viewed as the only variable parameters of a spiking neuron, whereas the effect of variable delays has not been addressed. 1.3 OVERVIEW In this article we investigate the complexity of learning for a spiking neuron within the PAC-learning framework. (For detailed definitions we refer the reader to <ref> [2, 4, 20] </ref>.) In Section 2 we estimate the computational power and the sample complexity of the spiking neuron model defined above. For binary coding we give upper and lower bounds for the computational power in terms of several classes of Boolean functions. <p> In a similar way, NP-completeness can be shown for the case that the delays are allowed to take on values from a bounded set f0; : : : ; k 1g where k 3. The reduction is from GRAPH-k-COLORABILITY and is basically a modification of the reduction used in <ref> [2] </ref> for the AND of k threshold gates. Again, the weights and the threshold can also be kept fixed. Combining this with Theorem 3.1 we get the following result.
Reference: [3] <author> A. L. Blum and R. L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: Theorem 3.1 The consistency problem for a spiking neuron where each delay is 0 or 1 is NP-complete. The proof is by a reduction from 3SET-SPLITTING [6], a problem which was also used in <ref> [3] </ref> for intractability results concerning certain two-layer networks of threshold gates. In fact, the problem considered here seems to be closely related to the consistency problem for the AND of two threshold gates analyzed in [3]. <p> proof is by a reduction from 3SET-SPLITTING [6], a problem which was also used in <ref> [3] </ref> for intractability results concerning certain two-layer networks of threshold gates. In fact, the problem considered here seems to be closely related to the consistency problem for the AND of two threshold gates analyzed in [3]. However, their reduction cannot be used here in a straightforward manner (e.g., by flipping the labels to change the AND into an OR), because due to our Theorem 2.1 (e) the OR of two threshold gates is not equivalent to a spiking neuron with delays from f0; 1g.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: the threshold were viewed as the only variable parameters of a spiking neuron, whereas the effect of variable delays has not been addressed. 1.3 OVERVIEW In this article we investigate the complexity of learning for a spiking neuron within the PAC-learning framework. (For detailed definitions we refer the reader to <ref> [2, 4, 20] </ref>.) In Section 2 we estimate the computational power and the sample complexity of the spiking neuron model defined above. For binary coding we give upper and lower bounds for the computational power in terms of several classes of Boolean functions. <p> The exclusive-OR of two bits can be computed by an OR of two threshold gates but is not in S bb 2 . 2.2 LOWER BOUND FOR THE VC-DIMENSION The VC-dimension of the classes T bb n and T ab n is known to be n + 1 (see e.g. <ref> [4] </ref>). Furthermore, using known results about the pseudo-dimension (see [9]) it is easy to derive that the class T aa n has pseudo-dimension n+ 1. <p> The class S aa n has pseudo-dimen sion fi (n log n). In the proof of Theorem 2.4 we will use the following result which is a consequence of Theorem 2 in [5] 1 and Proposition A2.1 of <ref> [4] </ref>. Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. <p> They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of [5], m hyperplanes through the origin partition IR n into at most 2 P n1 m1 different regions. By Proposition A2.1 (iii) of <ref> [4] </ref>, 2 P n1 m1 2 (e (m 1)=(n 1)) (n1) for m n. Proof of Theorem 2.4. The proof is structured as follows: We first estimate the number of dichotomies of an arbitrary finite set S IR n of size m that can be computed by a spiking neuron.
Reference: [5] <author> T. M. </author> <title> Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 14 </volume> <pages> 326-334, </pages> <year> 1965. </year>
Reference-contexts: Corollary 2.5 The classes S bb n and S ab n have VC-dimension fi (n log n). The class S aa n has pseudo-dimen sion fi (n log n). In the proof of Theorem 2.4 we will use the following result which is a consequence of Theorem 2 in <ref> [5] </ref> 1 and Proposition A2.1 of [4]. Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of [5], m hyperplanes through the origin partition IR n <p> which is a consequence of Theorem 2 in <ref> [5] </ref> 1 and Proposition A2.1 of [4]. Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of [5], m hyperplanes through the origin partition IR n into at most 2 P n1 m1 different regions. By Proposition A2.1 (iii) of [4], 2 P n1 m1 2 (e (m 1)=(n 1)) (n1) for m n. Proof of Theorem 2.4.
Reference: [6] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Moreover, the proof shows that the result also holds when the weights and the threshold are kept fixed. Theorem 3.1 The consistency problem for a spiking neuron where each delay is 0 or 1 is NP-complete. The proof is by a reduction from 3SET-SPLITTING <ref> [6] </ref>, a problem which was also used in [3] for intractability results concerning certain two-layer networks of threshold gates. In fact, the problem considered here seems to be closely related to the consistency problem for the AND of two threshold gates analyzed in [3]. <p> Then there exists c 2 C; c = fu i ; u j ; u k g and b 2 f0; 1g such that fi (u i ) = fi (u j ) = fi (u k ) = b: 2 Strictly speaking, the restriction of SET-SPLITTING as defined in <ref> [6] </ref> allows that jcj 3. However, it is straightforward to define a reduction that avoids subsets of size 2. (i) If b = 1 then g (1 f2l1;2lg ) = 1 for each l 2 fi; j; kg.
Reference: [7] <author> W. Gerstner. </author> <title> Time structure of the activity in neural network models. </title> <journal> Phys. Rev. E, </journal> <volume> 51 </volume> <pages> 738-758, </pages> <year> 1995. </year>
Reference-contexts: This issue will not be relevant for the results of this article. The model is a simple version of a leaky integrate-and-fire neuron. In contrast to more complex models (see e.g., <ref> [19, 7, 13] </ref>) it models a pulse as a step function, rather than a continuous function of a similar shape. Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons [17].
Reference: [8] <author> W. Gerstner, R. Kempter, J. L. van Hemmen, and H. Wagner. </author> <title> A neuronal learning rule for sub-millisecond temporal coding. </title> <journal> Nature, </journal> <volume> 383 </volume> <pages> 76-78, </pages> <year> 1996. </year>
Reference-contexts: One well-known mechanism is the selection of a few synapses out of an initially very large set of synapses between two neurons. Some other biological mechanisms for changing the effective delay between two neurons are discussed in <ref> [1, 8] </ref>. Theorem 2.2 The VC-dimension of a spiking neuron with n variable delays as programmable parameters is (n log n). This holds even if the inputs are restricted to binary values and all weights are kept fixed.
Reference: [9] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: It is well known that the VC-dimension of a function class gives fairly tight bounds on the sample complexity (i.e. the number of training examples needed) for PAC-learning this class. According to <ref> [9] </ref>, these estimates of the sample complexity in terms of the VC-dimension hold even in the case when the training examples are generated by some arbitrary probability distribution ("agnostic PAC-learning"). In particular, these bounds remain valid when the training examples are not generated by a spiking neuron. <p> Furthermore, using known results about the pseudo-dimension (see <ref> [9] </ref>) it is easy to derive that the class T aa n has pseudo-dimension n+ 1. We show now that the VC-dimension of the classes S bb n , n and the pseudo-dimension of the class S aa n is strictly larger by a factor of (log n).
Reference: [10] <author> K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. </author> <title> Robust trainability of single neurons. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50 </volume> <pages> 114-125, </pages> <year> 1995. </year>
Reference-contexts: This implies that there is no efficient PAC-learning algorithm for these hypothesis classes unless RP = NP. The intractability results presented in this section have also consequences for the case of agnostic PAC-learning. According to known results <ref> [12, 10] </ref>, polynomial-time agnostic PAC-learning with some hypothesis class H can be done only if the minimizing disagreement problem for H is in RP. Now, for each hypothesis class H the consistency problem can easily be reduced to the minimizing disagreement problem.
Reference: [11] <author> M. Kearns, M. Li, and L. Valiant. </author> <title> Learning Boolean formulas. </title> <journal> Journal of the ACM, </journal> <volume> 41 </volume> <pages> 1298-1328, </pages> <year> 1994. </year>
Reference-contexts: If S bb n were PAC-learnable with some arbitrary polynomial-time computable hypothesis class, then this would imply the same result for DNF (which is one of the major open problems in computational learning theory). This follows from our Theorem 2.1 (d) in combination with the corresponding result in <ref> [11] </ref>. In this section we consider the complexity of PAC-learning when only hypotheses from S bb n may be used by the learner ("proper PAC-learning"). This appears to be the more adequate assumption for the analysis of learning for a single spiking neuron.
Reference: [12] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1992, </year> <pages> pp. 341-352. </pages>
Reference-contexts: This implies that there is no efficient PAC-learning algorithm for these hypothesis classes unless RP = NP. The intractability results presented in this section have also consequences for the case of agnostic PAC-learning. According to known results <ref> [12, 10] </ref>, polynomial-time agnostic PAC-learning with some hypothesis class H can be done only if the minimizing disagreement problem for H is in RP. Now, for each hypothesis class H the consistency problem can easily be reduced to the minimizing disagreement problem.
Reference: [13] <author> W. Maass. </author> <title> Fast sigmoidal networks via spiking neurons. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 279-304, </pages> <year> 1997. </year>
Reference-contexts: This issue will not be relevant for the results of this article. The model is a simple version of a leaky integrate-and-fire neuron. In contrast to more complex models (see e.g., <ref> [19, 7, 13] </ref>) it models a pulse as a step function, rather than a continuous function of a similar shape. Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons [17].
Reference: [14] <author> W. Maass. </author> <title> Networks of spiking neurons: the third generation of neural network models. Neural Networks, </title> <booktitle> to appear; extended abstract in Proceedings of the Seventh Australian Conference on Neural Networks, </booktitle> <address> Canberra, </address> <year> 1996, </year> <pages> pp. 1-10. </pages>
Reference-contexts: We will focus in this article on a simple version of the spiking neuron model ("spiking neurons of type A" in the terminology of <ref> [14] </ref>). This model allows us to study some fundamental new learning problems that arise in the context of computation with temporal coding. <p> Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons [17]. A spiking neuron of this type was called a "spiking neuron of type A" in <ref> [14] </ref>. In this article we will refer to it simply as a spiking neuron. 1.2 TEMPORAL CODING A spiking neuron may be viewed as a digital or analog computational element, depending on the type of temporal coding that is used.
Reference: [15] <author> S. Muroga. </author> <title> Threshold Logic and Its Applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Thus one derives the upper bound 2 n 2 +O (n log n) for the number of Boolean functions. This result is particularly interesting in the light of the fact that there are at most 2 n 2 many different functions in T bb n <ref> [15] </ref>. 3 COMPUTATIONAL COMPLEXITY OF PAC-LEARNING FOR A SPIKING NEURON In order to investigate the computational complexity of learning within the PAC framework one has to specify which class of hypotheses the learner may use.
Reference: [16] <author> S. Muroga and I. </author> <title> Toda. Lower bound of the number of threshold functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15 </volume> <pages> 805-806, </pages> <year> 1966. </year>
Reference-contexts: The bound (1) can also be used to estimate the number of Boolean functions that can be computed by a spiking neuron. Substituting m = 2 n yields the bound 2 O (n 2 ) . Combining this with the lower bound 2 (n 2 ) of <ref> [16] </ref> for T bb n and our Theorem 2.1 (c), we get the upper and lower bound almost matching. Corollary 2.7 There are 2 fi (n 2 ) many Boolean func tions computable by a spiking neuron with binary coding of the inputs.
Reference: [17] <author> A. Murray and L. Tarassenko. </author> <title> Analogue Neural VLSI: A Pulse Stream Approach. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: In contrast to more complex models (see e.g., [19, 7, 13]) it models a pulse as a step function, rather than a continuous function of a similar shape. Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons <ref> [17] </ref>. A spiking neuron of this type was called a "spiking neuron of type A" in [14].
Reference: [18] <editor> L. Schlafli. Gesammelte Mathematische Abhand-lungen I. </editor> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1950, </year> <pages> pp. 209-212. </pages>
Reference-contexts: There are (2n) 2 such hyperplanes for each fixed s. They partition IR n into regions of delay vectors that are equivalent 1 Cover attributes the first proof of this theorem to Schlafli <ref> [18] </ref>. with regard to the computation of the neuron on input vector s.
Reference: [19] <author> H. C. Tuckwell. </author> <title> Introduction to Theoretical Neuro-biology, vols. 1 and 2. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: This issue will not be relevant for the results of this article. The model is a simple version of a leaky integrate-and-fire neuron. In contrast to more complex models (see e.g., <ref> [19, 7, 13] </ref>) it models a pulse as a step function, rather than a continuous function of a similar shape. Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons [17].
Reference: [20] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: the threshold were viewed as the only variable parameters of a spiking neuron, whereas the effect of variable delays has not been addressed. 1.3 OVERVIEW In this article we investigate the complexity of learning for a spiking neuron within the PAC-learning framework. (For detailed definitions we refer the reader to <ref> [2, 4, 20] </ref>.) In Section 2 we estimate the computational power and the sample complexity of the spiking neuron model defined above. For binary coding we give upper and lower bounds for the computational power in terms of several classes of Boolean functions.
Reference: [21] <author> A. M. Zador and B. A. Pearlmutter. </author> <title> VC dimension of an integrate-and-fire neuron model. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 611-624, </pages> <year> 1996. </year>
Reference-contexts: Lower bounds for sample or learning complexity tend to be more difficult for binary coding, upper bounds tend to be more difficult for analog coding. Our results about the VC-dimension of a spiking neuron are complementary to those achieved in <ref> [21] </ref>.
References-found: 21

