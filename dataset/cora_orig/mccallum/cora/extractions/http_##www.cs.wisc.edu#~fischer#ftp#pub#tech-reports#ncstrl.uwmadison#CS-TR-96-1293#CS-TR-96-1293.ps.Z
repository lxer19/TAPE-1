URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1293/CS-TR-96-1293.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1293/
Root-URL: http://www.cs.wisc.edu
Email: fgviswana,larusg@cs.wisc.edu  
Phone: Telephone: (608) 262-2542  
Title: User-defined Reductions for Communication in Data-Parallel Languages  
Author: Guhan Viswanathan James R. Larus 
Date: January 12, 1996  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David H. Bailey. </author> <title> FFTs in External or Hierarchical Memory. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4(1):20-??, </volume> <month> March </month> <year> 1990. </year> <month> 12 </month>
Reference-contexts: As a result, the best C** version is 1.05x faster than the Splash version. 5.4 FFT The FFT kernel, from the SPLASH suite [24], implements a complex 1-D version of the radix p N six-step algorithm that minimizes interprocessor communication <ref> [1] </ref>. The input 1-D vector of size N is organized as a N fi N square matrix. The most expensive phases of the algorithm are the three matrix transpose phases, each of which involve all-to-all interprocessor communication.
Reference: [2] <author> Guy E. Blelloch. NESL: </author> <title> A Nested Data-Parallel Language (Version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Improved parallel programming languages could reduce the difficulty of programming parallel computers by making programs less error prone and less machine specific. One promising approach is data-parallel languages, such as HPF [9], C* [18], or NESL <ref> [2] </ref>, which provide high-level abstractions for the three key facets of parallel programming: concurrency, synchronization, and communication. In these languages, programmers express parallelism by invoking a parallel operation simultaneously on a collection of data. Synchronization is implicit in the division of a program into sequential and data-parallel phases. <p> More interesting and varied are the mechanisms for passing values from the parallel to the sequential computations. Vector-based languages, such as Connection Machine Lisp [23] or NESL <ref> [2] </ref>, provide vector permutation and mapping operations that rearrange a vector's data. Point-based languages, such as HPF [9] and C* [18], fl This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> An invocation simultaneously executes data-parallel tasks for each element of the collection. The compiler and run-time system map tasks to physical processors in a machine. Data collections are specified using aggregate types (e.g., arrays in HPF [9], vectors in NESL <ref> [2] </ref>, special class constructs in PC++ [14]). C** overloads the class definition mechanism of C++ to define data collections that are called Aggregates. For example, Figure 1 declares a two-dimensional collection of floating point values whose size is specified when Grid objects are created.
Reference: [3] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The bulk reduction improves significantly over the simple reduction (1.3x) and is 1.11x faster than the optimized Hybrid version. 5.2 EM3D EM3D models the propagation of electromagnetic waves through objects in three dimensions <ref> [3] </ref>. The problem is formulated a bipartite graph of H nodes representing magnetic fields and E nodes representing electric fields, with directed edges between H nodes and E nodes.
Reference: [4] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: These programs use transparent shared memory as a basis, but communicate select data structures through custom shared-memory or message-passing protocols. For three programs, including DSMC and Moldyn, Mukherjee et al. demonstrated that this approach compares favorably with the well-known Maryland CHAOS library <ref> [4] </ref> for irregular applications. Falsafi et al. showed that the best optimized version of EM3D ran faster than a comparable message-passing program [5]. The other two applications (Water and FFT) are transparent shared memory programs from the Stanford SPLASH suite [24].
Reference: [5] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 380-389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Table 2 gives a high-level description of the benchmarks, including the input data sets. For three applications, DSMC, Moldyn, and EM3D, we compared against the best optimized programs previously written (by others) using a hybrid of shared-memory and message-passing techniques <ref> [5, 16] </ref>. These programs use transparent shared memory as a basis, but communicate select data structures through custom shared-memory or message-passing protocols. For three programs, including DSMC and Moldyn, Mukherjee et al. demonstrated that this approach compares favorably with the well-known Maryland CHAOS library [4] for irregular applications. <p> For three programs, including DSMC and Moldyn, Mukherjee et al. demonstrated that this approach compares favorably with the well-known Maryland CHAOS library [4] for irregular applications. Falsafi et al. showed that the best optimized version of EM3D ran faster than a comparable message-passing program <ref> [5] </ref>. The other two applications (Water and FFT) are transparent shared memory programs from the Stanford SPLASH suite [24]. We ran these programs on Blizzard, a fine-grain distributed shared memory system that runs on the CM-5 [20]. <p> In the first part of a time step, E nodes send their values (using reductions) to H nodes, where they are collected and combined. The defered reduction implementation mimics the producer-consumer data movement pattern of the program, which is essential to good performance <ref> [5] </ref>. The Hybrid version of EM3D uses a custom update protocol to transfer data in the producer-consumer pattern. Figures 5 compares the relative speeds of the Hybrid version with the simple and bulk-reduction C** versions. The bulk-reduction optimization improves the execution time by 6.76x.
Reference: [6] <institution> High Performance Fortran Forum. HPF-2 Scope of Activities and Motivating Applications, </institution> <month> November </month> <year> 1994. </year> <note> Available at ftp://hpsl.cs.umd.edu/pub/hpf bench/hpf2.ps. </note>
Reference-contexts: To remedy the problem, they introduced the sendToQueue reduction operator that is similar to append. Sharma et al. [21] used the intrinsic list operator to efficiently execute the particle-in-cell application DSMC. Several applications in the HPF-2 motivating applications suite <ref> [6] </ref> note their requirement for user-defined reductions. This paper describes a general mechanism supporting powerful reduction operators. Reductions are a feature in most, if not all, data-parallel languages.
Reference: [7] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Ulrich Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report CRPC-TR900749, </type> <institution> Centre for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: In addition, user-defined reductions offer a natural way to express many message-passing optimizations, such as update protocols and bulk data transfer, in a shared-memory program. Some languages include user-defined reductions, such as Connection Machine Lisp [23], Paralation Lisp [19], and Fortran D <ref> [7] </ref>. We are unaware of published descriptions of implementations on MIMD machines. This paper describes an efficient implementation for user-defined reductions and contains experiments that demonstrate that user-defined reductions both simplify writing parallel programs and improve the performance of programs written in a high-level parallel language. <p> If a variable target is the target of a reduction assignment, the language must specify the value seen by subsequent accesses to target by the task. Three approaches are possible: 1. The language may prohibit accesses to target, except as a reduction target, as does Fortran D <ref> [7] </ref>. Erroneous accesses can be identified syntactically. This approach allows the runtime system to defer updating the target. However, syntactic analysis may not identify all erroneous accesses, particularly those involving arrays or pointers. 2. The language may retain the old value of target after a reduction. <p> This paper describes a general mechanism supporting powerful reduction operators. Reductions are a feature in most, if not all, data-parallel languages. A few of these languages allow user-defined functions for reduction operations (e.g., Connection Machine Lisp [23], Paralation Lisp [19], and Fortran D <ref> [7] </ref>). However, we are unaware of papers describing implementations of user-defined reductions on parallel machines. 7 Conclusion Data-parallel languages mitigate the difficulty of parallel programming by providing high-level abstractions for concurrency, synchronization and communication.
Reference: [8] <author> Philip J. Hatcher and Michael J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: For example, Dataparallel C adds a tournament operator <ref> [8] </ref> to carry along an extra value in max-reductions. Also, in comparing the message-passing and data-parallel paradigms, Klaiber et al. [12] noted the inefficiency in expressing many-to-many communication in C*. To remedy the problem, they introduced the sendToQueue reduction operator that is similar to append.
Reference: [9] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The difficulty of programming parallel computers is now, by far, the largest obstacle to their widespread use. Improved parallel programming languages could reduce the difficulty of programming parallel computers by making programs less error prone and less machine specific. One promising approach is data-parallel languages, such as HPF <ref> [9] </ref>, C* [18], or NESL [2], which provide high-level abstractions for the three key facets of parallel programming: concurrency, synchronization, and communication. In these languages, programmers express parallelism by invoking a parallel operation simultaneously on a collection of data. <p> More interesting and varied are the mechanisms for passing values from the parallel to the sequential computations. Vector-based languages, such as Connection Machine Lisp [23] or NESL [2], provide vector permutation and mapping operations that rearrange a vector's data. Point-based languages, such as HPF <ref> [9] </ref> and C* [18], fl This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> An invocation simultaneously executes data-parallel tasks for each element of the collection. The compiler and run-time system map tasks to physical processors in a machine. Data collections are specified using aggregate types (e.g., arrays in HPF <ref> [9] </ref>, vectors in NESL [2], special class constructs in PC++ [14]). C** overloads the class definition mechanism of C++ to define data collections that are called Aggregates. For example, Figure 1 declares a two-dimensional collection of floating point values whose size is specified when Grid objects are created.
Reference: [10] <author> W. Daniel Hillis and Guy L. Steele, Jr. </author> <title> Data Parallel Algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Table 1 summarizes our results. Section 5 describes the benchmarks in more detail and presents complete performance results. 2 User-defined Reductions in C** Hillis and Steele popularized the data-parallel programming model for programming massively parallel processors <ref> [10] </ref>. The high-level features of this model, such as a global namespace and nearly deterministic execution, prompted the design of many other data-parallel programming languages. Our language, C** provides coarse-grain data-parallelism, but enforces independent execution of coarse-grain tasks.
Reference: [11] <author> W. Daniel Hillis and Lewis W. Tucker. </author> <title> The CM-5 Connection Machine: A Scalable Supercomputer. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 31-40, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Our C** compiler targets a Thinking Machines CM-5 <ref> [11] </ref>, a message-passing multiprocessor. The run-time system builds on a portable parallel substrate called Tempest [17], which provides mechanisms for shared memory and message passing on a wide range of parallel machines.
Reference: [12] <author> Alexander C. Klaiber and James L. Frankel. </author> <title> Comparing Data-Parallel and Message-Passing Paradigms. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages II-11-II-20, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: For example, Dataparallel C adds a tournament operator [8] to carry along an extra value in max-reductions. Also, in comparing the message-passing and data-parallel paradigms, Klaiber et al. <ref> [12] </ref> noted the inefficiency in expressing many-to-many communication in C*. To remedy the problem, they introduced the sendToQueue reduction operator that is similar to append. Sharma et al. [21] used the intrinsic list operator to efficiently execute the particle-in-cell application DSMC.
Reference: [13] <author> James R. Larus. </author> <title> C**: a Large-Grain, Object-Oriented, Data-Parallel Programming Language. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages And Compilers for Parallel Computing (5th International Workshop), </booktitle> <pages> pages 326-341. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Many-to-one communication results in data conflicts or collisions, when multiple values are stored in a location. Many data parallel languages leave the semantics of these conflicts undefined [14]. Other languages try to avoid potential errors due to data races by defining a semantics for these collisions <ref> [13, 22] </ref>. These languages typically use a binary reduction operator to combine colliding values into a single value that can be stored in a location. Reductions are also common in parallel applications, even those written in languages that do not provide first-class support for these operations. <p> Our language, C** provides coarse-grain data-parallelism, but enforces independent execution of coarse-grain tasks. Section 2.1 briefly describes the data-parallel execution model in C**, a detailed description of C** can be found elsewhere <ref> [13] </ref>. Section 2.2 describes the syntax of reduction assignments in C** and shows how user-defined extensions can be added with minor syntax extensions. 2.1 Data-parallelism in C** Data-parallel programs express parallelism by invoking a data-parallel operation on a data collection. <p> The pseudo variables #0 and #1 identify row and column positions within the collection and allow access to neighboring elements. C** provides large-grain data-parallelism <ref> [13] </ref>. It allows coarse-grain parallel tasks, but enforces task independence by providing local copies for global updates (i.e., copy-in, copy-out semantics). <p> The Splash version is consistently faster (as much as 1.6x on one processor) up to 16 processors, but the C** version is 1.06x faster on 32 processors. 4.4 Tree Combining C** also allows reductions to combine values returned by parallel tasks <ref> [13] </ref>. The combined return value is returned to the sequential phase as the result of a data-parallel operation. C** executes return reductions using processor combining trees for efficiency, much like combining tree barriers for synchronization [15]. The processors are organized as a tree.
Reference: [14] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object Oriented Parallel Programming, Experiments and Results. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 273-282, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Although widely used, assigning a value to a location only suffices for one-to-one or one-to-many communication. Many-to-one communication results in data conflicts or collisions, when multiple values are stored in a location. Many data parallel languages leave the semantics of these conflicts undefined <ref> [14] </ref>. Other languages try to avoid potential errors due to data races by defining a semantics for these collisions [13, 22]. These languages typically use a binary reduction operator to combine colliding values into a single value that can be stored in a location. <p> An invocation simultaneously executes data-parallel tasks for each element of the collection. The compiler and run-time system map tasks to physical processors in a machine. Data collections are specified using aggregate types (e.g., arrays in HPF [9], vectors in NESL [2], special class constructs in PC++ <ref> [14] </ref>). C** overloads the class definition mechanism of C++ to define data collections that are called Aggregates. For example, Figure 1 declares a two-dimensional collection of floating point values whose size is specified when Grid objects are created. Data-parallel operations in C** are specified using the parallel keyword.
Reference: [15] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The combined return value is returned to the sequential phase as the result of a data-parallel operation. C** executes return reductions using processor combining trees for efficiency, much like combining tree barriers for synchronization <ref> [15] </ref>. The processors are organized as a tree.
Reference: [16] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 68-79, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Table 2 gives a high-level description of the benchmarks, including the input data sets. For three applications, DSMC, Moldyn, and EM3D, we compared against the best optimized programs previously written (by others) using a hybrid of shared-memory and message-passing techniques <ref> [5, 16] </ref>. These programs use transparent shared memory as a basis, but communicate select data structures through custom shared-memory or message-passing protocols. For three programs, including DSMC and Moldyn, Mukherjee et al. demonstrated that this approach compares favorably with the well-known Maryland CHAOS library [4] for irregular applications. <p> This difference accounts for 17.5% of C**'s execution time slowdown compared to Hybrid. 5.3 Water and Moldyn Water and Moldyn are well-known molecular dynamics code used to model macromolecular systems <ref> [16] </ref>. Molecules are initially distributed uniformly in a cuboidal region with a Maxwellian distribution of initial velocities. A molecule moves under the influence of forces exerted on it by other molecules.
Reference: [17] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Our C** compiler targets a Thinking Machines CM-5 [11], a message-passing multiprocessor. The run-time system builds on a portable parallel substrate called Tempest <ref> [17] </ref>, which provides mechanisms for shared memory and message passing on a wide range of parallel machines. The compiler uses the Tempest shared-memory mechanisms to implement a global namespace and its message-passing primitives to implement reductions. Table 2 gives a high-level description of the benchmarks, including the input data sets.
Reference: [18] <author> John R. Rose and Guy L. Steele Jr. </author> <title> C*: An Extended C Language for Data Parallel Programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 2-16, </pages> <address> Santa Clara, California, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Improved parallel programming languages could reduce the difficulty of programming parallel computers by making programs less error prone and less machine specific. One promising approach is data-parallel languages, such as HPF [9], C* <ref> [18] </ref>, or NESL [2], which provide high-level abstractions for the three key facets of parallel programming: concurrency, synchronization, and communication. In these languages, programmers express parallelism by invoking a parallel operation simultaneously on a collection of data. <p> More interesting and varied are the mechanisms for passing values from the parallel to the sequential computations. Vector-based languages, such as Connection Machine Lisp [23] or NESL [2], provide vector permutation and mapping operations that rearrange a vector's data. Point-based languages, such as HPF [9] and C* <ref> [18] </ref>, fl This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no.
Reference: [19] <author> Gary W. Sabot. </author> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: In addition, user-defined reductions offer a natural way to express many message-passing optimizations, such as update protocols and bulk data transfer, in a shared-memory program. Some languages include user-defined reductions, such as Connection Machine Lisp [23], Paralation Lisp <ref> [19] </ref>, and Fortran D [7]. We are unaware of published descriptions of implementations on MIMD machines. This paper describes an efficient implementation for user-defined reductions and contains experiments that demonstrate that user-defined reductions both simplify writing parallel programs and improve the performance of programs written in a high-level parallel language. <p> The C** compiler relies on user guarantees that user-defined functions are safe. Another issue is that user-defined reductions may not be commutative or associative, so that different combining orders lead to non-deterministic results. This is not a problem for two reasons. First, user-defined reductions are typically effectively associative <ref> [19] </ref> functions in which the absence of associativity does not affect a program's result. For example, the combining function append collects values into a list. In many cases, the list is a set so that the order of elements is unimportant. <p> This paper describes a general mechanism supporting powerful reduction operators. Reductions are a feature in most, if not all, data-parallel languages. A few of these languages allow user-defined functions for reduction operations (e.g., Connection Machine Lisp [23], Paralation Lisp <ref> [19] </ref>, and Fortran D [7]). However, we are unaware of papers describing implementations of user-defined reductions on parallel machines. 7 Conclusion Data-parallel languages mitigate the difficulty of parallel programming by providing high-level abstractions for concurrency, synchronization and communication.
Reference: [20] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297-307, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The other two applications (Water and FFT) are transparent shared memory programs from the Stanford SPLASH suite [24]. We ran these programs on Blizzard, a fine-grain distributed shared memory system that runs on the CM-5 <ref> [20] </ref>. Table 2 also compares the program size (number of lines) of the C** version of each benchmark against the optimized versions. In all cases, the C** programs are significantly smaller.
Reference: [21] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan-Shin Hwang, Raja Das, and Joel Saltz. </author> <title> Run-time and Compile-time Support for Adaptive Irregular Problems. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 97-106, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In all cases, the C** programs are significantly smaller. The difference is more pronounced for the Hybrid codes (e.g., EM3D) that contain custom code to improve communication performance. 5.1 DSMC DSMC simulates particle movement and collision in a three dimensional domain using a Discrete Simulation Monte Carlo method <ref> [21] </ref>. DSMC divides the domain into cells in a static Cartesian grid and distributes molecules among cells. At each time step, the algorithm moves molecules under the influence of forces from interactions with other molecules, adds new molecules from a jet stream, and collides molecules in the same cell. <p> Also, in comparing the message-passing and data-parallel paradigms, Klaiber et al. [12] noted the inefficiency in expressing many-to-many communication in C*. To remedy the problem, they introduced the sendToQueue reduction operator that is similar to append. Sharma et al. <ref> [21] </ref> used the intrinsic list operator to efficiently execute the particle-in-cell application DSMC. Several applications in the HPF-2 motivating applications suite [6] note their requirement for user-defined reductions. This paper describes a general mechanism supporting powerful reduction operators. Reductions are a feature in most, if not all, data-parallel languages.
Reference: [22] <author> Guy L. Steele Jr. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In Conference Record of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Many-to-one communication results in data conflicts or collisions, when multiple values are stored in a location. Many data parallel languages leave the semantics of these conflicts undefined [14]. Other languages try to avoid potential errors due to data races by defining a semantics for these collisions <ref> [13, 22] </ref>. These languages typically use a binary reduction operator to combine colliding values into a single value that can be stored in a location. Reductions are also common in parallel applications, even those written in languages that do not provide first-class support for these operations. <p> This approach opens the door to difficult-to-find errors if a directive is incorrect. Third, a language may allow general functions, but require a run-time system to identify data access conflicts, as in Steele's Parallel Scheme <ref> [22] </ref>. Run-time conflict identification can be expensive and complex. The C** compiler relies on user guarantees that user-defined functions are safe. Another issue is that user-defined reductions may not be commutative or associative, so that different combining orders lead to non-deterministic results. This is not a problem for two reasons.
Reference: [23] <author> Guy L. Steele Jr. and W. Daniel Hillis. </author> <title> Connection Machine LISP: Fine-Grained Parallel Symbolic Processing. </title> <booktitle> In Proceedings of the 1986 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 279-297, </pages> <month> August </month> <year> 1986. </year> <month> 13 </month>
Reference-contexts: More interesting and varied are the mechanisms for passing values from the parallel to the sequential computations. Vector-based languages, such as Connection Machine Lisp <ref> [23] </ref> or NESL [2], provide vector permutation and mapping operations that rearrange a vector's data. Point-based languages, such as HPF [9] and C* [18], fl This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. <p> In addition, user-defined reductions offer a natural way to express many message-passing optimizations, such as update protocols and bulk data transfer, in a shared-memory program. Some languages include user-defined reductions, such as Connection Machine Lisp <ref> [23] </ref>, Paralation Lisp [19], and Fortran D [7]. We are unaware of published descriptions of implementations on MIMD machines. <p> This paper describes a general mechanism supporting powerful reduction operators. Reductions are a feature in most, if not all, data-parallel languages. A few of these languages allow user-defined functions for reduction operations (e.g., Connection Machine Lisp <ref> [23] </ref>, Paralation Lisp [19], and Fortran D [7]). However, we are unaware of papers describing implementations of user-defined reductions on parallel machines. 7 Conclusion Data-parallel languages mitigate the difficulty of parallel programming by providing high-level abstractions for concurrency, synchronization and communication.
Reference: [24] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Falsafi et al. showed that the best optimized version of EM3D ran faster than a comparable message-passing program [5]. The other two applications (Water and FFT) are transparent shared memory programs from the Stanford SPLASH suite <ref> [24] </ref>. We ran these programs on Blizzard, a fine-grain distributed shared memory system that runs on the CM-5 [20]. Table 2 also compares the program size (number of lines) of the C** version of each benchmark against the optimized versions. In all cases, the C** programs are significantly smaller. <p> A molecule moves under the influence of forces exerted on it by other molecules. In Moldyn, the force computation limits interactions to molecules within a cut-off radius by maintaining an interaction list that is updated infrequently. Water <ref> [24] </ref> computes interactions between all pairs of molecules. Evaluating an interaction involves reading the positions of two molecules, computing the resulting force, and updating each molecule with the resultant force. Force update involves many-to-one communication; each molecule receives force increments from many interacting molecules. <p> Between 1 and 16 processors, the sequential overhead dominates, but at 32 processors, the lower cost of communication overshadows the sequential overhead. As a result, the best C** version is 1.05x faster than the Splash version. 5.4 FFT The FFT kernel, from the SPLASH suite <ref> [24] </ref>, implements a complex 1-D version of the radix p N six-step algorithm that minimizes interprocessor communication [1]. The input 1-D vector of size N is organized as a N fi N square matrix.
Reference: [25] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 219-229, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: The first C** version uses null reductions to move each point in the matrix to its transposed position. The second C** version partitions the matrix into 4 fi 4 tiles at the source level to increase cache reuse <ref> [25] </ref>. The user-defined reduction function transfers a tile at a time and transposes the tile at the receiving end in an update function. of both C** programs.
Reference: [26] <author> Bwolen Yang, Jon Webb, James M. Stichnoth, David R. O'Halloran, and Thomas Gross. Do&Merge: </author> <title> Integrating Parallel Loops and Reductions. </title> <editor> In Utpal Bannerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing (Proceedings of the Sixth Internationa Workshop), </booktitle> <pages> pages 169-183, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. 14 </note>
Reference-contexts: This approach is also useful when combining values is more expensive than merging values one-by-one, as when each value modifies distinct parts of a large data structure <ref> [26] </ref>. 4 Implementation The C** compiler implements user-defined reductions with a small amount of runtime support.
References-found: 26

