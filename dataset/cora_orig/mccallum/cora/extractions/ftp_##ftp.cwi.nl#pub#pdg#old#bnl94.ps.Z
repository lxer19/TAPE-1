URL: ftp://ftp.cwi.nl/pub/pdg/old/bnl94.ps.Z
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: pdg@cwi.nl  
Title: Grammar Inference and the Minimum Description Length Principle  
Author: Peter Grunwald 
Date: 13 May 1994  
Address: Kruislaan 413, 1098 SJ Amsterdam, The Netherlands  
Affiliation: Centrum voor Wiskunde en Informatica  
Abstract: We describe a new abstract model for the computational learning of grammars. The model deals with a learning process in which an algorithm is given an input of a large set of training sentences that belong to some grammar G. The algorithm then tries to infer this grammar. Our model is based on the well-known Minimum Description Length Principle. It turns out that our model is, in a certain sense, a more general version of two seemingly different well-known other ones. Also, two other existing models turn out to be very similar to ours. We have made an initial implementation of the algorithm implied by the model. We have tried this implementation on natural language texts, and we give a short description of the results of these tests. The results of testing the algorithm in practice are quite interesting, but unfortunately they are neither encouraging nor discouraging enough to indicate whether our method of grammar induction, which hardly makes any use of any linguistic principles and makes no use at all of any semantical information, is really worth pursuing further.
Abstract-found: 1
Intro-found: 1
Reference: [BPd + 92] <author> P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18 </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: A little example may clarify: in figure 1 the Markov Chain with estimated probabilities corresponding to the following training text is shown: I am a man. I have a nose. I am not a nose. In <ref> [BPd + 92] </ref>, Brown et. al. use an approach along these lines. They extend their model by the notion of classes. All words in the training text are partitioned into equivalence classes. <p> In more realistic models there is a dependency on the last n preceding words, with n typically 3 or 4. Now in <ref> [BPd + 92] </ref> the Maximum Likelihood Principle is used as before but additionally one seeks to find the partition of words into classes that yields the highest probability of reproducing the training text when generating class and word sequences according to the estimated probabilities.
Reference: [FC91] <author> S. Finch and N. Chater. </author> <title> A hybrid approach to the automatic learning of linguistic categories. </title> <journal> AISB Quarterly, </journal> <volume> 78 </volume> <pages> 16-24, </pages> <year> 1991. </year>
Reference-contexts: In [Gru94], we explicitly show that it reduces to Brown et al.'s approach if one `forbids' constructs. But some other approaches are related too: In <ref> [FC91] </ref>, an approach using 3-grams, Rank Correlation Coefficients and hierarchical cluster analysis is given which also yields a dendrogram of related words. While on the surface quite different, this approach is very similar to Brown's and ours, as we show in [Gru94]. In [Wol82], G.
Reference: [Gal68] <author> R.G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: But in natural language, a single word can belong to more than one `class'. 1 word per class Starting from a basic theorem of information theory (which is a very important tool when using the MDL Principle in a probabilistic context; for an introduction see e.g. <ref> [Gal68] </ref>), we have proven in [Gru94] that , the `optimal partition' of words into classes that one is looking for and that maximizes the probability of 4 Due to limited computer resources, the algorithm had to be changed a little.
Reference: [Gru94] <author> P.D. Grunwald. </author> <title> Automatic grammar induction using the MDL Principle. </title> <type> Master's thesis, </type> <institution> University of Amsterdam, </institution> <address> Amsterdam, </address> <year> 1994. </year>
Reference-contexts: in natural language, a single word can belong to more than one `class'. 1 word per class Starting from a basic theorem of information theory (which is a very important tool when using the MDL Principle in a probabilistic context; for an introduction see e.g. [Gal68]), we have proven in <ref> [Gru94] </ref> that , the `optimal partition' of words into classes that one is looking for and that maximizes the probability of 4 Due to limited computer resources, the algorithm had to be changed a little. <p> Therefore, it is not at all clear when one should stop merging classes! Combining Brown's model with Solomonoff's, we were able to at least partially solve these three problems. In <ref> [Gru94] </ref> all of this is treated at length; here we will give a heuristic explanation of how the latter two problems may be solved by viewing the problems from another angle. <p> This was first proven in [Ris82]; for a more elaborate explanation, see <ref> [Gru94] </ref>. The proof stems again from information theory. From this it follows ([Gru94]) that we can change Brown et al.'s algorithm in the sense that at each step we let the two classes merge for which the resulting rise in description length of the data is the lowest, and still arrive <p> In the next section we will see if we can arrive at some reasonable value for X. 5.4 Related Approaches Our approach is an extension to and a more exact version of Solomonoff's original one. In <ref> [Gru94] </ref>, we explicitly show that it reduces to Brown et al.'s approach if one `forbids' constructs. But some other approaches are related too: In [FC91], an approach using 3-grams, Rank Correlation Coefficients and hierarchical cluster analysis is given which also yields a dendrogram of related words. <p> But some other approaches are related too: In [FC91], an approach using 3-grams, Rank Correlation Coefficients and hierarchical cluster analysis is given which also yields a dendrogram of related words. While on the surface quite different, this approach is very similar to Brown's and ours, as we show in <ref> [Gru94] </ref>. In [Wol82], G. Wolff gives an approach making explicit use of data-compression. This approach is very similar to ours too. It has an important extra feature (grammar rebuilding) which we would like to build into our model too in the near future. <p> In order for the algorithm to work within the limits of available computer power, some adjustments had to be made; details about this can be found in <ref> [Gru94] </ref>. An important one is the fact that we used as training text not the full Brown corpus, but the reduction of the Brown corpus to just those sentences all words of which are among the 10000 most frequent words in the corpus.
Reference: [KF67] <author> H. Kucera and W. Francis. </author> <title> Computational Analysis of Present Day American English. </title> <publisher> Brown University Press, </publisher> <year> 1967. </year>
Reference: [Lan87] <author> P. Langley. </author> <title> Machine learning and grammar induction. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 5-8, </pages> <year> 1987. </year> <note> Editorial of special issue on language learning. </note>
Reference: [LV93] <author> M. Li and P.M.B. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Of course, it is a hopeless overgeneralization. 2 See <ref> [LV93] </ref> for a good introduction. the ad hoc grammar This grammar is somewhat more conservative: it accepts exactly all the sentences it has seen so far, but no single different sentence.
Reference: [Ris82] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Ann. Statist., </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1982. </year>
Reference-contexts: This was first proven in <ref> [Ris82] </ref>; for a more elaborate explanation, see [Gru94]. The proof stems again from information theory.
Reference: [Sol64] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference, part 1 and part 2. </title> <journal> Inform. Contr., </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <year> 1964. </year>
Reference-contexts: Finally, we draw some conclusions from these results and indicate paths for future research. 2 The MDL Principle In <ref> [Sol64] </ref>, one of the papers that founded the subject of Kolmogorov Complexity, Ray Solomonoff has come up with some fundamental ideas about formalizing induction 2 . The general idea is that `learning' can, under the right circumstances, be seen as `finding a shorter description of the observed data'.
Reference: [Wol82] <author> J.G. Wolff. </author> <title> Language acquisition, data compression, and generalization. </title> <journal> Language and Communication, </journal> <volume> 2 </volume> <pages> 57-89, </pages> <year> 1982. </year>
Reference-contexts: While on the surface quite different, this approach is very similar to Brown's and ours, as we show in [Gru94]. In <ref> [Wol82] </ref>, G. Wolff gives an approach making explicit use of data-compression. This approach is very similar to ours too. It has an important extra feature (grammar rebuilding) which we would like to build into our model too in the near future.
References-found: 10

