URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-059.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Learning Topology-Preserving Maps Using Self-Supervised Backpropagation on a Parallel Machine  
Author: Arnfried Ossen 
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  Univer-sitat Berlin, Germany  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute, Berkeley &lt;ossen@icsi.berkeley.edu&gt;, and Technische  
Pubnum: TR-92-059  
Email: &lt;ao@cs.tu-berlin.de&gt;.  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Date: September 1992  
Abstract: Self-supervised backpropagation is an unsupervised learning procedure for feedfor-ward networks, where the desired output vector is identical with the input vector. For backpropagation, we are able to use powerful simulators running on parallel machines. Topology-preserving maps, on the other hand, can be developed by a variant of the competitive learning procedure. However, in a degenerate case, self-supervised backpropagation is a version of competitive learning. A simple extension of the cost function of backpropagation leads to a competitive version of self-supervised backpropagation, which can be used to produce topographic maps. We demonstrate the approach applied to the Traveling Salesman Problem (TSP). The algorithm was implemented using the backpropagation simulator (CLONES) on a parallel machine (RAP). 
Abstract-found: 1
Intro-found: 1
Reference: [ Hinton, 1987 ] <author> Geoffrey E. Hinton. </author> <title> Connectionist learning procedures. </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh PA 15213, </address> <year> 1987. </year>
Reference: [ Johnson, 1990 ] <author> D. S. Johnson. </author> <title> Local optimization and the traveling salesman problem. </title> <booktitle> In Proceedings 17th Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 446-461, </pages> <year> 1990. </year>
Reference-contexts: Application to the Traveling Salesman Problem The Traveling Salesman Problem is an NP-hard problem, and even very restricted versions of the TSP are hard problems. One way to cope with the intractability of the general TSP is to approximate the solution. Many global or local heuristics are known <ref> [ Johnson, 1990 ] </ref> . More recently, neural network approaches were suggested for the TSP. A summary can be found in [ Peterson, 1990 ] . We compare our approach to these algorithms.
Reference: [ Kohn et al., 1992 ] <author> Phil Kohn, Jeff Bilmes, Nelson Morgan, and James Beck. </author> <title> Software for ANN training on a ring array processor. </title> <editor> In John E. Moody, Steven J. Hanson, and Richard P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems IV, </booktitle> <pages> pages 781-788. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: The underlying assumption is that a "modern object-oriented language (such as C++) has all the functionality needed to build and train [Artificial Neural Nets] ANNs". Resulting programs for the simulation of Neural Nets should be shorter and easier to read, write and modify <ref> [ Kohn et al., 1992 ] </ref> . CLONES runs on UNIX platforms and on the Ring Array Processor (RAP). A RAP consists of a number of digital signal processors (nodes) which are added to a host workstation. The theoretical peak performance is 128 MFLOPS for a four node RAP.
Reference: [ Kohonen, 1989 ] <author> Teuvo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Thus, the above degenerate case of self-supervised backpropagation is equivalent to the simple competitive learning procedure. 2. Topology-Preserving Self-Supervised Backpropagation One important version of competitive learning is concerned with learning topographic maps or feature mapping <ref> [ Kohonen, 1989 ] </ref> . In this variant, there is some geometrical arrangement of the competing units. Usually they are placed on a line or in a plane.
Reference: [ Krogh et al., 1990 ] <author> Anders Krogh, G. I. Thorbergsson, and John A. Hertz. </author> <title> A cost function for internal representations. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II, </booktitle> <pages> pages 733-740. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference: [ Peterson, 1990 ] <author> Carsten Peterson. </author> <title> Parallel distributed approaches to combinatorial optimization: Benchmark studies on traveling salesman problem. </title> <journal> Neural Computation, </journal> <volume> 2(3) </volume> <pages> 261-269, </pages> <year> 1990. </year>
Reference-contexts: One way to cope with the intractability of the general TSP is to approximate the solution. Many global or local heuristics are known [ Johnson, 1990 ] . More recently, neural network approaches were suggested for the TSP. A summary can be found in <ref> [ Peterson, 1990 ] </ref> . We compare our approach to these algorithms. In order to apply the self-supervised backpropagation feature maps to the TSP, we use a strategy known from research on the Kohonen feature map. <p> Our approach has not yet been tested against a lower bound. Instead, we compare it to other Neural Network algorithms for the TSP. A detailed description of a benchmark study for the TSP can be found in <ref> [ Peterson, 1990 ] </ref> . This testbed consists of 50-, 100- and 200-city TSPs in the unit square.
Reference: [ Williams and Zipser, 1989 ] <author> Ronald J. Williams and David Zipser. </author> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 87-111, </pages> <year> 1989. </year> <pages> 7 8 9 </pages>
References-found: 7

