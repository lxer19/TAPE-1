URL: http://www-db.stanford.edu/~shiva/Pubs/web.ps
Refering-URL: http://google.stanford.edu/google_papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fshiva, hectorg@cs.stanford.edu  
Title: Finding near-replicas of documents on the web  
Author: Narayanan Shivakumar, Hector Garcia-Molina 
Address: Stanford, CA 94305.  
Affiliation: Department of Computer Science  
Abstract: We consider how to efficiently compute the overlap between all pairs of web documents. This information can be used to improve web crawlers, web archivers and in the presentation of search results, among others. We report statistics on how common replication is on the web, and on the cost of computing the above information for a relatively large subset of the web about 24 million web pages which corresponds to about 150 Gigabytes of textual information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alexa. Alexa technology. </institution> <note> http://www.alexa.com. </note>
Reference-contexts: Similar proposals are being considered for "result reordering" based on overlap in other database contexts as well [7, 8]. 3. Archiving applications: Since companies are trying to "archive" the web <ref> [1] </ref>, it may be use 1 Search engines are beginning to cluster exact copies of doc uments by computing simple checksums. 1 ful for them to identify near-copies of documents and compress these in a more efficient manner. <p> Also finding near-copies is useful in services that avoid the common "404 error" (error for "Document not found" on the web) by presenting an alternate near-copy on the web, or a copy from a web archive <ref> [1] </ref>. 1.1 Related work Manber considered computing pair-wise document overlap in the context of finding similar files in a large file system [13, 14]. Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5].
Reference: [2] <author> AltaVista. </author> <title> Altavista search engine. </title> <address> http://altavista.digital.com. </address>
Reference-contexts: We believe it is useful to identify near-replicas of documents and servers for the following applications: 1. Improved web crawling: Until recently, web crawlers <ref> [2, 10] </ref> crawled the "entire" web.
Reference: [3] <author> S. Brin, J. Davis, and H. Garcia-Molina. </author> <title> Copy detection mechanisms for digital documents. </title> <booktitle> In Proceedings of the ACM SIGMOD Annual Conference, </booktitle> <address> San Francisco, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection [9]. As part of the Stanford Digital Library project, we have developed the COPS <ref> [3] </ref> and SCAM [15, 16] experimental prototypes for finding intellectual property violations. All the above techniques use variations of the same basic idea compute "fingerprints" for a set of documents and store into a database. <p> In general, fingerprints that are computed on sequences of words or lines tend to be common, compared to fingerprints computed on paragraph level chunks. On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap <ref> [3, 15, 16] </ref>. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency. <p> In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency. On the other hand, in applications (like SCAM and COPS <ref> [3, 15] </ref>) where the goal is to identify document pairs with smaller overlaps, we choose to use the latter class of fingerprints.
Reference: [4] <author> A. Broder. </author> <title> On the resemblance and containment of documents. </title> <type> Technical report, </type> <institution> DIGITAL Systems Research Center Tech. </institution> <type> Report, </type> <year> 1997. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web <ref> [4, 5] </ref>. Heintze has developed the KOALA system for plagiarism detection [9]. As part of the Stanford Digital Library project, we have developed the COPS [3] and SCAM [15, 16] experimental prototypes for finding intellectual property violations.
Reference: [5] <author> A. Broder, S.C. Glassman, and M. S. Manasse. </author> <title> Syntactic Clustering of the Web. </title> <booktitle> In Sixth International World Wide Web Conference, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web <ref> [4, 5] </ref>. Heintze has developed the KOALA system for plagiarism detection [9]. As part of the Stanford Digital Library project, we have developed the COPS [3] and SCAM [15, 16] experimental prototypes for finding intellectual property violations. <p> This problem is especially hard when the number of documents is on the order of millions. The techniques we use to solve this problem are different from the techniques adopted by the above work <ref> [5, 9, 13, 14] </ref> as well as in our prior implementation in SCAM. Our new approach to solving the all-pairs document overlap problem is based on efficiently executing iceberg queries [17]. <p> The above is roughly the approach followed by earlier work in computing all-pairs overlap, including the GLIMPSE system [13, 14], the DEC prototype <ref> [5] </ref> and in our earlier implementations of our COPS and SCAM prototypes. The efficiency of the above implementation depends critically on how often fingerprints occur across documents. This is because Step (2) produces a cross-product of all document pairs that share a fingerprint, and subsequent steps process the resulting cross-product. <p> On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap [3, 15, 16]. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype <ref> [5] </ref>), the former class of fingerprints tend to be useful due to their efficiency. On the other hand, in applications (like SCAM and COPS [3, 15]) where the goal is to identify document pairs with smaller overlaps, we choose to use the latter class of fingerprints.
Reference: [6] <author> J. Cho, H. Garcia-Molina, and L. </author> <title> Page. Efficient crawling through url ordering. </title> <type> Technical report, </type> <institution> Stanford DBGroup Tech. </institution> <type> Report, </type> <month> November </month> <year> 1997. </year>
Reference-contexts: since the web is growing rapidly and changing even faster, current crawlers are concentrating on crawling some subset of the web that has "high importance." For example, crawlers may only choose to visit "hot pages" with a high back-link count, i.e., pages that are pointed to by many other pages <ref> [6, 11] </ref>. After finishing each crawl, these crawlers will recrawl the same pages to acquire updates to the hot pages since the last crawl.
Reference: [7] <author> D. Florescu, D. Koller, and A. Levy. </author> <title> Using probabilistic information in data integration. </title> <booktitle> In Proceedings of 23rd Conference on Very Large Databases (VLDB'97), </booktitle> <pages> pages 216 - 225, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: However, we believe the "interestingness" of B is lower than C, if B overlaps with A significantly, while C is a distinct result. Similar proposals are being considered for "result reordering" based on overlap in other database contexts as well <ref> [7, 8] </ref>. 3.
Reference: [8] <author> L. Gravano and H. Garcia-Molina. </author> <title> Merging ranks from heterogeneous internet sources. </title> <booktitle> In Proceedings of 23rd Conference on Very Large Databases (VLDB'97), </booktitle> <pages> pages 196-205, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: However, we believe the "interestingness" of B is lower than C, if B overlaps with A significantly, while C is a distinct result. Similar proposals are being considered for "result reordering" based on overlap in other database contexts as well <ref> [7, 8] </ref>. 3.
Reference: [9] <author> N. Heintze. </author> <title> Scalable document fingerprinting. </title> <booktitle> In Proceedings of Second USENIX Workshop on Electronic Commerce, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection <ref> [9] </ref>. As part of the Stanford Digital Library project, we have developed the COPS [3] and SCAM [15, 16] experimental prototypes for finding intellectual property violations. All the above techniques use variations of the same basic idea compute "fingerprints" for a set of documents and store into a database. <p> This problem is especially hard when the number of documents is on the order of millions. The techniques we use to solve this problem are different from the techniques adopted by the above work <ref> [5, 9, 13, 14] </ref> as well as in our prior implementation in SCAM. Our new approach to solving the all-pairs document overlap problem is based on efficiently executing iceberg queries [17].
Reference: [10] <author> Infoseek. </author> <title> Infoseek search engine. </title> <address> http://www.infoseek.com. </address>
Reference-contexts: We believe it is useful to identify near-replicas of documents and servers for the following applications: 1. Improved web crawling: Until recently, web crawlers <ref> [2, 10] </ref> crawled the "entire" web.
Reference: [11] <author> J. Kleinberg. </author> <title> Authoritative sources in a hyperlinked environment. </title> <booktitle> In Proceedings of 9th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <address> (SODA'98), </address> <year> 1998. </year>
Reference-contexts: since the web is growing rapidly and changing even faster, current crawlers are concentrating on crawling some subset of the web that has "high importance." For example, crawlers may only choose to visit "hot pages" with a high back-link count, i.e., pages that are pointed to by many other pages <ref> [6, 11] </ref>. After finishing each crawl, these crawlers will recrawl the same pages to acquire updates to the hot pages since the last crawl.
Reference: [12] <author> S. Brin L. </author> <title> Page. Google search engine/ backrub web crawler. </title> <address> http://google.stanford.edu. </address>
Reference-contexts: In fact in many cases, the above approach would finish computing the Overlap table, even before Step (2) in the "sort-based" approach terminates [17]. 3 Experiments We used 150 GBs of web data crawled by the Stanford BackRub web crawler <ref> [12] </ref> for our experiments. This corresponds to approximately 24 million web pages crawled primarily from domains located in the United States of America. We ran our experiments on a SUN UltraSPARC with dual processors, 256 MBs of RAM and 1:4 GBs of swap space, running SunOS 5.5.1.
Reference: [13] <author> U. Manber. </author> <title> Finding similar files in a large file system. </title> <type> Technical Report TR 93-33, </type> <institution> University of Arizona, Tuscon, Arizona, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: avoid the common "404 error" (error for "Document not found" on the web) by presenting an alternate near-copy on the web, or a copy from a web archive [1]. 1.1 Related work Manber considered computing pair-wise document overlap in the context of finding similar files in a large file system <ref> [13, 14] </ref>. Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection [9]. <p> This problem is especially hard when the number of documents is on the order of millions. The techniques we use to solve this problem are different from the techniques adopted by the above work <ref> [5, 9, 13, 14] </ref> as well as in our prior implementation in SCAM. Our new approach to solving the all-pairs document overlap problem is based on efficiently executing iceberg queries [17]. <p> The above is roughly the approach followed by earlier work in computing all-pairs overlap, including the GLIMPSE system <ref> [13, 14] </ref>, the DEC prototype [5] and in our earlier implementations of our COPS and SCAM prototypes. The efficiency of the above implementation depends critically on how often fingerprints occur across documents. <p> On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap [3, 15, 16]. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE <ref> [13, 14] </ref>, and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency.
Reference: [14] <author> U. Manber and S. Wu. Glimpse: </author> <title> A tool to search through entire file systems. </title> <booktitle> In Proceedings of the winter USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: avoid the common "404 error" (error for "Document not found" on the web) by presenting an alternate near-copy on the web, or a copy from a web archive [1]. 1.1 Related work Manber considered computing pair-wise document overlap in the context of finding similar files in a large file system <ref> [13, 14] </ref>. Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection [9]. <p> This problem is especially hard when the number of documents is on the order of millions. The techniques we use to solve this problem are different from the techniques adopted by the above work <ref> [5, 9, 13, 14] </ref> as well as in our prior implementation in SCAM. Our new approach to solving the all-pairs document overlap problem is based on efficiently executing iceberg queries [17]. <p> The above is roughly the approach followed by earlier work in computing all-pairs overlap, including the GLIMPSE system <ref> [13, 14] </ref>, the DEC prototype [5] and in our earlier implementations of our COPS and SCAM prototypes. The efficiency of the above implementation depends critically on how often fingerprints occur across documents. <p> On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap [3, 15, 16]. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE <ref> [13, 14] </ref>, and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency.
Reference: [15] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> SCAM: A copy detection mechanism for digital documents. </title> <booktitle> In Proceedings of 2nd International Conference in Theory and Practice of Digital Libraries (DL'95), </booktitle> <address> Austin, Texas, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection [9]. As part of the Stanford Digital Library project, we have developed the COPS [3] and SCAM <ref> [15, 16] </ref> experimental prototypes for finding intellectual property violations. All the above techniques use variations of the same basic idea compute "fingerprints" for a set of documents and store into a database. Two documents are defined to have significant overlap, if they share at least a certain number of fingerprints. <p> In this paper, we do not propose new notions of document similarity: we use the similarity measures we have been using in our SCAM (Stanford Copy Analysis Mechanism) prototype over the past three years <ref> [15, 16] </ref>. We primarily concentrate on efficiently solving the "clustering" problem of computing overlap between all document-pairs simultaneously. This problem is especially hard when the number of documents is on the order of millions. <p> In general, fingerprints that are computed on sequences of words or lines tend to be common, compared to fingerprints computed on paragraph level chunks. On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap <ref> [3, 15, 16] </ref>. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency. <p> In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency. On the other hand, in applications (like SCAM and COPS <ref> [3, 15] </ref>) where the goal is to identify document pairs with smaller overlaps, we choose to use the latter class of fingerprints.
Reference: [16] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> Building a scalable and accurate copy detection mechanism. </title> <booktitle> In Proceedings of 1st ACM Conference on Digital Libraries (DL'96), </booktitle> <address> Bethesda, Maryland, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: Researchers at the DEC SRC Lab are developing a similar tool to "syntactically" cluster the web [4, 5]. Heintze has developed the KOALA system for plagiarism detection [9]. As part of the Stanford Digital Library project, we have developed the COPS [3] and SCAM <ref> [15, 16] </ref> experimental prototypes for finding intellectual property violations. All the above techniques use variations of the same basic idea compute "fingerprints" for a set of documents and store into a database. Two documents are defined to have significant overlap, if they share at least a certain number of fingerprints. <p> In this paper, we do not propose new notions of document similarity: we use the similarity measures we have been using in our SCAM (Stanford Copy Analysis Mechanism) prototype over the past three years <ref> [15, 16] </ref>. We primarily concentrate on efficiently solving the "clustering" problem of computing overlap between all document-pairs simultaneously. This problem is especially hard when the number of documents is on the order of millions. <p> In general, fingerprints that are computed on sequences of words or lines tend to be common, compared to fingerprints computed on paragraph level chunks. On the other hand, similarity measures that use the latter fingerprints tend to be less effective in finding overlap <ref> [3, 15, 16] </ref>. In applications where the goal is to identify "approximately similar" documents (as in GLIMPSE [13, 14], and in DEC's prototype [5]), the former class of fingerprints tend to be useful due to their efficiency.
Reference: [17] <author> N. Shivakumar and H. Garcia-Molina. </author> <title> Computing iceberg queries effciently. </title> <type> Technical report, Stanford DBGroup Technical Report, </type> <month> October </month> <year> 1997. </year> <month> 6 </month>
Reference-contexts: The techniques we use to solve this problem are different from the techniques adopted by the above work [5, 9, 13, 14] as well as in our prior implementation in SCAM. Our new approach to solving the all-pairs document overlap problem is based on efficiently executing iceberg queries <ref> [17] </ref>. As we will see, our techniques take orders of magnitude less space and time. 2 Computing all-pairs docu ment overlap Each document is first converted from its native format (e,g., HTML, PostScript) into simple textual information using standard converters on UNIX (such as ps2ascii, html2ascii). <p> The candidate-selection scan in this simple coarse-counting algorithm may com 3 pute a large F , due to false-positives. We how-ever can progressively remove false-postives using sampling techniques, as well as multiple hash-functions <ref> [17] </ref>. In practice, we have seen that by using a small number of hash functions and a small pilot sample, we can remove a large fraction of false-positives for many real-world data sets [17]. <p> We how-ever can progressively remove false-postives using sampling techniques, as well as multiple hash-functions <ref> [17] </ref>. In practice, we have seen that by using a small number of hash functions and a small pilot sample, we can remove a large fraction of false-positives for many real-world data sets [17]. The above procedure allows us to avoid explicitly storing the cross-product produced in Step (2) of the "sort-based" algorithm, and the sorting required in the subsequent steps. <p> In fact in many cases, the above approach would finish computing the Overlap table, even before Step (2) in the "sort-based" approach terminates <ref> [17] </ref>. 3 Experiments We used 150 GBs of web data crawled by the Stanford BackRub web crawler [12] for our experiments. This corresponds to approximately 24 million web pages crawled primarily from domains located in the United States of America.
References-found: 17

