URL: http://www.research.att.com/~schapire/papers/FreundSc95.ps.Z
Refering-URL: http://www.research.att.com/~yoav/talks/boostforstat.html
Root-URL: 
Email: fyoav, schapireg@research.att.com  
Title: A decision-theoretic generalization of on-line learning and an application to boosting  
Author: Yoav Freund Robert E. Schapire 
Date: December 19, 1996  
Address: 180 Park Avenue Florham Park, NJ 07932  
Affiliation: AT&T Labs  
Abstract: In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [20] can be adapted to this model yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in R n . In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Eric B. Baum and David Haussler. </author> <title> What size net gives valid generalization? In Advances in Neural Information Processing Systems I, </title> <address> pages 81-90. </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: We call this loss function the mixture loss. In this paper, we always assume that the loss suffered by any strategy is bounded so that, without loss of generality, ` t i 2 <ref> [0; 1] </ref>. Besides this condition, we make no assumptions about the form of the loss vectors ` t , or about the manner in which they are generated; indeed, the adversary's choice for ` t may even depend on the allocator's chosen mixture p t . <p> In Section 5, we give two extensions of our boosting algorithm to multi-class prediction 3 Algorithm Hedge (fi) Parameters: fi 2 <ref> [0; 1] </ref> initial weight vector w 1 2 [0; 1] N with P N i = 1 number of trials T Do for t = 1; 2; : : : ; T 1. Choose allocation p t = P N i 2. Receive loss vector ` t 2 [0; 1] N <p> In Section 5, we give two extensions of our boosting algorithm to multi-class prediction 3 Algorithm Hedge (fi) Parameters: fi 2 <ref> [0; 1] </ref> initial weight vector w 1 2 [0; 1] N with P N i = 1 number of trials T Do for t = 1; 2; : : : ; T 1. Choose allocation p t = P N i 2. Receive loss vector ` t 2 [0; 1] N from environment. 3. <p> fi 2 <ref> [0; 1] </ref> initial weight vector w 1 2 [0; 1] N with P N i = 1 number of trials T Do for t = 1; 2; : : : ; T 1. Choose allocation p t = P N i 2. Receive loss vector ` t 2 [0; 1] N from environment. 3. Suffer loss p t ` t . 4. Set the new weights vector to be w t+1 i fi ` t problems in which each example belongs to one of several possible classes (rather than just two). <p> w t is updated using the multi plicative rule w t+1 i fi ` t 4 More generally, it can be shown that our analysis is applicable with only minor modification to an alternative update rule of the form w t+1 i U fi (` t where U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1] satisfying fi r U fi (r) 1 (1 fi)r 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth [20]. <p> updated using the multi plicative rule w t+1 i fi ` t 4 More generally, it can be shown that our analysis is applicable with only minor modification to an alternative update rule of the form w t+1 i U fi (` t where U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1] satisfying fi r U fi (r) 1 (1 fi)r 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth [20]. <p> fi ` t 4 More generally, it can be shown that our analysis is applicable with only minor modification to an alternative update rule of the form w t+1 i U fi (` t where U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1] satisfying fi r U fi (r) 1 (1 fi)r 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth [20]. <p> Lemma 1 For any sequence of loss vectors ` 1 ; : : : ; ` T , ln i=1 i (1 fi)L Hedge (fi) : Proof: By a convexity argument, it can be shown that ff r 1 (1 ff)r (3) for ff 0 and r 2 <ref> [0; 1] </ref>. <p> Consider the following set-up used by Chung [5]. We are given a decision space , a space of outcomes , and a bounded loss function : fi ! <ref> [0; 1] </ref>. (Actually, our results require only that be bounded, but, by rescaling, we can assume that its range is [0; 1].) At every time step t, the learning algorithm selects a decision ffi t 2 , receives an outcome ! t 2 , and suffers loss (ffi t ; ! <p> Consider the following set-up used by Chung [5]. We are given a decision space , a space of outcomes , and a bounded loss function : fi ! <ref> [0; 1] </ref>. (Actually, our results require only that be bounded, but, by rescaling, we can assume that its range is [0; 1].) At every time step t, the learning algorithm selects a decision ffi t 2 , receives an outcome ! t 2 , and suffers loss (ffi t ; ! t ). <p> the predicted point. 9 To see how to handle the problem of finding deterministic predictions, notice that the loss function (ffi; !) is convex with respect to ffi: jj (affi 1 + (1 a)ffi 2 ) !jj ajjffi 1 !jj + (1 a)jjffi 2 !jj (13) for any a 2 <ref> [0; 1] </ref> and any ! 2 . Thus we can do as follows. <p> After some amount of time, the learner must output a hypothesis h : X ! <ref> [0; 1] </ref>. <p> However, such a bound on the error need not be known ahead of time. Our results hold for any * t 2 <ref> [0; 1] </ref>, and depend only on the performance of the weak learner on those distributions that are actually generated during the boosting process. The parameter fi t is chosen as a function of * t and is used for updating the weight vector. <p> Set w t i=1 w t 2. Call WeakLearn, providing it with the distribution p t ; get back a hypothesis h t : X ! <ref> [0; 1] </ref>. 3. Calculate the error of h t : * t = P N i jh t (x i ) y i j. 5. <p> Proof: We use a result about the VC-dimension of computation networks proved by Baum and Haussler <ref> [1] </ref>. <p> Thus the sum over all computation units of the VC-dimensions of the classes of functions associated with each unit is T d + (T + 1) &lt; (T + 1)(d + 1). Baum and Haussler's Theorem 1 <ref> [1] </ref> implies that the number of different functions that can be realized by h 2 fi T (H) when the domain is restricted to a set of size m is at most ((T + 1)em=(T + 1)(d + 1)) (T +1)(d+1) . <p> We will here consider final hypotheses of the form h f (x i ) = F (r (x i )) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in 18 we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (x i ) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (x <p> We will here consider final hypotheses of the form h f (x i ) = F (r (x i )) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in 18 we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (x i ) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (x i ) as <p> We will here consider final hypotheses of the form h f (x i ) = F (r (x i )) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in 18 we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (x i ) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (x i ) as the probability of predicting 1. <p> h f (x i ) = F (r (x i )) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in 18 we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (x i ) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (x i ) as the probability of predicting 1. Then the error E i~D [jh f (x i ) y i j] is simply the probability of an incorrect prediction. <p> Let the modified final hypothesis be defined by h f = F (r (x i )) where F satisfies the following for r 2 <ref> [0; 1] </ref>: F (1 r) = 1 F (r); and F (r) 2 t=1 ! 1=2r Then the error * of h f is bounded above by * 2 T 1 t=1 * t (1 * t ): For instance, it can be shown that the sigmoid function F (r) = <p> In our second version of multi-class boosting, we attempt to overcome this difficulty by extending the communication between the boosting algorithm and the weak learner. First, we allow the weak learner to generate more expressive hypotheses whose output is a vector in <ref> [0; 1] </ref> k , rather than a single label in Y . Intuitively, the yth component of this vector represents a "degree of belief" that the correct label is y. The components with large values (close to 1) correspond to those labels considered to be plausible. <p> There are several standard ways of making such a conversion, one of the most successful being the error-correcting output coding approach advocated by Dietterich and Bakiri [7]. Finally, in Section 5.3 we extend AdaBoost to boosting regression algorithms. In this case Y = <ref> [0; 1] </ref>, and the error of a hypothesis is defined as E (x;y)~P fi . <p> Applying Theorem 6, we can obtain a bound on this error, completing the proof. It is possible, for this version of the boosting algorithm, to allow hypotheses which generate for each x, not only a predicted class label h (x) 2 Y , but also a "confidence" (x) 2 <ref> [0; 1] </ref>. The learner then suffers loss 1=2 (x)=2 if its prediction is correct and 1=2 + (x)=2 otherwise. (Details omitted.) 5.2 Second multi-class extension In this section we describe a second alternative extension of AdaBoost to the case where the label space Y is finite. <p> As described above, the weak learner generates hypotheses which have the form h : X fiY ! <ref> [0; 1] </ref>. Roughly speaking, h (x; y) measures the degree to which it is believed that y is the correct label associated with instance x. <p> If h (x i ; y) = h (x i ; y i ), then one of the two answers is chosen uniformly at random. In the more general case that h takes values in <ref> [0; 1] </ref>, we interpret h (x; y) as a randomized decision for the procedure above. That is, we first choose a random bit b (x; y) which is 1 with probability h (x; y) and 0 otherwise. We then apply the above procedure to the stochastically chosen binary function b. <p> y); the resulting formula is called the pseudo-loss of h on training instance i with respect to q: ploss q (h; i) : 2 @ 1 h (x i ; y i ) + y6=y i 1 The function q : f1; : : :; N g fi Y ! <ref> [0; 1] </ref>, called the label weighting function, assigns to each example i in the training set a probability distribution over the k 1 discrimination problems defined above. <p> Call WeakLearn, providing it with the distribution D t and label weighting function q t ; get back a hypothesis h t : X fi Y ! <ref> [0; 1] </ref>. 3. Calculate the pseudo-loss of h t : * t = 1 N X D t (i) @ 1 h t (x i ; y i ) + y6=y i 1 4. Set fi t = * t =(1 * t ). 5. <p> In this setting, the label space is Y = <ref> [0; 1] </ref>. As before, the learner receives examples (x; y) chosen at random according to some distribution P, and its goal is to find a hypothesis h : X ! Y which, given some x value, predicts approximately the value y that is likely to be seen. <p> For each example (x i ; y i ) in the training set, we define a continuum of examples indexed by pairs (i; y) for all y 2 <ref> [0; 1] </ref>: the associated instance is ~x i;y = (x i ; y), and the label is ~y i;y = [[y y i ]]. (Recall that [[]] is 1 if predicate holds and 0 otherwise.) Although it is obviously infeasible to explicitly maintain an infinitely large training set, we will see <p> is directly proportional to the mean squared error: N X Z 1 fi fi ~y i;y ~ h (~x i;y ) fi 1 N X D (i) fi fi Z h (x i ) jy y i jdy fi fi = 2Z i=1 The constant of proportionality is 1=(2Z) 2 <ref> [1; 2] </ref>. Unraveling this reduction, we obtain the regression boosting procedure AdaBoost.R shown in Figure 5. As prescribed by the reduction, AdaBoost.R maintains a weight w t i;y for each instance i and label y 2 Y . <p> which appear in the figure can be evaluated explicitly since these only involve integration of piece-wise linear functions. 29 Algorithm AdaBoost.R Input: sequence of N examples h (x 1 ; y 1 ); : : : ; (x N ; y N )i with labels y i 2 Y = <ref> [0; 1] </ref> distribution D over the examples weak learning algorithm WeakLearn integer T specifying number of iterations Initialize the weight vector: w 1 D (i)jy y i j for i = 1; : : : ; N , y 2 Y , where Z = i=1 Z 1 jy y i <p> A remedy to this problem might be to allow weak hypotheses from a more general class of functions. One simple generalization is to allow for weak hypotheses that are defined by two functions: h : X ! <ref> [0; 1] </ref> as before, and : X ! [0; 1] which associates a measure of confidence to each prediction of h. The reduced hypothesis which we associate with this pair of functions is ~ h (x; y) = (1 + (x))=2 if h (x) y (1 (x))=2 otherwise. <p> A remedy to this problem might be to allow weak hypotheses from a more general class of functions. One simple generalization is to allow for weak hypotheses that are defined by two functions: h : X ! <ref> [0; 1] </ref> as before, and : X ! [0; 1] which associates a measure of confidence to each prediction of h. The reduced hypothesis which we associate with this pair of functions is ~ h (x; y) = (1 + (x))=2 if h (x) y (1 (x))=2 otherwise. <p> The method presented in this section for boosting with square loss can be used with any reasonable bounded loss function L : Y fi Y ! <ref> [0; 1] </ref>. Here, L (y 0 ; y) is a measure of the "discrepancy" between the observed label y and a predicted label y 0 ; for instance, above we used L (y 0 ; y) = (y 0 y) 2 .
Reference: [2] <author> David Blackwell. </author> <title> An analog of the minimax theorem for vector payoffs. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 6(1) </volume> <pages> 1-8, </pages> <month> Spring </month> <year> 1956. </year>
Reference-contexts: Algorithms with similar properties (but weaker convergence bounds) were first devised by Blackwell <ref> [2] </ref> and Hannan [14]. For more details see our related paper [13]. Example 4. Suppose that = is the unit ball in R n , and that (ffi; !) = jjffi !jj. <p> is directly proportional to the mean squared error: N X Z 1 fi fi ~y i;y ~ h (~x i;y ) fi 1 N X D (i) fi fi Z h (x i ) jy y i jdy fi fi = 2Z i=1 The constant of proportionality is 1=(2Z) 2 <ref> [1; 2] </ref>. Unraveling this reduction, we obtain the regression boosting procedure AdaBoost.R shown in Figure 5. As prescribed by the reduction, AdaBoost.R maintains a weight w t i;y for each instance i and label y 2 Y .
Reference: [3] <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Unpublished manuscript. </type> <note> Available from ftp://ftp.stat.berkeley.edu/pub/users/breiman/arcall.ps.Z., 1996. 31 </note>
Reference-contexts: Since it was first introduced, several successful experiments have been conducted using AdaBoost, including work by the authors [12], Drucker and Cortes [8], Jackson and Craven [16], Quinlan [21], and Breiman <ref> [3] </ref>. 4.2 Analysis Comparing Figures 1 and 2, there is an obvious similarity between the algorithms Hedge (fi) and AdaBoost. This similarity reflects a surprising "dual" relationship between the on-line allocation model and the problem of boosting.
Reference: [4] <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year>
Reference-contexts: Thus, as T increases, this difference decreases to zero. Our results for the on-line allocation model can be applied to a wide variety of learning problems, as we describe in Section 3. In particular, we generalize the results of Littlestone and Warmuth [20] and Cesa-Bianchi et al. <ref> [4] </ref> for the problem of predicting a binary sequence using the advice of a team of "experts." Whereas these authors proved worst-case bounds for making on-line randomized decisions over a binary decision and outcome space with a f0; 1g-valued discrete loss, we prove (slightly weaker) bounds that are applicable to any <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth [20] using the same algorithm. Their algorithm was later improved by Vovk [25] and Cesa-Bianchi et al. <ref> [4] </ref>. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. 8 Example 2.
Reference: [5] <author> Thomas H. Chung. </author> <title> Approximate methods for sequential decision making using expert advice. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 183-189, </pages> <year> 1994. </year>
Reference-contexts: Related generalizations of the expert prediction model were studied by Vovk [25], Kivinen and Warmuth [19], and Haussler, Kivinen and Warmuth [15]. Like us, these authors focused primarily on multiplicative weight-update algorithms. Chung <ref> [5] </ref> also presented a generalization, giving the problem a game-theoretic treatment. 2 Boosting Returning to the horse-racing story, suppose now that the gambler grows weary of choosing among the experts and instead wishes to create a computer program that will accurately predict the winner of a horse race based on the <p> Consider the following set-up used by Chung <ref> [5] </ref>.
Reference: [6] <author> Thomas M. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematical Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Thus, it can also be applied, for example, to the squared-distance loss function (ffi; !) = jjffi !jj 2 , as well as the log loss function (ffi; !) = ln (ffi !) used by Cover <ref> [6] </ref> for the design of "universal" investment portfolios. (In this last case, is the set of probability vectors on n points, and = [1=B; B] n for some constant B &gt; 1.) In many of the cases listed above, superior algorithms or analyses are known.
Reference: [7] <author> Thomas G. Dietterich and Ghulum Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: There are several standard ways of making such a conversion, one of the most successful being the error-correcting output coding approach advocated by Dietterich and Bakiri <ref> [7] </ref>. Finally, in Section 5.3 we extend AdaBoost to boosting regression algorithms. In this case Y = [0; 1], and the error of a hypothesis is defined as E (x;y)~P fi .
Reference: [8] <author> Harris Drucker and Corinna Cortes. </author> <title> Boosting decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Since it was first introduced, several successful experiments have been conducted using AdaBoost, including work by the authors [12], Drucker and Cortes <ref> [8] </ref>, Jackson and Craven [16], Quinlan [21], and Breiman [3]. 4.2 Analysis Comparing Figures 1 and 2, there is an obvious similarity between the algorithms Hedge (fi) and AdaBoost. This similarity reflects a surprising "dual" relationship between the on-line allocation model and the problem of boosting. <p> which the error of the final hypothesis on the validation set is minimized. (For an extensive analysis of the relations between different methods for selecting model complexity in learning, see Kearns et al. [17].) 17 Some initial experiments using AdaBoost on real-world problems conducted by ourselves and Drucker and Cortes <ref> [8] </ref> indicate that AdaBoost tends not to over-fit; on many problems, even after hundreds of rounds of boosting, the generalization error continues to drop, or at least does not increase. 4.4 A Bayesian interpretation The final hypothesis generated by AdaBoost is closely related to one suggested by a Bayesian analysis.
Reference: [9] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 705-719, </pages> <year> 1993. </year>
Reference-contexts: Drucker, Schapire and Simard <ref> [9] </ref>, in experiments they performed using boosting to improve the performance of a real-valued neural network, observed that summing the outcomes of the networks and then selecting the best prediction performs better than selecting the best prediction of each network and then combining them with a majority rule.
Reference: [10] <author> Yoav Freund. </author> <title> Data Filtering and Distribution Modeling Algorithms for Machine Learning. </title> <type> PhD thesis, </type> <institution> University of California at Santa Cruz, </institution> <year> 1993. </year> <note> Retrievable from: ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-37.ps.Z. </note>
Reference-contexts: Thus, distribution D t specifies the relative importance of each example for the current round. After T rounds, the booster must combine the weak hypotheses into a single prediction rule. Unlike the previous boosting algorithms of Freund <ref> [10, 11] </ref> and Schapire [22], the new algorithm needs no prior knowledge of the accuracies of the weak hypotheses. Rather, it adapts to these accuracies and generates a weighted majority hypothesis in which the weight of each weak hypothesis is a function of its accuracy. <p> We use WeakLearn to denote a generic weak learning algorithm. Schapire [22] showed that any weak learning algorithm can be efficiently transformed or "boosted" into a strong learning algorithm. Later, Freund <ref> [10, 11] </ref> presented the "boost-by-majority" algorithm that is considerably more efficient than Schapire's. Both algorithms work by calling a given weak learning algorithm WeakLearn multiple times, each time presenting it with a different distribution over the domain X, and finally combining all of the generated hypotheses into a single hypothesis.
Reference: [11] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <note> To appear. An extended abstract appeared in Proceedings of the Third Annual Workshop on Computational Learning Theory, </note> <year> 1990. </year>
Reference-contexts: Thus, distribution D t specifies the relative importance of each example for the current round. After T rounds, the booster must combine the weak hypotheses into a single prediction rule. Unlike the previous boosting algorithms of Freund <ref> [10, 11] </ref> and Schapire [22], the new algorithm needs no prior knowledge of the accuracies of the weak hypotheses. Rather, it adapts to these accuracies and generates a weighted majority hypothesis in which the weight of each weak hypothesis is a function of its accuracy. <p> We use WeakLearn to denote a generic weak learning algorithm. Schapire [22] showed that any weak learning algorithm can be efficiently transformed or "boosted" into a strong learning algorithm. Later, Freund <ref> [10, 11] </ref> presented the "boost-by-majority" algorithm that is considerably more efficient than Schapire's. Both algorithms work by calling a given weak learning algorithm WeakLearn multiple times, each time presenting it with a different distribution over the domain X, and finally combining all of the generated hypotheses into a single hypothesis. <p> We start by describing our new boosting algorithm in the simplest case that the label set Y consists of just two possible labels, Y = f0; 1g. In later sections, we give extensions of the algorithm for more general label sets. Freund <ref> [11] </ref> describes two frameworks in which boosting can be applied: boosting by filtering and boosting by sampling. In this paper, we use the boosting by sampling framework, which is the natural framework for analyzing "batch" learning, i.e., learning using a fixed training set which is stored in the computer's memory. <p> This bound 15 has the same asymptotic behavior as the bound given for the boost-by-majority algorithm <ref> [11] </ref>.
Reference: [12] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: It is interesting that the new boosting algorithm's final hypothesis uses the same combination rule that was observed to be better in practice, but which previously lacked theoretical justification. Since it was first introduced, several successful experiments have been conducted using AdaBoost, including work by the authors <ref> [12] </ref>, Drucker and Cortes [8], Jackson and Craven [16], Quinlan [21], and Breiman [3]. 4.2 Analysis Comparing Figures 1 and 2, there is an obvious similarity between the algorithms Hedge (fi) and AdaBoost. This similarity reflects a surprising "dual" relationship between the on-line allocation model and the problem of boosting. <p> On the other hand, our theoretical bound for boosting using the prediction error (Theorem 10) is stronger than the bound for ploss (Theorem 11). Empirical tests <ref> [12] </ref> have shown that pseudo-loss is generally more successful when the weak learners use very restricted hypotheses. However, for more powerful weak learners, such as decision-tree learning algorithms, there is little difference between using pseudo-loss and prediction error. Our algorithm, called AdaBoost.M2, is shown in Figure 4.
Reference: [13] <author> Yoav Freund and Robert E. Schapire. </author> <title> Game theory, on-line prediction and boosting. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 325-332, </pages> <year> 1996. </year>
Reference-contexts: Algorithms with similar properties (but weaker convergence bounds) were first devised by Blackwell [2] and Hannan [14]. For more details see our related paper <ref> [13] </ref>. Example 4. Suppose that = is the unit ball in R n , and that (ffi; !) = jjffi !jj.
Reference: [14] <author> James Hannan. </author> <title> Approximation to Bayes risk in repeated play. </title> <editor> In M. Dresher, A. W. Tucker, and P. Wolfe, editors, </editor> <title> Contributions to the Theory of Games, </title> <booktitle> volume III, </booktitle> <pages> pages 97-139. </pages> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Algorithms with similar properties (but weaker convergence bounds) were first devised by Blackwell [2] and Hannan <ref> [14] </ref>. For more details see our related paper [13]. Example 4. Suppose that = is the unit ball in R n , and that (ffi; !) = jjffi !jj.
Reference: [15] <author> David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <booktitle> In Computational Learning Theory: Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> pages 69-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk [25], Kivinen and Warmuth [19], and Haussler, Kivinen and Warmuth <ref> [15] </ref>. Like us, these authors focused primarily on multiplicative weight-update algorithms.
Reference: [16] <author> Jeffrey C. Jackson and Mark W. Craven. </author> <title> Learning sparse perceptrons. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Since it was first introduced, several successful experiments have been conducted using AdaBoost, including work by the authors [12], Drucker and Cortes [8], Jackson and Craven <ref> [16] </ref>, Quinlan [21], and Breiman [3]. 4.2 Analysis Comparing Figures 1 and 2, there is an obvious similarity between the algorithms Hedge (fi) and AdaBoost. This similarity reflects a surprising "dual" relationship between the on-line allocation model and the problem of boosting.
Reference: [17] <author> Michael Kearns, Yishay Mansour, Andrew Y. Ng, and Dana Ron. </author> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <year> 1995. </year> <month> 32 </month>
Reference-contexts: The value of T is then chosen to be the one for which the error of the final hypothesis on the validation set is minimized. (For an extensive analysis of the relations between different methods for selecting model complexity in learning, see Kearns et al. <ref> [17] </ref>.) 17 Some initial experiments using AdaBoost on real-world problems conducted by ourselves and Drucker and Cortes [8] indicate that AdaBoost tends not to over-fit; on many problems, even after hundreds of rounds of boosting, the generalization error continues to drop, or at least does not increase. 4.4 A Bayesian interpretation
Reference: [18] <author> Michael J. Kearns and Umesh V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We very briefly review the PAC learning model (see, for instance, Kearns and Vazirani <ref> [18] </ref> for a more detailed description). Let X be a set called the domain. A concept is a Boolean function c : X ! f0; 1g. A concept class C is a collection of concepts.
Reference: [19] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Using experts for predicting continuous outcomes. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> EuroCOLT '93, </volume> <pages> pages 109-120. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk [25], Kivinen and Warmuth <ref> [19] </ref>, and Haussler, Kivinen and Warmuth [15]. Like us, these authors focused primarily on multiplicative weight-update algorithms. <p> In the one-dimensional case (n = 1), this case was previously analyzed by Littlestone and Warmuth [20], and later improved upon by Kivinen and Warmuth <ref> [19] </ref>. This result depends only on the convexity and the bounded range of the loss function (ffi; !) with respect to ffi.
Reference: [20] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: In Section 2, we show that Littlestone and Warmuth's <ref> [20] </ref> "weighted majority" algorithm can be generalized to handle this problem, and we prove a number of bounds on the net loss. <p> Thus, as T increases, this difference decreases to zero. Our results for the on-line allocation model can be applied to a wide variety of learning problems, as we describe in Section 3. In particular, we generalize the results of Littlestone and Warmuth <ref> [20] </ref> and Cesa-Bianchi et al. [4] for the problem of predicting a binary sequence using the advice of a team of "experts." Whereas these authors proved worst-case bounds for making on-line randomized decisions over a binary decision and outcome space with a f0; 1g-valued discrete loss, we prove (slightly weaker) bounds <p> The algorithm and its analysis are direct generalizations of Littlestone and Warmuth's weighted majority algorithm <ref> [20] </ref>. The pseudo-code for Hedge (fi) is shown in Figure 1. The algorithm maintains a weight vector whose value at time t is denoted w t = 1 ; : : :; w t ff . At all times, all weights will be nonnegative. <p> form w t+1 i U fi (` t where U fi : [0; 1] ! [0; 1] is any function, parameterized by fi 2 [0; 1] satisfying fi r U fi (r) 1 (1 fi)r 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth <ref> [20] </ref>. The main idea is to derive upper and lower bounds on P N i which, together, imply an upper bound on the loss of the algorithm. We begin with an upper bound. <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth <ref> [20] </ref> using the same algorithm. Their algorithm was later improved by Vovk [25] and Cesa-Bianchi et al. [4]. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. 8 Example 2. <p> Thus, our results in this case give explicit bounds on the total error (i.e., distance between predicted and observed points) for the learner relative to the best of a team of experts. In the one-dimensional case (n = 1), this case was previously analyzed by Littlestone and Warmuth <ref> [20] </ref>, and later improved upon by Kivinen and Warmuth [19]. This result depends only on the convexity and the bounded range of the loss function (ffi; !) with respect to ffi.
Reference: [21] <author> J. Ross Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings, Fourteenth National Conference on Artificial Intelligence, </booktitle> <year> 1996. </year>
Reference-contexts: Since it was first introduced, several successful experiments have been conducted using AdaBoost, including work by the authors [12], Drucker and Cortes [8], Jackson and Craven [16], Quinlan <ref> [21] </ref>, and Breiman [3]. 4.2 Analysis Comparing Figures 1 and 2, there is an obvious similarity between the algorithms Hedge (fi) and AdaBoost. This similarity reflects a surprising "dual" relationship between the on-line allocation model and the problem of boosting.
Reference: [22] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Thus, distribution D t specifies the relative importance of each example for the current round. After T rounds, the booster must combine the weak hypotheses into a single prediction rule. Unlike the previous boosting algorithms of Freund [10, 11] and Schapire <ref> [22] </ref>, the new algorithm needs no prior knowledge of the accuracies of the weak hypotheses. Rather, it adapts to these accuracies and generates a weighted majority hypothesis in which the weight of each weak hypothesis is a function of its accuracy. <p> A weak PAC-learning algorithm satisfies the same conditions but only for * 1=2fl where fl &gt; 0 is either a constant, or decreases as 1=p where p is a polynomial in the relevant parameters. We use WeakLearn to denote a generic weak learning algorithm. Schapire <ref> [22] </ref> showed that any weak learning algorithm can be efficiently transformed or "boosted" into a strong learning algorithm. Later, Freund [10, 11] presented the "boost-by-majority" algorithm that is considerably more efficient than Schapire's. <p> In fact, when the performance of the weak learner is measured only in terms of error rate, this difficulty is unavoidable as is shown by the following informal example (also presented by Schapire <ref> [22] </ref>): Consider a learning problem where Y = f0; 1; 2g and suppose that it is "easy" to predict whether the label is 2 but "hard" to predict whether the label is 0 or 1.
Reference: [23] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: As for the choice of T , various general methods can be devised. One popular method is to use an upper bound on the VC-dimension of the concept class. This method is sometimes called "structural risk minimization." See Vapnik's book <ref> [23] </ref> for an extensive discussion of the theory of structural risk minimization. For our purposes, we quote Vapnik's Theorem 6.7: Theorem 7 (Vapnik) Let H be a class of binary functions over some domain X. Let d be the VC-dimension of H.
Reference: [24] <author> V. G. Vovk. </author> <title> A game of prediction with expert advice. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: The bound given in Equation (9) can be written as L Hedge (fi) c min i where c = ln (1=fi)=(1fi) and a = 1=(1fi). Vovk <ref> [24] </ref> analyzes prediction algorithms that have performance bounds of this form, and proves tight upper and lower bounds for the achievable values of c and a. Using Vovk's results, we can show that the constants a and c achieved by Hedge (fi) are optimal.
Reference: [25] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk <ref> [25] </ref>, Kivinen and Warmuth [19], and Haussler, Kivinen and Warmuth [15]. Like us, these authors focused primarily on multiplicative weight-update algorithms. <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth [20] using the same algorithm. Their algorithm was later improved by Vovk <ref> [25] </ref> and Cesa-Bianchi et al. [4]. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. 8 Example 2.
Reference: [26] <author> R. S. Wenocur and R. M. Dudley. </author> <title> Some special Vapnik-Chervonenkis classes. </title> <journal> Discrete Mathematics, </journal> <volume> 33 </volume> <pages> 313-318, </pages> <year> 1981. </year>
Reference-contexts: The VC-dimension of the set of linear threshold functions over R T is T + 1 <ref> [26] </ref>. Thus the sum over all computation units of the VC-dimensions of the classes of functions associated with each unit is T d + (T + 1) &lt; (T + 1)(d + 1).
References-found: 26

