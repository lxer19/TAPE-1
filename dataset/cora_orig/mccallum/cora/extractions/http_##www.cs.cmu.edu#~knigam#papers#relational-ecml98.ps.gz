URL: http://www.cs.cmu.edu/~knigam/papers/relational-ecml98.ps.gz
Refering-URL: http://www.cs.cmu.edu/~knigam/resume.html
Root-URL: 
Email: e-mail: hfirstnamei.hlastnamei@cs.cmu.edu  
Title: First-Order Learning for Web Mining  
Author: Mark Craven, Sean Slattery and Kamal Nigam 
Address: Pittsburgh, PA 15213-3891, USA  
Affiliation: School of Computer Science, Carnegie Mellon University  
Note: To appear in the proceedings of the 10th European Conference on Machine Learning.  
Abstract: We present compelling evidence that the World Wide Web is a domain in which applications can benefit from using first-order learning methods, since the graph structure inherent in hypertext naturally lends itself to a relational representation. We demonstrate strong advantages for two applications learning classifiers for Web pages, and learning rules to discover relations among pages.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> W. W. Cohen. </author> <title> Learning to classify English text with ILP methods. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming. </booktitle> <publisher> IOS Press, </publisher> <year> 1995. </year>
Reference-contexts: The graph-like structure provided by pages on the World Wide Web is one domain that seems natural for first-order representation, yet has not been previously studied in this context. Cohen <ref> [1] </ref> has used first-order methods for text classification, but the focus was on finding relations between words rather than between documents.
Reference: 2. <author> M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <type> Technical report, </type> <institution> Department of Computer Science, Carnegie Mellon Univ., </institution> <year> 1998. </year>
Reference-contexts: Our work on these two learning tasks has been conducted as part of a larger effort aimed at developing methods for automatically constructing knowledge bases by extracting information from the Web <ref> [2] </ref>. <p> We first present two classification algorithms, a conventional text learning algorithm (Naive Bayes) which ignores document relationships, and a first-order learner (Foil) which can use such information. More complete details of the data set, algorithms and experiments can be found elsewhere <ref> [2] </ref>. Table 1. Recall (R) and precision (P) percentages on each binary classification task using Naive Bayes, Foil with words only, and Foil with words and links.
Reference: 3. <author> S. Dzeroski and I. Bratko. </author> <title> Handling noise in inductive logic programming. </title> <editor> In S.H. Muggleton and K. Furukawa, editors, </editor> <booktitle> Proc. of the 2nd International Workshop on Inductive Logic Programming. </booktitle>
Reference-contexts: Once such a path is found, an initial clause is formed from the relations that constitute the path, and the clause is further refined by a hill-climbing search. Like Dzeroski and Bratko's m-Foil <ref> [3] </ref>, Path-mcp uses m-estimates of a clause's error to guide its construction. We have found that this evaluation function results in fewer, more general clauses than Foil's information gain measure. We evaluate our approach using the same four-fold cross-validation methodology we used in Section 2.
Reference: 4. <author> T. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <year> 1997. </year>
Reference-contexts: For example, each instance of the InstructorsOfCourse relation consists of a Course home page and a Person home page. Our data set of relation instances comprises 251 InstructorsOfCourse instances, 392 MembersOfProject instances, and 748 DepartmentOfPerson instances. As a representative conventional text classifier, we use the Naive Bayes classifier <ref> [4] </ref>.
Reference: 5. <author> J. R. Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proc. of the European Conf. on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: We use version 4.2 of Quinlan's Foil <ref> [5] </ref> system for learning first-order clauses, and two types of background relations to describe the data: * has word (Page) : This set of relations indicates that word occurs on Page.
Reference: 6. <author> B. Richards and R. Mooney. </author> <title> Learning relations by pathfinding. </title> <booktitle> In Proc. of the 10th Nat. Conf. on Artificial Intelligence, </booktitle> <pages> pages 50-55, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A neighborhood is the paragraph, list item, table entry, title or heading in which a hyperlink is contained. The vocabulary for this set includes 200 words. The algorithm we use for learning page relations augments Foil's hill-climbing search with a deterministic variant of Richards and Mooney's relational pathfinding method <ref> [6] </ref>. The basic idea underlying this method is that a relational problem domain can be thought of as a graph in which the nodes are the domain's constants and the edges correspond to relations which hold among constants.
References-found: 6

