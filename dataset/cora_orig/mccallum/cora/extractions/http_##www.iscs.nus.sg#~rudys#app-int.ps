URL: http://www.iscs.nus.sg/~rudys/app-int.ps
Refering-URL: 
Root-URL: 
Email: Email:rudys,liuh@iscs.nus.sg  
Title: Improving Backpropagation Learning with Feature Selection  
Author: Rudy Setiono Huan Liu 
Address: Ridge, Singapore 0511 Republic of Singapore  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Note: Appears in Applied Intelligence, Vol. 6, No. 2, 1996, pp. 129-140  
Abstract: There exist redundant, irrelevant and noisy data. Using proper data to train a network can speed up training, simplify the learned structure, and improve its performance. A two-phase training algorithm is proposed. In the first phase, the number of input units of the network is determined by using an information base method. Only those attributes that meet certain criteria for inclusion will be considered as the input to the network. In the second phase, the number of hidden units of the network is selected automatically based on the performance of the network on the training data. One hidden unit is added at a time only if it is necessary. The experimental results show that this new algorithm can achieve a faster learning time, a simpler network and an improved performance. Keywords: Feedforward neural network, backpropagation, information theory, feature selection.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Ash, </author> <title> "Dynamic node creation in backpropagation networks," </title> <journal> Connection Science, vol.1, </journal> <volume> no. 4, </volume> <pages> pp. 365-375, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction We are concerned in this paper with the problem of distinguishing patterns from two disjoint sets in n-dimensional space. In recent years, much research have been done on algorithms that dynamically construct neural networks for solving this pattern classification problem. These algorithms include the dynamic node creation <ref> [1] </ref>, the cascade correlation algorithm [2], the tiling algorithm [3], the self-organizing neural network [4], and the upstart algorithm [5]. These construction algorithms were designed to eliminate the need to determine the number of hidden units prior to training required by backpropagation learning. <p> Note that throughout this paper, a pattern will be considered to be correctly classified if the following condition is satisfied e i = fi fi fi 0:45: 3.2 Quasi-Newton method for neural network training The main difference between our neural network construction algorithm and the Dynamic Node Creation <ref> [1] </ref> is in the training of the growing network. We use a variant of the quasi-Newton method which is considerably faster than the gradient descent method. This method is described in [12] and can be outlined as follows. SR1/BFGS Algorithm for miniming F (z) Step 0. Initialization.
Reference: [2] <author> S.E. Fahlman and C. Lebiere, </author> <booktitle> "The cascade-correlation learning architecture," in Ad vances in Neural Information Processing Systems II, </booktitle> <editor> edited by D. Touretzky, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <pages> pp. 524-532, </pages> <year> 1989. </year>
Reference-contexts: In recent years, much research have been done on algorithms that dynamically construct neural networks for solving this pattern classification problem. These algorithms include the dynamic node creation [1], the cascade correlation algorithm <ref> [2] </ref>, the tiling algorithm [3], the self-organizing neural network [4], and the upstart algorithm [5]. These construction algorithms were designed to eliminate the need to determine the number of hidden units prior to training required by backpropagation learning.
Reference: [3] <author> M. Mezard and J.P. Nadal, </author> <title> "Learning in feedforward layered networks: The tiling algo rithm," </title> <journal> Journal of Physics A, </journal> <volume> vol. 22, no. 12, </volume> <pages> pp. 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: In recent years, much research have been done on algorithms that dynamically construct neural networks for solving this pattern classification problem. These algorithms include the dynamic node creation [1], the cascade correlation algorithm [2], the tiling algorithm <ref> [3] </ref>, the self-organizing neural network [4], and the upstart algorithm [5]. These construction algorithms were designed to eliminate the need to determine the number of hidden units prior to training required by backpropagation learning.
Reference: [4] <author> M.F. Tenorio and W. Lee, </author> <title> "Self-organizing network for optimum supervised learning," </title> <journal> IEEE Transactions on Neural Networks, vol.1, </journal> <volume> no. 1, </volume> <pages> pp. 100-110, </pages> <year> 1990. </year>
Reference-contexts: In recent years, much research have been done on algorithms that dynamically construct neural networks for solving this pattern classification problem. These algorithms include the dynamic node creation [1], the cascade correlation algorithm [2], the tiling algorithm [3], the self-organizing neural network <ref> [4] </ref>, and the upstart algorithm [5]. These construction algorithms were designed to eliminate the need to determine the number of hidden units prior to training required by backpropagation learning. The aim of these algorithms is to use as few hidden units as possible to solve a given problem.
Reference: [5] <author> M. Frean, </author> <title> "The upstart algorithm: a method for constructing and training feedforward neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 2, no. 2, </volume> <pages> pp. 198-209, </pages> <year> 1990. </year>
Reference-contexts: In recent years, much research have been done on algorithms that dynamically construct neural networks for solving this pattern classification problem. These algorithms include the dynamic node creation [1], the cascade correlation algorithm [2], the tiling algorithm [3], the self-organizing neural network [4], and the upstart algorithm <ref> [5] </ref>. These construction algorithms were designed to eliminate the need to determine the number of hidden units prior to training required by backpropagation learning. The aim of these algorithms is to use as few hidden units as possible to solve a given problem.
Reference: [6] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Experimental results are reported in Section 4. We have tested our proposed algorithm on two problems: the MONK's problems and the mushroom problem. The data for these problems are available via ftp to ics.uci.edu <ref> [6] </ref>. Finally in Section 5, a brief summary of the paper is given. A brief word about our notation now. <p> The data can be obtained from the University of California-Irvine repository <ref> [6] </ref>. It was reported that an accuracy rate of 95 % had been obtained [18],[19]. There are a total of 8124 patterns in the data set, one thousand of which are selected randomly to be used for training and the rest for testing.
Reference: [7] <author> J.R. Quinlan, </author> <title> "Induction of decision trees," </title> <journal> Machine Learning, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: More informative attributes for classification are chosen as features to be used as input in neural network training. Suppose a set S of objects is divided into N c subsets, called classes. The expected information (or entropy) for classification is <ref> [7] </ref> I (S) = c=1 n n c ; I (S ik ) = c=1 n ik n ikc ; where n c is the number of objects belonging to class C c , and n is the total number of objects in S.
Reference: [8] <author> R. Setiono and L.C.K. Hui, </author> <title> "Use of quasi-Newton method in a feedforward neural net work construction algorithm," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 273-277, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: To overcome this difficulty, many algorithms that construct a network dynamically have been proposed. The algorithm which generates a single hidden layer feedforward network that we have recently proposed <ref> [8] </ref> can be outlined as follows Feedforward neural network construction algorithm 1. Let h = 1 be the initial number of hidden units in the network. Set all initial weights in the network randomly. 2. Find a point that minimizes an error function. 3.
Reference: [9] <author> K.J. Lang and M.J. Witbrock, </author> <title> "Learning to tell two spirals apart," </title> <booktitle> in Proceedings of the 1988 Connectionist Model Summer School, </booktitle> <editor> edited by D. Touretzky, G. Hinton, and T. Sejnowski, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA., </address> <pages> pp. 52-59, </pages> <year> 1988. </year>
Reference-contexts: For all the results reported in this 6 paper, we have used the hyperbolic tangent function as the activation function at the hidden units and the sigmoid function at the output unit. To improve the convergence in neural network training, it has been suggested by several authors <ref> [9] </ref> [10] that the cross-entropy error function be used F (w; v) = i=1 t i log S i + (1 t i ) log (1 S i ) : (6) We use this cross-entropy function in conjunction with our construction algorithm.
Reference: [10] <author> A. van Ooyen and B. Nienhuis, </author> <title> "Improving the convergence of the backpropagation algorithm," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 465-471, </pages> <year> 1992. </year>
Reference-contexts: For all the results reported in this 6 paper, we have used the hyperbolic tangent function as the activation function at the hidden units and the sigmoid function at the output unit. To improve the convergence in neural network training, it has been suggested by several authors [9] <ref> [10] </ref> that the cross-entropy error function be used F (w; v) = i=1 t i log S i + (1 t i ) log (1 S i ) : (6) We use this cross-entropy function in conjunction with our construction algorithm.
Reference: [11] <author> R. Setiono, </author> <title> "A neural network construction algorithm which maximizes the likelihood function," </title> <journal> Connection Science, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 147-166, </pages> <year> 1995. </year> <month> 19 </month>
Reference-contexts: Given two disjoint sets of patterns, it has been shown <ref> [11] </ref> that using this cross-entropy measure of the errors, it is always possible to construct a neural network such that all the patterns are correctly classified.
Reference: [12] <author> P.K.H. Phua and R. Setiono, </author> <title> "Combined quasi-Newton updates for unconstrained opti mization", </title> <institution> Dept. of Information Systems and Computer Science, National University of Singapore, </institution> <type> Technical Report TR41/92, </type> <year> 1992. </year>
Reference-contexts: We use a variant of the quasi-Newton method which is considerably faster than the gradient descent method. This method is described in <ref> [12] </ref> and can be outlined as follows. SR1/BFGS Algorithm for miniming F (z) Step 0. Initialization. Choose any z 1 as a starting point. Let H 1 = I, set k = 1. Let * &gt; 0 be a small terminating scalar. Step 1. Iterative Step.
Reference: [13] <author> C.G. </author> <title> Broyden, "The convergence of a class of double rank minimization, Algorithm 2, the new algorithm", </title> <journal> Journal of the Institute of Mathematics and Applications, </journal> <volume> no. 6, </volume> <pages> pp. 222-231, </pages> <year> 1970. </year>
Reference-contexts: The SR1 update ( 7) is called a Symmetric Rank-one update, because the H k is updated by a rank one matrix, while the BFGS update (8) was independently proposed by Broyden, Fletcher, Goldfarb, and Shanno in 1970 <ref> [13, 14, 15, 16] </ref>.
Reference: [14] <author> R. Fletcher, </author> <title> "A new approach to variable metric algorithms", </title> <journal> Computer Journal, </journal> <volume> no. 13, </volume> <pages> pp. </pages> <month> 317-322 </month> <year> (1970). </year>
Reference-contexts: The SR1 update ( 7) is called a Symmetric Rank-one update, because the H k is updated by a rank one matrix, while the BFGS update (8) was independently proposed by Broyden, Fletcher, Goldfarb, and Shanno in 1970 <ref> [13, 14, 15, 16] </ref>.
Reference: [15] <author> D. Goldfarb, </author> <title> "A family of variable metric algorithms derived by variational means", </title> <journal> Mathematics of Computation, </journal> <volume> no. 24, </volume> <pages> pp. 23-26, </pages> <year> 1970. </year>
Reference-contexts: The SR1 update ( 7) is called a Symmetric Rank-one update, because the H k is updated by a rank one matrix, while the BFGS update (8) was independently proposed by Broyden, Fletcher, Goldfarb, and Shanno in 1970 <ref> [13, 14, 15, 16] </ref>.
Reference: [16] <author> D.F. Shanno, </author> <title> "Conditioning of quasi-Newton methods for function minimization", </title> <journal> Math ematics of Computation, </journal> <volume> no. 24, </volume> <pages> pp. 647-656, </pages> <year> 1970. </year>
Reference-contexts: The SR1 update ( 7) is called a Symmetric Rank-one update, because the H k is updated by a rank one matrix, while the BFGS update (8) was independently proposed by Broyden, Fletcher, Goldfarb, and Shanno in 1970 <ref> [13, 14, 15, 16] </ref>.
Reference: [17] <editor> S.B. Thrun et al., </editor> <title> "The MONK's Problems A performance comparison of different learning algorithms", </title> <institution> Department of Computer Science, Carnegie Mellon University, CMU-CS-91-197, </institution> <year> 1991. </year>
Reference-contexts: These are the MONK's problems and the mushroom problem. They are selected because the data are available publicly. Many learning algorithms have been applied to solve these problems and the results can be used for comparison. 4.1 The MONK's problems We apply our algorithm to three benchmark MONK's problems <ref> [17] </ref>. <p> Also, networks with fewer hidden units have better generalization capability. 14 data and 100 % accuracy on the test data for problem M 3 . 15 4.1.3 Comparison with other works The MONK's problems have been the subject of many studies on learning algorithms <ref> [17] </ref>. The best results reported have been obtained from backpropagation networks with weight decay (BPWD). We compare our best results, both without feature selection (WOFS) and with feature selection (WFS) in Table 7 below.
Reference: [18] <author> J.S. Schlimmer, </author> <title> "Concept acquisition through representational adjustment," </title> <institution> Dept. of Information and Computer Science, University of California, Irvine, </institution> <type> Technical Report 87-19, </type> <year> 1987. </year>
Reference: [19] <author> W. Iba, J. Wogulis and P. Langley, </author> <title> "Trading off simplicity and coverage in incremen tal concept learning," </title> <booktitle> in Proceedings of the 5th International Conference on Machine Learning, </booktitle> <address> Ann Arbor, Michigan, </address> <year> 1988, </year> <pages> pp. 73-79. 20 </pages>
References-found: 19

