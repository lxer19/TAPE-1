URL: ftp://synapse.cs.byu.edu/pub/papers/andersen_95d.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: tim@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: A Provably Convergent Dynamic Training Method for Multilayer Perceptron Networks  
Author: Tim L. Andersen and Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: Proceedings of the 2nd International Symposium on Neuroinformatics and Neurocomputers, pp. 77-84, 1995.  
Abstract: This paper presents a new method for training multilayer perceptron networks called DMP1 (Dynamic Multilayer Perceptron 1). The method is based upon a divide and conquer approach which builds networks in the form of binary trees, dynamically allocating nodes and layers as needed. The individual nodes of the network are trained using a gentetic algorithm. The method is capable of handling real-valued inputs and a proof is given concerning its convergence properties of the basic model. Simulation results show that DMP1 performs favorably in comparison with other learning algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Azimi-Sadjadi, </author> <title> Mahmood (1993). Recursive Dynamic Node Creation in Multilayer Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol 4, No 2, </volume> <pages> pp 242 256. </pages>
Reference: <author> Bartlett, </author> <title> Eric (1994). Dynamic Node Architecture Learning: An Information Theoretic Approach. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 7, No 1, </volume> <pages> pp 129-140. </pages>
Reference: <author> Fahlman, S. and C. </author> <month> Lebiere </month> <year> (1991). </year> <title> The Cascade Correlation Learning Architecture. </title>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation <ref> (Fahlman 1991) </ref>, Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <author> Martinez, Tony and Jacques Vidal (1988). </author> <title> Adaptive Parallel Logic Networks. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <volume> Vol 5, </volume> <pages> pp 26-58. </pages>
Reference: <author> Martinez, T. R. and Campbell, D. M. </author> <year> (1991). </year> <title> "A Self-Adjusting Dynamic Logic Module", </title> <journal> Journal of Parallel and Distributed Computing, v11, </journal> <volume> no. 4, </volume> <pages> pp. 303-13. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The numbers in parenthesis are the standard deviations for the reported results. The other methods shown are c4.5 <ref> (Quinlan 1986) </ref>, and a multilayer back propogation network. These results are taken from (Zarndt 1995). The last row of the table gives the average accuracy across all of the data sets for each of the methods.
Reference: <author> Romaniuk, Steve and Lawrence Hall (1993). </author> <title> Divide and Conquer Neural Networks. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 6, </volume> <pages> pp 1105-1116. </pages>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. <p> Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP <ref> (Romaniuk 1993) </ref>, Cascade Correlation (Fahlman 1991), Iterative Atrophy (Smotroff 1991), DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase. <p> This paper presents a new dynamic method for training multilayer perceptron networks which we call DMP1 (Dynamic Multilayer Perceptron 1). This method is similar in spirit to the DCN method given in <ref> (Romaniuk 1993) </ref> in that: The network begins with a single perceptron node and dynamically adds nodes as needed. As nodes are added to the network, each node is trained independently from all other nodes.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The Perceptron: A probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> Vol. 65, No. </volume> <pages> 6. </pages>
Reference-contexts: 1 Introduction One of the first models used in the field of neural networking is the single layer, simple perceptron model <ref> (Rosenblatt 1958) </ref>. The well understood weakness of single layer networks is that they are able to learn only those functions which are linearly separable.
Reference: <author> Smotroff, Ira, David Friedman and Dennis Connolly (1991). </author> <title> Self Organizing Modular Neural Networks. </title> <booktitle> International Joint Conference on Neural Networks, II, </booktitle> <volume> 187 192. </volume>
Reference-contexts: Thus, several multilayer training algorithms have been proposed which do not require the user to specify the network architecture a-priori. Some of these include EGP (Romaniuk 1993), Cascade Correlation (Fahlman 1991), Iterative Atrophy <ref> (Smotroff 1991) </ref>, DCN (Romaniuk 1993), ASOCS (Martinez 1990), and DNAL (Bartlett 1993). All of these methods seek to dynamically generate an appropriate network structure for solving the given learning problem during the training phase.
Reference: <author> Zarndt, </author> <title> Frederick (1995). </title> <type> Masters Thesis, </type> <note> in preparation. </note>
Reference-contexts: The numbers in parenthesis are the standard deviations for the reported results. The other methods shown are c4.5 (Quinlan 1986), and a multilayer back propogation network. These results are taken from <ref> (Zarndt 1995) </ref>. The last row of the table gives the average accuracy across all of the data sets for each of the methods. The average accuracy on the training set across the 15 runs is reported in table 1.
References-found: 10

