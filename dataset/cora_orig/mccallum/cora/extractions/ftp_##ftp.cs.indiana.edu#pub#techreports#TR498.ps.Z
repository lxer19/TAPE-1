URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR498.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Dead Code Elimination Using Program-Based Regular Tree Grammars  
Author: Yanhong A. Liu and Scott D. Stoller 
Date: November 1997  
Abstract: This paper describes a general and powerful method for dead code analysis and elimination in the presence of recursive data constructions. We represent partially dead recursive data using liveness patterns based on general regular tree grammars extended with the notion of live and dead, and we formulate the analysis as computing transformers of liveness patterns. To guarantee that the analysis terminates, we use finite domains of liveness patterns. The domains are based on the programs being analyzed. This results in a most precise liveness pattern for the data at each point in a program, which is significantly more precise than results from previous methods. Also, the algorithms take in practice linear time in terms of the size of the program. We further refine the analysis results to obtain the most precise deterministic results. This refinement takes polynomial time in terms of the size of the program but linear time in terms of the size of the resulting grammar, so it is optimal in this sense. The analysis results are used to identify and eliminate dead code in given programs. The general framework for representing and analyzing properties of recursive data structures using general regular tree grammars applies to other analyses as well.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> S. Abramsky and C. Hankin, editors. </editor> <title> Abstract Interpretation of Declarative Languages. Ellis Horwood Series in Computers and Their Applications. </title> <editor> E. </editor> <publisher> Horwood, Chichester; Halsted Press, </publisher> <address> New York, </address> <year> 1987. </year> <month> 14 </month>
Reference-contexts: Using finite domains of liveness patterns. As with most analyses based on abstract interpretation <ref> [1] </ref>, we ensure termination by using a finite abstract domain D. We do this on a per-program basis. Since the set I of program points is finite for a given program, we just need to use a finite domain G of grammars.
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas--sachusetts, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Dead computations produce values that never get used <ref> [2] </ref>. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse [33, 2]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. <p> 1 Introduction Dead computations produce values that never get used [2]. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse <ref> [33, 2] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization [29, 28], and compile-time garbage collection [21, 18, 35].
Reference: [3] <author> A. Aiken and B. R. Murphy. </author> <title> Static type inference in a dynamically typed language. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [4] <author> K. Arnold and J. Golsing. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [18, 10, 23, 28, 37, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [40, 13, 30, 4] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253.
Reference: [5] <author> R. Bodk and R. Gupta. </author> <title> Partial dead code elimination using slicing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on PLDI, </booktitle> <pages> pages 159-170, </pages> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>. <p> Authors' address: Computer Science Department, Indiana University, 215 Lindley Hall, Bloomington, IN 47405. Email: fliu,stollerg@cs.indiana.edu. 1 This is different from partial dead code, code that is dead on some but not all computation paths <ref> [23, 5] </ref>. 1 difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures. Several methods have been studied, but all have limitations [21, 18, 28, 37].
Reference: [6] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In Proceedings of the 1992 ACM Conference on LFP, </booktitle> <pages> pages 11-20, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion <ref> [42, 6] </ref>, as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs.
Reference: [7] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization <ref> [7, 34, 29, 27] </ref>, caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [8] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers; Preliminary Notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference: [9] <author> P. Cousot and R. Cousot. </author> <title> Formal language, grammar and set-constraint-based program analysis by abstract interpretation. </title> <booktitle> In Proceedings of the 17th International Conference on FPCA, </booktitle> <pages> pages 170-181, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> Compared with that work, we also handle more program constructs, namely, binding expressions and user-defined constructors of arbitrary arity. We believe that our treatment is also more rigorous, since we adopt the view that regular-tree-grammar-based program analysis is also abstract interpretation <ref> [9] </ref>. We extend the grammars and handle L and D specially in grammar operations. We design appropriate finite grammar domains to yield precise and efficient analysis methods. There are two other standard ways to guarantee the termination of the analysis: using finite transformers and using approximation operations. <p> There are two other standard ways to guarantee the termination of the analysis: using finite transformers and using approximation operations. As discussed by Cousot and Cousot, using finite transformers is essentially a masking of the explicit use of an approximation, called widening, when defining transformers <ref> [9] </ref>. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. In a separate paper [26], we describe three approximation operations that together produce significantly more precise analysis results than previous methods. <p> The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 9] </ref>, we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [10] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. M. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>.
Reference: [11] <author> A. De Niel, E. Bevers, and K. De Vlaminck. </author> <title> Program bifurcation for a polymorphically typed functional langauge. </title> <booktitle> In Proceedings of the Symposium on PEPM, </booktitle> <pages> pages 142-153, </pages> <address> New Haven, Connecticut, </address> <month> June </month> <year> 1991. </year> <note> Published as SIGPLAN Notices, 26(9). </note>
Reference-contexts: It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [25]. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [32, 11] </ref>. Another kind of analysis for recursive data is escape analysis [35, 12], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [12] <author> A. Deutsch. </author> <title> On the complexity of escape analysis. </title> <booktitle> In Conference Record of the 24rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 358-371, </pages> <address> Paris, France, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [32, 11]. Another kind of analysis for recursive data is escape analysis <ref> [35, 12] </ref>, but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [21] uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values.
Reference: [13] <author> R. K. Dybvig. </author> <title> The Scheme Programming Language. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [18, 10, 23, 28, 37, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [40, 13, 30, 4] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253.
Reference: [14] <author> F. Gecseg and M. Steinb. </author> <title> Tree Automata. </title> <publisher> Akademiai Kiado, </publisher> <address> Budapest, </address> <year> 1984. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars <ref> [14] </ref>, which allow bounded, and often precise, representations of unbounded data [20, 31, 32, 3, 39, 9, 37]. <p> Simplification removes A ! D, B ! D, and productions for N 3 and N 6 . We obtain: getmin 1 (L) = fN ! nil () j cons (A; B); A! triple (D; L; D); B ! nil () j cons (A; B)g Further simplification requires minimization <ref> [14] </ref> and results in: getmin 1 (L) = fN ! nil () j cons (A; N); A! triple (D; L; D)g Note that minimization is not needed for the preciseness of the analysis results, nor is it needed for identifying and eliminating dead code.
Reference: [15] <author> C. A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections <ref> [38, 15] </ref>, which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest [43, 24, 32, 37]. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [16] <author> N. Heintze. </author> <title> Set-Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 9] </ref>, we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [17] <author> N. Heintze. </author> <title> Set-based analysis of ML programs. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LFP, </booktitle> <pages> pages 306-317, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [16, 17, 9] </ref>, we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [18] <author> J. Hughes. </author> <title> Compile-time analysis of functional programs. </title> <editor> In D. Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, chapter 5, </booktitle> <pages> pages 117-153. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization [29, 28], and compile-time garbage collection <ref> [21, 18, 35] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>. <p> Several methods have been studied, but all have limitations <ref> [21, 18, 28, 37] </ref>. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. <p> Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [21] uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes <ref> [18] </ref> uses contexts that correspond to liveness patterns. Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. <p> To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection <ref> [21, 18, 35] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [19] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on POPL, </booktitle> <pages> pages 244-256, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: However, methods used there for handling unbounded growth of such projections are crude. The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick <ref> [19] </ref>, where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times [31, 32, 3, 11, 39, 37].
Reference: [20] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 4, </booktitle> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> Doing all the simplifications together, say at the end, takes O (P ) time. The correctness of this algorithm can be proved in a similar way to when only the selector form is used <ref> [20] </ref>. Nonterminals are associated with program points, so there are O (P ) of them.
Reference: [21] <author> S. B. Jones and D. Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 54-74, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization [29, 28], and compile-time garbage collection <ref> [21, 18, 35] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> Several methods have been studied, but all have limitations <ref> [21, 18, 28, 37] </ref>. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. <p> Another kind of analysis for recursive data is escape analysis [35, 12], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer <ref> [21] </ref> uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes [18] uses contexts that correspond to liveness patterns. <p> To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection <ref> [21, 18, 35] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [22] <author> K. Kennedy. </author> <title> Use-definition chains with applications. </title> <journal> Journal of Computer Languages, </journal> <volume> 3(3) </volume> <pages> 163-179, </pages> <year> 1978. </year>
Reference: [23] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on PLDI, </booktitle> <pages> pages 147-158, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>. <p> Authors' address: Computer Science Department, Indiana University, 215 Lindley Hall, Bloomington, IN 47405. Email: fliu,stollerg@cs.indiana.edu. 1 This is different from partial dead code, code that is dead on some but not all computation paths <ref> [23, 5] </ref>. 1 difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures. Several methods have been studied, but all have limitations [21, 18, 28, 37].
Reference: [24] <author> J. Launchbury. </author> <title> Projection Factorisations in Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computing, University of Glasgow, </institution> <year> 1989. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [38, 15], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [43, 24, 32, 37] </ref>. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing . <p> Wadler and Hughes use projections for strictness analysis [43]. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation <ref> [24] </ref>. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [25].
Reference: [25] <author> J. Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 80-91, </pages> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [24]. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well <ref> [25] </ref>. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [32, 11].
Reference: [26] <author> Y. A. Liu. </author> <title> Dependence analysis for recursive data. </title> <month> October </month> <year> 1997. </year>
Reference-contexts: As discussed by Cousot and Cousot, using finite transformers is essentially a masking of the explicit use of an approximation, called widening, when defining transformers [9]. Approximation operations provide a more general solution and make the analysis framework more modular and flexible. In a separate paper <ref> [26] </ref>, we describe three approximation operations that together produce significantly more precise analysis results than previous methods. Each operation is efficient, but due to their generality and interaction, that work does not have an exact characterization of the total number of iterations needed.
Reference: [27] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 157-170, </pages> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization <ref> [7, 34, 29, 27] </ref>, caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [28] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization <ref> [29, 28] </ref>, and compile-time garbage collection [21, 18, 35]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>. <p> Several methods have been studied, but all have limitations <ref> [21, 18, 28, 37] </ref>. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. <p> Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. The analysis for pruning by Liu and Teitelbaum <ref> [28] </ref> uses projections to specify specific components of tuple values and thus provide more accurate information. However, methods used there for handling unbounded growth of such projections are crude. <p> To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement <ref> [28] </ref>, deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [29] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization <ref> [29, 28] </ref>, and compile-time garbage collection [21, 18, 35]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization <ref> [7, 34, 29, 27] </ref>, caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [30] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [18, 10, 23, 28, 37, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [40, 13, 30, 4] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253.
Reference: [31] <author> P. Mishra and U. Reddy. </author> <title> Declaration-free type checking. </title> <booktitle> In Conference Record of the 12th Annual ACM Symposium on POPL, </booktitle> <pages> pages 7-21, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [32] <author> T. Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 12-25, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [38, 15], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [43, 24, 32, 37] </ref>. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing . <p> Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [25]. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [32, 11] </ref>. Another kind of analysis for recursive data is escape analysis [35, 12], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [33] <editor> S. S. Muchnick and N. D. Jones, editors. </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: 1 Introduction Dead computations produce values that never get used [2]. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse <ref> [33, 2] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization [29, 28], and compile-time garbage collection [21, 18, 35].
Reference: [34] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization <ref> [7, 34, 29, 27] </ref>, caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [35] <author> Y. G. Park and B. Goldber. </author> <title> Escape analysis on lists. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 116-127, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization [37], incrementalization [29, 28], and compile-time garbage collection <ref> [21, 18, 35] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [32, 11]. Another kind of analysis for recursive data is escape analysis <ref> [35, 12] </ref>, but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [21] uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values. <p> To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection <ref> [21, 18, 35] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [36] <author> W. Pugh and E. Rosser. </author> <title> Iteration space slicing and its application to communication optimization. </title> <type> Technical Report CS-TR-3737, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: This paper discusses techniques for recursion. The basic ideas should extend to loops. A recent work has just started this direction; it extends slicing to symbolically capture particular iterations in a loop <ref> [36] </ref>. Object-oriented programming is used widely, but cross-class optimization heavily depends on inlining, which often causes code blow-up. Grammar-based analysis and transformation can be applied to methods across classes without inlining.
Reference: [37] <author> T. Reps and T. Turnidge. </author> <title> Program specialization via program slicing. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the Dagstuhl Seminar on Partial Evaluation, volume 1110 of Lecture Notes in Computer Science, </booktitle> <pages> pages 409-429. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [44, 37] </ref>, specialization [37], incrementalization [29, 28], and compile-time garbage collection [21, 18, 35]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [44, 37], specialization <ref> [37] </ref>, incrementalization [29, 28], and compile-time garbage collection [21, 18, 35]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [18, 10, 23, 28, 37, 5] </ref>. <p> Several methods have been studied, but all have limitations <ref> [21, 18, 28, 37] </ref>. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. <p> For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [38, 15], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [43, 24, 32, 37] </ref>. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing . <p> Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> It is later used to describe other data flow information such as types and binding times [31, 32, 3, 11, 39, 37]. In particular, the 13 analysis for backward slicing by Reps and Turnidge <ref> [37] </ref> explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis [16, 17, 9], we do not know any work that treats precise and efficient dead code analysis for recursive data as we do. To conclude, the method and algorithms studied here have many applications: program slicing and specialization <ref> [44, 37] </ref>, strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
Reference: [38] <author> D. S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In M. Broy and G. Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year> <booktitle> Lecture notes of 1981 Marktoberdorf Summer School on Theoretical Foundations of Programming Methodology, directed by F.L. Bauer, E.W. Dijkstra, and C.A.R. Hoare. </booktitle>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections <ref> [38, 15] </ref>, which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest [43, 24, 32, 37]. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [39] <author> M. H. Strensen. </author> <title> A grammar-based data-flow analysis to stop deforestation. </title> <editor> In S. Tison, editor, CAAP'94: </editor> <booktitle> Proceedings of the 19th International Colloquium on Trees in Algebra and Programming, volume 787 of Lecture Notes in Computer Science, </booktitle> <pages> pages 335-351, </pages> <address> Edinburgh, U.K., April 1994. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [14], which allow bounded, and often precise, representations of unbounded data <ref> [20, 31, 32, 3, 39, 9, 37] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [19], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [31, 32, 3, 11, 39, 37] </ref>. In particular, the 13 analysis for backward slicing by Reps and Turnidge [37] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [40] <author> G. L. Steele. </author> <title> Common Lisp the Language. </title> <publisher> Digital Press, </publisher> <year> 1984. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [18, 10, 23, 28, 37, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [40, 13, 30, 4] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253.
Reference: [41] <author> F. </author> <title> Tip. A survey of program slicing techniques. </title> <journal> Journal of Programming Languages, </journal> <volume> 3(3) </volume> <pages> 121-189, </pages> <month> September </month> <year> 1995. </year>
Reference: [42] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoretical Computer Science, </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year> <note> Special issue of selected papers from the 2nd ESOP. </note>
Reference-contexts: To conclude, the method and algorithms studied here have many applications: program slicing and specialization [44, 37], strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion <ref> [42, 6] </ref>, as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs.
Reference: [43] <author> P. Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on FPCA, volume 274 of Lecture Notes in Computer Science, </booktitle> <pages> pages 385-407, </pages> <address> Portland, Oregon, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [38, 15], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [43, 24, 32, 37] </ref>. Let X 3 be the domain of all possible values computed by our programs, including ? and values containing . <p> One needs to unfold the definition of minmax to removing such dead computations. 7 Related work and conclusion Our backward dependence analysis uses liveness patterns, which are domain projections, to specify sufficient information. Wadler and Hughes use projections for strictness analysis <ref> [43] </ref>. Their analysis is also backward but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [24].
Reference: [44] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year> <month> 16 </month>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [44, 37] </ref>, specialization [37], incrementalization [29, 28], and compile-time garbage collection [21, 18, 35]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 22, 34, 2, 21, 18, 10, 23, 29, 28, 41, 37]. <p> While regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis [16, 17, 9], we do not know any work that treats precise and efficient dead code analysis for recursive data as we do. To conclude, the method and algorithms studied here have many applications: program slicing and specialization <ref> [44, 37] </ref>, strength reduction, finite differencing, and incrementalization [7, 34, 29, 27], caching intermediate results for program improvement [28], deforestation and fusion [42, 6], as well as compile-time garbage collection [21, 18, 35]. The analysis results also provide a kind of type information.
References-found: 44

