URL: ftp://ftp.cs.utexas.edu/pub/garbage/neely-thesis.ps.gz
Refering-URL: http://www.cs.utexas.edu/users/oops/papers.html
Root-URL: http://www.cs.utexas.edu
Title: by  
Author: Michael Shannon Neely 
Date: 1996  
Note: Copyright  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David A. Barrett and Bejamin G. Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> In Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation [15], </booktitle> <pages> pages 187-196. </pages>
Reference-contexts: Unfortunately, the programs were all simple textbook-style examples and were less than 100 lines of code; this may have skewed the results since the programs are not representative. Zorn, Grunwald, Barrett, and Henderson have published a number of studies using sound methodology with real application trace data <ref> [1] </ref>, [2], [8] [7]. These studies focus primarily on the speed of allocation, rather than fragmentation. They also studied locality issues and garbage collection. In one study, they made an effort to create synthetic traces based on real distribution data, that support useful allocation experiments.
Reference: [2] <author> David A. Barrett and Benjamin G. Zorn. </author> <title> Garbage collection using a dynamic threatening boundary. </title> <booktitle> In Proceedings of the 1995 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 301-314, </pages> <address> La Jolla, California, June 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Unfortunately, the programs were all simple textbook-style examples and were less than 100 lines of code; this may have skewed the results since the programs are not representative. Zorn, Grunwald, Barrett, and Henderson have published a number of studies using sound methodology with real application trace data [1], <ref> [2] </ref>, [8] [7]. These studies focus primarily on the speed of allocation, rather than fragmentation. They also studied locality issues and garbage collection. In one study, they made an effort to create synthetic traces based on real distribution data, that support useful allocation experiments.
Reference: [3] <author> A. P. Batson and R. E. Brundage. </author> <title> Segment sizes and lifetimes in ALGOL 60 programs. </title> <journal> Communications of the ACM, </journal> <volume> 20(1) </volume> <pages> 36-44, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: See [25] for a more general survey of allocation studies. Some early work that supports allocation studies was published by Totschek [21], Minker [14], and Batson, Ju, and Wood [4], and Batson and Brundage <ref> [3] </ref>. These studies provided empirical data about memory usage in specific computer systems. These results were used in many subsequent studies. Unfortunately, the data were often used to generate pseudo-random traces with the same approximate distributions, but the effects of smoothing and random ordering make this an unsound methodology.
Reference: [4] <author> A. P. Batson, S. M. Ju, and D. C. Wood. </author> <title> Measurements of segment size. </title> <journal> Communications of the ACM, </journal> <volume> 13(3) </volume> <pages> 155-159, </pages> <month> March </month> <year> 1970. </year>
Reference-contexts: See [25] for a more general survey of allocation studies. Some early work that supports allocation studies was published by Totschek [21], Minker [14], and Batson, Ju, and Wood <ref> [4] </ref>, and Batson and Brundage [3]. These studies provided empirical data about memory usage in specific computer systems. These results were used in many subsequent studies.
Reference: [5] <author> J. S. Fenton and D. W. Payne. </author> <title> Dynamic storage allocations of arbitrary sized segments. </title> <booktitle> In Proc. IFIPS, </booktitle> <pages> pages 344-348, </pages> <year> 1974. </year>
Reference-contexts: If enough blocks of the right sizes are available, this technique can reduce fragmentation by eliminating gaps that are due to splitting. One variation of multiple fit is half fit <ref> [5] </ref>. A half fit allocator searches for a block that is twice the requested size. The search may be for a block that is exactly twice as large, or for an approximate match.
Reference: [6] <author> M. R. Garey, R. L. Graham, and J. D. Ullman. </author> <title> Worst-case analysis of memory allocation algorithms. </title> <booktitle> In Fourth Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1972. </year> <month> 77 </month>
Reference-contexts: However, an analytical analysis reveals that this level of performance is not guaranteed. In the worst case, its memory usage is proportional to the product of the amount of allocated data, M , and the ratio between the largest and smallest object size, n (i.e., M n) <ref> [6, 18] </ref>. This appears not to happen in practice, or at least not commonly. In comparison, address-ordered first fit has similar fragmentation and much better worst case. 2.7 Real vs. Virtual Memory In order to understand space costs of memory allocation, we must distinguish between real and virtual memory systems.
Reference: [7] <author> Dirk Grunwald and Benjamin Zorn. </author> <title> CustoMalloc: Efficient synthesized memory allocators. </title> <journal> Software Practice and Experience, </journal> <volume> 23(8) </volume> <pages> 851-869, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Zorn, Grunwald, Barrett, and Henderson have published a number of studies using sound methodology with real application trace data [1], [2], [8] <ref> [7] </ref>. These studies focus primarily on the speed of allocation, rather than fragmentation. They also studied locality issues and garbage collection. In one study, they made an effort to create synthetic traces based on real distribution data, that support useful allocation experiments.
Reference: [8] <author> Dirk Grunwald, Benjamin Zorn, and Robert Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation [15], </booktitle> <pages> pages 177-186. </pages>
Reference-contexts: Unfortunately, the programs were all simple textbook-style examples and were less than 100 lines of code; this may have skewed the results since the programs are not representative. Zorn, Grunwald, Barrett, and Henderson have published a number of studies using sound methodology with real application trace data [1], [2], <ref> [8] </ref> [7]. These studies focus primarily on the speed of allocation, rather than fragmentation. They also studied locality issues and garbage collection. In one study, they made an effort to create synthetic traces based on real distribution data, that support useful allocation experiments. <p> This may result in allocated blocks being scattered throughout the heap, which has a negative effect on fragmentation [25], and is likely to destroy locality <ref> [8] </ref>, since freed blocks are likely to be isolated from other freed blocks. Variations of next fit include FIFO and address-ordered free lists, splitting thresholds, and wilderness preservation.
Reference: [9] <author> Mark Johnstone. </author> <title> Memory allocation and garbage collection. </title> <booktitle> Work in progress, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Rather, they will be used in experiments to study allocation behavior of applications <ref> [9] </ref>. The linear allocator starts allocating blocks at the beginning of the heap and continues allocating at the next available location without ever reusing freed blocks. This allocator is used only as a test of some inherent fragmentation-related behavior in a trace. <p> Avg lifetm is the average of the lifetime of each object, from the time it is allocated to the time it is freed. Finally, language is the programming language in which the application was written. For more details about the applications themselves, see <ref> [9] </ref>. 36 4.2 Allocator Features In order to study the effects of different policy choices in allocators, we devised a set of features that can be turned on or off by parameters to the allocators. Most of these features apply mostly to the sequential fits allocators. <p> We used several kinds of graphs to analyze the data|object count data graphs (ocd), page count data graphs (pcd), memory density graphs, and neighborhood fragmentation graphs. Note that not all of these are used in this thesis, but the descriptions are included here for completeness. See <ref> [9] </ref> for. more analysis with these tools. 46 OCD Graphs. The ocd graphs show the amount of memory used by each of the top n sizes. The Y axis is the amount of memory and the X axis is allocation time. <p> The are two kinds of data in the results section|those that reflect header overhead for a good implementations, and those that ignore header and footer overhead. Results for fragmentation with no overhead (from any of the above factors) will be shown in <ref> [9] </ref>. <p> Results of these experiments will be presented in <ref> [9] </ref>. 67 Chapter 7 Conclusions and Future Work We made a point of separating the policy and the mechanism associated with an allocator. We believe that for almost any known allocator policy, an efficient mechanism can be found to implement it [25].
Reference: [10] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, volume 1: Fundamental Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year> <note> First edition published in 1968. </note>
Reference-contexts: The only solution is to increase the memory of the system. In a system with virtual memory, however, memory can be swapped from secondary storage 3 This phenomenon is known as "splinters" or "sawdust'.' It was noted by Knuth <ref> [10] </ref>, but seems not to be a serious problem with either real or synthetic workloads. 12 to free up pages of memory for another process (this is typically achieved using an approximate least-recently-used paging algorithm).
Reference: [11] <author> David G. Korn and Kiem-Phong Vo. </author> <title> In search of a better malloc. </title> <booktitle> In Proc. USENIX Summer 1985, </booktitle> <pages> pages 489-506, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1985. </year> <institution> USENIX Association. </institution>
Reference-contexts: Then there is a choice in satisfying subsequent requests of whether to start searching in the old heap area or in the new heap area. An allocator using the wilderness preservation heuristic always starts the search in the old heap area <ref> [11] </ref>. The idea is to save the largest block of free memory until last in order to prevent fragmentation. This is similar to the strategy implemented by a best fit allocator|avoid breaking up large blocks if a smaller one is available.
Reference: [12] <author> B. W. Leverett and P. G. Hibbard. </author> <title> An adaptive system for dynamic storage allocation. </title> <journal> Software Practice and Experience, </journal> <volume> 12(6) </volume> <pages> 543-556, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Unfortunately, these warnings were not paid much attention; many studies continued to use synthetic traces. Their focus 19 was on speed of the allocator. They considered address-ordered first fit and used deferred coalescing to attempt to optimize the allocator for speed. Later, Leverett and Hibbard <ref> [12] </ref> used a set of 5 programs written in Algol-68 to experiment with deferred coalescing and different free list orders. Unfortunately, the programs were all simple textbook-style examples and were less than 100 lines of code; this may have skewed the results since the programs are not representative.
Reference: [13] <author> B. H. Margolin, R. P. Parmelee, and M. Schatzoff. </author> <title> Analysis of free-storage algorithms. </title> <journal> IBM Systems Journal, </journal> <volume> 10(4) </volume> <pages> 283-304, </pages> <year> 1971. </year>
Reference-contexts: Batson and Brundage provided similar information for a varied collection of 34 Algol-60 programs. One of the earliest studies to use a sound empirical methodology was Mar-golin's study of memory allocation in the control program of the IBM System/360 mainframe <ref> [13] </ref>. This study was concerned with memory used by the operating system, not by applications. They found that many of assumptions underlying other studies were false.
Reference: [14] <editor> J. Minker et al. </editor> <title> Analysis of data processing systems. </title> <type> Technical Report 69-99, </type> <institution> University of Maryland, College Park, Maryland, </institution> <year> 1969. </year>
Reference-contexts: See [25] for a more general survey of allocation studies. Some early work that supports allocation studies was published by Totschek [21], Minker <ref> [14] </ref>, and Batson, Ju, and Wood [4], and Batson and Brundage [3]. These studies provided empirical data about memory usage in specific computer systems. These results were used in many subsequent studies.
Reference: [15] <institution> Proceedings of the 1993 SIGPLAN Conference on Programming Language Design and Implementation, </institution> <address> Albuquerque, New Mexico, June 1993. </address> <publisher> ACM Press. </publisher>
Reference: [16] <author> P. W. Purdom, S. M. Stigler, and Tat-Ong Cheam. </author> <title> Statistical investigation of three storage allocation algorithms. </title> <journal> BIT, </journal> <volume> 11 </volume> <pages> 187-195, </pages> <year> 1971. </year> <month> 78 </month>
Reference: [17] <author> Brian Randell. </author> <title> A note on storage fragmentation and program segmentation. </title> <journal> Communications of the ACM, </journal> <volume> 12(7) </volume> <pages> 365-372, </pages> <month> July </month> <year> 1969. </year>
Reference-contexts: Traditionally, fragmentation has been classified as internal or external <ref> [17] </ref>. Internal fragmentation is wasted memory due to an allocator's returning a block that is larger than the requested size.
Reference: [18] <author> J. M. Robson. </author> <title> Worst case fragmentation of first fit and best fit storage allocation strategies. </title> <journal> Computer Journal, </journal> <volume> 20(3) </volume> <pages> 242-244, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: However, an analytical analysis reveals that this level of performance is not guaranteed. In the worst case, its memory usage is proportional to the product of the amount of allocated data, M , and the ratio between the largest and smallest object size, n (i.e., M n) <ref> [6, 18] </ref>. This appears not to happen in practice, or at least not commonly. In comparison, address-ordered first fit has similar fragmentation and much better worst case. 2.7 Real vs. Virtual Memory In order to understand space costs of memory allocation, we must distinguish between real and virtual memory systems. <p> It also has a good theoretical bound on fragmentation in the worst case|it is nearly optimal within a small constant <ref> [18] </ref>. The disadvantage of address-ordered first fit is that it may be intrinsically somewhat slower and less scalable than some other good algorithms, such as best fit [25].
Reference: [19] <author> Thomas Standish. </author> <title> Data Structure Techniques. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1980. </year>
Reference-contexts: Best fit is also a good candidate for a general purpose allocator and there exists an efficient implementation (see <ref> [19] </ref>, [25] ). A binary tree of linear lists can be used, where each node in the tree holds a linear (LIFO or FIFO) list of free blocks of a particular size. <p> This suggest that seeming minor implementation decisions are important, once a good policy has been found. Future works should address issues of per-object overheads, especially for small objects. The results for best fit are especially useful since there is a simple, fast implementation <ref> [19] </ref>.
Reference: [20] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proceedings of the Ninth Symposium on Operating Systems Principles, </booktitle> <pages> pages 30-32, </pages> <address> Bretton Woods, New Hampshire, </address> <month> October </month> <year> 1983. </year> <note> ACM Press. Published as Operating Systems Review 17(5), </note> <month> October </month> <year> 1983. </year>
Reference-contexts: If a block has a free neighbor in the heap, the two blocks will also be directly linked in the free list. A novel mechanism for implementing address-ordered first fit and an approximation of best fit was described by Stephensen <ref> [20] </ref>. A Cartesian tree of free blocks ordered primarily by address and secondarily by block size was used in the "fast fit" allocator. Depending on which search key is used, the allocator can search for blocks by size or by address.
Reference: [21] <author> R. A. Totschek. </author> <title> An empirical investigation into the behavior of the SDC timesharing system. </title> <type> Technical Report SP2191, </type> <institution> Systems Development Corporation, </institution> <year> 1965. </year>
Reference-contexts: See [25] for a more general survey of allocation studies. Some early work that supports allocation studies was published by Totschek <ref> [21] </ref>, Minker [14], and Batson, Ju, and Wood [4], and Batson and Brundage [3]. These studies provided empirical data about memory usage in specific computer systems. These results were used in many subsequent studies.
Reference: [22] <author> Kiem-Phong Vo. </author> <title> Vmalloc: A general and efficient memory allocator. </title> <journal> Software Practice and Experience, </journal> <note> 1995. To appear. </note>
Reference-contexts: Recently, Vo has studied a variety of allocators in the context of his vmalloc () allocator framework, which allows the use of multiple underlying allocators for data with different expected characteristics <ref> [22] </ref>. Like Zorn, Grunwald, et al., Vo does not clearly separate out issues of policy and mechanism. His focus is on allowing the programmer to control which allocator is used for which data, to adjust various tradeoffs.
Reference: [23] <author> Paul R. Wilson. </author> <title> Uniprocessor garbage collection techniques. </title> <editor> In Yves Bekkers and Jacques Cohen, editors, </editor> <booktitle> International Workshop on Memory Management, number 637 in Lecture Notes in Computer Science, </booktitle> <pages> pages 1-42, </pages> <address> St. Malo, France, </address> <month> September </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference: [24] <author> Paul R. Wilson. </author> <title> Garbage collection. </title> <journal> Computing Surveys, </journal> <note> 1995. Expanded version of [23]. Draft available via anonymous internet FTP from cs.utexas.edu as pub/garbage/bigsurv.ps. In revision, to appear. </note>
Reference-contexts: Programming paradigms such as object-oriented 1 C and C++ programs typically rely on the library routines malloc () and free () to allocate and free memory. 6 programming and functional programming rely heavily on dynamic memory alloca-tion. Some kinds of garbage collection also depend on good non-moving memory allocation <ref> [24] </ref>. For these reasons, it is important that the allocator function be efficient in its use of memory. The performance of an allocator depends crucially on the input stream of requests (i.e. the sequence of allocations and frees requested by the application).
Reference: [25] <author> Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles. </author> <title> Dynamic storage allocation: A survey and critical review. </title> <booktitle> In 1995 International Work 79 shop on Memory Management, </booktitle> <address> Kinross, Scotland, UK, 1995. </address> <publisher> Springer Verlag LNCS. </publisher>
Reference-contexts: Due to the unpredictability of allocation requests, it is difficult to create a useful mathematical model with which to study it. The simplifying assumptions used in most earlier work have been shown to be invalid and systematically biased (see <ref> [25] </ref> and section 6.4 of this thesis). Therefore we have used an experimental methodology to analyze the behavior of allocators under real workloads. <p> Most applications exhibit some regularities in their allocation, such as allocating many requests for blocks of the same size, or freeing many blocks at the end of a phase. These regularities can be exploited by the allocator to help in the choice of placement of blocks <ref> [25] </ref>. The allocator may also be able to split and coalesce free blocks in order to maintain the availability of large blocks of memory. <p> We have focused attention on policy in the belief that implementation is largely an independent issue. Past studies have dismissed some allocation policies simply because the best-known implementation is inefficient; they have generally overlooked straightforward algorithmic improvements <ref> [25] </ref>. 2 Thus we have attempted to avoid any biasing of our results due to implementation considerations that we consider relatively minor. <p> This means that most of the fragmentation related behavior from the original trace is missing from the randomized trace. Since the earliest studies of dynamic memory allocation, it was known that studies using random request streams were not methodologically sound, but these warnings were largely ignored <ref> [25] </ref>. Some studies using real workloads were published, but often the data and allocator details were not available to verify the results. <p> However, we believe this tradeoff is often overemphasized, and that allocators with excellent memory usage can also be quite fast <ref> [25] </ref>. Similarly, speed optimizations may affect policy. In this study, we have chosen to use fragmentation as the primary measure of allocator performance, but we have included effects of some important speed-oriented optimizations in our results. <p> The best general allocation policies are best fit and address-ordered first fit (and variations), both of which can be implemented efficiently, despite the fact that the straightforward implementations do not scale well with the number of free blocks <ref> [25] </ref>. Other issues associated with an allocator function are locality of reference and amount of overhead. We have included some important policy variations that can be expected to affect locality, and have experiments that simulate the memory costs of different implementations. <p> The design of an allocator can be broken into three parts|strategy, policy, and mechanism <ref> [25] </ref>. An allocator algorithm can be considered as the mechanism that implements a policy, which is motivated by a strategy for minimizing fragmentation. The strategy must take into account patterns of allocation in the request stream. <p> relative to the requested size, in order to balance internal and external fragmentation. 2.10 Allocation Patterns in Real Applications By examining the memory allocation behavior of real application programs, we were able to determine which kinds of patterns allocators are able to take advantage of to reduce the overall fragmentation <ref> [25] </ref>. These patterns are caused by loops, sequential access of arrays, I/O routines, and other common operations in structured programs. Nested phase behavior creates patterns of memory usage that makes efficient allocation easier. <p> Exceptions to these patterns occur in programs that are heavily dependent on input such as gcc and espresso. In these cases, spikes in allocation may appear as a result of varying input. For a more detailed description of common allocation behaviors of applica 18 tions, see <ref> [25] </ref>. 2.11 Related Work Since we have shown the traditional synthetic trace methodology to be unsound, the discussion of related work in this report will be limited to studies using real traces or contributing to a sounder methodology. See [25] for a more general survey of allocation studies. <p> detailed description of common allocation behaviors of applica 18 tions, see <ref> [25] </ref>. 2.11 Related Work Since we have shown the traditional synthetic trace methodology to be unsound, the discussion of related work in this report will be limited to studies using real traces or contributing to a sounder methodology. See [25] for a more general survey of allocation studies. Some early work that supports allocation studies was published by Totschek [21], Minker [14], and Batson, Ju, and Wood [4], and Batson and Brundage [3]. These studies provided empirical data about memory usage in specific computer systems. <p> and segregated fits * Buddy Systems include conventional binary, weighted, and Fibonacci buddies, and double buddies * Indexed Fits use a structured index to implement a desired fit policy 22 * Bitmapped Fits are a particular kind of indexed fit For a thorough survey of allocator policies and mechanisms, see <ref> [25] </ref>. 3.1 Sequential Fits We will begin the discussion of allocator types with an overview of the sequential fits mechanism. <p> We stress, however, that this is purely for convenience in varying parameters for experimental purposes. For the better placement policies, more efficient implementations are known, and should be used for any production allocator where speed is an issue <ref> [25] </ref>. There are a number of important details in the implementation of a sequential fits allocator that can strongly affect its policy and performance. In many previous studies, the descriptions of allocators lack sufficient detail to adequately differentiate one policy from another. <p> It also has a good theoretical bound on fragmentation in the worst case|it is nearly optimal within a small constant [18]. The disadvantage of address-ordered first fit is that it may be intrinsically somewhat slower and less scalable than some other good algorithms, such as best fit <ref> [25] </ref>. Other variations of first fit include FIFO and LIFO ordered free lists, with deferred coalescing, and splitting thresholds. 3.1.2 Best Fit The best fit allocator was described in the previous section. Variations of best fit include FIFO and address-ordered free lists, deferred coalescing, splitting thresholds, and pre-allocation. <p> Best fit is also a good candidate for a general purpose allocator and there exists an efficient implementation (see [19], <ref> [25] </ref> ). A binary tree of linear lists can be used, where each node in the tree holds a linear (LIFO or FIFO) list of free blocks of a particular size. <p> This may result in allocated blocks being scattered throughout the heap, which has a negative effect on fragmentation <ref> [25] </ref>, and is likely to destroy locality [8], since freed blocks are likely to be isolated from other freed blocks. Variations of next fit include FIFO and address-ordered free lists, splitting thresholds, and wilderness preservation. <p> As a result, large spaces are not preserved, and fragmentation is increased. This, along with the difficulty in its implementation, make next fit unattractive for a general purpose allocator. For a thorough discussion of the next fit allocator, see <ref> [25] </ref>. 28 3.2 Simple Segregated Storage A simple segregated storage allocator divides the heap into pages 2 such that blocks of the same size are always allocated in the same page. <p> Other properties of objects may give clues to the time they will be freed, or at least the fact that they are likely to be freed at 39 different times. (We refer to this as death time discrimination <ref> [25] </ref>.) If different--sized objects are likely to be of different types, and objects of different types are likely to die at different times, then grouping same-sized objects together may reduce fragmentation. <p> LIFO first fit suffers from more fragmentation, possibly due to splitting up large free blocks to satisfy smaller requests. LIFO best fit compares favorably with address-ordered first fit. The similarity between the two plots suggests that the strategy behind these two policies 54 may be related <ref> [25] </ref>. 6.2 Deferred Coalescing The mechanism we used for implementing deferred coalescing uses "quick lists" to store freed blocks of the same size. There is a quick list for each size from 1 to 64 bytes. <p> These results are misleading since simple segregated storage has much greater fragmentation with the original traces. We believe that shu*ing traces helps allocators that don't deal well with splitting and coalescing, or with strong phase behavior in programs <ref> [25] </ref>. In general, shu*ed traces cause more "holes" in the heap, but also smooth the distribution of objects in the heap so that the importance of coalescing is reduced. <p> A shu*ed trace is less likely to exhibit the pattern of allocation that hurts simple segregated storage|freeing many objects of one size and allocating many objects of another size <ref> [25] </ref>. When a page of memory is set aside for one size, it cannot 63 be reclaimed or reused for other sizes. If the objects of that size are mostly freed, then that memory is wasted. <p> Results of these experiments will be presented in [9]. 67 Chapter 7 Conclusions and Future Work We made a point of separating the policy and the mechanism associated with an allocator. We believe that for almost any known allocator policy, an efficient mechanism can be found to implement it <ref> [25] </ref>. Our measure of fragmentation is based on the amount of wasted memory at the point of maximum memory usage. This is a useful measure in both real and virtual memory environments, because the maximum memory usage is the "bottleneck" for program performance. <p> Buddy systems generally incur higher average internal fragmentation than the total fragmentation of the best allocators. Double buddies reduce internal fragmentation, as they were designed to, but the remaining fragmentation is still fairly high. Buddy systems may be viewed as a coarse approximation of|and poor substitute for|best fit allocation <ref> [25] </ref>. Since there are now efficient implementation techniques for best fit, including best fit with deferred coalescing to improve speed, 70 buddy systems do not seem attractive for general-purpose allocation.
Reference: [26] <author> Benjamin Zorn and Dirk Grunwald. </author> <title> Evaluating models of memory allocation. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(4) </volume> <pages> 107-131, </pages> <year> 1994. </year> <month> 80 </month>
Reference-contexts: In one study, they made an effort to create synthetic traces based on real distribution data, that support useful allocation experiments. They concluded that no model of allocation based on synthetic traces is as reliable as trace-driven simulation, in terms of error introduced by the model <ref> [26] </ref>. In general, the studies by Zorn, Grunwald et al. focus on bottom-line space and time performance for fairly complex allocators, including hybrids. The do not clearly separate out basic issues of policy from effects of implementation details, as we do here.
References-found: 26

