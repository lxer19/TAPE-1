URL: http://www.eecs.umich.edu/~jeffo/PS/icassp99.ps.gz
Refering-URL: http://www.eecs.umich.edu/~jeffo/Papers/index.html
Root-URL: http://www.cs.umich.edu
Title: CRAM ER-RAO LOWER BOUNDS FOR ATOMIC DECOMPOSITION  
Author: Jeffrey C. O'Neill Patrick Flandrin 
Affiliation: Electrical and Computer Engineering Boston University  Laboratoire de Physique Ecole Normale Superieure de Lyon  
Abstract: In a previous paper [1] we presented a method for atomic decomposition with chirped, Gabor functions based on maximum likelihood estimation. In this paper we present the Cramer-Rao lower bounds for estimating the seven chirp parameters, and the results of a simulation showing that our sub-optimal, but computationally tractable, estimators perform well in comparison to the bound at low signal-to-noise ratios. We also show that methods based on signal dictionaries will require much higher computations to per form well in low signal-to-noise ratios.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. C. O'Neill and P. Flandrin. </author> <title> Chirp hunting. </title> <booktitle> In Proc. of the IEEE Int. Symp. on Time-Frequency and Time-Scale Analysis, </booktitle> <year> 1998. </year>
Reference-contexts: 2d 2 n nt 2 2 (n t) 2 + j!(n t) : The parameters t, !, c, and d represent, respectively, the location in time, the location in frequency, the chirp rate, and the duration, and s () is defined such that ks t;!;c;d k 2 = n In <ref> [1] </ref> we presented a method for obtaining this decomposition based on the principles of maximum likelihood estimation which we will briefly review in the next section. We will then present the Cramer-Rao lower bounds along with estimates of the mean and variance of the estimators proposed in [1]. <p> = n In <ref> [1] </ref> we presented a method for obtaining this decomposition based on the principles of maximum likelihood estimation which we will briefly review in the next section. We will then present the Cramer-Rao lower bounds along with estimates of the mean and variance of the estimators proposed in [1]. Finally we will compare our methods with those that use signal dictionaries and investigate the issue of greed. 2. ESTIMATION OF CHIRP PARAMETERS In [1] we presented the maximum likelihood estimator (MLE) for the following signal model x (n) = A e s (n; t; !; c; d) + w <p> We will then present the Cramer-Rao lower bounds along with estimates of the mean and variance of the estimators proposed in <ref> [1] </ref>. Finally we will compare our methods with those that use signal dictionaries and investigate the issue of greed. 2. ESTIMATION OF CHIRP PARAMETERS In [1] we presented the maximum likelihood estimator (MLE) for the following signal model x (n) = A e s (n; t; !; c; d) + w (n) n = 1; : : : ; N; (1) where w (n) is complex, white, gaussian noise (CWGN) with a mean of zero and <p> However in the presence of noise, solving equation 2a is computationally expensive, so we will approximate it with the following sub-optimal estimator based on the results in <ref> [1] </ref>. 1. Estimate, globally, the chirp rate and duration with the am biguity function. 2. Using the current estimate of the chirp rate and the dura tion, estimate the location in time and frequency with the spectrogram. 3. <p> Estimate, globally, the chirp rate and duration with the am biguity function. 2. Using the current estimate of the chirp rate and the dura tion, estimate the location in time and frequency with the spectrogram. 3. Estimate, locally, the chirp rate and the duration using the methods proposed in <ref> [1] </ref>. 4. Go to step 2 until the parameters converge. 5. Find the nearest local maximum of the likelihood function by using a quasi-Newton procedure. The estimated chirp is then subtracted from the signal, and the procedure repeated to find the next element in the decomposition. 3. <p> Thus, we expect the performance to greatly deteriorate as N increases. With dictionary methods, it does not seem possible to have a computationally feasible algorithm that will reliably find the global maximum. In contrast, the performance of the method that we present here and in <ref> [1] </ref> is independent of N and requires a reason able O (N 2 log N ) computations. 6.
Reference: [2] <author> S. Kay. </author> <title> Fundamentals of statistical signal processing: estimation theory. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: As the the duration increases, it should become easier to estimate the chirp rate. The calculation of the Fisher information matrix is tedious, though straightforward <ref> [2] </ref> and the inverse is I () = A 2 6 6 6 6 4 N 0 0 0 0 0 0 0 0 2d 2 0 0 0 0 d 4 0 0 1 0 0 0 0 4d 2 4cd 2 4!d 2 d 2 + 4c 2 d
Reference: [3] <author> B.D. Rao. </author> <title> Signal processing with the sparseness constraint. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Acoust., Speech, and Signal Processing, </booktitle> <year> 1998. </year>
Reference-contexts: However, the sub-optimal estimator performs well in comparison to the unrealistic estimator, described above, and thus performs well in comparison to the MLE. 5. DICTIONARY METHODS Another approach to performing atomic decomposition is based on signal dictionaries <ref> [3] </ref>. To do this, one creates a signal dictionary matrix, D, where the columns are chirped, Gabor functions with different parameter values.
Reference: [4] <author> S. Mallat and Z. Zhang. </author> <title> Matching pursuits with time-frequency dictionaries. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 41(12):33973415, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: The Moore-Penrose inverse provides the minimum ` 2 norm solution, but this solution will not be sparse. Some methods which attempt to find sparse solutions to the above linear, inverse problem are Matching Pursuit <ref> [4, 5, 6] </ref>, Basis Pursuit [7], and FOCUSS [8]. If one discretizes the parameters of t, !, c, and d to O (N ) values, then the dictionary will contain O (N 4 ) elements and all of the above dictionary methods will be extremely expensive com-putationally. <p> To circumvent this problem, one can discretize the parameters at lower rates to decrease the size of the dictionary. For example in <ref> [4, 5] </ref>, the location in time was discretized at a rate of O ( N ), but since the minimum distance between dictionary elements in time will be O ( p N ), Matching Pursuit will likely find a local maximum, rather than the global maximum, as N increases.
Reference: [5] <author> A. Bultan. </author> <title> A four-parameter atomic decomposition of chirplets. </title> <note> to appear in IEEE Trans. on Signal Processing, </note> <year> 1998. </year>
Reference-contexts: The Moore-Penrose inverse provides the minimum ` 2 norm solution, but this solution will not be sparse. Some methods which attempt to find sparse solutions to the above linear, inverse problem are Matching Pursuit <ref> [4, 5, 6] </ref>, Basis Pursuit [7], and FOCUSS [8]. If one discretizes the parameters of t, !, c, and d to O (N ) values, then the dictionary will contain O (N 4 ) elements and all of the above dictionary methods will be extremely expensive com-putationally. <p> To circumvent this problem, one can discretize the parameters at lower rates to decrease the size of the dictionary. For example in <ref> [4, 5] </ref>, the location in time was discretized at a rate of O ( N ), but since the minimum distance between dictionary elements in time will be O ( p N ), Matching Pursuit will likely find a local maximum, rather than the global maximum, as N increases.
Reference: [6] <author> S. Qian and D. Chen. </author> <title> Signal representation using adaptive normalized Gaussian functions. </title> <booktitle> Signal Processing, </booktitle> <pages> pages 1 11, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The Moore-Penrose inverse provides the minimum ` 2 norm solution, but this solution will not be sparse. Some methods which attempt to find sparse solutions to the above linear, inverse problem are Matching Pursuit <ref> [4, 5, 6] </ref>, Basis Pursuit [7], and FOCUSS [8]. If one discretizes the parameters of t, !, c, and d to O (N ) values, then the dictionary will contain O (N 4 ) elements and all of the above dictionary methods will be extremely expensive com-putationally.
Reference: [7] <author> S. Chen and D. Donoho. </author> <title> Atomic decomposition by basis pursuit. </title> <note> to appear in Siam Journal on Scientific Computing, </note> <year> 1998. </year>
Reference-contexts: The Moore-Penrose inverse provides the minimum ` 2 norm solution, but this solution will not be sparse. Some methods which attempt to find sparse solutions to the above linear, inverse problem are Matching Pursuit [4, 5, 6], Basis Pursuit <ref> [7] </ref>, and FOCUSS [8]. If one discretizes the parameters of t, !, c, and d to O (N ) values, then the dictionary will contain O (N 4 ) elements and all of the above dictionary methods will be extremely expensive com-putationally.
Reference: [8] <author> I. Gorodnitsky and B.D. Rao. </author> <title> Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 45(3):600616, </volume> <month> March </month> <year> 1997. </year>
Reference-contexts: The Moore-Penrose inverse provides the minimum ` 2 norm solution, but this solution will not be sparse. Some methods which attempt to find sparse solutions to the above linear, inverse problem are Matching Pursuit [4, 5, 6], Basis Pursuit [7], and FOCUSS <ref> [8] </ref>. If one discretizes the parameters of t, !, c, and d to O (N ) values, then the dictionary will contain O (N 4 ) elements and all of the above dictionary methods will be extremely expensive com-putationally.
Reference: [9] <author> M.L. Brown. </author> <title> Optimal representation of transient biological signals using the adaptive Gabor transform. </title> <type> PhD thesis, </type> <institution> Univ. of Michigan, </institution> <year> 1994. </year>
Reference: [10] <author> A.P. Dempster, N.M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood estimation from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B (methodological), </journal> <volume> 39:138, </volume> <year> 1977. </year>
Reference-contexts: One can try to maximize this likelihood function with a quasi-Newton procedure, though this becomes increasing difficult as the number of parameters is increased. Initial results with the Expectation-Maximization (EM) algorithm <ref> [10, 11, 12] </ref> are promising, though the rate of convergence and the convergence to local, rather than global, maxima needs to be investigated. Software implementing the decomposition described here is available at http://www.eecs.umich.edu/~jeffo.
Reference: [11] <author> R.A. Redner and H.F. Walker. </author> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2):195 239, </volume> <month> April </month> <year> 1984. </year>
Reference-contexts: One can try to maximize this likelihood function with a quasi-Newton procedure, though this becomes increasing difficult as the number of parameters is increased. Initial results with the Expectation-Maximization (EM) algorithm <ref> [10, 11, 12] </ref> are promising, though the rate of convergence and the convergence to local, rather than global, maxima needs to be investigated. Software implementing the decomposition described here is available at http://www.eecs.umich.edu/~jeffo.
Reference: [12] <author> M. Feder and E. Weinstein. </author> <title> Parameter estimation of superimposed signals using the EM algorithm. </title> <journal> IEEE Trans. on Acoust., Speech, and Signal Processing, </journal> <volume> 36(4):477489, </volume> <month> April </month> <year> 1988. </year>
Reference-contexts: One can try to maximize this likelihood function with a quasi-Newton procedure, though this becomes increasing difficult as the number of parameters is increased. Initial results with the Expectation-Maximization (EM) algorithm <ref> [10, 11, 12] </ref> are promising, though the rate of convergence and the convergence to local, rather than global, maxima needs to be investigated. Software implementing the decomposition described here is available at http://www.eecs.umich.edu/~jeffo.
References-found: 12

