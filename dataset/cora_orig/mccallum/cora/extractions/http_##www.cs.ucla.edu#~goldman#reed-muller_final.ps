URL: http://www.cs.ucla.edu/~goldman/reed-muller_final.ps
Refering-URL: http://www.cs.ucla.edu/~goldman/papers.html
Root-URL: http://www.cs.ucla.edu
Email: mperkows@ee.pdx.edu tross@mbvlab.wpafb.af.mil  ning@lattice.com  
Title: APPLICATION OF ESOP MINIMIZATION IN MACHINE LEARNING AND KNOWLEDGE DISCOVERY  
Author: Marek A. Perkowski Tim Ross, Dave Gadd, Jeffrey A. Goldman Ning Song 
Address: WL/AARA-3 P.O. Box 751 2010 5th Street Bld 23 Portland, Oregon, 97207-0751 Wright-Patterson AFB, 45433-7001  1820 McCarthy Blvd. Milpitas CA 95035  
Affiliation: Dept. Electr. Engin. Wright-Laboratory Portland State University  Lattice Logic Corp.  
Abstract: This paper presents a new application of an Exclusive-Sum-Of-Products (ESOP) minimizer EXORCISM-MV-2: to Machine Learning, and particularly, in Pattern Theory. An analysis of various logic synthesis programs has been conducted at Wright Laboratory for machine learning applications. Creating a robust and efficient Boolean minimizer for machine learning that would minimize a decomposed function cardinality (DFC) measure of functions would help to solve practical problems in application areas that are of interest to the Pattern Theory Group especially those problems that require strongly unspecified multiple-valued-input functions with a large number of variables. For many functions, the complexity minimization of EXORCISM-MV-2 is better than that of Espresso. For small functions, they are worse than those of the Curtis-like Decomposer. However, EXORCISM is much faster, can run on problems with more variables, and significant DFC improvements have also been found. We analyze the cases when EXORCISM is worse than Espresso and propose new improvements for strongly unspecified functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y.S. Abu-Mostafa, </author> <title> "Complexity in Information Theory", </title> <publisher> Springer-Verlag, </publisher> <address> New York, 150pp, ISBN 0-387-96600-5, </address> <year> 1988. </year>
Reference-contexts: Pattern Theory [18] treats robust minimal complexity determination as the problem of finding a pattern. Pattern theory uses Decomposed Function Cardinality (DFC), proposed by Y. S. Abu-Mostafa as a general mea 1 Trademark of AbTech Corp. sure of complexity <ref> [1, p.128] </ref>. DFC is based on the car--dinality of a function. After all, a function is a set of ordered pairs and, as with any set, has a definite property in its number of elements or cardinality.
Reference: [2] <author> A.R. Barron, and R.L. Barron, </author> <title> "Statistical Learning Networks: A Unifying View", </title> <booktitle> Symposium on the Interface: Statistics and Computing Science, </booktitle> <year> 1988. </year>
Reference-contexts: Induction is often modeled as the process of extrapolating samples of a function. This extrapolation requires both the samples and the "inductive bias." The bias towards low complexity, as in Occam's Razor, is particularly important. There is a strong theoretical basis for Occam-based learning, see for example <ref> [2, 3] </ref>. Kolmogorov complexity was developed specifically for induction [14], however, it has been proven that its exact computation is not tractable.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth, </author> <title> "Occam's Razor", </title> <journal> Information Processing Letters, </journal> <month> Oct. </month> <year> 1987, </year> <pages> pp. 377-380. </pages>
Reference-contexts: Induction is often modeled as the process of extrapolating samples of a function. This extrapolation requires both the samples and the "inductive bias." The bias towards low complexity, as in Occam's Razor, is particularly important. There is a strong theoretical basis for Occam-based learning, see for example <ref> [2, 3] </ref>. Kolmogorov complexity was developed specifically for induction [14], however, it has been proven that its exact computation is not tractable.
Reference: [4] <author> M.A. Breen, T.D. Ross, M.J. Noviskey, </author> <title> and M.L. Axtell, "Pattern Theoretic Image Restoration," </title> <booktitle> Proc. SPIE'93 Nonlinear Image Processing, Intern. </booktitle> <institution> Soc. for Optical Engineering, </institution> <month> Jan-uary </month> <year> 1993. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso. <p> Another situation is that the value of some minterm is or becomes unknown a "don't care". This would be referred to as "unknown" in the ML community. ML techniques can be used to restore noisy images <ref> [4] </ref>. Of course, we do not know which pixels (or feature values) are "noisy" so we treat those that are most suspicious as don't cares. The more robust generalization performance of AFD, including dealing with noise and unknown data, is a reflection of its robust measure of complexity, DFC.
Reference: [5] <author> H.A. Curtis, </author> <title> "A New Approach to the Design of Switching Circuits", </title> <publisher> Princeton, </publisher> <address> N.J., </address> <publisher> Van Nostrand, </publisher> <year> 1962. </year>
Reference-contexts: Let us observe that both in the Curtis decomposition and the ESOP minimization approach of EXORCISM, we look for certain patterns in data: in Curtis decomposition these patterns are in columns of the map corresponding to the cofactors of the bond set of variables <ref> [5] </ref>. The patterns for ESOP minimization are based on certain rules of Boolean Algebra [24]. Let us also observe that in both cases we minimize a certain cost of the circuit. Traditionally in ESOP minimization one calculates the number of terms and the number of literals.
Reference: [6] <author> J. A. Goldman, </author> <title> "Machine Learning: A Comparative Study of Pattern Theory and C4.5," Wright Laboratory, </title> <type> USAF, Technical Report, </type> <address> WL-TR-94-1102, WL/AART, WPAFB, OH 45433-6543, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: This points out to the complementary nature of these two programs in the search of low DFC solutions. Lower combinational size complexity (DFC, gate count, Decision Tree node count, Decision Diagram size, etc) provides better generalization <ref> [15, 6, 19, 22] </ref>. There is greater than 0.9 correlation between complexity reduction and generalization performance for both SMOG (RODD size) and FLASH (DFC) [22].
Reference: [7] <author> J. A. Goldman and M. L. Axtell, </author> <title> "On Using Logic Synthesis for Supervised Classification Learning," </title> <booktitle> 7th IEEE International Conference on Tools with Artificial Intelligence, IEEE, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso. <p> This system is a Testbed for machine learning based on the logic synthesis approach. The group also compares FLASH to other logic optimizers and machine learning programs, (such as Espresso and C4.5, respectively) from the point of view of the robustness of learning <ref> [7, 8] </ref>. Simplifying, the main difference of the logic approach and the previous approaches to machine learning is that in these previous methods, the recognizing network had some assumed structure which was "tuned" by the learning process (for instance, by decreasing and increasing the numerical values of some coefficients). <p> None of the learning was incremental. All of the runs were independent. For each function, the average number of errors for the entire run was recorded in the table. Espresso generalizes as well as C4.5 amain stream ML method <ref> [7] </ref>. It, however, does have a weakness when the function to be learned is most naturally represented with EXORs. This points out to the complementary nature of these two programs in the search of low DFC solutions. <p> RANDOM MINORITY ELEMENTS. There are 5 functions generated with a fixed number of minority elements placed at random. The seed for all was 1: rnd m1, rnd m5, rnd m10, rnd m25, rnd m50. BOOLEAN EXPRESSIONS. These are functions intended to represent database concepts for knowledge discovery <ref> [7] </ref>. kdd1: (x1 x3) + x2'; kdd2: (x1 x2' x3)(x4 + x6'); kdd3: NOT (x1 OR x2) + (x1' x4 x6); kdd4: x4'; 2 More "machine learning benchmarks" are available from U.C.
Reference: [8] <author> J.A. Goldman, T.D. Ross, and D.A. Gadd, </author> <title> "Pattern Theoretic Learning", </title> <booktitle> AAAI Spring Symposium Series on Systematic Methods of Scientific Discovery, AAAI, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso. <p> This system is a Testbed for machine learning based on the logic synthesis approach. The group also compares FLASH to other logic optimizers and machine learning programs, (such as Espresso and C4.5, respectively) from the point of view of the robustness of learning <ref> [7, 8] </ref>. Simplifying, the main difference of the logic approach and the previous approaches to machine learning is that in these previous methods, the recognizing network had some assumed structure which was "tuned" by the learning process (for instance, by decreasing and increasing the numerical values of some coefficients). <p> Essentially, this is a 2-ply look ahead search where we only explore 40 grand children. It chooses a particular partition over another based on the function's DFC. Exorcism finds an ESOP, Espresso finds a SOP, and C4.5 finds a low complexity decision tree. Minimizing complexity provides good generalization performance <ref> [8, 15, 18] </ref>. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see [18, 8, 19]. <p> Minimizing complexity provides good generalization performance [8, 15, 18]. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see <ref> [18, 8, 19] </ref>. The DFC of over 800 non-randomly generated functions was measured, including many classes of functions (numeric, symbolic, chaotic, string-based, graph-based, images and files). Roughly 98 percent of the non-randomly generated functions had low DFC (versus less than 1 percent for random functions).
Reference: [9] <author> S. J. Hong, "R-MINI: </author> <title> A Heuristic Algorithm for Generating Minimal Rules from Examples", </title> <booktitle> Pacific Rim International Conference on Artificial Intelligence, </booktitle> <address> PRICAI, </address> <year> 1994. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso.
Reference: [10] <author> R. Kohavi, and B. Frasca, </author> <title> "Useful Feature Subsets and Rough Set Reducts", </title> <booktitle> Third International Workshop on Rough Sets and Soft Computing, </booktitle> <year> 1994. </year>
Reference-contexts: The first research results that appreciate this synergy and try to link the two worlds of the "machine learning community" and the "design automation community" already start to appear: new decision-diagram approaches were presented in 1994 by Ron Kohavi <ref> [10, 11] </ref>, and Arlindo Oliveira [15]. It is our hope that the participants of this Workshop will also partake in this challenge, develop new theories and software, and will test them on the machine learning benchmarks.
Reference: [11] <author> R. Kohavi, </author> <title> "Bottom-up Induction of Oblivious Read-Once Decision Diagrams," </title> <booktitle> In European Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: The first research results that appreciate this synergy and try to link the two worlds of the "machine learning community" and the "design automation community" already start to appear: new decision-diagram approaches were presented in 1994 by Ron Kohavi <ref> [10, 11] </ref>, and Arlindo Oliveira [15]. It is our hope that the participants of this Workshop will also partake in this challenge, develop new theories and software, and will test them on the machine learning benchmarks.
Reference: [12] <editor> J. Koza, </editor> <booktitle> "Genetic Programming," </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 0 2 4 12 0.11 0.8 TABLE I Espresso on Machine Learning Benchmarks. kdd5: (x1 x2 x4')+(x3 x5' x7 x8)+(x1 x2 x5 x6 + (x3 x4) + (x5 x6) + (x7 x8); kdd8: (x1 x2') XOR (x1 ! x4) XOR ( NOT (x7 x8) )(x2 + x3). multiplexer, used in <ref> [12] </ref>, this is a 2-address bit, 4-data bit multiplexer with two vacuous variables (x0 and x1) to make 8 inputs. STRING FUNCTIONS. Palindrome acceptor, pal, palindrome output, pal output, randomly generated 128 bits then mirror imaged them to create the outputs of an 8 variable function.
Reference: [13] <author> T. Kozlowski, E.L. Dagless, J.M. Saul, </author> <title> "An Enhanced Algorithm for the Minimization of Exclusive-Or Sum-Of-Products for Incompletely Specified Functions", private information, </title> <year> 1995. </year>
Reference-contexts: V. ESOP Minimization for Machine Learning In the past, EXORCISM-mv-2 was tested very successfully on industrial circuits that have up to 80% of the values as don't cares [24]. Comparisons with EXMIN [23] and MINT <ref> [13] </ref> demonstrates that EXORCISM either finds the best solution, or finds one that is nearly the best one of the three. EXMIN does not allow for don't cares so it is of little use to machine learning, and MINT is slower than EXORCISM.
Reference: [14] <author> M. Li and P. M. B. Vitanyi, </author> <title> "Inductive Reasoning and Kol-mogorov Complexity", </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 44, </volume> <pages> pp. 343-384, </pages> <year> 1992. </year>
Reference-contexts: This extrapolation requires both the samples and the "inductive bias." The bias towards low complexity, as in Occam's Razor, is particularly important. There is a strong theoretical basis for Occam-based learning, see for example [2, 3]. Kolmogorov complexity was developed specifically for induction <ref> [14] </ref>, however, it has been proven that its exact computation is not tractable. There have been some tractable measures of complexity used in actual implementations of Occam-based learning, such as the Abductory Inference Mechanism which uses polynomial networks and C4.5 [17] which uses decision trees.
Reference: [15] <author> A.L. de Oliveira, </author> <title> "Inductive Learning by Selection of Minimal Complexity Representations," </title> <type> Ph.D. Thesis, </type> <institution> University of California at Berkeley, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso. <p> Essentially, this is a 2-ply look ahead search where we only explore 40 grand children. It chooses a particular partition over another based on the function's DFC. Exorcism finds an ESOP, Espresso finds a SOP, and C4.5 finds a low complexity decision tree. Minimizing complexity provides good generalization performance <ref> [8, 15, 18] </ref>. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see [18, 8, 19]. <p> This shows DFC's ability to reflect patterns within each domain, despite their different nature. In learning experiments, AFD has been compared to back-propagation trained Neural Networks (NN), AIM, C4.5, SMOG <ref> [15] </ref> and standard logic minimization tools. These comparisons used a broad range of function types (from the 800 mentioned above). For each of the test functions, AFD performed near the best of any method, while the other methods generalized well on some functions but not on others. <p> This points out to the complementary nature of these two programs in the search of low DFC solutions. Lower combinational size complexity (DFC, gate count, Decision Tree node count, Decision Diagram size, etc) provides better generalization <ref> [15, 6, 19, 22] </ref>. There is greater than 0.9 correlation between complexity reduction and generalization performance for both SMOG (RODD size) and FLASH (DFC) [22]. <p> The first research results that appreciate this synergy and try to link the two worlds of the "machine learning community" and the "design automation community" already start to appear: new decision-diagram approaches were presented in 1994 by Ron Kohavi [10, 11], and Arlindo Oliveira <ref> [15] </ref>. It is our hope that the participants of this Workshop will also partake in this challenge, develop new theories and software, and will test them on the machine learning benchmarks.
Reference: [16] <author> M. A. Perkowski, T. Ross, D. Gadd, J. A. Goldman, and N. Song, </author> <title> "Application of ESOP Minimization in Machine Learning and Knowledge Discovery," </title> <type> Report, </type> <institution> Department of Electrical Engineering, Portland State University, </institution> <year> 1995. </year>
Reference-contexts: The data support the conclusion that EXORCISM compares favorably to Espresso in general and fills in some of its weaknesses. While EXORCISM in its present form does not minimize partial functions well, we outlined some proposed changes and have further developed the algorithm in <ref> [16] </ref>. We can then conclude that EXORCISM has potential as a viable ML method that would especially complement existing ML techniques. Our goal is the creation of a practical "machine learning" algorithm, which means at the minimum, 30 binary input variables but more likely, about 100 multiple-valued variables.
Reference: [17] <author> J. R. Quinlan, "C4.5: </author> <title> Programs for Machine Learning", </title> <publisher> Mor-gan Kaufmann, </publisher> <address> 1993, Palo Alto, Ca. </address>
Reference-contexts: Kolmogorov complexity was developed specifically for induction [14], however, it has been proven that its exact computation is not tractable. There have been some tractable measures of complexity used in actual implementations of Occam-based learning, such as the Abductory Inference Mechanism which uses polynomial networks and C4.5 <ref> [17] </ref> which uses decision trees.
Reference: [18] <author> T. D. Ross, M.J. Noviskey, T.N. Taylor, D.A. Gadd, </author> <title> "Pattern Theory: An Engineering Paradigm for Algorithm Design," </title> <type> Final Technical Report WL-TR-91-1060, </type> <institution> Wright Laboratories, USAF, </institution> <address> WL/AART/WPAFB, OH 45433-6543, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Interestingly, in contrast to most of the well-known approaches, the approach of the PTG is based on logic synthesis methods: the so-called Curtis Decomposition of Boolean functions is applied here as the main component <ref> [18] </ref>. Many decomposition ideas have been implemented in the programming system FLASH (the Function Learning and Synthesis Hotbed) developed by this group [20]. This system is a Testbed for machine learning based on the logic synthesis approach. <p> The challenge is to develop robust and tractable measures of complexity. Pattern Theory <ref> [18] </ref> treats robust minimal complexity determination as the problem of finding a pattern. Pattern theory uses Decomposed Function Cardinality (DFC), proposed by Y. S. Abu-Mostafa as a general mea 1 Trademark of AbTech Corp. sure of complexity [1, p.128]. DFC is based on the car--dinality of a function. <p> Its robustness is supported by its relationship to more conventional measures of complexity, including circuit size complexity, time complexity, decision tree or diagram size and Kolmogorov complexity. If a problem has low complexity by any of these measures then it will also have a low DFC <ref> [18, Chapter 4] </ref>. The PTG work has concentrated on functions with binary inputs, but the concept is easily extended to continuous and multiple-valued functions [21]. The decompositions are evaluated based on the sum of the function cardinality of the decomposed partial blocks. <p> Essentially, this is a 2-ply look ahead search where we only explore 40 grand children. It chooses a particular partition over another based on the function's DFC. Exorcism finds an ESOP, Espresso finds a SOP, and C4.5 finds a low complexity decision tree. Minimizing complexity provides good generalization performance <ref> [8, 15, 18] </ref>. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see [18, 8, 19]. <p> Minimizing complexity provides good generalization performance [8, 15, 18]. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see <ref> [18, 8, 19] </ref>. The DFC of over 800 non-randomly generated functions was measured, including many classes of functions (numeric, symbolic, chaotic, string-based, graph-based, images and files). Roughly 98 percent of the non-randomly generated functions had low DFC (versus less than 1 percent for random functions).
Reference: [19] <author> T.D. Ross, </author> <title> M.L. Axtell, M.J. Noviskey, "Logic Minimization as a Robust Pattern Finder", </title> <booktitle> International Workshop on Logic Synthesis, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Minimizing complexity provides good generalization performance [8, 15, 18]. III. Summary of DFC Measurements and Applications A number of experiments have been conducted to assess the generality of DFC across different problem domains (see <ref> [18, 8, 19] </ref>. The DFC of over 800 non-randomly generated functions was measured, including many classes of functions (numeric, symbolic, chaotic, string-based, graph-based, images and files). Roughly 98 percent of the non-randomly generated functions had low DFC (versus less than 1 percent for random functions). <p> This points out to the complementary nature of these two programs in the search of low DFC solutions. Lower combinational size complexity (DFC, gate count, Decision Tree node count, Decision Diagram size, etc) provides better generalization <ref> [15, 6, 19, 22] </ref>. There is greater than 0.9 correlation between complexity reduction and generalization performance for both SMOG (RODD size) and FLASH (DFC) [22].
Reference: [20] <author> T.D. Ross, M.J. Noviskey, </author> <title> M.L. Axtell, D.A. Gadd, "Flash user's guide," </title> <type> Technical report, </type> <institution> Wright Laboratory, USAF, </institution> <address> WL/AART, WPAFB, OH 45433-6543, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Many decomposition ideas have been implemented in the programming system FLASH (the Function Learning and Synthesis Hotbed) developed by this group <ref> [20] </ref>. This system is a Testbed for machine learning based on the logic synthesis approach. The group also compares FLASH to other logic optimizers and machine learning programs, (such as Espresso and C4.5, respectively) from the point of view of the robustness of learning [7, 8].
Reference: [21] <author> T.D. Ross, J.A. Goldman, D.A. Gadd, M.J. Noviskey, </author> <title> and M.L. Axtell, "On the Decomposition of Real-Valued Functions", </title> <booktitle> "Third International Workshop on Post-Binary ULSI Systems in affiliation with the Twenty-Fourth International Symposium on Multiple-Valued Logic", </booktitle> <year> 1994. </year>
Reference-contexts: If a problem has low complexity by any of these measures then it will also have a low DFC [18, Chapter 4]. The PTG work has concentrated on functions with binary inputs, but the concept is easily extended to continuous and multiple-valued functions <ref> [21] </ref>. The decompositions are evaluated based on the sum of the function cardinality of the decomposed partial blocks. Of course, the real DFC of f is less than or equal to this sum in any particular (usually approximate) realization.
Reference: [22] <author> T.D. Ross, </author> <title> "Variable Partition Search for Function Decomposition," </title> <type> Technical report, </type> <institution> Wright Laboratory, USAF, </institution> <address> WL/AARA-3, WPAFB, OH 45433-6543, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In the last few years, however, one can observe some increased trend to apply these methods also in image processing, machine learning, knowledge discovery, knowledge acquisition, data-base optimization, AI, image coding, automatic theorem proving and verification of software and hardware <ref> [4, 15, 22, 7, 8, 9] </ref>. This paper is related to the use of an ESOP minimizer in Machine Learning (ML) and Knowledge Discovery in Databases (KDD) applications. We will examine the performance of the EXORCISM circuit minimizer on small binary functions comparing the results with Espresso. <p> This points out to the complementary nature of these two programs in the search of low DFC solutions. Lower combinational size complexity (DFC, gate count, Decision Tree node count, Decision Diagram size, etc) provides better generalization <ref> [15, 6, 19, 22] </ref>. There is greater than 0.9 correlation between complexity reduction and generalization performance for both SMOG (RODD size) and FLASH (DFC) [22]. <p> Lower combinational size complexity (DFC, gate count, Decision Tree node count, Decision Diagram size, etc) provides better generalization [15, 6, 19, 22]. There is greater than 0.9 correlation between complexity reduction and generalization performance for both SMOG (RODD size) and FLASH (DFC) <ref> [22] </ref>. In the two comprehensive tables, we provide some results for functions that do not have a low complexity SOP representation but do have a low complexity ESOP for total functions, such as parity and palindrome. VII.
Reference: [23] <author> T. Sasao, "Exmin2: </author> <title> A Simplification Algorithm for Exclusive-Or Sum-of-Products Expressions for Multiple-Valued-Input Two-Valued-Output Functions", </title> <journal> IEEE Trans. on CAD., </journal> <volume> Vol. 12, No. 5, </volume> <pages> pp. 621-632, </pages> <year> 1993. </year>
Reference-contexts: V. ESOP Minimization for Machine Learning In the past, EXORCISM-mv-2 was tested very successfully on industrial circuits that have up to 80% of the values as don't cares [24]. Comparisons with EXMIN <ref> [23] </ref> and MINT [13] demonstrates that EXORCISM either finds the best solution, or finds one that is nearly the best one of the three. EXMIN does not allow for don't cares so it is of little use to machine learning, and MINT is slower than EXORCISM.
Reference: [24] <author> N. Song, and M.A. Perkowski, </author> <title> "Minimization of Exclusive Sum of Products Expressions for Multi-Output Multiple-Valued Input Switching Functions," </title> <note> accepted to IEEE Trans. on CAD. </note>
Reference-contexts: The patterns for ESOP minimization are based on certain rules of Boolean Algebra <ref> [24] </ref>. Let us also observe that in both cases we minimize a certain cost of the circuit. Traditionally in ESOP minimization one calculates the number of terms and the number of literals. <p> ML problems involve missing values in inputs (missing fields) and conflicting data for discrete as well as continuous fields. V. ESOP Minimization for Machine Learning In the past, EXORCISM-mv-2 was tested very successfully on industrial circuits that have up to 80% of the values as don't cares <ref> [24] </ref>. Comparisons with EXMIN [23] and MINT [13] demonstrates that EXORCISM either finds the best solution, or finds one that is nearly the best one of the three. EXMIN does not allow for don't cares so it is of little use to machine learning, and MINT is slower than EXORCISM.
References-found: 24

