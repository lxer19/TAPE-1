URL: http://vibes.cs.uiuc.edu/Publications/Papers/PerfEval98.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Email: email: esmirni@cs.wm.edu  
Phone: tel: (757) 221-3580  
Title: Title: "Workload Characterization of Input/Output Intensive Parallel Applications"  
Address: P.O. Box 8795 Williamsburg, VA 23187-8795 USA  
Affiliation: College of William and Mary Department of Computer Science  
Note: Corresponding author: Evgenia Smirni  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bennett, R., Bryant, K., Sussman, A., Das, R., and Saltz, J. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference (October 1994), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 10-20. </pages>
Reference-contexts: Furthermore, many experimental I/O libraries focus on specific classes of I/O patterns and provide flexible data distribution and data management policies. For example, Galley [18], Panda [26], PASSION [2] and Jovian <ref> [1] </ref> support external multi-dimensional arrays, can restructure disk requests, and provide collective I/O operations that effectively utilize the available disk bandwidth. The desire for intuitive APIs with high expressive power has led to a host of domain specific I/O libraries that support specific problem domains and access pattern classes.
Reference: [2] <author> Bordawekar, R., Thakur, R., and Choudhary, A. </author> <title> Efficient compilation of out-of-core data parallel programs. </title> <type> Tech. Rep. SCCS-622, </type> <institution> NPAC, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Furthermore, many experimental I/O libraries focus on specific classes of I/O patterns and provide flexible data distribution and data management policies. For example, Galley [18], Panda [26], PASSION <ref> [2] </ref> and Jovian [1] support external multi-dimensional arrays, can restructure disk requests, and provide collective I/O operations that effectively utilize the available disk bandwidth.
Reference: [3] <author> C., W., H., P., and V., M. </author> <title> Parallel computation of electron-molecule collisions. </title> <booktitle> IEEE Computational Science and Engineering (1995), </booktitle> <pages> 34-42. </pages>
Reference-contexts: During the final or post processing phase, after the end of the integration, final results are written to disk. ESCAT The ESCAT code is a parallel implementation of the Schwinger Multichannel method written in C, FORTRAN, and assembly language <ref> [3] </ref>. The Schwinger Multichannel (SMC) method is an adaptation of Schwinger's variational principle for the scattering amplitude that makes it suitable for calculating low-energy electron-molecule collisions. The scattering probabilities are obtained by solving linear systems whose terms must be evaluated by numerical quadrature.
Reference: [4] <author> Corbett, P. F., Baylor, S. J., and Feitelson, D. G. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems (1993), </booktitle> <pages> pp. 1-16. </pages>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [5] <author> Corbett, P. F., Feitelson, D. G., Prost, J.-P., and Baylor, S. J. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93 (1993), </booktitle> <pages> pp. 472-481. </pages>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [6] <author> Corbett, P. F., Prost, J.-P., Demetriou, C., Gibson, G., Riedel, E., Zelenka, J., Chen, Y., Felten, E., Li, K., Hartman, J., Peterson, L., Bershad, B., Wolman, A., and Aydt, R. </author> <title> Proposal for a common parallel file system programming interface version 1.0, </title> <month> September </month> <year> 1996. </year>
Reference-contexts: Therefore, the design and implementation of a standard parallel I/O API that is expressive, compact, intuitively appealing, and at the same time offers high performance <ref> [6] </ref> is one of the goals of the SIO effort.
Reference: [7] <author> Crandall, P., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> Input/Output characterization of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing 1995 (1996), </booktitle> <pages> pp. CDROM. 17 </pages>
Reference-contexts: In contrast to the results on vector systems, recent analyses on the Intel Paragon XP/S <ref> [7] </ref>, the Intel iPSC/860 [13], and the CM-5 [23], showed greater irregularity in I/O access patterns, with the majority of file request being small but with the greatest data volume transferred by a few large requests.
Reference: [8] <author> Foster, I., and Nieplocha, J. </author> <title> ChemIO: High-performance I/O for computational chemistry applications. </title> <note> WWW http://www.mcs.anl.gov/chemio/, February 1996. </note>
Reference-contexts: The desire for intuitive APIs with high expressive power has led to a host of domain specific I/O libraries that support specific problem domains and access pattern classes. Such libraries have emerged for computational chem 2 istry <ref> [8] </ref> and for out-of-core linear algebra computations [29]. Early experience with these APIs has shown major I/O performance improvements | with high-level descriptions of I/O request patterns, file system policies can more intelligently prefetch and cache I/O data.
Reference: [9] <author> Henderson, R. D., and Karniadakis, G. E. </author> <title> Unstructured spectral element methods or simulation of turbulent flows. </title> <journal> Journal of Computational Physics 122(2) (1995), </journal> <pages> 191-217. </pages>
Reference-contexts: Due to the iterative nature of the numerical algorithms used by QCRD, there is intensive I/O activity on disks. I/O demands on disk have a repetitive, cyclic pattern. PRISM The PRISM code is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations written in C <ref> [9] </ref> and models high speed turbulent flow that is periodic in one direction. Slides of the periodic domain are proportionally distributed among processors and a combination of spectral elements and Fourier modes are used to investigate the dynamics and transport properties of turbulent flow.
Reference: [10] <institution> High Performance Computational Chemistry Group, Pacific Northwest National Laboratory. </institution> <month> NWChem, </month> <title> A Computational Chemistry Package for Parallel Computers, Version 1.1. </title> <address> Richland, Washington, 99352, USA, </address> <year> 1995. </year>
Reference-contexts: In this study, we selected a representative subset from the SIO application suite that comprises the access pattern attributes commonly found in parallel codes. 1 A high level description of the selected applications follows. MESSKIT and NWChem The MESSKIT and the NWChem codes <ref> [10] </ref> are two distinct Fortran implementations of the Hartree-Fock self consistent field (SCF) method that calculates the electron density around a molecule by considering each electron in the molecule in the collective field of the others.
Reference: [11] <author> Intel Corporation. </author> <title> Paragon System User's Guide. </title> <publisher> Intel SSD, </publisher> <address> Beaverton, OR, </address> <year> 1995. </year>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>. <p> In this paper we present the I/O behavior of five representative scientific applications from the SIO code suite. Using the Pablo performance analysis tool and its I/O extensions, we captured and analyzed the access patterns of the SIO applications and their interaction with the Intel's parallel file system (PFS) <ref> [11] </ref>. This analysis proved instrumental in guiding the development of new file system application programming interfaces (APIs) that drive adaptive file system policies. We demonstrate why API controls for efficient data distribution, collective I/O, and data caching are necessary to maximize I/O throughput.
Reference: [12] <author> King, S. M. Installing, </author> <title> Managing, and Using the IBM AIX Parallel I/O File System. </title> <institution> Information Development Department, IBM Kingston, </institution> <address> New York, </address> <year> 1994. </year>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [13] <author> Kotz, D., and Nieuwejaar, N. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994), </booktitle> <pages> pp. 640-649. </pages>
Reference-contexts: In contrast to the results on vector systems, recent analyses on the Intel Paragon XP/S [7], the Intel iPSC/860 <ref> [13] </ref>, and the CM-5 [23], showed greater irregularity in I/O access patterns, with the majority of file request being small but with the greatest data volume transferred by a few large requests.
Reference: [14] <author> LoVerso, S. J., Isman, M., Nanopoulos, A., Nesheim, W., Milne, E. D., and Wheeler, R. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference (1993), </booktitle> <pages> pp. 291-305. </pages>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [15] <author> Madhyastha, T., and Reed, D. A. </author> <title> Intelligent, adaptive file system policy selection. </title> <booktitle> In Proceedings of Frontiers'96 (1996). </booktitle>
Reference-contexts: Instead, one needs a file system API via which users can "inform" the file system of expected access patterns. Using such hints or an automatic access pattern classification scheme <ref> [15] </ref>, an adaptive file system could then choose those file policies and policy parameters best matched to the access pattern. For example, via user controls and hints one might advise the file system that the file access pattern is read only, write only, mixed read/write sequential, or strided.
Reference: [16] <author> Miller, E. L., and Katz, R. H. </author> <title> Input/Output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91 (November 1991), </booktitle> <pages> pp. 567-576. </pages>
Reference-contexts: Finally, x7 summarizes our observations. 2 Related Work The first I/O characterization efforts of scientific applications on vector supercomputers concluded that I/O behavior is regular, recurrent, and predictable <ref> [16, 21] </ref>, characteristics that were attributed to the iterative nature of such applications.
Reference: [17] <author> Moyer, S. A., and Sunderam, V. S. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference (1994), </booktitle> <pages> pp. 71-78. </pages>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [18] <author> Nieuwejaar, N., and Kotz, D. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing (May 1996). To appear. </booktitle>
Reference-contexts: Furthermore, many experimental I/O libraries focus on specific classes of I/O patterns and provide flexible data distribution and data management policies. For example, Galley <ref> [18] </ref>, Panda [26], PASSION [2] and Jovian [1] support external multi-dimensional arrays, can restructure disk requests, and provide collective I/O operations that effectively utilize the available disk bandwidth.
Reference: [19] <author> Nieuwejaar, N., Kotz, D., Purakayastha, A., Ellis, C. S., and Best, M. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Tech. Rep. </type> <institution> PCS-TR95-263, Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: Subsequent studies <ref> [19, 27] </ref> indicated that users attempt to adjust the application I/O access patterns to match the characteristics of the underlying parallel file system to maximize performance.
Reference: [20] <author> Nitzberg, B. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Tech. Rep. </type> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: To improve performance, advances of I/O hardware and file system parallelism are of principal importance. In the last few years, a wide variety of parallel I/O systems have been proposed and built <ref> [11, 12, 4, 5, 20, 14, 17] </ref>.
Reference: [21] <author> Pasquale, B. K., and Polyzos, G. C. </author> <title> Dynamic I/O characterization of I/O intensive scientific applications. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994), </booktitle> <pages> pp. 660-669. </pages>
Reference-contexts: Finally, x7 summarizes our observations. 2 Related Work The first I/O characterization efforts of scientific applications on vector supercomputers concluded that I/O behavior is regular, recurrent, and predictable <ref> [16, 21] </ref>, characteristics that were attributed to the iterative nature of such applications.
Reference: [22] <author> Poole, J. T. </author> <title> Scalable I/O Initiative. </title> <institution> California Institute of Technology, </institution> <note> Available at http://www.ccsf.caltech.edu/SIO/, 1996. </note>
Reference-contexts: Understanding the interactions between application I/O request patterns and the hardware and software of parallel I/O systems is necessary for the design of more effective I/O management policies. The primary objectives of the Scalable I/O initiative (SIO) <ref> [22] </ref> are (a) to assemble a suite of I/O intensive, national challenge applications, (b) to collect detailed performance data on application characteristics and access patterns, and (c) use this information to design and evaluate parallel file system management policies and parallel file system application programming interfaces (APIs).
Reference: [23] <author> Purakayastha, A., Ellis, C. S., Kotz, D., Nieuwejaar, N., and Best, M. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium (April 1995), </booktitle> <pages> pp. 165-172. 18 </pages>
Reference-contexts: In contrast to the results on vector systems, recent analyses on the Intel Paragon XP/S [7], the Intel iPSC/860 [13], and the CM-5 <ref> [23] </ref>, showed greater irregularity in I/O access patterns, with the majority of file request being small but with the greatest data volume transferred by a few large requests.
Reference: [24] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable performance analysis: The Pablo performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: To capture and analyze the I/O access patterns of these applications, we used the Pablo performance environment and its I/O extensions. Below, we present the Pablo environment and its I/O analysis mechanisms followed by a brief description of the selected applications. 3.1 The Pablo Performance Environment Pablo <ref> [24] </ref> is a portable performance environment that supports performance data capture and analysis. The instrumentation software captures dynamic performance data via instrumented source code that is linked with a data capture library.
Reference: [25] <author> Reed, D. A., Elford, C. L., Madhyastha, T., Scullin, W. H., Aydt, R. A., and Smirni, E. </author> <title> I/O, performance analysis, and performance data immersion. </title> <booktitle> In Proceedings of MASCOTS '96 (Feb. </booktitle> <year> 1996), </year> <pages> pp. 1-12. </pages>
Reference-contexts: File region summaries provide comprehensive information over accesses to a file region. Finally, using a set of data extraction tools, it is possible to visualize I/O activity as a function of the application elapsed time using either workstation graphics or immersive 3 virtual environments <ref> [25] </ref>. Collectively, the event traces, the statistical summaries, and the visualization toolkit provide a powerful set of I/O analysis and display options. 3.2 The SIO Applications Space precludes a complete description of the I/O behavior of all SIO applications.
Reference: [26] <author> Seamons, K. E., Chen, Y., Jones, P., Jozwiak, J., and Winslett, M. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95 (December 1995). </booktitle>
Reference-contexts: Furthermore, many experimental I/O libraries focus on specific classes of I/O patterns and provide flexible data distribution and data management policies. For example, Galley [18], Panda <ref> [26] </ref>, PASSION [2] and Jovian [1] support external multi-dimensional arrays, can restructure disk requests, and provide collective I/O operations that effectively utilize the available disk bandwidth.
Reference: [27] <author> Smirni, E., Aydt, R. A., Chien, A. A., and Reed, D. A. </author> <title> I/O requirements of scientific applications: An evolutionary view. </title> <booktitle> In High Performance Distributed Computing (1996), </booktitle> <pages> pp. 49-59. </pages>
Reference-contexts: Subsequent studies <ref> [19, 27] </ref> indicated that users attempt to adjust the application I/O access patterns to match the characteristics of the underlying parallel file system to maximize performance. <p> Despite the demonstrated performance rewards from use of more expressive APIs, several studies have shown that users frequently opt to continue using UNIX I/O primitives on parallel systems. The rationale for this lies in the desire to maximize code portability across diverse parallel platforms and to minimize software restructuring <ref> [27] </ref>. Simply put, many scientific application developers are unwilling to sacrifice portability for performance.
Reference: [28] <author> The MPI-IO Committee. </author> <title> MPI-IO: a parallel file I/O interface for MPI, </title> <month> April </month> <year> 1996. </year> <note> Version 0.5. </note>
Reference-contexts: Therefore, the design and implementation of a standard parallel I/O API that is expressive, compact, intuitively appealing, and at the same time offers high performance [6] is one of the goals of the SIO effort. Our I/O characterization study has been instrumental in the design and implementation of MPI-IO <ref> [28] </ref> that is adopted by the SIO community as the standard parallel I/O interface. 6.2 Emerging I/O APIs Even within our small application sample, the diversity of I/O request sizes and patterns suggests that achieving high performance is unlikely with a single file system policy.
Reference: [29] <author> Toledo, S., and Gustavson, F. G. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems (Philadelphia, </booktitle> <month> May </month> <year> 1996), </year> <pages> pp. 28-40. </pages>
Reference-contexts: The desire for intuitive APIs with high expressive power has led to a host of domain specific I/O libraries that support specific problem domains and access pattern classes. Such libraries have emerged for computational chem 2 istry [8] and for out-of-core linear algebra computations <ref> [29] </ref>. Early experience with these APIs has shown major I/O performance improvements | with high-level descriptions of I/O request patterns, file system policies can more intelligently prefetch and cache I/O data.
Reference: [30] <author> Wu, Y.-S. M., Cuccaro, S. A., Hipes, P. G., and Kuppermann, A. </author> <title> Quantum chemical reaction dynamics on a highly parallel supercomputer. </title> <journal> Theoretica Chimica Acta 79 (1991), </journal> <pages> 225-239. </pages>
Reference-contexts: This results in a different number of iterations and different number of Fock matrix constructions even if both codes execute the same input. The codes use different data formats and produce different volumes of data even for matching inputs. QCRD This quantum chemical reaction dynamics (QCRD) application <ref> [30] </ref> is used to understand the nature of elementary chemical reactions. The code is written in C and uses the method of symmetrical hyperspherical coordinates and local hyperspherical surface functions to solve the Schrodinger equation for the cross sections of the scattering of an atom by a diatomic molecule.
References-found: 30

