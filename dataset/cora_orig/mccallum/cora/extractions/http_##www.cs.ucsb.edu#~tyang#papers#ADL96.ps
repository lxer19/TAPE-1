URL: http://www.cs.ucsb.edu/~tyang/papers/ADL96.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/SWEB.html
Root-URL: http://www.cs.ucsb.edu
Email: smithtrg@cs.ucsb.edu  
Title: Scalability Issues for High Performance Digital Libraries on the World Wide Web  
Author: Daniel Andresen, Tao Yang, Omer Egecioglu, Oscar H. Ibarra, and Terence R. Smith fdandrese, tyang, omer, ibarra, 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: We investigate scalability issues involved in developing high performance digital library systems. Our observations and solutions are based on our experience with the Alexandria Digital Library (ADL) testbed under development at UCSB. The current ADL system provides on-line browsing and processing of digitized maps and other geo-spatially mapped data via the World Wide Web (WWW). A primary activity of the ADL system involves computation and disk I/O for accessing compressed multi-resolution images with hierarchical data structures, as well as other duties such as supporting database queries and on-the-fly HTML page generation. Providing multi-resolution image browsing services can reduce network traffic but impose some additional cost at the server. We discuss the necessity of having a multi-processor DL server to match potentially huge demands in simultaneous access requests from the Internet. We have developed a distributed scheduling system for processing DL requests, which actively monitors the usages of CPU, I/O channels and the interconnection network to effectively distribute work across processing units to exploit task and I/O parallelism. We present an experimental study on the performance of our scheme in addressing the scalability issues arising in ADL wavelet processing and file retrieval. Our results indicate that the system delivers good performance on these types of tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Andresen, T. Yang, V. Holmedahl, O. Ibarra, SWEB: </author> <title> Towards a Scalable WWW Server on MultiComputers, </title> <booktitle> Proc. of Intl. Symp. on Parallel Processing, IEEE, </booktitle> <month> April, </month> <year> 1996. </year> <note> HTTP://www.cs.ucsb.edu/Research/rapid sweb/SWEB.html. </note>
Reference-contexts: In particular, by actively monitoring the run-time CPU, disk I/O, and network loads of system resource units, we can dynamically schedule user requests to nodes in a manner that provides the greatest overall processing efficiency. Our strategies are based on our work for a scalable WWW server called SWEB <ref> [1] </ref>. We assume that the system contains a set of networked workstations (see Figure 3). Some of the workstations are connected to SCSI-II disks or mass storage subsystems. In this paper, the terms workstation unit, node, and processor are interchangeable. <p> The primary disadvantage of URL redirection in practice is the added overhead of an additional connect/pass request/parse/respond cycle after the redirection occurs. We will show that such overhead is more than negated by improved performance overall. 4.2. The processor assignment of ADL requests In <ref> [1] </ref>, we designed an algorithm that decides the routing for a general HTTP request. In this subsection, we discuss the strategies for the ADL system. In the previous work on load balancing (e.g. [13]), usually one factor (CPU load) is considered.
Reference: [2] <author> D.Andresen, L.Carver, R.Dolin, C.Fischer, J.Frew, M.Goodchild, O.Ibarra, R.Kothuri, M.Larsgaard, B.Manjunath, D.Nebert, J.Simpson, T.Smith, T.Yang, Q.Zheng, </author> <title> The WWW Prototype of the Alexandria Digital Library, </title> <booktitle> Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan August 22 - 25, </address> <year> 1995. </year>
Reference-contexts: 1. Introduction The number of digital library (DL) projects is increasing rapidly at both the national and the international levels (see, for example, <ref> [6, 2] </ref>). Many of the current projects are moving rapidly towards their goals of supporting on-line retrieval and processing of major collections of digitized documents over the Internet. Performance and scalability issues are especially important for the Alexandria Digital Library (ADL) project [2, 14]. <p> Many of the current projects are moving rapidly towards their goals of supporting on-line retrieval and processing of major collections of digitized documents over the Internet. Performance and scalability issues are especially important for the Alexandria Digital Library (ADL) project <ref> [2, 14] </ref>. The fundamental goal of this project is to provide users with the ability to access and process broad classes of spatially-referenced materials from the Internet. <p> Considering that popular WWW sites such as Alta Vista, Lycos and Yahoo have been receiving over two million accesses per day (or 20-30 requests per second), and the ADL server involves much more intensive I/O and heterogeneous CPU activities, a multi-processor server becomes indispensable <ref> [2] </ref>. In this paper, we investigate the network bandwidth requirement in the ADL system using progressive image browsing retrieval and the computational and I/O demands for supporting such activities. <p> To support these features, the ADL system is using a wavelet-based hierarchical data representation for multi-resolution decomposition of images. Images and their subregions can be browsed in different levels of resolution and can be delivered in a progressive manner <ref> [2] </ref>. We briefly describe the techniques of wavelet image data retrieval and transformation below. Given an image, a forward wavelet transform produces a sub-sampled image of lower resolution called a thumbnail, and three additional coefficient data sets. <p> It should be noted that there are other operations performed in the ADL server. For example, content-based database queries to find suitable images are important, so the speed of the database server and its supporting mass storage is vital <ref> [2, 12] </ref>. We assume that the database functionality is provided by a separate computer within ADL, and so focus our attention on the problem of delivering data, whether simple files or wavelet data, to the user as quickly as possible over the Net.
Reference: [3] <author> E. Brewer, </author> <type> Personal communication, </type> <address> http://inktomi.berkeley.edu, Jan., </address> <year> 1996. </year>
Reference-contexts: Related Work Numerous other initiatives to create high-performance HTTP servers have been reported. The Inktomi server at UC Berkeley is based on NOW technology <ref> [3] </ref>, and the research focus is on information searching and indexing. NCSA [10] has built a multi-workstation HTTP server based on round-robin domain name resolution (DNS) to assign requests to workstations.
Reference: [4] <author> E.C.K. Chui, </author> <title> Wavelets: A Tutorial in Theory and Applications, </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: We model such a process as follows. subregion (I 1 ) = Inverse W avelet (subregion (I 2 ); subregion (C 1 ); subregion (C 2 ); subregion (C 3 )): A detailed definition of forward and inverse wavelet functions can be found in <ref> [4] </ref>. The time complexity of wavelet transforms is proportional to the image size. The wavelet transform can be applied recursively, namely the thumbnail I 2 can be decomposed further to produce smaller thumbnails I 2 ; I 3 ; .
Reference: [5] <author> C. Fischer, J.Frew, M. Larsgaard, T.R. Smith and Q.Zheng. </author> <title> Alexandria Digital Library: Rapid Prototype and Metadata Schema. </title> <booktitle> Proceedings of Advances in Digital Libraries 1995, </booktitle> <volume> Vol. </volume> <pages> ,pp. </pages> , <year> 1995. </year>
Reference-contexts: A major reason for adopting an evolutionary and incremental approach to design and development stems from the rapidity of developments in Internet technology. The first increment in the development of ADL involved the design and construction of a stand-alone rapid prototype (RP) system <ref> [5] </ref>. A second, and now completed, increment provided an augmented version of the functionality of the RP over WWW. The third increment is focused on developing a greatly enhanced catalog component based on a general model of metadata and supporting catalog interoperability with other DLs. <p> The collections on which ADL is initially focused include spatially-referenced materials, such as digitized maps, digitized aerial photographs, and images from many domains of application <ref> [5] </ref>. An important aspect of the ADL collection is that individual items are typically very large. <p> Footprints are represented in an extensible metadata model for spatially-referenced information (currently combining the FGDC and USMARC standards <ref> [5] </ref>) and are indexed to support efficient search over the catalog holdings. The metadata model also incorporates extensions involving gazetteers (i.e. mappings between named geographic features and the footprints of their spatial extent) and preselected image textures features.
Reference: [6] <author> E. Fox, Akscyn, R., Furuta, R. and Leggett, J. </author> <title> (Eds), </title> <journal> Special issue on digital libraries, CACM, </journal> <month> April </month> <year> 1995. </year>
Reference-contexts: 1. Introduction The number of digital library (DL) projects is increasing rapidly at both the national and the international levels (see, for example, <ref> [6, 2] </ref>). Many of the current projects are moving rapidly towards their goals of supporting on-line retrieval and processing of major collections of digitized documents over the Internet. Performance and scalability issues are especially important for the Alexandria Digital Library (ADL) project [2, 14].
Reference: [7] <author> K. Goswami, M. Devarakonda, R. Iyer, </author> <title> Prediction-based Dynamic Load-sharing Heuristics, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4:6, </volume> <pages> pp. 638-648, </pages> <year> 1993. </year>
Reference-contexts: The optimal HTTP request assignment to processors does not solely depend on CPU loads. Thus we develop a multi-faceted scheduling scheme that can effectively utilize the system resources by considering the aggregate impact of multiple parameters on system performance. In <ref> [7] </ref>, resource requirements are predicted and suggested to guide the load sharing. They discuss multiple factors, but utilize only the CPU factor in predicting response times. With ADL applications we can take advantage of specific domain knowledge to accurately predict multiple resource requirements for each request. 7.
Reference: [8] <author> Hypertext Transfer Protocol(HTTP): </author> <title> A protocol for networked information, </title> <address> http://www.w3.org/ hypertext/WWW/Protocols/HTTP/HTTP2.html, </address> <month> June, </month> <year> 1995. </year>
Reference-contexts: The URL defines which resource the user wishes to access, the HTML language allows the information to be presented in a platform-independent but still well-formatted manner, while the HTTP protocol is the application-level mechanism for achieving the transfer of information <ref> [8] </ref>. The WWW supports general types of multimedia information systems while a DL system provides more advanced features for browsing, searching, and delivering digitized documents. A major reason for adopting an evolutionary and incremental approach to design and development stems from the rapidity of developments in Internet technology.
Reference: [9] <author> Lycos Usage: </author> <title> Accesses per Day, </title> <address> http://lycos.cs.cmu.edu/usage-day.html, June 17, </address> <year> 1995. </year>
Reference: [10] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: The current version of our system uses a distributed scheduler. The user requests are first evenly routed to processors via Domain Name System (DNS) rotation. DNS rotation provides the initial assignment of HTTP requests and is used in the NCSA multi--workstation server <ref> [10] </ref>. In this scheme, multiple real machines are mapped to the same IP name. When a client requests the network ID of the machine name (e.g., www.cs.ucsb.edu), the DNS at the server site rotates the network IDs, picking one (e.g., 1.1.1.1) to send back to the client. <p> The rotation on available workstation network IDs is in a round-robin fashion. This functionality is available in current DNS systems. The major advantages of this technique are simplicity and ease of implementation <ref> [10] </ref>. The DNS is subject to caching problems when attempting to do dynamic load balancing, and it assigns requests without consulting dynamically-changing system load information. Thus our scheduler conducts a further re-direction of requests. <p> Related Work Numerous other initiatives to create high-performance HTTP servers have been reported. The Inktomi server at UC Berkeley is based on NOW technology [3], and the research focus is on information searching and indexing. NCSA <ref> [10] </ref> has built a multi-workstation HTTP server based on round-robin domain name resolution (DNS) to assign requests to workstations. The round-robin technique is effective when HTTP requests access HTML information of relatively uniform size and the load and computing power of workstations is relatively comparable.
Reference: [11] <author> A. Poulakidas, A. Srinivasan, O. Egecioglu, O. Ibarra, and T. Yang, </author> <title> Experimental Studies on a Compact Storage Scheme for Wavelet-based Multiresolution Subregion Retrieval, </title> <booktitle> Proceedings of NASA 1996 Combined Industry, Space and Earth Science Data Compression Workshop, </booktitle> <address> Utah, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Table 1 gives the compressed size for a number of greyscale images. Each image was eight bits per pixel, and was a square image. We use the com-pression algorithm developed in <ref> [11] </ref> whose compression ratio is approximately 90%. The compressed full images are still sizeable. Using progressive image delivery can reduce network demands since full resolution images are not always required.
Reference: [12] <author> S. Prabhakar, D. Agrawal, A. El Abbadi, A. Singh, T. Smith, </author> <title> Content based placement and browsing, </title> <type> CS Tech Report, </type> <institution> UCSB, </institution> <year> 1995. </year>
Reference-contexts: It should be noted that there are other operations performed in the ADL server. For example, content-based database queries to find suitable images are important, so the speed of the database server and its supporting mass storage is vital <ref> [2, 12] </ref>. We assume that the database functionality is provided by a separate computer within ADL, and so focus our attention on the problem of delivering data, whether simple files or wavelet data, to the user as quickly as possible over the Net.
Reference: [13] <author> B. A. Shirazi, A. R. Hurson, and K. M. Kavi (Eds), </author> <title> Scheduling and Load Balancing in Parallel and Distributed Systems, </title> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: The processor assignment of ADL requests In [1], we designed an algorithm that decides the routing for a general HTTP request. In this subsection, we discuss the strategies for the ADL system. In the previous work on load balancing (e.g. <ref> [13] </ref>), usually one factor (CPU load) is considered. A processor can be classified as lightly loaded and heavily loaded based on the CPU load. One purpose of such a classification is to update load information only when a classification changes. Such a strategy reduces unnecessary overhead. <p> We have not addressed scheduling on processors with different architectures in this paper; however, our techniques can be extended to such cases. Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems, for which a collection of papers is available in <ref> [13] </ref>. In these studies, task arrivals may temporarily be uneven among processors and the goal of load balancing is to adjust the imbalance between processors by appropriately transferring tasks from overloaded processors to underloaded processors.
Reference: [14] <author> T. R. Smith and J. Frew. </author> <title> Alexandria digital library. </title> <journal> CACM, </journal> <volume> 38(4) </volume> <pages> 61-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Many of the current projects are moving rapidly towards their goals of supporting on-line retrieval and processing of major collections of digitized documents over the Internet. Performance and scalability issues are especially important for the Alexandria Digital Library (ADL) project <ref> [2, 14] </ref>. The fundamental goal of this project is to provide users with the ability to access and process broad classes of spatially-referenced materials from the Internet.
Reference: [15] <author> R. Wolski, C. Anglano, J. Schopf, F. Berman, </author> <title> Developing Heterogeneous Applications Using Zoom and HeNCE, </title> <booktitle> Proceedings of the Heterogeneous Computing Workshop, </booktitle> <volume> HCW '95, </volume> <pages> pp. 12-21, </pages> <address> Santa Barbara, CA, </address> <publisher> IEEE, </publisher> <month> April, </month> <year> 1995. </year>
Reference-contexts: They can be used for other computing needs, and can leave and join the system resource pool at any time. Thus scheduling techniques which are adaptive to the dynamic change of system load and configuration are desirable. Heterogeneous computing is studied in <ref> [15] </ref>. We have not addressed scheduling on processors with different architectures in this paper; however, our techniques can be extended to such cases. Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems, for which a collection of papers is available in [13].
References-found: 15

