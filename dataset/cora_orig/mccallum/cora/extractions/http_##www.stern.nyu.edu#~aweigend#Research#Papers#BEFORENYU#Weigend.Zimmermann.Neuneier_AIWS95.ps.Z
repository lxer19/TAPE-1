URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Weigend.Zimmermann.Neuneier_AIWS95.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  georg.zimmermann@zfe.siemens.de  
Title: The Observer-Observation Dilemma in Neuro-Forecasting: Reliable Models From Unreliable Data Through CLEARNING  
Author: Andreas S. Weigend Hans Georg Zimmermann Ralph Neuneier Otto Hahn 
Address: Boulder, CO 80309-0430  SN 4  D-81739 Munchen, Germany  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Siemens AG, ZFE T  Ring 6  
Abstract: This paper introduces the idea of clearning, of simultaneously cleaning data and learning the underlying structure. The cleaning step can be viewed as top-down processing (the model modifies the data), and the learning step can be viewed as bottom-up processing (where the data modifies the model). After discussing the statistical foundation of the proposed method from a maximum likelihood perspective, we apply clearning to a notoriously hard problem where benchmark performances are very well known: the prediction of foreign exchange rates. On the difficult 1993-1994 test period, clearning in conjunction with pruning yields an annualized return between 35 and 40% (out-of-sample), significantly better than an otherwise identical network trained without cleaning. The network was started with 69 inputs and 15 hidden units and ended up with only 39 non-zero weights between inputs and hidden units. The resulting ultra-sparse final architectures obtained with clearning and pruning are immune against overfitting, even on very noisy problems since the cleaned data allow for a simpler model. Apart from the very competitive performance, clearning gives insight into the data: we show how to estimate the overall signal-to-noise ratio of each input variable, and we show that error estimates for each pattern can be used to detect and remove outliers, and to replace missing or corrupted data by cleaned values. Clearning can be used in any nonlinear regression or classification problem.
Abstract-found: 1
Intro-found: 1
Reference: [Abu-Mostafa, 1995] <author> Abu-Mostafa, Y. </author> <year> (1995). </year> <title> Hints. Neural Computation, </title> <publisher> 7:(in press). </publisher>
Reference-contexts: In terms of overfitting problems, this corresponds to building a model that is resistant against outliers. A recent example is the insertion of priors that describe the derivative of the outputs with respect the inputs [Neuneier and Zimmermann, 1995]. * Hints. <ref> [Abu-Mostafa, 1995] </ref> gives the example of the symmetry-hint for predicting foreign exchange data. This hint corresponds to viewing exchange rate returns first from one country, then from the other country. The hint suggests that the dynamics should be the same.
Reference: [Bollerslev, 1986] <author> Bollerslev, T. </author> <year> (1986). </year> <title> Generalized autoregressive conditional heteroskedasticity. </title> <journal> Journal of Econometrics, </journal> <volume> 21 </volume> <pages> 307-328. </pages>
Reference-contexts: If there are significant non-zero off-diagonal contribution, the modeling can be improved by transforming the data by the inverse of the noise covariance matrix. * ARSCH Models (AutoRegressive Special Conditional Heteroskedasticity). Estimating the noise levels enables us to generalize ARCH and GARCH models <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>: since we allow for nonlinearities at every level, we call these models where we input the noise levels averaged over an exponentially decaying window in time ARSCH models (AutoRegressive Special Conditional Heteroskedasticity).
Reference: [Bollerslev et al., 1990] <author> Bollerslev, T., Chou, R. Y., Jayaraman, N., and Kroner, K. F. </author> <year> (1990). </year> <title> ARCH modeling in finance: A review of the theory and empirical evidence. </title> <journal> Journal of Econometrics, </journal> <volume> 52(1) </volume> <pages> 5-60. </pages>
Reference-contexts: If there are significant non-zero off-diagonal contribution, the modeling can be improved by transforming the data by the inverse of the noise covariance matrix. * ARSCH Models (AutoRegressive Special Conditional Heteroskedasticity). Estimating the noise levels enables us to generalize ARCH and GARCH models <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>: since we allow for nonlinearities at every level, we call these models where we input the noise levels averaged over an exponentially decaying window in time ARSCH models (AutoRegressive Special Conditional Heteroskedasticity).
Reference: [Bonnlander and Weigend, 1994] <author> Bonnlander, B. V. and Weigend, A. S. </author> <year> (1994). </year> <title> Selecting input variables using mutual information and nonparamteric density estimation. </title> <booktitle> In Proceedings of the 1994 International Symposium on Artificial Neural Networks (ISANN'94), </booktitle> <pages> pages 42-50, </pages> <address> Tainan, Taiwan. </address>
Reference-contexts: our model to a higher than average degree, using the method of gated experts [Weigend and Mangeas, 1995]-important for very noisy processes. 5 We are particularly interested in removing inputs completely; an alternative method for variable subset selection based on the information theoretic measure of mutual information is described in <ref> [Bonnlander and Weigend, 1994] </ref>. with pruning only. (Screendump from the SENN simlulator.) The top two rows are for the weights that are active; the bottow row for the weights that are pruned away.
Reference: [Buntine and Weigend, 1991] <author> Buntine, W. L. and Weigend, A. S. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 603-643. </pages>
Reference-contexts: at iteration i is given by x i+1 = x i @x 1 This approach can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables [Seber and Wild, 1989] and to methods dealing with missing data <ref> [Buntine and Weigend, 1991, Tresp et al., 1994] </ref>.
Reference: [Engle, 1982] <author> Engle, R. F. </author> <year> (1982). </year> <title> Autoregressive conditional heteroskedasticity with estimates of the variance of united kingdom inflation. </title> <journal> Econometrica, </journal> <volume> 50 </volume> <pages> 987-1007. </pages>
Reference-contexts: If there are significant non-zero off-diagonal contribution, the modeling can be improved by transforming the data by the inverse of the noise covariance matrix. * ARSCH Models (AutoRegressive Special Conditional Heteroskedasticity). Estimating the noise levels enables us to generalize ARCH and GARCH models <ref> [Engle, 1982, Bollerslev, 1986, Bollerslev et al., 1990] </ref>: since we allow for nonlinearities at every level, we call these models where we input the noise levels averaged over an exponentially decaying window in time ARSCH models (AutoRegressive Special Conditional Heteroskedasticity).
Reference: [Finnoff et al., 1993] <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving generalization performance by nonconvergent model selection methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: to the standard deviation of its fluctuations. (The fluctuations occur in response to the training inputs on a pattern-by-pattern basis.) We start with an oversized network, rank all weights in terms of a test statistic (Eq.9), and remove those of low significance in order to obtain a sparse network topology <ref> [Finnoff et al., 1993] </ref>. The pruning step is performed in conjunction with early stopping.
Reference: [Huffel and Vanderwalle, 1991] <author> Huffel, S. V. and Vanderwalle, J. </author> <year> (1991). </year> <title> The total least squares problem: Computational aspects and analysis. </title> <booktitle> In Frontiers in Applied Mathematics, </booktitle> <volume> volume 9. </volume> <publisher> SIAM. </publisher>
Reference-contexts: rule for the weights is identical to standard backpropagation w i+1 = w i @w @w The update rule for the cleaned input x i at iteration i is given by x i+1 = x i @x 1 This approach can be related to the method of total least squares <ref> [Huffel and Vanderwalle, 1991] </ref> (in the context of linear models), to error in variables [Seber and Wild, 1989] and to methods dealing with missing data [Buntine and Weigend, 1991, Tresp et al., 1994].
Reference: [Neuneier and Zimmermann, 1995] <author> Neuneier, R. and Zimmermann, H. </author> <year> (1995). </year> <title> An efficient method of neurofuzzy in forecasting. </title> <booktitle> In Proceedings of ICANN 95 (in press). </booktitle>
Reference-contexts: In terms of overfitting problems, this corresponds to building a model that is resistant against outliers. A recent example is the insertion of priors that describe the derivative of the outputs with respect the inputs <ref> [Neuneier and Zimmermann, 1995] </ref>. * Hints. [Abu-Mostafa, 1995] gives the example of the symmetry-hint for predicting foreign exchange data. This hint corresponds to viewing exchange rate returns first from one country, then from the other country. The hint suggests that the dynamics should be the same.
Reference: [Rumelhart et al., 1995] <author> Rumelhart, D. E., Durbin, R., Golden, R., and Chauvin, Y. </author> <year> (1995). </year> <title> Backpropagation: The basic theory. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, Backpropagation: </editor> <booktitle> Theory, Architectures, and Applications, pages 1-??, </booktitle> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The ratio between and describes the trade off between the stiffnesses (or importances) between the output errors and input errors. A statistical interpretation of the cost function can be given in a maximum likelihood framework <ref> [Rumelhart et al., 1995] </ref>. We assume that each pattern was generated by a "true" input (estimated by x) and a "true" output (estimated by y). We then assume that it is corrupted by Gaussian noise (additive to all the inputs and all the outputs, independent in each component).
Reference: [Seber and Wild, 1989] <author> Seber, G. A. F. and Wild, C. J. </author> <year> (1989). </year> <title> Nonlinear Regression. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: @w @w The update rule for the cleaned input x i at iteration i is given by x i+1 = x i @x 1 This approach can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables <ref> [Seber and Wild, 1989] </ref> and to methods dealing with missing data [Buntine and Weigend, 1991, Tresp et al., 1994].
Reference: [Tresp et al., 1994] <author> Tresp, V., Ahmad, S., and Neuneier, R. </author> <year> (1994). </year> <title> Training neural networks with deficient data. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6 (NIPS*93), </booktitle> <pages> pages 128-135, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: at iteration i is given by x i+1 = x i @x 1 This approach can be related to the method of total least squares [Huffel and Vanderwalle, 1991] (in the context of linear models), to error in variables [Seber and Wild, 1989] and to methods dealing with missing data <ref> [Buntine and Weigend, 1991, Tresp et al., 1994] </ref>.
Reference: [Weigend, 1994] <author> Weigend, A. S. </author> <year> (1994). </year> <title> On overfitting and the effective number of hidden units. </title> <editor> In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 335-342, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: We briefly describe some of the methods that are useful on financial data. One of them, pruning, will play a crucial role in combination with clearning (discussed in the Section 2). * Stop early. Backpropagation is an iterative procedure: the complexity of the model gradually increases with training time <ref> [Weigend, 1994] </ref>. Starting training with small weights and stopping early introduces a preference for linear models since the weights do not have enough time to grow large enough to express significant nonlinearities. This can be a serious problem when trying to find nonlinear structure in noisy data.
Reference: [Weigend, 1995] <author> Weigend, A. S. </author> <year> (1995). </year> <title> Time series analysis and prediction. </title> <editor> In Smolensky, P., Mozer, M. C., and Rumelhart, D. E., editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: The difference between hints and pseudo-data is that any input vector can be used to descend on the hint, whereas pseudo-data tend to stay close to the actual data since the added noise has them explore the vicinity of the data points <ref> [Weigend, 1995] </ref>.
Reference: [Weigend et al., 1990] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference-contexts: Adding a complexity term to the cost function that effectively counts the number of significantly sized weights is known as weight-elimination <ref> [Weigend et al., 1990] </ref>. This method treats the weights as independent; it has first been applied to financial data in [Weigend et al., 1991]. * Prune weights.
Reference: [Weigend and LeBaron, 1994] <author> Weigend, A. S. and LeBaron, B. </author> <year> (1994). </year> <title> Evaluating neural network predictors by bootstrapping. </title> <booktitle> In Proceedings of International Conference on Neural Information Processing (ICONIP'94), </booktitle> <pages> pages 1207-1212. </pages> <note> Technical Report CU-CS-725-94, </note> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <note> also submitted to IEEE TNN. </note>
Reference-contexts: The pruning step is performed in conjunction with early stopping. This pruning limits the ability of the network to memorize the training data without introducing a bias towards linear modelsthis is an important distinction to the use of early stopping alone <ref> [Weigend and LeBaron, 1994] </ref>. * Neuro-fuzzy methods. Constraints, extracted from humans in the form of logical rules, can be used to constrain the network architecture and learning. In terms of overfitting problems, this corresponds to building a model that is resistant against outliers. <p> This process can be iterated. 4 Examples are (1) estimating uncertainties due to the splitting of the data <ref> [Weigend and LeBaron, 1994] </ref>-crucial when estimate the error in the outputs due to the uncertainty in the inputs. We start by computing the matrix of empirical input errors D (t); this matrix consists of one vector (across inputs) for each time step (or pattern).
Reference: [Weigend and Mangeas, 1995] <author> Weigend, A. S. and Mangeas, M. </author> <year> (1995). </year> <title> Experts for prediction: discovering regimes and avoiding overfitting. </title> <type> Technical Report CU-CS-764-95, </type> <institution> University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/experts.ps.Z. </institution>
Reference-contexts: with the method of fractional binning [Weigend and Srivastava, 1995]-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4) finding trading days where we can trust our model to a higher than average degree, using the method of gated experts <ref> [Weigend and Mangeas, 1995] </ref>-important for very noisy processes. 5 We are particularly interested in removing inputs completely; an alternative method for variable subset selection based on the information theoretic measure of mutual information is described in [Bonnlander and Weigend, 1994]. with pruning only. (Screendump from the SENN simlulator.) The top two
Reference: [Weigend and Nix, 1994] <author> Weigend, A. S. and Nix, D. A. </author> <year> (1994). </year> <title> Predictions with confidence intervals (local error bars). </title> <booktitle> In Proceedings of the International Conference on Neural Information Processing (ICONIP'94), </booktitle> <pages> pages 1207-1212, </pages> <address> Seoul, Korea. </address>
Reference-contexts: We are primarily interested in pruning connections from the inputs to the hidden units. 5 All the weights of the input-to-hidden layer are ranked according to their test value. cross-validation sets are set aside to determine meta-parameters, (2) estimating confidence intervals for unimodal distributions <ref> [Weigend and Nix, 1994] </ref>-useful for problems that consider Sharpe ratios, (3) obtaining essentially model free arbitrary distributions with the method of fractional binning [Weigend and Srivastava, 1995]-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4) finding trading days where we
Reference: [Weigend et al., 1991] <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Adding a complexity term to the cost function that effectively counts the number of significantly sized weights is known as weight-elimination [Weigend et al., 1990]. This method treats the weights as independent; it has first been applied to financial data in <ref> [Weigend et al., 1991] </ref>. * Prune weights.
Reference: [Weigend and Srivastava, 1995] <author> Weigend, A. S. and Srivastava, A. N. </author> <year> (1995). </year> <title> Predicting probability distributions: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6. </volume>
Reference-contexts: weights of the input-to-hidden layer are ranked according to their test value. cross-validation sets are set aside to determine meta-parameters, (2) estimating confidence intervals for unimodal distributions [Weigend and Nix, 1994]-useful for problems that consider Sharpe ratios, (3) obtaining essentially model free arbitrary distributions with the method of fractional binning <ref> [Weigend and Srivastava, 1995] </ref>-important for multi-modal processes, e.g., when we expect a big move that could go either up or down, and (4) finding trading days where we can trust our model to a higher than average degree, using the method of gated experts [Weigend and Mangeas, 1995]-important for very noisy
Reference: [Winograd and Flores, 1986] <author> Winograd, T. and Flores, F. </author> <year> (1986). </year> <title> Understanding Computers and Cognition. </title> <publisher> Ablex Publishing, </publisher> <address> Norwood, New Jersey. </address>
References-found: 21

