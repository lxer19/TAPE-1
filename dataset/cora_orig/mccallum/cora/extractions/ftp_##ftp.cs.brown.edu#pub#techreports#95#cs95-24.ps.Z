URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-24.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-24.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AbW] <author> N. Abe and M. Warmuth, </author> <title> "On the Computational Complexity of Approximating Distributions by Probabilistic Automata," </title> <type> UCSC, </type> <institution> UCSC-CRL-90-63, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: DISCUSSION 23 our prefetcher with a logical prefetcher based on the semantics of the application, so as to get the best of both worlds. Similar techniques for cache replacement appear in [FKL]. The framework of Abe and Warmuth <ref> [AbW] </ref>, who investigated a quite different learning problem related to FSAs, suggests a static PAC-learning framework for prefetching, in which the prefetcher is trained on several independently generated sequences of a particular length.
Reference: [AlV] <author> D. Aldous and U. Vazirani, </author> <title> "A Markovian Extension of Valiant's Learning Model," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 392-396. </pages>
Reference-contexts: A PAC learning framework incorporating Markov sources of examples is developed in <ref> [AlV] </ref>.
Reference: [Ale] <author> T. Alexander, </author> <type> personal communication. </type>
Reference-contexts: Alexander et al. also study higher-order Markov models for prediction; however the necessity of having a small fixed amount of memory for hardware prefetching and the restrictions imposed by the hashing schemes make it difficult for them to gain extra performance improvements with higher-order models <ref> [Ale] </ref>. Our model of prefetching described in this chapter concentrates on prefetching for the next page request. Like in compiler-based techniques for prefetching, our model could be generalized to prefetching much in advance of an anticipated page request, by looking "down" into the model.
Reference: [AlK] <author> T. Alexander and G. Kedem, </author> <title> "Design and Evaluation of a Distributed Cache Architecture with Prediction," </title> <institution> Duke University Technical Report, DUKE-CS-1994-05. </institution>
Reference-contexts: Song et al. study prefetching based on page fault history [SoC]. Alexander et al. consider hardware prefetching schemes between processor and main memory by extracting tables of fixed length contexts from past user requests <ref> [AlK] </ref>. Our approach to prefetching as described in Section 2.2 is also based on monitoring past user behavior; we develop a systematic approach to prefetching based on data compression principles. Prefetching has also been studied in other systems environments. Prefetching in a parallel environment is studied in [KoE]. <p> Notice that the predictors developed in this paper can also be used from session to session, by saving 56 CHAPTER 5. PRACTICAL PREFETCHING VIA DATA COMPRESSION the model built in earlier sessions. Independently to our work, Alexander et al. propose an interesting architecture for hardware prefetching <ref> [AlK] </ref>. Their algorithm for hardware prefetching of memory addresses into cache is closely related to the FOM algorithm using the move-to-front heuristic.
Reference: [AmM] <author> Y. Amit and M. Miller, </author> <title> "Large Deviations for Coding Markov Chains and Gibbs Random Fields," </title> <institution> Washington University, </institution> <type> Technical Report, </type> <year> 1990. </year>
Reference-contexts: vectors (p 1 ; : : : ; p ff ) and (r 1 ; : : : ; r ff ), we have ff X jp i r i j 2 i=1 p i : Proof : The above lemma is well known; however, we reproduce the proof from <ref> [AmM] </ref> here for the sake of completeness. From the fact that x ln x x + 1 0 and that 3 (x 1) 2 (4x + 2)(x ln x x + 1), we get jx 1j (4x + 2)=3 x ln x x + 1.
Reference: [App] <author> Applelink, </author> <title> "Developer Support-Developer Talk," </title> <journal> AppleLink bulletin board, </journal> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: Even with a cache size of 256 Kbytes, the maximum transfer that can be cached is only 8176 bytes <ref> [App] </ref>. Second, writes that are cached in the buffer cache are later passed to the disk in 512-byte units, which can degrade performance relative to transferring larger files as a unit.
Reference: [ASW] <author> M. M. Astrahan, M. Schkolnick, and K.-Y. Whang, </author> <title> "Approximating the Number of Unique Values of an Attribute Without Sorting ," Information Systems 12 (1987), </title> <type> 11-15. </type>
Reference-contexts: Query optimizers have cost models that estimate the access cost as a function of the predicted number of qualifying rows and find the cheaper alternative. Models already exist in current day relational database management systems (RDBMSs) to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate [Iye]. <p> In Chapter 11 we present our techniques for predicting selectivity for the like predicate; i.e., techniques for estimating alphanumeric selectivity. 10.1 Background and Related Work Models already exist in current day RDBMSs to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. Typically, in the preprocessing phase, a few numbers that "capture" the distribution of data are accumulated and stored in the catalog. <p> A well-studied issue related to predicting selectivity is estimating the number of unique values in a column of the table. In <ref> [ASW, FlM, WVT] </ref>, interesting linear time algorithms for finding the number of unique values in a column based on probabilistic and approximate counting methods have been described.
Reference: [BAD] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer, </author> <title> "Non-volatile Memory for Fast, Reliable File Systems," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992), </booktitle> <pages> 10-22, </pages> <publisher> ACM. </publisher>
Reference-contexts: A third possible metric is the number of operations delayed. The metrics of number of reads delayed and number of operations delayed are very close; intuitively, if writes (including synchronous writes) can be decoupled from disk latency with a small amount of nonvolatile memory <ref> [BAD, RuWa] </ref>, then the number of writes delayed can be ignored. traces, with a 1-Mbyte cache, and Figure 7.3 shows the same for the HP-UX traces (which has an implicit buffer cache, as discussed above).
Reference: [Bel] <author> L. A. Belady, </author> <title> "A Study of Replacement Algorithms for Virtual Storage Computers," </title> <journal> IBM Systems Journal 5 (1966), </journal> <pages> 78-101. </pages>
Reference-contexts: Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs. <p> We find that the page fault rate (number of page faults divided by the length of the request sequence) decreases significantly compared to that of demand fetching, in which the cache is organized using the least-recently used (LRU) heuristic or using the optimal o*ine algorithm, OPT for cache replacement <ref> [Bel] </ref> (in which the page evicted from cache is the one whose next request is furthest in the future). The reduction in fault-rate is also better than that of recent proposed schemes for prefetching [PaZb]. In Section 5.1 we describe the system environment.
Reference: [BCW] <author> T. C. Bell, J. C. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice Hall Advanced Reference Series, </publisher> <year> 1990. </year>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [BCW, Lana] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in [ZiLb] hold without change for the character-based approach. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiLb] hold without change for the character-based approach. Example 3.1 Assume for simplicity that our alphabet is fa; bg. We consider the page request sequence "aaaababaabbbabaa: : : ". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa): : : ". <p> In typical databases, ff is large and k t ff. In this section, we describe our three simple, deterministic prefetching algorithms based on practical data compressors. (An elegant discussion of the data compressors appears in <ref> [BCW] </ref>.) We describe our prefetchers in Sections 5.2.1-5.2.3 in their "generic" form, as pure prefetchers that can store their entire data structure in cache. These prefetchers make k suggestions for prefetch ordered by their relative merit. To make these suggestions the algorithms use O (k) time. <p> arise in practice (for example, when the data structure cannot be stored entirely in cache) are discussed in Section 5.3. 5.2.1 Algorithm LZ Algorithm LZ is almost exactly the same as prefetcher P described in Section 3.2, with one difference: algorithm LZ uses a heuristic that parallels the Welsh implementation <ref> [BCW] </ref> of the Ziv-Lempel data compressor. While LZ is at a leaf, instead of fetching in k pages at random, it resets its current node to be the root (that is, it goes to the root one step early). <p> n j+1 ); select the t j pages not selected in an earlier loop that have the highest probabilities in the current jth-order context; prefetch the selected pages not already in cache; end The above technique is based on the prediction by partial match algorithm for data compression using exclusion <ref> [BCW] </ref>. In our simulations we use PPM of order 3 and order 1. The various jth-order Markov predictors, j = 0; 1; : : : ; m, can be represented and updated simultaneously in an efficient manner using a forward tree with vine pointers [BCW]. <p> algorithm for data compression using exclusion <ref> [BCW] </ref>. In our simulations we use PPM of order 3 and order 1. The various jth-order Markov predictors, j = 0; 1; : : : ; m, can be represented and updated simultaneously in an efficient manner using a forward tree with vine pointers [BCW]. An example of a forward tree with vine pointers is given in Figure 5.2.
Reference: [Bla] <author> D. Blackwell, </author> <title> "An Analog to the Minimax Theorem for Vector Payoffs," </title> <journal> Pacific Journal of Mathematics 6 (1956), </journal> <pages> 1-8. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal [Cov]. In information theory and in statistics <ref> [Bla, CoS, FMG, Han] </ref> interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed, but the ff = 2, k = 1 case is clearly unsuitable for our
Reference: [BEHa] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <note> "Occam's Razor," Information Processing Letters 24 (1987). </note>
Reference-contexts: We evaluate our prefetching algorithm relative to the best online algorithm that has complete knowledge of the structure and transition probabilities of the Markov source. Prefetching is a learning problem that involves predicting the page requests of the user. Work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is 3.1. PAGE REQUEST MODELS AND MAIN RESULTS 9 synonymous with generalization and data compression. If a data compressor expects a certain character to be next with a very high probability, it will assign that character a relatively small code.
Reference: [BEHb] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> "Learnability and the Vapnik Chervonenkis Dimension," </title> <note> Journal of the ACM (October 1989). </note>
Reference-contexts: We evaluate our prefetching algorithm relative to the best online algorithm that has complete knowledge of the structure and transition probabilities of the Markov source. Prefetching is a learning problem that involves predicting the page requests of the user. Work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is 3.1. PAGE REQUEST MODELS AND MAIN RESULTS 9 synonymous with generalization and data compression. If a data compressor expects a certain character to be next with a very high probability, it will assign that character a relatively small code. <p> As a result of the VC theory <ref> [BEHb, VaC] </ref>, it is easy to partition [0; M ] into v intervals satisfying the density criterion with high probability, by storing = fi (v ln v) examples, and calling a procedure generate cutoffs (w; ; ) on [0; M ]. <p> Then, for any * that is suitable, Pr (E 1 ) *=(2 (c + M )). To prove the above lemma, we use a technique due to Kearns and Schapire [KeS]. Lemma 8.2 below follows immediately from the results of Blumer et al. <ref> [BEHb] </ref> using the techniques of Vapnik and Chervonenkis [VaC]. Informally, Lemma 8.2 says that m points are enough to simultaneously estimate the probabilities of every interval. Lemma 8.2 Choose 0 &lt; ff; fi 1=2; c 1; and a probability distribution D on IR + .
Reference: [BoP] <author> R. Board and L. Pitt, </author> <title> "On the Necessity of Occam Algorithms," </title> <booktitle> Proceedings of the 22nd Annual ACM Symposium on Theory of Computation (May 1990), </booktitle> <pages> 54-63. 131 </pages>
Reference-contexts: We evaluate our prefetching algorithm relative to the best online algorithm that has complete knowledge of the structure and transition probabilities of the Markov source. Prefetching is a learning problem that involves predicting the page requests of the user. Work in computational learning theory <ref> [BEHa, BEHb, BoP] </ref> has shown that prediction is 3.1. PAGE REQUEST MODELS AND MAIN RESULTS 9 synonymous with generalization and data compression. If a data compressor expects a certain character to be next with a very high probability, it will assign that character a relatively small code.
Reference: [BIR] <author> A. Borodin, S. Irani, P. Raghavan, and B. Schieber, </author> <title> "Competitive Paging with Locality of Reference," </title> <booktitle> Proceedings of the 23rd Annual ACM Symposium on Theory of Computation (May 1991). </booktitle>
Reference-contexts: Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs.
Reference: [Bra] <author> J. T. Brady, </author> <title> "A theory of productivity in the creative process," </title> <journal> IEEE CG&A (May 1986), </journal> <pages> 25-34. </pages>
Reference-contexts: Current database systems perform prefetching using such sequential prefetching techniques derived from older virtual memory prefetchers. The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing <ref> [Bra] </ref>. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL].
Reference: [BuB] <author> S. Bunton and G. Borriello, </author> <title> "Practical Dictionary Management for Hardware Data Compression," </title> <institution> Department of Computer Science, University of Washington, FR-35, </institution> <year> 1991. </year>
Reference-contexts: There are also more sophisticated techniques that use an LRU-type strategy on the data structure to maintain its size <ref> [BuB] </ref>. Our ongoing work studies these techniques 46 CHAPTER 5. <p> In the context of data compression, least recently used-based strategies for deciding which nodes to prune <ref> [BuB] </ref>. We expect these strategies to be less suited to our environment, since we care more about the count at the nodes.
Reference: [CFK] <author> P. Cao, E. W. Felten, A. Karlin, and K. Li, </author> <title> "A Study of Integrated Prefetching and Caching Strategies," </title> <booktitle> Proceedings of the 1995 SIGMETRICS Conference, to appear. </booktitle>
Reference-contexts: Prefetching has also been studied in other systems environments. Prefetching in a parallel environment is studied in [KoE]. More recently, the possibility of predictive prefetching in mobile computing has been studied by Kuenning et al. [KPR]. Cao et al. study integrated prefetching and caching strategies <ref> [CFK] </ref> in file systems assuming full knowledge of future requests; the concern in this case is to decide when to prefetch so as to minimize the total time consumed to satisfy a page request sequence. 2.2 Our Approach In Chapter 3, we explore the idea of using data compression techniques for
Reference: [CDN] <author> M. J. Carey, D. J. DeWitt, and J. F. Naughton, </author> <title> "The DEC OO7 Benchmark," </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data (May 1993). </booktitle>
Reference-contexts: Any specific knowledge about the sequence of page requests can be utilized to improve the performance further using the techniques of [FKL].) We simulate our prefetchers on page request sequences derived from the Object Operations (OO1) benchmark [CaS], the OO7 benchmark <ref> [CDN] </ref>, and from CAD applications used at Digital Equipment Corporation (DEC). <p> EXPERIMENTAL RESULTS 49 5.5 Experimental Results This section presents the results of simulating our prefetcher on request traces generated by a CAD application, the Object Operations Benchmark (OO1), and the OO7 benchmark written at the University of Wisconsin <ref> [CDN] </ref>. We first describe the request traces and then present our results. <p> The more interesting phases include traversal of the structure in both the forward and reverse directions. The OO1 benchmark tests aspects of a DBMS that are critical in computer-aided software engineering (CASE) and computer-aided design (CAD) applications [CaS]. The OO7 benchmark, developed at the University of Wisconsin <ref> [CDN] </ref>, tests critical aspects of object-oriented database systems not covered by other benchmarks. This suite of tests was also run on the DEC Object/DB product used for the OO1 tests. <p> The benchmark performs traversals, associative queries, insert/delete operations, and multiuser tests <ref> [CDN] </ref>. We tested our prefetcher running with traces from the traversal query portion of the benchmark. 5.5.2 Prefetch Results for Uniform Prefetching 4 The simulation method for uniform prefetching was described in Section 5.4.2.
Reference: [CaS] <author> R. G. G. Cattell and J. Skeen, </author> <title> "Object Operations Benchmark," </title> <journal> ACM Transactions on Database Systems 17 (March 1992), </journal> <pages> 1-31. </pages>
Reference-contexts: The usefulness of universality is extremely significant in current databases [Sal]. Any specific knowledge about the sequence of page requests can be utilized to improve the performance further using the techniques of [FKL].) We simulate our prefetchers on page request sequences derived from the Object Operations (OO1) benchmark <ref> [CaS] </ref>, the OO7 benchmark [CDN], and from CAD applications used at Digital Equipment Corporation (DEC). <p> The more interesting phases include traversal of the structure in both the forward and reverse directions. The OO1 benchmark tests aspects of a DBMS that are critical in computer-aided software engineering (CASE) and computer-aided design (CAD) applications <ref> [CaS] </ref>. The OO7 benchmark, developed at the University of Wisconsin [CDN], tests critical aspects of object-oriented database systems not covered by other benchmarks. This suite of tests was also run on the DEC Object/DB product used for the OO1 tests.
Reference: [ChB] <author> T. F. Chen and J. L. Baer, </author> <title> "Reducing Memory Latency via Non-blocking and Prefetching Caches," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference-contexts: The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Our work [CKV, KrV, ViK] described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> Hardware schemes of non-blocking and prefetching caches that let processing continue when a cache miss occurs, blocking only when the missed data is actually needed have also been proposed <ref> [ChB] </ref>. In [RoL], a combined hardware and software approach has been studied where an optimizing compiler and speculative loads are used to issue read requests in anticipation of a demand request.
Reference: [Con] <author> Connectix Corporation, </author> <title> "CPU Connectix PowerBook Utilities Version 2.0 Addendum," </title> <address> San Mateo, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Some commercial products have recently begun to address the disk spindown issue. For example, while the standard Apple Powerbook control panel only allows the user to choose broadly between "maximum conservation" and "maximum performance," the Connectix Powerbook Utilities (CPU) <ref> [Con] </ref> provide fine-grained control over such details as the disk spindown threshold and processor speed, as well as feedback on the current state of the disk (such as a count-down to when the disk will spin down). 6.3 Our Approach In Chapter 7, we study fixed threshold algorithms for the disk
Reference: [Cor] <author> Maxtor Corporation, "The MXL-105-III," </author> <year> 1993. </year>
Reference-contexts: These disks have small size, high density, high shock tolerance, and multiple energy states; in a sleep or spundown state, the disks consume comparatively no energy, but need to be spunup before data can be accessed <ref> [Cor, Pac, Qua] </ref>. The spinup takes on the order of seconds, as opposed to accessing data from a spinning disk which takes on the order of milliseconds.
Reference: [Cov] <author> T. M. </author> <title> Cover, "Behavior of Predictors of Binary Sequences," </title> <booktitle> Proceedings of the 4th Prague Conference on Information Theory, Statistical Decision Functions, Random Processes (1967), </booktitle> <pages> 263-272. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal <ref> [Cov] </ref>. <p> This extends our results from Chapter 3 where we considered the sequence of pages requested as being generated by a general Markov source. It has been shown in <ref> [Cov] </ref> that any optimal algorithm for the binary alphabet case has to be necessarily randomized. By the way our algorithm is designed, we need spend at most constant expected time in making the random choices for each prediction, which is optimal.
Reference: [CoS] <author> T. M. Cover and A. Shenhar, </author> <title> "Compound Bayes Predictors with Apparent Markov Structure," </title> <journal> IEEE Transactions on System Man and Cybernetics SMC-7 (June 1977), </journal> <pages> 421-424. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal [Cov]. In information theory and in statistics <ref> [Bla, CoS, FMG, Han] </ref> interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed, but the ff = 2, k = 1 case is clearly unsuitable for our <p> In Section 4.2, we present our core prefetcher algorithm P 1 , which makes use of sampling without replacement, and we analyze it in Section 4.3 by comparing it against the best one-state prefetcher. In Section 4.4 we draw on ideas from information theory [CoT] applied to predicting <ref> [CoS, FMG, MeF] </ref> and generalize P 1 to get a universal prefetcher P that is optimal in the limit against a general finite state prefetcher.
Reference: [CoT] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In Section 4.2, we present our core prefetcher algorithm P 1 , which makes use of sampling without replacement, and we analyze it in Section 4.3 by comparing it against the best one-state prefetcher. In Section 4.4 we draw on ideas from information theory <ref> [CoT] </ref> applied to predicting [CoS, FMG, MeF] and generalize P 1 to get a universal prefetcher P that is optimal in the limit against a general finite state prefetcher. <p> We note that even for the ff = 2; k = 1 case, the convergence rate cannot be faster than O (1= p The importance of the above theorem lies in its generalization to higher order using techniques from information theory <ref> [CoT] </ref>. The approach of [FMG] allows us to combine P 1 with a prefetcher [ViK] based on the Ziv-Lempel data compressor [HoV, Lana, ZiLb] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers.
Reference: [CKV] <author> K. Curewitz, P. Krishnan, and J. S. Vitter, </author> <title> "Practical Prefetching via Data Compression," </title> <booktitle> Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data (May 1993), </booktitle> <pages> 257-266. </pages>
Reference-contexts: The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL]. Our work <ref> [CKV, KrV, ViK] </ref> described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> (when the data structure is allowed to grow unbounded) is given in 2 The traces were provided as part of the DEC-ERP grant 1139. 3 DEC Object/DB is a trademark of Digital Equipment Corporation, Maynard MA. 4 The results reported here use slightly different parameters from the ones reported in <ref> [CKV] </ref>. 50 CHAPTER 5. <p> One implementation issue we have not discussed in detail in this chapter is how to optimize for the space used by each node of the data structure. There are two promising heuristics that we experimented with prior to <ref> [CKV] </ref> which reduce the number of bits required to store the count associated with a node (or transition): the regular halving and the move-to-front heuristics. <p> The maximum count can be different for different levels of the tree. (Although not explicitly stated, some of the results reported in <ref> [CKV] </ref> used the regular halving heuristic; without the regular halving heursitic, the fault rates were slightly lesser than what was reported in [CKV].) In the move-to-front heuristic, instead of prefetching the pages with the maximum k counts out of the current node, we prefetch the k pages that were accessed most <p> The maximum count can be different for different levels of the tree. (Although not explicitly stated, some of the results reported in <ref> [CKV] </ref> used the regular halving heuristic; without the regular halving heursitic, the fault rates were slightly lesser than what was reported in [CKV].) In the move-to-front heuristic, instead of prefetching the pages with the maximum k counts out of the current node, we prefetch the k pages that were accessed most recently while at the node. (Notice that the move-to-front heuristic when prefetching one page between any two user page requests is related
Reference: [Dat] <author> C. Date, </author> <title> An Introduction to Database Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: SELECTIVITY 3 1.3 Selectivity The question of predicting how much is going to happen in the future is well illustrated by the problem of predicting selectivity in relational databases. Query optimization is an important part of database management systems <ref> [Dat, Ull] </ref>. The size of databases is growing rapidly and decisions support is an important component of databases. A typical query to a database is to return all records of the database satisfying predicate P . To answer such queries, the database performs query optimization to develop a search strategy. <p> Samuelson (1966) Query optimization is an integral part of relational databases <ref> [Dat, SAC, Ull] </ref>. For example, consider the selection predicate salary between 30K and 60K (10.1) of a payroll database, which requests records of all employees whose salary lies between 30K and 60K.
Reference: [Del] <institution> Dell Computer Corporation, </institution> <note> "Dell System 320SLi User's Guide," </note> <month> June </month> <year> 1992. </year>
Reference-contexts: The Hewlett-Packard Kittyhawk C3014A takes about three seconds to spindown and spin back up; its manufacturer recommends spinning it down after about five seconds of inactivity [Hewb]; most other disks take several seconds for spindown/spinup and are recommended to spindown only after a period of minutes <ref> [Del, Zen] </ref>. The main intuition underlying the variations among disk spindown policies is identifying periods of inactivity at disk that are "sufficiently large." Fixed threshold policies wait a fixed period of time to be sure that the period of inactivity is large enough.
Reference: [Den] <author> P. J. Denning, </author> <title> "Working Sets Past and Present," </title> <journal> IEEE Transactions on Software Engg., </journal> <month> SE-6 </month> <year> (1980), </year> <pages> 64-84. </pages>
Reference-contexts: Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs.
Reference: [DKB] <author> F. Douglis, P. Krishnan, and B. Bershad, </author> <title> "Adaptive Disk Spindown Policies for Mobile Computers," </title> <booktitle> Proceedings of the Second USENIX Symposium on Mobile and Location Independent Computing. </booktitle>
Reference-contexts: Adaptive spindown policies that continually change the spindown threshold based on the perceived inconvenience to the user are studied in <ref> [DKB] </ref>. Golding et al. [GBS] have studied idle-time detection and prediction, and proposed a taxonomy of idle-time detection 6.3. OUR APPROACH 59 algorithms; they report the effect of their different methods in the context of the TickerTAIP simulation system [RuWb]. <p> We observe that by increasing the value of one parameter a (equivalent to varying the value of the buy cost c), we can effectively trade power for response time. Concerns on how to effectively trade power for response time have been raised for the disk spindown problem <ref> [DKB, DKM] </ref>, and the rent-to-buy model provides an elegant way of achieving this tradeoff. 9.3.5 Other Observations Some other observations from our simulations are as follows: 1.
Reference: [DKM] <author> F. Douglis, P. Krishnan, and B. Marsh, </author> <title> "Thwarting the Power Hungry Disk," </title> <booktitle> Proceedings of the 1994 Winter USENIX Conference (January 1994). </booktitle> <volume> 132 CHAPTER 12. </volume> <month> CONCLUSIONS </month>
Reference-contexts: Current techniques for conserving power are based on shutting down components of the system after reasonably long periods of inactivity. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Most disks used for portable computers (e.g., the small, light-weight Kitty-hawk from Hewlett Packard [Pac]) have multiple energy states. <p> Energy conservation is critical in these mobile systems. This has motivated both hardware and software approaches for reducing power consumption in these systems. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Disk manufacturers are developing special types of drives especially designed for the portable market. <p> DISK SPINDOWN AND RENT-TO-BUY: THE WHEN the next access at disk is going to happen, we can make an optimal spindown decision. Alternatively, if we can learn a distribution of inter-arrival times at disk by observing past inter-arrival times, we can adaptively vary our spindown threshold. Our work <ref> [DKM, KLV] </ref> described in Chapters 7-9 analyzes different methods for disk spindown: simple fixed threshold policies, predictive techniques, and adaptive methods derived from modeling the disk spindown problem by the rent-to-buy framework. 6.1 The Rent-to-Buy Framework The disk spindown problem can be well-modeled by the the rent-to-buy framework. <p> As described in <ref> [DKM] </ref>, disk densities are increasing, making it possible to carry more data. Even though disk densities have increased, the power used by the largest disks has stayed about the same, around 1 Watt for an idle spinning disk. <p> Proper disk management can improve battery life. If the disk drive is used with any frequency, it will have a significant impact on the length of time the computer can operate on a single battery charge. Measurements of power consumed by notebook computers as described in <ref> [DKM] </ref> suggest that battery life for the Dell 320 could be improved 20-31%, the amount that would be saved if the disk were off all the time. Put another way, a battery that lasts 5 hours could last from 6 to 6.5 hours instead. <p> Most notably, the HP-UX trace is over a prolonged period of time (about 2 months), with about the same number of accesses as the Powerbook trace had over four hours spread out instead over a week's time. (The simulations in <ref> [DKM] </ref> were done using only the first week's worth of data; our results in this chapter use the whole trace. <p> We observe that by increasing the value of one parameter a (equivalent to varying the value of the buy cost c), we can effectively trade power for response time. Concerns on how to effectively trade power for response time have been raised for the disk spindown problem <ref> [DKB, DKM] </ref>, and the rent-to-buy model provides an elegant way of achieving this tradeoff. 9.3.5 Other Observations Some other observations from our simulations are as follows: 1. <p> For measuring response time performance, we used the metric of the number of operations delayed. An alternative measure of response time performance is R X , the number of read operations delayed by a spinup for algorithm X <ref> [DKM] </ref>. This metric redefines the effective cost from (9.1) as E + a R X . The rent-to-buy model can be easily modified to evaluate this measure, by having different costs for a spindown (i.e., different c's) depending on whether the operation is a read or a write.
Reference: [FMG] <author> M. Feder, N. Merhav, and M. Gutman, </author> <title> "Universal Prediction of Individual Sequences," </title> <journal> IEEE Transactions on Information Theory IT-38 (July 1992), </journal> <pages> 1258-1270. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal [Cov]. In information theory and in statistics <ref> [Bla, CoS, FMG, Han] </ref> interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed, but the ff = 2, k = 1 case is clearly unsuitable for our <p> In Section 4.2, we present our core prefetcher algorithm P 1 , which makes use of sampling without replacement, and we analyze it in Section 4.3 by comparing it against the best one-state prefetcher. In Section 4.4 we draw on ideas from information theory [CoT] applied to predicting <ref> [CoS, FMG, MeF] </ref> and generalize P 1 to get a universal prefetcher P that is optimal in the limit against a general finite state prefetcher. <p> We note that even for the ff = 2; k = 1 case, the convergence rate cannot be faster than O (1= p The importance of the above theorem lies in its generalization to higher order using techniques from information theory [CoT]. The approach of <ref> [FMG] </ref> allows us to combine P 1 with a prefetcher [ViK] based on the Ziv-Lempel data compressor [HoV, Lana, ZiLb] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers. <p> ONE-STATE CASE: OPTIMALITY OF P 1 VS. F (1) 29 4.3.1 The Approximately Balanced Sequence is Sufficiently Worst-Case In this subsection we prove Theorem 4.4 using an interesting extension of the switch analysis of <ref> [FMG] </ref> in conjunction with the important notion of boosted frequency counts (4.2). We denote the jth r-subsequence j by 1 , where = 4 j 4 j1 is the length of j . The sequence 1 can be converted to a balanced form b 1 by an iterative balancing strategy. <p> Our proof of Theorem 4.4 consists in showing that each switch in the balancing strategy does not lower the page fault rate of the entire sequence. A similar but simpler idea worked in the binary case for a different algorithm <ref> [FMG] </ref>, in which the sequence did not need to be broken up into subsequences, and the sequence b n 1 could be shown to be strictly worst-case. We break n 1 into subsequences as part of our method for achieving optimal computational efficiency (as discussed in Section 4.5). <p> An example snapshot of the data structure of P is given in Figure 4.1b. We now briefly explain why P is optimal against an arbitrary s-state machine (Theorem 4.2), using the interesting approach of <ref> [FMG] </ref>. An mth-order Markov prefetcher predicts its k choices for the next page based solely on the previous m page requests of the sequence. The prefetcher P can be looked upon as a Markov prefetcher of growing order. <p> Hence it achieves the fault rate of the best s-state prefetcher for any s. Given that P 1 is optimal against F (1) (Theorem 4.1), to show optimality of P against F (s) by the approach described above, we need to extend some results of <ref> [FMG] </ref> to hold for the prefetching problem. These extensions are simple as described below in Section 4.4.1. <p> OPTIMAL PREFETCHING IN THE WORST CASE 4.4.1 Proof of Theorem 4.2 In this section we give the required extensions to the results of <ref> [FMG] </ref> to prove optimality of prefetcher P . Definition 4.6 Given an ff-probability vector ~p = (p 0 ; : : : ; p ff1 ), we denote by min ffk (~p) the sum of the minimum ff k elements of ~p. <p> We denote the fault rate of an m-th order prefetcher by Fault M (ff m ) ( n 1 ), where M (ff m ) F (ff m ). To verify Theorem 4.2, we need to show that Lemma 1, Theorem 2, and Theorem 4 from <ref> [FMG] </ref> hold for prefetching. We make the following observations: 1. <p> To verify Theorem 4.2, we need to show that Lemma 1, Theorem 2, and Theorem 4 from [FMG] hold for prefetching. We make the following observations: 1. In <ref> [FMG, Lemma 1, Theorems 2, 4 ] </ref>, uniformly replace minfp 0 ; p 1 g, where p 0 and p 1 are the probabilities of a 0 and a 1, by min ffk (~p), where ~p is the corresponding ff-probability vector for prefetching. <p> Also, replace summations over f0; 1g by summa tions over the alphabet A. 2. Lemmas 4.4 and 3.6 replace the utility of [FMG,(B.1)]in the proof of <ref> [FMG, Lemma 1] </ref>. It can be verified now that [FMG,Theorem 2 ] holds for the prefetching problem; in particular, Fault M (ff m ) ( n 1 ) Fault F (s) ( n 1 ) + O log s ! 3. <p> Although P 1 (the online algorithm for prefetching described in Section 4.2) is very different from the algorithm for prediction of binary sequences in <ref> [FMG] </ref>, algorithm P has the properties used in the proof of [FMG, Theorem 4 ]. <p> Although P 1 (the online algorithm for prefetching described in Section 4.2) is very different from the algorithm for prediction of binary sequences in [FMG], algorithm P has the properties used in the proof of <ref> [FMG, Theorem 4 ] </ref>.
Reference: [FiG] <author> E. R. Fiala and D. H. Greene, </author> <title> "Data Compression with Finite Windows," </title> <booktitle> Communications of the ACM 32 (April 1989), </booktitle> <pages> 490-505. </pages>
Reference-contexts: We present a version of the suffix tree [McC] as an appropriate data structure for the selectivity problem, since it allows searches of arbitrary substrings of the original database items; suffix trees are used by Fiala and Greene <ref> [FiG] </ref> to implement Ziv-Lempel-based algorithms for data compression. 101 Chapter 11 Alphanumeric Selectivity in the Presence of Wildcards In this chapter, we describe our strategy to predict alphanumeric selectivity. Our algorithms must work under the following constraints required by practical applications [Iye]. <p> ALPHANUMERIC SELECTIVITY WITH WILDCARDS (a) Suffix tree for string "greener!" (b) Adding a string to a suffix tree The value of ff as described in Example 11.1 is "e". 11.1 The Suffix Tree A suffix tree [McC] is a trie-based data structure [Knua, Mor] used for data compression <ref> [FiG] </ref>, pattern matching [Wei], and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC]. <p> words, while building the suffix tree for string , no string we add to the tree can be a prefix of another string added to the tree. 2 Various optimizations with respect to time and space to the basic suffix tree construction algorithm we have just described have been studied <ref> [FiG, McC] </ref>.
Reference: [FKL] <author> A. Fiat, R. M. Karp, M. Luby, L. A. McGeoch, D. D. Sleator, and N. E. Young, </author> <title> "On Competitive Algorithms for Paging Problems," </title> <booktitle> Journal of Algorithms 12 (1991), </booktitle> <pages> 685-699. </pages>
Reference-contexts: Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> In practice, when such knowledge is available [PGS], we could combine 3.5. DISCUSSION 23 our prefetcher with a logical prefetcher based on the semantics of the application, so as to get the best of both worlds. Similar techniques for cache replacement appear in <ref> [FKL] </ref>. The framework of Abe and Warmuth [AbW], who investigated a quite different learning problem related to FSAs, suggests a static PAC-learning framework for prefetching, in which the prefetcher is trained on several independently generated sequences of a particular length. <p> The usefulness of universality is extremely significant in current databases [Sal]. Any specific knowledge about the sequence of page requests can be utilized to improve the performance further using the techniques of <ref> [FKL] </ref>.) We simulate our prefetchers on page request sequences derived from the Object Operations (OO1) benchmark [CaS], the OO7 benchmark [CDN], and from CAD applications used at Digital Equipment Corporation (DEC).
Reference: [FlM] <author> P. Flajolet and G. N. Martin, </author> <title> "Probabilistic Counting Algorithms for Database Applications," </title> <journal> Journal of Computer and Systems Sciences 31 (1985), </journal> <pages> 182-209. </pages>
Reference-contexts: Query optimizers have cost models that estimate the access cost as a function of the predicted number of qualifying rows and find the cheaper alternative. Models already exist in current day relational database management systems (RDBMSs) to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate [Iye]. <p> In Chapter 11 we present our techniques for predicting selectivity for the like predicate; i.e., techniques for estimating alphanumeric selectivity. 10.1 Background and Related Work Models already exist in current day RDBMSs to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. Typically, in the preprocessing phase, a few numbers that "capture" the distribution of data are accumulated and stored in the catalog. <p> A well-studied issue related to predicting selectivity is estimating the number of unique values in a column of the table. In <ref> [ASW, FlM, WVT] </ref>, interesting linear time algorithms for finding the number of unique values in a column based on probabilistic and approximate counting methods have been described.
Reference: [Gal] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: We denote the cache size by k and the total number of different pages (or alphabet size) by ff. The logarithm to the base two is denoted by "lg," the natural logarithm by "ln," and the empty string by . Definition 3.1 <ref> [Gal] </ref> We define a probabilistic finite state automaton (probabilistic FSA) as a quintuple (S; A; g; p; z 0 ), where S is a finite set of states with jSj = s, A is a finite alphabet of size ff, g is a deterministic "next state" function that maps S fi <p> Definition 3.2 <ref> [Gal] </ref> Let M be a Markov source. The minimum average encoding length per character of M for input sequences of length n is given by H M (n) = n z2S `=0 ! i 1 ! 3.3. <p> Since we throw away our data structures at the end of each block, each of the b random variables Fault P;n , for 1 i b, depends only on the start state for each block, and our result follows by the ergodic theorem <ref> [Gal] </ref>. 2 The proof of Theorem 3.2 essentially deals with showing that F converges to F M for almost all as n ! 1.
Reference: [GBS] <author> R. Golding, P. Bosch, C. Staelin, T. Sullivan, and J. Wilkes, </author> <title> "Idleness is not Sloth," </title> <booktitle> Proceedings of the USENIX 1995 Technical Conference on UNIX and Advanced Computing Systems (January, </booktitle> <year> 1995), </year> <pages> 201-212. </pages>
Reference-contexts: Adaptive spindown policies that continually change the spindown threshold based on the perceived inconvenience to the user are studied in [DKB]. Golding et al. <ref> [GBS] </ref> have studied idle-time detection and prediction, and proposed a taxonomy of idle-time detection 6.3. OUR APPROACH 59 algorithms; they report the effect of their different methods in the context of the TickerTAIP simulation system [RuWb].
Reference: [GKP] <author> R. L. Graham, D. E. Knuth, and O. Patashnik, </author> <title> Concrete Mathematics, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: F j 1 ( i ) equals 1 ffi1 X j i j=(ffi) X minfk;ffig X k 1 =1 where *(u; i; k 1 ) = (1 ffi 1 (u; i) ffi 2 (u; i)) k 1 1: The following facts can be verified by using the asymptotic techniques from <ref> [GKP, Chap ter 9] </ref>: 2. ffi 2 (u; i) (i=(ff i)) fi exp (ru=2L i ) if u L i ; and ffi 2 (u; i) (i=(ff i)) fi 2 r if 34 CHAPTER 4.
Reference: [GrR] <author> J. Gray and A. Reuter, </author> <title> Transaction Processing: Concepts and Techniques, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The server has the ability to handle demand read requests from the application and prefetch read requests from the prefetcher. The server gives priority to the client's requests, flushing prefetch requests in its queue when a demand request arrives. Such provisions are generally available in prefetching systems <ref> [GrR, PaZa] </ref>. The prefetcher can be either part of the application or a separate entity distinct from the application. It works by processing the sequence of the client's previous page requests and making requests for data from the server.
Reference: [Gre] <author> P. Greenawalt, </author> <title> "Modeling Power Management for Hard Disks," </title> <booktitle> Proceedings of the Symposium on Modeling and Simulation of Computer Telecommunication Systems (1994). </booktitle>
Reference-contexts: Golding et al. [GBS] have studied idle-time detection and prediction, and proposed a taxonomy of idle-time detection 6.3. OUR APPROACH 59 algorithms; they report the effect of their different methods in the context of the TickerTAIP simulation system [RuWb]. Greenawalt <ref> [Gre] </ref> did an analytical study of disk management strategies, assuming a Poisson process for request arrival. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. [KMM].
Reference: [HMM] <author> T. Hagerup, K. Mehlhorn, and I. Munro, </author> <title> "Optimal Algorithms for Generating Discrete Random Variables with Changing Distributions," Max Planck Institute, </title> <type> Technical Report MPI-I-92-145, </type> <month> October 15, </month> <year> 1992. </year>
Reference-contexts: implement the prefetcher in constant expected time per prefetched page, independent of alphabet size ff and cache size k, by use of the newly developed optimal dynamic algorithm for generating discrete random variates of Matias, Vitter, and Ni [MVN], which uses a table lookup method of Hagerup, Mehlhorn, and Munro <ref> [HMM] </ref>. In Section 4.6, we look at an interesting proof technique that shows the optimality of predictor P 0 1 , an algorithm closely related to P 1 , for general ff, when k = 1. <p> of generating a random variate with a value in the range f0, 1, 2, : : : , ff 1g according to a set of ff dynamically changing weights is solved optimally by Matias, Vitter, and Ni [MVN, Section 5] using the table lookup procedure of Hagerup, Mehlhorn, and Munro <ref> [HMM] </ref>. The idea at an intuitive level is to group the weights into ranges according to their values. Range j stores weights with value in the range [2 j ; 2 j+1 ). Each range is said to have a weight equal to the sum of the weights it contains. <p> After two recursive levels, the problem reduces to generating one of O (log log ff) weights, each in the range [1; log ff], which can be done dynamically in constant time by the clever table lookup method of <ref> [HMM] </ref>.
Reference: [Han] <author> J. F. Hannan, </author> <title> "Approximation to Bayes Risk in Repeated Plays," Contributions to the Theory of Games, </title> <booktitle> Vol 3, Annals of Mathematical Studies (1957), </booktitle> <pages> 97-139. </pages>
Reference-contexts: Randomness is required in order for a predictor or prefetcher to be optimal [Cov]. In information theory and in statistics <ref> [Bla, CoS, FMG, Han] </ref> interesting algorithms for binary sequences (corresponding to an alphabet size of ff = 2 pages) that make one prediction for the next page (corresponding to cache size k = 1) have been developed, but the ff = 2, k = 1 case is clearly unsuitable for our <p> The procedure in <ref> [Han] </ref> may be generalizable to the arbitrary alphabet case ff 2 for cache 4.1. ANALYSIS MODEL AND MAIN RESULTS 25 size k = 1, but it cannot possibly make a prediction in constant time independent of ff, and the k &gt; 1 case is open.
Reference: [Hewa] <author> Hewlett Packard, </author> <title> "Omnibook 300 Operating Guide, </title> <booktitle> 1st edition," </booktitle> <address> Corvallis, OR, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: With the exception of the smallest and lightest computers, such as the Hewlett-Packard Omnibook <ref> [Hewa] </ref>, the trend seems to be to carry a larger disk with the same mass rather than a smaller disk with the same number of bytes. Proper disk management can improve battery life.
Reference: [Hewb] <institution> Hewlett Packard, "Kittyhawk Power Management Modes," </institution> <month> April </month> <year> 1993, </year> <title> Internal Document. </title>
Reference-contexts: The fixed threshold is typically on the order of many seconds or minutes to minimize the delay from on-demand disk spinups. The Hewlett-Packard Kittyhawk C3014A takes about three seconds to spindown and spin back up; its manufacturer recommends spinning it down after about five seconds of inactivity <ref> [Hewb] </ref>; most other disks take several seconds for spindown/spinup and are recommended to spindown only after a period of minutes [Del, Zen].
Reference: [HoV] <author> P. G. Howard and J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763, </pages> <note> invited paper in Special Issue on Data Compression for Images and Texts. </note>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiLb] hold without change for the character-based approach. Example 3.1 Assume for simplicity that our alphabet is fa; bg. We consider the page request sequence "aaaababaabbbabaa: : : ". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa): : : ". <p> To 14 CHAPTER 3. OPTIMAL PREFETCHING VIA DATA COMPRESSION identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 3.1 being used as a model by an arithmetic coder. <p> The resulting optimal prefetcher P is a blend of P 1 and the prefetcher P from Chapter 3 based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> (on which the UNIX compress program is based). <p> The approach of [FMG] allows us to combine P 1 with a prefetcher [ViK] based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers.
Reference: [IKP] <author> S. Irani, A. R. Karlin, and S. Phillips, </author> <title> "Strongly Competitive Algorithms for Paging with Locality of Reference," </title> <booktitle> Proceedings of the 3rd Annual ACM-SIAM Symposium of Discrete Algorithms (January 1992). </booktitle>
Reference-contexts: Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs.
Reference: [Iye] <author> B. Iyer, </author> <title> IBM, Santa Teresa, </title> <type> personal communication. </type>
Reference-contexts: Finding a good search strategy depends on the query optimizer's ability to predict or estimate the percentage of rows that satisfy the predicate. A particularly difficult prediction scenario involves estimating non-numeric selectivity in relational databases in the presence of wildcards <ref> [Iye] </ref>. The problem here is to preprocess the relational database (during off-hours, e.g., during the weekend) and come up with a compact structure that will allow online estimation of non-numeric queries involving wildcards. <p> Query optimizers have cost models that estimate the access cost as a function of the predicted number of qualifying rows and find the cheaper alternative. Models already exist in current day relational database management systems (RDBMSs) to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate [Iye]. <p> With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate <ref> [Iye] </ref>. For example, consider the inventory of a department store that has a parts table, one of whose columns, part.color , is the color of the part. <p> In Chapter 11 we present our techniques for predicting selectivity for the like predicate; i.e., techniques for estimating alphanumeric selectivity. 10.1 Background and Related Work Models already exist in current day RDBMSs to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. Typically, in the preprocessing phase, a few numbers that "capture" the distribution of data are accumulated and stored in the catalog. <p> To the best of our knowledge, the problem of predicting alphanumeric selectivity in the presence of wildcards has not been studied, although it is turning out to be a pressing concern <ref> [Iye] </ref>. The techniques used for estimating numeric selectivity are unsuitable for estimating alphanumeric selectivity, since the techniques for predicting numeric selectivity intimately use the fact that they are dealing with numbers. <p> ALPHANUMERIC SELECTIVITY: THE HOW MUCH 10.2 Our Approach In Chapter 11, we present our strategies for estimating selectivity of alphanumeric fields with the like predicate. There are stringent limits on the memory and processing allowable to our predictor <ref> [Iye] </ref>; in other words, the size of the catalog must be small and there must be minimal processing in the online phase. <p> Our algorithms must work under the following constraints required by practical applications <ref> [Iye] </ref>. We can make one preprocessing pass through the database. Information collected during this pass must be summarized and stored in a small catalog. <p> The space allowed for a catalog for any one column of the database depends on the complexity of the column; for our domain of alphanumeric selectivity, the size of the catalog is expected to be on the order of 0.5-1 Kbyte <ref> [Iye] </ref>. To create the catalog, we can use (reasonably) large amounts of memory and compute power; the preprocessing phase will be performed when the database is not being used much, e.g., during the weekend. <p> We first concentrate on trying to predict selectivity for unit patterns, i.e., patterns where a string is sandwiched between two wildcards (e.g., the pattern "flgreenfl" in the like predicate from (10.2)). Unit patterns are the most common type of patterns used with the like predicate <ref> [Iye] </ref>. We describe strategies to deal with general patterns in Section 11.8. In Section 11.1, we present the suffix tree data structure, which lies at the heart of our prediction strategy. In Section 11.2, we describe our strategy in the preprocessing phase. <p> We derive a catalog from the suffix tree based on the size c that we are allowed for storing the catalog. (The size c of the catalog for the specific type of columns and queries we are dealing with is expected to be on the order of 0.5-1 Kbyte <ref> [Iye] </ref>.) We do this by 11.2. THE PREPROCESSING PHASE 107 in the catalog. <p> These patterns also exist in the column, and we call this the positive double pattern study. Most patterns used with the like predicate are positive patterns, i.e., patterns that are present in the database <ref> [Iye] </ref>; hence, the above two studies are most important. Although negative patterns, i.e., patterns not found in the database, are seldom seen in query patterns [Iye], it is important to understand the effect of our matching strategies via-a-vis negative patterns. <p> Most patterns used with the like predicate are positive patterns, i.e., patterns that are present in the database <ref> [Iye] </ref>; hence, the above two studies are most important. Although negative patterns, i.e., patterns not found in the database, are seldom seen in query patterns [Iye], it is important to understand the effect of our matching strategies via-a-vis negative patterns. We generated three sets of negative patterns by taking the 92 base patterns and introducing one, two, and three errors in each basic pattern, respectively.
Reference: [KLM] <author> A. R. Karlin, K. Li, M. S. Manasse, and S. </author> <title> Owicki, </title> <booktitle> "Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor ," Proceedings of the 1991 ACM Symposium on Operating System Principles (1991), </booktitle> <pages> 41-55. </pages>
Reference-contexts: The rent-to-buy framework is also useful in studying other related systems problems like the spin/block problem from multiprocessor applications <ref> [KLM] </ref> and deciding virtual circuit holding times in IP-over-ATM networks [SaK]. We elaborate on this in Chapter 8. 6.2 Background and Related Work The area of disk spindown and device management has attracted a lot of attention recently. <p> The rent-to-buy framework models not only the disk spindown problem, but other interesting systems problems as well, specifically, thread blocking decisions during lock acquisition in multiprocessor applications <ref> [KLM] </ref>, and virtual circuit holding times in IP-over-ATM networks [KLP, SaK]. <p> The spin/block problem, another interesting and important problem from multiprocessor applications, can be modeled by the rent-to-buy framework. The spin/block problem involves threads trying to acquire locks to protect access to shared data <ref> [KLM] </ref>. A round is defined by a thread requesting locked data and eventually acquiring the lock. <p> The spinning can thus be viewed as renting, and a block can be viewed as a buy. In this situation too, practical studies suggest that lock-waiting times can be assumed to obey some unknown but time-invariant probability distribution <ref> [KLM] </ref>. Deciding virtual circuit holding times in IP-over-ATM networks is another scenario modeled by the rent-to-buy framework [SaK].
Reference: [KMM] <author> A. R. Karlin, M. S. Manasse, L. A. McGeoch, and S. Owicki, </author> <title> "Competitive Randomized Algorithms for Non-Uniform Problems," </title> <booktitle> Proceedings of the 1st ACM-SIAM Symposium on Discrete Algorithms (1990), </booktitle> <pages> 301-309. 133 </pages>
Reference-contexts: It is easy to verify that an online algorithm that buys after waiting c units of time incurs a cost that is within a factor of two of the cost of the optimal o*ine algorithm; this is optimal in the worst case under the competitive model of analysis <ref> [KMM] </ref>. (Competitive analysis was defined in Chapter 3.) For a fixed c, this 2-competitive algorithm corresponds to a fixed threshold algorithm than spins down the disk after waiting c seconds. <p> Greenawalt [Gre] did an analytical study of disk management strategies, assuming a Poisson process for request arrival. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. <ref> [KMM] </ref>. Some commercial products have recently begun to address the disk spindown issue. <p> This competitive factor 1 of 2 is the best possible (for deterministic algorithms) in the worst case <ref> [KMM] </ref>. If we know of a probability distribution on the time the resource is needed, we can usually find a rent-to-buy strategy whose expected cost is substantially less than that of the online algorithm that waits c time units before buying. <p> In this chapter we develop online algorithms for the rent-to-buy problem in probabilistic environments, assuming that the resource use times are independently randomly drawn from a fixed but unknown probability distribution. The most straightforward solution to the problem <ref> [KMM] </ref> is to store all past resource use times, and use that cutoff b for the current round which would have had the lowest total cost had we used it in the past. <p> Markov models have been effectively used to analyze caching and prefetching algorithms, as discussed in Chapters 3 and 4, and in [KPR]. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. <ref> [KMM] </ref>. In particular, 2-competitive deterministic algorithms and e=(e1)-competitive randomized algorithms have been developed. In [KMM] it was claimed that there is an adaptive algorithm achieving a competitive ratio approaching e=(e 1) on input sequences generated according to any time invariant probability distribution. However, their technique as stated is computationally inefficient. <p> The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. <ref> [KMM] </ref>. In particular, 2-competitive deterministic algorithms and e=(e1)-competitive randomized algorithms have been developed. In [KMM] it was claimed that there is an adaptive algorithm achieving a competitive ratio approaching e=(e 1) on input sequences generated according to any time invariant probability distribution. However, their technique as stated is computationally inefficient.
Reference: [KPR] <author> A. R. Karlin, S. J. Phillips, and P. Raghavan, </author> <title> "Markov Paging," </title> <booktitle> Proceedings of the 33rd Annual IEEE Conference on Foundations of Computer Science (October 1992), </booktitle> <pages> 208-217. </pages>
Reference-contexts: Prefetching has also been studied in other systems environments. Prefetching in a parallel environment is studied in [KoE]. More recently, the possibility of predictive prefetching in mobile computing has been studied by Kuenning et al. <ref> [KPR] </ref>. <p> Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> Cache replacement has been studied by Karlin, Phillips, and Raghavan <ref> [KPR] </ref> under a different stochastic version of the competitive framework; the sequence of page requests is assumed to be generated by a Markov chain (a subset of Markov sources). A PAC learning framework incorporating Markov sources of examples is developed in [AlV]. <p> Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs. <p> It will also be interesting from a theoretical point of view to analyze the effect of these heuristics using an appropriate model of locality as in <ref> [KPR, SlT, ViK] </ref>. As described earlier, the approach from [PaZb] is based on training the predictor on a page request sequence, fixing the predictor, and using it for subsequent user sessions. <p> Can our strategy of using LRU with prefetching be shown to be optimal in some reasonable models? Otherwise, is there some other provably optimal cache replacement strategy that can be blended with prefetchers? We expect that recent work on caching models in <ref> [KPR] </ref> may be relevant. We observed that the first prefetch is almost always the most significant in terms of performance improvement. This suggests a simple theoretical model of uniform prefetching, where exactly one prefetch can be made between any two page requests. <p> It would be interesting to model the resource use times as being generated by a Hidden Markov Model (HMM). Markov models have been effectively used to analyze caching and prefetching algorithms, as discussed in Chapters 3 and 4, and in <ref> [KPR] </ref>. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. [KMM]. In particular, 2-competitive deterministic algorithms and e=(e1)-competitive randomized algorithms have been developed.
Reference: [KeS] <author> M. J. Kearns and R. E. Schapire, </author> <title> "Efficient Distribution-Free Learning of Probabilistic Concepts," </title> <booktitle> Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science (October 1990), </booktitle> <pages> 382-391. </pages>
Reference-contexts: A harder model is to assume that the prefetcher is trained on one sufficiently long sequence generated by a source. For certain special cases of sources, like mth order Markov sources, we expect that the optimal prefetcher is PAC-learnable. An interesting related model is that of probabilistic concepts <ref> [KeS] </ref>. <p> Then, for any * that is suitable, Pr (E 1 ) *=(2 (c + M )). To prove the above lemma, we use a technique due to Kearns and Schapire <ref> [KeS] </ref>. Lemma 8.2 below follows immediately from the results of Blumer et al. [BEHb] using the techniques of Vapnik and Chervonenkis [VaC]. Informally, Lemma 8.2 says that m points are enough to simultaneously estimate the probabilities of every interval.
Reference: [KLP] <author> S. Keshav, C. Lund, S. J. Phillips, N. Reingold, and H. Saran, </author> <title> "An Empirical Evaluation of Virtual Circuit Holding Time Policies in IP-over-ATM Networks," </title> <note> Proceedings of INFOCOM '95 , to appear. </note>
Reference-contexts: The rent-to-buy framework models not only the disk spindown problem, but other interesting systems problems as well, specifically, thread blocking decisions during lock acquisition in multiprocessor applications [KLM], and virtual circuit holding times in IP-over-ATM networks <ref> [KLP, SaK] </ref>. <p> The first LRU-based holding time policy they study is the 2-competitive algorithm described earlier in this chapter, and their second holding time policy involves estimating the mean inter-reference interval with exponential averaging. In <ref> [KLP] </ref>, Keshav et al. empirically study an adaptive policy for the virtual circuit problem that tries to estimate the distribution of inter-arrival times by keeping a histogram of observed inter-arrival times grouped into fixed size buckets.
Reference: [Knua] <author> D. Knuth, </author> <title> The Art of Computer Programming, Volume 3: Sorting and Searching, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: ALPHANUMERIC SELECTIVITY WITH WILDCARDS (a) Suffix tree for string "greener!" (b) Adding a string to a suffix tree The value of ff as described in Example 11.1 is "e". 11.1 The Suffix Tree A suffix tree [McC] is a trie-based data structure <ref> [Knua, Mor] </ref> used for data compression [FiG], pattern matching [Wei], and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC].
Reference: [Knub] <author> D. Knuth, </author> <booktitle> The Art of Computer Programming, Volume 2: Seminumerical Algorithms, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition 1981. </note>
Reference-contexts: With high probability, the individual weight chosen during the generation will be within the first O (log ff) ranges, so each successive group of O (log ff) ranges should be processed in a recursive data structure according to the weights of the ranges. The use of the rejection method <ref> [Knub] </ref> is used to adjust the probabilities of generation appropriately, since the weights in each bucket may vary by a factor of 2. <p> It suffices to determine if lg U r lg f i . This can be done in constant time by generating the exponentially distributed random variate lg U directly using finite-precision <ref> [Knub, page 128] </ref>. The expected number of bits needed before the acceptance or 38 CHAPTER 4. OPTIMAL PREFETCHING IN THE WORST CASE rejection is determined is a small constant, so finite precision suffices.
Reference: [KoE] <author> D. F. Kotz and C. S. Ellis, </author> <title> "Prefetching in File Systems for MIMD Multiprocessors," </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1 (April 1990), </journal> <pages> 218-230. </pages>
Reference-contexts: Our approach to prefetching as described in Section 2.2 is also based on monitoring past user behavior; we develop a systematic approach to prefetching based on data compression principles. Prefetching has also been studied in other systems environments. Prefetching in a parallel environment is studied in <ref> [KoE] </ref>. More recently, the possibility of predictive prefetching in mobile computing has been studied by Kuenning et al. [KPR].
Reference: [KLV] <author> P. Krishnan, P. M. Long, and J. S. Vitter, </author> <title> "Adaptive Disk Spindown via Optimal Rent-to-Buy in Probabilistic Environments," </title> <institution> Duke University Technical Report, CS-1995-08, </institution> <month> March, </month> <year> 1995, </year> <title> to appear in Machine Learning: </title> <booktitle> Proceedings of the Twelfth International Conference, </booktitle> <editor> Armand Prieditis and Stuart Russell, eds., </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Fransisco, CA, </address> <year> 1995. </year>
Reference-contexts: DISK SPINDOWN AND RENT-TO-BUY: THE WHEN the next access at disk is going to happen, we can make an optimal spindown decision. Alternatively, if we can learn a distribution of inter-arrival times at disk by observing past inter-arrival times, we can adaptively vary our spindown threshold. Our work <ref> [DKM, KLV] </ref> described in Chapters 7-9 analyzes different methods for disk spindown: simple fixed threshold policies, predictive techniques, and adaptive methods derived from modeling the disk spindown problem by the rent-to-buy framework. 6.1 The Rent-to-Buy Framework The disk spindown problem can be well-modeled by the the rent-to-buy framework.
Reference: [KrV] <author> P. Krishnan and J. S. Vitter, </author> <title> "Optimal Prediction for Prefetching in the Worst Case," </title> <note> Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (January 1994), Also appears as Duke University Technical Report CS-1993-26.. </note>
Reference-contexts: The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL]. Our work <ref> [CKV, KrV, ViK] </ref> described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2.
Reference: [KPR] <author> G. H. Kuenning, G. J. Popek, and P. L. Reiher, </author> <title> "An Analysis of Trace Data for Predictive File Caching in Mobile Computing," </title> <booktitle> Proceedings of the 1994 Summer USENIX </booktitle> . 
Reference-contexts: Prefetching has also been studied in other systems environments. Prefetching in a parallel environment is studied in [KoE]. More recently, the possibility of predictive prefetching in mobile computing has been studied by Kuenning et al. <ref> [KPR] </ref>. <p> Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> Cache replacement has been studied by Karlin, Phillips, and Raghavan <ref> [KPR] </ref> under a different stochastic version of the competitive framework; the sequence of page requests is assumed to be generated by a Markov chain (a subset of Markov sources). A PAC learning framework incorporating Markov sources of examples is developed in [AlV]. <p> Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs. <p> It will also be interesting from a theoretical point of view to analyze the effect of these heuristics using an appropriate model of locality as in <ref> [KPR, SlT, ViK] </ref>. As described earlier, the approach from [PaZb] is based on training the predictor on a page request sequence, fixing the predictor, and using it for subsequent user sessions. <p> Can our strategy of using LRU with prefetching be shown to be optimal in some reasonable models? Otherwise, is there some other provably optimal cache replacement strategy that can be blended with prefetchers? We expect that recent work on caching models in <ref> [KPR] </ref> may be relevant. We observed that the first prefetch is almost always the most significant in terms of performance improvement. This suggests a simple theoretical model of uniform prefetching, where exactly one prefetch can be made between any two page requests. <p> It would be interesting to model the resource use times as being generated by a Hidden Markov Model (HMM). Markov models have been effectively used to analyze caching and prefetching algorithms, as discussed in Chapters 3 and 4, and in <ref> [KPR] </ref>. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. [KMM]. In particular, 2-competitive deterministic algorithms and e=(e1)-competitive randomized algorithms have been developed.
Reference: [Lai] <author> P. Laird, </author> <title> "Discrete Sequence Prediction and its Applications," </title> <institution> AI Research Branch, NASA Ames Research Center, </institution> <type> manuscript, </type> <year> 1992. </year>
Reference-contexts: The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Our work [CKV, KrV, ViK] described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> In [RoL], a combined hardware and software approach has been studied where an optimizing compiler and speculative loads are used to issue read requests in anticipation of a demand request. Interesting approaches have been proposed for prefetching based on predicting future page requests by monitoring past user behavior <ref> [Lai, PaZb, Sal] </ref>. Palmer and Zdonik [PaZb] use a pattern matching approach to prediction. Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. <p> Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. Salem computes various first-order statistics for prediction [Sal], and Laird uses a growing Markov predictor <ref> [Lai] </ref>. Song et al. study prefetching based on page fault history [SoC]. Alexander et al. consider hardware prefetching schemes between processor and main memory by extracting tables of fixed length contexts from past user requests [AlK].
Reference: [Lana] <author> G. G. Langdon, </author> <title> "A note on the Ziv-Lempel model for compressing individual sequences," </title> <journal> IEEE Transactions on Information Theory 29 (March 1983), </journal> <pages> 284-287. </pages>
Reference-contexts: The Ziv-Lempel encoder can be converted from a word-based method to a character-based algorithm E by building a probabilistic model that feeds probability information to an arithmetic coder <ref> [BCW, Lana] </ref>, as explained in the example below. It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in [ZiLb] hold without change for the character-based approach. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach <ref> [BCW, HoV, Lana] </ref>. Hence, the optimality results in [ZiLb] hold without change for the character-based approach. Example 3.1 Assume for simplicity that our alphabet is fa; bg. We consider the page request sequence "aaaababaabbbabaa: : : ". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa): : : ". <p> The resulting optimal prefetcher P is a blend of P 1 and the prefetcher P from Chapter 3 based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> (on which the UNIX compress program is based). <p> The approach of [FMG] allows us to combine P 1 with a prefetcher [ViK] based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers.
Reference: [Lanb] <author> G. G. Langdon, </author> <title> "An Introduction to Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 28 (March </month> <year> 1984), </year> <pages> 135-149. </pages>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> To 14 CHAPTER 3. OPTIMAL PREFETCHING VIA DATA COMPRESSION identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 3.1 being used as a model by an arithmetic coder.
Reference: [LeZ] <author> A. Lempel and J. Ziv, </author> <title> "On the Complexity of Finite Sequences," </title> <note> IEEE Transactions on Information Theory 22 (January 1976). </note>
Reference-contexts: It is shown in <ref> [LeZ] </ref> that 0 c ( n n lg ff ; lim * n = 0: (3.9) Theorem 3.4 is clearly true when c ( n 1 ) = o (n= lg n) since Compression E;n ( n 1 ) ~ 0 as n ! 1.
Reference: [LKH] <author> K. Li, R. Kumpf, P. Horton, and T. Anderson, </author> <title> "A Quantitative Analysis of Disk Drive Power Management in Portable Computers," </title> <booktitle> Proceedings of the 1994 Winter USENIX (January 1994). </booktitle>
Reference-contexts: Current techniques for conserving power are based on shutting down components of the system after reasonably long periods of inactivity. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Most disks used for portable computers (e.g., the small, light-weight Kitty-hawk from Hewlett Packard [Pac]) have multiple energy states. <p> Energy conservation is critical in these mobile systems. This has motivated both hardware and software approaches for reducing power consumption in these systems. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Disk manufacturers are developing special types of drives especially designed for the portable market. <p> We elaborate on this in Chapter 8. 6.2 Background and Related Work The area of disk spindown and device management has attracted a lot of attention recently. Li et al. have also investigated the issue of disk drive power management <ref> [LKH] </ref>. They used trace-driven simulation to look at a fixed threshold policy for disk spin-control, and studied buffer cache parameters. <p> The Macintosh enforces a minimum cache size of 32 Kbytes; we varied the cache size from having no cache at all to a maximum of 1 Mbyte. Because a relatively large cache is essential to eliminating enough disk accesses to make spinning down the disk worthwhile <ref> [LKH] </ref>, in this chapter we report results for the 1-Mbyte cache. Also, in our Powerbook traces, about 2% of accesses went to files on the RAM disk, and we ignored these accesses in the simulator. <p> In Chapter 9, we consider the related metric of total number of operations delayed. As mentioned in Chapter 6, independently to our work, Li et al. have also investigated the issue of disk drive power management <ref> [LKH] </ref>. They used trace-driven simulation to look at a threshold demand policy for disk spin-control, and studied buffer cache parameters. While we considered multiple algorithms for determining when to spindown the disk (offline optimal, threshold demand, and predictive), they focused on threshold demand.
Reference: [LPR] <author> C. Lund, S. Phillips, and N. Reingold, </author> <title> "IP over Connection-Oriented Networks and Distributed Paging," </title> <booktitle> Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science (November 1994), </booktitle> <pages> 424-434. </pages> <note> 134 CHAPTER 12. CONCLUSIONS </note>
Reference-contexts: Keeping the virtual circuit open can be thought of as a "rent" while closing it can be considered a "buy." The inter-arrival time of packets on a circuit (i.e., the resource use times in the rent-to-buy model) can be modeled as being drawn independently from a probability distribution <ref> [LPR, SaK] </ref>. An algorithm for the sequential rent-to-buy problem can be visualized in two ways.
Reference: [MDK] <author> B. Marsh, F. Douglis, and P. Krishnan, </author> <title> "Flash Memory File Caching for Mobile Computers," </title> <booktitle> Proceedings of the 27th IEEE Hawaii Conference on System Sciences (January 1994). </booktitle>
Reference-contexts: Current techniques for conserving power are based on shutting down components of the system after reasonably long periods of inactivity. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Most disks used for portable computers (e.g., the small, light-weight Kitty-hawk from Hewlett Packard [Pac]) have multiple energy states. <p> Energy conservation is critical in these mobile systems. This has motivated both hardware and software approaches for reducing power consumption in these systems. Recent studies show that the disk sub-system on notebook computers is a major consumer of power <ref> [DKM, LKH, MDK] </ref>. Disk manufacturers are developing special types of drives especially designed for the portable market.
Reference: [MVN] <author> Y. Matias, J. S. Vitter, and W. C. Ni, </author> <title> "Dynamic Generation of Discrete Random Variates," </title> <booktitle> Proceedings of the 4th Annual SIAM/ACM Symposium on Discrete Algorithms (January 1993). </booktitle>
Reference-contexts: We show that apart from our prefetcher achieving optimality in terms of fault rate, we simultaneously achieve the computational goal of implementing our prefetcher in optimal constant expected time per prefetched page, using the optimal dynamic discrete random variate generator of <ref> [MVN] </ref>. Cache replacement has been studied by Karlin, Phillips, and Raghavan [KPR] under a different stochastic version of the competitive framework; the sequence of page requests is assumed to be generated by a Markov chain (a subset of Markov sources). <p> We show in Section 4.5 how to implement the prefetcher in constant expected time per prefetched page, independent of alphabet size ff and cache size k, by use of the newly developed optimal dynamic algorithm for generating discrete random variates of Matias, Vitter, and Ni <ref> [MVN] </ref>, which uses a table lookup method of Hagerup, Mehlhorn, and Munro [HMM]. In Section 4.6, we look at an interesting proof technique that shows the optimality of predictor P 0 1 , an algorithm closely related to P 1 , for general ff, when k = 1. <p> The expected running time for prefetcher P can be made optimal, by use of the optimal dynamic random variate generator of <ref> [MVN] </ref>: Theorem 4.3 The prefetcher P runs in constant expected time (independent of ff and k) for each page prefetched; that is, it requires an average of O (k) time to determine which k pages to prefetch. <p> a large power and then choosing a page with probability proportional to its boosted count. (The boosted counts will be very large, but can be represented with O (log n) bits, using the scheme discussed in Section 4.5.) Efficient random variate generation with dynamically changing weights can be done using <ref> [MVN] </ref>, as discussed in Section 4.5. The algorithm P 0 1 is a simple randomized weighting algorithm that makes k predictions at each time step for the next page request. <p> counts f i are reset to zero when r changes.) The general problem of generating a random variate with a value in the range f0, 1, 2, : : : , ff 1g according to a set of ff dynamically changing weights is solved optimally by Matias, Vitter, and Ni <ref> [MVN, Section 5] </ref> using the table lookup procedure of Hagerup, Mehlhorn, and Munro [HMM]. The idea at an intuitive level is to group the weights into ranges according to their values. Range j stores weights with value in the range [2 j ; 2 j+1 ). <p> After two recursive levels, the problem reduces to generating one of O (log log ff) weights, each in the range [1; log ff], which can be done dynamically in constant time by the clever table lookup method of [HMM]. There is also extensive concern in <ref> [MVN] </ref> about the choice of hashing parameters in the universal hashing schemes used to get linear space, since no a priori bound on the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in [MVN].) The model of computation allows arithmetic computation and truncated <p> There is also extensive concern in <ref> [MVN] </ref> about the choice of hashing parameters in the universal hashing schemes used to get linear space, since no a priori bound on the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in [MVN].) The model of computation allows arithmetic computation and truncated logarithms of quantities up to value O (W ), where W is the maximum weight. In our application, the computation assumption of [MVN] is unreasonable. <p> the key values is known. (In fact, a constant-time solution to the general dictionary problem is proposed in <ref> [MVN] </ref>.) The model of computation allows arithmetic computation and truncated logarithms of quantities up to value O (W ), where W is the maximum weight. In our application, the computation assumption of [MVN] is unreasonable. We make the strong requirement that constant-time computations must operate on operands of at most O (log n) bits, where n is the length of the sequence of page requests.
Reference: [McC] <author> E. M. McCreight, </author> <title> "A Space-Economical Suffix Tree Construction Algorithm," </title> <journal> Journal of the ACM 23 (1976), </journal> <pages> 262-272. </pages>
Reference-contexts: Such a structure will aid in the online search. It is important to note that not every data compressor's data structure is good enough for the selectivity estimation problem, since we ultimately have to match patterns that have wildcards. We present a version of the suffix tree <ref> [McC] </ref> as an appropriate data structure for the selectivity problem, since it allows searches of arbitrary substrings of the original database items; suffix trees are used by Fiala and Greene [FiG] to implement Ziv-Lempel-based algorithms for data compression. 101 Chapter 11 Alphanumeric Selectivity in the Presence of Wildcards In this chapter, <p> Other related issues are discussed in Section 11.9. 102 CHAPTER 11. ALPHANUMERIC SELECTIVITY WITH WILDCARDS (a) Suffix tree for string "greener!" (b) Adding a string to a suffix tree The value of ff as described in Example 11.1 is "e". 11.1 The Suffix Tree A suffix tree <ref> [McC] </ref> is a trie-based data structure [Knua, Mor] used for data compression [FiG], pattern matching [Wei], and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC]. <p> A suffix tree <ref> [McC] </ref> is a trie-based data structure [Knua, Mor] used for data compression [FiG], pattern matching [Wei], and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC]. Example 11.1 (See Figure 11.1.) A suffix tree is based on the Patricia trie data structure. A trie is a multiway tree with a path from the root to a unique node for each string added to the trie. <p> words, while building the suffix tree for string , no string we add to the tree can be a prefix of another string added to the tree. 2 Various optimizations with respect to time and space to the basic suffix tree construction algorithm we have just described have been studied <ref> [FiG, McC] </ref>. <p> While adding i to the tree, the process of determining the largest prefix ff of i and the node z such that ff is a prefix of the string corresponding to node z can be done efficiently using extra "suffix pointers" in the tree <ref> [McC] </ref>. 1 The difference between the string associated with a node and the string corresponding to a node is important; these terms are used throughout this chapter in the sense presented here. 104 CHAPTER 11. <p> Since the database is typically large, it will be in secondary memory, and retrieving the strings from the pointers will cause I/Os. "Suffix pointers" as mentioned at the end of Section 11.1 and a hashed implementation <ref> [McC] </ref> need to be used when the string associated with a node is stored as two pointers; suffix pointers short-circuit the search for node z and ensure that minimal I/O is required per string insertion.
Reference: [McS] <author> L. A. McGeoch and D. D. Sleator, </author> <title> "A Strongly Competitive Randomized Paging Algorithm," </title> <institution> Carnegie-Mellon University, CS-89-122, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time.
Reference: [MeF] <author> N. Merhav and M. Feder, </author> <title> "Universal Sequential Learning and Decision from Individual Data Sequences," </title> <booktitle> Proceedings of the 5th ACM Workshop on Computational Learning Theory (July 1992), </booktitle> . 
Reference-contexts: The procedure in [Han] may be generalizable to the arbitrary alphabet case ff 2 for cache 4.1. ANALYSIS MODEL AND MAIN RESULTS 25 size k = 1, but it cannot possibly make a prediction in constant time independent of ff, and the k &gt; 1 case is open. In <ref> [MeF] </ref>, predictors are developed for various continuous loss functions, but they are not relevant to the harder-to-analyze discontinuous 0-1 loss functions associated with cache replacement and prefetching. <p> In Section 4.2, we present our core prefetcher algorithm P 1 , which makes use of sampling without replacement, and we analyze it in Section 4.3 by comparing it against the best one-state prefetcher. In Section 4.4 we draw on ideas from information theory [CoT] applied to predicting <ref> [CoS, FMG, MeF] </ref> and generalize P 1 to get a universal prefetcher P that is optimal in the limit against a general finite state prefetcher.
Reference: [Mor] <author> D. R. Morrison, </author> <title> "Patricia-A Practical Algorithm to Retrieve Information Coded in Alphanumeric," </title> <journal> Journal of the ACM 15 (1968), </journal> <pages> 514-534. </pages>
Reference-contexts: ALPHANUMERIC SELECTIVITY WITH WILDCARDS (a) Suffix tree for string "greener!" (b) Adding a string to a suffix tree The value of ff as described in Example 11.1 is "e". 11.1 The Suffix Tree A suffix tree [McC] is a trie-based data structure <ref> [Knua, Mor] </ref> used for data compression [FiG], pattern matching [Wei], and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC].
Reference: [MLG] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and Evaluation of a Compiler Algorithm for Prefetching," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (Octo-ber 1992). </booktitle>
Reference-contexts: The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Our work [CKV, KrV, ViK] described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> Database researchers have studied prefetching based on application level knowledge [Stoa]. Compiler-based techniques for prefetching lie at a lower level of abstraction. Under this paradigm, the compiler reorders instructions in application code and introduces explicit prefetching instructions to reduce the effect of cache misses <ref> [MLG] </ref>. These schemes have been applied to scientific programs, by having the compiler perform locality analysis to selectively prefetch only those references that are likely to cause cache misses.
Reference: [Nat] <author> S. Natarajan, </author> <title> "Large Deviations, Hypothesis Testing, and Source Coding for Finite Markov Sources ," IEEE Transactions on Information Theory 31 (1985), </title> <type> 360-365. </type>
Reference-contexts: The probability of sequences of length n for which X f z f z;i ln p z;i 22 CHAPTER 3. OPTIMAL PREFETCHING VIA DATA COMPRESSION is exponentially small in n <ref> [Nat, Theorem 2] </ref>, for ffi &gt; 0. By Lemmas 3.5 and 3.6, we have E (Fault X;n ) F p with exponentially small probability. By the definition of fault rate and Lemma 3.4, it follows that E (Fault X;n ) = F M (n) E (Fault M 0 ;n ). <p> The prefetcher estimates the probability of each transition to be the frequency that it is taken. These frequencies converge to the actual probabilities exponentially fast <ref> [Nat] </ref>. Since the state structure of the source is known, M is always in the same state that the source M is in. Let the algorithm M prefetch the pages with the top k estimated probabilities.
Reference: [OuD] <author> J. Ousterhout and F. Douglis, </author> <title> "Beating the I/O Bottleneck: A Case for Log-Structured File Systems," </title> <editor> U. C. </editor> <address> Berkeley, UCB/CSD 88/467, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: ALPHANUMERIC SELECTIVITY WITH WILDCARDS enhancements to this strategy suggested by cleaning policies in log structured file systems <ref> [OuD] </ref> that can be used, especially if we observe during the tree building phase that the pruning process is occurring too often; we could prune out more nodes and ensure that a fraction of total memory is unused.
Reference: [Pac] <author> Hewlett Packard, </author> <title> "Kittyhawk HP C3013A/C3014A Personal Storage Modules Technical Reference Manual," </title> <journal> March 1993, </journal> <volume> HP Part No. </volume> <pages> 5961-4343. </pages>
Reference-contexts: Recent studies show that the disk sub-system on notebook computers is a major consumer of power [DKM, LKH, MDK]. Most disks used for portable computers (e.g., the small, light-weight Kitty-hawk from Hewlett Packard <ref> [Pac] </ref>) have multiple energy states. Conceptually, the disk can be thought of as having two states: the spinning state in which the disk can access data but consumes a lot of power and a spundown state in which the disk consumes effectively no power but cannot access data. <p> These disks have small size, high density, high shock tolerance, and multiple energy states; in a sleep or spundown state, the disks consume comparatively no energy, but need to be spunup before data can be accessed <ref> [Cor, Pac, Qua] </ref>. The spinup takes on the order of seconds, as opposed to accessing data from a spinning disk which takes on the order of milliseconds. <p> Typically, the disk consumes a very small amount of power in the spundown state, and little or no energy in the sleep state. Table 7.1 lists the main characteristics of two disk drives for mobile computers, the Hewlett-Packard Kittyhawk C3014A <ref> [Pac] </ref> and the Quantum Go*Drive 120 [Qua]. (The quantity T d in Table 7.1 is explained in Section 7.2.1.) 7.2 Policies In this chapter, we investigate two types of algorithms for spinning a disk up and down: offline, which can use future knowledge, and online, which can use only past behavior.
Reference: [PaZa] <author> M. Palmer and S. Zdonik, </author> <title> "Predictive Caching," </title> <institution> Brown University, CS-90-29, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The server has the ability to handle demand read requests from the application and prefetch read requests from the prefetcher. The server gives priority to the client's requests, flushing prefetch requests in its queue when a demand request arrives. Such provisions are generally available in prefetching systems <ref> [GrR, PaZa] </ref>. The prefetcher can be either part of the application or a separate entity distinct from the application. It works by processing the sequence of the client's previous page requests and making requests for data from the server.
Reference: [PaZb] <author> M. Palmer and S. Zdonik, </author> <title> "Fido: A Cache that Learns to Fetch," </title> <booktitle> Proceedings of the 1991 International Conference on Very Large Databases (September 1991). </booktitle>
Reference-contexts: The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Our work [CKV, KrV, ViK] described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> In [RoL], a combined hardware and software approach has been studied where an optimizing compiler and speculative loads are used to issue read requests in anticipation of a demand request. Interesting approaches have been proposed for prefetching based on predicting future page requests by monitoring past user behavior <ref> [Lai, PaZb, Sal] </ref>. Palmer and Zdonik [PaZb] use a pattern matching approach to prediction. Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. <p> Interesting approaches have been proposed for prefetching based on predicting future page requests by monitoring past user behavior [Lai, PaZb, Sal]. Palmer and Zdonik <ref> [PaZb] </ref> use a pattern matching approach to prediction. Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. <p> The reduction in fault-rate is also better than that of recent proposed schemes for prefetching <ref> [PaZb] </ref>. In Section 5.1 we describe the system environment. We describe our three prefetchers in Section 5.2. In Section 5.3 we look closely at problems stemming from memory and time restrictions unique to prefetching in some systems. We propose solutions to these problems and bound their worst-case behavior. <p> Any cache replacement algorithm can be suitably modified to work with the "generic" prefetchers described earlier. In particular, we can use the probabilities of the generic prefetcher to determine what to evict from cache, or adapt strategies like the MLP replacement strategy from <ref> [PaZb] </ref>, or adapt well-known cache replacement algorithms like FIFO or LRU. In our simulations, we use a version of LRU suitably modified to handle prefetched pages. Prefetched items are put into cache as if they were demand fetched. <p> Statistics are given in Table 5.1. CAD1 and CAD2 are object ID (UID) traces from a CAD tool written at Digital's CAD/CAM Technology Center in Chelmsford MA. We include them here as a comparison to the Fido <ref> [PaZb] </ref> algorithm that analyzed prefetching on the same traces. The OO1 database benchmark, also known as the "Sun Benchmark," was run on the DEC Object/DB product 3 to generate page fault information for all phases of the benchmark. <p> The cache replacement strategy used in conjunction with the uniform prefetcher is extremely relevant. Our cache replacement scheme performs very well as seen. Some other cache replacement strategy may give even better improvements. For comparison with Fido <ref> [PaZb] </ref>, we simulated our algorithms on the same trace (CAD2) with the same cache sizes for LRU (2,000) and the prefetcher (1,500) as used in [PaZb]. Fido decreased the fault rate from 45.8% to about 23.5%. <p> Our cache replacement scheme performs very well as seen. Some other cache replacement strategy may give even better improvements. For comparison with Fido <ref> [PaZb] </ref>, we simulated our algorithms on the same trace (CAD2) with the same cache sizes for LRU (2,000) and the prefetcher (1,500) as used in [PaZb]. Fido decreased the fault rate from 45.8% to about 23.5%. Our improvement (for PPM of order 1) was better; from 45.8% to 18.2%. (In Fido, the predictor is trained on a request sequence, the model is frozen, and it is used for prefetching on request traces from similar applications. <p> It will also be interesting from a theoretical point of view to analyze the effect of these heuristics using an appropriate model of locality as in [KPR, SlT, ViK]. As described earlier, the approach from <ref> [PaZb] </ref> is based on training the predictor on a page request sequence, fixing the predictor, and using it for subsequent user sessions. Notice that the predictors developed in this paper can also be used from session to session, by saving 56 CHAPTER 5.
Reference: [PGS] <author> R. H. Patterson, G. A. Gibson, and M. Satyanarayanan, </author> <title> "A Status Report on Research in Transparent Informed Prefetching," </title> <booktitle> ACM Operating Systems Review 27 (April 1993), </booktitle> <pages> 21-34. </pages>
Reference-contexts: PREFETCHING: THE WHAT 2.1 Background and Related Work At a high level, recent work in prefetching can be divided into three categories: transparent informed prefetching, compiler-based techniques for prefetching, and automatic prediction for prefetching. In Transparent Informed Prefetching (or TIP) <ref> [PGS] </ref>, the application gives explicit hints for prefetching; i.e., application levels of the system disclose future access patterns rather than advise lower-level policies. This information is used for converting the high throughput of new technologies such as disk-arrays and log-structured file systems into low latency for applications. <p> Our prefetching algorithms are adaptive and based only on the page request sequence. They do not attempt to take advantage of possible knowledge of the application that is issuing the requests. In practice, when such knowledge is available <ref> [PGS] </ref>, we could combine 3.5. DISCUSSION 23 our prefetcher with a logical prefetcher based on the semantics of the application, so as to get the best of both worlds. Similar techniques for cache replacement appear in [FKL].
Reference: [Pol] <author> D. Pollard, </author> <title> Convergence of Stochastic Processes, </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Lemma 8.5 (see <ref> [Pol] </ref>) Choose M &gt; 0, a probability distribution D on [0; M ], and m 2 IN. Then Pr ~x2D m fi fi 1 m X x i E u2D (u) fi fi * 2e 2* 2 m=M 2 8.4.
Reference: [Qua] <author> Quantum, </author> <title> "Go*Drive 60/120S Product Manual," </title> <month> May </month> <year> 1992. </year>
Reference-contexts: These disks have small size, high density, high shock tolerance, and multiple energy states; in a sleep or spundown state, the disks consume comparatively no energy, but need to be spunup before data can be accessed <ref> [Cor, Pac, Qua] </ref>. The spinup takes on the order of seconds, as opposed to accessing data from a spinning disk which takes on the order of milliseconds. <p> Typically, the disk consumes a very small amount of power in the spundown state, and little or no energy in the sleep state. Table 7.1 lists the main characteristics of two disk drives for mobile computers, the Hewlett-Packard Kittyhawk C3014A [Pac] and the Quantum Go*Drive 120 <ref> [Qua] </ref>. (The quantity T d in Table 7.1 is explained in Section 7.2.1.) 7.2 Policies In this chapter, we investigate two types of algorithms for spinning a disk up and down: offline, which can use future knowledge, and online, which can use only past behavior. <p> As a result, the break-even point for the Kittyhawk is about a fourth that of the Go*Drive, making a short spindown threshold much more important for the Kittyhawk. Also, the Kittyhawk spins down in a half a second, while the Go*Drive takes "&lt; 6s" <ref> [Qua] </ref>. power consumption. Leaving the disk spinning all the time will produce the minimal impact on response time, but will waste power if the disk isn't accessed for long periods of time.
Reference: [RoL] <author> A. Rogers and K. Li, </author> <title> "Software Support for Speculative Loads," </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (October 1992). </booktitle>
Reference-contexts: The I/O bottleneck is seriously impeding performance in large-scale databases, and the demand for improving response time performance is growing [Bra]. The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching <ref> [ChB, Lai, MLG, PaZb, RoL] </ref>. Our work [CKV, KrV, ViK] described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> Hardware schemes of non-blocking and prefetching caches that let processing continue when a cache miss occurs, blocking only when the missed data is actually needed have also been proposed [ChB]. In <ref> [RoL] </ref>, a combined hardware and software approach has been studied where an optimizing compiler and speculative loads are used to issue read requests in anticipation of a demand request. Interesting approaches have been proposed for prefetching based on predicting future page requests by monitoring past user behavior [Lai, PaZb, Sal].
Reference: [RuWa] <author> C. Ruemmler and J. Wilkes, </author> <title> "UNIX Disk Access Patterns," </title> <booktitle> Proceedings of the 1993 Winter USENIX Conference (January 1993), </booktitle> <pages> 405-420. </pages>
Reference-contexts: In addition, we used traces from an HP-UX workstation, documented by Ruemmler and Wilkes <ref> [RuWa] </ref>. We used the HP-UX traces for a couple of reasons: first, because there are a number of UNIX-based mobile platforms available, so a UNIX trace might be indicative of actual mobile usage patterns; and second, because UNIX does the sort of aggressive 7.3. <p> A third possible metric is the number of operations delayed. The metrics of number of reads delayed and number of operations delayed are very close; intuitively, if writes (including synchronous writes) can be decoupled from disk latency with a small amount of nonvolatile memory <ref> [BAD, RuWa] </ref>, then the number of writes delayed can be ignored. traces, with a 1-Mbyte cache, and Figure 7.3 shows the same for the HP-UX traces (which has an implicit buffer cache, as discussed above). <p> cost of the buy, if the algorithm is given as an input the relative importance of conserving energy and responding quickly to disk accesses. (This is discussed in detail in Chapter 9, and forms an integral component of adaptive spindown.) Based on observations of disk access patterns in workstation environments <ref> [RuWa] </ref>, the times between accesses to disk (which define the rounds) can be assumed to be generated by a probability distribution. The spin/block problem, another interesting and important problem from multiprocessor applications, can be modeled by the rent-to-buy framework. <p> We first present the methodology of our simulations in Section 9.2, and then present our results in Section 9.3. 9.2 Methodology We simulated algorithm L using a disk access trace from a Hewlett-Packard 9000/845 personal workstation running HP-UX. This trace is described in <ref> [RuWa] </ref>, and this trace was also used in our experimental study described in Chapter 7. The trace was obtained by Ruemmler and Wilkes by monitoring the disk for roughly two months; it consisted of 416262 accesses to disk.
Reference: [RuWb] <author> C. Ruemmler and J. Wilkes, </author> <title> "An Introduction to Disk Drive Modeling," </title> <booktitle> IEEE Computer 27 (March 1994), </booktitle> <pages> 17-28. </pages>
Reference-contexts: Golding et al. [GBS] have studied idle-time detection and prediction, and proposed a taxonomy of idle-time detection 6.3. OUR APPROACH 59 algorithms; they report the effect of their different methods in the context of the TickerTAIP simulation system <ref> [RuWb] </ref>. Greenawalt [Gre] did an analytical study of disk management strategies, assuming a Poisson process for request arrival. The single rent-to-buy problem has been studied in the worst-case setting and efficient deterministic and randomized algorithms have been developed for the problem by Karlin et al. [KMM].
Reference: [Sal] <author> K. Salem, </author> <title> "Adaptive Prefetching for Disk Buffers," </title> <type> CESDIS, </type> <institution> Goddard Space Flight Center, TR-91-64, </institution> <month> January </month> <year> 1991. </year> <month> 135 </month>
Reference-contexts: In [RoL], a combined hardware and software approach has been studied where an optimizing compiler and speculative loads are used to issue read requests in anticipation of a demand request. Interesting approaches have been proposed for prefetching based on predicting future page requests by monitoring past user behavior <ref> [Lai, PaZb, Sal] </ref>. Palmer and Zdonik [PaZb] use a pattern matching approach to prediction. Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. <p> Palmer and Zdonik [PaZb] use a pattern matching approach to prediction. Based on previous user sessions, they build a model in a training phase; this model is frozen, and in the prediction phase the model is used for prefetching. Salem computes various first-order statistics for prediction <ref> [Sal] </ref>, and Laird uses a growing Markov predictor [Lai]. Song et al. study prefetching based on page fault history [SoC]. Alexander et al. consider hardware prefetching schemes between processor and main memory by extracting tables of fixed length contexts from past user requests [AlK]. <p> PRACTICAL PREFETCHING VIA DATA COMPRESSION prefetch pages in sequence, that is, prefetch page i + 1 when page i was being requested, are not universal. The usefulness of universality is extremely significant in current databases <ref> [Sal] </ref>.
Reference: [SaK] <author> H. Saran and S. Keshav, </author> <title> "An Empirical Evaluation of Virtual Circuit Holding Times in IP-over-ATM networks," </title> <booktitle> Proceedings of INFOCOM '94 (June 1994). </booktitle>
Reference-contexts: The rent-to-buy framework is also useful in studying other related systems problems like the spin/block problem from multiprocessor applications [KLM] and deciding virtual circuit holding times in IP-over-ATM networks <ref> [SaK] </ref>. We elaborate on this in Chapter 8. 6.2 Background and Related Work The area of disk spindown and device management has attracted a lot of attention recently. Li et al. have also investigated the issue of disk drive power management [LKH]. <p> The rent-to-buy framework models not only the disk spindown problem, but other interesting systems problems as well, specifically, thread blocking decisions during lock acquisition in multiprocessor applications [KLM], and virtual circuit holding times in IP-over-ATM networks <ref> [KLP, SaK] </ref>. <p> In this situation too, practical studies suggest that lock-waiting times can be assumed to obey some unknown but time-invariant probability distribution [KLM]. Deciding virtual circuit holding times in IP-over-ATM networks is another scenario modeled by the rent-to-buy framework <ref> [SaK] </ref>. When carrying Internet protocol (IP) traffic over an Asynchronous Transfer Mode (ATM) network, a virtual circuit is opened upon the arrival of an IP datagram, and the ATM adaptation layer has to decide how long to hold a virtual circuit open. <p> There are many possible pricing policies for virtual circuit holding times. As described in <ref> [SaK, Section 5] </ref>, in future ATM networks, it is expected that a large number of virtual circuits could be held open by paying a charge per unit time to keep the circuit open. <p> Keeping the virtual circuit open can be thought of as a "rent" while closing it can be considered a "buy." The inter-arrival time of packets on a circuit (i.e., the resource use times in the rent-to-buy model) can be modeled as being drawn independently from a probability distribution <ref> [LPR, SaK] </ref>. An algorithm for the sequential rent-to-buy problem can be visualized in two ways. <p> However, their technique as stated is computationally inefficient. Karlin et al. [KLM]have studied the spin/block problem empirically, evaluating different spin/block strategies including fixed-threshold and adaptive strategies. The virtual circuit problem has been empirically studied by Saran et al. <ref> [SaK] </ref>, where they propose a Least Recently Used (LRU)-based holding time policy as performing well in their studies. The first LRU-based holding time policy they study is the 2-competitive algorithm described earlier in this chapter, and their second holding time policy involves estimating the mean inter-reference interval with exponential averaging.
Reference: [SAC] <author> P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price, </author> <title> "Access Path Selection in a Relational Database Management System," </title> <booktitle> Proceedings of the 1979 ACM SIGMOD Conference, </booktitle> <pages> 23-34. </pages>
Reference-contexts: Samuelson (1966) Query optimization is an integral part of relational databases <ref> [Dat, SAC, Ull] </ref>. For example, consider the selection predicate salary between 30K and 60K (10.1) of a payroll database, which requests records of all employees whose salary lies between 30K and 60K. <p> Query optimizers have cost models that estimate the access cost as a function of the predicted number of qualifying rows and find the cheaper alternative. Models already exist in current day relational database management systems (RDBMSs) to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate [Iye]. <p> In Chapter 11 we present our techniques for predicting selectivity for the like predicate; i.e., techniques for estimating alphanumeric selectivity. 10.1 Background and Related Work Models already exist in current day RDBMSs to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. Typically, in the preprocessing phase, a few numbers that "capture" the distribution of data are accumulated and stored in the catalog.
Reference: [ShT] <author> G. S. Shedler and C. Tung, </author> <title> "Locality in Page Reference Strings," </title> <journal> SIAM Journal on Computing 1 (1972), </journal> <pages> 218-241. </pages>
Reference-contexts: Algorithm P 1 may also perform better than algorithm P 0 1 in practice, since it captures the effect of locality of reference found in page request sequences <ref> [Bel, BIR, Den, IKP, KPR, ShT] </ref>. 4.3 One-State Case: Optimality of P 1 vs.
Reference: [SlT] <author> D. D. Sleator and R. E. Tarjan, </author> <title> "Amortized Efficiency of List Update and Paging Rules," </title> <booktitle> Communications of the ACM 28 (February 1985), </booktitle> <pages> 202-208. </pages>
Reference-contexts: An algorithm is online if it must make its decisions based only on past history. An o*ine algorithm can use knowledge of the future. Any implementable algorithm for cache replacement or prefetching must clearly be online. The notion of competitiveness introduced by Sleator and Tarjan <ref> [SlT] </ref> determines the goodness of an online algorithm by comparing its performance to that of o*ine algorithms. <p> Competitive algorithms for cache replacement are well examined in the literature <ref> [BIR, FKL, IKP, KPR, McS, SlT] </ref>. It is unreasonable to expect algorithms to be competitive in this sense for prefetching. An optimal o*ine algorithm for prefetching would never fault, if it can prefetch at least one page every time. <p> It will also be interesting from a theoretical point of view to analyze the effect of these heuristics using an appropriate model of locality as in <ref> [KPR, SlT, ViK] </ref>. As described earlier, the approach from [PaZb] is based on training the predictor on a page request sequence, fixing the predictor, and using it for subsequent user sessions.
Reference: [SoC] <author> I. Song and Y. Cho, </author> <title> "Page Prefetching Based on Fault History," </title> <booktitle> Proceedings of the USENIX Mach III Symposium, </booktitle> <pages> 203-213. </pages>
Reference-contexts: Salem computes various first-order statistics for prediction [Sal], and Laird uses a growing Markov predictor [Lai]. Song et al. study prefetching based on page fault history <ref> [SoC] </ref>. Alexander et al. consider hardware prefetching schemes between processor and main memory by extracting tables of fixed length contexts from past user requests [AlK].
Reference: [Stoa] <author> M. Stonebraker, </author> <title> "Operating System Support for Database Management," </title> <booktitle> Communications of the ACM 24 (July 1981), </booktitle> <pages> 412-418. </pages>
Reference-contexts: The idea of giving hints has been proposed in many contexts; Trivedi [Tri] suggested using programmer or compiler generated hints for prefetching. Database researchers have studied prefetching based on application level knowledge <ref> [Stoa] </ref>. Compiler-based techniques for prefetching lie at a lower level of abstraction. Under this paradigm, the compiler reorders instructions in application code and introduces explicit prefetching instructions to reduce the effect of cache misses [MLG].
Reference: [Stob] <author> J. A. Storer, </author> <title> Data Compression Methods and Theory, </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: In some applications this is justified. However, we cannot expect all systems to have this facility. Several techniques are known for limiting data structure size in data compressors <ref> [Stob] </ref>. An explicit upper bound M is placed on the size of the data structure.
Reference: [TPC] <author> Transaction Processing Performance Council, </author> <title> TPC, "TPC Benchmark TM D (Decision Support), </title> <note> Working Draft 6.0," 1993, </note> <editor> F. </editor> <booktitle> Raab (editor) </booktitle> . 
Reference-contexts: As described in Section 10.2, our strategy is based on our intuition developed in previous chapters of the close relationship between data compression and prediction. There is an emerging industry standard Transaction Processing Council (TPC) benchmark, known as the TPC-D benchmark <ref> [TPC] </ref>, that involves predicates such as the like predicate. We study our techniques in the context of this benchmark. We first concentrate on trying to predict selectivity for unit patterns, i.e., patterns where a string is sandwiched between two wildcards (e.g., the pattern "flgreenfl" in the like predicate from (10.2)). <p> There is an emerging industry standard Transaction Processing Council (TPC) benchmark, known as the TPC-D benchmark <ref> [TPC] </ref>, that involves predicates such as the like predicate. We studied our techniques in the context of this benchmark.
Reference: [Tri] <author> K. S. Trivedi, </author> <title> "An Analysis of Prepaging," </title> <booktitle> Computing 22 (1979), </booktitle> <pages> 191-210. </pages>
Reference-contexts: This information is used for converting the high throughput of new technologies such as disk-arrays and log-structured file systems into low latency for applications. The idea of giving hints has been proposed in many contexts; Trivedi <ref> [Tri] </ref> suggested using programmer or compiler generated hints for prefetching. Database researchers have studied prefetching based on application level knowledge [Stoa]. Compiler-based techniques for prefetching lie at a lower level of abstraction.
Reference: [TsN] <author> M. M. Tsangaris and J. F. Naughton, </author> <title> "On the Performance of Object Clustering Techniques," </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data (June 1992), </booktitle> <pages> 144-153, </pages> <note> Also appears as University of Wiscon-sin Madison Technical Report number 1090-1992. </note>
Reference-contexts: Clustering algorithms attempt to improve the performance of database systems by placing related sets of objects on the same page in the hope of reducing the average number of I/Os needed to retrieve objects. There has been extensive work in clustering (e.g., <ref> [TsN] </ref> and references therein). It would be interesting to see the effect of the combination of clustering and our prefetching techniques on response-time performance. Using prefetch data structures for clustering could also be considered. There are many open problems that this work motivates, both theoretical and practical.
Reference: [Ull] <author> J. D. Ullman, </author> <title> Principles of Database Systems, </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: SELECTIVITY 3 1.3 Selectivity The question of predicting how much is going to happen in the future is well illustrated by the problem of predicting selectivity in relational databases. Query optimization is an important part of database management systems <ref> [Dat, Ull] </ref>. The size of databases is growing rapidly and decisions support is an important component of databases. A typical query to a database is to return all records of the database satisfying predicate P . To answer such queries, the database performs query optimization to develop a search strategy. <p> Samuelson (1966) Query optimization is an integral part of relational databases <ref> [Dat, SAC, Ull] </ref>. For example, consider the selection predicate salary between 30K and 60K (10.1) of a payroll database, which requests records of all employees whose salary lies between 30K and 60K.
Reference: [Vapa] <author> V. N. Vapnik, </author> <title> Estimation of Dependencies based on Empirical Data, </title> <publisher> Springer Ver-lag, </publisher> <year> 1982. </year>
Reference-contexts: bytes (or, as described in Section 11.5.3, the catalog used 436 bytes if an extra count was stored with each node). 11.7 A "Strawman" Random Sampling Scheme It is instructive to compare the results presented in Section 11.6 against a simple random sampling strategy motivated by work in learning theory <ref> [Vapa] </ref>. The comparisons are especially interesting given that we know of no prior work in predicting alphanumeric selectivity.
Reference: [Vapb] <author> V. N. Vapnik, </author> <title> "Inductive principles of the search for empirical dependences (methods based on weak convergence of probability measures)," </title> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory (1989). </booktitle>
Reference-contexts: The most straightforward solution to the problem [KMM] is to store all past resource use times, and use that cutoff b for the current round which would have had the lowest total cost had we used it in the past. Straightforward application of results of Vapnik <ref> [Vapb] </ref> implies that the expected rent-to-buy cost of this strategy converges to that of the best fixed cutoff.
Reference: [VaC] <author> V. N. Vapnik and A. Y. Chervonenkis, </author> <title> "On the Uniform Convergence of Relative Frequencies of Events to their Probabilities," </title> <booktitle> Theoretical Probability and Its Applications 16 (1971), </booktitle> <pages> 264-280. </pages>
Reference-contexts: As a result of the VC theory <ref> [BEHb, VaC] </ref>, it is easy to partition [0; M ] into v intervals satisfying the density criterion with high probability, by storing = fi (v ln v) examples, and calling a procedure generate cutoffs (w; ; ) on [0; M ]. <p> To prove the above lemma, we use a technique due to Kearns and Schapire [KeS]. Lemma 8.2 below follows immediately from the results of Blumer et al. [BEHb] using the techniques of Vapnik and Chervonenkis <ref> [VaC] </ref>. Informally, Lemma 8.2 says that m points are enough to simultaneously estimate the probabilities of every interval. Lemma 8.2 Choose 0 &lt; ff; fi 1=2; c 1; and a probability distribution D on IR + .
Reference: [ViK] <author> J. S. Vitter and P. Krishnan, </author> <title> "Optimal Prefetching via Data Compression," </title> <booktitle> Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science (October 1991), </booktitle> <pages> 121-130, </pages> <note> Also appears as Brown University Technical Report No. CS-91-46. </note>
Reference-contexts: The older virtual memory-based prefetchers are inadequate for newer object-oriented and hypertext applications, and this has stimulated renewed interest in developing improved algorithms for prefetching [ChB, Lai, MLG, PaZb, RoL]. Our work <ref> [CKV, KrV, ViK] </ref> described in Chapters 3-5 develops provably optimal and practical prefetchers by understanding and exploiting the intimate relationship between data compressors and predictors. 6 CHAPTER 2. <p> The approach of [FMG] allows us to combine P 1 with a prefetcher <ref> [ViK] </ref> based on the Ziv-Lempel data compressor [HoV, Lana, ZiLb] to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers. <p> It will also be interesting from a theoretical point of view to analyze the effect of these heuristics using an appropriate model of locality as in <ref> [KPR, SlT, ViK] </ref>. As described earlier, the approach from [PaZb] is based on training the predictor on a page request sequence, fixing the predictor, and using it for subsequent user sessions.
Reference: [Wei] <author> P. Weiner, </author> <title> "Linear Pattern Matching Algorithms," </title> <booktitle> Proceedings of the IEEE 14th Annual Symposium on Switching and Automata Theory (October, </booktitle> <year> 1973), </year> <pages> 1-11. </pages>
Reference-contexts: SELECTIVITY WITH WILDCARDS (a) Suffix tree for string "greener!" (b) Adding a string to a suffix tree The value of ff as described in Example 11.1 is "e". 11.1 The Suffix Tree A suffix tree [McC] is a trie-based data structure [Knua, Mor] used for data compression [FiG], pattern matching <ref> [Wei] </ref>, and other applications. We use a suffix tree-based structure for predicting the selectivity of alphanumeric strings. In this section we explain suffix trees with an example; for a formal description, see [McC]. Example 11.1 (See Figure 11.1.) A suffix tree is based on the Patricia trie data structure.
Reference: [WVT] <author> K.Y. Whang, B. T. Vander-Zanden, and H. M. Taylor, </author> <title> "A Linear-Time Probabilistic Counting Algorithm for Database Applications," </title> <journal> ACM Transactions on Database Systems 15 (June 1990), </journal> <pages> 208-229. </pages>
Reference-contexts: Query optimizers have cost models that estimate the access cost as a function of the predicted number of qualifying rows and find the cheaper alternative. Models already exist in current day relational database management systems (RDBMSs) to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. With the popularity of textual data being stored in RDBMS, it has become important to predict the selectivity accurately even for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the like predicate [Iye]. <p> In Chapter 11 we present our techniques for predicting selectivity for the like predicate; i.e., techniques for estimating alphanumeric selectivity. 10.1 Background and Related Work Models already exist in current day RDBMSs to predict selectivity for numeric fields <ref> [ASW, FlM, Iye, SAC, WVT] </ref>. Typically, in the preprocessing phase, a few numbers that "capture" the distribution of data are accumulated and stored in the catalog. <p> A well-studied issue related to predicting selectivity is estimating the number of unique values in a column of the table. In <ref> [ASW, FlM, WVT] </ref>, interesting linear time algorithms for finding the number of unique values in a column based on probabilistic and approximate counting methods have been described.
Reference: [Wil] <author> J. Wilkes, </author> <title> "Predictive Power Conservation," </title> <institution> Hewlett Packard Laboratories Technical Report HPL-CSP-92-5, </institution> , <month> February, </month> <year> 1994. </year> <note> 136 CHAPTER 12. CONCLUSIONS </note>
Reference-contexts: Wilkes hypothesized that it would be effective to use a weighted average of a few previous interarrival times at disk to decide when to spindown the disk on a mobile computer <ref> [Wil] </ref>. Adaptive spindown policies that continually change the spindown threshold based on the perceived inconvenience to the user are studied in [DKB]. Golding et al. [GBS] have studied idle-time detection and prediction, and proposed a taxonomy of idle-time detection 6.3. <p> He noted as well that if inactive intervals were of roughly fixed duration, the disk could be spun up in advance of the expected time of the next operation <ref> [Wil] </ref>. If access patterns are not so consistent, however, these techniques may not prove to be helpful. <p> Their basic conclusion regarding threshold demand algorithms is the same as ours: short timeouts on the order of a few seconds greatly reduce power consumption by the disk and do not significantly degrade performance. As mentioned above in Section 7.2.3, Wilkes proposed a predictive algorithm for disk management <ref> [Wil] </ref>. He suggested adjusting spindown timeouts based on a weighted average of recent interarrival times. Picking the weights may be a difficult task; we attempted to implement this strategy but were unable to out-perform threshold demand with the particular sets of weights we tried.
Reference: [WNC] <author> I. H. Witten, R. M. Neal, and J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Communications of the ACM 30 (June 1987), </journal> <pages> 520-540. </pages>
Reference-contexts: It encodes the substring x j by the value i, using dlg je bits, followed by the ascii encoding of the last character of x j , using dlg ffe bits. Arithmetic coding <ref> [HoV, Lanb, WNC] </ref> is a coding technique that achieves a coding length equal to the entropy of the data model. Sequences of probability p are encoded using lg (1=p) bits. <p> To 14 CHAPTER 3. OPTIMAL PREFETCHING VIA DATA COMPRESSION identify a subinterval of length u, an arithmetic coder has to output at least lg (1=u) bits; for more details refer <ref> [HoV, Lanb, WNC] </ref>. As an example, consider the probabilistic FSA of Figure 3.1 being used as a model by an arithmetic coder.
Reference: [Zen] <institution> Zenith Data Systems, Groupe Bull, "MastersPort 386L/386Le Owners Manual," </institution> <year> 1991. </year>
Reference-contexts: The Hewlett-Packard Kittyhawk C3014A takes about three seconds to spindown and spin back up; its manufacturer recommends spinning it down after about five seconds of inactivity [Hewb]; most other disks take several seconds for spindown/spinup and are recommended to spindown only after a period of minutes <ref> [Del, Zen] </ref>. The main intuition underlying the variations among disk spindown policies is identifying periods of inactivity at disk that are "sufficiently large." Fixed threshold policies wait a fixed period of time to be sure that the period of inactivity is large enough.
Reference: [ZiLa] <author> J. Ziv and A. Lempel, </author> <title> "A Universal Algorithm for Sequential Data Compression," </title> <note> IEEE Transactions on Information Theory 23 (May 1977). </note>
Reference-contexts: However, we notice an interesting intuitive link between the current prediction scenarios and the ones we have looked at in the previous chapter: the model built by a data compressor on an input text encapsulates information about common substrings in the text. Intuitively, text data compressors (as in <ref> [ZiLa, ZiLb] </ref>) remove repetition in the input to encapsulate information better.
Reference: [ZiLb] <author> J. Ziv and A. Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <journal> IEEE Transactions on Information Theory 24 (September 1978), </journal> <pages> 530-536. </pages>
Reference-contexts: In general, prefetching requests would be interrupted by the user's actual read requests; we consider non-pure prefetching in Chapter 5. Our models and results are summarized in Section 3.1. In Section 3.2, for our main Markov source model we apply a character-by-character version of the Ziv-Lempel data compression algorithm <ref> [ZiLb] </ref> upon which the UNIX compress program is based. In Section 3.3, we compare our online algorithm to the best algorithm that has full knowledge of the Markov source. <p> The trick is to show that good compression results in good prefetching. In the process of showing the fault rate convergence, we extend results in data compression related to the optimality of the Ziv-Lempel data compressor <ref> [ZiLb] </ref>. In Section 3.4 we show faster convergence to optimality for mth order Markov sources. Other related issues are discussed in Section 3.5. 3.1 Page Request Models and Main Results In keeping with the analogy between prefetching and text compression, we use the terms "page" and "character" interchangeably. <p> This minimum fault probability, weighted by the probability of being in state z, summed over all states z, gives us F M . This is formalized later in Definition 3.4. 10 CHAPTER 3. OPTIMAL PREFETCHING VIA DATA COMPRESSION We adapt a character-by-character version of the Ziv-Lempel <ref> [ZiLb] </ref> data compressor to get our optimal prefetcher P. Theorems 3.1 and 3.2 below are our main results. Theorem 3.1 Let M be a Markov source. The expected page fault rate achieved by P approaches F M , as the page request sequence length n ! 1. <p> The original Ziv-Lempel algorithm <ref> [ZiLb] </ref> 3.2. A PREFETCHING ALGORITHM BASED ON ZIV-LEMPEL 11 "aaaababaabbbabaa", as described in Example 3.1. is a word-based data compression algorithm. <p> It has been shown that the coding length obtained in this character-based approach is at least as good as that obtained using the word-based approach [BCW, HoV, Lana]. Hence, the optimality results in <ref> [ZiLb] </ref> hold without change for the character-based approach. Example 3.1 Assume for simplicity that our alphabet is fa; bg. We consider the page request sequence "aaaababaabbbabaa: : : ". The Ziv-Lempel encoder parses this string as "(a)(aa)(ab)(aba)(abb)(b)(abaa): : : ". <p> sequence n 1 and also to encode n 1 (via arithmetic coding), the average compression achieved is equal to the entropy of M ; that is, E (Compression M;n ) = H M (n): (3.3) The compression definitions in Definition 3.3 above are similar to those of Ziv and Lempel <ref> [ZiLb] </ref>, except that they define M (s) to be a class of "information lossless" non-probabilistic FSA encoders, use in place of Compression , and use n lg ff in place of n in (3.2) to get a ratio of output length to input length. <p> We generalize Ziv and Lempel's main result <ref> [ZiLb] </ref> to our model M (s) of probabilistic FSAs, using an iterative analysis based on arithmetic coding, to get the following theorem: Theorem 3.4 The compression of E on n 1 is no worse than the best probabilistic FSA in the limit as n ! 1. <p> Proof of Theorem 3.4: It has been shown in <ref> [ZiLb] </ref> that Compression E;n ( n 1 ) 1 ) + 1 lg (2ff (c ( n where c ( n 1 ) is the maximum number of nodes in any parse tree 1 for n 1 . <p> Corollary 3.2 Let M be a Markov source with s states. Then we have E Compression E;n s (n); lim ffi 0 1 This is not the definition of c ( n 1 ) in <ref> [ZiLb] </ref> but it is easy to verify that the proofs in [ZiLb] also hold under this definition of c ( n 1 ). 3.3. <p> Corollary 3.2 Let M be a Markov source with s states. Then we have E Compression E;n s (n); lim ffi 0 1 This is not the definition of c ( n 1 ) in <ref> [ZiLb] </ref> but it is easy to verify that the proofs in [ZiLb] also hold under this definition of c ( n 1 ). 3.3. <p> The resulting optimal prefetcher P is a blend of P 1 and the prefetcher P from Chapter 3 based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> (on which the UNIX compress program is based). <p> The approach of [FMG] allows us to combine P 1 with a prefetcher [ViK] based on the Ziv-Lempel data compressor <ref> [HoV, Lana, ZiLb] </ref> to get a prefetcher P that is optimal in the limit against the class of finite state prefetchers. <p> However, we notice an interesting intuitive link between the current prediction scenarios and the ones we have looked at in the previous chapter: the model built by a data compressor on an input text encapsulates information about common substrings in the text. Intuitively, text data compressors (as in <ref> [ZiLa, ZiLb] </ref>) remove repetition in the input to encapsulate information better.
References-found: 106

