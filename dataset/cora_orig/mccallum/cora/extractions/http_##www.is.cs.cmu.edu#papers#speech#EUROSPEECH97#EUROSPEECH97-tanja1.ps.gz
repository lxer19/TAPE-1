URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-tanja1.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: ftanja,koll,waibelg@ira.uka.de  
Title: JAPANESE LVCSR ON THE SPONTANEOUS SCHEDULING TASK WITH JANUS-3  
Author: T. Schultz, D. Koll, and A. Waibel 
Address: (USA)  
Affiliation: Interactive Systems Laboratories University of Karlsruhe (Germany), Carnegie Mellon University  
Abstract: This paper presents our findings during the development of the recognition engine for the Japanese part of the VERBMOBIL speech-to-speech translation project. We describe an efficient method to bootstrap a large vocabulary speech recognizer for spontaneously spoken Japanese speech from a German recognizer and show that the amount of effort in developing the system could be reduced by using this rapid cross language bootstrapping technique. The Japanese recognizer is integrated into the VERBMO-BIL system and shows very promising results achieving 9.3% word error rate. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Bub, W. Wahlster, and A. Waibel: VERB-MOBIL: </author> <title> The Combination of Deep and Shallow Processing for Spontaneous Speech Translation in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Munich 1997. </address>
Reference-contexts: 1. INTRODUCTION The overall goal of the first phase of the VERBMO-BIL project is to build a speech-to-speech translation system from both German and Japanese spontaneously spoken input speech to English, German and Japanese output in an appointment scenario <ref> [1] </ref>. The Japanese recognizer described in this paper is beeing designed to be part of this translation system. Unlike Japanese dictation systems [2] there is no need for our recognizer to map the output onto Japanese kanji/kana characters.
Reference: [2] <author> T. Matsuoka, K. Ohtsuki, T. Mori, S. Furui, and K. Shirai: </author> <title> Japanese Large-Vocabulary Continous-Speech Recognition Using a Business-Newspaper Corpus in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Munich 1997. </address>
Reference-contexts: The Japanese recognizer described in this paper is beeing designed to be part of this translation system. Unlike Japanese dictation systems <ref> [2] </ref> there is no need for our recognizer to map the output onto Japanese kanji/kana characters. Due to the many homophones in the Japanese language this mapping requires syntactic and semantic knowledge about the uttered sentence.
Reference: [3] <author> M. Siegel: </author> <title> Definiteness and Number in Japa-nese to German Machine Translation in: </title> <booktitle> Natural Language Processing and Speech Technology: Results of the 3rd KONVENS Conference, </booktitle> <address> Bielefeld (Germany) 1996. </address>
Reference-contexts: Due to the many homophones in the Japanese language this mapping requires syntactic and semantic knowledge about the uttered sentence. In the VERBMOBIL system this knowledge can be postponed to the syntactic and semantic analysis modules <ref> [3] </ref>. One peculiarity of Japanese is, that sentences are written in strings of kanji and kana characters without white space between adjacent words.
Reference: [4] <author> A. Kurematsu, M. Kitamura, T. Nakasuji, and A. Waibel: </author> <title> Data Collection of Japanese Spontaneous Speech on Scheduling Task and Development of Speech Dictionary in: </title> <booktitle> Proc. of Eu-rospeech, Rhodes 1997. </booktitle>
Reference-contexts: In order to determine such basic speech units for the recognition system the transcribed speech data has been segmented by a semi-automatic morphological analysis program described in <ref> [4] </ref>. <p> On average the dialogs cover 14 utterances each of a length of about 30 words. All dialogs are collected by ATR Interpreting Telecommunication Laboratories and the University of Electro-Communications in Tokyo (Japan). Further information about the corpus and collection procedures are given in <ref> [4] </ref>. Human-to-human dialogs tend to be at a higher level of spontaneity than in human-to-computer sce narios, used for example in the ATIS-Task. A com-parison of cross-talk vs push-to-talk scenario for the Spanish scheduling task showed that cross-talk is harder to recognize because it is more noisy [5].
Reference: [5] <author> P. Zhan, K. Ries, M. Gavalda, D. Gates, A. Lavie, and A. Waibel: </author> <title> JANUS-II: Towards Spontaneous Spanish Speech Recognition in: </title> <booktitle> Proc. of ICSLP, </booktitle> <address> Philadelphia 1996. </address>
Reference-contexts: Human-to-human dialogs tend to be at a higher level of spontaneity than in human-to-computer sce narios, used for example in the ATIS-Task. A com-parison of cross-talk vs push-to-talk scenario for the Spanish scheduling task showed that cross-talk is harder to recognize because it is more noisy <ref> [5] </ref>. On the other hand push-to-talk leads to longer and more complex utterances (38 vs 10 words per utterance on average), making this task more difficult for speech translation. 3.
Reference: [6] <author> B. Wheatley, K. Kondo, W. Anderson, and Y. Muthusamy: </author> <title> On Evaluation of Cross Language Adaptation for Rapid HMM Development in a New Language in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Adelaide 1994. </address>
Reference-contexts: On the other hand push-to-talk leads to longer and more complex utterances (38 vs 10 words per utterance on average), making this task more difficult for speech translation. 3. SYSTEM BOOTSTRAPPING It has been shown earlier in <ref> [6] </ref> that for small vocabulary continuous read speech the cross language bootstrapping technique from English to Japanese leads to reasonable results. We expand this approach to large vocabulary spontaneously spoken speech. 3.1.
Reference: [7] <author> T. Schultz and I. Rogina: </author> <title> Acoustic and Language Modeling of Human and Nonhuman Noises for Human-to-Human Spontaneous Speech Recognition in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Detroit 1995. </address>
Reference-contexts: These were bootstrapped as follows:/4/ from /u/, /4:/ from /u:/, /&0/ from /s/, and /dZ/ from /tS/. To cope with the effects arising in the spontaneously spoken data, like i.e. stuttering, false starts, or mumbling, special noise models <ref> [7] </ref> were included. All together 44 different phonemes are used to model Japanese speech: 31 speech models, 11 noise models, 1 silence and 1 glottal stop (ref. table 2). <p> Language Modeling For the final system, trigram language models have been built on the 730 dialogs of the training set using a Kneser/Ney backoff scheme for unseen bi- respectively trigrams. Our previous studies suggest that modeling noises like regular words improves the recognition performance moderately <ref> [7] </ref>. Breathing and key click noises are for example much more common at the beginning and the end than in the middle of an utterance. Therefore the noise events and hesitations (e.g. /eeto/ and /ano/) are modeled like regular words in computing their language model probabilities.
Reference: [8] <author> M. Finke and I. Rogina: </author> <title> Wide Context Acoustic Modeling in Read vs Spontaneous Speech in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Munich 1997. </address>
Reference-contexts: We modeled 54000 sept-phones in the context-dependent Japanese system and clustered these sept-phones to 600 decision-tree-clustered polyphone models as described in <ref> [8] </ref>. The final context-dependent system has about 2000 distributions over the 600 polyphone models. The phonetic questions needed for the clustering procedure could be derived from the German phonetic questions.The resulting context-dependent Japanese speech recognition system achieves 13.0% word error rate. 4.
Reference: [9] <author> L.J. Tomokyo-Mayfield and K. Ries: </author> <title> What makes a word: Learning base units in Japanese for speech recognition in: </title> <booktitle> Proc. of the ACL Natural Language Learning workshop, </booktitle> <address> Madrid July 1997. </address>
Reference-contexts: In doing so we get shorter, more frequent sequences. One drawback is however the reduction of the predictive power of n-gram models. Another is the necessity for checking the output of the segmentation tool which has to be done by native experts. In <ref> [9] </ref> a fully automatic statistical approach is described which find sequences of text that are both statistically important and semantically meaningful. Whereas the data being sufficient for acoustic modeling, the corpus of approximately 300K words over a 2598 word vocabulary might still be a little small for accurate trigramm estimation.
Reference: [10] <author> M. Finke, P. Geutner, H. Hild, T. Kemp, K. Ries, and Martin Westphal: </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine in: </title> <booktitle> Proc. of ICASSP, </booktitle> <address> Munich 1997. </address>
Reference-contexts: The recognition accuracy of the resulting final system using the full dictionary of 2598 words could thus be improved to 9.3% word error rate. Table 3 compares the word error rate between the described Japanese systems and the German recognition engine of Karlsruhe University <ref> [10] </ref>, which was the winner of the VERBMOBIL evaluation in 1996. 5. CONCLUSION From the above experiments we conclude that the cross language bootstrapping of Japanese acoustic models from German models is a very efficient technique even for large vocabulary spontaneously spoken speech.
References-found: 10

