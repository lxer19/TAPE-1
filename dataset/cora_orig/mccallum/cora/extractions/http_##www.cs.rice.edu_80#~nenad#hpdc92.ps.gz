URL: http://www.cs.rice.edu:80/~nenad/hpdc92.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~nenad/papers.html
Root-URL: 
Title: Data-Parallel Programming on a Network of Heterogeneous Workstations  
Author: Nenad Nedeljkovic Michael J. Quinn 
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: We describe a compiler and run-time system that allow data-parallel programs to execute on a network of heterogeneous UNIX workstations. The programming language supported is Dataparallel C, a SIMD language with virtual processors and a global name space. This parallel programming environment allows the user to take advantage of the power of multiple workstations without adding any message-passing calls to the source program. Because the performance of individual workstations in a multi-user environment may change during the execution of a Dataparallel C program, the run-time system automatically performs dynamic load balancing. We present experimental results that demonstrate the usefulness of dynamic load balancing in a multi-user environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. J. Anderson, </author> <title> "Interprocessor Communication in Dataparallel C," </title> <type> Technical Report 91-80-2, </type> <institution> Department of Computer Science, Oregon State University, </institution> <year> 1991. </year>
Reference-contexts: Using connection-oriented sockets and the standard TCP/IP communication protocol, we build a fully-connected topology among the participating workstations. Each processor can directly communicate with any other processor. Communication patterns and functions in the routing library for hypercube multicomputers are described in <ref> [1] </ref>. Initially, our goal was to use the library as is and only re-implement hypercube communication primitives for sending and receiving messages. While this approach provided almost immediate functionality of the system, further experiments have shown more efficient ways to implement some of the communication routines.
Reference: [2] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek, and V. Sunderam, </author> <title> "A Users' Guide to PVM (Parallel Virtual Machine)," </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Amber [5] is an object-based system with primitives for thread management and object mobility. Munin [3] is a distributed shared memory system that allows parallel programs written for shared memory multiprocessors to be executed in a distributed environment. Several software packages, such as PVM from Oak Ridge National Laboratory <ref> [2] </ref> and TCGMSG from Argonne National Laboratory, provide low-level functions for communication between processes on different machines. These low-level functions are similar to message-passing primitives for a distributed memory multicomputer. This paper focuses on data-parallel programming in a distributed computing environment.
Reference: [3] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel, </author> <title> "Implementation and Performance of Munin," </title> <type> Technical Report 91-150, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: More recently, however, significant work has been done to develop programming systems that would support development and execution of parallel applications across a network of workstations. Amber [5] is an object-based system with primitives for thread management and object mobility. Munin <ref> [3] </ref> is a distributed shared memory system that allows parallel programs written for shared memory multiprocessors to be executed in a distributed environment.
Reference: [4] <author> T. L. Casavant and J. G.Kuhl, </author> <title> "A Taxonomy of Scheduling in General-Purpose Distributed Computing Systems," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 14, no. 2, </volume> <pages> pp. 141-153, </pages> <year> 1988. </year>
Reference-contexts: It seems natural that load balancing could be achieved by dynamically changing the number of virtual processors that each physical processor has to emulate. According to the taxonomy given in <ref> [4] </ref>, our algorithm falls in the class of dynamic physically distributed cooperative load balancing algorithms. There is no one location where load information of all the processors is collected, but rather, all the processors cooperate in making the decision about load redistribution.
Reference: [5] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield, </author> <title> "The Amber System: Parallel Programming on a Network of Multiprocessors," </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 147-158, </pages> <year> 1989. </year>
Reference-contexts: More recently, however, significant work has been done to develop programming systems that would support development and execution of parallel applications across a network of workstations. Amber <ref> [5] </ref> is an object-based system with primitives for thread management and object mobility. Munin [3] is a distributed shared memory system that allows parallel programs written for shared memory multiprocessors to be executed in a distributed environment.
Reference: [6] <author> D. R. Cheriton and W. Zwaenepoel, </author> <title> "The Distributed V Kernel and its Performance for Disk-less Workstations," </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 129-140, </pages> <year> 1983. </year>
Reference-contexts: They provide high quality personalized computing along with resource sharing, and they offer unmatched price/performance ratios. In the first half of the 1980s, most of the research in the area of distributed computing was concentrated on building distributed operating systems such as V-Kernel <ref> [6] </ref>, Accent [16], Amoeba [19] and Eden [15]. Goscinski [11] describes a main goal of a distributed operating system to be the allocation of network resources that allows their use in the most effective way.
Reference: [7] <author> D. Conklin, J. Cleary, and B. Unger, </author> <title> "The Sharks World (a study in distributed simulation design)," </title> <journal> Distributed Simulation, </journal> <volume> vol. 22, </volume> <pages> pp. 157-160, </pages> <year> 1990. </year>
Reference-contexts: The program solves the equations using finite difference approximations. In its original form the program used a 64fi64 grid. We have increased the grid size to 128fi128 to increase the running time of the program. Program sharks implements the time-driven simulation algorithm proposed in <ref> [7] </ref>. The Dataparallel C implementation of this problem is presented in detail in [12]. We have measured the performance for the case when the number of sharks and fish is 2048, the total simulation period is 100000, and the time step is 10000.
Reference: [8] <author> R. J. Cypser, </author> <title> Communications for Cooperating Systems OSI, SNA and TCP/IP, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [9] <author> G. C. Fox, </author> <title> "What have we learnt from using real parallel machines to solve real problems?," </title> <booktitle> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pp. 897-955, </pages> <year> 1988. </year>
Reference-contexts: These low-level functions are similar to message-passing primitives for a distributed memory multicomputer. This paper focuses on data-parallel programming in a distributed computing environment. Data parallelism has proven to be the natural paradigm for a large number of problems in science and engineering <ref> [9, 10] </ref>. The programming language used is Dataparal-lel C, derived from the original C* language developed by Thinking Machines Corporation. Dataparal-lel C is a high-level SIMD language, free of message-passing primitives.
Reference: [10] <author> G. C. Fox, </author> <booktitle> "1989 The first year of the parallel supercomputer," Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers and Applications, </booktitle> <pages> pp. 1-37, </pages> <year> 1989. </year>
Reference-contexts: These low-level functions are similar to message-passing primitives for a distributed memory multicomputer. This paper focuses on data-parallel programming in a distributed computing environment. Data parallelism has proven to be the natural paradigm for a large number of problems in science and engineering <ref> [9, 10] </ref>. The programming language used is Dataparal-lel C, derived from the original C* language developed by Thinking Machines Corporation. Dataparal-lel C is a high-level SIMD language, free of message-passing primitives.
Reference: [11] <author> A. Goscinski, </author> <title> Distributed Operating Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: In the first half of the 1980s, most of the research in the area of distributed computing was concentrated on building distributed operating systems such as V-Kernel [6], Accent [16], Amoeba [19] and Eden [15]. Goscinski <ref> [11] </ref> describes a main goal of a distributed operating system to be the allocation of network resources that allows their use in the most effective way.
Reference: [12] <author> P. J. Hatcher and M. J. Quinn, </author> <title> Data-Parallel Programming on MIMD Computers, </title> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Dataparal-lel C is a high-level SIMD language, free of message-passing primitives. Prior publications have described the implementation of Dataparallel C compilers for Sequent shared memory multiprocessors and Intel and nCUBE distributed memory multicomputers <ref> [12, 13] </ref>. We have modified the multicomputer-targeted compiler and written a new run-time system that allows Dataparallel C programs to execute on a network of heterogeneous UNIX workstations. <p> Section 5 presents our preliminary experimental results, and Section 6 summarizes the contributions of this work. 2 Dataparallel C Dataparallel C is a variant of the original data-parallel programming language C* [17]. The Data-parallel C language is described in detail in <ref> [12] </ref>. <p> For example, the keyword block in line 6 of Figure 1 directs the compiler to map nearly square submatrices of the domain element matrix to the same physical processor. 3 Compiler Technology The starting point of our work was the implementation of Dataparallel C for hypercube multicomputers <ref> [12, 14] </ref>. The Dataparallel C compiler accepts a Data-parallel C program and produces a C program which is replicated on all physical processors. In that sense the compiler can be viewed as translating a SIMD specification into a SPMD implementation. <p> In its original form the program used a 64fi64 grid. We have increased the grid size to 128fi128 to increase the running time of the program. Program sharks implements the time-driven simulation algorithm proposed in [7]. The Dataparallel C implementation of this problem is presented in detail in <ref> [12] </ref>. We have measured the performance for the case when the number of sharks and fish is 2048, the total simulation period is 100000, and the time step is 10000. There is no processor interaction during the simulation, and therefore this program can achieve almost perfect efficiency. program Seq.
Reference: [13] <author> P. J. Hatcher, M. J. Quinn, A. J. Lapadula, B. K. Seevers, R. J. Anderson, and R. R. Jones, </author> <title> "Data-Parallel Programming on MIMD Computers," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 377-383, </pages> <year> 1991. </year>
Reference-contexts: Dataparal-lel C is a high-level SIMD language, free of message-passing primitives. Prior publications have described the implementation of Dataparallel C compilers for Sequent shared memory multiprocessors and Intel and nCUBE distributed memory multicomputers <ref> [12, 13] </ref>. We have modified the multicomputer-targeted compiler and written a new run-time system that allows Dataparallel C programs to execute on a network of heterogeneous UNIX workstations.
Reference: [14] <author> A. J. Lapadula, </author> <title> "An Optimizing Dataparallel C Cross-Compiler for Hypercube Multicomputers," </title> <type> Technical Report 92-04, </type> <institution> Department of Computer Science, University of New Hampshire, </institution> <year> 1992. </year>
Reference-contexts: For example, the keyword block in line 6 of Figure 1 directs the compiler to map nearly square submatrices of the domain element matrix to the same physical processor. 3 Compiler Technology The starting point of our work was the implementation of Dataparallel C for hypercube multicomputers <ref> [12, 14] </ref>. The Dataparallel C compiler accepts a Data-parallel C program and produces a C program which is replicated on all physical processors. In that sense the compiler can be viewed as translating a SIMD specification into a SPMD implementation.
Reference: [15] <author> E. D. Lazowska, H. M. Levy, G. T. Almes, M. J. Fisher, R. J. Fowler, and S. C. Vestal, </author> <title> "The Architecture of the Eden System," </title> <booktitle> Proceedings of the Eight Symposium on Operating Systems Principles, </booktitle> <pages> pp. 148-159, </pages> <year> 1981. </year>
Reference-contexts: In the first half of the 1980s, most of the research in the area of distributed computing was concentrated on building distributed operating systems such as V-Kernel [6], Accent [16], Amoeba [19] and Eden <ref> [15] </ref>. Goscinski [11] describes a main goal of a distributed operating system to be the allocation of network resources that allows their use in the most effective way.
Reference: [16] <author> R. F. Rashid and G. G. Robertson, </author> <title> "Accent: </title>
Reference-contexts: They provide high quality personalized computing along with resource sharing, and they offer unmatched price/performance ratios. In the first half of the 1980s, most of the research in the area of distributed computing was concentrated on building distributed operating systems such as V-Kernel [6], Accent <ref> [16] </ref>, Amoeba [19] and Eden [15]. Goscinski [11] describes a main goal of a distributed operating system to be the allocation of network resources that allows their use in the most effective way.
References-found: 16

