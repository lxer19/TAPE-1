URL: http://www.tc.cornell.edu/UserDoc/Software/PTools/mpi/paper_mpif.ps
Refering-URL: http://www.tc.cornell.edu/UserDoc/Software/PTools/mpi/
Root-URL: http://www.tc.cornell.edu
Email: email: ffrankeh,cwu,riviere,pratap,snirg@watson.ibm.com  
Title: MPI Programming Environment for IBM SP1/SP2  
Author: Hubertus Franke, C. Eric Wu, Michel Riviere, Pratap Pattnaik, Marc Snir 
Address: P.O. 218, Yorktown Heights, NY 10598.  
Affiliation: IBM T. J. Watson Research Center,  
Abstract: In this paper we discuss an implementation of the Message Passing Interface standard (MPI) for the IBM Scalable Power PARALLEL 1 and 2 (SP1, SP2). Key to a reliable and efficient implementation of a message passing library on these machines is the careful design of a UNIX-Socket like layer in the user space with controlled access to the communication adapters and with adequate recovery and flow control. The performance of this implementation is at the same level as the IBM-proprietary message passing library (MPL). We also show that in the IBM SP1 and SP2 we achieve integrated tracing ability, where both system events, such as context switches and page fault etc., and MPI related activities are traced, with minimal overhead to the application program, thus presenting application programmers the trace of all the events that ultimately affect efficiency of a parallel program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey, J. Barton, T. Lasinski, and H. Si-mon, </author> <title> The NAS Parallel Benchmarks, </title> <journal> Interna tional Journal of Supercomputer Applications, </journal> <volume> vol 5, </volume> <pages> pp. 63-73, </pages> <year> 1991. </year>
Reference: [2] <author> V. Bala et al., </author> <title> The IBM external user interface for scalable parallel systems, </title> <booktitle> Parallel Computing 20 (1994), </booktitle> <pages> pp. 445-462. </pages>
Reference-contexts: The parallel operating environment (POE) on the SP consists of several components: (1) communication libraries, (2) single parallel job management, (3) overall job management, and (4) tools. Including MPI-F, three high performance communication protocols are currently supported on the SP: MPL <ref> [2] </ref>, PVM [5], and MPI [10]. The communication library is linked with the application. Note however, that it is not a requirement to link with one of these libraries to execute a parallel job.
Reference: [3] <author> H. Davis, S. Goldschmidt, and J. Hennessy, </author> <title> Multiprocessor Simulation And Tracing Using Tango, </title> <booktitle> Proc. 1991 Int'l Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-99 II-107, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The AIX tracing facility allows the definition of user specific events to be generated with the same mechanism. Using this extension together with the name shifting provided by the MPI profiling interface was chosen to implement the UTE/MPI tracing library. Many trace systems such as those in <ref> [3, 6, 11] </ref> require source code modification to generate trace events. The UTE/MPI trace library, on the other hand, requires only re-linking for trace generation. If the application source code is available, additional user markers provided by the trace facility can be inserted into the source code.
Reference: [4] <author> N. Doss, W. Gropp, E. Lusk, and A. Skjellum, </author> <title> An Initial Implementation of MPI, </title> <type> Technical Report MCS-P393-1193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Even though public domain versions of MPI are available <ref> [4] </ref>, it is expected that parallel computer manufacturers will develop efficient implementations of MPI for their own hardware. <p> The first part of this paper discusses the design of our MPI implementation, called MPI-F, and it's integration with the SPx system software. We compare MPI-F performance with MPL, the proprietary library for the SP, and the public domain version of MPICH <ref> [4] </ref> running on top of MPL. MPI provides a simple but powerful profiling interface definition. <p> All other communication services, e.g. collective communication and virtual topologies services, are implemented on top of point-to-point message passing. In MPI-F we utilize public domain code <ref> [4] </ref>, but we are gradually switching to proprietary code.
Reference: [5] <author> A. Geist et al., </author> <title> PVM 3 user's guide and reference manual, </title> <type> Tech. rep. </type> <institution> ORNL/TM-1287, Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The parallel operating environment (POE) on the SP consists of several components: (1) communication libraries, (2) single parallel job management, (3) overall job management, and (4) tools. Including MPI-F, three high performance communication protocols are currently supported on the SP: MPL [2], PVM <ref> [5] </ref>, and MPI [10]. The communication library is linked with the application. Note however, that it is not a requirement to link with one of these libraries to execute a parallel job.
Reference: [6] <author> G. Geist, M. Heath, B. Peyton, and P. Wor-ley, </author> <title> A Users' Guide to PICL: A Portable Instrumented Communication, Library. </title> <type> Technical Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: The AIX tracing facility allows the definition of user specific events to be generated with the same mechanism. Using this extension together with the name shifting provided by the MPI profiling interface was chosen to implement the UTE/MPI tracing library. Many trace systems such as those in <ref> [3, 6, 11] </ref> require source code modification to generate trace events. The UTE/MPI trace library, on the other hand, requires only re-linking for trace generation. If the application source code is available, additional user markers provided by the trace facility can be inserted into the source code.
Reference: [7] <author> IBM, </author> <title> IBM AIX parallel environment operation and use, Manual SH26-7230, </title> <institution> IBM Corp., </institution> <month> Sept. </month> <year> 1993. </year>
Reference: [8] <author> G. Khermouch, </author> <title> Technology 1994: large computers, </title> <journal> IEEE Spectrum, </journal> <month> 31.1 </month> <year> (1994), </year> <pages> pp. 46-49. </pages>
Reference-contexts: and shows that, while one obtains the traces for system events such as page fault, process swap and I/O delays along with MPI related activities, the overhead encounted at application level is very small. 2 System Architecture The 9076 PowerPARALLEL System (SP1/SP2) is a distributed memory multiprocessor produced by IBM <ref> [8] </ref>. Each SP1/SP2 node consists of an RS/6000 processor. Each node runs a full copy of the AIX UNIX operating system.
Reference: [9] <author> L. Lamport, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM, July 1978, </journal> <volume> vol. 21, no. 7, </volume> <pages> pp. 558-565. </pages>
Reference-contexts: Since separate trace streams are produced independently by multiple processors in a SP system, the logical order of events cannot be guaranteed due to discrepancy among local clocks. This may lead to the partial ordering problem described in <ref> [9] </ref>. In the presence of a global clock, as is provided by the IBM SP1/SP2 communication switch, this problem could be completely avoided if all events use the global clock instead of the local clock.
Reference: [10] <author> MPI Forum, </author> <title> Document for a standard message-passing interface, </title> <type> Tech. Rep. </type> <institution> CS-93-214, University of Tennessee, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: MPI provides a rich set of both basic and advanced communication functions (a total of 132), which can be classified into one of the following categories: (1) point-to-point communication, (2) group management, (3) collective communication, (4) virtual topology, and (5) environment management; details of which can be found in <ref> [10] </ref>. The first part of this paper discusses the design of our MPI implementation, called MPI-F, and it's integration with the SPx system software. We compare MPI-F performance with MPL, the proprietary library for the SP, and the public domain version of MPICH [4] running on top of MPL. <p> The parallel operating environment (POE) on the SP consists of several components: (1) communication libraries, (2) single parallel job management, (3) overall job management, and (4) tools. Including MPI-F, three high performance communication protocols are currently supported on the SP: MPL [2], PVM [5], and MPI <ref> [10] </ref>. The communication library is linked with the application. Note however, that it is not a requirement to link with one of these libraries to execute a parallel job.
Reference: [11] <author> K. So, A. Bolmarcich, F. Darema, and V. </author> <title> Norton, </title>
Reference-contexts: The AIX tracing facility allows the definition of user specific events to be generated with the same mechanism. Using this extension together with the name shifting provided by the MPI profiling interface was chosen to implement the UTE/MPI tracing library. Many trace systems such as those in <ref> [3, 6, 11] </ref> require source code modification to generate trace events. The UTE/MPI trace library, on the other hand, requires only re-linking for trace generation. If the application source code is available, additional user markers provided by the trace facility can be inserted into the source code.
References-found: 11

