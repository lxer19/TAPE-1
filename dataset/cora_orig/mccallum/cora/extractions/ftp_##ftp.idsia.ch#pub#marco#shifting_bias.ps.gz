URL: ftp://ftp.idsia.ch/pub/marco/shifting_bias.ps.gz
Refering-URL: http://www.idsia.ch/~marco/publications.html
Root-URL: 
Title: Machine Learning,  Shifting Inductive Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement  
Author: J URGEN SCHMIDHUBER JIEYU ZHAO MARCO WIERING Editors: Lorien Pratt and Sebastian Thrun 
Keyword: inductive bias, reinforcement learning, reward acceleration, Levin search, success-story algorithm, incremental self-improvement  
Address: Corso Elvezia 36, CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Note: c 1997 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 28,  
Email: juergen@idsia.ch  jieyu@idsia.ch  marco@idsia.ch  
Date: 105-132 (1997)  
Abstract: We study task sequences that allow for speeding up the learner's average reward intake through appropriate shifts of inductive bias (changes of the learner's policy). To evaluate long-term effects of bias shifts setting the stage for later bias shifts we use the "success-story algorithm" (SSA). SSA is occasionally called at times that may depend on the policy itself. It uses backtracking to undo those bias shifts that have not been empirically observed to trigger long-term reward accelerations (measured up until the current SSA call). Bias shifts that survive SSA represent a lifelong success history. Until the next SSA call, they are considered useful and build the basis for additional bias shifts. SSA allows for plugging in a wide variety of learning algorithms. We plug in (1) a novel, adaptive extension of Levin search and (2) a method for embedding the learner's policy modification strategy within the policy itself (incremental self-improvement). Our inductive transfer case studies involve complex, partially observable environments where traditional reinforcement learning fails. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berry, D. A. and Fristedt, B. </author> <year> (1985). </year> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Boddy, M. and Dean, T. L. </author> <year> (1994). </year> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285. </pages>
Reference: <author> Caruana, R., Silver, D. L., Baxter, J., Mitchell, T. M., Pratt, L. Y., and Thrun, S. </author> <year> (1995). </year> <title> Learning to learn: </title> <booktitle> knowledge consolidation and transfer in inductive systems. Workshop held at NIPS-95, </booktitle> <address> Vail, CO, </address> <note> see http://www.cs.cmu.edu/afs/user/caruana/pub/transfer.html. </note>
Reference-contexts: 1. Introduction / Overview Fundamental transfer limitations. Inductive transfer of knowledge from one task solution to the next <ref> (e.g., Caruana et al. 1995, Pratt and Jennings 1996) </ref> requires the solutions to share mutual algorithmic information.
Reference: <author> Chaitin, G. </author> <year> (1969). </year> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159. </pages>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188. </pages> <publisher> AAAI Press, </publisher> <address> San Jose, California. </address>
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference-contexts: Partially observable maze problems. The next subsections will describe experiments validating the usefulness of LS, ALS, and SSA. To begin with, in an illustrative application with a partially observable maze that has more states and obstacles than those presented in other POE work (see, e.g., <ref> (Cliff and Ross, 1994) </ref>), we will show how LS by itself can solve POEs with large state spaces but low-complexity solutions (Q-learning variants fail to solve these tasks). Then we will present experimental case studies with multiple, more and more difficult tasks (inductive transfer).
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Dickmanns, D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1987). </year> <title> Der genetische Algorithmus: Eine Implementierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference: <author> Fogel, L., Owens, A., and Walsh, M. </author> <year> (1966). </year> <title> Artificial Intelligence through Simulated Evolution. </title> <address> Willey, New York. </address>
Reference: <author> Gittins, J. C. </author> <year> (1989). </year> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY. </address>
Reference: <author> Greiner, R. </author> <year> (1996). </year> <title> PALO: A probabilistic hill-climbing algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2). </volume>
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Kaelbling, L. </author> <year> (1993). </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press. </publisher>
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11. </pages>
Reference-contexts: Of course, LS and any other algorithm will fail to quickly solve problems whose solutions all have high algorithmic complexity. Unfortunately, almost all possible problems are of this kind <ref> (Kolmogorov, 1965, Chaitin, 1969, Solomonoff, 1964) </ref>. In fact, the realm of practical computer science is limited to solving the comparatively few tasks with low-complexity solutions. Fortunately such tasks are rather common in our regular universe. Practical implementation. <p> If nr-times-to-go = 0, set nr-times-to-go to nr-times. Note that nr-times = M AXR may cause an infinite loop. The Jump instruction is essential for exploiting the possibility that solutions may consist of repeatable action sequences and "subprograms", thus having low algorithmic complexity <ref> (Kolmogorov, 1965, Chaitin, 1969, Solomonoff, 1964) </ref>. LS' incrementally growing time limit automatically deals with those programs that do not halt, by preventing them from consuming too much time. As mentioned in section 3.1, the probability of a program is the product of the probabilities of its constituents.
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 313-324. </pages> <publisher> Addison Wesley Publishing Company. </publisher>
Reference: <author> Kumar, P. R. and Varaiya, P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Lenat, D. </author> <year> (1983). </year> <title> Theory formation by heuristic search. </title> <journal> Machine Learning, </journal> <volume> 21. </volume>
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: WIERING (the system's modifiable learning strategy is able to modify itself). Experimental case studies will involve complex environments where standard RL algorithms fail. Section 5 will conclude. 3. Implementation 1: Plugging LS into SSA Overview. In this section we introduce an adaptive extension of Levin search (LS) <ref> (Levin, 1973, Levin, 1984) </ref> as only learning action to be plugged into the basic cycle. <p> Levin Search (LS) Unbeknownst to many machine learning researchers, there exists a search algorithm with amazing theoretical properties: for a broad class of search problems, Levin search (LS) <ref> (Levin, 1973, Levin, 1984) </ref> has the optimal order of computational complexity. See (Li and Vitanyi, 1993) for an overview. See (Schmidhuber 1995, 1997a) for recent implementations/applications. Basic concepts. <p> Optimality. Given primitives representing a universal programming language, for a broad class of problems LS can be shown to be optimal with respect to total expected search time, leaving aside a constant factor independent of the problem size <ref> (Levin, 1973, Levin, 1984, Li and Vitanyi, 1993) </ref>. More formally: a problem is a symbol string that conveys all information about another symbol string called its solution, where the solution can be extracted by some (search) algorithm, given the problem.
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference-contexts: Levin Search (LS) Unbeknownst to many machine learning researchers, there exists a search algorithm with amazing theoretical properties: for a broad class of search problems, Levin search (LS) (Levin, 1973, Levin, 1984) has the optimal order of computational complexity. See <ref> (Li and Vitanyi, 1993) </ref> for an overview. See (Schmidhuber 1995, 1997a) for recent implementations/applications. Basic concepts. LS requires a set of n ops primitive, prewired instructions b 1 ; :::; b n ops that can be composed to form arbitrary sequential programs. <p> in order of their Levin complexities Kt (s) = min q flogD P (q) + log t (q; s)g, where q stands for a program that computes s in t (q; s) time steps, and D P (q) is the probability of guessing q according to a fixed Solomonoff-Levin distribution <ref> (Li and Vitanyi, 1993) </ref> on the set of possible programs (in section 3.2, however, we will make the distribution variable). Optimality. <p> A complex POE. Schmidhuber et al. (1996) describe two agents A and B living in a partially observable 600 fi 500 pixel environment with obstacles. They learn to solve a complex task that could not be solved by various TD () Q-learning variants <ref> (Lin, 1993) </ref>.
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution> <note> SHIFTING BIAS WITH SUCCESS-STORY ALGORITHM 131 Littman, </note> <author> M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J. A. M. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: A complex POE. Schmidhuber et al. (1996) describe two agents A and B living in a partially observable 600 fi 500 pixel environment with obstacles. They learn to solve a complex task that could not be solved by various TD () Q-learning variants <ref> (Lin, 1993) </ref>.
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 387-395. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Task. Figure 1 shows a 39 fi 38-maze with 952 free fields, a single start position (S) and a single goal position (G). The maze has more fields and obstacles than mazes used by previous authors working on POMs | for instance, McCallum's maze has only 23 free fields <ref> (McCallum, 1995) </ref>. The goal is to find a program that makes an agent move from S to G. Instructions. Programs can be composed from 9 primitive instructions. These instructions represent the initial bias provided by the programmer (in what follows, superscripts will indicate instruction numbers).
Reference: <author> Narendra, K. S. and Thathatchar, M. A. L. </author> <year> (1974). </year> <title> Learning automata a survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 4 </volume> <pages> 323-334. </pages>
Reference-contexts: This will increase the probability of the entire program. The probability adjustment is controlled by a learning rate fl (0 &lt; fl &lt; 1). ALS is related to the linear reward-inaction algorithm, e.g., <ref> (Narendra and Thathatchar, 1974, Kaelbling, 1993) </ref> | the main difference is: ALS uses LS to search through program space as opposed to single action space. As in the previous section, the probability distribution D P is determined by P . Initially, all P ij = 1 n ops .
Reference: <author> Pratt, L. and Jennings, B. </author> <year> (1996). </year> <title> A survey of transfer between connectionist networks. </title> <journal> Connection Science, </journal> <volume> 8(2) </volume> <pages> 163-184. </pages>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Rosenbloom, P. S., Laird, J. E., and Newell, A. </author> <year> (1993). </year> <title> The SOAR Papers. </title> <publisher> MIT Press. </publisher>
Reference: <author> Russell, S. and Wefald, E. </author> <year> (1991). </year> <title> Principles of Metareasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 361-395. </pages>
Reference-contexts: resource scenarios focuses on bandit problems, e.g., Berry and Fristedt (1985), Gittins (1989), and references therein: you have got a limited amount of money; how do you use it to figure out the expected return of certain simple gambling automata and exploit this knowledge to maximize your reward? See also <ref> (Russell and Wefald, 1991, Boddy and Dean, 1994, Greiner, 1996) </ref> for limited resource studies in planning contexts. Unfortunately the corresponding theorems are not applicable to our more general lifelong learning scenario. 2. This may be termed "metalearning" or "learning to learn".
Reference: <author> Schmidhuber, J. </author> <year> (1987). </year> <title> Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... </title> <type> hook. </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1991). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1993). </year> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1994). </year> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> Revised 1995. </note>
Reference-contexts: It is easy to show <ref> (Schmidhuber, 1994, 1996) </ref> that the current SSA call will have enforced the following "success-story criterion" SSC (t is the t in the most recent step SSA.2): R (t) &lt; t t 1 R (t) R (t 2 ) &lt; : : : &lt; t t V (t) SSC demands that each
Reference: <author> Schmidhuber, J. </author> <year> (1995). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Levin Search (LS) Unbeknownst to many machine learning researchers, there exists a search algorithm with amazing theoretical properties: for a broad class of search problems, Levin search (LS) (Levin, 1973, Levin, 1984) has the optimal order of computational complexity. See (Li and Vitanyi, 1993) for an overview. See <ref> (Schmidhuber 1995, 1997a) </ref> for recent implementations/applications. Basic concepts. LS requires a set of n ops primitive, prewired instructions b 1 ; :::; b n ops that can be composed to form arbitrary sequential programs. <p> Subsection 4.3 will mention additional IS experiments involving complex POEs and interacting learning agents that influence each other's task difficulties. 4.1. Policy and Program Execution Storage / Instructions. The learner makes use of an assembler-like programming language similar to but not quite as general as the one in <ref> (Schmidhuber, 1995) </ref>. It has n addressable work cells with addresses ranging from 0 to n 1. The variable, real-valued contents of the work cell with address k are denoted c k . Processes in the external environment occasionally write inputs into certain work cells.
Reference: <author> Schmidhuber, J. </author> <year> (1997a). </year> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. Neural Networks. </title> <publisher> In press. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1997b). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. In press. </publisher>
Reference: <author> Schmidhuber, J., Zhao, J., and Wiering, M. </author> <year> (1996). </year> <title> Simple principles of metalearning. </title> <type> Technical Report IDSIA-69-96, </type> <institution> IDSIA. </institution>
Reference-contexts: While this analysis is running, time is running, too. Thus, the complexity of the Bayesian approach is automatically taken into account. In section 3 we will actually plug in an adaptive Levin search extension. Similarly, actions may be calls of a Q-learning variant | see experiments in <ref> (Schmidhuber et al., 1996) </ref>. Plugging Q into SSA makes sense in situations where Q by itself is questionable because the environment might not satisfy the preconditions that would make Q sound. <p> Experiment 1: A Big Partially Observable Maze (POM) The current section is a prelude to section 3.5 which will address inductive transfer issues. Here we will only show that LS by itself can be useful for POE problems. See also <ref> (Wiering and Schmidhuber, 1996) </ref>. Task. Figure 1 shows a 39 fi 38-maze with 952 free fields, a single start position (S) and a single goal position (G). <p> In the beginning, the goal is found only every 300,000 basic cycles. Through self-modifications and SSA, however, within 130,000 trials (10 9 basic cycles) the average trial length decreases by a factor of 60 (mean of 4 simulations). Both agents learn to cooperate to accelerate reward intake. See <ref> (Schmidhuber et al., 1996) </ref> for details. Zero sum games. Even certain zero sum reward tasks allow for achieving success stories. <p> Both agents learn to cooperate to accelerate reward intake. See (Schmidhuber et al., 1996) for details. Zero sum games. Even certain zero sum reward tasks allow for achieving success stories. This has been shown in an experiment with three IS-based agents <ref> (Zhao and Schmidhuber, 1996) </ref>: each agent is both predator and prey; it receives reward 1 for catching its prey and reward -1 for being caught. Since all agents learn each agent's task gets more and more difficult over time.
Reference: <author> Schwefel, H. P. </author> <year> (1974). </year> <title> Numerische Optimierung von Computer-Modellen. Dissertation. </title> <note> Published 1977 by Birkhauser, Basel. </note>
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference-contexts: Inductive transfer of knowledge from one task solution to the next (e.g., Caruana et al. 1995, Pratt and Jennings 1996) requires the solutions to share mutual algorithmic information. Since almost all sequences of solutions to well-defined problems are incompressible and have maximal Kolmogorov complexity <ref> (Solomonoff, 1964, Kolmogorov, 1965, Chaitin, 1969, Li and Vitanyi, 1993) </ref>, arbitrary task solutions almost never share mutual information. This implies that inductive transfer and "generalization" are almost always impossible | see, e.g., Schmidhuber (1997a); for related results see Wolpert (1996).
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Return empty program fg. Here c and P hase MAX are prespecified constants. The procedure above is essentially the same (has the same order of complexity) as the one described in the second paragraph of this section | see, e.g., <ref> (Solomonoff, 1986, Li and Vitanyi, 1993) </ref>. 3.2. Adaptive Levin Search (ALS) LS is not necessarily optimal for "incremental" learning problems where experience with previous problems may help to reduce future search costs. To make an incremental search method out of non-incremental LS, we introduce a simple, 112 J. SCHMIDHUBER, J.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: Previous RL approaches. To deal with issues (1) and (2), what can we learn from traditional RL approaches? Convergence theorems for existing RL algorithms such as Q-learning (Watkins and Dayan, 1992) require infinite sampling size as well as strong (usually Markovian) assumptions about the environment, e.g., <ref> (Sutton, 1988, Watkins and Dayan, 1992, Williams, 1992) </ref>. They are of great theoretical interest but not extremely relevant to our realistic limited life case. For instance, there is no proof that Q-learning will converge within finite given time, not even in Markovian environments.
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, R., Carbonell, J., and Mitchell, T., editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 2, </volume> <pages> pages 163-190. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference-contexts: Our learner's single life lasts from time 0 to time T (time is not reset in case of new learning trials). Each modification of its policy corresponds to a shift of inductive bias <ref> (Utgoff, 1986) </ref>. By definition, "good" bias shifts are those that help to accelerate long-term average reward intake.
Reference: <author> Watanabe, O. </author> <year> (1992). </year> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: Despite this strong result, until recently LS has not received much attention except in purely theoretical studies | see, e.g., <ref> (Watanabe, 1992) </ref>. Of course, LS and any other algorithm will fail to quickly solve problems whose solutions all have high algorithmic complexity. Unfortunately, almost all possible problems are of this kind (Kolmogorov, 1965, Chaitin, 1969, Solomonoff, 1964).
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference-contexts: Previous RL approaches. To deal with issues (1) and (2), what can we learn from traditional RL approaches? Convergence theorems for existing RL algorithms such as Q-learning <ref> (Watkins and Dayan, 1992) </ref> require infinite sampling size as well as strong (usually Markovian) assumptions about the environment, e.g., (Sutton, 1988, Watkins and Dayan, 1992, Williams, 1992). They are of great theoretical interest but not extremely relevant to our realistic limited life case. <p> In a realistic application, however, the time consumed by a robot move would by far exceed the time consumed by a Jump instruction | we omit this (negligible) cost in the experimental results. Comparison. We compare LS to three variants of Q-learning <ref> (Watkins and Dayan, 1992) </ref> and random search. Random search repeatedly and randomly selects and executes one of the instructions (1-8) until the goal is hit (like with Levin search, the agent is reset to its start position whenever it hits the wall).
Reference: <author> Whitehead, S. and Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 409-419. </pages>
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address> <note> 132 J. </note> <author> SCHMIDHUBER, J. ZHAO AND M. WIERING Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>
Reference-contexts: Experiment 1: A Big Partially Observable Maze (POM) The current section is a prelude to section 3.5 which will address inductive transfer issues. Here we will only show that LS by itself can be useful for POE problems. See also <ref> (Wiering and Schmidhuber, 1996) </ref>. Task. Figure 1 shows a 39 fi 38-maze with 952 free fields, a single start position (S) and a single goal position (G).
Reference: <author> Wolpert, D. H. </author> <year> (1996). </year> <title> The lack of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1341-1390. </pages>
Reference: <author> Zhao, J. and Schmidhuber, J. </author> <year> (1996). </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books. </publisher> <month> Received August 21, </month> <note> 1996 Accepted February 24, 1997 Final Manuscript March 1, </note> <year> 1997 </year>
Reference-contexts: Both agents learn to cooperate to accelerate reward intake. See (Schmidhuber et al., 1996) for details. Zero sum games. Even certain zero sum reward tasks allow for achieving success stories. This has been shown in an experiment with three IS-based agents <ref> (Zhao and Schmidhuber, 1996) </ref>: each agent is both predator and prey; it receives reward 1 for catching its prey and reward -1 for being caught. Since all agents learn each agent's task gets more and more difficult over time.
References-found: 47

