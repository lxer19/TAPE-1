URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/COLT95.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: lksaul@psyche.mit.edu, singh@psyche.mit.edu  
Title: Markov Decision Processes in Large State Spaces  
Author: Lawrence K. Saul and Satinder P. Singh 
Address: 79 Amherst Street, E10-243 Cambridge, MA 02139  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: In this paper we propose a new framework for studying Markov decision processes (MDPs), based on ideas from statistical mechanics. The goal of learning in MDPs is to find a policy that yields the maximum expected return over time. In choosing policies, agents must therefore weigh the prospects of short-term versus long-term gains. We study a simple MDP in which the agent must constantly decide between exploratory jumps and local reward mining in state space. The number of policies to choose from grows exponentially with the size of the state space, N . We view the expected returns as defining an energy landscape over policy space. Methods from statistical mechanics are used to analyze this landscape in the thermodynamic limit N ! 1. We calculate the overall distribution of expected returns, as well as the distribution of returns for policies at a fixed Hamming distance from the optimal one. We briefly discuss the problem of learning optimal policies from empirical estimates of the expected return. As a first step, we relate our findings for the entropy to the limit of high-temperature learning. Numerical simulations support the theoretical results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. J. Amit. </author> <title> Modeling brain function. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: Real-world problems in decision and control necessarily involve a large number of degrees of freedom. Our motivation was to build a sta tistical mechanical framework for these problems, simi-lar to ones that exist for other problems in memory and learning <ref> [1, 12] </ref>. This paper does not achieve this goal, but should be viewed as a first step in this direction. The organization of the paper is as follows.
Reference: [2] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72: </booktitle> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: The goal of learning is to find a policy, or set of actions, that yields the maximum expected return over time. Successful strategies for learning must therefore look beyond immediate rewards and concentrate on long-term gains. There is a large literature on methods for finding optimal policies in MDPs <ref> [3, 2, 6] </ref>. If a model of the Markov environment is assumed known, then there exist classical methods such as value iteration for finding optimal policies in polynomial time [11]. For many problems of interest, a model of the environment is not available. <p> For many problems of interest, a model of the environment is not available. In this case, there are two basic strategies|direct and indirect|for finding optimal policies <ref> [2] </ref>. Direct methods attempt to learn good approximations to optimal policies without estimating a model of the environment. Reinforcement learning algorithms, such as TD () [10] and Q-learning [13, 14], are examples of direct methods.
Reference: [3] <author> D. P. Bertsekas. </author> <title> Dynamic programming: deterministic and stochastic models. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Many real-world tasks in machine learning, particularly those in navigation and control, require agents with decision-making abilities. Markov decision processes (MDPs) <ref> [3] </ref> provide a theoretical framework for modeling tasks in which an agent must constantly monitor its environment and take appropriate courses of action. The fundamental problem in MDPs is one of tempo ral credit assignment|determining which actions have important long-term consequences. <p> The goal of learning is to find a policy, or set of actions, that yields the maximum expected return over time. Successful strategies for learning must therefore look beyond immediate rewards and concentrate on long-term gains. There is a large literature on methods for finding optimal policies in MDPs <ref> [3, 2, 6] </ref>. If a model of the Markov environment is assumed known, then there exist classical methods such as value iteration for finding optimal policies in polynomial time [11]. For many problems of interest, a model of the environment is not available. <p> The appendix contains technical details of the calculations that appear in section 3. 2 Markov Decision Processes This section presents a brief review of MDPs, concentrating on those aspects most relevant to our work. A more thorough introduction may be found in <ref> [3] </ref>. 2.1 Background A Markov decision process (MDP) models an agent's environment by a set of N states. In each of these states, the agent is required to choose from a set of possible actions. <p> In particular, eq. (3) weights the reward at time t by fl t , setting an effective horizon time t = t=0 for the decision process. The Markov property leads to a recurrence relation for the value functions <ref> [3] </ref>: V X P ij V Note that the rewards R i and the transition probabili ties P ij implicitly depend on the policy through eqs. (1) and (2).
Reference: [4] <author> C. N. Fiechter. </author> <title> Efficient reinforcement learning. </title> <booktitle> In Proc. 7th Annual Workshop on Comput. Learning Theory, </booktitle> <pages> pages 88-97. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA 1994. </address>
Reference-contexts: Unlike direct methods, indirect methods attempt to estimate a model of the Markov environment and then derive control policies from the estimated model. Recently, Fiechter <ref> [4] </ref> has studied the problem of model estimation in MDPs and given a PAC-learning algorithm for finding near-optimal policies. In this paper we propose an alternative framework for studying MDPs, based on ideas from statistical mechanics.
Reference: [5] <author> D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <booktitle> In Proc. 7th Annual Workshop on Comput. Learning Theory, </booktitle> <pages> pages 76-87. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: In this paper we propose an alternative framework for studying MDPs, based on ideas from statistical mechanics. Our approach draws on previous work in statistical mechanics and computational learning theory <ref> [5, 9, 12] </ref>. We have not made an effort to be rigorous, relying instead on numerical simulations to check the soundness of our methods.
Reference: [6] <author> R. Howard. </author> <title> Dynamic programming and Markov processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1960. </year>
Reference-contexts: The goal of learning is to find a policy, or set of actions, that yields the maximum expected return over time. Successful strategies for learning must therefore look beyond immediate rewards and concentrate on long-term gains. There is a large literature on methods for finding optimal policies in MDPs <ref> [3, 2, 6] </ref>. If a model of the Markov environment is assumed known, then there exist classical methods such as value iteration for finding optimal policies in polynomial time [11]. For many problems of interest, a model of the environment is not available.
Reference: [7] <author> K. Huang. </author> <title> Statistical Mechanics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: The entropy can be calculated from eq. (15) by rewriting the sum as an integral and using the method of sad-dlepoint integration <ref> [7] </ref>. The details of this calculation are presented in the appendix. There we show that s (v) = min Z h 1 + e (rflv) i (17) Given a discount factor fl and a reward distribution (r), eq. (17) can be solved numerically for s (v). <p> Figure 1 shows a plot of s (v) for discount factors fl = 0:75 and fl = 0:95 and the box distribution (r) = 1 0 otherwise: (18) The asymptotic behavior of s (v) for (v fl v) t 1 may be calculated by performing a Sommerfeld expansion <ref> [7] </ref> in the parameter of eq. (17). The results s (v) ~ (v fl v) 1=2 ; (19) h p i are derived in the appendix for the box distribution of rewards, eq. (18).
Reference: [8] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: For fl &lt; 1, however, the magnitude of this error is trivially bounded by fl K (1 fl) 1 R, where K is the number of steps and R = max i jr i j. Stochastic exploration of this landscape may be done by Monte Carlo simulation <ref> [8] </ref>, in which policies are updated by the following rule. Let E denote the energy of the agent's current policy , and E 0 the energy of a nearby policy 0 with d H (; 0 ) = 1.
Reference: [9] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review A 45: </journal> <pages> 6056-6091, </pages> <year> 1992. </year>
Reference-contexts: In this paper we propose an alternative framework for studying MDPs, based on ideas from statistical mechanics. Our approach draws on previous work in statistical mechanics and computational learning theory <ref> [5, 9, 12] </ref>. We have not made an effort to be rigorous, relying instead on numerical simulations to check the soundness of our methods. <p> In the last part of the section, we discuss the problem of learning optimal policies from empirical estimates of the expected return. We relate our findings for the entropy to the well-known limit of high temperature learning <ref> [9] </ref>. Numerical evidence is presented to support the theoretical results. Finally, section 4 presents our conclusions and ideas for future work. <p> The results s (v) ~ (v fl v) 1=2 ; (19) h p i are derived in the appendix for the box distribution of rewards, eq. (18). The exponent of eq. (19) determines the asymptotic behavior for learning curves in the limit of high temperature learning <ref> [9] </ref>. <p> Our goal is not to introduce yet another algorithm for solving MDPs, but to study how inferences about the optimal policy improve with sample size, i.e. the number of trials available to the agent. For this purpose, we will borrow a framework from statistical mechanics <ref> [9] </ref>, originally developed to analyze learning curves in simple perceptrons. Our algorithm takes the following form. We suppose that, for each policy , the agent is allowed to determine an estimate of the expected return by executing m trials of the decision process. <p> In this paper, we therefore focus on a highly simplified limit in which this average is unnecessary: this is the combined limit of high temperatures and large sample size. In particular, let T = ~ff (finite): (32) This is the limit of high temperature learning <ref> [9] </ref>. In this limit, the quantity fiE in the exponent of the Gibbs distribution may be replaced by N ~ffv , so that the typical learning behavior becomes a function of the single parameter ~ff. <p> High temperature learning is clearly an artificial paradigm for learning in MDPs. The results are of interest, though, for two reasons: first, because they have analogs in the perceptron literature <ref> [9] </ref> that can guide our thinking, and second, because they provide a numerical check on the entropy curves of section 3.1.
Reference: [10] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In this case, there are two basic strategies|direct and indirect|for finding optimal policies [2]. Direct methods attempt to learn good approximations to optimal policies without estimating a model of the environment. Reinforcement learning algorithms, such as TD () <ref> [10] </ref> and Q-learning [13, 14], are examples of direct methods. The main results for these algorithms are that they converge with probability one in the limit of infinite experience; no rate of convergence results are available.
Reference: [11] <author> P. Tseng. </author> <title> Solving H-horizon, stationary Markov decision problems in time proportional to log(H), </title> <journal> Operations Research Letters, </journal> <volume> 9 </volume> <pages> 287-297, </pages> <year> 1990. </year>
Reference-contexts: There is a large literature on methods for finding optimal policies in MDPs [3, 2, 6]. If a model of the Markov environment is assumed known, then there exist classical methods such as value iteration for finding optimal policies in polynomial time <ref> [11] </ref>. For many problems of interest, a model of the environment is not available. In this case, there are two basic strategies|direct and indirect|for finding optimal policies [2]. Direct methods attempt to learn good approximations to optimal policies without estimating a model of the environment.
Reference: [12] <author> T. Watkin, A. Rau, and M. Biehl. </author> <title> The statistical mechanics of learning a rule. </title> <journal> Reviews of Modern Physics 65 </journal> <pages> 499-556, </pages> <year> 1993. </year>
Reference-contexts: In this paper we propose an alternative framework for studying MDPs, based on ideas from statistical mechanics. Our approach draws on previous work in statistical mechanics and computational learning theory <ref> [5, 9, 12] </ref>. We have not made an effort to be rigorous, relying instead on numerical simulations to check the soundness of our methods. <p> Real-world problems in decision and control necessarily involve a large number of degrees of freedom. Our motivation was to build a sta tistical mechanical framework for these problems, simi-lar to ones that exist for other problems in memory and learning <ref> [1, 12] </ref>. This paper does not achieve this goal, but should be viewed as a first step in this direction. The organization of the paper is as follows.
Reference: [13] <author> C. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <year> 1989. </year>
Reference-contexts: In this case, there are two basic strategies|direct and indirect|for finding optimal policies [2]. Direct methods attempt to learn good approximations to optimal policies without estimating a model of the environment. Reinforcement learning algorithms, such as TD () [10] and Q-learning <ref> [13, 14] </ref>, are examples of direct methods. The main results for these algorithms are that they converge with probability one in the limit of infinite experience; no rate of convergence results are available.
Reference: [14] <author> C. Watkins and P. </author> <title> Dayan. </title> <booktitle> Q-learning. Machine Learning 8: </booktitle> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In this case, there are two basic strategies|direct and indirect|for finding optimal policies [2]. Direct methods attempt to learn good approximations to optimal policies without estimating a model of the environment. Reinforcement learning algorithms, such as TD () [10] and Q-learning <ref> [13, 14] </ref>, are examples of direct methods. The main results for these algorithms are that they converge with probability one in the limit of infinite experience; no rate of convergence results are available.
References-found: 14

