URL: http://www.cs.cmu.edu/~wvcii/papers/CMU-CS-96-137.ps
Refering-URL: http://www.cs.cmu.edu/~wvcii/
Root-URL: 
Title: A Structured Approach to Redundant Disk Array Implementation  
Author: *William V. Courtright II, Garth Gibson, *Mark Holland, Jim Zelenka 
Date: Sept. 4-6, 1996.  10 June 1996  
Note: Appears in the Proceedings of the International Computer Performance and Dependability Symposium (IPDS),  This work was supported in part by Data General, DEC, EMC, Hewlett-Packard, IBM, Seagate, Storage Technology, and Symbios Logic. IBM and Symbios Logic provided student fellowships. CMUs Data Storage Systems Center also provided funding from the National Science Foundation under grant number ECD-8907068. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of sponsoring companies, agencies or the U.S. government.  CMU-CS-96-137  
Address: Pittsburgh, PA 15213  5000 Forbes Avenue, Pittsburgh, PA  
Affiliation: School of Computer Science Carnegie Mellon University  *Department of Electrical and Computer Engineering, School of Computer Science, Carnegie Mellon University,  
Abstract: Error recovery in redundant disk arrays is typically performed in an ad hoc fashion, requiring architecture-specific code which limits extensibility and is difficult to verify. In this paper, we describe a technique for automating the execution of redundant disk array operations, including recovery from errors, independent of array architecture. Our approach employs a graphical representation of array operations and a two-phase error-recovery scheme we refer to as roll-away error recovery. We demonstrate the validity of this approach in RAIDframe, a prototyping framework that separates architectural policy from execution mechanism. RAIDframe facilitates rapid prototyping of new RAID architectures by localizing modifications. In addition, RAIDframe-implemented architectures run the same code when configured as an event-driven simulator, a user-level application managing raw disks, and as a Digital Unix device-driver capable of mounting a filesystem. Evaluation shows that RAIDframe performance is equivalent to less complex array implementations and that case studies of RAID levels 0, 1, 4, 5, 6, and parity declustering achieve expected performance. 
Abstract-found: 1
Intro-found: 1
Reference: [Accetta86] <editor> M. Accetta, et. al Mach: </editor> <title> a new kernel foundation for Unix development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Workshop. </booktitle> <year> (1986) </year> <month> 93-113. </month>
Reference: [ATC90] <author> Product Description, </author> <title> RAID+ Series Model RX. Array Technology Corp., </title> <address> Boulder, CO (1990). </address>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92, Mogi94, Polyzois93, Solworth91, Stodol-sky94], array controller design and organization [Cao94, Drapeau94, Menon93], multiple fault tolerance <ref> [ATC90, Blaum94, STC94] </ref>, performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $9 billion this year and to surpass $13 billion in 1998.
Reference: [Bitton88] <author> D. Bitton and J. Gray. </author> <title> Disk shadowing. </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases. </booktitle> <month> (Sept. </month> <year> 1988) </year> <month> 331-338. </month>
Reference-contexts: read accesses to achieve the same performance in all architectures except RAIDframe, RD RAIDframe, WR Striper, RD Striper, WR 50 55 60 65 70 75 80 85 90 Throughput (IO/sec) 0 200 400 Response time (ms) RAID level 1, whose shortest queue discipline should improve throughput and decrease response time <ref> [Chen90b, Bitton88] </ref>. For fault-free small writes, we expect minimum average response time for the parity-based architectures to be about twice that of the direct write architectures (RAID levels 0 and 1).
Reference: [Blaum94] <author> M. Blaum, J. Brady, J. Bruck, and J. Menon. EVENODD: </author> <title> an optimal scheme for tolerating double disk failures in RAID architectures. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture. </booktitle> <address> Chicago (April 18-21, </address> <year> 1994) </year> <month> 245-254. </month>
Reference-contexts: These include designs for emphasizing improved write performance [Menon92, Mogi94, Polyzois93, Solworth91, Stodol-sky94], array controller design and organization [Cao94, Drapeau94, Menon93], multiple fault tolerance <ref> [ATC90, Blaum94, STC94] </ref>, performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Hartman93, Long94]. Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $9 billion this year and to surpass $13 billion in 1998.
Reference: [Brown72] <author> D. T. Brown, R. L Eibsen, and C. A. Thorn. </author> <title> Channel and direct access device architecture. </title> <journal> IBM Systems Journal 11(3). </journal> <year> (1972) </year> <month> 186-199. </month>
Reference-contexts: Approach 1.1 Modeling array operations as DAGs Our development of an access sequencing abstraction owes much to prior storage systems. A powerful approach for localizing access-specific control is suggested by IBMs channel command words and channel programs <ref> [Brown72] </ref>. Although this model serializes accesses, its success indicates that the range of primitives needed to support storage control is small and may directly correspond to operations provided by lower level devices. Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers [Long94].
Reference: [Cao94] <author> P. Cao, S.B. Lim, S. Venkataraman, and J. Wilkes, </author> <title> The TickerTAIP parallel RAID architecture, </title> <journal> ACM Transactions on Computer Systems 12(3). </journal> <month> (August </month> <year> 1994) </year> <month> 236-269. </month>
Reference-contexts: Popularized by the RAID taxonomy and driven by a broad spectrum of application demands for performance, dependability, capacity, and cost, a significant number of redundant disk architectures have been proposed. These include designs for emphasizing improved write performance [Menon92, Mogi94, Polyzois93, Solworth91, Stodol-sky94], array controller design and organization <ref> [Cao94, Drapeau94, Menon93] </ref>, multiple fault tolerance [ATC90, Blaum94, STC94], performance in the presence of failure [Holland92, Muntz90], and network-based RAID [Hartman93, Long94]. <p> Similar abstractions are found in SCSI chip control scripts [NCR91] and network-RAID node-to-node data transfers [Long94]. A more exible example of a storage control abstraction specifically developed for RAID architectures is the parallel state table approach in TickerTAIP, a distributed implementation of RAID level 5 <ref> [Cao94] </ref>. Our approach expands on the example of TickerTAIP, simplifying the expression of potentially concurrent orderings of operations by employing directed acyclic graphs (DAGs) of primitive actions [Courtright94].
Reference: [Chen90a] <author> P. Chen and D. Patterson, </author> <title> Maximizing performance in a striped disk array. </title> <booktitle> Proceedings of International Symposium on Computer Architecture. </booktitle> <year> (1990) </year> <month> 322-331. </month>
Reference-contexts: Finally, the importance of redundant disk arrays is evidenced by their pronounced growth in revenue, projected to exceed $9 billion this year and to surpass $13 billion in 1998. Architects currently evaluate novel disk array architectures using back-of-the-envelope analytical models and simulation experiments <ref> [Chen90a, Lee91] </ref>. In our experience, the specification and implementation of a detailed array simulator, even if it does not closely model a running system, is an arduous task. <p> We began with raidSim, a 92 file, 13,886 line detailed array simulator derived at Berkeley from an implementation of a functional device driver in the Sprite operating system <ref> [Chen90a, Lee91] </ref>. To this simulator we added new array functions, replaced previously unessential simple mechanisms, and augmented statistics recording, workload generation, and debugging functions. <p> This allowed us to apply identical, high-concurrency access sequences to each architecture executing in the simulation, user-level, and device driver environments. We constructed trace files which are broadly familiar to readers of RAID related papers: random small accesses all of the same type <ref> [Patterson88, Chen90a, Chen90b, Muntz90, Holland92] </ref>. Specifically, our tests use 1, 2, 5, 10, 15, 20, 30, and 40 threads to issue blocking 4 KB operations on random 4 KB aligned addresses with no intervening delay. We show results for 100% reads and 100% writes separately.
Reference: [Chen90b] <author> P. Chen, et. al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> (1990) </year> <month> 74-85. </month>
Reference-contexts: This allowed us to apply identical, high-concurrency access sequences to each architecture executing in the simulation, user-level, and device driver environments. We constructed trace files which are broadly familiar to readers of RAID related papers: random small accesses all of the same type <ref> [Patterson88, Chen90a, Chen90b, Muntz90, Holland92] </ref>. Specifically, our tests use 1, 2, 5, 10, 15, 20, 30, and 40 threads to issue blocking 4 KB operations on random 4 KB aligned addresses with no intervening delay. We show results for 100% reads and 100% writes separately. <p> read accesses to achieve the same performance in all architectures except RAIDframe, RD RAIDframe, WR Striper, RD Striper, WR 50 55 60 65 70 75 80 85 90 Throughput (IO/sec) 0 200 400 Response time (ms) RAID level 1, whose shortest queue discipline should improve throughput and decrease response time <ref> [Chen90b, Bitton88] </ref>. For fault-free small writes, we expect minimum average response time for the parity-based architectures to be about twice that of the direct write architectures (RAID levels 0 and 1).
Reference: [Chen94] <author> Chen, P. M., Lee, E. K., Gibson, G. A., Katz, R. H., and Patterson, D. A. </author> <title> RAID: High-performance, reliable secondary storage. </title> <booktitle> ACM Computing Surveys 26(2) (1994) 143-185. </booktitle>
Reference-contexts: Case studies In this section we present six array architectures implemented in RAIDframe. A full description of these architectures is beyond the scope of this paper, but excellent tutorials are widely available <ref> [Chen94, Holland94, RAB96] </ref>. 3.1 Setup, configuration, and workloads 100 MB/s Turbochannel bus. These adapters connect to five shelves of disks in a DEC StorageWorks 800 cabinet. Each shelf contains a DEC DWZZA fast-wide-differential to fast single-ended SCSI converter, and three one-gigabyte Hewlett-Packard model 2247 disk drives.
Reference: [Clarke94] <author> Clarke, E., Grumberg, O., and Long, D. </author> <title> Model checking. </title> <booktitle> Proceedings of the International Summer School on Deductive Program Design. Marktoberdorf, </booktitle> <address> Germany (July 26 - August 27, </address> <year> 1994). </year>
Reference-contexts: This localizes and delineates the code modified to experiment with RAID architectures. Second, because DAGs represent RAID architectures as well-defined connections of primitive actions, it should be possible to apply well-known verification techniques such as model checking <ref> [Clarke94] </ref> to demonstrate the correctness of an implementation.
Reference: [Courtright94] <author> W. V. Courtright II and G. A. Gibson. </author> <title> Backward error recovery in redundant disk arrays. </title> <booktitle> Proceedings of the 20th International Conference for the Resource Management and Performance Evaluation of Enterprise Computing Systems (CMG). </booktitle> <month> (December 4-9, </month> <year> 1994) </year> <month> 63-74. </month>
Reference-contexts: With this development cost in mind, we set out in 1994 to construct an array evaluation system that reduced the cost and complexity of experimenting with new array architectures. We advocate a graphical programming abstraction for use in redundant disk array development <ref> [Courtright94] </ref>. Given this structured representation, we are able to eliminate the need for architecture-specific error recovery code using an technique we refer to as roll-away error recovery. <p> Our approach expands on the example of TickerTAIP, simplifying the expression of potentially concurrent orderings of operations by employing directed acyclic graphs (DAGs) of primitive actions <ref> [Courtright94] </ref>. We believe that a graphical abstraction has the added benefit of compactly and visually conveying the essential ordering aspects of a RAID architecture. Using DAGs to model RAID operations has a number of additional advantages.
Reference: [Courtright96] <author> W. V. Courtright II, M. Holland, G. Gibson, L. N. Reilly, and J. Zelenka. </author> <note> RAIDframe: </note>
Reference-contexts: Representing RAID operations as directed acyclic graphs (DAGs), and using a simple state machine capable of executing operations represented as DAGs, we have constructed RAIDframe, a rapid pro-totyping framework for redundant disk arrays <ref> [Courtright96] </ref>. RAIDframe is designed to allow new array architectures to be implemented with small, localized changes to existing code. <p> By the time of publication, RAIDframe source code, including the library of array architecture described in this paper, are expected to be available for public use on our web pages (http://www.cs.c mu.edu/Web/Groups/PDL/). Included in this release is a users manual <ref> [Courtright96] </ref> which describes the installation and use of RAIDframe.
References-found: 12

