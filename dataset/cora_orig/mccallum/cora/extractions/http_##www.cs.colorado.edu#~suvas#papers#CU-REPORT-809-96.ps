URL: http://www.cs.colorado.edu/~suvas/papers/CU-REPORT-809-96.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Email: fsuvas,grunwaldg@cs.colorado.edu  
Title: Dependence Driven Execution for Data Parallelism  
Author: Suvas Vajracharya and Dirk Grunwald 
Note: This asynchrony allows high degree of parallelism.  
Address: Campus Box 430, Boulder, Colorado 80309  
Affiliation: University of Colorado,  
Abstract: This paper proposes an efficient run-time system to schedule general nested loops on multiprocessors. The work extends existing one-dimensional loop scheduling strategies such as static scheduling, affinity scheduling and various dynamic scheduling methods. The extensions are twofold. First, multiple independent loops as found in different branches of parbegin/parend constructs can be scheduled simultaneously. Secondly, multidimensional loops with dependencies and conditions can be aggressively scheduled. The ability to schedule multidimensional loops with dependencies is made possible by providing a dependence vector as an input to the scheduler. Based on this application-specific input, a continuation-passing run-time system using non-blocking threads efficiently orchestrates the parallelism on shared memory MIMD and DSM multicomputers. The run-time system uses a dependence-driven execution which is similar to data-driven and message-driven executions in that it is asynchronous. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6) </volume> <pages> 233-246, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by loop interchange <ref> [1, 28] </ref>. Other compiler transformations attempt to organize loops such that the loop with maximum parallelism is the outer loop. Compiler transformations such as loop normalization and loop fusion can also be used to collapse a multi-dimensional iteration space to a single dimensional iteration space.
Reference: 2. <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [2, 7, 3] </ref> could be used. In the proposed work, descriptors for iteration spaces are generated and manipulated during run-time by representing them using recursive structures called quad-trees. This allows (independent) portions of multiple loops to be executed in parallel.
Reference: 3. <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of teh ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 41-53, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [2, 7, 3] </ref> could be used. In the proposed work, descriptors for iteration spaces are generated and manipulated during run-time by representing them using recursive structures called quad-trees. This allows (independent) portions of multiple loops to be executed in parallel.
Reference: 4. <author> U. Banerjee. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> J. Supercomp., </journal> <volume> 2(2) </volume> <pages> 133-149, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: General loop scheduling is possible if the programmer or the compiler passes application-specific dependencies to the scheduler. Parallelizing compilers can make some of this information available in the form of dependence vectors <ref> [28, 4] </ref>. By passing this information to the run-time layer, a dependence-driven run-time system can exploit application-specific parallelism in a general, portable manner. The compiler describes the parallelism declaratively in the form of dependence vectors. All the procedural details of synchronization and scheduling are handled by the run-time system. <p> A compiler can compute this vector by taking the difference between the iteration vectors of the source and target iterations. A good description of this dependence analysis can be found in [28] <ref> [4] </ref>. Line 5 specifies the dependences for stencil computation. Data Parallel Operation: The runtime system handles the Data parallel operation and dependence satisfaction. When the user calls fireItUp as done in line 11, the data parallel operation begins.
Reference: 5. <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We are not aware of runtime systems that efficiently supports the execution of such independent loops in parallel. 2.2 The Chores Runtime Systems The Chores system [9] is implemented on top of Presto <ref> [5] </ref> and runs on the Sequent Symmetry. A per-processor worker (an user-level thread) grab chunks of work from a central queue using the guided self-scheduling method. As in the proposed work, the Chores system supports loop iterations with dependencies by using data-level synchronization.
Reference: 6. <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Programmers need to lock the user-defined data structures to atomically decrement the counters that keep track of the number of unsatisfied precedence arcs. The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk <ref> [6] </ref>, Mentat [14, 13] and Tam [8], but also suffers from the disadvantages of forcing users to use a low-level abstractions that these system provide. Users are responsible for decomposition and scheduling using the given abstractions. 2.
Reference: 7. <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [2, 7, 3] </ref> could be used. In the proposed work, descriptors for iteration spaces are generated and manipulated during run-time by representing them using recursive structures called quad-trees. This allows (independent) portions of multiple loops to be executed in parallel.
Reference: 8. <author> D.E. Culler, A. Sah, K.E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [6], Mentat [14, 13] and Tam <ref> [8] </ref>, but also suffers from the disadvantages of forcing users to use a low-level abstractions that these system provide. Users are responsible for decomposition and scheduling using the given abstractions. 2.
Reference: 9. <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support fo shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Loops that appear in different branches parbegin/parend constructs are also inde-pendent and therefore can be scheduled simultaneously. We are not aware of runtime systems that efficiently supports the execution of such independent loops in parallel. 2.2 The Chores Runtime Systems The Chores system <ref> [9] </ref> is implemented on top of Presto [5] and runs on the Sequent Symmetry. A per-processor worker (an user-level thread) grab chunks of work from a central queue using the guided self-scheduling method. As in the proposed work, the Chores system supports loop iterations with dependencies by using data-level synchronization.
Reference: 10. <author> Zhixi Fang, Peiyi Tang, Pen chung Yew, and Chuan qi Zhu. </author> <title> Dynamic processor self-scheduling for general parallel nested loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 919-929, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The runtime system manipulates this structure to coalesce or divide the iteration space during scheduling. As we will describe, this allows the proposed run-time system to schedule multi-dimensional loops with dependencies across iterations and across dimensions. Runtime Methods: Fang et. al. <ref> [10] </ref> described a two-level scheduling scheme for general arbitrarily nested parallel loops. Each loop doser, doall or doacross is termed a task. An extremely large-grain data-flow graph, where each node is an entire loop, is created.
Reference: 11. <author> D. Gannon and J.K. Lee. </author> <title> Object oriented parallelism. </title> <booktitle> In Proceedings of 1991 Japan Society for Parallel Processing, </booktitle> <pages> pages 13-23, </pages> <year> 1991. </year>
Reference-contexts: In the example, this is done in lines 2-3 of Figure 2. A programmer or compiler chooses the decomposition method such as by BLOCK or CYCLIC. The decomposition methods are similar to the data decomposition and distribution utilities available in HPF [16] and pC++ <ref> [11] </ref>. One important difference, however, is that in the process of data decomposition, DUDE takes flat data and creates objects called Iterates which is a tuple consisting of both data and operation. Iterates defines the granularity of parallelism in DUDE.
Reference: 12. <author> Susan Graham, Steven Lucco, and Oliver Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> April </month> <year> 1993. </year> <note> ACM, ACM. </note>
Reference-contexts: The current size of the batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [26], tapering <ref> [12, 20] </ref>, and safe self-scheduling [19]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling [21], which takes affinity of data to processors into account.
Reference: 13. <author> A. S. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [6], Mentat <ref> [14, 13] </ref> and Tam [8], but also suffers from the disadvantages of forcing users to use a low-level abstractions that these system provide. Users are responsible for decomposition and scheduling using the given abstractions. 2.
Reference: 14. <author> A. S. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <booktitle> IEEE Parallel and Distributed Technology: Systems and Applications, </booktitle> <pages> pages 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The add-atom () library call provides the means to schedule new enabled work. Dynamic Chores has the same benefits of flexibility and generality as Cilk [6], Mentat <ref> [14, 13] </ref> and Tam [8], but also suffers from the disadvantages of forcing users to use a low-level abstractions that these system provide. Users are responsible for decomposition and scheduling using the given abstractions. 2.
Reference: 15. <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: In contrast the Jacobi method, for example, can be scheduled without using barriers in DUDE. Furthermore, DUDE presents a distributed scheduler whereas the Chores system uses a centralized. 3 The DUDE Run-time System The DUDE runtime system is built on the top of an existing thread library, AWESIME <ref> [15] </ref>, a thread library implemented in C++. Together they provide a powerful way to take advantage of both task and data parallelism in applications. In this paper, we concentrate on DUDE's data parallelism since this is the most novel aspect of the runtime system.
Reference: 16. <author> High Performance Fortran Forum HPFF. </author> <title> Draft high performance fortran specificition, </title> <note> version 0.4. In Proceedings of 1991 Japan Society for Parallel Processing, page Available from anonymous ftp site titan.cs.rice.edu, </note> <year> 1992. </year>
Reference-contexts: In the example, this is done in lines 2-3 of Figure 2. A programmer or compiler chooses the decomposition method such as by BLOCK or CYCLIC. The decomposition methods are similar to the data decomposition and distribution utilities available in HPF <ref> [16] </ref> and pC++ [11]. One important difference, however, is that in the process of data decomposition, DUDE takes flat data and creates objects called Iterates which is a tuple consisting of both data and operation. Iterates defines the granularity of parallelism in DUDE.
Reference: 17. <author> S. F. Hummel, Edith Schonberg, and L. E. Flynn. </author> <title> Factoring, a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In guided-self scheduling [22], the number of iterations assigned to a processor is a function of number iterations left to be scheduled. A variation on this method is factoring <ref> [17] </ref> where at each scheduling operation the scheduler computes the size of a batch of chunks with the motivation to reduce the number of scheduling operations. The current size of the batch is exactly half of the previous batch.
Reference: 18. <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-10016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The simplest dynamic method is self-scheduling [25] where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking <ref> [18] </ref> was proposed, where each process grabs chunks of K iterations instead of one iteration. If the processors do not start simultaneously or if one processor gets an expensive chunk, the potential for load imbalance still exists.
Reference: 19. <author> Liu and et al. </author> <title> Scheduling parallel loops with variable length iteration execution times on parallel computers. </title> <booktitle> In Proc. 5th Intl. Conf. Parallel and Distributed Computing and System, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [26], tapering [12, 20], and safe self-scheduling <ref> [19] </ref>. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling [21], which takes affinity of data to processors into account.
Reference: 20. <author> Steven Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In Proceedings of ACM SIGPLAN '92 Conference on Porgramming Language Design and Implementation, </booktitle> <pages> pages 200-211. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [26], tapering <ref> [12, 20] </ref>, and safe self-scheduling [19]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling [21], which takes affinity of data to processors into account.
Reference: 21. <author> E.P Markatos and T. J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared Memory Multiprocessors. </title> <booktitle> In Intl. Conference on Parallel Processing, </booktitle> <pages> pages 258-257, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [26], tapering [12, 20], and safe self-scheduling [19]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling <ref> [21] </ref>, which takes affinity of data to processors into account. This scheme is most like chunking except that chunks have an affinity to a particular processor Another difference is that chunking is a centralized algorithm while affinity scheduling is a distributed algorithm based on work stealing.
Reference: 22. <author> C. D. Polochronopoulous and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If the processors do not start simultaneously or if one processor gets an expensive chunk, the potential for load imbalance still exists. In guided-self scheduling <ref> [22] </ref>, the number of iterations assigned to a processor is a function of number iterations left to be scheduled.
Reference: 23. <author> C. D. Polychronopoulos. </author> <title> Loop coalescing: A compiler transformation for parallel machines. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: Static Methods: A special case of multiply-nested loops are the one-way (or perfectly) nested doall loops in which there exists exactly one loop at each nest level. Given a one-way nested doall loop, a compile time transformation, loop coalescing, <ref> [23] </ref> can be used to coalesce m doall loops into a single doall with N = Q m i=1 (N i ) iterations. This transformation then allows the application of the methods described in the previous section.
Reference: 24. <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In the quadtree decomposition, each subdivision is a block divided into four equal-sized parts. Alternatively, at each subdivision, the block could be divided into two parts as in bintree. A comprehensive description of these data-structures can be found in <ref> [24] </ref>. These structure solve the problem of inefficient indexing into the pools of Iterates because the chunk sizes are uniform at any given level. Given the level and the index, Iterates from a pool can be retrieved by simply using random access.
Reference: 25. <author> P. Tang and P.C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages 528-535. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1986. </year>
Reference-contexts: This scheme is simple and there is no runtime scheduling overhead. The disadvantage, however, is that the method cannot adapt to dynamic fluctuation of the load in the application. Dynamic Scheduling: Dynamic assignment of iterations to processors can achieve better load balance. The simplest dynamic method is self-scheduling <ref> [25] </ref> where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking [18] was proposed, where each process grabs chunks of K iterations instead of one iteration.
Reference: 26. <author> T.H. Tzen and L.M. Ni. </author> <title> Trapezoid self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions Parrallel Distributed Systems, </journal> <volume> 4 </volume> <pages> 87-98, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch. Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling <ref> [26] </ref>, tapering [12, 20], and safe self-scheduling [19]. Hybrid Methods: A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling [21], which takes affinity of data to processors into account.
Reference: 27. <author> Chien-Min Wang and Sheng-De Wang. </author> <title> A hybrid scheme for efficiently executing loops on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 625-637, </pages> <year> 1992. </year>
Reference-contexts: Hybrid Methods: An optimized compile/run-time version of loop interchange and loop distribution called IGSS and MGSS was proposed in <ref> [27] </ref>. These schemes also allowed the scheduling of hybrid loops consisting of doser and doalls. This method is useful in cases where the number of iterations of a doall loop is much larger than the number of processors involved.
Reference: 28. <author> M.J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana, </institution> <month> April </month> <year> 1987. </year> <title> Rep. 329. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: General loop scheduling is possible if the programmer or the compiler passes application-specific dependencies to the scheduler. Parallelizing compilers can make some of this information available in the form of dependence vectors <ref> [28, 4] </ref>. By passing this information to the run-time layer, a dependence-driven run-time system can exploit application-specific parallelism in a general, portable manner. The compiler describes the parallelism declaratively in the form of dependence vectors. All the procedural details of synchronization and scheduling are handled by the run-time system. <p> This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by loop interchange <ref> [1, 28] </ref>. Other compiler transformations attempt to organize loops such that the loop with maximum parallelism is the outer loop. Compiler transformations such as loop normalization and loop fusion can also be used to collapse a multi-dimensional iteration space to a single dimensional iteration space. <p> A compiler can compute this vector by taking the difference between the iteration vectors of the source and target iterations. A good description of this dependence analysis can be found in <ref> [28] </ref> [4]. Line 5 specifies the dependences for stencil computation. Data Parallel Operation: The runtime system handles the Data parallel operation and dependence satisfaction. When the user calls fireItUp as done in line 11, the data parallel operation begins.
References-found: 28

