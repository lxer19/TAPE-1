URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-251.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-251/
Root-URL: http://www.cs.dartmouth.edu
Title: Disk-directed I/O for an Out-of-core Computation  
Author: David Kotz 
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Note: Available at  This research was supported by Dartmouth College, by NSF under grant number CCR 9404919, and by NASA Ames Research Center under Agreement Number NCC 2-849.  
Pubnum: Technical Report PCS-TR95-251  
Email: dfk@cs.dartmouth.edu  
Date: January 13, 1995  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR95-251.ps.Z  
Abstract: New file systems are critical to obtain good I/O performance on large multiprocessors. Several researchers have suggested the use of collective file-system operations, in which all processes in an application cooperate in each I/O request. Others have suggested that the traditional low-level interface (read, write, seek) be augmented with various higher-level requests (e.g., read matrix), allowing the programmer to express a complex transfer in a single (perhaps collective) request. Collective, high-level requests permit techniques like two-phase I/O and disk-directed I/O to significantly improve performance over traditional file systems and interfaces. Neither of these techniques have been tested on anything other than simple benchmarks that read or write matrices. Many applications, however, intersperse computation and I/O to work with data sets that cannot fit in main memory. In this paper, we present the results of experiments with an "out-of-core" LU-decomposition program, comparing a traditional interface and file system with a system that has a high-level, collective interface and disk-directed I/O. We found that a collective interface was awkward in some places, and forced additional synchronization. Nonetheless, disk-directed I/O was able to obtain much better performance than the traditional system. 
Abstract-found: 1
Intro-found: 1
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [CFH + 94], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [dBC93]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. <p> = min (first col) over all processors; ncols = sum (ncols) over all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>, or generated by a smart compiler [CC94, TBC94, BTC94]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year> <month> 14 </month>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [CFH + 94], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [dBC93]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. <p> = min (first col) over all processors; ncols = sum (ncols) over all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>, or generated by a smart compiler [CC94, TBC94, BTC94]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: In these experiments, we ran the program in Figure 2 with both the "traditional caching" file system (Figure 3a) and the disk-directed file system (Figure 3b), on top of our parallel file-system simulator [Kot94]. This simulator ran on top of the 7 Proteus parallel-architecture simulator <ref> [BDCW91] </ref>, which in turn ran on a DEC-5000 workstation. We configured Proteus as in [Kot94], except as noted below. Simulation overhead limited our experiments to decomposing a 1024 fi 1024 matrix of single-precision numbers, using eight compute processors (CPs), eight I/O processors (IOPs), and eight disks (one on each IOP).
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs <ref> [LIN + 93, BGST93] </ref>, Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload.
Reference: [BTC94] <author> Rajesh Bordawekar, Rajeev Thakur, and Alok Choudhary. </author> <title> Efficient compilation of out-of-core data parallel programs. </title> <type> Technical Report SCCS-622, </type> <institution> NPAC, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library [GGL93, BdC93, BBS + 94, SW94], or generated by a smart compiler <ref> [CC94, TBC94, BTC94] </ref>. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library [GGL93, BdC93, BBS + 94, SW94], or generated by a smart compiler <ref> [CC94, TBC94, BTC94] </ref>. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta <ref> [CF94] </ref>, nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. <p> This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the CHARISMA project. Higher-level interfaces, such as specifying a strided series of requests [NK94, Cra94] or accessing data through a mapping function <ref> [CF94, DdR92, Kot93] </ref> provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system.
Reference: [CFH + 94] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Technical Report RC 19841 (87784), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> November </month> <year> 1994. </year> <note> Version 0.2. </note>
Reference-contexts: Unfortunately, few multiprocessor file systems provide a collective interface. CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time, operating system, and hardware. The MPI message-passing interface may soon be extended to include I/O <ref> [CFH + 94] </ref>, including collective I/O. Finally, there are several libraries for collective matrix I/O [GGL93, BdC93, BBS + 94, SW94]. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [dBC93]. <p> In our LU-decomposition example the code needed some careful structuring to ensure that all processes participated in all I/O requests. Clearly, a collective interface that supported subsets of processes would reduce the need to structure the code this way (the MPI-IO proposal <ref> [CFH + 94] </ref> appears to have this support). Otherwise, any of the common collective matrix-I/O interfaces could be adapted for use. Ultimately, more cases need to be studied to determine an appropriate general-purpose interface. Thus disk-directed I/O was successful for out-of-core computations, despite the additional synchronization of a collective interface.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dart-mouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Part of this improvement is because the application could make better use of the memory to reduce I/O demands (many I/O algorithms do asymptotically less I/O given more memory <ref> [CK93] </ref>), and part is because the larger request sizes enable disk-directed I/O to better optimize the I/O. In summary, disk-directed I/O often improved the performance of the LU-decomposition program. In a random layout, it was able to optimize the order of disk access within each disk-directed request.
Reference: [Cra94] <institution> Cray Research. </institution> <note> listio manual page, 1994. Publication SR-2012. </note>
Reference-contexts: This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the CHARISMA project. Higher-level interfaces, such as specifying a strided series of requests <ref> [NK94, Cra94] </ref> or accessing data through a mapping function [CF94, DdR92, Kot93] provide valuable semantic information to the file system, which can then be used for optimization purposes.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [CFH + 94], including collective I/O. Finally, there are several libraries for collective matrix I/O [GGL93, BdC93, BBS + 94, SW94]. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface <ref> [dBC93] </ref>. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. When reading, the compute processors cooperate to read a matrix in a "conforming distribution", chosen for best I/O performance, and then the data is redistributed to its ultimate destination.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Proceedings of the Eleventh Annual IEEE International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE <ref> [DdR92] </ref>, TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. <p> This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the CHARISMA project. Higher-level interfaces, such as specifying a strided series of requests [NK94, Cra94] or accessing data through a mapping function <ref> [CF94, DdR92, Kot93] </ref> provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system.
Reference: [dHC94] <author> Juan Miguel del Rosario, Michael Harry, and Alok Choudhary. </author> <title> The design of VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <type> Technical Report SCCS-628, </type> <institution> NPAC, Syracuse, </institution> <address> NY 13244, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS <ref> [dHC94] </ref>. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload. The CHARISMA project traced production parallel scientific computing workloads on an Intel iPSC/860 [KN94] and on a TMC CM-5 [PEK + 94] to characterize their file-system activity.
Reference: [DSE88] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge <ref> [DSE88] </ref>, Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94].
Reference: [FBD95] <author> Craig S. Freedman, Josef Burger, and David J. Dewitt. </author> <title> SPIFFI | a scalable parallel file system for the Intel Paragon. </title> <note> Submitted to IEEE TPDS. </note>
Reference-contexts: There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI <ref> [FBD95] </ref>. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload.
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: The use of disk striping [SGM86] to access many disks in parallel has alleviated some of the hardware limitations by providing greater capacity, bandwidth, and throughput. Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance <ref> [FPD93, Nit92] </ref>. Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O [Kot94] to improve performance by orders of magnitude. In [Kot94], however, experiments were limited to simple benchmarks that read or wrote matrices.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [CFH + 94], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [dBC93]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. <p> = min (first col) over all processors; ncols = sum (ncols) over all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>, or generated by a smart compiler [CC94, TBC94, BTC94]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload. The CHARISMA project traced production parallel scientific computing workloads on an Intel iPSC/860 <ref> [KN94] </ref> and on a TMC CM-5 [PEK + 94] to characterize their file-system activity. In both cases, applications accessed large files (megabytes or gigabytes in size) using 2 surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes). <p> While this interface is comfortable to parallel programmers familiar with sequential programming, it is inadequate for expressing their needs <ref> [KN94] </ref>. Given this interface and the amount of interprocessor spatial locality arising from interleaving tiny requests 13 from many processors, caching is essential for reasonable performance [KN94]. <p> While this interface is comfortable to parallel programmers familiar with sequential programming, it is inadequate for expressing their needs <ref> [KN94] </ref>. Given this interface and the amount of interprocessor spatial locality arising from interleaving tiny requests 13 from many processors, caching is essential for reasonable performance [KN94]. A file system based on traditional caching, however, can have terrible performance [Nit92] and, as we show in this paper, can have counter-intuitive performance characteristics (increasing the block size from 4 KB to 8 KB, or increasing the slab size from 16 to 32 columns, sometimes decreased performance).
Reference: [Kot93] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year> <month> 15 </month>
Reference-contexts: This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the CHARISMA project. Higher-level interfaces, such as specifying a strided series of requests [NK94, Cra94] or accessing data through a mapping function <ref> [CF94, DdR92, Kot93] </ref> provide valuable semantic information to the file system, which can then be used for optimization purposes. Interfaces that allow the programmer to express collective I/O activity, in which all processes cooperate to make a single, large request, provide even more semantic information to the file system.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Sympo--sium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance [FPD93, Nit92]. Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O <ref> [Kot94] </ref> to improve performance by orders of magnitude. In [Kot94], however, experiments were limited to simple benchmarks that read or wrote matrices. In this paper we evaluate the performance of disk-directed I/O on a much more complex program, an out-of-core LU-decomposition program. <p> Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O <ref> [Kot94] </ref> to improve performance by orders of magnitude. In [Kot94], however, experiments were limited to simple benchmarks that read or wrote matrices. In this paper we evaluate the performance of disk-directed I/O on a much more complex program, an out-of-core LU-decomposition program. <p> When writing, the data is first redistributed and then written in a conforming distribution. There are no published performance results for two-phase writing, or for an out-of-core application using two-phase I/O. Disk-directed I/O. Disk-directed I/O is a technique for optimizing data transfer given a high-level, collective interface <ref> [Kot94] </ref>. <p> In experiments with reading and writing one- and two-dimensional matrices, disk-directed I/O was as much as 18 times faster than traditional caching in some access patterns, and was never slower <ref> [Kot94] </ref>. 3 LU decomposition LU decomposition represents the bulk of the effort in one technique for solving linear systems of equations. An N fi N matrix M is decomposed into two matrices, a lower-triangular matrix L and an upper-triangular matrix U , such that LU = M . <p> In these experiments, we ran the program in Figure 2 with both the "traditional caching" file system (Figure 3a) and the disk-directed file system (Figure 3b), on top of our parallel file-system simulator <ref> [Kot94] </ref>. This simulator ran on top of the 7 Proteus parallel-architecture simulator [BDCW91], which in turn ran on a DEC-5000 workstation. We configured Proteus as in [Kot94], except as noted below. <p> 2 with both the "traditional caching" file system (Figure 3a) and the disk-directed file system (Figure 3b), on top of our parallel file-system simulator <ref> [Kot94] </ref>. This simulator ran on top of the 7 Proteus parallel-architecture simulator [BDCW91], which in turn ran on a DEC-5000 workstation. We configured Proteus as in [Kot94], except as noted below. Simulation overhead limited our experiments to decomposing a 1024 fi 1024 matrix of single-precision numbers, using eight compute processors (CPs), eight I/O processors (IOPs), and eight disks (one on each IOP). <p> While this cache may seem small, it is consistent with the size of the system and problem, and with our previous experiments <ref> [Kot94] </ref>. In the disk-directed file system, the IOPs allocated two one-block buffers per disk (for double-buffering each disk), or 16 blocks of total buffer space. <p> The former would require a very large cache, and the latter would have the effect of spreading out simultaneous multi-block requests into multiple localities, counteracting the benefits of the contiguous layout <ref> [Kot94] </ref>. The results of experiments with 1 KB blocks support this statement. <p> As we show here and in <ref> [Kot94] </ref>, disk-directed I/O can lead to much better performance than traditional caching. This paper shows that disk-directed I/O, using a collective, high-level interface, could be used effectively for an out-of-core LU-decomposition computation. The additional synchronization of the collective interface appeared not to be a significant factor here.
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System <ref> [Kri94] </ref>, and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs <ref> [LIN + 93, BGST93] </ref>, Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload.
Reference: [MS94] <author> Steven A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS <ref> [MS94] </ref> and VIP-FS [dHC94]. All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The use of disk striping [SGM86] to access many disks in parallel has alleviated some of the hardware limitations by providing greater capacity, bandwidth, and throughput. Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance <ref> [FPD93, Nit92] </ref>. Recent work shows that if an application could make high-level, collective I/O requests, the file system can optimize I/O transfers using disk-directed I/O [Kot94] to improve performance by orders of magnitude. In [Kot94], however, experiments were limited to simple benchmarks that read or wrote matrices. <p> Given this interface and the amount of interprocessor spatial locality arising from interleaving tiny requests 13 from many processors, caching is essential for reasonable performance [KN94]. A file system based on traditional caching, however, can have terrible performance <ref> [Nit92] </ref> and, as we show in this paper, can have counter-intuitive performance characteristics (increasing the block size from 4 KB to 8 KB, or increasing the slab size from 16 to 32 columns, sometimes decreased performance).
Reference: [NK94] <author> Nils Nieuwejaar and David Kotz. </author> <title> A multiprocessor extension to the conventional file system interface. </title> <type> Technical Report PCS-TR94-230, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: In both cases, applications accessed large files (megabytes or gigabytes in size) using 2 surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes). On further examination, we discovered that most of the files were accessed in complex yet highly regular patterns <ref> [NK94] </ref>, most likely due to accessing multidimensional matrices. Interfaces. Most parallel file systems present the traditional abstraction of a file as a sequence of bytes with Unix interface semantics, and add a few extensions to control the behavior of an implicit file pointer shared among the processes. <p> This low-level interface, which restricts each request to a contiguous portion of the file, is one reason for the predominance of small requests found by the CHARISMA project. Higher-level interfaces, such as specifying a strided series of requests <ref> [NK94, Cra94] </ref> or accessing data through a mapping function [CF94, DdR92, Kot93] provide valuable semantic information to the file system, which can then be used for optimization purposes.
Reference: [PEK + 94] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <type> Technical Report CS-1994-33, </type> <institution> Dept. of Computer Science, Duke University, </institution> <month> October </month> <year> 1994. </year> <note> To appear in IPPS '95. </note>
Reference-contexts: All of these systems decluster file data across many disks to provide parallel access to the data of any file. Workload. The CHARISMA project traced production parallel scientific computing workloads on an Intel iPSC/860 [KN94] and on a TMC CM-5 <ref> [PEK + 94] </ref> to characterize their file-system activity. In both cases, applications accessed large files (megabytes or gigabytes in size) using 2 surprisingly small requests (on the Intel, 96% of read requests were for less than 200 bytes).
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160. </pages>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS <ref> [Pie89] </ref>, Intel PFS [Roy93], IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94].
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: We conclude with commentary on the advantages and disadvantages of high-level, collective requests, and on the underlying technique of disk-directed I/O. 2 Background File systems. There are many parallel file systems today, including Bridge [DSE88], Intel CFS [Pie89], Intel PFS <ref> [Roy93] </ref>, IBM Vesta [CF94], nCUBE [DdR92], TMC sfs [LIN + 93, BGST93], Hurricane File System [Kri94], and SPIFFI [FBD95]. There are also several systems intended for workload clusters, such as PIOUS [MS94] and VIP-FS [dHC94].
Reference: [SGM86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In Proceedings of the IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction Although multiprocessor systems have increased their computational power dramatically in the last decade, the design of hardware and software for I/O has lagged and become an increasing bottleneck in the overall performance of parallel applications. The use of disk striping <ref> [SGM86] </ref> to access many disks in parallel has alleviated some of the hardware limitations by providing greater capacity, bandwidth, and throughput. Good parallel file-system software, however, is critical to a system's I/O performance, and early file systems often had disappointing performance [FPD93, Nit92].
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The MPI message-passing interface may soon be extended to include I/O [CFH + 94], including collective I/O. Finally, there are several libraries for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>. Two-phase I/O. Two-phase I/O is a technique for optimizing data transfer given a high-level, collective interface [dBC93]. A library implementing the interface breaks the request into two phases, an I/O phase and a redistribution phase. <p> = min (first col) over all processors; ncols = sum (ncols) over all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library <ref> [GGL93, BdC93, BBS + 94, SW94] </ref>, or generated by a smart compiler [CC94, TBC94, BTC94]. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [TBC94] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The combined slab size is eight columns. The code for parallel, out-of-core LU-decomposition (based on that in <ref> [TBC94] </ref>) is shown in the outer loop into two loops, with the I/O pulled out of the second loop. <p> all processors; disk-directed read of (first col) through (first col + ncols - 1); barrier (); g Finally, we note that code like that in Figure 2 could be written by hand, incorporated in a parallel matrix library [GGL93, BdC93, BBS + 94, SW94], or generated by a smart compiler <ref> [CC94, TBC94, BTC94] </ref>. 4 Experiments To gain a better understanding of the benefits of disk-directed I/O to an application like LU decomposition, we ran several experiments.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 56-63, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <month> 16 </month>
Reference-contexts: N mult (j) = M (j,i) / M (i,i) for k = i+1 to N // each row j, update cols i+1 .. N M (j,k) = M (j,k) - mult (j) * M (i,k) end end One simple parallelization of this algorithm (although not the best; see <ref> [WGWR93] </ref> for a better algorithm) is to distribute responsibility for columns of the matrix among P processors in a cyclic pattern; that is, column k is handled by processor k mod P (see Figure 1.
References-found: 32

