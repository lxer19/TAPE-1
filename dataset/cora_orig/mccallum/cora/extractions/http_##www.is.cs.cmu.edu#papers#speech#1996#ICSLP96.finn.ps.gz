URL: http://www.is.cs.cmu.edu/papers/speech/1996/ICSLP96.finn.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: finndag@ira.uka.de  
Title: Learning to Parse Spontaneous Speech  
Author: Finn Dag But and Alex Waibel 
Address: (USA)  
Affiliation: Interactive Systems Laboratories University of Karlsruhe (Germany) Carnegie Mellon University  
Abstract: We describe and experimentally evaluate a system, FeasPar, that learns parsing spontaneous speech. To train and run FeasPar (Feature Structure Parser), only limited handmod-eled knowledge is required. The FeasPar architecture consists of neural networks and a search. The networks spilt the incoming sentence into chunks, which are labeled with feature values and chunk relations. Then, the search finds the most probable and consistent feature structure. FeasPar is trained, tested and evaluated with the Spontaneous Scheduling Task, and compared with two samples of a handmodeled GLR* parser, developed for 4 months and 2 years, respectively. The handmodeling effort for FeasPar is 2 weeks. FeasPar performes better than the GLR* parser developed 4 months in all six comparisons that are made and has a similar performance as the GLR* parser developed for 2 years. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> George Berg. </author> <title> Learning Recursive Phrase Structure: Combining the Strengths of PDP and X-Bar Syntax. </title> <type> Technical report TR 91-5, </type> <institution> Dept. of Computer Science, University at Albany, State University of New York, </institution> <year> 1991. </year>
Reference-contexts: A small community have experimented with either purely statistical approaches [2, 14] or connectionist based approaches <ref> [1, 12, 9, 16] </ref>. Their main advantages are learn-ability and robustness.
Reference: 2. <author> Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin. </author> <title> A Statistical Approach To Machine Translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A small community have experimented with either purely statistical approaches <ref> [2, 14] </ref> or connectionist based approaches [1, 12, 9, 16]. Their main advantages are learn-ability and robustness.
Reference: 3. <author> Finn Dag But. </author> <title> FeasPar A Feature Structure Parser Learning to Parse Spontaneous Speech. </title> <type> PhD thesis, </type> <institution> University of Karlsruhe, </institution> <month> upcoming </month> <year> 1996. </year>
Reference-contexts: The Chunk Relation Finder (CRF) determines how a chunk relates to its parent chunk. It has one network per chunk level and chunk relation element. Further details on the three modules can be found in <ref> [4, 3] </ref>. 2.1. Lexicon and Neural Architecture FeasPar uses a full word form lexicon. The lexicon consists of three parts: one, a syntactic and semantic microfeature vector per word, second, lexical feature values, and three, statistical microfeatures. <p> These tables are generated automatically from the training data, and can easily be extended by hand for more generality and new words. An automatic ambiguity checker warns if similar words or phrases map to ambiguous lexical feature values. Further information on the lexicon can be found in <ref> [3] </ref>. All neural networks have one hidden layer, and are con-ventional feed-forward networks. The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL [9], where learning starts using a small context, which is increased later in the learning process. <p> The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL [9], where learning starts using a small context, which is increased later in the learning process. This causes local dependencies to be learned first. Further techniques for improving performance are described in <ref> [3] </ref>. For the neural networks, the average test set performance is 95.4 %. 3. Consistency Checking Search The complete parse depends on many neural networks. Most networks have a certain error rate; only a few networks are perfect. <p> This specification was already available as an interlingua specification document. Using these two information sources, the search finds the feature structure with the highest score, under the constraint of being consistent. The search is described in more detail in <ref> [4, 3] </ref>. 4. Performance Comparison To show the learning ability of FeasPar, it is compared with the GLR* parser [7, 11], applying ESST as domain, and the JANUS speech translation system [7] as environment. An ESST GLR* semantic grammar only exists for English. Hence, FeasPar is trained with English. 4.1. <p> For technical reasons, a comparison fair to the GLR* parser could only be made with performance measure 2E (see <ref> [3] </ref> for a discussion). All output is graded by an independent person, being native speaker and not involved in parser research or development. Grading results are shown in Table 2.
Reference: 4. <author> Finn Dag But and Alex Waibel. </author> <title> Search in a Learnable Spoken Language Parser. </title> <booktitle> In Proceedings of the 12th European Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: The Chunk Relation Finder (CRF) determines how a chunk relates to its parent chunk. It has one network per chunk level and chunk relation element. Further details on the three modules can be found in <ref> [4, 3] </ref>. 2.1. Lexicon and Neural Architecture FeasPar uses a full word form lexicon. The lexicon consists of three parts: one, a syntactic and semantic microfeature vector per word, second, lexical feature values, and three, statistical microfeatures. <p> This specification was already available as an interlingua specification document. Using these two information sources, the search finds the feature structure with the highest score, under the constraint of being consistent. The search is described in more detail in <ref> [4, 3] </ref>. 4. Performance Comparison To show the learning ability of FeasPar, it is compared with the GLR* parser [7, 11], applying ESST as domain, and the JANUS speech translation system [7] as environment. An ESST GLR* semantic grammar only exists for English. Hence, FeasPar is trained with English. 4.1.
Reference: 5. <author> J. Dowding, J. M. Gawron, D. Appelt, J. Bear, L. Cherny, R. Moore, and D. Moran. </author> <title> Gemini: A Natural Language System for Spoken-Language Understanding. </title> <booktitle> In Proceedings ARPA Workshop on Human Language Technology, </booktitle> <pages> pages 43-48, </pages> <address> Princeton, New Jersey, March 1993. </address> <publisher> Morgan Kaufmann Publisher. </publisher>
Reference-contexts: The latter also contains spontaneous effects and speech recognition errors. (On the other hand, the good thing is that spoken language tend to contain less complex structures than written language.) Several methods have been suggested compensate for these speech related problems: e.g. score and penalties, probabilistic rules, and skipping words <ref> [5, 15, 11, 8] </ref>. A small community have experimented with either purely statistical approaches [2, 14] or connectionist based approaches [1, 12, 9, 16]. Their main advantages are learn-ability and robustness.
Reference: 6. <author> G. Gazdar, E. Klein, G. K. Pullum, and I. A. Sag. </author> <title> A theory of syntactic features. In Generalized Phrase Structure Grammar, chapter 2. </title> <publisher> Blackwell Publishing, Oxford, England and Harvard University Press, </publisher> <address> Cambridge, MA, USA, </address> <year> 1985. </year>
Reference-contexts: Three, the approach has not been evaluated with real world data, but with highly regular sentences. Four, millions of training sentences are required. In this paper, we present a parser that produces complex feature structures, as known from e.g. GPSG <ref> [6] </ref>. This parser requires only minor hand labeling, and learns the parsing task itself. It generalizes well, and is robust towards spontaneous effects and speech recognition errors.
Reference: 7. <author> P. Geutner, B. Suhm, F. D. But, T. Kemp, L. May-field, A. E. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna, and A. Waibel. </author> <title> Integrating Different Learning Approaches into a Multilingual Spoken Language Translation System. </title> <booktitle> In Workshop on New Approaches to Learning for Natural Language Processing, International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: The parser is trained with transcribed data only, but evaluated with transcribed and speech data (including speech recognition errors). The parser produces feature structures, holding semantic information. Feature structures are used as interlingua in the JANUS speech-to-speech translation system <ref> [7] </ref>. Within our research team, the design of the interlingua ILT was determined by the needs of unification based parser and generator writers. Consequently, the ILT design was not tuned towards connectionist systems. On the contrary, our parser must learn the form of output provided by a unification based parser. <p> Using these two information sources, the search finds the feature structure with the highest score, under the constraint of being consistent. The search is described in more detail in [4, 3]. 4. Performance Comparison To show the learning ability of FeasPar, it is compared with the GLR* parser <ref> [7, 11] </ref>, applying ESST as domain, and the JANUS speech translation system [7] as environment. An ESST GLR* semantic grammar only exists for English. Hence, FeasPar is trained with English. 4.1. PM 1: Parse Quality The first performance measure, PM 1, expresses the parse quality. <p> The search is described in more detail in [4, 3]. 4. Performance Comparison To show the learning ability of FeasPar, it is compared with the GLR* parser [7, 11], applying ESST as domain, and the JANUS speech translation system <ref> [7] </ref> as environment. An ESST GLR* semantic grammar only exists for English. Hence, FeasPar is trained with English. 4.1. PM 1: Parse Quality The first performance measure, PM 1, expresses the parse quality.
Reference: 8. <author> Sunil Issar and Wayne Ward. </author> <title> CMU's robust spoken language understanding system. </title> <booktitle> In Proceedings of Eu-rospeech, </booktitle> <year> 1993. </year>
Reference-contexts: The latter also contains spontaneous effects and speech recognition errors. (On the other hand, the good thing is that spoken language tend to contain less complex structures than written language.) Several methods have been suggested compensate for these speech related problems: e.g. score and penalties, probabilistic rules, and skipping words <ref> [5, 15, 11, 8] </ref>. A small community have experimented with either purely statistical approaches [2, 14] or connectionist based approaches [1, 12, 9, 16]. Their main advantages are learn-ability and robustness.
Reference: 9. <author> Ajay N. Jain. </author> <title> A Connectionist Learning Architecture for Parsing Spoken Language. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: A small community have experimented with either purely statistical approaches [2, 14] or connectionist based approaches <ref> [1, 12, 9, 16] </ref>. Their main advantages are learn-ability and robustness. <p> Further information on the lexicon can be found in [3]. All neural networks have one hidden layer, and are con-ventional feed-forward networks. The learning is done with standard back-propagation, combined with the constructive learning algorithm PCL <ref> [9] </ref>, where learning starts using a small context, which is increased later in the learning process. This causes local dependencies to be learned first. Further techniques for improving performance are described in [3]. For the neural networks, the average test set performance is 95.4 %. 3.
Reference: 10. <author> R. Kaplan and J. Bresnan. </author> <title> Lexical-Functional Grammar: </title>
References-found: 10

