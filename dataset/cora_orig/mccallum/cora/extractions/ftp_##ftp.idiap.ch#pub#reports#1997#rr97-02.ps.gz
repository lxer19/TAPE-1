URL: ftp://ftp.idiap.ch/pub/reports/1997/rr97-02.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: E  IDIAP Martigny Valais Suisse Discrete All-Positive Multilayer Perceptrons for Optical Implementation  
Author: E R H E R R P P. Moerland a E. Fiesler b I. Saxena b 
Note: internet  published in Optical Engineering, vol. 37, no.  a IDIAP,  
Address: I  P.O.Box 592 Martigny Valais Switzerland  CP 592, CH-1920 Martigny, Switzerland,  Gramercy Place, Torrance, CA 90501, USA  
Affiliation: I  for Perceptual Artificial Intelligence  b Physical Optics Corporation,  
Pubnum: IDIAP-RR 97-02  
Email: e-mail secretariat@idiap.ch  E-mail: Perry.Moerland@idiap.ch  
Phone: phone +41 27 721 77 11 fax +41 27 721 77 12  
Date: February 1997 revised in November 1997  4, April 1998  
Web: http://www.idiap.ch  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> I. Saxena and P. Horan, </author> <title> "Optical Implementations," in the Handbook of Neural Computation, </title> <editor> E. Fiesler and R. Beale, eds., chapter E1.3, </editor> <publisher> Institute of Physics Publishers & Oxford University Press, </publisher> <address> New York (1997). </address>
Reference-contexts: Conventional digital computers, however, cannot take advantage of the parallelism inherent in MLP computation. A promising alternative is the use of optics which offers the potential of parallel three-dimensional interconnections in a compact way, using, for example, spatial light modulators as two-dimensional weighting devices <ref> [1] </ref>. In addition to matrix-vector multiplication that is performed optically, efficient optical MLP (OMLP) implementations should incorporate a successful optical thresholding technique for non-linear processing to preserve the parallelism inherent in optics, in contrast to the use of electronic non-linear thresholding [2, 3, 4]. <p> Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock [23]. Each pattern is a set of 60 numbers in the range <ref> [0; 1] </ref>. The corresponding output patterns are the two unit vectors. Wine is the result of a chemical analysis of wines grown in a region in Italy which are derived from three different cultivars. <p> The analysis determined the quantities of 13 constituents found in each of the three types of wines. A wine has to be classified using these 13 values, which have been scaled to the interval <ref> [0; 1] </ref>. The target patterns are the three unit vectors [24]. Digit This benchmark consists of a subset of 1000 patterns out of a database of more than 20000 digitized handwritten characters [25]. <p> To save computation time and space the 32 fi 32 matrix has been converted to an 8 fi 8 matrix, by taking the average of 4 fi 4 sub-matrices, and scaling the input values to the interval <ref> [0; 1] </ref>. The target patterns are the ten unit vectors. The benchmark characteristics and the training parameters are listed in Table 3. <p> replaced by these minimal values to be able to compare the different curves in a fair way. 8 IDIAP-RR 97-02 benchmark network pattern set sizes #Runs " c " d learning momentum initial topology 1 train. valid. test rate weight range XOR 2-2-1 4 - 100 0.1 0.1 0.3 0.9 <ref> [1; +1] </ref> Sonar 60-8-2 104 - 5 0.3 0.4 0.1 0.9 [0:5; +0:5] Wine 13-6-3 89 44 45 10 - 0.3 0.9 [0:5; +0:5] Digit 64-64-10 500 250 250 5 - 0.1 0.5 [0:5; +0:5] Table 3: Summary of the benchmarks and parameters used in the experiments.
Reference: [2] <author> F. Yu, T. Lu, X. Yang, and D. Gregory, </author> <title> "Optical Neural Network with Pocket-Sized Liquid-Crystal Televisions," </title> <journal> Optics Letters, </journal> <volume> 15(15), </volume> <pages> 863-865, </pages> <year> (1990). </year>
Reference-contexts: In addition to matrix-vector multiplication that is performed optically, efficient optical MLP (OMLP) implementations should incorporate a successful optical thresholding technique for non-linear processing to preserve the parallelism inherent in optics, in contrast to the use of electronic non-linear thresholding <ref> [2, 3, 4] </ref>. Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier [5, 6]. <p> Some other options are offered in the use of different encoding schemes such as phase or wavelength encoding, but these techniques are virtually unexplored. Subtraction is, therefore, usually realized as an electronic difference of two photo-detected quantities that have been separated spatially [17], temporally <ref> [2] </ref>, or by polarization encoding [18]. Such schemes prevent all-optical neural processing at hidden layers and necessitate serial processing for electronic thresholding instead of spatial parallelism using LCLVs.
Reference: [3] <author> J.-S. Jang, S.-G. Shin, S.-W Yuk, S.-Y Shin, and S.-Y. Lee, </author> <title> "Dynamic Optical Interconnections Using Holographic Lenslet Arrays for Adaptive Neural Networks," </title> <journal> Optical Engineering, </journal> <volume> 32(1), </volume> <month> 80-87 </month> <year> (1993). </year>
Reference-contexts: In addition to matrix-vector multiplication that is performed optically, efficient optical MLP (OMLP) implementations should incorporate a successful optical thresholding technique for non-linear processing to preserve the parallelism inherent in optics, in contrast to the use of electronic non-linear thresholding <ref> [2, 3, 4] </ref>. Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier [5, 6].
Reference: [4] <author> R. G. Stearns, </author> <title> "Optically Programmed Neural Network Capable of Stand-Alone Operation," </title> <journal> Applied Optics, </journal> <volume> 32(26), </volume> <month> 5141-5152 </month> <year> (1993). </year> <pages> IDIAP-RR 97-02 15 </pages>
Reference-contexts: In addition to matrix-vector multiplication that is performed optically, efficient optical MLP (OMLP) implementations should incorporate a successful optical thresholding technique for non-linear processing to preserve the parallelism inherent in optics, in contrast to the use of electronic non-linear thresholding <ref> [2, 3, 4] </ref>. Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier [5, 6]. <p> However, a further reduction of the number of different weight values is beneficial for various reasons. Firstly, the mapping of weight values to an optical device is often non-linear <ref> [4, 21] </ref>. When only a few weight levels are used, it is simpler to linearize such a weight mapping and reduce the inaccuracy. Secondly, a small number of weight levels provides the possibility of using ferroelectric liquid crystals with binary valued pixels to represent the weights in an efficient way.
Reference: [5] <author> I. Saxena and E. Fiesler, </author> <title> "Adaptive Multilayer Optical Neural Network with Optical Thresholding," Optical Engineering, special on Optics in Switzerland, </title> <editor> P. Rastogi, ed., </editor> <volume> 34(8), </volume> <month> 2435-2440 </month> <year> (1995). </year>
Reference-contexts: Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier <ref> [5, 6] </ref>. This architecture consists of liquid crystal television screens to implement the inputs and the weights [7], and liquid crystal light valves (LCLVs) to implement non-linear thresholding. <p> A comparison of properties (translation, asymptotes, and steepness) of five sigmoid-like responses of four different LCLVs was made earlier <ref> [5] </ref> based on close approximations [10, 11], with a generic sigmoid curve fit. <p> weights, and learning rate when increasing the gain with a factor fi. x-axis by ffi and along the y-axis by ff, a range fl, and a gain fi: OE (x) = ff + fl showed that the gain parameters fi of LCLV1-3 differ greatly from the standard value of one <ref> [5] </ref> (see the first row of Table 1). Such a non-standard gain cannot be combined with rules of thumb for choosing the parameters of the backpropagation learning rule and does not guarantee convergence of network training. <p> IDIAP-RR 97-02 5 3.1 On Subtraction Compensation and Weight Discretization 3.1.1 Subtraction Compensation In optical neural networks where information is coded in light intensity, as in our ONN <ref> [5] </ref>, all variables including the interconnection weights can only be represented by non-negative quantities. This lack of negative values and the intricacy of an optical mechanism for irradiance subtraction are important limiting factors for the development of optical neural networks. <p> In the ONN architecture <ref> [5] </ref>, the interconnection weights are represented by the pixels of a liquid crystal television screen that, in principle, provides 256 grey levels. However, a further reduction of the number of different weight values is beneficial for various reasons.
Reference: [6] <author> N. Collings, </author> <title> "Design Considerations for a Useful Two-Layer Neural Network," </title> <booktitle> presented at the Euro-American Workshop on Optical Pattern Recognition, </booktitle> <address> La Rochelle, France (June 1994). </address>
Reference-contexts: Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier <ref> [5, 6] </ref>. This architecture consists of liquid crystal television screens to implement the inputs and the weights [7], and liquid crystal light valves (LCLVs) to implement non-linear thresholding.
Reference: [7] <author> N. Collings, A. R. Pourzand, and R. Volkel, </author> <title> "Construction of a Programmable Multilayer Analogue Neural Network Using Space Invariant Interconnects," </title> <booktitle> vol. SPIE 2565, </booktitle> <pages> pp. </pages> <month> 40-47 </month> <year> (1995). </year>
Reference-contexts: Such a technique avoids photo-electric conversion of light and eliminates electronic thresholding. An adaptive OMLP trained under computer control which could implement this was described earlier [5, 6]. This architecture consists of liquid crystal television screens to implement the inputs and the weights <ref> [7] </ref>, and liquid crystal light valves (LCLVs) to implement non-linear thresholding. The inclusion of commercially available LCLVs to implement optical thresholding into the backpropagation rule for MLPs has been reported in a previous paper by the authors [8].
Reference: [8] <author> P. Moerland, E. Fiesler, and I. Saxena, </author> <title> "Incorporation of Liquid-Crystal Light Valve Non-Linearities in Optical Multilayer Neural Networks," </title> <journal> Applied Optics, </journal> <volume> 35(26), </volume> <month> 5301-5307 </month> <year> (1996). </year>
Reference-contexts: The inclusion of commercially available LCLVs to implement optical thresholding into the backpropagation rule for MLPs has been reported in a previous paper by the authors <ref> [8] </ref>. In specific, this paper [8] has resolved the constraints of non-standard non-linear thresholding and its adapted backpropagation rule compensates for the gain (steepness) of the activation function by modification of the initial values of training parameters, showing good performance in computer simulations including laboratory data for the LCLVs. <p> The inclusion of commercially available LCLVs to implement optical thresholding into the backpropagation rule for MLPs has been reported in a previous paper by the authors <ref> [8] </ref>. In specific, this paper [8] has resolved the constraints of non-standard non-linear thresholding and its adapted backpropagation rule compensates for the gain (steepness) of the activation function by modification of the initial values of training parameters, showing good performance in computer simulations including laboratory data for the LCLVs. <p> Following up the work on the adapted algorithm <ref> [8] </ref>, in this paper our method further resolves the restriction on the weights to non-negative values and to a small number of discrete levels, while integrating the use of LCLVs for optical thresholding. All-positive network parameters stem from the intensity-modulation based OMLP processing scheme. <p> The main differences between the LCLV response curves and the standard sigmoid, 1=(1+e x ), are that they are located in the non-negative quadrant and have a non-standard gain (steepness) (Figure 1 gives a typical example). The backpropagation algorithm has been adapted <ref> [8] </ref> to compensate for these differences; this solution is summarized in section 2.1. To perform computer simulations with the measured non-negative LCLV response data, a continuous approximation by linear interpolation is used here. <p> The midpoint of the interpolated data curves is defined as the x-value corresponding to a normalized y-value of a half, in analogy with the standard sigmoid. 2.1 Adaptations for the LCLV Activation Functions Simulation results <ref> [8] </ref> have shown that the backpropagation algorithm [12] with a standard choice for the initial parameters (typically weights in a small interval symmetric around zero and a learning rate less than 1) fails to converge when using the LCLV response data as non-linear thresholding functions. <p> However, this initialization method leads to non-convergence results when using a sigmoidal activation function which has been translated along the x-axis, like the LCLV response curves. Therefore, a weight initialization method resulting in neuron inputs centered around the midpoint of the activation function has been used instead <ref> [8] </ref>. <p> In this paper, the respective gains of the five LCLV response curves are directly calculated from their sampled data and not based on the generic curve fits as in our previous work <ref> [8] </ref>.
Reference: [9] <author> D. Psaltis and Y. Qiao, </author> <title> "Adaptive Multilayer Optical Networks," in Progress in Optics, </title> <editor> E. Wolf, ed., </editor> <volume> vol. 31, chap. 4, </volume> <pages> pp. 227-261, </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands (1993). </address>
Reference-contexts: Finally, some first results of recall on an optical perceptron for handwritten digit recognition are presented. 2 LCLV Activation Functions Foremost amongst optical thresholding devices are liquid crystal light valves with their non-linear, sigmoid-like optical activation functions <ref> [9, 10] </ref>, in addition to their practical characteristics (low operating voltage, high contrast ratio at visible wavelengths, and low intensities required) which make them easy-to-use devices.
Reference: [10] <author> W. Xue, </author> <title> "Characterization of Liquid Crystal Light Valves for Neural Network Applications," </title> <type> Ph.D. thesis, </type> <institution> Institute of Micro-technology, University of Neuch^atel, </institution> <address> Neuch^atel, Switzerland (1994). </address>
Reference-contexts: Finally, some first results of recall on an optical perceptron for handwritten digit recognition are presented. 2 LCLV Activation Functions Foremost amongst optical thresholding devices are liquid crystal light valves with their non-linear, sigmoid-like optical activation functions <ref> [9, 10] </ref>, in addition to their practical characteristics (low operating voltage, high contrast ratio at visible wavelengths, and low intensities required) which make them easy-to-use devices. <p> A comparison of properties (translation, asymptotes, and steepness) of five sigmoid-like responses of four different LCLVs was made earlier [5] based on close approximations <ref> [10, 11] </ref>, with a generic sigmoid curve fit.
Reference: [11] <author> N. Collings and W. Xue, </author> <title> "Liquid Crystal Light Valves as Thresholding Elements in Neural Networks: Basic Device Requirements," </title> <journal> Applied Optics, </journal> <volume> 33(14), </volume> <month> 2829-2833 </month> <year> (1994). </year>
Reference-contexts: A comparison of properties (translation, asymptotes, and steepness) of five sigmoid-like responses of four different LCLVs was made earlier [5] based on close approximations <ref> [10, 11] </ref>, with a generic sigmoid curve fit.
Reference: [12] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning Internal Representations by Error Propagation", </title> <booktitle> in Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations, chap. </booktitle> <volume> 8, </volume> <pages> pp. 318-362, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts (1986). </address>
Reference-contexts: The midpoint of the interpolated data curves is defined as the x-value corresponding to a normalized y-value of a half, in analogy with the standard sigmoid. 2.1 Adaptations for the LCLV Activation Functions Simulation results [8] have shown that the backpropagation algorithm <ref> [12] </ref> with a standard choice for the initial parameters (typically weights in a small interval symmetric around zero and a learning rate less than 1) fails to converge when using the LCLV response data as non-linear thresholding functions. <p> It is the classical example of a simple problem that is not linearly separable <ref> [12] </ref>. Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock [23]. <p> In all simulations two standard training acceleration parameters have been used, namely a momentum term <ref> [12] </ref> (see Table 3) and a flat spot constant [26] of 0:1. A momentum term incorporates previous weight updates in the weight update equation (step 4 in the backpropagation algorithm described in the Appendix) to smooth out oscillations.
Reference: [13] <author> G. Thimm and E. Fiesler, </author> <title> "Weight Initialization in Higher Order and Multi-Layer Perceptrons," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8(2), </volume> <month> 349-359 </month> <year> (1997). </year>
Reference-contexts: In this section, an adapted backpropagation learning rule is described that consists in modifying the initial training conditions as well as compensating for the gain of the activation function. The initial weights for a multilayer perceptron are best chosen uniformly distributed in an interval symmetric around zero <ref> [13] </ref>. However, this initialization method leads to non-convergence results when using a sigmoidal activation function which has been translated along the x-axis, like the LCLV response curves. Therefore, a weight initialization method resulting in neuron inputs centered around the midpoint of the activation function has been used instead [8]. <p> The on-line backpropagation algorithm is described by the following five steps: 1. Initialization Weights and biases are initialized with random values <ref> [13] </ref>. 2. Pattern presentation An input pattern, which is used to initialize the activation values of the neurons in the input layer, and the corresponding target pattern are presented. 3. Forward propagation During this phase, the activation values of the input neurons are propag ated through the network.
Reference: [14] <author> I. Shariv, O. Gila, and A.A. Friesem, </author> <title> "All-Optical Bipolar Neural Network with Polarization-Modulating Neurons," </title> <journal> Optics Letters, </journal> <volume> 16(21), </volume> <month> 1692-1694 </month> <year> (1991). </year>
Reference-contexts: This lack of negative values and the intricacy of an optical mechanism for irradiance subtraction are important limiting factors for the development of optical neural networks. Optical subtraction based on superposing polarized light intensities has only been demonstrated in rather simple optical systems <ref> [14] </ref>, and it is practically impossible to realize in massively parallel optical neural networks. Some other options are offered in the use of different encoding schemes such as phase or wavelength encoding, but these techniques are virtually unexplored.
Reference: [15] <author> G. Thimm, P. Moerland, and E. Fiesler, </author> <title> "The Interchangeability of Learning Rate and Gain in Backpropagation Neural Networks," </title> <journal> Neural Computation, </journal> <volume> 8(2), </volume> <month> 451-460 </month> <year> (1996). </year>
Reference-contexts: This influence can be eliminated by applying a simple and precise relationship (Table 2) that enables compensating for the non-standard gain in backpropagation neural networks by changing the learning rate and the initial weights <ref> [15] </ref> while maintaining equivalent network behaviour during training. The change of the gain with a factor fi (as in network N in Table 2) can therefore be compensated for by dividing the initial weights by fi and the learning rate by fi 2 . <p> It has been shown that the gain theorem can easily be extended to include these parameters <ref> [15] </ref>. 3.2.1 Measuring Generalization Performance To validate the quality of a network during training and to test the performance afterwards, the complete available pattern set was partitioned, respecting an equal distribution of the different classes, into three sets: a training set with 50% of the total patterns and a validation set,
Reference: [16] <author> E. Fiesler, A. Choudry, and H. J. Caulfield, </author> <title> "A Weight Discretization paradigm for Optical Neural Networks," </title> <booktitle> in Proceedings of the International Congress on Optical Science and Engineering, </booktitle> <volume> vol. SPIE 1281, </volume> <pages> pp. 164-173, </pages> <address> SPIE, Bellingham, Washington (1990). </address>
Reference-contexts: 2), so that actual difference in performance as compared with those in Ref. 6 is expected to be small. 3 Discrete Non-Negative Multilayer Perceptrons: Theory and Experiments In this section, the subtraction compensation method is described and how it can be combined with an adaptation of the weight discretization method <ref> [16] </ref>. The performance of these techniques is evaluated in a series of experiments on training all-positive discrete MLPs with five different LCLV response curves as non-linear activation functions. <p> Important additional advantages of such a device are increased processing speed, a linear mapping of the weights, and compactness [21]. To obtain successful network learning in the presence of discrete weights, an existing weight dis-cretization method based on backpropagation <ref> [16] </ref> (described in detail in the Appendix) has been adapted for a OMLP with optical thresholding. It is easily implemented, and is suited for chip-in-the-loop learning. It integrates well with the subtraction compensation technique and can handle a precision of as low as a few bits. <p> Note that pre-training with continuous weights is not necessary, but is often included for faster convergence. The discretization of the weights described by Fiesler et al. <ref> [16] </ref> uses d discretization levels with d = 2, 3, 5, 7, or 9. The discretization levels are symmetric around zero, except for d = 2 and are equidistant: ae 2 fi fi n = 1; 2; :::; d .
Reference: [17] <author> N. Kasama, Y. Hayasaki, T. Yatagai, M. Mori, and S. Ishihara, </author> <title> "Experimental Demonstration of Optical Three Layer Neural Network," </title> <journal> Japanese Journal of Applied Physics, </journal> <volume> 29(8), </volume> <month> L1565-L1568 </month> <year> (1990). </year>
Reference-contexts: Some other options are offered in the use of different encoding schemes such as phase or wavelength encoding, but these techniques are virtually unexplored. Subtraction is, therefore, usually realized as an electronic difference of two photo-detected quantities that have been separated spatially <ref> [17] </ref>, temporally [2], or by polarization encoding [18]. Such schemes prevent all-optical neural processing at hidden layers and necessitate serial processing for electronic thresholding instead of spatial parallelism using LCLVs.
Reference: [18] <author> M. Kranzdorf, B. Bibner, L. Zhang, and K. Johnson, </author> <title> "Optical Connectionist Machine with Polarization-Based Bipolar Weight Values," </title> <journal> Optical Engineering, </journal> <volume> 28, </volume> <month> 844-848 </month> <year> (1989). </year>
Reference-contexts: Some other options are offered in the use of different encoding schemes such as phase or wavelength encoding, but these techniques are virtually unexplored. Subtraction is, therefore, usually realized as an electronic difference of two photo-detected quantities that have been separated spatially [17], temporally [2], or by polarization encoding <ref> [18] </ref>. Such schemes prevent all-optical neural processing at hidden layers and necessitate serial processing for electronic thresholding instead of spatial parallelism using LCLVs.
Reference: [19] <author> I. Saxena, E. Fiesler, and P. Moerland, </author> <title> "A Method for All-Positive Optical Multilayer Perceptrons," </title> <booktitle> in Proceedings of the Third IEEE International Conference on Electronics, Circuits, and Systems (ICECS'96), </booktitle> <volume> vol. 1, </volume> <pages> pp. 448-451, </pages> <publisher> IEEE, </publisher> <address> Piscataway, NJ (1996). </address>
Reference-contexts: In this publication, a method is used that is based on a mathematical technique for subtraction compensation, which offers a scheme for implementing all-positive neural networks that are trained under computer control as described earlier <ref> [19] </ref>. This solution is based on a transformation of the network weights to the positive domain, enabling uninterrupted forward propagation of light in OMLPs. <p> Transformed all-positive weights w 00 ij have been obtained <ref> [19] </ref>: w 00 ij = maxfw 0 ij (1 j P ij a i where w 0 and j = j w min i where w min is the minimum over all original weight values w ij and bias values j .
Reference: [20] <author> R. C. Frye, E. A. Rietman, and C. C. Wong, </author> <title> "Back-Propagation Learning and Nonidealities in Analog Neural Network Hardware," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1), </volume> <month> 110-117 </month> <year> (1991). </year>
Reference-contexts: Examples are optically controlled photoconductive synapses <ref> [20] </ref>, the strength of 6 IDIAP-RR 97-02 0.94 0.928 1.0 0.94 0.0 0.94 0.928 0.0 0.0 0.0 corresponds to a zero weight otherwise the weight is equal to 6.985.
Reference: [21] <author> M. G. Robinson and K. M. Johnson, </author> <title> "Noise Analysis of Polarization-Based Optoelectronic Connectionist Machines," </title> <journal> Applied Optics, </journal> <volume> 31(2), </volume> <month> 263-272 </month> <year> (1992). </year>
Reference-contexts: network is depicted from left (inputs) to right (outputs). which can be modulated by changing the discrete pixel lengths of bars of light, and a polarization based implementation that represents each weight by 16 spatially multiplexed binary valued pixels in a ferroelectric liquid crystal (hence allowing 16 gray level weights) <ref> [21] </ref>. In the ONN architecture [5], the interconnection weights are represented by the pixels of a liquid crystal television screen that, in principle, provides 256 grey levels. However, a further reduction of the number of different weight values is beneficial for various reasons. <p> However, a further reduction of the number of different weight values is beneficial for various reasons. Firstly, the mapping of weight values to an optical device is often non-linear <ref> [4, 21] </ref>. When only a few weight levels are used, it is simpler to linearize such a weight mapping and reduce the inaccuracy. Secondly, a small number of weight levels provides the possibility of using ferroelectric liquid crystals with binary valued pixels to represent the weights in an efficient way. <p> Secondly, a small number of weight levels provides the possibility of using ferroelectric liquid crystals with binary valued pixels to represent the weights in an efficient way. Important additional advantages of such a device are increased processing speed, a linear mapping of the weights, and compactness <ref> [21] </ref>. To obtain successful network learning in the presence of discrete weights, an existing weight dis-cretization method based on backpropagation [16] (described in detail in the Appendix) has been adapted for a OMLP with optical thresholding. It is easily implemented, and is suited for chip-in-the-loop learning.
Reference: [22] <author> L. Prechelt, </author> <title> "PROBEN1 | A Set of Benchmarks and Benchmarking Rules for Neural Network Training Algorithms," </title> <type> Technical report 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, D-76128 Karlsruhe, </institution> <note> Germany (1994). The benchmark set is available at ftp://ftp.ira.uka.de/pub/neuron/proben1.tar.gz. </note>
Reference-contexts: This is a good way to avoid over-fitting of the network to the particular training set used. For the experiments in this section, a description of cross validation with early stopping has been used as a basis <ref> [22] </ref>. <p> In addition, results on the classification performance of the networks will also be presented. A pattern is considered correctly classified whenever the network output corresponding to the correct one is higher in value than all the other outputs. This procedure is called winner-takes-all (WTA) <ref> [22] </ref> and can only be used in problems whose output is implemented as a 1-of-N L representation. 3.2.2 Experimental Results for Training Non-Negative MLPs The experiments described in this section have been performed to evaluate the combined use of the measured LCLV response data as non-linearities and the transformation of the
Reference: [23] <author> R. P. Gorman and T. J. Sejnowski, </author> <title> "Analysis of hidden units in a layered network trained to classify sonar targets," Neural Networks, </title> <type> 1, </type> <month> 75-89 </month> <year> (1988). </year> <pages> 16 IDIAP-RR 97-02 </pages>
Reference-contexts: Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock <ref> [23] </ref>. Each pattern is a set of 60 numbers in the range [0; 1]. The corresponding output patterns are the two unit vectors. Wine is the result of a chemical analysis of wines grown in a region in Italy which are derived from three different cultivars.
Reference: [24] <author> Data made available in 1994 by librarians P. M. Murphy and D. W. </author> <title> Aha from the UCI Repository of Machine Learning Databases, a machine-readable data repository accessible via anonymous-ftp: </title> <publisher> ftp://ftp.ics.uci.edu/pub/machine-learning-databases. </publisher>
Reference-contexts: The analysis determined the quantities of 13 constituents found in each of the three types of wines. A wine has to be classified using these 13 values, which have been scaled to the interval [0; 1]. The target patterns are the three unit vectors <ref> [24] </ref>. Digit This benchmark consists of a subset of 1000 patterns out of a database of more than 20000 digitized handwritten characters [25]. Each digit was scaled to fit into a 32 fi 32 matrix, and each pixel is represented by an eight bit value.
Reference: [25] <author> M. D. Garris and R. A. Wilkinson, </author> <title> NIST Special Database 3, </title> <booktitle> National Institute of Standards and Technology, Advanced System Division, Image Recognition Group (1992). </booktitle>
Reference-contexts: A wine has to be classified using these 13 values, which have been scaled to the interval [0; 1]. The target patterns are the three unit vectors [24]. Digit This benchmark consists of a subset of 1000 patterns out of a database of more than 20000 digitized handwritten characters <ref> [25] </ref>. Each digit was scaled to fit into a 32 fi 32 matrix, and each pixel is represented by an eight bit value.
Reference: [26] <author> S. E. Fahlman, </author> <title> "An Empirical Study of Learning Speed in Backpropagation Networks," </title> <type> Technical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, USA (1988). </address>
Reference-contexts: In all simulations two standard training acceleration parameters have been used, namely a momentum term [12] (see Table 3) and a flat spot constant <ref> [26] </ref> of 0:1. A momentum term incorporates previous weight updates in the weight update equation (step 4 in the backpropagation algorithm described in the Appendix) to smooth out oscillations.
Reference: [27] <author> M. Moreira and E. Fiesler, </author> <title> Neural Networks with Adaptive Learning Rate and Momentum Terms. </title> <address> IDIAP-RR 95-04, IDIAP, Martigny, Switzerland. ftp://ftp.idiap.ch/pub/reports/1995/95-04.ps.Z. </address>
Reference-contexts: A comparison of these results with the ones obtained in a benchmarking study (also including Xor (2), Sonar, Wine, and Digit benchmarks) of different adaptive learning rate algorithms for ordinary MLPs <ref> [27] </ref>, shows that the influence of the optical non-idealities is as good as negligible and gives comparable results. 3.2.3 Experimental Results for Training Discrete Non-Negative MLPs For most benchmarks, the number of discretization levels, d, used in the simulations is subsequently 2, 4, 6, 8, and 16.
Reference: [28] <author> T. Lundin and P. Moerland, </author> <title> Quantization and Pruning of Multilayer Perceptrons: Towards Compact Neural Networks. </title> <address> IDIAP-Com 97-02, IDIAP, Martigny, Switzerland. </address> <note> Available via anonymous ftp: ftp://ftp.idiap.ch/pub/reports/1997/com97-02.ps.gz. </note>
Reference-contexts: This problem caused by the discrete network getting trapped on an error plateau, is inherent to most weight discretization methods and might be resolved by introducing a random (annealing) factor in the weight updates <ref> [28] </ref>. To get a less biased idea of the possible performance, the average over the best five runs out of ten is listed in Table 11. In this case, the results are close to the continuous ones with the number of weight levels equal to 4 or 6.
Reference: [29] <author> I. Saxena, P. Moerland, E. Fiesler, A. R. Pourzand, and N. Collings, </author> <title> "An Optical Thresholding Perceptron," </title> <booktitle> in Proceedings of the Workshop on Optics and Computer Science, </booktitle> <address> Geneva, Switzerland (1997). </address>
Reference-contexts: The resultant discrete weights were implemented on the LCTV of the optical perceptron for testing recall with 10 of the 50 digits in the training set. The optical recall was satisfactory with quite a good agreement with the values obtained in the computer simulations <ref> [29] </ref>. A binary perceptron, trained using our adapted training rule, implemented with binary inputs and weights in the same optical system also showed satisfactory recall results [30].
Reference: [30] <author> I. Saxena, P. Moerland, E. Fiesler, and A. Pourzand, </author> <title> "Handwritten Digit Recognition with Binary Optical Perceptron," </title> <booktitle> in Artificial Neural Networks - ICANN'97, Lecture Notes in Computer Science, </booktitle> <volume> vol. 1327, </volume> <editor> W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, </editor> <booktitle> eds., </booktitle> <pages> pp. 1253-1258, </pages> <publisher> Springer Verlag, </publisher> <address> Berlin (1997). </address>
Reference-contexts: The optical recall was satisfactory with quite a good agreement with the values obtained in the computer simulations [29]. A binary perceptron, trained using our adapted training rule, implemented with binary inputs and weights in the same optical system also showed satisfactory recall results <ref> [30] </ref>. The advantage of having binary weights and input values is that they permit the use of ferroelectric liquid crystals having thousandfold faster response times than nematic LCLVs. Binary neural networks also have the inherent advantages of digital vs. analog implementations.
References-found: 30

