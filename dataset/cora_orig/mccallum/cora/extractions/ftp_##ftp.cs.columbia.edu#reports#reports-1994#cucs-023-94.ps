URL: ftp://ftp.cs.columbia.edu/reports/reports-1994/cucs-023-94.ps
Refering-URL: http://www.cs.columbia.edu/~library/1994.html
Root-URL: http://www.cs.columbia.edu
Email: paskov@cs.columbia.edu  
Title: Computing High Dimensional Integrals with Applications to Finance  
Author: SPASSIMIR H. PASKOV 
Note: This research was supported in part by the the National Science Foundation and the Air Force Office of Scientific Research. I am grateful to Goldman Sachs for providing the finance problem.  
Date: October, 1994  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: Technical Report CUCS-023-94  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adler, S. J., </author> <title> The Geometry of Random Fields, Wiley Series in Prob. </title> <journal> and Math. Stat., </journal> <year> 1981. </year>
Reference-contexts: We now present the classical Monte Carlo algorithm. For brevity, we refer to it as the Monte Carlo algorithm. Let t 1 ; : : : ; t n be n randomly selected points which are independent and uniformly distributed over D = <ref> [0; 1] </ref> d , the d dimensional unit cube. Consider a function f from the space L 2 (D) of L 2 -integrable functions. The problem is to approximately compute I (f ) = D using function evaluations at randomly chosen points. <p> This concept of effective dimension can be defined rigorously as follows. Let f be a function with domain <ref> [0; 1] </ref> d and let " &gt; 0. <p> dimension K (") of f 16 Tranche Maximum of k T A 189 C 278 E 309 H 91 R 360 Table 1: The maximum of the numbers k T obtained by generating 3,000,000 random sample points for each tranche of the CMO problem is the smallest integer k 2 <ref> [1; d ] </ref> such that fi fi fi [0;1] k Z f (x)dx fi fi " fi fi Z f (x)dx fi fi : We apply this definition of effective dimension to the ten tranches of the CMO problem. Clearly, the effective dimension is a function of ".
Reference: [2] <author> Antonov, I.A., and Saleev, </author> <title> V.M., An Economic Method of Computing LP t -sequences, </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 19, </volume> <pages> 252-256, </pages> <year> 1979. </year>
Reference-contexts: For example, k = 3 is 11 in base 2. If v 1 = 0:1 and v 2 = 0:11 then b 1 v 1 b 2 v 2 = 0:1 0:11 = 0:01: Antonov and Saleev <ref> [2] </ref> suggest a shu*ing of the original Sobol sequence which preserves good convergence properties and which can be generated much faster. This version of the Sobol sequence is used here. <p> Our generator of Sobol points can start from an arbitrary point and continue to generate the rest of the points using Antonov-Saleev's recursive approach <ref> [2] </ref>. In addition, the tables of the primitive polynomials and the initial direction numbers have been extended for dimensions up to 360. 4.1.3 Generators of Uniformly Distributed Random Points The software uses various kinds of random number generators. <p> We checked computationally that the values of the integrals of all tranches are about a10 6 with a 2 <ref> [2; 42] </ref>. Hence by taking " = 0:001 we introduce an absolute error which is on the order of several thousand dollars. We estimate K T (") for " = 0:001, where K T (") denotes the effective dimension of tranche T.
Reference: [3] <author> Bratley, P. and Fox, B.L., </author> <title> Algorithm 659, Implementing Sobol's Quasirandom Sequence Generator, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 14, </volume> <pages> 88-100, </pages> <year> 1988. </year>
Reference: [4] <author> Chelson, P., </author> <title> Quasi-random Techniques for Monte Carlo Methods, </title> <type> PhD Dissertation, </type> <institution> The Claremont Graduate School, </institution> <year> 1976. </year>
Reference-contexts: For the low discrepancy algorithms, the error bound depends on the variation of the integrand, see Section 3. Therefore the error bound will be decreased by reducing of the variation. Reducing the variation of the integrand has not been studied as extensively as reducing the variance. See, however, <ref> [4] </ref>, [17], [20], and [35]. This is a major area for further research. An important advantage of the classical Monte Carlo algorithm and of the deterministic algorithms studied here, is that they can be utilized very generally.
Reference: [5] <author> Fabozzi, F. J., </author> <title> Handbook of Mortgage Backed Securities, </title> <publisher> Probus Publishing Co., </publisher> <year> 1992. </year>
Reference-contexts: The technique of distributing the cash flows transfers the prepayment risk among different tranches. This results in financial instruments with varying characteristics which might be more suitable to the needs and expectations of investors. For more details on CMOs, we refer to <ref> [5] </ref>. We stress that the amount of obtained cash flows will depend upon the future level of interest rates. Our problem is to estimate the expected value of the sum of present values of future cash flows for each of the tranches.
Reference: [6] <editor> Fabozzi, F. J., </editor> <publisher> Fixed Income Mathematics Probus Publishing Co., </publisher> <year> 1988. </year>
Reference: [7] <author> Fox, B.L., </author> <title> Algorithm 647, Implementation and Relative Efficiency of Quasirandom Sequence Generators, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 12, </volume> <pages> 362-376, </pages> <year> 1986. </year>
Reference-contexts: Halton [9] suggests a fast recursive algorithm for generation of Halton points which is implemented in <ref> [7] </ref> and [10]. However, this algorithm imposes some restrictions on the maximum number of dimensions and the maximum number of sample points. For example, the maximum dimension used in [7] is 40 and the maximum number of sample points used is 100,000. <p> Halton [9] suggests a fast recursive algorithm for generation of Halton points which is implemented in <ref> [7] </ref> and [10]. However, this algorithm imposes some restrictions on the maximum number of dimensions and the maximum number of sample points. For example, the maximum dimension used in [7] is 40 and the maximum number of sample points used is 100,000. Since we want to solve integrals with dimension up to 360 and to use a much larger number of sample points we modify this algorithm. The modification is trivial and we omit the details.
Reference: [8] <author> Geweke, J., </author> <title> Monte Carlo Simulations and Numerical Integration, </title> <note> to appear in Handbook of Computational Economics. </note>
Reference: [9] <author> Halton, J.H., </author> <title> On the efficiency of certain quasi-random sequences of points in evaluating multi-dimensional integrals, </title> <journal> Numer. Math., </journal> <volume> 2, </volume> <pages> 84-90, </pages> <year> 1960. </year>
Reference-contexts: Examples of such infinite sequences are the Halton <ref> [9] </ref> and Sobol [32] sequences. For the reader's convenience, we give a short description of both these sequences later in this section. <p> We now give some implementation details on the generators for each of the above set of sample points. 11 4.1.1 Generator of Halton Points The generator of Halton points takes as input integers n and d and returns the n-th d-dimensional Halton point. Halton <ref> [9] </ref> suggests a fast recursive algorithm for generation of Halton points which is implemented in [7] and [10]. However, this algorithm imposes some restrictions on the maximum number of dimensions and the maximum number of sample points.
Reference: [10] <author> Halton, J.H. and Smith, </author> <title> G.B., Algorithm 247, Radical-inverse quasi-random point sequence, </title> <journal> Commun. ACM, </journal> <volume> 7, </volume> <pages> 701-702, </pages> <year> 1964. </year>
Reference-contexts: Halton [9] suggests a fast recursive algorithm for generation of Halton points which is implemented in [7] and <ref> [10] </ref>. However, this algorithm imposes some restrictions on the maximum number of dimensions and the maximum number of sample points. For example, the maximum dimension used in [7] is 40 and the maximum number of sample points used is 100,000.
Reference: [11] <author> Halton, J.H., </author> <title> A retrospective and prospective survey of the Monte Carlo method SIAM Review, </title> <type> 12, </type> <year> 1970, </year> <pages> 1-63. </pages>
Reference-contexts: For more details, see for example, <ref> [11] </ref>, [12], [15], and [33]. We now present the classical Monte Carlo algorithm. For brevity, we refer to it as the Monte Carlo algorithm.
Reference: [12] <author> Hammersley, J. and Handscomb, D., </author> <title> Monte Carlo methods, </title> <publisher> Methuen, </publisher> <address> London, </address> <year> 1964. </year> <month> 41 </month>
Reference-contexts: For more details, see for example, [11], <ref> [12] </ref>, [15], and [33]. We now present the classical Monte Carlo algorithm. For brevity, we refer to it as the Monte Carlo algorithm.
Reference: [13] <author> Hoel, P. G., </author> <title> Elementary Statistics, </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year>
Reference-contexts: In Section 8, we showed that it is reasonable to assume that m 14; 865; 801. We seek the number of antithetic variables runs, each using 4,000 sample points, which after averaging would give an error ffi with probability . This procedure is widely used in statistical analysis, see <ref> [13] </ref>. That is, let ~ 1 ; ~ 2 ; : : : ; ~ k be the results for k antithetic variables runs each using 4,000 f function evaluations.
Reference: [14] <author> Janse van Rensburg, E. J. and Torrie, G. M., </author> <title> Estimation of multidimensional integrals: Is Monte Carlo the best method? J. </title> <journal> Phys. A.: Math. Gen., </journal> <volume> 26, </volume> <year> 1993, </year> <pages> 943-953. </pages>
Reference-contexts: 1 Introduction High-dimensional integrals are usually solved with Monte Carlo algorithms. Vast sums are spent annually on these algorithms. Theory [21], [42] suggests that low discrepancy algorithms are sometimes superior to Monte Carlo algorithms. However, a number of researchers, <ref> [14] </ref>, [19], [20], report that their numerical tests show that this theoretical advantage decreases with increasing dimension. Furthermore, they report that the theoretical advantage of low discrepancy algorithms disappears for rather modest values of the dimension, say, d 30.
Reference: [15] <author> Kalos, M. H. and Whitlock, P. A., </author> <title> Monte Carlo Methods, Volume I, </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: For more details, see for example, [11], [12], <ref> [15] </ref>, and [33]. We now present the classical Monte Carlo algorithm. For brevity, we refer to it as the Monte Carlo algorithm. <p> In fact, this is the main idea underlying the various variance reduction techniques which are often used in combination with the Monte Carlo algorithm. Examples of variance reduction techniques are importance sampling, control variates, antithetic variables, see for example <ref> [15] </ref>. Antithetic variables will be discussed in Section 7. Let B (L 2 (D)) denote the unit ball of L 2 (D). <p> Therefore by reducing the variance of the integrand the expected error would also decrease. Various variance reduction techniques such as importance sampling, control variates, antithetic variables and others, see for example <ref> [15] </ref>, are often used with the classical Monte Carlo algorithm. For the low discrepancy algorithms, the error bound depends on the variation of the integrand, see Section 3. Therefore the error bound will be decreased by reducing of the variation. <p> Furthermore, the cost of both algorithms is about the same. Since r can be smaller than one for some functions, the antithetic variables variance reduction technique, although simple to use, does not work in general and should be used with care, see also <ref> [15] </ref>. We tested the antithetic variables algorithm for the CMO problem with the ten tranches. Let f T be the integrand for tranche T . We approximately computed (f T ), (g T ), and r T = (f T )=( 2 (g T )) for all tranches T . <p> According to the Central Limit Theorem, see e.g. <ref> [15] </ref>, the distributions of U n (f ) and U n (g) are normal for n ! 1 with the same mean R R D g (x)dx and variances 2 (f )=n and 2 (g)=n, respectively.
Reference: [16] <author> Knuth, D.E., </author> <title> Seminumerical Algorithms, </title> <booktitle> vol. 2 of The Art of Computer Programming, </booktitle> <publisher> Addison Wesley, </publisher> <year> 1981. </year>
Reference-contexts: We mention just three of them here. Even though the rate of convergence is independent of the dimension, it is quite slow. Furthermore, there are fundamental philosophical and practical problems with generating independent random points; instead, pseudorandom numbers are used, see <ref> [16] </ref> and [40]. <p> This version of the Sobol sequence is used here. The sequence of direction numbers v i is generated by a primitive polynomial, see <ref> [16] </ref> and [32], with coefficients in the field Z 2 with elements f0; 1g. Consider, for example, the primitive polynomial P (x) = x n + a 1 x n1 + : : : + a n1 x + 1 of degree n in Z 2 [x].
Reference: [17] <author> Maize, E., </author> <title> Contributions to the theory of error reduction in quasi-Monte Carlo methods, </title> <type> PhD Dissertation, </type> <institution> The Claremont Graduate School, </institution> <year> 1981. </year>
Reference-contexts: For the low discrepancy algorithms, the error bound depends on the variation of the integrand, see Section 3. Therefore the error bound will be decreased by reducing of the variation. Reducing the variation of the integrand has not been studied as extensively as reducing the variance. See, however, [4], <ref> [17] </ref>, [20], and [35]. This is a major area for further research. An important advantage of the classical Monte Carlo algorithm and of the deterministic algorithms studied here, is that they can be utilized very generally.
Reference: [18] <author> Morokoff, W. J. and Caflisch, R. E., </author> <title> Quasi-Random Sequences and their Discrepancies, </title> <note> to appear in SIAM J. </note> <institution> Sci. Stat. Comp., </institution> <year> 1994. </year>
Reference-contexts: We believe that 2 Other low discrepancy sequences also satisfy this property, see [21] 39 this behavior is due to the fact that the Halton points are less uniformly distributed than the Sobol points; especially in high dimensions and sample sizes of 1,000,000 or less, see also <ref> [18] </ref>.
Reference: [19] <author> Morokoff, W. J. and Caflisch, R. E., </author> <title> Quasi-Monte Carlo Integration, </title> <note> to appear in J. Comp. Physics. </note>
Reference-contexts: 1 Introduction High-dimensional integrals are usually solved with Monte Carlo algorithms. Vast sums are spent annually on these algorithms. Theory [21], [42] suggests that low discrepancy algorithms are sometimes superior to Monte Carlo algorithms. However, a number of researchers, [14], <ref> [19] </ref>, [20], report that their numerical tests show that this theoretical advantage decreases with increasing dimension. Furthermore, they report that the theoretical advantage of low discrepancy algorithms disappears for rather modest values of the dimension, say, d 30.
Reference: [20] <author> Moskowitz, B. and Caflisch, R. E., </author> <title> Smoothness and Dimension Reduction in Quasi-Monte Carlo Methods, </title> <note> to appear in Math. Comp. Modeling. </note>
Reference-contexts: 1 Introduction High-dimensional integrals are usually solved with Monte Carlo algorithms. Vast sums are spent annually on these algorithms. Theory [21], [42] suggests that low discrepancy algorithms are sometimes superior to Monte Carlo algorithms. However, a number of researchers, [14], [19], <ref> [20] </ref>, report that their numerical tests show that this theoretical advantage decreases with increasing dimension. Furthermore, they report that the theoretical advantage of low discrepancy algorithms disappears for rather modest values of the dimension, say, d 30. <p> Therefore the error bound will be decreased by reducing of the variation. Reducing the variation of the integrand has not been studied as extensively as reducing the variance. See, however, [4], [17], <ref> [20] </ref>, and [35]. This is a major area for further research. An important advantage of the classical Monte Carlo algorithm and of the deterministic algorithms studied here, is that they can be utilized very generally.
Reference: [21] <author> Niederreiter, H., </author> <title> Random Number Generation and Quasi-Monte Carlo Methods, </title> <publisher> CBMS-NSF,63, SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction High-dimensional integrals are usually solved with Monte Carlo algorithms. Vast sums are spent annually on these algorithms. Theory <ref> [21] </ref>, [42] suggests that low discrepancy algorithms are sometimes superior to Monte Carlo algorithms. However, a number of researchers, [14], [19], [20], report that their numerical tests show that this theoretical advantage decreases with increasing dimension. <p> Nevertheless, the Monte Carlo algorithm has several serious deficiencies, see for example, <ref> [21] </ref> and [35]. We mention just three of them here. Even though the rate of convergence is independent of the dimension, it is quite slow. Furthermore, there are fundamental philosophical and practical problems with generating independent random points; instead, pseudorandom numbers are used, see [16] and [40]. <p> desirable guarantee for problems where highly reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. <ref> [21] </ref>, [22], [39], [42], [23], and [36]. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D. <p> Then, very briefly, we give the theoretical basis of the low discrepancy sequences to be used 7 as sample points for computing multivariate integrals. For more detailed description and treatment of these results, see <ref> [21] </ref>. For t = [t 1 ; : : : ; t d ] 2 D, define [0; t) = [0; t 1 ) fi fi [0; t d ): Let [0;t) be the characteristic (indicator) function of [0; t). <p> 1 ; : : : ; z n )k 2 kR n (; z 1 ; : : : ; z n )k 1 = O (log n) d ! This is the best upper bound known and it is widely believed that it is sharp for these sequences, see <ref> [21] </ref>. We stress that the constants in the bounds (3) and (4) depend on the dimension d and good estimates of these constants are not known. Bounds with known constants and n independent of d are studied in [41] and [43]. <p> Let V (f ) &lt; 1 be the variation of f on D in the sense of Hardy and Krause, see <ref> [21] </ref>, and let fz k g be a low discrepancy sequence. <p> The Koksma-Hlawka inequality guarantees that fi fi fi 1 n X f (z k ) fi fi V (f ) kR n ( ; z 1 ; : : : ; z n )k 1 : (5) Upper bounds in terms of L 2 discrepancy have also been proven, see <ref> [21] </ref>. Therefore, (4) and (5) provide a worst case assurance for the use of low discrepancy sequences as sample points for numerical integration of functions with bounded variation in the sense of Hardy and Krause. <p> Using the identity [0;t) (z k ) = (1t;1] (x k ) and Proposition 2.4 in <ref> [21] </ref>, we conclude that if fz k g is a low discrepancy sequence then fx k g is also a low discrepancy sequence. 9 3.1 Halton Points We give a short description of the Halton low discrepancy sequence. Section 4 discusses some of the implementation details for Halton points. <p> We believe that our choices of the initial direction numbers contributes to the successful performance of the Sobol points. The Halton algorithm did not perform as well as the Sobol algorithm. We believe that 2 Other low discrepancy sequences also satisfy this property, see <ref> [21] </ref> 39 this behavior is due to the fact that the Halton points are less uniformly distributed than the Sobol points; especially in high dimensions and sample sizes of 1,000,000 or less, see also [18].
Reference: [22] <author> Novak, E., </author> <title> Deterministic and Stochastic Error Bounds in Numerical Analysis, </title> <booktitle> Lecture Notes in Math., </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: guarantee for problems where highly reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. [21], <ref> [22] </ref>, [39], [42], [23], and [36]. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D.
Reference: [23] <author> Paskov, S. H., </author> <title> Average Case Complexity of Multivariate Integration for Smooth Functions, </title> <journal> J. Complexity, </journal> <volume> 9, </volume> <pages> 291-312, </pages> <year> 1993. </year>
Reference-contexts: where highly reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. [21], [22], [39], [42], <ref> [23] </ref>, and [36]. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D.
Reference: [24] <author> Paskov, S. H., </author> <title> Computing High Dimensional Integrals with Applications to Finance, </title> <booktitle> Joint Summer Research Conference on Continuous Algorithms and Complexity, </booktitle> <address> Mount Holyoke College, </address> <month> June, </month> <year> 1994. </year>
Reference-contexts: A January, 1994 article in Scientific American discussed the theoretical issues and reported that "Preliminary results obtained by testing certain finance problems suggest the superiority of the deterministic methods in practice." Further results were reported at a number of conferences including <ref> [24] </ref>, [37], [38]. We summarize our main conclusions regarding the evaluation of this CMO. The conclusions may be divided into three groups. 4 I. Deterministic and Monte Carlo Algorithms The Sobol algorithm consistently outperforms the Monte Carlo algorithm. The Sobol algorithm consistently outperforms the Halton algorithm.
Reference: [25] <author> Paskov, S. H., </author> <title> Termination Criteria for Linear Problems, </title> <note> to appear in J. Complexity, </note> <year> 1995. </year>
Reference-contexts: Automatic termination criteria are often used in computational practice. It is of interest to study the relation between the threshold value and the actual error of approximation. See Paskov <ref> [25] </ref> for the approximation of linear operators in the average case setting assuming that arbitrary linear continuous functionals can be computed. It is proved that standard termination criteria work well.
Reference: [26] <author> Press, W., Teukolsky S., Vetterling, W., and B. Flannery, </author> <title> Numerical Recipes in C, First Edition, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year> <month> 42 </month>
Reference-contexts: More specifically, the random number generators ran1 and ran2 from the first edition of Numerical Recipes <ref> [26] </ref>, and RAN1 and RAN2 from the second edition of Numerical Recipes [27] are used because of their wide availability and popularity. All of the above random number generators are based on linear congruential generators with some additional features. For more details on these random number generators we refer to [26] <p> <ref> [26] </ref>, and RAN1 and RAN2 from the second edition of Numerical Recipes [27] are used because of their wide availability and popularity. All of the above random number generators are based on linear congruential generators with some additional features. For more details on these random number generators we refer to [26] and [27]. 4.2 Systems Since workstation clusters and networks provide cost-effective means to perform large-scale computation, we have built and debugged a software system under PVM 3.2 (Parallel Virtual Machine) for computing multivariate integrals. This system runs on a heterogeneous network of machines. <p> This makes automatic termination easier for the Sobol algorithm; see the discussion below; * The Sobol algorithm outperforms the Halton algorithm. 18 19 namely ran1 from <ref> [26] </ref>, is used to generate four Monte Carlo runs using four randomly chosen initial seeds. The plot again exhibits sensitivity of the Monte Carlo algorithm to the initial seed. <p> Then the speedup of the distributed on a network of workstations software system is also measured for the CMO problem. The CPU time in seconds for simultaneous evaluation of the ten tranches is given in Table 10 for Sobol, Halton, RAN2 from [27], RAN1 from [27], ran1 from <ref> [26] </ref>, and ran2 from [26] generators. The real time in seconds, as should be expected, is slightly higher than the CPU time. It is given in the second column since it is later compared with the real time for the network of workstations to derive the parallel speedup. <p> The CPU time in seconds for simultaneous evaluation of the ten tranches is given in Table 10 for Sobol, Halton, RAN2 from [27], RAN1 from [27], ran1 from <ref> [26] </ref>, and ran2 from [26] generators. The real time in seconds, as should be expected, is slightly higher than the CPU time. It is given in the second column since it is later compared with the real time for the network of workstations to derive the parallel speedup.
Reference: [27] <author> Press, W., Teukolsky S., Vetterling, W., and B. Flannery, </author> <title> Numerical Recipes in C, Second Edition, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The function sobseq from Numerical Recipes <ref> [27] </ref> was used as a basis for implementation of the generator of Sobol points. However, it was significantly changed. The generator of Sobol points in [27] starts always from the first point and does not have an efficient way of skipping an arbitrary initial part of the sequence. <p> The function sobseq from Numerical Recipes <ref> [27] </ref> was used as a basis for implementation of the generator of Sobol points. However, it was significantly changed. The generator of Sobol points in [27] starts always from the first point and does not have an efficient way of skipping an arbitrary initial part of the sequence. Our generator of Sobol points can start from an arbitrary point and continue to generate the rest of the points using Antonov-Saleev's recursive approach [2]. <p> More specifically, the random number generators ran1 and ran2 from the first edition of Numerical Recipes [26], and RAN1 and RAN2 from the second edition of Numerical Recipes <ref> [27] </ref> are used because of their wide availability and popularity. All of the above random number generators are based on linear congruential generators with some additional features. For more details on these random number generators we refer to [26] and [27]. 4.2 Systems Since workstation clusters and networks provide cost-effective means <p> RAN1 and RAN2 from the second edition of Numerical Recipes <ref> [27] </ref> are used because of their wide availability and popularity. All of the above random number generators are based on linear congruential generators with some additional features. For more details on these random number generators we refer to [26] and [27]. 4.2 Systems Since workstation clusters and networks provide cost-effective means to perform large-scale computation, we have built and debugged a software system under PVM 3.2 (Parallel Virtual Machine) for computing multivariate integrals. This system runs on a heterogeneous network of machines. <p> For the reader's convenience, the results are summarized in a number of graphs. two randomly chosen initial seeds. The pseudorandom generator RAN2 from <ref> [27] </ref> is used to generate random sample points for the Monte Carlo runs. Figure 1 exhibits behavior that is common in all our comparisons of the Sobol algorithm with the Monte Carlo algorithm. <p> This claim is also supported by the fact that the same effect has been observed for sixteen additional Monte Carlo runs. We assume that is why ran1 has been replaced by a different random number generator in <ref> [27] </ref>. Figure 2 is included to show that the results can be affected by the poor performance of the random number generator. 20 Monte Carlo runs using RAN2 from [27]. We stress that the number of sample points on the x-axis is correct only for the deterministic algorithms. <p> We assume that is why ran1 has been replaced by a different random number generator in <ref> [27] </ref>. Figure 2 is included to show that the results can be affected by the poor performance of the random number generator. 20 Monte Carlo runs using RAN2 from [27]. We stress that the number of sample points on the x-axis is correct only for the deterministic algorithms. The actual number of sample points for the averaged Monte Carlo graph is twenty times the number of sample points on the x-axis. <p> We thus conclude that to achieve similar performances, we have to take about 20 times more random than deterministic sample points. In Figure 4, an automatic termination criterion is applied to Sobol, Halton, and three Monte Carlo runs using RAN2 from <ref> [27] </ref> as the pseudorandom generator. We choose a standard automatic termination criterion. <p> That is why, the spread and the jaggedness of antithetic variables runs in Figure 5 are smaller than the corresponding ones for Monte Carlo runs in Figure 1. antithetic variables runs using RAN2 from <ref> [27] </ref>. We stress that the number of sample points 25 26 Tranche r A 2.82 C 3.09 E 4.70 H 2.81 R 1.55 Table 3: The ratios r for the ten tranches of the CMO on the x-axis is correct only for the deterministic algorithms. <p> Then the speedup of the distributed on a network of workstations software system is also measured for the CMO problem. The CPU time in seconds for simultaneous evaluation of the ten tranches is given in Table 10 for Sobol, Halton, RAN2 from <ref> [27] </ref>, RAN1 from [27], ran1 from [26], and ran2 from [26] generators. The real time in seconds, as should be expected, is slightly higher than the CPU time. <p> Then the speedup of the distributed on a network of workstations software system is also measured for the CMO problem. The CPU time in seconds for simultaneous evaluation of the ten tranches is given in Table 10 for Sobol, Halton, RAN2 from <ref> [27] </ref>, RAN1 from [27], ran1 from [26], and ran2 from [26] generators. The real time in seconds, as should be expected, is slightly higher than the CPU time.
Reference: [28] <author> Roth, K. F., </author> <title> On irregularities of distribution, </title> <journal> Mathematika, </journal> <volume> 1, </volume> <pages> 73-79, </pages> <year> 1954. </year>
Reference-contexts: R n ( ; z 1 ; : : : ; z n ), i.e., Z R 2 1=2 kR n ( ; z 1 ; : : : ; z n )k 1 = sup jR n (t; z 1 ; : : : ; z n )j: Roth, <ref> [28] </ref> and [29], proves that inf kR n ( ; z 1 ; : : : ; z n )k 2 = fi (n 1 (log n) (d1)=2 ): (3) Of special interest for numerical integration are infinite low discrepancy sequences fz k g in which the definition of z k
Reference: [29] <author> Roth, K. F., </author> <title> On irregularities of distribution, IV, </title> <journal> Acta Arith., </journal> <volume> 37, </volume> <pages> 67-75, </pages> <year> 1980. </year>
Reference-contexts: ( ; z 1 ; : : : ; z n ), i.e., Z R 2 1=2 kR n ( ; z 1 ; : : : ; z n )k 1 = sup jR n (t; z 1 ; : : : ; z n )j: Roth, [28] and <ref> [29] </ref>, proves that inf kR n ( ; z 1 ; : : : ; z n )k 2 = fi (n 1 (log n) (d1)=2 ): (3) Of special interest for numerical integration are infinite low discrepancy sequences fz k g in which the definition of z k does not
Reference: [30] <author> Rust, J., </author> <title> Using randomization to break the curse of dimensionality, </title> <journal> Social Systems Research Institute Working Paper Series, </journal> <volume> No. 9429, </volume> <year> 1994. </year>
Reference: [31] <author> Sarkar, P.K. and Prasad, M. A., </author> <title> A comparative study of pseudo and quasi random sequences for the solution of integral equations, </title> <journal> J. Computational Physics, </journal> <volume> 68, </volume> <pages> 66-88, </pages> <year> 1978. </year>
Reference: [32] <author> Sobol, </author> <title> I.M., On the distribution of points in a cube and the approximate evaluation of integrals, </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 7, </volume> <pages> 86-112, </pages> <year> 1967. </year>
Reference-contexts: Examples of such infinite sequences are the Halton [9] and Sobol <ref> [32] </ref> sequences. For the reader's convenience, we give a short description of both these sequences later in this section. <p> This version of the Sobol sequence is used here. The sequence of direction numbers v i is generated by a primitive polynomial, see [16] and <ref> [32] </ref>, with coefficients in the field Z 2 with elements f0; 1g. Consider, for example, the primitive polynomial P (x) = x n + a 1 x n1 + : : : + a n1 x + 1 of degree n in Z 2 [x]. <p> The presence of prepayments and rules of distribution to different tranches will distort this monotonicity property. However, this property will be reflected to some extent in the cash flows of the ten tranches. It is a well-known property of the Sobol points 2 , see <ref> [32] </ref>, that the low dimensional components are more uniformly distributed than the high dimensional components. As pointed out by Sobol in [34], the Sobol points will be more efficient for numerical integration of functions for which the influence of the x j -th variable decreases as j increases.
Reference: [33] <author> Sobol, </author> <title> I.M., Numerical Monte Carlo methods (in Russian), </title> <publisher> Izdat "Nauka", </publisher> <address> Moscow, </address> <year> 1973. </year>
Reference-contexts: For more details, see for example, [11], [12], [15], and <ref> [33] </ref>. We now present the classical Monte Carlo algorithm. For brevity, we refer to it as the Monte Carlo algorithm.
Reference: [34] <author> Sobol, </author> <title> I.M., Quadrature formulas for functions of several variables satisfying general Lipschiz condition, </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 29, </volume> <pages> 935-941, </pages> <year> 1989. </year>
Reference-contexts: It is a well-known property of the Sobol points 2 , see [32], that the low dimensional components are more uniformly distributed than the high dimensional components. As pointed out by Sobol in <ref> [34] </ref>, the Sobol points will be more efficient for numerical integration of functions for which the influence of the x j -th variable decreases as j increases.
Reference: [35] <author> Spanier, J. and Maize, E., </author> <title> Quasi-Random Methods for Estimating Integrals using Relatively Small Samples, </title> <journal> SIAM Review, </journal> <volume> 36, </volume> <pages> 19-44, </pages> <year> 1994. </year>
Reference-contexts: Nevertheless, the Monte Carlo algorithm has several serious deficiencies, see for example, [21] and <ref> [35] </ref>. We mention just three of them here. Even though the rate of convergence is independent of the dimension, it is quite slow. Furthermore, there are fundamental philosophical and practical problems with generating independent random points; instead, pseudorandom numbers are used, see [16] and [40]. <p> Therefore the error bound will be decreased by reducing of the variation. Reducing the variation of the integrand has not been studied as extensively as reducing the variance. See, however, [4], [17], [20], and <ref> [35] </ref>. This is a major area for further research. An important advantage of the classical Monte Carlo algorithm and of the deterministic algorithms studied here, is that they can be utilized very generally.
Reference: [36] <author> Tezuka, Shu, </author> <title> A Generalization of Faure Sequences and its Efficient Implementation, </title> <type> Technical Report, </type> <institution> IBM Research, </institution> <address> Tokyo, </address> <year> 1994. </year>
Reference-contexts: reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. [21], [22], [39], [42], [23], and <ref> [36] </ref>. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D.
Reference: [37] <author> Traub, J. F., </author> <title> Average Case Computational Complexity of High-Dimensional Integration with Applications to Finance, NSF Symposium on Simulation and Estimation, </title> <institution> Department of Economics, University of California, Berkeley, </institution> <month> August, </month> <year> 1994. </year>
Reference-contexts: A January, 1994 article in Scientific American discussed the theoretical issues and reported that "Preliminary results obtained by testing certain finance problems suggest the superiority of the deterministic methods in practice." Further results were reported at a number of conferences including [24], <ref> [37] </ref>, [38]. We summarize our main conclusions regarding the evaluation of this CMO. The conclusions may be divided into three groups. 4 I. Deterministic and Monte Carlo Algorithms The Sobol algorithm consistently outperforms the Monte Carlo algorithm. The Sobol algorithm consistently outperforms the Halton algorithm.
Reference: [38] <author> Traub, J. F., </author> <title> Solving Hard Problems with Applications to Finance, </title> <booktitle> Thirteenth World Computer Congress, IFIP 94, </booktitle> <address> Hamburg, </address> <month> August, </month> <year> 1994. </year>
Reference-contexts: A January, 1994 article in Scientific American discussed the theoretical issues and reported that "Preliminary results obtained by testing certain finance problems suggest the superiority of the deterministic methods in practice." Further results were reported at a number of conferences including [24], [37], <ref> [38] </ref>. We summarize our main conclusions regarding the evaluation of this CMO. The conclusions may be divided into three groups. 4 I. Deterministic and Monte Carlo Algorithms The Sobol algorithm consistently outperforms the Monte Carlo algorithm. The Sobol algorithm consistently outperforms the Halton algorithm.
Reference: [39] <author> Traub, J. F., Wasilkowski, G. W., and Wozniakowski, H., </author> <title> Information-based Complexity, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year> <month> 43 </month>
Reference-contexts: for problems where highly reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. [21], [22], <ref> [39] </ref>, [42], [23], and [36]. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D.
Reference: [40] <author> Traub, J. F. and Wozniakowski, H., </author> <title> The Monte Carlo Algorithm with a Pseudorandom Generator, </title> <journal> Mathematics of Computation, </journal> <volume> 58, </volume> <pages> 323-339, </pages> <year> 1992. </year>
Reference-contexts: We mention just three of them here. Even though the rate of convergence is independent of the dimension, it is quite slow. Furthermore, there are fundamental philosophical and practical problems with generating independent random points; instead, pseudorandom numbers are used, see [16] and <ref> [40] </ref>.
Reference: [41] <author> Wasilkowski, G. W. and Wozniakowski, H., </author> <title> Explicit Cost Bounds of Algorithms for Multivariate Tensor Product Problems, </title> <note> to appear in J. Complexity, </note> <month> March </month> <year> 1995. </year>
Reference-contexts: We stress that the constants in the bounds (3) and (4) depend on the dimension d and good estimates of these constants are not known. Bounds with known constants and n independent of d are studied in <ref> [41] </ref> and [43]. Sequences satisfying the upper bound in (4) are known as low discrepancy sequences or quasi-random sequences. Although in widespread use, we believe the latter term to be misleading since there is nothing random about these deterministic sequences. <p> uniformly distributed for a small number of points; * Characterize analytic properties of classes of financial derivatives and design new al gorithms tuned to these classes; * Study error reduction techniques for deterministic algorithms; * There are numerous open theoretical problems concerning high dimensional integration and low discrepancy sequences; see <ref> [41] </ref> for some of them. We believe that their solution will aid in the design of better algorithms for finance problems. Acknowledgments I express my gratitude to I. Vanderhoof for his invaluable help, numerous discussions and suggestions, and introducing me to Goldman Sachs. I also express my gratitude to J.
Reference: [42] <author> Wozniakowski, H., </author> <title> Average Case Complexity of Multivariate Integration, </title> <journal> Bull. AMS (New Series), </journal> <volume> 24, </volume> <pages> 185-194, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction High-dimensional integrals are usually solved with Monte Carlo algorithms. Vast sums are spent annually on these algorithms. Theory [21], <ref> [42] </ref> suggests that low discrepancy algorithms are sometimes superior to Monte Carlo algorithms. However, a number of researchers, [14], [19], [20], report that their numerical tests show that this theoretical advantage decreases with increasing dimension. <p> problems where highly reliable results are needed. 3 Low Discrepancy Deterministic Algorithms In an attempt to avoid the deficiencies of the Monte Carlo algorithm, many deterministic algorithms have been proposed for computing high dimensional integrals for functions belonging to various subsets of L 2 (D), see e.g. [21], [22], [39], <ref> [42] </ref>, [23], and [36]. One class of such deterministic algorithms is based on low discrepancy sequences. First, we define discrepancy, which is a measure of deviation from uniformity of a sequence of points in D. <p> We approximate the integral of f from C d by the arithmetic mean of its values at x k , I (f ) = D 1 n X f (x k ); 8f 2 C d : Wozniakowski <ref> [42] </ref> relates the average case error of U n (f ) with the L 2 discrepancy, Z (I (f ) U n (f )) 2 w (df ) = D n (t; z 1 ; : : : ; z n )dt: (6) Thus, (3), (4), and (6) provide an average <p> We checked computationally that the values of the integrals of all tranches are about a10 6 with a 2 <ref> [2; 42] </ref>. Hence by taking " = 0:001 we introduce an absolute error which is on the order of several thousand dollars. We estimate K T (") for " = 0:001, where K T (") denotes the effective dimension of tranche T.
Reference: [43] <author> Wozniakowski, H., </author> <title> Tractability and Strong Tractability of Linear Multivariate Problems, </title> <journal> J. Complexity, </journal> <volume> 10, </volume> <pages> 96-128, </pages> <year> 1994. </year> <month> 44 </month>
Reference-contexts: We stress that the constants in the bounds (3) and (4) depend on the dimension d and good estimates of these constants are not known. Bounds with known constants and n independent of d are studied in [41] and <ref> [43] </ref>. Sequences satisfying the upper bound in (4) are known as low discrepancy sequences or quasi-random sequences. Although in widespread use, we believe the latter term to be misleading since there is nothing random about these deterministic sequences.
References-found: 43

