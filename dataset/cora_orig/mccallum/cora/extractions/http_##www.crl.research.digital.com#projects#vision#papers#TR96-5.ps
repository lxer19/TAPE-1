URL: http://www.crl.research.digital.com/projects/vision/papers/TR96-5.ps
Refering-URL: http://www.crl.research.digital.com/projects/vision/smart-kiosk.html
Root-URL: http://www.research.digital.com
Title: Visual Sensing of Humans for Active Public Interfaces  
Author: K. Waters, J. Rehg, M. Loughlin, S. B. Kang, and D. Terzopoulos 
Date: CRL 96/5 March, 1996  
Affiliation: Digital Equipment Corporation Cambridge Research Lab  
Abstract-found: 0
Intro-found: 1
Reference: <institution> References </institution>
Reference: [1] <author> J. Aggarwal and T. Huang, </author> <title> editors. Workshop on Motion of Non-Rigid and Articulated Objects, </title> <address> Austin, TX, November 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Using unobtrusive video cameras, they can provide a wealth of information about users, ranging from their three dimensional location to their facial expressions and body language. Although vision-based human sensing has received increasing attention in the past five years (see the proceedings <ref> [18, 1, 5] </ref>) relatively little work has been done on integrating this technology into functioning user-interfaces.
Reference: [2] <author> M. Argyle and M. Cook. </author> <title> Gaze and Mutual Gaze. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1985. </year>
Reference-contexts: For example, when communicating with a user, DECface is motivated to look at the person. Thus gaze punctuates the interaction <ref> [2] </ref>. This gaze behavior combines sensed information about the user's current location with predefined rules about the role of gaze in human interactions.
Reference: [3] <author> A. Azarbayejani and A. Pentland. </author> <title> Real-time self-calibrating stereo person tracking using 3-D shape estimation from blob features. </title> <type> Technical Report 363, </type> <institution> MIT Media Lab, Perceptual Computing Section, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The left hand portion of each display shows a plan view of the scene with a cross marking the 3D location of the individual projected onto the ground plane. gulate on the blob centroids from two cameras to localize each user in the environment (see <ref> [3] </ref> for a related approach.) Stereo tracking provides 3D localization of users relative to the kiosk display. This information can be used to initiate interactions based on distance from the kiosk and to provide DECface with cues for gaze behavior in a multi-user setting. <p> In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system. <p> We use stereo for depth recovery rather than the ground plane approach used in Alive because we do not want to segment the entire body or rely on the visibility of the user's feet (also see <ref> [3] </ref>). 7 Future Work The key to an effective public interface is natural communication between kiosk and users within the framework of the users' world. There are many ways in which we can develop our kiosk to approach this goal.
Reference: [4] <author> A. Baumberg and D. Hogg. </author> <title> An efficient method for contour tracking using active shape models. </title> <editor> In J. Aggarwal and T. Huang, editors, </editor> <booktitle> Proc. of Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <pages> pages 194-199, </pages> <address> Austin, Texas, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system.
Reference: [5] <editor> M. Bichsel, editor. </editor> <booktitle> Int. Workshop on Automatic Face and Gesture Recognition, </booktitle> <address> Zurich, Switzerland, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Using unobtrusive video cameras, they can provide a wealth of information about users, ranging from their three dimensional location to their facial expressions and body language. Although vision-based human sensing has received increasing attention in the past five years (see the proceedings <ref> [18, 1, 5] </ref>) relatively little work has been done on integrating this technology into functioning user-interfaces.
Reference: [6] <author> R.H.S. Carpenter. </author> <title> Movements of the Eyes. </title> <publisher> Pion Limited, </publisher> <year> 1972. </year>
Reference-contexts: Psychophysical studies of the human oculomotor system reveal that eye and head motions are coupled, with the relatively larger mass of the head resulting in longer transients compared to those of the eyes <ref> [6] </ref>. By contrast, a motivational behavior is determined by the internal "mental state" of the kiosk, which will in general encode the emotional condition 8 4 IMPLEMENTATION of the kiosk and any task directed plans that it may have.
Reference: [7] <institution> Digital Equipment Corporation. </institution> <note> DECtalk Programmers Reference Manual, </note> <year> 1985. </year>
Reference-contexts: The 3D position is chosen as the point of closest approach in the scene to the rays from the two cameras that pass through the detected centroid positions. 3.3 Feedback: DECface DECface, a talking synthetic face, is the visual complement of the speech synthesizer DECtalk <ref> [7] </ref>. Where DECtalk provides synthesized speech, DECface provides a synthetic face [27]. By combining the audio functionality of a speech synthesizer with the graphical functionality of a computer-generated face, it is possible to create a real-time agent as illustrated in Figure 3.
Reference: [8] <author> W. Freeman and C. Weissman. </author> <title> Television control by hand gestures. </title> <editor> In M. Bichsel, editor, </editor> <booktitle> Proc. of Intl. Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 179-183, </pages> <address> Zurich, Switzerland, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes <ref> [8] </ref>, and virtual environments [13, 9, 23, 15, 14, 17]. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment.
Reference: [9] <author> Mandala Group. Mandala: </author> <title> Virtual village. </title> <booktitle> In SIGGRAPH-93 Visual Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: Although vision-based human sensing has received increasing attention in the past five years (see the proceedings [18, 1, 5]) relatively little work has been done on integrating this technology into functioning user-interfaces. A few notable exceptions are the pioneering work of Krueger [13], the Mandala Group <ref> [9] </ref>, the Alive system [14], and a small body of work on gesture-based control for desktop and set-top box environments (see [17] for a survey.) This chapter describes our prototype kiosk and some experiments with vision-based sensing. <p> We obtain histogram models of each user through a manual segmentation stage. Like other researchers <ref> [28, 15, 9, 24] </ref>, we have found normalized color to be a descriptive, inexpensive, and reasonably stable feature for human tracking. We use local search during tracking to improve robustness and speed. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. <p> The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments [13, 9, 23, 15, 14, 17]. In particular, the Alive system [14], and the works that preceded it <ref> [13, 9] </ref>, have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images [19, 16, 21, 4, 22, 28, 3].
Reference: [10] <author> S. B. Kang, A. Johnson, and R. Szeliski. </author> <title> Extraction of concise and realistic 3-D models from real data. </title> <type> Technical Report 95/7, </type> <institution> Digital Equipment Corporation, Cambridge Research Lab, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: This 3D point distribution is then used to create a 3D mesh that is texture-mapped with a color panoramic image to produce a 3D reconstructed scene model (Figure 8 (right)) <ref> [10] </ref>. We plan to incorporate models created using this method into the kiosk that we are developing. We also plan to add alternate input modalities to our kiosk. Speech understanding will enable a user to interact with the kiosk in a direct way.
Reference: [11] <author> S. B. Kang and R. Szeliski. </author> <title> 3-D scene data recovery using omnidirectional multibaseline stereo. </title> <booktitle> In Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 364-370, </pages> <month> June </month> <year> 1996. </year> <note> REFERENCES 15 </note>
Reference-contexts: The 3D model of the scene is recovered by applying stereo on the multiple panoramic views to create a 3D point distribution (Figure 8 (left)) <ref> [11] </ref>. This 3D point distribution is then used to create a 3D mesh that is texture-mapped with a color panoramic image to produce a 3D reconstructed scene model (Figure 8 (right)) [10]. We plan to incorporate models created using this method into the kiosk that we are developing.
Reference: [12] <author> S. B. Kang, J. Webb, L. Zitnick, and T. Kanade. </author> <title> A multibaseline stereo system with active illumination and real-time image acquisition. </title> <booktitle> In Fifth International Conference on Computer Vision (ICCV'95), </booktitle> <pages> pages 88-93, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In our kiosk prototype, we use a pair of verged cameras with a six foot baseline. Extrinsic and intrinsic camera parameters are calibrated using a non-linear least-squares algorithm [25] and a planar target <ref> [12] </ref>. Given blob centroids in two images, triangulation proceeds through ray intersection.
Reference: [13] <author> M. Krueger. </author> <title> Artificial Reality II. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Although vision-based human sensing has received increasing attention in the past five years (see the proceedings [18, 1, 5]) relatively little work has been done on integrating this technology into functioning user-interfaces. A few notable exceptions are the pioneering work of Krueger <ref> [13] </ref>, the Mandala Group [9], the Alive system [14], and a small body of work on gesture-based control for desktop and set-top box environments (see [17] for a survey.) This chapter describes our prototype kiosk and some experiments with vision-based sensing. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. <p> The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments [13, 9, 23, 15, 14, 17]. In particular, the Alive system [14], and the works that preceded it <ref> [13, 9] </ref>, have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images [19, 16, 21, 4, 22, 28, 3].
Reference: [14] <author> P. Maes, T. Darrell, B. Blumberg, and A. Pentland. </author> <title> The ALIVE system: Wireless, full-body interaction with autonomous agents. </title> <journal> ACM Multimedia Systems, </journal> <note> Spring 1996. Accepted for publication. </note>
Reference-contexts: A few notable exceptions are the pioneering work of Krueger [13], the Mandala Group [9], the Alive system <ref> [14] </ref>, and a small body of work on gesture-based control for desktop and set-top box environments (see [17] for a survey.) This chapter describes our prototype kiosk and some experiments with vision-based sensing. <p> We refer to this as the public user-interface problem, to differentiate it from interactions that take place in structured, single-user desktop [23] or virtual reality <ref> [14] </ref> environments. We have developed a prototype Smart Kiosk which we are using to explore the space of effective public interactions. Our prototype has three functional components: human sensing, behavior, and graphical/audio output. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. <p> The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments [13, 9, 23, 15, 14, 17]. In particular, the Alive system <ref> [14] </ref>, and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images [19, 16, 21, 4, 22, 28, 3].
Reference: [15] <author> C. Maggioni. </author> <title> Gesturecomputer New ways of operating a computer. </title> <booktitle> In Proc. of Intl. Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 166-171, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We obtain histogram models of each user through a manual segmentation stage. Like other researchers <ref> [28, 15, 9, 24] </ref>, we have found normalized color to be a descriptive, inexpensive, and reasonably stable feature for human tracking. We use local search during tracking to improve robustness and speed. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment.
Reference: [16] <author> D. Metaxas and D. Terzopoulos. </author> <title> Shape and nonrigid motion estimation through physics-based synthesis. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6) </volume> <pages> 580-591, </pages> <year> 1993. </year>
Reference-contexts: In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system.
Reference: [17] <author> V. Pavlovic, R. Sharma, and T. Huang. </author> <title> Visual interpretation of hand gestures for human-computer interaction: A review. </title> <type> Technical Report UIUC-BI-AI-RCV-95-10, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: A few notable exceptions are the pioneering work of Krueger [13], the Mandala Group [9], the Alive system [14], and a small body of work on gesture-based control for desktop and set-top box environments (see <ref> [17] </ref> for a survey.) This chapter describes our prototype kiosk and some experiments with vision-based sensing. The kiosk prototype currently consists of a set of software modules that run on several workstations and communicate through message-passing. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment.
Reference: [18] <editor> A. Pentland, editor. </editor> <booktitle> Looking at People Workshop, </booktitle> <address> Chambery, France, </address> <month> August </month> <year> 1993. </year> <pages> IJCAI. </pages>
Reference-contexts: Using unobtrusive video cameras, they can provide a wealth of information about users, ranging from their three dimensional location to their facial expressions and body language. Although vision-based human sensing has received increasing attention in the past five years (see the proceedings <ref> [18, 1, 5] </ref>) relatively little work has been done on integrating this technology into functioning user-interfaces.
Reference: [19] <author> A. Pentland and B. Horowitz. </author> <title> Recovery of nonrigid motion and structure. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 13(7) </volume> <pages> 730-742, </pages> <year> 1991. </year>
Reference-contexts: In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system.
Reference: [20] <author> J. Rehg and T. Kanade. DigitEyes: </author> <title> Vision-based hand tracking for human-computer interaction. </title> <editor> In J. Aggarwal and T. Huang, editors, </editor> <booktitle> Proc. of Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <pages> pages 16-22, </pages> <address> Austin, TX, 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing <ref> [20] </ref>, set-top boxes [8], and virtual environments [13, 9, 23, 15, 14, 17]. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment.
Reference: [21] <author> J. Rehg and T. Kanade. </author> <title> Visual tracking of high DOF articulated structures: An application to human hand tracking. </title> <editor> In J. Eklundh, editor, </editor> <booktitle> Proc. of Third European Conf. on Computer Vision, </booktitle> <volume> volume 2, </volume> <pages> pages 35-46, </pages> <address> Stockholm, Sweden, 1994. </address> <publisher> Springer-Verlag. </publisher> <address> 16 REFERENCES </address>
Reference-contexts: In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system.
Reference: [22] <author> J. Rehg and T. Kanade. </author> <title> Model-based tracking of self-occluding articulated objects. </title> <booktitle> In Proc. of Fifth Intl. Conf. on Computer Vision, </booktitle> <pages> pages 612-617, </pages> <address> Boston, MA, 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system.
Reference: [23] <author> J. Segen. </author> <title> Controlling computers with gloveless gestures. </title> <booktitle> In Proc. Virtual Reality Systems Conf., </booktitle> <pages> pages 2-6, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: We refer to this as the public user-interface problem, to differentiate it from interactions that take place in structured, single-user desktop <ref> [23] </ref> or virtual reality [14] environments. We have developed a prototype Smart Kiosk which we are using to explore the space of effective public interactions. Our prototype has three functional components: human sensing, behavior, and graphical/audio output. <p> Figure 6 shows two snapshots of the audience during the story-telling experiment. 11 6 Previous Work There are two bodies of work that relate closely to the Smart Kiosk system. The first are investigations into vision-based interfaces for desktop computing [20], set-top boxes [8], and virtual environments <ref> [13, 9, 23, 15, 14, 17] </ref>. In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment.
Reference: [24] <author> M. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> Int. J. Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: This simple approach is very fast, and has proven useful in our current kiosk environment. 6 3 THE SMART KIOSK INTERFACE We use a modified version of the color histogram indexing and back-projection algorithm of Swain and Ballard <ref> [24] </ref> to track multiple people in real-time within approximately 15 feet of the kiosk. We obtain histogram models of each user through a manual segmentation stage. Like other researchers [28, 15, 9, 24], we have found normalized color to be a descriptive, inexpensive, and reasonably stable feature for human tracking. <p> We obtain histogram models of each user through a manual segmentation stage. Like other researchers <ref> [28, 15, 9, 24] </ref>, we have found normalized color to be a descriptive, inexpensive, and reasonably stable feature for human tracking. We use local search during tracking to improve robustness and speed. <p> Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system. The color histogram representation for blobs <ref> [24] </ref> that we employ is more descriptive than their single color blob model and therefore more appropriate to our task of identifying multiple users based on color alone.
Reference: [25] <author> R. Szeliski and S. B. Kang. </author> <title> Recovering 3D shape and motion from image streams using nonlinear least squares. </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 5(1) </volume> <pages> 10-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: We use motion stereo for proximity sensing at far distances, and color stereo for short range tracking of multiple users. In our kiosk prototype, we use a pair of verged cameras with a six foot baseline. Extrinsic and intrinsic camera parameters are calibrated using a non-linear least-squares algorithm <ref> [25] </ref> and a planar target [12]. Given blob centroids in two images, triangulation proceeds through ray intersection.
Reference: [26] <author> K. Waters. </author> <title> A muscle model for animating three-dimensional facial expressions. </title> <journal> Computer Graphics (SIGGRAPH '87), </journal> <volume> 21(4) </volume> <pages> 17-24, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: DECface has been created with the following key attributes for our reactive agent: an ability to speak an arbitrary piece of text at a specific speech rate in one of eight voices from one of eight faces, the creation of simple facial expressions under control of a facial muscle model <ref> [26] </ref>, and simple head and eye rotation. 3.4 Behavior The behavior module dictates the actions that the smart kiosk carries out in response to internal and external events. The behavioral repertoire of 3.4 Behavior 7 the kiosk is coded as a collection of behavior routines.
Reference: [27] <author> K. Waters and T. Levergood. </author> <title> An automatic lip-synchronization algorithm for synthetic faces. </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> 1(4) </volume> <pages> 349-366, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: The kiosk prototype currently consists of a set of software modules that run on several workstations and communicate through message-passing. It includes modules for real-time visual sensing (including motion detection, colored object tracking, and stereo ranging), a synthetic agent called DECface <ref> [27] </ref>, and behavior-based control. We describe this architecture and its implementation in the following sections and present experimental results related to proximity-based interactions and active gaze control. Section 2 describes some characteristics of the user-interface problem for public kiosk-like devices. <p> Where DECtalk provides synthesized speech, DECface provides a synthetic face <ref> [27] </ref>. By combining the audio functionality of a speech synthesizer with the graphical functionality of a computer-generated face, it is possible to create a real-time agent as illustrated in Figure 3.
Reference: [28] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <type> Technical Report 353, </type> <institution> MIT Media Lab, Perceptual Computing Section, </institution> <year> 1995. </year>
Reference-contexts: We obtain histogram models of each user through a manual segmentation stage. Like other researchers <ref> [28, 15, 9, 24] </ref>, we have found normalized color to be a descriptive, inexpensive, and reasonably stable feature for human tracking. We use local search during tracking to improve robustness and speed. <p> In particular, the Alive system [14], and the works that preceded it [13, 9], have explored the use of vision sensing to support interactions with autonomous agents in a virtual environment. The second body of related work is on algorithms for tracking human motion using video images <ref> [19, 16, 21, 4, 22, 28, 3] </ref>. Our color and motion blob algorithms are most closely related to those of Wren et al. [28], which are employed in the Alive system. <p> The second body of related work is on algorithms for tracking human motion using video images [19, 16, 21, 4, 22, 28, 3]. Our color and motion blob algorithms are most closely related to those of Wren et al. <ref> [28] </ref>, which are employed in the Alive system. The color histogram representation for blobs [24] that we employ is more descriptive than their single color blob model and therefore more appropriate to our task of identifying multiple users based on color alone.
References-found: 29

