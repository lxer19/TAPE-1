URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.locality_based_scheduling.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/leblanc/pubs.html
Root-URL: 
Title: Locality-Based Scheduling for Shared-Memory Multiprocessors  
Author: Evangelos P. Markatos and Thomas J. LeBlanc 
Address: Rochester, NY 14627 USA  
Affiliation: Computer Science Department University of Rochester  
Abstract: The last decade has produced enormous improvements in microprocessor performance without a corresponding improvement in memory or interconnection network performance. As a result, the relative cost of communication in shared-memory multiprocessors has increased dramatically. Although many applications could ignore the cost of communication and still achieve good performance on the previous generations of shared-memory machines, good performance on modern machines requires that communication be reduced or eliminated. One way to reduce the need for communication is to use scheduling polices that exploit knowledge of the location of data when assigning processes to processors, improving locality of reference by co-locating a process with the data it will require. This chapter presents an overview of the tradeoffs to be made in process scheduling, and evaluates locality-based scheduling techniques at the level of the operating system kernel, thread package, and parallelizing compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [Agarwal et al., 1990] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz, </author> <month> "APRIL: </month> <title> A Processor Architecture for Multiprocessing," </title> <booktitle> In Proceedings of the Seventeenth International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One way to address these problems is to recognize the dominant role of communication in current systems, and to adopt techniques for reducing communication in parallel programs. Cache architecture sensitive parallel application restructuring (CASPAR) [Cheriton et al., 1991], latency-tolerant techniques <ref> [Agarwal et al., 1990; Gupta et al., 1991a] </ref>, and the scheduling schemes discussed here are all steps in the right direction. These techniques will be even more important in the future if shared-memory machines are to be used efficiently for parallel programming.
Reference: [Anderson et al., 1991a] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> "Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism," </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 53-79, </pages> <month> October </month> <year> 1991. </year> <month> 49 </month>
Reference-contexts: The operating system can exploit this fact when preempting a thread that holds a critical resource, allowing the thread to complete the critical section on another processor <ref> [Anderson et al., 1991a] </ref>. * Lightweight threads admit a fine-grain decomposition, which offers many opportunities to perform load balancing. In addition, load balancing can be implemented via thread placement rather than process migration.
Reference: [Anderson et al., 1989] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy, </author> <title> "The Performance Implications of Thread Management Alternatives for Shared Memory Multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Historically, this overhead has been dominated by the cost of thread creation, destruction, and context switching. However, recent work has shown that the cost of these thread management operations can be drastically reduced, so that threads need only be an order of magnitude more expensive than a procedure call <ref> [Anderson et al., 1989] </ref>. Under these circumstances, threads should be cheap enough to use for fine-grain parallelism. Unfortunately, the overhead associated with lightweight threads is not limited to the cost of thread management.
Reference: [Anderson et al., 1991b] <author> T.E. Anderson, H.M. Levy, B.N. Bershad, and E.D. Lazowska, </author> <title> "The Interaction of Architecture and Operating System Design," </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the dramatic improvements in microprocessor performance due to recent advances in VLSI and RISC technology have not produced a corresponding improvement in application performance on shared-memory ma chines. Just as increased integer performance does not produce a corresponding improvement in operating system performance <ref> [Anderson et al., 1991b; Ousterhout, 1990] </ref>, an increase in computational power in shared-memory multiprocessors does not guarantee a corresponding improvement in application performance because communication quickly becomes the bottleneck.
Reference: [Bershad et al., 1988] <author> B.N. Bershad, E.D. Lazowska, H.M. Levy, and D.B. Wagner, </author> <title> "An Open Environment for Building Parallel Programming Systems," </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988 Symposium on Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pages 1-9, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Previous work on thread scheduling has focussed on the goal of load balancing. For example, in the process control scheme [Tucker and Gupta, 1989], Uniform System [Thomas and Crowther, 1988], Brown Threads [Doeppner Jr., 1987], and Presto <ref> [Bershad et al., 1988] </ref>, all threads of the same application are placed in a FIFO central work queue. Processors take threads from this queue and run them to completion. The load is evenly balanced in that no processor remains idle as long as there is work to be done.
Reference: [Black, 1990] <author> D. L. Black, </author> <title> "Scheduling Support for Concurrency and Parallelism in the Mach Operating System," </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: When hardware partitions are used, no two applications share a processor. A set of processors may be dedicated to an application for a relatively long fixed interval <ref> [Black, 1990] </ref> or for the entire duration of the application. Within its own hardware partition, each application may choose to allocate one process per processor, thereby avoiding entirely the overhead attributed to multiprogramming.
Reference: [Bokhari, 1987] <author> S. H. Bokhari, </author> <title> Assignment problems in parallel and distributed computing, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference: [Bolosky et al., 1989] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott, "Simple But Effective Techniques for NUMA Memory Management," </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <month> December </month> <year> 1989. </year>
Reference: [Cheriton et al., 1991] <author> D. R. Cheriton, H. A. Goosen, and P. Machanick, </author> <title> "Restructuring a Parallel Simulation to Improve Cache Behavior in a Shared-Memory Multiprocessor: </title>
Reference-contexts: One way to address these problems is to recognize the dominant role of communication in current systems, and to adopt techniques for reducing communication in parallel programs. Cache architecture sensitive parallel application restructuring (CASPAR) <ref> [Cheriton et al., 1991] </ref>, latency-tolerant techniques [Agarwal et al., 1990; Gupta et al., 1991a], and the scheduling schemes discussed here are all steps in the right direction. These techniques will be even more important in the future if shared-memory machines are to be used efficiently for parallel programming.
References-found: 9

