URL: ftp://arch.cs.ucdavis.edu/papers/sensitivity.ps.Z
Refering-URL: http://www.cs.washington.edu/education/courses/590o/
Root-URL: 
Email: chong@cs.ucdavis.edu  
Title: The Sensitivity of Communication Mechanisms to Bandwidth and Latency  
Author: Frederic T. Chongy, Rajeev Barua*, Fredrik Dahlgrenz, John D. Kubiatowicz*, and Anant Agarwal* 
Address: Contact:  
Affiliation: *Massachusetts Institute of Technology yUniversity of California at Davis zChalmers University  
Abstract: The goal of this paper is to gain insight into the relative performance of communication mechanisms as bisection bandwidth and network latency vary. We compare shared memory with and without prefetching, message passing with interrupts and with polling, and bulk transfer via DMA. We present two sets of experiments involving four irregular applications on the MIT Alewife multiprocessor. First, we introduce I/O cross-traffic to vary bisection bandwidth. Second, we change processor clock speeds to vary relative network latency. We establish a framework from which to understand a range of results. On Alewife, shared memory provides good performance, even on producer-consumer applications with little data-reuse. On machines with lower bisection bandwidth and higher network latency, however, message-passing mechanisms become important. In particular, the high communication volume of shared memory threatens to become difficult to support on future machines without expensive, high-dimensional networks. Furthermore, the round-trip nature of shared memory may not be able to tolerate the latencies of future networks. Keywords: shared-memory, message-passing, bandwidth, latency, multiprocessor 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Ye-ung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [43] supports both shared memory and messaging styles of communication, the Stanford Dash [28] supports shared memory and prefetching, MIT Alewife <ref> [1] </ref>, Fugu [30], and the Wisconsin Typhoon [38] support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] [46] [21] [10]. <p> Because message passing and prefetching can have some number of outstanding requests, the slope of their performance degradation is shallower than that for shared memory without prefetching. 3 Experimental Platform In this study, we made use of the MIT Alewife machine <ref> [1] </ref>. Alewife provides a unique opportunity to explore the behavior of a number of different communication mechanisms in a single hardware environment. <p> Data - message-passing payload and shared memory cache lines. 11 Machine Proc Topology Bisection Bandwidth Network Remote Local Refs (32 Processors) MHz Mbytes/s bytes/ Latency Miss Miss cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 <ref> [1] </ref> TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [45][27] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [7] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23]
Reference: [2] <author> R. Arpaci et al. </author> <title> Empirical evaluation of the Cray T3D: A compiler perspective. </title> <booktitle> In Proceedings of the 22nd Annual Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37] Cray T3D 150.0 4 fi 2 fi 2 Torus 4800 32.0 15 100 23 [40] <ref> [2] </ref> 2-proc clusters Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 64.0 110 300-600 80 [43][42] SGI Origin 200.0 Hypercube 10800 54.0 60 150 61 [25][17] 4-proc clusters * projected, # simulated, latencies given in processor cycles. Table 1: Parameter estimates for various 32-processor multiprocessors.
Reference: [3] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: We started from existing parallel iccg algorithms [20] [41] and implemented an interrupt message-passing version. The remaining four versions were all derived from the initial message-passing code. Implementation details are available in [9]. 4.3.1 iccg with Message Passing The iccg computation graph is essentially a dataflow computation <ref> [3] </ref> and is easily implemented via active messages. Non-local edges, i.e. edges between nodes on different processors, are communicated with active messages. Each processor keeps a presence counter per local node 9 to keep track of how many incoming edges have been satisfied for each node in its local memory.
Reference: [4] <author> M. J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for PDEs across multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: A molecule's position is determined by its own velocity and the force employed by other molecules within a certain cut-off radius. The molecules are partitioned into groups to minimize the communication between the groups, which is done with the RCB algorithm from <ref> [4] </ref>. These groups of molecules are then allocated on the nodes. Instead of relating each molecule to every other at each iteration of the application, a list of potentially interacting molecule-pairs is created every 20 iterations based on twice the cut-off radius.
Reference: [5] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, Shamik D. Sharma, and John Kubiatowicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In 1995 Symposium on Parallel Architectures and Algorithms, </booktitle> <address> Santa Barbara, Califor-nia, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: For active-message reception via polling, the receiving processor is computing along its main thread of computation, and if messages arrive they are deferred until the computation reaches a point where the user or compiler has explicitly inserted a polling call in the code. We use the Remote Queues abstraction <ref> [5] </ref>, which supports polling with selective interrupts for sys tem messages. 5 Bulk transfer: Bulk transfer is accomplished in Alewife by adding (address, length) pairs that describe blocks of data to the end of an active message. <p> Progress is more important to iccg than the other applications because of the data dependencies in its DAG computation. Polling provides greater control of message reception and computation progress, which allows for more balance processor progress in the computation <ref> [5] </ref>. The other mechanisms also avoid imbalance. Shared memory mechanisms do not use interrupts and are similar to polling.
Reference: [6] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <address> Can-cun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Polling cuts this overhead by about 35 percent. More importantly, interrupts cause dramatically more synchronization time than polling. Asynchronous interrupts can cause some processors to fall behind in the computation, causing long idle times in others <ref> [6] </ref>. Progress is more important to iccg than the other applications because of the data dependencies in its DAG computation. Polling provides greater control of message reception and computation progress, which allows for more balance processor progress in the computation [5]. The other mechanisms also avoid imbalance.
Reference: [7] <author> H. Burkhardt, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Topology Bisection Bandwidth Network Remote Local Refs (32 Processors) MHz Mbytes/s bytes/ Latency Miss Miss cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 [1] TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [45][27] KSR-2 20.0 Ring 1000 50.0 ? 126 18 <ref> [7] </ref> MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] [34] Intel Paragon 50.0 4 fi 8
Reference: [8] <author> S. Chandra, J.R. Larus, and A. Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs ? In Sixth International Conference on Architectural Support for Programming Languages 16 and Operating Systems (ASPLOS VI). ACM, </booktitle> <month> Oc--tober </month> <year> 1994. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications <ref> [8] </ref> [15] [44] [46] [21] [10]. <p> Because the relative effectiveness of communication mechanisms is also tied to basic machine parameters such as available bandwidth and latency, not surprisingly, various studies offer differing conclusions on the relative effectiveness of the mechanisms on the same application. For example, the simulation study of Chandra, Rogers, and Larus <ref> [8] </ref> using a basic machine model similar to the CM5 found that message passing EM3D performed roughly a factor of two better than the shared memory version. The two mechanisms were more or less indistinguishable on Alewife for the same application. <p> In contrast to ICCG, described later, the graph is undirected and updates are independent within each of the two phases. The code is barrier-synchronized between iterations and phases. We started with shared-memory and CM-5 bulk-transfer codes from the University of Wis-consin at Madison <ref> [8] </ref>. The interrupt and polling message-passing versions were developed from the bulk-transfer code, which was itself first adapted to Alewife. <p> Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers <ref> [8] </ref>, but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al [21], which focuses exclusively upon shared-memory mechanisms.
Reference: [9] <author> Frederic T. Chong and Anant Agarwal. </author> <title> Shared memory versus message passing for iterative solution of sparse, irregular problems. </title> <type> Technical report, </type> <institution> mit-lcs-tr-697, MIT Laboratory for Computer Science, </institution> <address> Cambridge, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: We started from existing parallel iccg algorithms [20] [41] and implemented an interrupt message-passing version. The remaining four versions were all derived from the initial message-passing code. Implementation details are available in <ref> [9] </ref>. 4.3.1 iccg with Message Passing The iccg computation graph is essentially a dataflow computation [3] and is easily implemented via active messages. Non-local edges, i.e. edges between nodes on different processors, are communicated with active messages.
Reference: [10] <author> Frederic T. Chong, Beng-Hong Lim, Ricardo Bian-chini, John Kubiatowicz, and Anant Agarwal. </author> <title> Application performance on the mit alewife multiprocessor. IEEE Computer: Special Issue on Emerging Applications for Shared-Memory Multiprocessors, </title> <month> De-cember </month> <year> 1996. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] [46] [21] <ref> [10] </ref>.
Reference: [11] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: We also observe this effect, to a lesser degree, on em3d and unstruc. The remainder of this section provides brief application descriptions and more detail on performance results. 4.1 em3d em3d is a small benchmark code originally developed at UC Berkeley to exercise the Split-C parallel language <ref> [11] </ref>. <p> This step makes sure that all non-local data necessary for the subsequent computation is available before any computation starts. Pre-communicating all the data simplifies the computation step. It requires, however, that non-local data be stored in buffers until they are needed in the computation. Berkeley Split-C study <ref> [11] </ref> calls these buffered copies "ghost 7 nodes," and they are equivalent to software man-aged cache. Our fine-grained message-passing implementations communicate values of these ghost nodes five double-words at a time. Each double-word represents the value of one remote node to be copied into a ghost node.
Reference: [12] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Technical Report TR/PA/92/86, CER-FACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: We measure the performance of the iccg sparse triangular solve kernel running on a large structural finite-element matrix, the bcsstk32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite <ref> [12] </ref>. We started from existing parallel iccg algorithms [20] [41] and implemented an interrupt message-passing version. The remaining four versions were all derived from the initial message-passing code.
Reference: [13] <author> Thomas H. Dunigan. </author> <title> Communication performance of the Intel Tochston Delta mesh. </title> <type> ORNL Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: 20.0 Ring 1000 50.0 ? 126 18 [7] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 <ref> [13] </ref> [34] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 [28] 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200
Reference: [14] <author> Thorsten von Eicken et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Queensland, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We will briefly describe how each of these operates on the Alewife machine. Message passing with interrupts: For message passing, Alewife supports active messages <ref> [14] </ref> of the form: send am (proc, handler, args...) which causes a message to be sent to processor proc, interrupt the processor, and invoke handler with args. An active message with a null handler, no body and no arguments, only takes 102 cycles plus .8 cycles per hop.
Reference: [15] <author> Babak Falsafi, Alvin R. Lebeck, Steven K. Rein-hardt, Ioannis Schoinas, Mark D. Hill James R. Larus, Anne Rogers, and David A. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] <ref> [15] </ref> [44] [46] [21] [10]. <p> They also found that node-to-network bandwidth was not critical in modern multiprocessors. Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols <ref> [15] </ref> increasingly important as processor speeds increase. Woo et al. [46] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [19] running the SPLASH [18] suite.
Reference: [16] <author> Marco Fillo, Stephen W. Keckler, W.J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gure-vich, and Whay S. Lee. </author> <booktitle> The M-Machine Multicom-puter. In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <address> Ann Arbor, MI, </address> <month> November </month> <year> 1995. </year> <journal> IEEE Computer Society. </journal>
Reference: [17] <author> Mike Galles. </author> <title> The SGI SPIDER chip. </title> <note> Available from http://www.sgi.com/Technology/spider paper/, </note> <year> 1996. </year>
Reference: [18] <author> Maya Gokhale, William Holmes, Andrew Kopser, Sara Lucas, Ronald Minnich, Douglas Sweeney, and Daniel Lopresti. </author> <title> Building and using a highly parallel programmable logic array. </title> <journal> Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Such costs will make message passing and specialized user-level protocols [15] increasingly important as processor speeds increase. Woo et al. [46] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [19] running the SPLASH <ref> [18] </ref> suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer. Although, Alewife's DMA mechanism is cheaper to initiate 15 than theirs, we also found bulk transfer to have performance problems.
Reference: [19] <author> Mark Heinrich, Jeffrey Kuskin, David Ofelt, John Heinlein, Joel Baxter, Jaswinder Pal Singh, Richard Simoni, Kourosh Gharachorloo, David Nakahira, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 274-285, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: fi 8 Mesh 216 5.4 15 N/A 10 [13] [34] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 [28] 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 <ref> [19] </ref> Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37] Cray T3D 150.0 4 fi 2 fi 2 Torus 4800 32.0 15 100 23 [40] [2] 2-proc clusters Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 <p> However, we notice that low-dimensional mesh architectures such as DASH and FLASH 1 approach the cross-over points. As processor speed increase, providing adequate bisection bandwidth will become increasingly expensive. 1 Note that FLASH has been redesigned to use the Ori gin network since <ref> [19] </ref>. 13 (See Table 1). node clock. Latency is for 1-way delivery of 24-bytes (see Table 1). Alewife is at 15. Context-Switching. 5.3 Network Latency Emulation We also perform an experiment which demonstrates that shared memory is less tolerant of network latency than message passing. <p> Such costs will make message passing and specialized user-level protocols [15] increasingly important as processor speeds increase. Woo et al. [46] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor <ref> [19] </ref> running the SPLASH [18] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
Reference: [20] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide. </title> <type> Technical Report SAND94-2692, </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: We measure the performance of the iccg sparse triangular solve kernel running on a large structural finite-element matrix, the bcsstk32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite [12]. We started from existing parallel iccg algorithms <ref> [20] </ref> [41] and implemented an interrupt message-passing version. The remaining four versions were all derived from the initial message-passing code. Implementation details are available in [9]. 4.3.1 iccg with Message Passing The iccg computation graph is essentially a dataflow computation [3] and is easily implemented via active messages.
Reference: [21] <author> C. Holt, M. Heinrich, J. P. Singh, E. Rothberg, and J. Hennessy. </author> <title> The effects of latency, occupancy and bandwidth on the performance of cache-coherent multprocessors. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, California, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] [46] <ref> [21] </ref> [10]. <p> Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers [8], but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al <ref> [21] </ref>, which focuses exclusively upon shared-memory mechanisms. Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin [38].
Reference: [22] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: fi 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] [34] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 <ref> [22] </ref> [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 [28] 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37]
Reference: [23] <author> Steve Keckler. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [24] <author> A. Klaiber and H. Levy. </author> <title> A comparison of message passing and shared memory for data-parallel programs. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This class includes papers by Lin and Snyder [29], Martonosi and Gupta [33], and LeBlanc and Markatos [26]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy <ref> [24] </ref> study the performance of programs which accesses shared memory or message passing runtime libraries. These libraries generated traces for shared memory and message passing simulators, to generate statistics on message traffic. However, their programs were not fined tuned for any particular architecture, and hence not fair to either.
Reference: [25] <author> James Laudon and Daniel Lenoski. </author> <title> The SGI Origin: A ccNUMA highly scalable server. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 241-251, </pages> <year> 1997. </year>
Reference: [26] <author> T. LeBlanc and E. Markatos. </author> <title> Shared memory vs. message passing in shared-memory multiprocesors. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <year> 1992. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [29], Martonosi and Gupta [33], and LeBlanc and Markatos <ref> [26] </ref>. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [24] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [27] <author> Charles E. Leiserson et al. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference: [28] <author> Daniel Lenoski, James Laudon, Kourosh Gharachor-loo, Wolf-Dietrich Weber, Anoop Gupta, John Hen-nessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-80, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Many research and commercial machines also sport various combinations of mechanisms. For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [43] supports both shared memory and messaging styles of communication, the Stanford Dash <ref> [28] </ref> supports shared memory and prefetching, MIT Alewife [1], Fugu [30], and the Wisconsin Typhoon [38] support several variants of shared memory and messaging styles. <p> fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] [34] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 <ref> [28] </ref> 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37] Cray T3D 150.0 4 fi 2 fi 2 Torus 4800 32.0 15 100
Reference: [29] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared-memory multiprocessors. </title> <booktitle> In ICPP, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder <ref> [29] </ref>, Martonosi and Gupta [33], and LeBlanc and Markatos [26]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [24] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [30] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agar-wal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Protection and Virtual Memory in a Multiuser, Multimodel Multiprocessor. </title> <note> Technical Memo MIT/LCS/TM-503, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [43] supports both shared memory and messaging styles of communication, the Stanford Dash [28] supports shared memory and prefetching, MIT Alewife [1], Fugu <ref> [30] </ref>, and the Wisconsin Typhoon [38] support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] [46] [21] [10].
Reference: [31] <author> N. K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for Maxwell's curl equations using non-orthogonal unstructured grids. </title> <type> Technical Report 92.04, </type> <institution> RIACS, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: The remainder of this section provides brief application descriptions and more detail on performance results. 4.1 em3d em3d is a small benchmark code originally developed at UC Berkeley to exercise the Split-C parallel language [11]. It models the propagation of electromagnetic waves through three-dimensional objects using algorithms described in <ref> [31] </ref>. em3d operates on an irregular bipartite graph which consists of E nodes on one side, representing electric field value at that point, and H nodes on the other, representing magnetic field value at that point.
Reference: [32] <author> Richard P. Martin, Amin M. Vahdat, David E. Culler, and Thomas E. Anderson. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 85-97, </pages> <year> 1997. </year>
Reference-contexts: The message-passing and bulk transfer curves are plotted for reference only. Their network latencies are not varied and are based upon Alewife network latencies. However, since our message-passing applications use asynchronous, unacknowledged communication, we expect that message-passing performance will remain relatively constant. Other studies <ref> [32] </ref> have found that asynchronous implementations of applications such as em3d are relatively insensitive to microsecond-latencies on networks of workstations. Referring back to Table 1, we see that network latency is a serious issue for shared memory that will worsen as processor speeds increase. <p> Although, Alewife's DMA mechanism is cheaper to initiate 15 than theirs, we also found bulk transfer to have performance problems. Our problems arose from the irregularity of our application suite, which caused high scatter/gather copying costs and limited data transfer size. Concurrent work at Berkeley <ref> [32] </ref> explores the effect of message-passing latency, overhead and bandwidth on networks of workstations. They measured performance of several programs written in Split-C and compared their results with predictions from the LogP model. The effects of overhead and gap on applications were predicted well by LogP.
Reference: [33] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in Message Passing and Shared Memory Implementations of a Standard Cell Router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages III 88-96, </pages> <year> 1989. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [29], Martonosi and Gupta <ref> [33] </ref>, and LeBlanc and Markatos [26]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [24] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [34] <author> Steven A. Moyer. </author> <title> Performance of the iPSC/860 node architecture. </title> <type> Technical Report IPC-TR-91-007, </type> <institution> Institute for Parallel Computation, School of Engineering and Applied Science, University of Virginia, </institution> <address> Charlottesville, VA 22903, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Ring 1000 50.0 ? 126 18 [7] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] <ref> [34] </ref> Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 [28] 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 <p> 2 Mesh 3200 256.0 7 N/A 7 [36] MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] <ref> [34] </ref> Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480 14.5 31 120 30 [28] 4-proc clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37] Cray
Reference: [35] <author> S.S. Mukherjee, S.D.Sharma, M.D.Hill, J.R.Larus, A.Rogers, and J.Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP) 1995, </booktitle> <pages> pages 68-79, </pages> <address> Santa Clara, CA, </address> <month> June </month> <year> 1995. </year> <journal> ACM. </journal> <volume> 17 </volume>
Reference-contexts: As explained earlier in this section, this stems from its low ratio of computation to communication, making its communication time gains more significant. 4.2 unstruc unstruc <ref> [35] </ref> simulates fluid flows over three-dimensional physical objects, represented by an unstructured mesh. The code operates upon nodes, edges between nodes, and faces that connect three or four nodes. We used mesh2k as an input dataset, a 2000 node irregular mesh provided with the code. <p> Bulk transfer uses fewer messages than finer-grained message-passing and thus few interrupts. 4.4 moldyn moldyn is a molecular dynamics application, and like UNSTRUC, it was developed by the University of Maryland and the University of Wis-consin at Madison <ref> [35] </ref>. The molecules are uniformly distributed over a cuboidal region with a Maxwellian distribution of initial velocities. A molecule's position is determined by its own velocity and the force employed by other molecules within a certain cut-off radius.
Reference: [36] <author> M.D. Noakes, D.A.Wallach, and W.J. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In In Proceedings of the 20th Annual International Symposium on Computer Architecture 1993, </booktitle> <pages> pages 224-235, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: cycle Latency Latency MIT Alewife 20.0 4 fi 8 Mesh 360 18.0 15 50 11 [1] TMC CM5 33.0 4-ary Fat-Tree 640 19.4 50 N/A 16 [45][27] KSR-2 20.0 Ring 1000 50.0 ? 126 18 [7] MIT J-Machine 12.5 4 fi 4 fi 2 Mesh 3200 256.0 7 N/A 7 <ref> [36] </ref> MIT M-Machine #100.0 4 fi 4 fi 2 Mesh 12800 128.0 10 154 21 [16][23] Intel Delta 40.0 4 fi 8 Mesh 216 5.4 15 N/A 10 [13] [34] Intel Paragon 50.0 4 fi 8 Mesh 2800 56.0 12 N/A 10 [22] [34] Stanford DASH 33.0 2 fi 4 480
Reference: [37] <author> Steven K. Reinhardt. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [38] <author> Steven K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [43] supports both shared memory and messaging styles of communication, the Stanford Dash [28] supports shared memory and prefetching, MIT Alewife [1], Fugu [30], and the Wisconsin Typhoon <ref> [38] </ref> support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] [46] [21] [10]. <p> Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin <ref> [38] </ref>. Chandra, Larus and Rogers compare four applications on a simulation of a message-passing machine similar to a CM-5 multiprocessor against a simulation of a hypothetical machine also similar to a CM-5, but extended by shared-memory hardware.
Reference: [39] <author> Steven K. Reinhardt, Robert W. Pfile, and David A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference: [40] <author> Steven P. Reinhardt and Winnie Williams. </author> <title> Cray T3D Software: Delivering the performance. 55 min. video, </title> <year> 1994. </year>
Reference-contexts: clusters Stanford FLASH *200.0 4 fi 8 Mesh 3200 16.0 62 352 40 [19] Wisconsin T0 #200.0 none simulated N/A N/A 200 1461 40 [39][37] Wisconsin T1 #200.0 none simulated N/A N/A 200 401 40 [39][37] Cray T3D 150.0 4 fi 2 fi 2 Torus 4800 32.0 15 100 23 <ref> [40] </ref> [2] 2-proc clusters Cray T3E 300.0 4 fi 4 fi 2 Torus 19200 64.0 110 300-600 80 [43][42] SGI Origin 200.0 Hypercube 10800 54.0 60 150 61 [25][17] 4-proc clusters * projected, # simulated, latencies given in processor cycles. Table 1: Parameter estimates for various 32-processor multiprocessors.
Reference: [41] <author> R. Schreiber and W. Tang. </author> <title> Vectorizing the conjugate gradient method. </title> <booktitle> In Proceedings Symposium CYBER 205 Applications, </booktitle> <address> Ft. Collins, CO, </address> <year> 1982. </year>
Reference-contexts: We measure the performance of the iccg sparse triangular solve kernel running on a large structural finite-element matrix, the bcsstk32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite [12]. We started from existing parallel iccg algorithms [20] <ref> [41] </ref> and implemented an interrupt message-passing version. The remaining four versions were all derived from the initial message-passing code. Implementation details are available in [9]. 4.3.1 iccg with Message Passing The iccg computation graph is essentially a dataflow computation [3] and is easily implemented via active messages.
Reference: [42] <author> Steve Scott. </author> <type> Personal communication, </type> <month> November </month> <year> 1996. </year>
Reference: [43] <author> Steven L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS VII, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: We now understand to a much greater extent than before the implementation tradeoffs. Many research and commercial machines also sport various combinations of mechanisms. For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E <ref> [43] </ref> supports both shared memory and messaging styles of communication, the Stanford Dash [28] supports shared memory and prefetching, MIT Alewife [1], Fugu [30], and the Wisconsin Typhoon [38] support several variants of shared memory and messaging styles.
Reference: [44] <author> Jaswinder Pal Singh, Chris Holt, and John Hennessy. </author> <title> Load balancing and data locality in adaptive hierarchical N-body methods: Barnes-hut, fast multipole, and radiosity. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 27(2), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] <ref> [44] </ref> [46] [21] [10].
Reference: [45] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: This interrupt-driven approach is the most intuitive notion of active messages, but processor interrupts can be very expensive. Message passing with polling: Active messages come in two flavors, those received via interrupt and those received via polling. In fact, on systems such as the Thinking Machines CM5 <ref> [45] </ref>, the expense of interrupts led to the predominant use of polling.
Reference: [46] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI). ACM, </booktitle> <month> October </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [8] [15] [44] <ref> [46] </ref> [21] [10]. <p> Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols [15] increasingly important as processor speeds increase. Woo et al. <ref> [46] </ref> compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [19] running the SPLASH [18] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
References-found: 46

