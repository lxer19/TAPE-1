URL: http://www.cs.ualberta.ca/~upal/cluster/p2/p2.ps
Refering-URL: http://www.cs.ualberta.ca/~upal/cluster/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Monte Carlo Comparison of Non-hierarchical Unsupervised Classifiers  
Author: Muhammad Afzal Upal 
Date: November 5, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [And73] <author> Michael R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Cluster analysis is based on a mathematical formulation of a measure of similarity. Some well known cluster analysis algorithms are the leader algorithm, the k-means algorithm, ISODATA, and the quick partition algorithm <ref> [And73] </ref> [Har75]. The leader algorithm starts by choosing the first object as a seed for the first class. The rest of the objects are processed sequentially in order. <p> There is a variety of choices for the distance measure to be used and the user has to decide which one to use for clustering data. Some typical distance measures include Euclidean distance, Manhatten distance, Calhoun distance and Lance and Williams' distance <ref> [And73] </ref>. Without any a priori knowledge about the target classification it is difficult to decide which distance measure to prefer over the others. The problem with the formulation of a general cluster analysis algorithm is the difficulty of finding a universal similarity measure. <p> Each of these measures makes sense in certain cases. As Anderberg <ref> [And73] </ref> states: There might be several different but meaningful ways of organizing the data into groups; each of these alternative clusterings may be taken as the realization of some classification principle embodied in the data and representing one of perhaps several facets of the problem. <p> In spirit of [Ran71] our approach is that, "every definition of cluster is natural for some situation and therefore that problem can be considered without this aspect." We will not discuss the problem of defining a class; the interested reader is referred to <ref> [And73] </ref> and [DJ88]. Anderberg [And73] suggests classifying the classification algorithms themselves in order to quantify their similarities and differences. While this approach highlights the similarities and differences among classifiers, it begs the question because we still must determine the rationale for so classifying the algorithms. <p> In spirit of [Ran71] our approach is that, "every definition of cluster is natural for some situation and therefore that problem can be considered without this aspect." We will not discuss the problem of defining a class; the interested reader is referred to <ref> [And73] </ref> and [DJ88]. Anderberg [And73] suggests classifying the classification algorithms themselves in order to quantify their similarities and differences. While this approach highlights the similarities and differences among classifiers, it begs the question because we still must determine the rationale for so classifying the algorithms. <p> The computation of external criteria involves classifying a labeled data set and then measuring the extent of retrieval of the "original" classification. Various indices have been proposed to measure the recovery of the classification including the Rand index [Ran71], the corrected Rand index [DJ88], the Jaccard index <ref> [And73] </ref>, the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. <p> u (a + b)(a + c) a (a + b)(a + c) 6.1.4 Jaccard Statistic The Jaccard statistic, used to measure cluster recovery in [Mil81a] and [MC85], ignores d (i.e., the number of pairs of objects classified in different classes by both the classifications) both in the numerator and denominator <ref> [And73] </ref>. <p> One of the problems with existing cluster analysis algorithms is that they find classes in random data <ref> [And73] </ref>. To study the behavior of ART2 and Snob on random data, 50 fi 8 random numbers (shown in Figure 7.19) were generated and fed to both classifiers as an eight dimensional 50 point data set.
Reference: [Bak74] <author> F. B. Baker. </author> <title> Stability of two hierarchical grouping techniques case I: sensitivity of data to errors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 69 </volume> <pages> 440-445, </pages> <year> 1974. </year>
Reference-contexts: Various indices have been proposed to measure the recovery of the classification including the Rand index [Ran71], the corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index <ref> [Bak74] </ref>, and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques.
Reference: [BAM82] <author> R. K. Blashfield, M. S. Aldenderfer, and L. C. Morey. </author> <title> Cluster analysis software. </title> <journal> Handbook of Statistics, </journal> <volume> 2 </volume> <pages> 245-266, </pages> <year> 1982. </year>
Reference-contexts: The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] <ref> [BAM82] </ref> [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [BBBK80] <author> C. K. Bayne, C. L. Beauchmap, C. L. Begovich, and V. E. Kane. </author> <title> Monte Carlo comparison of selected clustering procedures. </title> <journal> Pattern Recognition, </journal> <volume> 12 </volume> <pages> 51-60, </pages> <year> 1980. </year>
Reference-contexts: Jain et al. [JIG86] studied the performance of classification algorithms on random data and concluded that the complete linkage method performed best. Mezzich and Solmonoff [MS80], after comparing eighteen clustering methods using multiple indices on four data sets, also concluded that complete linkage method was best. Bayne et al. <ref> [BBBK80] </ref> compared thirteen methods in a Monte Carlo study and concluded that the k-means algorithm performed best. Milligan et al. [MSS83] compared the single 8 link, complete link, group average and Ward's technique and concluded that Ward's method performed best. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] <ref> [BBBK80] </ref> [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [BJ90] <author> Russel Bearle and Tom Jackson, </author> <title> editors. </title> <booktitle> Neural computing. </booktitle> <address> Bris-tol, New York, </address> <year> 1990. </year>
Reference-contexts: Minsky and Pappert later showed that the simple two layered feed-forward models were unable to learn non-linearly separable functions like the Xor function [MP88]. However, by adding feedback connections and/or 22 adding hidden layers between the input and output layers, neural networks can learn to approximate arbitrary functions <ref> [BJ90] </ref>. Their learning ability can be further increased if, instead of performing a weighted sum of the input patterns, a non-linear function of the inputs is used. The perceptron model, however, cannot learn autonomously as it needs an external supervisor (see Figure 3.4) to train it on previously classified data.
Reference: [Bla76] <author> R. K. Blashfield. </author> <title> Mixtures model test of cluster analysis. </title> <journal> Psychological Bulliten, </journal> <volume> 83 </volume> <pages> 377-388, </pages> <year> 1976. </year>
Reference-contexts: The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] <ref> [Bla76] </ref> [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [BPL69] <author> D. M. Boulton, I. Pilowsky, and S. LeVine. </author> <title> The classification of depression by numerical taxonomy. </title> <journal> British Journal of Psychiatry, </journal> <volume> 115 </volume> <pages> 937-945, </pages> <year> 1969. </year>
Reference-contexts: For performing experiments we chose Snob and ART2 as representative systems. These systems have been widely used. Applications of Snob include classification of depression <ref> [BPL69] </ref>, classification of spectral data [DPC93], classification of protein spectra [DZC94], classification of facial emotions [PK94a], and classification of illness behavior in pain [PK94b]. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88].
Reference: [Bur91] <author> Laura I. Burke. </author> <title> Clustering characterstization of adaptive resonance. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 485-491, </pages> <year> 1991. </year>
Reference-contexts: Considering one class to be an ideal classification we get Table 7.41. 92 Chapter 8 Conclusions Various theoretical studies compare ART2 with cluster analysis algorithms such as the k-means and the leader algorithm [Lip87] <ref> [Bur91] </ref>. Lippmann [Lip87] states that ART "...implements a clustering algorithm that is very similar to the simple sequential leader algorithm." Burke [Bur91], on the other hand, concludes that the k-means algorithm is "more fundamentally analogous" to ART2. Our results support both these theoretical claims. <p> an ideal classification we get Table 7.41. 92 Chapter 8 Conclusions Various theoretical studies compare ART2 with cluster analysis algorithms such as the k-means and the leader algorithm [Lip87] <ref> [Bur91] </ref>. Lippmann [Lip87] states that ART "...implements a clustering algorithm that is very similar to the simple sequential leader algorithm." Burke [Bur91], on the other hand, concludes that the k-means algorithm is "more fundamentally analogous" to ART2. Our results support both these theoretical claims. Like the leader algorithm, ART2 requires specification of similarity distance (vigilance threshold) and like the k-means algorithm ART2's classification is affected by the order of data.
Reference: [CKS + 88a] <author> Peter Cheeseman, James Kelly, Mathew Self, John Stutz, Will Taylor, and Don Freeman. </author> <title> Bayesian classification. </title> <booktitle> In Proceedings of 7th National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year> <month> 97 </month>
Reference-contexts: The designers of a Bayesian classifier are faced with the computationally intractable problem of searching the hypothesis space for the optimal distribution that produced the observed data and the controversial problem of estimating the priors. 2.1 Autoclass Cheeseman et al. [CKS + 88b], <ref> [CKS + 88a] </ref>, [CSH91] implement Bayesian classification in Autoclass. Their strategy involves making simplifying assumptions about the classification model.
Reference: [CKS + 88b] <author> Peter Cheeseman, James Kelly, Matthew Self, John Stutz, Will Taylor, and Don Freeman. </author> <title> Autoclass: A Bayesian classification system. </title> <editor> In Michael B. Morgan, editor, </editor> <booktitle> Proceedings of Fifth International Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <address> San Mateo, California, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The ad hoc work that has been done has come from the developers of various classification systems, each in turn claiming their system to be superior to other systems without any comparative evaluation <ref> [CKS + 88b] </ref> [Wal90]. Lutz Prechelt in a recent study [Pre94] examined 113 journal articles about neural net learning algorithms. Only 6% show results for more than one problem using real-world data. <p> This leads us to formulation of the Bayesian approach to classification [Seb62] [Hun74] <ref> [CKS + 88b] </ref>. An alternative criterion is that best theory is the one that provides least complex explanation of data [Sol64] [Hun74] [WG84]. Explanation includes both the description of the theory as well as the specification of data given that theory. <p> If H denotes the hypothesis and D the observed data, Bayes' rule states that the probability P (HjD) that the hypothesis explains the data is proportional to the probability P (DjH) of observing the data if the hypothesis were true <ref> [CKS + 88b] </ref>, P (HjD) = P (D) where P (D) is called the prior probability of data and can be obtained as a normalizing constant. In case of classification, the hypothesis is the number and probability distribution of classes. <p> Bayes' rule does not provide an algorithm for classification. The designers of a Bayesian classifier are faced with the computationally intractable problem of searching the hypothesis space for the optimal distribution that produced the observed data and the controversial problem of estimating the priors. 2.1 Autoclass Cheeseman et al. <ref> [CKS + 88b] </ref>, [CKS + 88a], [CSH91] implement Bayesian classification in Autoclass. Their strategy involves making simplifying assumptions about the classification model. <p> 1.00 1.00 1.00 1.00 Table 7.40: Unordered data with error dimensions: dimensions = 8+3, size = 50 and varying classes from 2 to 24 90 on R 0 . 7.5 Performance on random data It is a desirable property of classifiers that they do not find classes in random data <ref> [CKS + 88b] </ref>.
Reference: [CSH91] <author> Peter Cheeseman, John Stutz, and Robin Hanson. </author> <title> Bayesian classification with correlation and inheritance. </title> <editor> In John Mylopoulos and Ray Reiter, editors, </editor> <booktitle> Proceedings of 12th International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <pages> pages 692-698, </pages> <address> San Mateo, California, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Applications of Snob include classification of depression [BPL69], classification of spectral data [DPC93], classification of protein spectra [DZC94], classification of facial emotions [PK94a], and classification of illness behavior in pain [PK94b]. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra <ref> [CSH91] </ref>. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88]. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals [Sim90] and predicting object motion [ZT95]. Here we will analyze these systems to assess the potential and limitations of each technique. <p> The designers of a Bayesian classifier are faced with the computationally intractable problem of searching the hypothesis space for the optimal distribution that produced the observed data and the controversial problem of estimating the priors. 2.1 Autoclass Cheeseman et al. [CKS + 88b], [CKS + 88a], <ref> [CSH91] </ref> implement Bayesian classification in Autoclass. Their strategy involves making simplifying assumptions about the classification model. Rather than searching the entire hypothesis space by considering all possible states of the world they focus on a limited number of possible states thereby reducing the number of possibilities to be analyzed. <p> Cheeseman et al. <ref> [CSH91] </ref> describe different models including single discrete model, single real attribute model, independent attribute model, fully co-variant discrete model, fully co-variant real model, the block co-variance model and the model for a mixed population. All have been incorporated in the most recent version of Autoclass, Autoclass IV, released in 1991.
Reference: [Dem77] <author> Arthur P. Dempster. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Royal Journal of Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: generated independently of each other to facilitate the calculation of P (Xj; ; m), P (Xj; ; m) = i=1 Autoclass also assumes that the attributes are independent of each other given the classification, which gives, P (Xj; ; m) = i=1 k=1 Autoclass uses the Expectation Maximization (EM) algorithm <ref> [Dem77] </ref> to estimates the class parameters that maximize the posterior probability of the parameters for a given number of classes.
Reference: [DJ79] <author> Richard C. Dubes and Anil K. Jain. </author> <title> Validity studies in clustering methodologies. </title> <journal> Pattern Recognition, </journal> <volume> 11 </volume> <pages> 235-254, </pages> <year> 1979. </year>
Reference-contexts: Another approach adopted by some researchers is to compile a list of generally acceptable criteria (called admissibility criteria) such as correctness and then examine which of the algorithms adhere to these criteria [FV71] <ref> [DJ79] </ref>. <p> By providing information about their behavior on different kinds of data, these studies have increased our understanding of clustering algorithms. Most of these studies focus on the cluster analysis techniques. As Dubes et al. <ref> [DJ79] </ref> have noted, the trend in AI and the pattern recognition literature has been to ignore the problems of validation and comparison, and focus on applications of these methods.
Reference: [DJ88] <author> Richard C. Dubes and Anil K. Jain. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Unsupervised classification seeks a convenient and valid organization of data and not to establish rules for separating future data into categories <ref> [DJ88] </ref>. Objects can be also classified either hierarchically or non-hierarchically. Hierarchical classification is a sequence of classifications in which larger classes are obtained through the merger of smaller ones. <p> Without any a priori knowledge about the target classification it is difficult to decide which distance measure to prefer over the others. The problem with the formulation of a general cluster analysis algorithm is the difficulty of finding a universal similarity measure. Dubes et al. <ref> [DJ88] </ref> write, There is no single best criterion for obtaining a partition because no precise and workable definition of "cluster" exists. Clusters can be of any arbitrary shapes and sizes in a multidimensional pattern space. <p> In spirit of [Ran71] our approach is that, "every definition of cluster is natural for some situation and therefore that problem can be considered without this aspect." We will not discuss the problem of defining a class; the interested reader is referred to [And73] and <ref> [DJ88] </ref>. Anderberg [And73] suggests classifying the classification algorithms themselves in order to quantify their similarities and differences. While this approach highlights the similarities and differences among classifiers, it begs the question because we still must determine the rationale for so classifying the algorithms. <p> The admissibility approach provides insight into the classification methods but it does not provide any basis for a fair comparison of the systems <ref> [DJ88] </ref> as one would still need to measure the degree of admissibility of a criterion in order to measure the extent of conformity to a particular criterion. <p> A theoretical comparison of unsupervised classifiers is also not feasible as they are almost impossible to model mathematically in such a way that models can be compared <ref> [DJ88] </ref>. As Milligan notes, none of the classifiers in use provide any theoretical proof which ensures that the algorithm will recover the true cluster structure. <p> The computation of external criteria involves classifying a labeled data set and then measuring the extent of retrieval of the "original" classification. Various indices have been proposed to measure the recovery of the classification including the Rand index [Ran71], the corrected Rand index <ref> [DJ88] </ref>, the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. <p> Some suggest that the practical range of values is narrow and the limit is approached quickly <ref> [DJ88] </ref>.
Reference: [Dod87] <editor> S. Dodanki. Connectionism. In Stuart C. Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 199-203. </pages> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: to be able to recognize edges in a picture in about 200 ms, whereas a typical edge detection algorithm would detect edges in a 1K fi 1K image in about 10 7 steps which would take in the order of 500 ms on a state of the art serial computer <ref> [Dod87] </ref>. The comparison becomes more amazing if we consider that the switching time of a typical gate on a silicon computer is 10 ns whereas it takes on the order of 1ms for a neuron to change its state, 10 5 times slower. <p> Some believe that our brain must perform these tasks in a highly parallel and distributed manner <ref> [Dod87] </ref>. Evidence from neurophysiology suggests on average there are about 100 billion neurons in human brain, each connected to thousands of others. This lends support to the theory of a parallel and distributed architecture of the brain. <p> This lends support to the theory of a parallel and distributed architecture of the brain. Connectionism, as a field, attempts to formalize such a conceptual paradigm and uses it to further our understanding of intelligent systems <ref> [Dod87] </ref>. The basic computing elements in a connectionist architecture are simple processing units called neurons. Figure 3.3 shows model of a typical biological neuron shown in Figure 3.2. The neurons are extensively interconnected into networks called neural networks. Neural network research analyzes and studies properties of these networks.
Reference: [DPC93] <author> David L. Dowe, E' Papp, and S. J. Cox. </author> <title> Spectral classification of radiometric data. </title> <booktitle> In Proceedings of Advanced Remote Sensing Conference, </booktitle> <volume> volume 2, </volume> <pages> pages 223-232. </pages> <address> UNSW, Sydney, </address> <year> 1993. </year>
Reference-contexts: For performing experiments we chose Snob and ART2 as representative systems. These systems have been widely used. Applications of Snob include classification of depression [BPL69], classification of spectral data <ref> [DPC93] </ref>, classification of protein spectra [DZC94], classification of facial emotions [PK94a], and classification of illness behavior in pain [PK94b]. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88].
Reference: [DZC94] <author> David L. Dowe, J. D. Zakis, and I. Cosic. </author> <title> Classification of the protein spectra derived for the Resonant Recognition model using Minimum Message Length principle. </title> <booktitle> In Proceedings of 17th Australian Computer Science Conference, Christchurch, NZ, </booktitle> <pages> pages 209-216. </pages> <year> 1994. </year>
Reference-contexts: For performing experiments we chose Snob and ART2 as representative systems. These systems have been widely used. Applications of Snob include classification of depression [BPL69], classification of spectral data [DPC93], classification of protein spectra <ref> [DZC94] </ref>, classification of facial emotions [PK94a], and classification of illness behavior in pain [PK94b]. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88].
Reference: [EA81] <author> Renee' Elio and John R. Anderson. </author> <title> The effect of information order and learning mode on schema abstraction. </title> <journal> Memory and Cognition, </journal> <volume> 12 </volume> <pages> 20-30, </pages> <year> 1981. </year> <month> 98 </month>
Reference-contexts: Thus, when the sequence of the patterns changes, the learned weights may be different, which may lead to a different classification. This makes cognitive sense as humans classify objects differently depending upon the order in which they are shown <ref> [EA81] </ref>, but this is not desirable in scientific applications. In exploratory analysis, a user has no knowledge about the order of inputs and it would be reassuring to know that reordering the inputs would not give a totally different classification.
Reference: [Ede79] <author> C. Edelbrock. </author> <title> Mixture model tests of hierarchical clustering algorithms-problem of classifying everybody. </title> <journal> Multivariate Behavioral Research, </journal> <volume> 14 </volume> <pages> 367-384, </pages> <year> 1979. </year>
Reference-contexts: c = i n i: ! X X 2 ; (6.3) 2 2 @ i 2 + j 2 A : (6.4) 6.1.1 Rand Statistic The Rand statistic [Ran71] is an objective criterion for evaluation of classification methods used extensively in Monte Carlo studies [Mil81a] ,[MSS83] ,[MC85] [KF75], [Moj77] and <ref> [Ede79] </ref>. Hubert and Arabie say the Rand index, "... seems to be one of the most popular alternatives for comparing classifications." Green and Rao independently suggest a statistic that reduces to the complement of Rand statistic [Roh74]. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] <ref> [Ede79] </ref> [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [Eve74] <author> B. S. Everitt, </author> <title> editor. Cluster Analysis. </title> <publisher> Halstead Press, </publisher> <address> London, </address> <year> 1974. </year>
Reference-contexts: The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] <ref> [Eve74] </ref> [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [FM83] <author> E. Fowlkes and C. Mallows. </author> <title> A method for comparing two hierarchical clusterings. </title> <journal> Journal of American Statistical Association, </journal> <volume> 78 </volume> <pages> 553-569, </pages> <year> 1983. </year>
Reference-contexts: Various indices have been proposed to measure the recovery of the classification including the Rand index [Ran71], the corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index <ref> [FM83] </ref>, the Gamma index [Bak74], and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. <p> It is defined as R = [a + d]=[a + b + c + d] R = [a + d]= n ! It can be shown that the mean value of the Rand measure, E (R), approaches 1:00 as the number of classes increases without limit [Ran71] <ref> [FM83] </ref>. Some suggest that the practical range of values is narrow and the limit is approached quickly [DJ88]. <p> 0 = i j n ij ! " 2 P 2 P 2 1 " j n :j ! P 2 1= n !# i n i: ! j n :j ! : 6.1.3 Fowlkes and Mallows Statistic This statistic is given by the formula derived by Fowlkes and Mallows <ref> [FM83] </ref> for comparing hierarchical clustering methods. It is general enough to be used for comparing non-hierarchical methods and has been used widely [Mil80] [FM83]. <p> !# i n i: ! j n :j ! : 6.1.3 Fowlkes and Mallows Statistic This statistic is given by the formula derived by Fowlkes and Mallows <ref> [FM83] </ref> for comparing hierarchical clustering methods. It is general enough to be used for comparing non-hierarchical methods and has been used widely [Mil80] [FM83].
Reference: [FM89] <author> Douglas H. Fisher and Kathleen B. McKusick. </author> <title> An empirical comparison of ID3 and back propagation. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 788-793, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The connectionist researchers model self organizing properties of the brain using networks of neurons. 1.3 Previous Comparative Studies Thus given raw data, there are many choices of classification algorithms, and it is difficult for applied researchers to decide which algorithm to prefer over the others. Many studies [MSTG89] <ref> [FM89] </ref> [WK89] [WK91] [FSK + 93] compare machine learning and neural network techniques in a supervised environment. Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification.
Reference: [FSK + 93] <author> C. Feng, A. Sutherland, R. King, S. Muggleton, and R. Henry. </author> <title> Comparison of machine learning classifiers to statistics and neural networks. </title> <booktitle> In Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 41-52, </pages> <year> 1993. </year>
Reference-contexts: Many studies [MSTG89] [FM89] [WK89] [WK91] <ref> [FSK + 93] </ref> compare machine learning and neural network techniques in a supervised environment. Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification. Comparative evaluation of unsupervised classifiers has not received much attention from AI researchers.
Reference: [FV71] <author> L. Fisher and J. W. Van Ness. </author> <title> Admissible clustering procedures. </title> <journal> Biometrika, </journal> <volume> 58 </volume> <pages> 91-104, </pages> <year> 1971. </year>
Reference-contexts: Another approach adopted by some researchers is to compile a list of generally acceptable criteria (called admissibility criteria) such as correctness and then examine which of the algorithms adhere to these criteria <ref> [FV71] </ref> [DJ79].
Reference: [GC87] <author> Stephen Grossberg and Gail Carpenter. Art2: </author> <title> Self-organization of stable category recognition codes for analog pattern recognitions. </title> <journal> Applied Optics, </journal> <volume> 23 </volume> <pages> 4919-4930, </pages> <year> 1987. </year>
Reference-contexts: To avoid re-orientations right in the beginning, the network is initialized so as to be more tolerant of differences between inputs and this tolerance decreases as the nodes learn more <ref> [GC87] </ref>.
Reference: [GC91] <author> Stephen Grossberg and Gail Carpentar. </author> <title> Pattern Recognition by Self-Organizing Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachussets, </address> <year> 1991. </year>
Reference-contexts: The weights are modified only when a match is found. If none of the presently active nodes matches it a new node is created. ART1, ART2, ART3, ART Map, ART Star and Fuzzy ART are variations on the Adaptive Resonance architecture <ref> [GC91] </ref>. ART1 classifies binary input patterns in an unsupervised environment whereas ART2 works in the real pattern space. ART Map and Art Star modify the basic ART architecture for training in a supervised environment. Fuzzy Art performs a fuzzy classification. We used ART2 in our study.
Reference: [Gje90] <author> R. O. Gjerdingen. </author> <title> Categorization of musical patterns by self organizing neuron-like networks. </title> <journal> Music Perception, </journal> <volume> 7 </volume> <pages> 339-370, </pages> <year> 1990. </year>
Reference-contexts: Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88]. Applications of ART2 include discovering structure in musical patterns <ref> [Gje90] </ref>, detecting Sonar signals [Sim90] and predicting object motion [ZT95]. Here we will analyze these systems to assess the potential and limitations of each technique. Next we provide an overview of the algorithms, the test statistics used, the data-sets generated and finally the results obtained by comparing Snob and ART2.
Reference: [Gro72] <author> A. L. Gross. </author> <title> Monte Carlo study of the accuracy of a hierarchical grouping procedure. </title> <journal> Multivariate Behavioral Research, </journal> <volume> 7 </volume> <pages> 379-389, </pages> <year> 1972. </year>
Reference-contexts: The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] <ref> [Gro72] </ref> [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [Gro76] <author> Stephen Grossberg. </author> <title> Adaptive pattern classification and universal recoding I: Parallel development and coding of neural feature detectors. </title> <journal> Biological Cybernetics, </journal> <volume> 23 </volume> <pages> 121-134, </pages> <year> 1976. </year>
Reference-contexts: The question of deciding when to switch to a stable mode and when to remain in an adaptive or plastic mode is called as the plasticity stability dilemma <ref> [Gro76] </ref>. The basic competitive learning model suffers from this problem. Grossberg [Gro76] shows that a competitive learning model does not always learn a stable code in response to arbitrary input patterns and problems arise if too many input patterns are presented to the network or if input patterns form too many <p> The question of deciding when to switch to a stable mode and when to remain in an adaptive or plastic mode is called as the plasticity stability dilemma <ref> [Gro76] </ref>. The basic competitive learning model suffers from this problem. Grossberg [Gro76] shows that a competitive learning model does not always learn a stable code in response to arbitrary input patterns and problems arise if too many input patterns are presented to the network or if input patterns form too many classes [Gro76]. <p> Grossberg <ref> [Gro76] </ref> shows that a competitive learning model does not always learn a stable code in response to arbitrary input patterns and problems arise if too many input patterns are presented to the network or if input patterns form too many classes [Gro76]. This is because as new input patterns arrive, old patterns keep getting washed away, due to the plasticity or instability of competitive learning model. Adaptive Resonance Theory (ART) addresses this problem by adding feedback connections.
Reference: [HA85] <author> L. J. Hubert and P. Arabie. </author> <title> Comparing partitions. </title> <journal> Journal of Classification, </journal> <volume> 2 </volume> <pages> 193-218, </pages> <year> 1985. </year>
Reference-contexts: Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification. Comparative evaluation of unsupervised classifiers has not received much attention from AI researchers. The problem is similar to the cluster validation problem studied by social scientists <ref> [HA85] </ref> [MC85] [MS80]. According to Rand [Ran71], classifiers can be evaluated using two basic approaches: There are two general ways of comparing clustering methods; the first is to consider how easy they are to use, and the second is to evaluate how well they perform when used. <p> Then the corrected statistic S 0 can be stated as S 0 = M ax (S) E (S) where M ax and E denote the maximum and average value of the statistic, respectively. Different corrections have been proposed. Hubert and Arabie 39 <ref> [HA85] </ref> assume the maximum value of Rand to be 1 which leads to the following corrected Rand statistic, R 0 = 2 E ([a + d]= n ! 1 E ([a + d]= n ! ; substituting the values of a, and d from Equations 6.1 and 6.4 gives R 0
Reference: [Har75] <author> John Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Another drawback of existing hierarchical classifiers is their inherent feature 3 that an early split is definitive for the rest of the procedure [Pla86] <ref> [Har75] </ref>. Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 [Qui86], C 4.5 [Qui86], CART [SBFO84], the AQ-series [MS86], and Back-propagation [RMt86]. <p> Early work in the formalization of unsupervised classification techniques was done by taxonomists for naming new species of animals and plants <ref> [Har75] </ref>. Similar requirements arose for naming a variety of phenomena in both social and natural sciences. Statisticians developed tools that employ objective methodology for classification. <p> Cluster analysis is based on a mathematical formulation of a measure of similarity. Some well known cluster analysis algorithms are the leader algorithm, the k-means algorithm, ISODATA, and the quick partition algorithm [And73] <ref> [Har75] </ref>. The leader algorithm starts by choosing the first object as a seed for the first class. The rest of the objects are processed sequentially in order.
Reference: [Heb49] <author> Donald O. Hebb. </author> <title> Organization of Behavior. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: The neurons are extensively interconnected into networks called neural networks. Neural network research analyzes and studies properties of these networks. The conception of neurons as computing elements was first laid out by McCulloch and Pitts [PM43]. Donald Hebb <ref> [Heb49] </ref> subsequently proposed that neural networks can support learning by changing the strengths of their synaptic connections. Rosenblatt [Ros62] pioneered the simple two layered networks known as the perceptrons. Since then, a variety of neural network models have been proposed. Although details vary, all share the basic structure.
Reference: [Hot33] <author> H. Hotelling. </author> <title> Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology, </journal> <volume> 24 </volume> <pages> 417-441, </pages> <year> 1933. </year>
Reference-contexts: A standard statistical technique of principal component analysis <ref> [Hot33] </ref> allows an optimal linear transformation for reducing the dimensions of the data.
Reference: [Hun74] <author> E. B. Hunt. </author> <booktitle> Artificial Intelligence. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: This leads us to formulation of the Bayesian approach to classification [Seb62] <ref> [Hun74] </ref> [CKS + 88b]. An alternative criterion is that best theory is the one that provides least complex explanation of data [Sol64] [Hun74] [WG84]. Explanation includes both the description of the theory as well as the specification of data given that theory. <p> This leads us to formulation of the Bayesian approach to classification [Seb62] <ref> [Hun74] </ref> [CKS + 88b]. An alternative criterion is that best theory is the one that provides least complex explanation of data [Sol64] [Hun74] [WG84]. Explanation includes both the description of the theory as well as the specification of data given that theory. The minimum message length (MML) approach suggests using information theoretic principles to measure lengths of explanations offered by competing theories and preferring the theory that gives a shorter explanation.
Reference: [JIG86] <author> Anil Jain, A. Indrayan, and L. Goel. </author> <title> Monte Carlo comparison of six hierarchical clustering methods on random data. </title> <journal> Pattern Recognition, </journal> <volume> 19 </volume> <pages> 95-100, </pages> <year> 1986. </year>
Reference-contexts: Jain et al. <ref> [JIG86] </ref> studied the performance of classification algorithms on random data and concluded that the complete linkage method performed best. Mezzich and Solmonoff [MS80], after comparing eighteen clustering methods using multiple indices on four data sets, also concluded that complete linkage method was best.
Reference: [KF75] <author> F. K. Kuiper and L. Fisher. </author> <title> Monte Carlo comparison of six clustering procedures. </title> <journal> Biometrics, </journal> <volume> 31 </volume> <pages> 777-783, </pages> <year> 1975. </year>
Reference-contexts: n ij ! c = i n i: ! X X 2 ; (6.3) 2 2 @ i 2 + j 2 A : (6.4) 6.1.1 Rand Statistic The Rand statistic [Ran71] is an objective criterion for evaluation of classification methods used extensively in Monte Carlo studies [Mil81a] ,[MSS83] ,[MC85] <ref> [KF75] </ref>, [Moj77] and [Ede79]. Hubert and Arabie say the Rand index, "... seems to be one of the most popular alternatives for comparing classifications." Green and Rao independently suggest a statistic that reduces to the complement of Rand statistic [Roh74]. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] <ref> [KF75] </ref> [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. <p> Of the two systems, ART2 seems to perform better if the phenomena are not normally distributed. Few Monte Carlo studies of clustering algorithms focus on non-hierarchical techniques. These studies are also very limited in the variation of the test conditions. Fisher <ref> [KF75] </ref> varied the cluster size from 5 to 15 and the number of clusters from 2 to 5. Mojena [Moj77] tested for varying the number of clusters from 2 to 4. Milligan [MSS83] varied the number of clusters from 2 to 5 and dimensions from 2 to 8.
Reference: [KMS84] <author> Tuevo Kohonen, K. Makisara, and T. Sarmak. </author> <title> Phonotypic maps insightful representation of phonological features for speech recognition. </title> <booktitle> In Proceedings of Seventh International Conference on Pattern Recognition, </booktitle> <pages> pages 182-185. </pages> <address> 1984. Montreal, Canada. </address>
Reference-contexts: Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals <ref> [KMS84] </ref> [Koh88]. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals [Sim90] and predicting object motion [ZT95]. Here we will analyze these systems to assess the potential and limitations of each technique.
Reference: [Koh84] <author> Tuevo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: It was also found that adjacent parts of the body were controlled by the neighboring regions in the motoric cortex [MR91]. Kohonen proposed a biological mechanism that allows neurons to organize themselves so that adjacent neurons best respond to the similar external stimuli <ref> [Koh84] </ref>. The basic architecture of Kohonen's self organizing maps is a two dimensional grid map. The map has two layers F1 and F2, as shown in Figure 4.1, that act as the input and output layer respectively. The neurons are extensively interconnected.
Reference: [Koh88] <author> Tuevo Kohonen. </author> <title> The "neural" phonetic typewriter. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 11-22, </pages> <year> 1988. </year>
Reference-contexts: Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] <ref> [Koh88] </ref>. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals [Sim90] and predicting object motion [ZT95]. Here we will analyze these systems to assess the potential and limitations of each technique.
Reference: [Koh93] <author> Tuevo Kohonen. </author> <title> Physiological interpretation of the self organis-ing map algorithm. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 895-905, </pages> <year> 1993. </year> <month> 100 </month>
Reference-contexts: The investigators found that neural regions controlling motion of the various parts of body were arranged side by side in the cortex <ref> [Koh93] </ref>. It was also found that adjacent parts of the body were controlled by the neighboring regions in the motoric cortex [MR91]. Kohonen proposed a biological mechanism that allows neurons to organize themselves so that adjacent neurons best respond to the similar external stimuli [Koh84].
Reference: [Lip87] <author> Richard Lippmann. </author> <title> An introduction to computing with neural networks. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 29(4) </volume> <pages> 4-20, </pages> <year> 1987. </year>
Reference-contexts: Considering one class to be an ideal classification we get Table 7.41. 92 Chapter 8 Conclusions Various theoretical studies compare ART2 with cluster analysis algorithms such as the k-means and the leader algorithm <ref> [Lip87] </ref> [Bur91]. Lippmann [Lip87] states that ART "...implements a clustering algorithm that is very similar to the simple sequential leader algorithm." Burke [Bur91], on the other hand, concludes that the k-means algorithm is "more fundamentally analogous" to ART2. Our results support both these theoretical claims. <p> Considering one class to be an ideal classification we get Table 7.41. 92 Chapter 8 Conclusions Various theoretical studies compare ART2 with cluster analysis algorithms such as the k-means and the leader algorithm <ref> [Lip87] </ref> [Bur91]. Lippmann [Lip87] states that ART "...implements a clustering algorithm that is very similar to the simple sequential leader algorithm." Burke [Bur91], on the other hand, concludes that the k-means algorithm is "more fundamentally analogous" to ART2. Our results support both these theoretical claims.
Reference: [MC85] <author> Glenn W. Milligan and C. Cooper. </author> <title> An examination of the procedures for determining the number of clusters in data. </title> <journal> Psychome-trika, </journal> <volume> 50 </volume> <pages> 159-179, </pages> <year> 1985. </year>
Reference-contexts: Comparative evaluation of unsupervised classifiers has not received much attention from AI researchers. The problem is similar to the cluster validation problem studied by social scientists [HA85] <ref> [MC85] </ref> [MS80]. According to Rand [Ran71], classifiers can be evaluated using two basic approaches: There are two general ways of comparing clustering methods; the first is to consider how easy they are to use, and the second is to evaluate how well they perform when used. <p> Sneath [SS73] specifies two types of criteria for cluster validation: external and internal. An external criterion uses the information obtained from outside the classification process to evaluate the resulting classification, whereas an internal measure uses information obtained strictly from within the classification process <ref> [MC85] </ref>. The computation of external criteria involves classifying a labeled data set and then measuring the extent of retrieval of the "original" classification. <p> , then the Fowlkes and Mallows statistic measures the geometric mean of the two proportions i.e., F = P C P C 0 v u (a + b)(a + c) a (a + b)(a + c) 6.1.4 Jaccard Statistic The Jaccard statistic, used to measure cluster recovery in [Mil81a] and <ref> [MC85] </ref>, ignores d (i.e., the number of pairs of objects classified in different classes by both the classifications) both in the numerator and denominator [And73]. <p> This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], <ref> [MC85] </ref>. The algorithm generates externally isolated and internally cohesive classes. The user specifies some number k of distinctly non-overlapping classes to be generated. Each class c can have m c objects each embedded in d c dimensional space. Different types of noise can be added to the well separated classes.
Reference: [Mic83] <author> Ryszard S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference-contexts: This lets us formulate the classification problem as a problem of inductive inference. 1.2 AI Techniques In AI unsupervised classification is studied by both connectionist and symbolic camps. The symbolic research is carried out by researchers in the traditional machine learning community, under the name of conceptual clustering <ref> [Mic83] </ref>, as well as by those employing methodologies with a statistical flavor. The basic idea behind conceptual clustering is that instead of considering just the similarity between objects, the method uses the conceptual cohesiveness among the objects as a criterion for classification. <p> Conceptual cohesiveness depends not only on those objects and the surrounding objects but also on the information that describes the objects together. Conceptual clustering 5 techniques thus perform a context based clustering and arrange objects in a hierarchical fashion <ref> [Mic83] </ref>. According to the statistical approach properties of objects are distributed according to some probability distribution. This approach suggests estimating the parameters of these distributions based on some assumptions.
Reference: [Mil80] <author> Glenn W. Milligan. </author> <title> An examination of effect of six types of error perturbation algorithms on fifteen clustering algorithms. </title> <journal> Psychometrika, </journal> <volume> 45 </volume> <pages> 325-342, </pages> <year> 1980. </year>
Reference-contexts: In order to measure how well a classifier performs one must define a measure of wellness and optimality of the classification which is difficult to formulate as there is no consensus on the definition of a class. Milligan <ref> [Mil80] </ref> attributes the lack of systematic research tools for comparative study of unsupervised classification methods to the difficulty of defining a class: "...few systematic studies have been executed on a large scale basis using a Monte 6 Carlo approach. <p> Various indices have been proposed to measure the recovery of the classification including the Rand index [Ran71], the corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index <ref> [Mil80] </ref>. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. Milligan et al. [Mil80] [Mil81a] [MSS83] compared different clustering algorithms by testing them on artificially generated data sets. [Mil80] examined the recovery behavior of eleven hierarchical <p> corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index <ref> [Mil80] </ref>. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. Milligan et al. [Mil80] [Mil81a] [MSS83] compared different clustering algorithms by testing them on artificially generated data sets. [Mil80] examined the recovery behavior of eleven hierarchical and four nonhierarchical clustering algorithms by studying the effects of varying number, size, and dimensionality of clusters on the performance of various clustering algorithms. <p> Gamma index [Bak74], and the kappa index <ref> [Mil80] </ref>. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. Milligan et al. [Mil80] [Mil81a] [MSS83] compared different clustering algorithms by testing them on artificially generated data sets. [Mil80] examined the recovery behavior of eleven hierarchical and four nonhierarchical clustering algorithms by studying the effects of varying number, size, and dimensionality of clusters on the performance of various clustering algorithms. <p> It is general enough to be used for comparing non-hierarchical methods and has been used widely <ref> [Mil80] </ref> [FM83]. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] <ref> [Mil80] </ref> [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. <p> This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] <ref> [Mil80] </ref> [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. The user specifies some number k of distinctly non-overlapping classes to be generated. Each class c can have m c objects each embedded in d c dimensional space.
Reference: [Mil81a] <author> Glenn W. Milligan. </author> <title> A Monte Carlo comparison of 30 internal criterion measures for cluster analysis. </title> <journal> Psychometrika, </journal> <volume> 46 </volume> <pages> 187-195, </pages> <year> 1981. </year>
Reference-contexts: In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. Milligan et al. [Mil80] <ref> [Mil81a] </ref> [MSS83] compared different clustering algorithms by testing them on artificially generated data sets. [Mil80] examined the recovery behavior of eleven hierarchical and four nonhierarchical clustering algorithms by studying the effects of varying number, size, and dimensionality of clusters on the performance of various clustering algorithms. <p> 2 i j n ij ! c = i n i: ! X X 2 ; (6.3) 2 2 @ i 2 + j 2 A : (6.4) 6.1.1 Rand Statistic The Rand statistic [Ran71] is an objective criterion for evaluation of classification methods used extensively in Monte Carlo studies <ref> [Mil81a] </ref> ,[MSS83] ,[MC85] [KF75], [Moj77] and [Ede79]. Hubert and Arabie say the Rand index, "... seems to be one of the most popular alternatives for comparing classifications." Green and Rao independently suggest a statistic that reduces to the complement of Rand statistic [Roh74]. <p> a a+b , then the Fowlkes and Mallows statistic measures the geometric mean of the two proportions i.e., F = P C P C 0 v u (a + b)(a + c) a (a + b)(a + c) 6.1.4 Jaccard Statistic The Jaccard statistic, used to measure cluster recovery in <ref> [Mil81a] </ref> and [MC85], ignores d (i.e., the number of pairs of objects classified in different classes by both the classifications) both in the numerator and denominator [And73]. <p> This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], <ref> [Mil81a] </ref>, [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. The user specifies some number k of distinctly non-overlapping classes to be generated. Each class c can have m c objects each embedded in d c dimensional space.
Reference: [Mil81b] <author> Glenn W. Milligan. </author> <title> A review of Monte Carlo tests of cluster analysis. </title> <journal> Multivariate Behvioral Research, </journal> <volume> 16 </volume> <pages> 379-407, </pages> <year> 1981. </year>
Reference-contexts: Unsupervised classification is a basic problem and its solution is of interest not only to the cognitive scientists but also to applied researchers. This explains the large number of papers published on the subject in a variety of areas. There are literally thousands of classification algorithms published every year <ref> [Mil81b] </ref> and yet developments are expected to accelerate as advances in information acquisition technologies enable scientists to collect large amounts of raw data to be analyzed. Continuing research is needed to empirically compare the classification algorithms.
Reference: [Moj77] <author> R. Mojena. </author> <title> Hierarchical grouping methods and stopping rules: an evaluation. </title> <journal> The Computer Journal, </journal> <volume> 20 </volume> <pages> 359-363, </pages> <year> 1977. </year>
Reference-contexts: ij ! c = i n i: ! X X 2 ; (6.3) 2 2 @ i 2 + j 2 A : (6.4) 6.1.1 Rand Statistic The Rand statistic [Ran71] is an objective criterion for evaluation of classification methods used extensively in Monte Carlo studies [Mil81a] ,[MSS83] ,[MC85] [KF75], <ref> [Moj77] </ref> and [Ede79]. Hubert and Arabie say the Rand index, "... seems to be one of the most popular alternatives for comparing classifications." Green and Rao independently suggest a statistic that reduces to the complement of Rand statistic [Roh74]. <p> Few Monte Carlo studies of clustering algorithms focus on non-hierarchical techniques. These studies are also very limited in the variation of the test conditions. Fisher [KF75] varied the cluster size from 5 to 15 and the number of clusters from 2 to 5. Mojena <ref> [Moj77] </ref> tested for varying the number of clusters from 2 to 4. Milligan [MSS83] varied the number of clusters from 2 to 5 and dimensions from 2 to 8. Our experience shows that trends observed in a small test interval are not always generalizable outside the interval.
Reference: [MP88] <author> M. L. Minsky and S. A. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachus-sets, </address> <year> 1988. </year>
Reference-contexts: Minsky and Pappert later showed that the simple two layered feed-forward models were unable to learn non-linearly separable functions like the Xor function <ref> [MP88] </ref>. However, by adding feedback connections and/or 22 adding hidden layers between the input and output layers, neural networks can learn to approximate arbitrary functions [BJ90].
Reference: [MR91] <author> B. Muller and J. Reinhardt. </author> <title> Neural Networks. </title> <publisher> Springer Verlag, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The Competitive Learning Model In experiments with animals, it was recognized that the visual cortex contains specific neurons that recognize particular patterns. These neurons are activated only when certain patterns are present in the input <ref> [MR91] </ref>. Other experiments also support the idea of localization of the areas of the brain for certain tasks. The localization principle seems to apply to a wide variety of cognitive tasks such as vision, speech and motor-control. <p> It has been 23 postulated that since it is unlikely that assignment of neurons to specific stimuli is determined genetically, biological mechanisms establish this relationship during the growth of the brain <ref> [MR91] </ref>. The process of competitive learning explains the localization of the brain functions [Von88]. The basic idea behind competitive or winner take all learning is similar to the natural selection principle proposed by Charles Darwin to account for human and animal evolution. <p> The investigators found that neural regions controlling motion of the various parts of body were arranged side by side in the cortex [Koh93]. It was also found that adjacent parts of the body were controlled by the neighboring regions in the motoric cortex <ref> [MR91] </ref>. Kohonen proposed a biological mechanism that allows neurons to organize themselves so that adjacent neurons best respond to the similar external stimuli [Koh84]. The basic architecture of Kohonen's self organizing maps is a two dimensional grid map. <p> It is usually modeled by the Gaussian function, g (p p 0 ) = e jjp p 0 jj The parameters *(t) and are the learning parameters with typical values between 3 and 4. Finite and time decaying values of these parameters lead to stable learning in the network <ref> [MR91] </ref>. 4.1 Classification Characteristics The approach does not require the number of neurons in the output layer to be initialized to the number of classes in the pattern space. Rather it can be initialized to the maximum number of classes possible, that is, the number of input patterns n.
Reference: [MS80] <author> J. E. Mezzich and H. Solomonoff. </author> <title> Taxonomy and Behavioral Sciences. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1980. </year>
Reference-contexts: Comparative evaluation of unsupervised classifiers has not received much attention from AI researchers. The problem is similar to the cluster validation problem studied by social scientists [HA85] [MC85] <ref> [MS80] </ref>. According to Rand [Ran71], classifiers can be evaluated using two basic approaches: There are two general ways of comparing clustering methods; the first is to consider how easy they are to use, and the second is to evaluate how well they perform when used. <p> Jain et al. [JIG86] studied the performance of classification algorithms on random data and concluded that the complete linkage method performed best. Mezzich and Solmonoff <ref> [MS80] </ref>, after comparing eighteen clustering methods using multiple indices on four data sets, also concluded that complete linkage method was best. Bayne et al. [BBBK80] compared thirteen methods in a Monte Carlo study and concluded that the k-means algorithm performed best.
Reference: [MS86] <author> Ryszard S. Michalski and Robert E. </author> <title> Step. Conceptual clustering of structured objects: A goal oriented approach. </title> <journal> Artificial Intelligence, </journal> <volume> 29 </volume> <pages> 43-69, </pages> <year> 1986. </year> <month> 101 </month>
Reference-contexts: Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 [Qui86], C 4.5 [Qui86], CART [SBFO84], the AQ-series <ref> [MS86] </ref>, and Back-propagation [RMt86]. The supervised classifiers are useful in applications where categorized training data is available from previous trials. But in exploratory analysis this information may not be available.
Reference: [MSS83] <author> Glenn W. Milligan, Lisa M. Sokol, and S. C. Soon. </author> <title> The effect of cluster size, dimensionality, and the number of clusters on recovery of true cluster structure. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(1) </volume> <pages> 40-47, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. Milligan et al. [Mil80] [Mil81a] <ref> [MSS83] </ref> compared different clustering algorithms by testing them on artificially generated data sets. [Mil80] examined the recovery behavior of eleven hierarchical and four nonhierarchical clustering algorithms by studying the effects of varying number, size, and dimensionality of clusters on the performance of various clustering algorithms. <p> Mezzich and Solmonoff [MS80], after comparing eighteen clustering methods using multiple indices on four data sets, also concluded that complete linkage method was best. Bayne et al. [BBBK80] compared thirteen methods in a Monte Carlo study and concluded that the k-means algorithm performed best. Milligan et al. <ref> [MSS83] </ref> compared the single 8 link, complete link, group average and Ward's technique and concluded that Ward's method performed best. By providing information about their behavior on different kinds of data, these studies have increased our understanding of clustering algorithms. Most of these studies focus on the cluster analysis techniques. <p> Milligan et al. <ref> [MSS83] </ref> conclude that "the Jaccard index exhibited a wider range of variance than the Fowlkes and Mallows measure, the Jaccard index may be the best choice from this set." The set included all four measures we are using, however, they use a different correction of the Rand index. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] <ref> [MSS83] </ref>. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. The user specifies some number k of distinctly non-overlapping classes to be generated. <p> This approach has been used in various comparisons of clustering algorithms [Ran71] [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] <ref> [MSS83] </ref>. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes. The user specifies some number k of distinctly non-overlapping classes to be generated. Each class c can have m c objects each embedded in d c dimensional space. <p> These studies are also very limited in the variation of the test conditions. Fisher [KF75] varied the cluster size from 5 to 15 and the number of clusters from 2 to 5. Mojena [Moj77] tested for varying the number of clusters from 2 to 4. Milligan <ref> [MSS83] </ref> varied the number of clusters from 2 to 5 and dimensions from 2 to 8. Our experience shows that trends observed in a small test interval are not always generalizable outside the interval. In some cases, the general trend was opposite to that observed in a small interval.
Reference: [MSTG89] <author> Raymond Mooney, Jude Shavlik, Geoffrey Towell, and Allen Gove. </author> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Joint Conference On Artificial Intelligence, </booktitle> <pages> pages 775-780, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The connectionist researchers model self organizing properties of the brain using networks of neurons. 1.3 Previous Comparative Studies Thus given raw data, there are many choices of classification algorithms, and it is difficult for applied researchers to decide which algorithm to prefer over the others. Many studies <ref> [MSTG89] </ref> [FM89] [WK89] [WK91] [FSK + 93] compare machine learning and neural network techniques in a supervised environment. Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification.
Reference: [Mur94] <author> Patrick M Murphy. </author> <title> UCI Repository of machine learning databses [Machine readable data repository]. </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: In an earlier study [UN94] we studied the classification recovery characteristics of Autoclass and ART2 on real life data sets obtained from the UCI Machine Learning repository <ref> [Mur94] </ref>. The problem with this approach, however, is that the validity 7 of the "original" classification, with which we can compare the classification resulting from an algorithm, is open to question.
Reference: [PK94a] <author> I. Pilowsky and M. Katsikitis. </author> <title> The classification of facial emotions: a computer based taxonomic approach. </title> <journal> Journal of Affective Disorders, </journal> <volume> 30 </volume> <pages> 61-71, </pages> <year> 1994. </year>
Reference-contexts: For performing experiments we chose Snob and ART2 as representative systems. These systems have been widely used. Applications of Snob include classification of depression [BPL69], classification of spectral data [DPC93], classification of protein spectra [DZC94], classification of facial emotions <ref> [PK94a] </ref>, and classification of illness behavior in pain [PK94b]. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88].
Reference: [PK94b] <author> I. Pilowsky and M. Katsikitis. </author> <title> A classification of illness behaviour in pain clinic patients. </title> <journal> Pain, </journal> <volume> 57 </volume> <pages> 91-94, </pages> <year> 1994. </year>
Reference-contexts: For performing experiments we chose Snob and ART2 as representative systems. These systems have been widely used. Applications of Snob include classification of depression [BPL69], classification of spectral data [DPC93], classification of protein spectra [DZC94], classification of facial emotions [PK94a], and classification of illness behavior in pain <ref> [PK94b] </ref>. Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88]. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals [Sim90] and predicting object motion [ZT95].
Reference: [Pla86] <author> Frank Plastira. </author> <title> Two hierarchies associated with each clustering scheme. </title> <journal> Pattern Recognition, </journal> <volume> 2 </volume> <pages> 193-196, </pages> <year> 1986. </year>
Reference-contexts: Another drawback of existing hierarchical classifiers is their inherent feature 3 that an early split is definitive for the rest of the procedure <ref> [Pla86] </ref> [Har75]. Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 [Qui86], C 4.5 [Qui86], CART [SBFO84], the AQ-series [MS86], and Back-propagation [RMt86].
Reference: [PM43] <author> W. Pitts and Warren S. McCulloch. </author> <title> A logical calculus of ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 9 </volume> <pages> 127-147, </pages> <year> 1943. </year>
Reference-contexts: Figure 3.3 shows model of a typical biological neuron shown in Figure 3.2. The neurons are extensively interconnected into networks called neural networks. Neural network research analyzes and studies properties of these networks. The conception of neurons as computing elements was first laid out by McCulloch and Pitts <ref> [PM43] </ref>. Donald Hebb [Heb49] subsequently proposed that neural networks can support learning by changing the strengths of their synaptic connections. Rosenblatt [Ros62] pioneered the simple two layered networks known as the perceptrons. Since then, a variety of neural network models have been proposed.
Reference: [Pre94] <author> Lutz Prechelt. </author> <title> A study of experimental evaluations of neural network learning algorithms: Current research practice. </title> <type> Technical Report 19/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Karlsruhe, Germany, </institution> <month> August 24 </month> <year> 1994. </year>
Reference-contexts: The ad hoc work that has been done has come from the developers of various classification systems, each in turn claiming their system to be superior to other systems without any comparative evaluation [CKS + 88b] [Wal90]. Lutz Prechelt in a recent study <ref> [Pre94] </ref> examined 113 journal articles about neural net learning algorithms. Only 6% show results for more than one problem using real-world data. One third present no quantitative comparison with any previous algorithm, and one third use no realistic problem at all [Pre94]. 1.4 Objective The objective of the present work is <p> Lutz Prechelt in a recent study <ref> [Pre94] </ref> examined 113 journal articles about neural net learning algorithms. Only 6% show results for more than one problem using real-world data. One third present no quantitative comparison with any previous algorithm, and one third use no realistic problem at all [Pre94]. 1.4 Objective The objective of the present work is to perform an impartial and systematic comparison of various non-hierarchical unsupervised classifiers by measuring the extent of recovery of the true classification using standard statistics.
Reference: [Qui86] <author> Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 <ref> [Qui86] </ref>, C 4.5 [Qui86], CART [SBFO84], the AQ-series [MS86], and Back-propagation [RMt86]. The supervised classifiers are useful in applications where categorized training data is available from previous trials. But in exploratory analysis this information may not be available. <p> Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 <ref> [Qui86] </ref>, C 4.5 [Qui86], CART [SBFO84], the AQ-series [MS86], and Back-propagation [RMt86]. The supervised classifiers are useful in applications where categorized training data is available from previous trials. But in exploratory analysis this information may not be available.
Reference: [Ran71] <author> W. M. Rand. </author> <title> Objective criterion for evaluation of clustering methods. </title> <journal> Journal of American Statistical Association, </journal> <volume> 66 </volume> <pages> 846-851, </pages> <year> 1971. </year>
Reference-contexts: Comparative evaluation of unsupervised classifiers has not received much attention from AI researchers. The problem is similar to the cluster validation problem studied by social scientists [HA85] [MC85] [MS80]. According to Rand <ref> [Ran71] </ref>, classifiers can be evaluated using two basic approaches: There are two general ways of comparing clustering methods; the first is to consider how easy they are to use, and the second is to evaluate how well they perform when used. <p> In spirit of <ref> [Ran71] </ref> our approach is that, "every definition of cluster is natural for some situation and therefore that problem can be considered without this aspect." We will not discuss the problem of defining a class; the interested reader is referred to [And73] and [DJ88]. <p> The computation of external criteria involves classifying a labeled data set and then measuring the extent of retrieval of the "original" classification. Various indices have been proposed to measure the recovery of the classification including the Rand index <ref> [Ran71] </ref>, the corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. <p> been proposed to measure the recovery of the classification including the Rand index <ref> [Ran71] </ref>, the corrected Rand index [DJ88], the Jaccard index [And73], the Fowlkes and Mallows index [FM83], the Gamma index [Bak74], and the kappa index [Mil80]. In one of the first Monte Carlo studies of clustering algorithms, Rand [Ran71] compared the AA and the T/N algorithms; two hierarchical clustering techniques. <p> 6.2 we can define a; b; c and d as, X X 2 ; (6.1) X 2 i j n ij ! c = i n i: ! X X 2 ; (6.3) 2 2 @ i 2 + j 2 A : (6.4) 6.1.1 Rand Statistic The Rand statistic <ref> [Ran71] </ref> is an objective criterion for evaluation of classification methods used extensively in Monte Carlo studies [Mil81a] ,[MSS83] ,[MC85] [KF75], [Moj77] and [Ede79]. <p> It is defined as R = [a + d]=[a + b + c + d] R = [a + d]= n ! It can be shown that the mean value of the Rand measure, E (R), approaches 1:00 as the number of classes increases without limit <ref> [Ran71] </ref> [FM83]. Some suggest that the practical range of values is narrow and the limit is approached quickly [DJ88]. <p> The Jaccard statistic is given by, J = a=[a + b + c]: 6.2 Data Sets The data sets were generated using multivariate normal distributions. This approach has been used in various comparisons of clustering algorithms <ref> [Ran71] </ref> [Gro72] [Eve74] [KF75] [Bla76] [Ede79] [Mil80] [BBBK80] [BAM82] [MSS83]. We used a modified form of the data generation algorithm that Milligan et al. used to examine the properties of cluster analysis algorithms [Mil80], [Mil81a], [MSS83], [MC85]. The algorithm generates externally isolated and internally cohesive classes.
Reference: [RMt86] <author> D. Rumelhart, J. Mclelland, </author> <title> and the PDP group. Parallel and Distributed Processing, Vol I and Vol II. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachussetes, </address> <year> 1986. </year>
Reference-contexts: Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 [Qui86], C 4.5 [Qui86], CART [SBFO84], the AQ-series [MS86], and Back-propagation <ref> [RMt86] </ref>. The supervised classifiers are useful in applications where categorized training data is available from previous trials. But in exploratory analysis this information may not be available. In these problems it is simply not possible to perform trials to get this information because of practical or ethical reasons. <p> Thus the presence of a class in the input patterns corresponds to an active node in the map and the weight vector of a node specifies a typical vector in that class. 28 29 adapted from <ref> [RMt86] </ref>. 30 Chapter 5 Adaptive Resonance Theory One property of intelligent self-organizing agents is their ability to learn new information without necessarily forgetting previous experiences. At the same time behavioral evidence suggests that we do not remember every detail of an observed object or event.
Reference: [Roh74] <author> F. James Rohlf. </author> <title> Methods of comparing classifications. </title> <booktitle> In Annual Review of Ecology and Systematics, </booktitle> <volume> volume 5, </volume> <pages> pages 101-113. </pages> <address> Palo Alto, CA, </address> <year> 1974. </year>
Reference-contexts: Hubert and Arabie say the Rand index, "... seems to be one of the most popular alternatives for comparing classifications." Green and Rao independently suggest a statistic that reduces to the complement of Rand statistic <ref> [Roh74] </ref>. We can define a pair &lt; X i ; X j &gt; of patterns as correctly coplaced if X i and X j are classified into the same class if and only if they belong to the same class. The Rand statistic measures the proportion of correctly coplaced pairs.
Reference: [Ros62] <author> Frank Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> Washington, </address> <year> 1962. </year>
Reference-contexts: Neural network research analyzes and studies properties of these networks. The conception of neurons as computing elements was first laid out by McCulloch and Pitts [PM43]. Donald Hebb [Heb49] subsequently proposed that neural networks can support learning by changing the strengths of their synaptic connections. Rosenblatt <ref> [Ros62] </ref> pioneered the simple two layered networks known as the perceptrons. Since then, a variety of neural network models have been proposed. Although details vary, all share the basic structure. <p> vector then the simple perceptron algorithm can be described as follows, If X 2 c 2 and W X &gt; 0 increase the weight W by X Rosenblatt proved that a perceptron can learn a weight vector W that correctly classifies linearly separable patterns in a finite number of steps <ref> [Ros62] </ref>. Minsky and Pappert later showed that the simple two layered feed-forward models were unable to learn non-linearly separable functions like the Xor function [MP88]. However, by adding feedback connections and/or 22 adding hidden layers between the input and output layers, neural networks can learn to approximate arbitrary functions [BJ90].
Reference: [SBFO84] <author> C. J. Stone, L. Breiman, J. H. Friedman, and R. A. Olshen. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Non-hierarchical classifiers, on the other hand, partition data into some k classes. Supervised classification is relatively well understood. Some of the well known supervised classifiers are ID3 [Qui86], C 4.5 [Qui86], CART <ref> [SBFO84] </ref>, the AQ-series [MS86], and Back-propagation [RMt86]. The supervised classifiers are useful in applications where categorized training data is available from previous trials. But in exploratory analysis this information may not be available.
Reference: [Seb62] <author> G. S. Sebestyen. </author> <title> Decision Making Processes in Pattern Recognition. </title> <publisher> MacMillan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: This leads us to formulation of the Bayesian approach to classification <ref> [Seb62] </ref> [Hun74] [CKS + 88b]. An alternative criterion is that best theory is the one that provides least complex explanation of data [Sol64] [Hun74] [WG84]. Explanation includes both the description of the theory as well as the specification of data given that theory.
Reference: [Sha48] <author> C. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 24 </volume> <pages> 379-423 and 623-656, </pages> <year> 1948. </year>
Reference-contexts: Shannon's information theory states that the probability p of an event can be encoded (for instance, using Huffman encoding) by a message log (p) bits long <ref> [Sha48] </ref>. Therefore, total message length will be the sum of log P (H) and log P (DjH).
Reference: [Sim90] <author> P. K. Simpson. </author> <title> Neural networks for Sonar signal processing. </title> <editor> In C. T. Harston, R. M. Pap, and A. J. Maren, editors, </editor> <booktitle> Handbook of Neural Computing Applications, </booktitle> <pages> pages 319-335. </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88]. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals <ref> [Sim90] </ref> and predicting object motion [ZT95]. Here we will analyze these systems to assess the potential and limitations of each technique. Next we provide an overview of the algorithms, the test statistics used, the data-sets generated and finally the results obtained by comparing Snob and ART2.
Reference: [Sol64] <author> R Solomonoff. </author> <title> Formal theories of inductive inference I and II. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22 and 224-254, </pages> <year> 1964. </year>
Reference-contexts: This leads us to formulation of the Bayesian approach to classification [Seb62] [Hun74] [CKS + 88b]. An alternative criterion is that best theory is the one that provides least complex explanation of data <ref> [Sol64] </ref> [Hun74] [WG84]. Explanation includes both the description of the theory as well as the specification of data given that theory. The minimum message length (MML) approach suggests using information theoretic principles to measure lengths of explanations offered by competing theories and preferring the theory that gives a shorter explanation.
Reference: [SS73] <author> P. H. Sneath and P. R. Sokal. </author> <title> Numerical Taxonomy. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, </address> <year> 1973. </year>
Reference-contexts: This is the idea behind the Monte Carlo approach; Dubes et al. state: Monte Carlo analysis is a method for estimating parameters and probabilities by computer sampling when the quantities are diffi cult or impossible to calculate directly. Sneath <ref> [SS73] </ref> specifies two types of criteria for cluster validation: external and internal. An external criterion uses the information obtained from outside the classification process to evaluate the resulting classification, whereas an internal measure uses information obtained strictly from within the classification process [MC85]. <p> is active, otherwise g (y j ) = 0. 36 Chapter 6 Experiments The experimental setup involves generating the data sets using the data generation algorithm and then measuring the class recovery using test statistics. 6.1 Test Statistics The test statistics are basically of two kinds, namely, internal and external <ref> [SS73] </ref>. Internal statistics use only the internal knowledge available and measure the degree to which a classification obtained from a classifier is justified in light of the input patterns whereas external statistics measure the extent of recovery of the true classification by using external knowledge about the true classification.
Reference: [UN94] <author> M. Afzal Upal and Eric Neufeld. </author> <title> Comparison of Bayesian and neural net unsupervised classification techniques. </title> <booktitle> In Proceedings of Sixth Annual Symposium on Computational Science, University of Saskatchewan, </booktitle> <pages> pages 152-163. </pages> <year> 1994. </year> <month> 103 </month>
Reference-contexts: A reasonable approach then is to evaluate the classifications obtained from the classifiers to establish their characteristics as exhibited on the classification of some data sets. In an earlier study <ref> [UN94] </ref> we studied the classification recovery characteristics of Autoclass and ART2 on real life data sets obtained from the UCI Machine Learning repository [Mur94].
Reference: [Von88] <author> C. Von der Malsburg. </author> <title> Self organization of orientation sensitive cells in the striate cortex. </title> <journal> Kybernetik, </journal> <volume> 14 </volume> <pages> 85-100, </pages> <year> 1988. </year>
Reference-contexts: It has been 23 postulated that since it is unlikely that assignment of neurons to specific stimuli is determined genetically, biological mechanisms establish this relationship during the growth of the brain [MR91]. The process of competitive learning explains the localization of the brain functions <ref> [Von88] </ref>. The basic idea behind competitive or winner take all learning is similar to the natural selection principle proposed by Charles Darwin to account for human and animal evolution.
Reference: [Wal90] <author> C. S. Wallace. </author> <title> Classification by minimum-message-length inference. </title> <editor> In S. G. Akl, editor, </editor> <booktitle> Proceedings of ICCI-90: Advances in Computing and Information, </booktitle> <pages> pages 72-81. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The ad hoc work that has been done has come from the developers of various classification systems, each in turn claiming their system to be superior to other systems without any comparative evaluation [CKS + 88b] <ref> [Wal90] </ref>. Lutz Prechelt in a recent study [Pre94] examined 113 journal articles about neural net learning algorithms. Only 6% show results for more than one problem using real-world data.
Reference: [WG84] <author> C. S. Wallace and M. P. Goorgeff. </author> <title> A general selection criterion for inductive inference. </title> <editor> In T. O'Shea, editor, </editor> <booktitle> Proceedings of ECAI-84: Advances in Artificial Intelligence, </booktitle> <pages> pages 473-482. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: This leads us to formulation of the Bayesian approach to classification [Seb62] [Hun74] [CKS + 88b]. An alternative criterion is that best theory is the one that provides least complex explanation of data [Sol64] [Hun74] <ref> [WG84] </ref>. Explanation includes both the description of the theory as well as the specification of data given that theory. The minimum message length (MML) approach suggests using information theoretic principles to measure lengths of explanations offered by competing theories and preferring the theory that gives a shorter explanation.
Reference: [WK89] <author> Sholom M. Weiss and Ioannis Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Proceedings of Eleventh International Conference On Artificial Intelligence, </booktitle> <pages> pages 781-787, </pages> <address> San Mateo, California, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The connectionist researchers model self organizing properties of the brain using networks of neurons. 1.3 Previous Comparative Studies Thus given raw data, there are many choices of classification algorithms, and it is difficult for applied researchers to decide which algorithm to prefer over the others. Many studies [MSTG89] [FM89] <ref> [WK89] </ref> [WK91] [FSK + 93] compare machine learning and neural network techniques in a supervised environment. Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification.
Reference: [WK91] <author> Sholom M. Weiss and C. A. </author> <title> Kulikowski. Computer Systems that Learn: Classification and Prediction Methods From Statistics, Neural Networks, Machine Learning and Expert Systems. </title> <publisher> Mor-gan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: Many studies [MSTG89] [FM89] [WK89] <ref> [WK91] </ref> [FSK + 93] compare machine learning and neural network techniques in a supervised environment. Comparing unsupervised classifiers, however, is not as straightforward as comparing supervised classifiers because of the absence of class labels in the experimental classification.
Reference: [ZT95] <author> Q. Zhu and A. Y. Tawfik. </author> <title> Quantitative motion prediction using a combined ART2 and madaline neural network. </title> <journal> Neural Processing Letters, </journal> <volume> 2(1) </volume> <pages> 19-21, </pages> <month> January </month> <year> 1995. </year> <month> 104 </month>
Reference-contexts: Autoclass is credited 9 with discovering novel classification of protein structures and stellar spectra [CSH91]. Kohonen's self-organizing maps are used to recognize speech signals [KMS84] [Koh88]. Applications of ART2 include discovering structure in musical patterns [Gje90], detecting Sonar signals [Sim90] and predicting object motion <ref> [ZT95] </ref>. Here we will analyze these systems to assess the potential and limitations of each technique. Next we provide an overview of the algorithms, the test statistics used, the data-sets generated and finally the results obtained by comparing Snob and ART2.
References-found: 77

