URL: http://www.cl.cam.ac.uk:80/ftp/papers/reports/TR426-mh10006-phd.ps.gz
Refering-URL: http://www.cl.cam.ac.uk:80/ftp/papers/reports/
Root-URL: 
Title: Action Selection methods using Reinforcement Learning  
Author: Mark Humphrys 
Address: Hall  
Affiliation: Trinity  
Abstract: A dissertation submitted for the degree of Doctor of Philosophy in the University of Cambridge Technical Report June 1997 
Abstract-found: 1
Intro-found: 1
Reference: [Aylett, 1995] <author> Aylett, </author> <title> Ruth (1995), Multi-Agent Planning: Modelling Execution Agents, </title> <booktitle> Papers of the 14th Workshop of the UK Planning and Scheduling Special Interest Group. </booktitle>
Reference-contexts: If we multiply the Q-values by a constant, the W-value would remain unchanged. We have no easy mechanism for making agents stronger or weaker. The argument against scaling was presented in x8.1. 15.4.6 The BSA Architecture The Behavioral Synthesis Architecture <ref> [Aylett, 1995] </ref> is another example of hand-designed utility functions for behaviors.
Reference: [Baum, 1996] <author> Baum, Eric B. </author> <year> (1996), </year> <title> Toward a Model of Mind as a Laissez-Faire Economy of Idiots, </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle>
Reference-contexts: Note that the summation method of Maximize Collective Happiness would have chosen action (1) in this case. 134 15.5.3 The Economy of Mind Baum <ref> [Baum, 1996] </ref> introduces an Economy of Mind model ("A Model of Mind as a Laissez-Faire Economy of Idiots"), where new agents can be created dynamically, and agents that do not thrive die off. An "agent" here though is simply a rule (a condition-action pair).
Reference: [Blumberg, 1994] <editor> Blumberg, Bruce (1994), Action-Selection in Hamsterdam: </editor> <title> Lessons from Ethology, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: In interleaving different behaviors, various authors have argued for time-based switching (e.g. see [Ring, 1992]). Blumberg <ref> [Blumberg, 1994] </ref> argues the need for a model of fatigue, where a switch of activity becomes more likely the longer an activity goes on. He points out that animals sometimes appear to engage in a form of time-sharing. This is the same philosophy as Lorenz's `Psycho-Hydraulic' model in ethology. <p> It's only information. 15.1.2 Low-priority activities A classic ethology problem is: if priorities are assigned to entire activities, how does a low-priority activity ever manage to interrupt a higher-priority one? For example, the conflict between high-priority feeding and low-priority body maintenance discussed by <ref> [Blumberg, 1994] </ref>. Here is how W-learning would solve it. The Food-seeking agent A f suggests actions with weight W f (x). The Cleaning agent A c suggests actions with weight W c (x).
Reference: [Brooks, 1986] <author> Brooks, Rodney A. </author> <year> (1986), </year> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE Journal of Robotics and Automation 2 </journal> <pages> 14-23. </pages>
Reference-contexts: Like Watkins, I will propose methods deriving from this motivation rather than trying to copy anything seen in nature. The starting point for this exploration of decentralised minds is Rod-ney Brooks' contrast in <ref> [Brooks, 1986] </ref> between the traditional, vertical AI architecture (Figure 5.1) where representations come together in a central `reasoning' area, and his horizontal subsumption architecture (Figure 5.2) where there are multiple paths from sensing to acting, and representations used by one path may not be shared by another.
Reference: [Brooks, 1991] <author> Brooks, Rodney A. </author> <year> (1991), </year> <title> Intelligence without Representation, </title> <booktitle> Artificial Intelligence 47 </booktitle> <pages> 139-160. </pages>
Reference: [Brooks, 1991a] <author> Brooks, Rodney A. </author> <year> (1991), </year> <title> Intelligence without Reason, </title> <booktitle> Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI-91). </booktitle>
Reference-contexts: In this problem, the best solutions were not strict hierarchies. Note also that with W-learning, a hierarchy may form only temporarily at run-time, and be dissipated when new agents are created that disrupt the existing competition. 124 Hierarchies may themselves be what Brooks <ref> [Brooks, 1991a] </ref> criticised traditional AI for structures whose main attraction is that we find it easy to think about them.
Reference: [Brooks, 1994] <author> Brooks, Rodney A. </author> <year> (1994), </year> <title> Coherent Behavior from Many Adaptive Processes, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: One agent sometimes forbids the other, then later vice versa. Brooks acknowledges this to a certain extent in <ref> [Brooks, 1994, x3] </ref>, but then avoids the question of who should win in a competition between peers by moving on (in the Cog project) to models of spreading excitation and inhibition through networks. 15.2.2 Hierarchies in Ethology Hierarchies have also long been popular in ethology.
Reference: [Charpillet et al., 1996] <author> Charpillet, Francois; Chevrier, Vincent; Foisel, Remy and Haton, </author> <title> Jean-Paul (1996), Organizing a Society of Softbots for World Wide Web Applications, </title> <booktitle> workshop on Artificial Intelligence-based tools to help W3 users, Fifth International World Wide Web Conference. </booktitle>
Reference-contexts: Also this work is not just about robotic or animal-like behavior problems. We are addressing the general issue of a system with multiple conflicting parallel goals. The model could be applied, for example, to such problems as lift scheduling, intelligent building control or Internet agents (see <ref> [Charpillet et al., 1996] </ref>). 4.1.1 Separation of world from user interface Much time is often spent on the visual user interface of such artificial worlds. But the only important thing is the actual problem to be solved.
Reference: [Clocksin and Moore, 1989] <author> Clocksin, William F. and Moore, Andrew W. </author> <year> (1989), </year> <title> Experiments in Adaptive State-Space Robotics, </title> <booktitle> Proceedings of the 7th Conference of the Society for Artificial Intelligence and Simulation of Behaviour (AISB-89). </booktitle>
Reference-contexts: introduction to this approach is <ref> [Clocksin and Moore, 1989] </ref>, which uses a state-space approach to control a robot arm.
Reference: [Dennett, 1978] <author> Dennett, Daniel C. </author> <year> (1978), </year> <title> Why not the whole iguana?, </title> <booktitle> Behavioral and Brain Sciences 1 </booktitle> <pages> 103-104. </pages>
Reference-contexts: How to parallelise is explained in more detail above in x14. See the `Number of updates required per timestep per agent' and also the `Restrictions on Decentralisation'. 17.5 The Bottom-up Animat approach The whole motivation for the behavior-based AI project (see <ref> [Dennett, 1978] </ref>) is to understand simple complete creatures and gradually move on to more complex complete creatures (as opposed to the traditional AI approach of trying to understand sub-parts of already-complex creatures in the hope of combining these parts into an understanding of a whole complex creature).
Reference: [Dennett, 1991] <author> Dennett, Daniel C. </author> <year> (1991), </year> <title> Consciousness Explained, </title> <editor> Allen Lane, </editor> <publisher> The Penguin Press. </publisher>
Reference-contexts: Where all this is leading is away from the simplistic idea of a single thread of control. Any complex mind should have alternative strategies constantly bubbling up, seeking attention, wanting to be given control of the body. As Dennett <ref> [Dennett, 1991] </ref> says, the Cartesian Theatre may be officially dead, but it still haunts our thinking. <p> We should not be so afraid of multiple unexpressed behaviors: "We can suppose that all of this happens in swift generations of `wasteful' parallel processing, with hordes of anonymous demons and their hopeful constructions never seeing the light of day . . . " <ref> [Dennett, 1991, x8.2] </ref> The concept of ideas having to fight for actual expression is of course not original. The idea of competition between selfish sub-parts of the mind is at least as old as William James and Sigmund Freud.
Reference: [Digney, 1996] <author> Digney, Bruce L. </author> <year> (1996), </year> <title> Emergent Hierarchical Control Structures: Learning Reactive/Hierarchical Relationships in Reinforcement Environments, </title> <booktitle> Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior (SAB-96). </booktitle>
Reference-contexts: For instance, when I need to wander, ideally I give way to a highly-specialised Explore agent which does nothing else. But if Explore isn't able to win on its own, I have to come in with my own rudimentary version of the skill. 18.1 Nested W-learning Digney's Nested Q-learning <ref> [Digney, 1996] </ref> shows us an alternative way, where A i can force A k to win, even if A k couldn't manage to win on its own. In 161 the basic setup (Figure 18.1), each agent is a combination both of a normal Q-learning agent and a Hierarchical Q-learning switch. <p> For example, the efficient, specialised Explore agent might almost never win on its own, but regularly win because it was being promoted by somebody else. Digney's actual model is more like a hierarchy <ref> [Digney, 1996, x2.3] </ref> with the Action Selection among the agents decided similar to a Hierarchical Q-learning switch with a global reward function.
Reference: [Edelman, 1989] <author> Edelman, Gerald M. </author> <year> (1989), </year> <title> The Remembered Present: A Biological Theory of Consciousness, </title> <publisher> Basic Books. </publisher>
Reference-contexts: carefully from the bottom up, instead of jumping direct to such complex systems. 17.6 Dynamically changing collections of agents 17.6.1 The Ecosystem of Mind This work was partly inspired by Edelman's vision of a "rainforest" mind, with life and death, growth and decay, in a dynamic ecosystem inside the head <ref> [Edelman, 1989, Edelman, 1992] </ref>. In his model, called Neural Darwinism, competition is between structures called neuronal groups. The idea is appealing, but it is difficult to tell whether neuronal groups are meant to be action-producing agents or just pattern classifiers.
Reference: [Edelman, 1992] <author> Edelman, Gerald M. </author> <year> (1992), </year> <title> Bright Air, Brilliant Fire: On the Matter of the Mind, </title> <publisher> Basic Books. </publisher>
Reference-contexts: carefully from the bottom up, instead of jumping direct to such complex systems. 17.6 Dynamically changing collections of agents 17.6.1 The Ecosystem of Mind This work was partly inspired by Edelman's vision of a "rainforest" mind, with life and death, growth and decay, in a dynamic ecosystem inside the head <ref> [Edelman, 1989, Edelman, 1992] </ref>. In his model, called Neural Darwinism, competition is between structures called neuronal groups. The idea is appealing, but it is difficult to tell whether neuronal groups are meant to be action-producing agents or just pattern classifiers.
Reference: [Grefenstette, 1992] <author> Grefenstette, John J. </author> <year> (1992), </year> <title> The Evolution of Strategies for Multi 192 agent Environments, </title> <booktitle> Adaptive Behavior 1 </booktitle> <pages> 65-89. </pages>
Reference-contexts: We will return to this in x17.2. Note that classifier systems are really searching through rules of the form: if x then execute a They are solving the Q-problem, not the W-problem. 15.5.1 Grefenstette Grefenstette's SAMUEL system <ref> [Grefenstette, 1992] </ref> uses competition among agents, where an agent (or strategy) is a set of rules. 1 A rule is of the form: IF (condition) THEN suggest (set of actions with strengths). Hence `strength' here is the equivalent of a Q-value for the action. <p> A.2 Single variable (time-weighted) An alternative approach (e.g. see <ref> [Grefenstette, 1992] </ref>) is to build up not the average of all samples, but a time-weighted sum of them, giving more weight to the most recent ones.
Reference: [Holland, 1975] <author> Holland, John H. </author> <year> (1975), </year> <booktitle> Adaptation in Natural and Artificial Systems, </booktitle> <address> Ann Arbor, </address> <publisher> Univ. Michigan Press. </publisher>
Reference-contexts: In this thesis we have agents with reward functions of the form: if (condition) then reward r 1 else reward r 2 and we use a genetic algorithm to search through different combinations of r 1 and r 2 . This may remind some of classifier systems <ref> [Holland, 1975] </ref>, which run genetic search on production rules of the form: if (condition) then execute (action) The comparison is misleading. In this thesis we are not searching the space of reward function rules. We are not inventing new reward functions, only modifying the strengths of the existing ones.
Reference: [Humphrys, 1995] <author> Humphrys, </author> <title> Mark 1 (1995), W-learning: Competition among selfish Q-learners, </title> <type> technical report no.362, </type> <institution> University of Cambridge, Computer Laboratory. </institution>
Reference-contexts: Reward functions are the "black art" of Reinforcement Learning, the place where design comes in. RL papers often list unintuitive and apparently arbitrary reward schemes which one realises may be the result of a lengthy process of trial-and-error. 5 Note that in <ref> [Humphrys, 1995, x3] </ref> I assumed that agents share the same suite of actions. 38 To summarise, much attention has been given to breaking up the states--pace of large problems, but the reward functions do not scale well either. Multi-reward functions like our global reward function are hard to design. <p> But note in passing that the W-value again avoids such complication by its strategy of simply building up an averaged estimate of how bad it is not to be obeyed. We will need more than one sample to build up a proper estimate. W-learning (introduced in <ref> [Humphrys, 1995] </ref>) is a way of building up such an 57 estimate when agents do not share the same suite of actions. <p> But this is not the same error term as in W-learning: W := W + ff ((D f ) W ) 6.2 Progress of competition In general, we assume that Q is learnt before W. Either we delay the learning of W (see <ref> [Humphrys, 1995, x3.1] </ref>) or, alternatively, imagine a dynamically 59 changing collection with agents being continually created and destroyed over time, and the surviving agents adjusting their W-values as the nature of their competition changes. <p> This is what I allowed for in <ref> [Humphrys, 1995, x4] </ref>, where the longer walk will terminate within n 2 steps. <p> Unlike in the House Robot problem, here the world is a proper torus, so the creature can always run away from the predator it cannot get stuck in corners. The creature senses x = (i; n; f; p) 1 Problem Details For these results (from <ref> [Humphrys, 1995] </ref>), when a piece of food is picked up its square becomes blank. The creature makes repeated short runs of length 50 timesteps each. Each run starts with the creature, NOPREDATORS predators and NOFOOD new pieces of food placed randomly. <p> As before, the creature takes actions a, which take values 0-7 (move in that direction) and 8 (stay still). 7.1 Analysis of best food-finding solution We searched for a good food-finding solution and a good predator-avoiding solution. See <ref> [Humphrys, 1995] </ref> for the details of what exactly our search was. The important thing here is just to show how agents can successfully interact via W-learning. <p> solution was this collection of agents: A f senses: (i,f) reward: if (picked up food) 1.62 else 0 A n senses: (n) reward: if (arrived at nest) 0.15 else 0 A p senses: (p) reward: if (just shook off predator) 0.17 else 0 This is the collection called EVO1 in <ref> [Humphrys, 1995] </ref>, rewritten to take advantage of the fact that an agent with reward function if (condition) r else s is interchangeable with one with reward function if (condition) (r-s) else 0, in the sense that both its policy and its W-values will be the same (see xC.3). <p> In this way the two agents combine to forage food, even though both are pursuing their own agendas. EVO1 is a good forager partly because A f turns out to have discovered a trick in searching for food. In <ref> [Humphrys, 1995] </ref>, I hand-coded a creature for the Ant World (similar to the hand-coded program for the House Robot that we saw above in x4.3.3). When the creature couldn't see food, my hand-coded program just adopted the strategy of making a random move 0-7. <p> solu- tion Here is the predator-avoiding solution: A f senses: (i,f) reward: if (picked up food) 1.65 else 0 A n senses: (n) reward: if (arrived at nest) 0.19 else 0 A p senses: (p) reward: if (just shook off predator) 1.23 else 0 This is the collection EVO2 in <ref> [Humphrys, 1995] </ref>. The predator-sensing agent is much stronger, and the contrast in behavior is dramatic. <p> The predator-avoiding 1 This is something I neglected to exploit in <ref> [Humphrys, 1995, x5.4] </ref>, where I used 2-reward functions with 2 non-zero rewards and needlessly evolved both rewards. 81 agent in both will be suggesting the same actions, but in the former creature it is more likely to actually win its competitions. <p> The solutions generated by learning are far superior, though difficult to visualise or translate into a concise set of instructions. 142 In the much simpler Ant World problem, W-learning with subspaces performed similar to hand-coded programs (see <ref> [Humphrys, 1995] </ref>), and the statespace was small enough to map, so we could translate the winning strategy into a set of rules (as in x7.1).
Reference: [Humphrys, 1995a] <author> Humphrys, </author> <title> Mark (1995), Towards self-organising Action Selection, </title> <booktitle> Papers of the 14th Workshop of the UK Planning and Scheduling Special Interest Group. </booktitle>
Reference-contexts: food.W (1,8) -0.096 7.3 MPEG Movie demo of basic W-learning An MPEG Movie demo of a W-learning forager in the Ant World 3 can be viewed at the web page: http://www.cl.cam.ac.uk/~mh10006/w.html The MPEG Movie demo is actually of the best forager found in the Sys tematic search section (x4.2) of <ref> [Humphrys, 1995a] </ref>. The following graphics are screen shots of the web page. <p> Note (see the text of the web page) how rewarding on transitions makes A n happier to cooperate with the other agents. It does not resist leaving the nest since it only gets rewards for the moment of arrival. 3 Problem Details For the MPEG Movie experiment (from <ref> [Humphrys, 1995a] </ref>), when a piece of food is picked up another one grows in a random location. So at all times there are fully NOFOOD pieces on the grid. There is no such thing as `runs' instead the world can run continuously for thousands of steps.
Reference: [Humphrys, 1996] <author> Humphrys, </author> <title> Mark (1996), Action Selection in a hypothetical house robot: Using those RL numbers, </title> <booktitle> Proceedings of the First International ICSC Symposia on Intelligent Industrial Automation (IIA-96) and Soft Computing (SOCO-96). </booktitle>
Reference-contexts: The optimal solution means maximising the rewards, which may or may not actually solve the problem the designer of the rewards had in mind. The agent may find a policy that maximizes the rewards in unexpected and unwelcome ways (see <ref> [Humphrys, 1996, x4.1] </ref> for an example). Still, the promise of RL is that designing reward functions will be easier than designing behavior. We will return to the theme of designing reward functions in x4.3.1 and x4.4.3. Also note that Q-learning's infinite number of (x; a) visits can only be approximated. <p> Even so, designing the global reward function here is not easy (see <ref> [Humphrys, 1996, x4.1] </ref> for an example of accidentally designing one in which the optimum solution was to jump in and out of the plug non-stop). <p> More formally, the only proper analysis of the adaptive landscape during this work was, ironically, for the case of the W-learning with subspaces in x4.1 of <ref> [Humphrys, 1996] </ref> where a badly-designed global reward function caused the optimal behavior to be to jump in and out of the plug non-stop. 1 For what it's worth, here is an illustration of that landscape.
Reference: [Humphrys, 1996a] <author> Humphrys, </author> <title> Mark (1996), Action Selection methods using Reinforcement Learning, </title> <type> PhD thesis (first version), </type> <institution> University of Cambridge, Computer Laboratory. </institution>
Reference-contexts: In a dynamic collection, agents would continually adjust their W-values as new agents came into existence, old agents died, and the nature of their competition changed. 17.6.2 Invasion of strong new agents In the earlier version of this document <ref> [Humphrys, 1996a, x17.5.1] </ref> I briefly discussed the possibilities for constructing dynamically changing collections.
Reference: [Jackson, 1987] <author> Jackson, John V. </author> <year> (1987), </year> <title> Idea for a Mind, </title> <journal> SIGART Newsletter, </journal> <volume> Number 101, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Again though, while a detailed implementation is useful, this is still directed towards the pattern recognition or category formation problem. It hasn't answered the question of how to apply this to Action Selection. 15.4.4 Jackson Jackson <ref> [Jackson, 1987] </ref> attempts to apply a pandemonium model to Action Selection. He has demons taking actions on a playing field, forging links with successor demons in the stands by exciting them, to get temporal chains of demons.
Reference: [Kaelbling, 1993] <author> Kaelbling, </author> <title> Leslie Pack (1993), Learning in Embedded Systems, </title> <publisher> The MIT Press/Bradford Books. </publisher>
Reference-contexts: But here we could learn the entire world model by interaction. What would be the advantage of doing this? For a single problem in a given world, a model is of limited use. As <ref> [Kaelbling, 1993] </ref> points out, in the time it takes to learn a model, the agent can learn a good policy anyway.
Reference: [Kaelbling, 1993a] <author> Kaelbling, </author> <title> Leslie Pack (1993), Hierarchical Learning in Stochastic Domains, </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: Kaelbling's Hierarchical Distance to Goal (HDG) algorithm <ref> [Kaelbling, 1993a] </ref> addresses the issue of giving the agent new explicit z goals at run-time. 2.2 Discrete Q-learning In the discrete case, we store each Q (x; a) explicitly, and update: Q (x; a) := (1 ff)Q (x; a) + ff (r + fl max Q (y; b)) for some learning
Reference: [Kaelbling et al., 1996] <author> Kaelbling, Leslie Pack; Littman, Michael L. and Moore, Andrew W. </author> <year> (1996), </year> <title> Reinforcement Learning: A Survey, </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 237-285. </pages>
Reference-contexts: The problem then is to calculate the max b2A Q (y; b) term we can't enumerate the actions. See <ref> [Kaelbling et al., 1996, x6.2] </ref> for a survey of generalising action spaces. 33 network to the Q a (x) estimate. We are telling the network to update in a certain direction. Next time round we may tell it to update in a different direction. <p> It should also include an explicit penalty for solutions that ignore subgoals. 17.2 Scaling up Reinforcement Learning In scaling up Reinforcement Learning to complex problems (such as robot control), ways of breaking up the problem and its statespace are clearly needed. The problem is normally phrased (e.g. see <ref> [Kaelbling et al., 1996] </ref>) as that of task decomposition breaking up the problem into subtasks that must be achieved in serial or in parallel. It would be misleading to see this thesis as providing a solution to the general problem of decomposing problems into sub-tasks.
Reference: [Karlsson, 1997] <author> Karlsson, </author> <title> Jonas (1997), Learning to Solve Multiple Goals, </title> <type> PhD thesis, </type> <institution> University of Rochester, Department of Computer Science. </institution>
Reference-contexts: In [Tyrrell, 1993, x11.1.2], Tyrrell's hierarchical decision structure implements a form of Maximize the Best Happiness, while his free-flow hierarchy implements a form of Maximize Collective Happiness. 15.4.9 Modular Q-learning (University of Rochester) Work at the University of Rochester <ref> [Whitehead et al., 1993, Karlsson, 1997] </ref> addresses the same question as this thesis given a collection of competing peers, who should win? Their nearest neighbor strategy is effectively Maximize the Best Happiness (W=Q). Their greatest mass strategy is Maximize Collective Happiness. <p> Their greatest mass strategy is Maximize Collective Happiness. A Minimize the Worst Unhappiness strategy was not tried. <ref> [Karlsson, 1997] </ref> argues that a modular approach learns a sub-optimal policy quicker and with less memory than a monolithic approach.
Reference: [Lin, 1992] <author> Lin, </author> <month> Long-Ji </month> <year> (1992), </year> <title> Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 293-321. </pages>
Reference-contexts: So for reasons of both memory and time, instead of using lookup tables we need to use some sort of generalisation here, multi-layer neural networks. 2 Following Lin <ref> [Lin, 1992] </ref>, because we have a small finite number of actions we can reduce interference by breaking the state-action space up into one network per action a. We have 9 separate nets acting as function approx-imators. <p> Instead we repeatedly apply the same set of examples many times. Also, we don't want to replay an experience multiple times immediately. Instead we save a number of experiences and mix them up in the replay, each time factoring in new Q (y)'s. Our strategy roughly follows <ref> [Lin, 1992] </ref>. We do 100 trials. In each trial we interact with the world 1400 times, remember our experiences, and then replay the experiences 30 times, each time factoring in more accurate Q (y) estimates. <p> Like Lin, we use backward replay as more efficient (update Q (y) before updating the Q (x) that led to it). <ref> [Lin, 1992] </ref> points out doing one huge trial and then replaying it is a bad idea because the actions will all be random.
Reference: [Lin, 1993] <author> Lin, </author> <month> Long-Ji </month> <year> (1993), </year> <title> Scaling up Reinforcement Learning for robot control, </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning. </booktitle>
Reference-contexts: Typically, each subtask can only ever be partially satisfied [Maes, 1989]. 3.1 Hierarchical Q-learning Lin has devised a form of multi-module RL suitable for such problems, and this will be the second method tested below. Lin <ref> [Lin, 1993] </ref> suggests breaking up a complex problem into sub-problems, having a collection of Q-learning agents A 1 ; : : : ; A n learn the sub-problems, and then have a single controlling Q-learning agent which learns Q (x; i), where i is which agent to choose in state x. <p> A better strategy is to continually update the control policy to choose better actions over time. Doing lots of short trials, replaying after each one, allows our control policy to improve for each trial. As Lin points out <ref> [Lin, 1993] </ref>, experience replay is actually like building a model and running over it in our head, as in Sutton's DYNA architecture (x2.1.7). <p> But if the world was that simple, Q-learning would do fine and we wouldn't even be interested in Hierarchical Q-learning let alone any of the self-organising, decentralised methods. It's because we are not in such a simple world that Q-learning (and indeed Hierarchical Q-learning too) can be beaten. <ref> [Lin, 1993] </ref> has already pointed out that the reason Hierarchical Q-learning beat Q-learning was because Q-learning could not learn the optimal solution in its neural network. Q-learning and Hierarchical Q-learning suffer from a similar flaw to the Collective methods (x12.3) they will tend to lose minority agents.
Reference: [Maes, 1989] <author> Maes, </author> <title> Pattie (1989), How To Do the Right Thing, </title> <booktitle> Connection Science 1 </booktitle> <pages> 291-323. </pages>
Reference-contexts: The Action Selection problem essentially concerns subtasks acting in parallel, and interrupting each other rather than running to completion. Typically, each subtask can only ever be partially satisfied <ref> [Maes, 1989] </ref>. 3.1 Hierarchical Q-learning Lin has devised a form of multi-module RL suitable for such problems, and this will be the second method tested below. <p> One agent is generally in charge, but will be corrected by the other agents whenever it offends them too much. W-learning tries to keep everyone on board, while still going somewhere. For a further analysis of how Minimize the Worst Unhappiness gets tasks completed (enforces persistence) see x15.1.3. <ref> [Maes, 1989] </ref> lists desirable criteria for action selection schemes, and in it we see this tension between wanting actions that contribute to several goals at once and yet wanting to stick at goals until their conclusion. <p> was addressed in x3. 15.3.2 Wixson Wixson's model (to be introduced in x18.1) is basically a serial form of Nested Q-learning, where we must wait for agents to terminate at their goal states before control may switch to a new agent. 15.3.3 Maes' Spreading Activation Networks Maes' Spreading Activation Networks <ref> [Maes, 1989, Maes, 1989a] </ref> or Behavior Networks consist of a network of agents (or nodes) which are aware of their preconditions. <p> Most parallel multi-agent systems postulate low-bandwidth communications between agents anyway. With basic (not Negotiated) W-learning there is perfect parallelism, where there is no communication between agents at all. What communication there is takes place between agents and the switch. Similar to schemes such as Maes' Spreading Activation Networks <ref> [Maes, 1989, x7.2] </ref>, no problem-specific agent language is used. Only numbers are communicated. Consider again Watkins' motivation [Watkins, 1989] of finding learning methods that might plausibly occur in nature, in particular methods that impose limited computational demands per timestep.
Reference: [Maes, 1989a] <author> Maes, </author> <title> Pattie (1989), The dynamics of action selection, </title> <booktitle> Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89). </booktitle>
Reference-contexts: was addressed in x3. 15.3.2 Wixson Wixson's model (to be introduced in x18.1) is basically a serial form of Nested Q-learning, where we must wait for agents to terminate at their goal states before control may switch to a new agent. 15.3.3 Maes' Spreading Activation Networks Maes' Spreading Activation Networks <ref> [Maes, 1989, Maes, 1989a] </ref> or Behavior Networks consist of a network of agents (or nodes) which are aware of their preconditions.
Reference: [Mataric, 1994] <author> Mataric, Maja J. </author> <year> (1994), </year> <title> Learning to behave socially, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Since the methods we will test take as input any vector of numbers, and can produce any vector as output, they can be reapplied afterwards to other problems with more realistic sensory and motor data. RL has been applied e.g. by <ref> [Mataric, 1994] </ref> to multiple, real autonomous mobile robots. I make no stronger assumptions about the world than RL does, so if RL can be applied to real robots then so can my work. Also this work is not just about robotic or animal-like behavior problems.
Reference: [McFarland, 1989] <author> McFarland, David (1989), </author> <title> Problems of Animal Behaviour, </title> <publisher> Longman. </publisher>
Reference-contexts: Hence the differences between Q-values are larger (not discounted). As its goal approaches, the agent will generate higher W-values until satisfied. Ethologists have generally proposed positive feedback models to explain persistence (see <ref> [McFarland, 1989, x1] </ref>), where the strength W f of the feeding agent is being increased by the act of feeding itself (while at the same time being decreased by the reduction in hunger, so that it is still probably decreasing overall). <p> Our method factors in what McFarland <ref> [McFarland, 1989] </ref> calls the cost of switching from one activity to another, where nothing useful may happen for some time. Here, the Q-values are a promise of what will happen in the future, suitably discounted, and the W-value scheme makes an action selection decision based on the different agents' promises. <p> High rewards far away may compete against low rewards close by. 123 15.1.4 McFarland Looking closer at McFarland's work, a W-value can be seen as equivalent to the `tendency' to perform a behavior in his model of motivational competition <ref> [McFarland, 1989] </ref>.
Reference: [Metcalfe and Boggs, 1976] <author> Metcalfe, Robert M. and Boggs, David R. </author> <year> (1976), </year> <title> Ethernet: Distributed Packet Switching for Local Computer Networks, </title> <journal> Communications of the ACM 19 </journal> <pages> 395-404. </pages>
Reference-contexts: In an Operating System all losses are assumed the same the agent just has to wait some time. There is also no concept that a non-winner can profit from what the winner is doing. 15.7 Ethernet protocols Another interesting comparison is with how the Ethernet <ref> [Metcalfe and Boggs, 1976] </ref> works. In an ethernet, stations only transmit when they find the Ether is silent. When multiple stations transmit at the same time, a collision occurs and they back off at different rates, so that one will be left to transmit while the others defer their transmission.
Reference: [Minsky, 1986] <author> Minsky, </author> <title> Marvin (1986), The Society of Mind, </title> <publisher> Simon and Schuster, </publisher> <address> New York. </address>
Reference-contexts: In this work, goals are implicitly activated and deactivated by suitable accompanying internal and external senses. 15.1.3 Dithering and Persistence Minsky <ref> [Minsky, 1986] </ref> warns that too simple forms of state-based switching will be unable to engage in opportunistic behavior. His example is of a hungry and thirsty animal. Food is only found in the North, water in the South. <p> If there are multiple unexpressed behaviors, and lots of overlap, this may be a small price to pay if there is a robust solution. This has some resemblance to Minsky's "accumulation" of multiple different ways of solving a problem in his Society of Mind <ref> [Minsky, 1986] </ref>. Some may be more efficient than others, but we keep them all for robustness. Or more subtly, it might be hard to rank them in a single order. Some may be more efficient some times (for some x) but less efficient other times (for other x). <p> The basic ideas running throughout this thesis, of overlap driven by strong individuals, of multiple unexpressed behaviors and agents dropping out of competitions, all have a parallel in Minsky's "If it's broken don't fix it, suppress it" maxim in the Society of Mind <ref> [Minsky, 1986] </ref>. In Minsky's model, the normally-good behavior is allowed expression most of the time, except in some states where a censor overrides it and prevents its expression.
Reference: [Moore, 1990] <author> Moore, Andrew W. </author> <year> (1990), </year> <title> Efficient Memory-based Learning for Robot Control, </title> <type> PhD thesis, </type> <institution> University of Cambridge, Computer Laboratory. </institution> <note> 1 All my publications are at: http://www.cl.cam.ac.uk/~mh10006/publications.html 193 </note>
Reference-contexts: We will clearly be interested in methods of breaking problems up into subproblems which can work with smaller statespaces and simpler reward functions, and then having some method of combining the subproblems to solve the main task. Most of the work in RL either designs the decomposition by hand <ref> [Moore, 1990] </ref>, or deals with problems where the sub-tasks have termination conditions and combine sequentially to solve the main problem [Singh, 1992, Tham and Prager, 1994]. The Action Selection problem essentially concerns subtasks acting in parallel, and interrupting each other rather than running to completion.
Reference: [Ono et al., 1996] <author> Ono, Norihiko; Fukumoto, Kenji and Ikeda, </author> <title> Osamu (1996), Collective Behavior by Modular Reinforcement-Learning Animats, </title> <booktitle> Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior (SAB-96). </booktitle>
Reference-contexts: A Minimize the Worst Unhappiness strategy was not tried. [Karlsson, 1997] argues that a modular approach learns a sub-optimal policy quicker and with less memory than a monolithic approach. While this is true, I further point out that the monolithic approach may not even learn the optimal policy itself. <ref> [Ono et al., 1996, x2] </ref>, in their implementation of greatest mass, implement exactly the Maximize Collective Happiness strategy. 15.4.10 W-learning For completeness, I point out which category this work itself belongs to.
Reference: [Ray, 1991] <author> Ray, Thomas S. </author> <year> (1991), </year> <title> An Approach to the Synthesis of Life, </title> <booktitle> Artificial Life II. </booktitle>
Reference-contexts: But it no longer need be available to the agents as an explicit function they can learn against. It is only used to test them. Hence the fitness function could be just implicit in the environment, as in the best Artificial Life research <ref> [Ray, 1991, Todd et al., 1994] </ref>. As has been said many times in contrasting natural evolution with the standard Genetic Algorithm, living things don't have an explicit global reward 84 defined or available to learn from.
Reference: [Ring, 1992] <author> Ring, </author> <title> Mark (1992), Two Methods for Hierarchy Learning in Reinforcement Environments, </title> <booktitle> Proceedings of the Second International Conference on Simulation of Adaptive Behavior (SAB-92). </booktitle>
Reference-contexts: In interleaving different behaviors, various authors have argued for time-based switching (e.g. see <ref> [Ring, 1992] </ref>). Blumberg [Blumberg, 1994] argues the need for a model of fatigue, where a switch of activity becomes more likely the longer an activity goes on. He points out that animals sometimes appear to engage in a form of time-sharing.
Reference: [Rosenblatt, 1995] <author> Rosenblatt, Julio K. </author> <year> (1995), </year> <title> DAMN: A Distributed Architecture for Mobile Navigation, </title> <booktitle> Proceedings of the 1995 AAAI Spring Symposium on Lessons Learned from Implemented Software Architectures for Physical Agents. </booktitle>
Reference-contexts: It would be interesting to see a model detailed enough for implementation. 15.4.5 The DAMN Architecture The Distributed Architecture for Mobile Navigation or `DAMN' architecture <ref> [Rosenblatt, 1995, Rosenblatt and Thorpe, 1995] </ref> is a pandemonium-like model for Action Selection. Agents vote for actions, and the action which receives the most votes is executed. The action selection method is similar to Maximize Collective Happiness. <p> Rosenblatt acknowledges this, and suggests the w i weightings can be changed dynamically as the creature moves through different states x. But he has no automatic way of generating these weights. This is all part of the burden of hand-design. The utility theory developed in <ref> [Rosenblatt, 1995] </ref> develops a hand-designed equivalent of a static W = importance scheme (as in x5.4), where the agent's vote for an action is calculated relative to all alternative actions. One difference is that the agent has a vote for every action W (x; a).
Reference: [Rosenblatt and Thorpe, 1995] <author> Rosenblatt, Julio K. and Thorpe, Charles E. </author> <year> (1995), </year> <title> Combining Multiple Goals in a Behavior-Based Architecture, </title> <booktitle> Proceedings of the 1995 International Conference on Intelligent Robots and Systems (IROS-95). </booktitle>
Reference-contexts: It would be interesting to see a model detailed enough for implementation. 15.4.5 The DAMN Architecture The Distributed Architecture for Mobile Navigation or `DAMN' architecture <ref> [Rosenblatt, 1995, Rosenblatt and Thorpe, 1995] </ref> is a pandemonium-like model for Action Selection. Agents vote for actions, and the action which receives the most votes is executed. The action selection method is similar to Maximize Collective Happiness.
Reference: [Ross, 1983] <author> Ross, Sheldon M. </author> <year> (1983), </year> <title> Introduction to Stochastic Dynamic Programming, </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: [Rummery and Niranjan, 1994] <author> Rummery, Gavin and Niranjan, </author> <month> Mahesan </month> <year> (1994), </year> <title> On-line Q-learning using Connectionist systems, </title> <type> technical report no.166, </type> <institution> University of Cam-bridge, Engineering Department. </institution>
Reference-contexts: Each net is responsible for a different action a. This also allows us to easily calculate the max b2A Q (y; b) term just enumerate the actions and get a value from each network. 3 We also note that, as in <ref> [Rummery and Niranjan, 1994] </ref>, although we have a large statespace, each element of the input vector x takes only a small number of discrete values. <p> This makes it easier for the network to identify and separate the inputs. Employing this strategy, we represent all possible inputs x in 57 input units which are all binary 0 or 1. Also like <ref> [Rummery and Niranjan, 1994] </ref>, we found that a small number of hidden units (10 here) gave the best performance. To summarise, we have 9 nets. Each has 57 input units, 10 hidden units, and a single output unit. <p> This approach is pointless because it means just obey one of the agents and cause unhappiness zero for them. I have not seen an example of straightforward use of W=Q in Reinforcement Learning, but it can hardly be an original idea. What look like examples <ref> [Rummery and Niranjan, 1994] </ref> turn out only to be using multiple neural networks for storing Q-values Q a (x) in a monolithic (single reward function) Q-learning system (x4.3.2) and then letting through the action with the highest Q-value.
Reference: [Sahota, 1994] <author> Sahota, Michael K. </author> <year> (1994), </year> <title> Action Selection for Robots in Dynamic Environments through Inter-behaviour Bidding, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Even before it has got south, it will be starving again. One solution to this would be time-based, where agents get control for some minimum amount of time. Again, however, time-based switching is not the only answer. As Sahota <ref> [Sahota, 1994] </ref> points out, the real problem here is having actions based only on the urgencies of the goal, independent of the current situation.
Reference: [Scheier and Pfeifer, 1995] <author> Scheier, Christian and Pfeifer, </author> <title> Rolf (1995), Classification as Sensory-Motor Coordination, </title> <booktitle> Proceedings of the 3rd European Conference on Artificial Life (ECAL-95). </booktitle>
Reference-contexts: Note that none of these are schemes where the actions of agents are merged. Merging or averaging actions does not make sense in general but only with certain types of actions. For example, see <ref> [Scheier and Pfeifer, 1995] </ref>, where all agents suggest a speed value for the left motor and one for the right motor. These type of actions are amenable to addition and subtraction. 14.3 Single-Mindedness We will attempt to summarise both this chapter and the preceding discussion in one diagram.
Reference: [Selfridge and Neisser, 1960] <author> Selfridge, Oliver G. and Neisser, </author> <month> Ulric </month> <year> (1960), </year> <title> Pattern recognition by machine, </title> <publisher> Scientific American 203 </publisher> <pages> 60-68. </pages>
Reference-contexts: simultaneously, and some intelligent (or dumb) switch deciding which one of them (or combination of them) to let through. 15.4.1 Lin's Hierarchical Q-learning Lin's Hierarchical Q-learning has already been introduced (in x3) and subsequently tested. 15.4.2 Pandemonium The ancestor of our abstract decentralised model of mind is Selfridge's Pandemonium model <ref> [Selfridge and Neisser, 1960] </ref>. This is a collection of "demons" 127 shrieking to be listened to. The highest shriek wins.
Reference: [Singh, 1992] <author> Singh, Satinder P. </author> <year> (1992), </year> <title> Transfer of Learning by Composing Solutions of Elemental Sequential Tasks, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 323-339. </pages>
Reference-contexts: Most of the work in RL either designs the decomposition by hand [Moore, 1990], or deals with problems where the sub-tasks have termination conditions and combine sequentially to solve the main problem <ref> [Singh, 1992, Tham and Prager, 1994] </ref>. The Action Selection problem essentially concerns subtasks acting in parallel, and interrupting each other rather than running to completion. <p> The Action Selection problem is essentially about the latter. Serial or sequential models have already been criticised in x3. 125 15.3.1 Singh's Compositional Q-learning Singh's Compositional Q-learning <ref> [Singh, 1992] </ref> was addressed in x3. 15.3.2 Wixson Wixson's model (to be introduced in x18.1) is basically a serial form of Nested Q-learning, where we must wait for agents to terminate at their goal states before control may switch to a new agent. 15.3.3 Maes' Spreading Activation Networks Maes' Spreading Activation
Reference: [Singh et al., 1994] <author> Singh, Satinder P.; Jaakkola, Tommi and Jordan, Michael I. </author> <year> (1994), </year> <title> Learning without state-estimation in Partially Observable Markovian Decision Processes, </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference-contexts: P xa (r) is non-stationary. In fact, the artificial world is a Partially-Observable MDP <ref> [Singh et al., 1994] </ref>.
Reference: [Sporns, 1995] <author> Sporns, </author> <title> Olaf (1995), </title> <type> personal communication. </type>
Reference-contexts: But from the point of view of engineering, concepts like `individual' are only useful myths. If a collection is 1 This may all be a matter of terminology. Edelman's co-worker Olaf Sporns says <ref> [Sporns, 1995] </ref> that they do not like to use the word algorithm for a self-modifying algorithm embedded in the real world.
Reference: [Steels, 1994] <author> Steels, </author> <title> Luc (1994), A case study in the Behavior-Oriented design of Autonomous Agents, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: Watkins argues that the latter methods characterise machines (or at least, machines as traditionally conceived) while the former characterise what animals actually do. Recently there has been an emphasis on autonomous agents as dynamical systems (e.g. <ref> [Steels, 1994] </ref>), emphasising the continuous interaction between the agent and its world such that the two cannot be meaningfully separated.
Reference: [Sutton, 1988] <author> Sutton, Richard S. </author> <year> (1988), </year> <title> Learning to Predict by the Methods of Temporal Differences, </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: In the discrete case, Q-learning would be: D := (1 ff)D + ffd and W-learning would be: W := (1 ff)W + ff (D f ) this is confusing because it looks like a standard way <ref> [Sutton, 1988] </ref> of writing the Q-learning update: D := D + ff (d D) where the expected value of the error term (d D) goes to zero as we learn.
Reference: [Sutton, 1990] <author> Sutton, Richard S. </author> <year> (1990), </year> <title> Integrated Architectures for Learning, Planning and Reacting Based on Approximating Dynamic Programming, </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning. </booktitle>
Reference-contexts: The specific control policy used is a standard one in the field and originally comes from [Watkins, 1989] and <ref> [Sutton, 1990] </ref>. The agent tries out actions probabilistically based on their Q-values using a Boltzmann or soft max distribution.
Reference: [Sutton, 1990a] <author> Sutton, Richard S. </author> <year> (1990), </year> <title> Reinforcement Learning Architectures for An-imats, </title> <booktitle> Proceedings of the First International Conference on Simulation of Adaptive Behavior (SAB-90). </booktitle>
Reference-contexts: The real advantage of a model is to avoid having to re-learn everything when either the world or problem change: * Changing P xa (y) when the dynamics of the world changes. Sutton shows in <ref> [Sutton, 1990a] </ref> how a model can be exploited to deal with a changing world. His agent keeps running over the world model in its head, updating values based on its estimates for the transition 17 probabilities, as in Dynamic Programming. <p> The effect of time-based switching occurs when the state-based creature is in continuous interaction with a changing world. Having internal hunger that increases over time does not necessarily break our MDP model. As <ref> [Sutton, 1990a] </ref> points out, we simply redraw the boundary between the creature and its environment. Where x is `hungry', y is `very hungry', and a is `do nothing', we might have say P xa (x) = 0:9 and P xa (y) = 0:1 for any timestep.
Reference: [Tan, 1993] <author> Tan, </author> <title> Ming (1993), Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, Proceedings of the Tenth International Conference on Machine Learn 194 ing. </booktitle>
Reference: [Tesauro, 1992] <author> Tesauro, </author> <title> Gerald (1992), Practical Issues in Temporal Difference Learning, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 257-277. </pages>
Reference-contexts: Also like [Rummery and Niranjan, 1994], we found that a small number of hidden units (10 here) gave the best performance. To summarise, we have 9 nets. Each has 57 input units, 10 hidden units, and a single output unit. As <ref> [Tesauro, 1992] </ref> notes, learning here is not like ordinary supervised learning where we learn from (Input,Output) exemplars.
Reference: [Tham and Prager, 1994] <author> Tham, Chen K. and Prager, Richard W. </author> <year> (1994), </year> <title> A modular Q-learning architecture for manipulator task decomposition, </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference-contexts: Most of the work in RL either designs the decomposition by hand [Moore, 1990], or deals with problems where the sub-tasks have termination conditions and combine sequentially to solve the main problem <ref> [Singh, 1992, Tham and Prager, 1994] </ref>. The Action Selection problem essentially concerns subtasks acting in parallel, and interrupting each other rather than running to completion.
Reference: [Todd et al., 1994] <author> Todd, Peter M.; Wilson, Stewart W.; Somayaji, Anil B. and Yanco, Holly A. </author> <year> (1994), </year> <title> The blind breeding the blind: Adaptive behavior without looking, </title> <booktitle> Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB-94). </booktitle>
Reference-contexts: But it no longer need be available to the agents as an explicit function they can learn against. It is only used to test them. Hence the fitness function could be just implicit in the environment, as in the best Artificial Life research <ref> [Ray, 1991, Todd et al., 1994] </ref>. As has been said many times in contrasting natural evolution with the standard Genetic Algorithm, living things don't have an explicit global reward 84 defined or available to learn from. <p> ani-mat with needs and a sensory/motor system that satisfies these needs to some criterion, increase the difficulty of the environment or the complexity of the needs and find the minimum increase in animat complexity necessary to satisfy the needs to the same criterion." [Wilson, 1990] For example, Todd et al. <ref> [Todd et al., 1994] </ref> show that even the possibilities of sensor-less (!) memory-less creatures have not yet been fully explored. Their creatures find (by evolution) the optimum p (a), the probability of generating an action (independent of the state of the world).
Reference: [Tyrrell, 1993] <author> Tyrrell, </author> <title> Toby (1993), Computational Mechanisms for Action Selection, </title> <type> PhD thesis, </type> <institution> University of Edinburgh, Centre for Cognitive Science. </institution>
Reference-contexts: There is no attempt to simulate realistic robot sensors. There 28 is no explicit simulated noise, though we do have limited and confusing sen-sory information. All I am interested in is setting up a hard action selection problem. And it is hard, as we shall see. Tyrrell <ref> [Tyrrell, 1993, x1.1] </ref> defends the value of such microworld experiments in relation to the Action Selection problem at least. At the moment, it is difficult enough to set up a robotics experiments with a single goal. <p> The black area shows the states that the `Food' agent wins. 47 5.2 W = Drive Strength Our abstract decentralised model of mind is a form of the drives model commonly used in ethology (dating back to Hull's work in the 1940s), where the `drive strength' or `importance variable' <ref> [Tyrrell, 1993] </ref> is equivalent to the W-value, and the highest one wins (the exact action of that agent is executed). <p> Tyrrell's equation <ref> [Tyrrell, 1993, x8.1] </ref>: drive_strength = stimulus_strength = f (internal, external and indeterminate stimuli) is equivalent to saying that: W = W (x) It is drive strength relative to the other drives that matters rather than absolute drive strength. <p> They tend to just leave it as a problem for evolution, or in our case a designer. See <ref> [Tyrrell, 1993, x4.2,x9.3.1] </ref> for the difficult design problems in actually implementing a drives model. Tyrrell has the problem of designing sensible drive strengths, relative drive strengths, and further has to design the appropriate action for each agent to execute if it wins. <p> Similarly, pressure is reducing on agents that 118 are being expressed, which may stop them even though they are not finished their task. While some animals do indeed appear to engage in vacuum activity, Tyrrell <ref> [Tyrrell, 1993] </ref> argues convincingly that vacuum activity should be seen as a flaw in any action selection scheme. Control should switch for a reason. It is not clear anyway that time-sharing effects cannot be achieved by a suitable state representation x. <p> Baerends, for example, put a great deal of work into breaking down the behaviors of the digger wasp and the herring gull into multi-level hierarchical structures. See <ref> [Tyrrell, 1993, x8] </ref> for a survey of this and Tinbergen's hierarchical models. 15.2.3 Nested Q-learning Nested Q-learning will be introduced in x18.1. <p> The advantage of mixing the `Q-problem' and the `W-problem' here is that appetitive nodes can be shared by multiple goals. But Tyrrell then shows <ref> [Tyrrell, 1993, x9.3.3] </ref> that there may be problems caused if we can't tell if the `Explore' node is excited multiple times because it is serving multiple goals or because it is serving multiple paths to the same goal. <p> Here the dif ference is we provide an answer to the question of where the drive strengths come from. 15.4.8 Tyrrell Implementing a W = D F measure is essentially what Tyrrell is trying to do in his parameter tuning to get an implementation of drives working 130 <ref> [Tyrrell, 1993, x9.3.1] </ref>. The amount of hand-design he needs to do only points out the value of self-organisation of these numbers. For example, if a predator is visible he designs his `Avoid predator' drive to have a high drive strength (it must be listened to). <p> A h would know that only that particular direction was bad, and would learn a low W-value (would not compete) if other agents were going in some other direction anyway. In <ref> [Tyrrell, 1993, x11.1.2] </ref>, Tyrrell's hierarchical decision structure implements a form of Maximize the Best Happiness, while his free-flow hierarchy implements a form of Maximize Collective Happiness. 15.4.9 Modular Q-learning (University of Rochester) Work at the University of Rochester [Whitehead et al., 1993, Karlsson, 1997] addresses the same question as this thesis <p> It does seem wrong that there should exist such a structure, for what does it mean? Is it not merely a number of unrelated observations from different senses combined together. In Tyrrell's Simulated Environment <ref> [Tyrrell, 1993] </ref> the full space would have to be of size 10 100 . Across all our methods, those huge full spaces are mostly wasted. The two best methods were both with tiny subspaces. Some of the results were as expected.
Reference: [Varian, 1993] <author> Varian, Hal R. </author> <year> (1993), </year> <title> Intermediate Microeconomics, </title> <publisher> W.W.Norton and Co. </publisher>
Reference-contexts: That is, we are only interested in the best possible individual happiness. We are going to start drawing economic analogies to our various approaches. In economic theory, this would be the equivalent of a Nietzschean social welfare function <ref> [Varian, 1993, x30] </ref>, where the value of an allocation depends on the welfare of the best off agent. <p> The winning action may be an action that none of the agents would have suggested. In economics, this method would be equivalent to the classic utilitarian social welfare function <ref> [Varian, 1993, x30] </ref> (the greatest happiness for the greatest number). If the agents don't share the same suite of actions, it's hard to see what we can do. We can't predict the happiness of other agents if one agent's action is taken. <p> It would be simple if you could just take a majority vote, but here we're trying to respect individual stories. 13.1 Pure Minimize the Worst Unhappiness If we could make a global decision, the decision we want is probably something like John Rawls' maximin principle from economics <ref> [Varian, 1993, x30] </ref>. This says that the social welfare of an allocation depends only on the welfare of the worst off agent. <p> Because of different assumptions though, much of the work is not easily transferable to the Society of Mind. In economics, utility <ref> [Varian, 1993, x4] </ref> is used as a way of ranking an agent's preferences. It can be seen as analogous to Q-values for actions. In the standard ordinal utility theory, the precise numbers are not important. What matters is only the order in which things are ranked. <p> Here we can introduce an analog of W-values, where the difference between the Q-values is measured. We can ask the question: `You want choice A. If we force you to take choice B, how bad is that?' Welfare theory <ref> [Varian, 1993, x30] </ref> is the branch of economics with the most relevance to this thesis. Welfare theory makes ethical judgements about what kind of economic society we want, and suggests various social welfare functions to be optimised.
Reference: [Watkins, 1989] <author> Watkins, Christopher J.C.H. </author> <year> (1989), </year> <title> Learning from delayed rewards, </title> <type> PhD thesis, </type> <institution> University of Cambridge, Psychology Department. </institution>
Reference-contexts: Q-learning is asynchronous and sampled each Q (x; a) is updated one at a time, and the control policy may visit them in any order, so long as it visits them an infinite number of times. Watkins <ref> [Watkins, 1989] </ref> describes his algorithm as "incremental dynamic programming by a Monte Carlo method". After convergence, the agent then will maximize its total discounted ex pected reward if it always takes the action with the highest Q fl -value. <p> The specific control policy used is a standard one in the field and originally comes from <ref> [Watkins, 1989] </ref> and [Sutton, 1990]. The agent tries out actions probabilistically based on their Q-values using a Boltzmann or soft max distribution. <p> In other words, the agents could have organised themselves like this in the absence of a global reward. In this chapter I consider how agents might organise themselves sensibly in the absence of a global reward. Watkins in his PhD <ref> [Watkins, 1989] </ref> was interested in learning methods that might plausibly take place within an animal, involving a small number of simple calculations per timestep, and so on. <p> State-space agents are completely interruptable. In the terminology of <ref> [Watkins, 1989, x9] </ref>, the Action Selection methods I propose in this thesis are supervisory methods, where there is a continuous stream of commands interrupting each other, rather than delegatory methods, where a top level sends a command and then is blocked waiting for some lower level loop to terminate. <p> W (full space) n.xa + n.X n.1 + 1.(n-1) 0 Partial Negotiated Coll. W n.xa n.1 n.(n-1) Full Max Coll. Happ. n.xa n.1 | Full General comparisons between the action selection methods. A dash indicates `not applicable' here. `Number of updates per timestep' is modelled on Watkins <ref> [Watkins, 1989] </ref> where he wanted to impose limited computational demands on his creature per timestep. The format here is (number of agents) times (updates per agent). We are looking for updates that can be carried out in parallel in isolation. <p> What communication there is takes place between agents and the switch. Similar to schemes such as Maes' Spreading Activation Networks [Maes, 1989, x7.2], no problem-specific agent language is used. Only numbers are communicated. Consider again Watkins' motivation <ref> [Watkins, 1989] </ref> of finding learning methods that might plausibly occur in nature, in particular methods that impose limited computational demands per timestep. Once the Q-values have been learnt, Q-learning provides a fast control policy ignore rewards, don't learn, and simply act on the best Q-value. <p> The winner may or may not call an agent in the lower layer. 165 said before though (x15.2.1), hierarchies are only particular cases of the more general model, and not necessarily the most interesting cases either. 18.2 Feudal W-learning Watkins' Feudal Q-learning <ref> [Watkins, 1989, x9] </ref> shows another way of having agents use other agents by sending explicit orders.
Reference: [Watkins and Dayan, 1992] <author> Watkins, Christopher J.C.H. and Dayan, </author> <title> Peter (1992), Technical Note: </title> <booktitle> Q-Learning, Machine Learning 8 </booktitle> <pages> 279-292. </pages>
Reference-contexts: When we are in a well-visited area of state-space, the learning rate is low. We have taken lots of samples, so the single current sample can't make much difference to the average of all of them (the Q-value). 2.2.1 Convergence <ref> [Watkins and Dayan, 1992] </ref> proved that the discrete case of Q-learning will converge to an optimal policy under the following conditions. <p> work) is, where n (x; a) = 1; 2; 3; : : : is the number of times Q (x; a) has been visited: ff (x; a) = 1 = 1; 1 3 ; : : : If each pair (x; a) is visited an infinite number of times, then <ref> [Watkins and Dayan, 1992] </ref> shows that for lookup tables Q-learning converges to a unique set of values Q (x; a) = Q fl (x; a) which define a stationary deterministic optimal policy. <p> Although the fl max b2A Q (y; b) term may factor in an inaccurate initial Q-value from elsewhere, these Q-values will themselves be wiped out by their own first update, and poor initial samples have no effect if in the long run samples are accurate (Theorem A.2). See <ref> [Watkins and Dayan, 1992] </ref> for the proof that initial values are irrelevant. Note that ff i = 1 i p will satisfy the conditions for any 1 2 &lt; p 1.
Reference: [Weir, 1984] <author> Weir, </author> <title> Michael (1984), Goal-Directed Behaviour, </title> <publisher> Gordon and Breach. </publisher>
Reference: [Whitehead et al., 1993] <author> Whitehead, Steven; Karlsson, Jonas and Tenenberg, </author> <title> Josh (1993), Learning Multiple Goal Behavior via Task Decomposition and Dynamic Policy Merging, </title> <editor> in Connell and Mahadevan, eds., </editor> <title> Robot Learning, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: The need for time-based action selection (above) seems to come when behaviors can go on receiving high rewards indefinitely. <ref> [Whitehead et al., 1993] </ref> make this explicit in their scheme, where goals are inactive (don't compete), start up at random (are activated), compete until they are fulfilled, and then fall inactive again. <p> In [Tyrrell, 1993, x11.1.2], Tyrrell's hierarchical decision structure implements a form of Maximize the Best Happiness, while his free-flow hierarchy implements a form of Maximize Collective Happiness. 15.4.9 Modular Q-learning (University of Rochester) Work at the University of Rochester <ref> [Whitehead et al., 1993, Karlsson, 1997] </ref> addresses the same question as this thesis given a collection of competing peers, who should win? Their nearest neighbor strategy is effectively Maximize the Best Happiness (W=Q). Their greatest mass strategy is Maximize Collective Happiness. <p> Another solution (discussed in <ref> [Whitehead et al., 1993] </ref>) is to use a model to predict what state our Action Selection policy will lead to next, what decision Action Selection will take then, what state that will lead to, and so on. <p> It would have a much higher W-value than any other individual. Our four approaches to action selection all decide competition based only on the current state x and the Q-values the agents have for x. <ref> [Whitehead et al., 1993] </ref> call this a simple merging strategy, but are too quick to move on to more complex methods without considering what properties a Minimize the Worst Unhappiness simple merging strategy could have. 16.4.2 What the Global reward function cannot ex press So we have argued that Minimize the <p> Any scaled static or dynamic measures of W (x8.1.2,x15.4.5). These won't provide any advantage because we do not want to fix agents' inequalities relative to each other (see x8.1.2). 152 The Collective methods (x12). <ref> [Whitehead et al., 1993] </ref> found that Maximize Collective Happiness performed not much different from W=Q (while both were better than monolithic Q-learning). The product method of Maximize Collective Happiness (x15.5.2) once the correction is made that rewards (and hence Q-values) are all made &gt; 0.
Reference: [Wilson, 1990] <author> Wilson, Stewart W. </author> <year> (1990), </year> <title> The animat path to AI, </title> <booktitle> Proceedings of the First International Conference on Simulation of Adaptive Behavior (SAB-90). </booktitle>
Reference-contexts: One ends up with a hand-designed ad-hoc system from which very few if any general principles can be learnt. I take the broad approach of Wilson <ref> [Wilson, 1990] </ref> in pushing the limits of simple animat models, and resisting as long as possible the temptation to introduce more complex models: "The essential process is . . . given an environment and an ani-mat with needs and a sensory/motor system that satisfies these needs to some criterion, increase the <p> . . given an environment and an ani-mat with needs and a sensory/motor system that satisfies these needs to some criterion, increase the difficulty of the environment or the complexity of the needs and find the minimum increase in animat complexity necessary to satisfy the needs to the same criterion." <ref> [Wilson, 1990] </ref> For example, Todd et al. [Todd et al., 1994] show that even the possibilities of sensor-less (!) memory-less creatures have not yet been fully explored. Their creatures find (by evolution) the optimum p (a), the probability of generating an action (independent of the state of the world).
Reference: [Wixson, 1991] <author> Wixson, Lambert E. </author> <year> (1991), </year> <title> Scaling reinforcement learning techniques via modularity, </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning. </booktitle> <pages> 195 </pages>
Reference-contexts: Each agent has its own set of actions Q i (x; a) and a set of actions Q i (x; k) where action k means "do whatever agent A k wants to do". Wixson <ref> [Wixson, 1991] </ref> seems to have invented an earlier, but more restricted, form of Nested Q-learning, where the called agent is called not just to take an action for this timestep, but to take actions repeatedly from now on until its goal state is reached.
References-found: 63

