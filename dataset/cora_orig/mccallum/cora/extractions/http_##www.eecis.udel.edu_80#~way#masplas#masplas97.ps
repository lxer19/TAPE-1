URL: http://www.eecis.udel.edu:80/~way/masplas/masplas97.ps
Refering-URL: http://www.eecis.udel.edu:80/~way/publications.html
Root-URL: http://www.cis.udel.edu
Email: fway,pollockg@cis.udel.edu  
Title: Towards Identifying and Monitoring Optimization Impacts  
Author: Thomas P. Way Lori L. Pollock 
Address: 19716  
Affiliation: High Performance Computing Software Laboratory Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract: This paper investigates the monitoring of these complex interactions, particularly as they apply to architectures with instruction-level parallelism (ILP). The paper examines the issues regarding how to determine what type of information adequately reflects the effect of individual optimizations on individual as well as the entire system of optimizations, and how to collect and quantify this information. A general framework for constructing an optimization interaction monitor and the design of a prototype tool based on this framework are described. Implications of using this monitoring tool in research directed towards the development of a tunable optimizing compiler for ILP architectures are examined. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <type> Technical Report CSD-93-781, </type> <institution> Computer Science Division, University of Cali-fornia, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: The static analyzer provides a wealth of information about the current state of the program. Common techniques are data flow analysis, value numbering, memory usage summarization, and alias analysis <ref> [1] </ref>. The optimization monitor coordinates the efforts of the other functional units.
Reference: [2] <author> Manuel E. Benitez and Jack W. Davidson. </author> <title> A portable global optimizer and linker. </title> <booktitle> In ACM SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 329-338, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To enable easy retargeting of an optimizer, flexibility in optimization ordering, and use of machine-specific information in higher-level optimizations, specific information about the target architecture in the form of a machine description is used throughout optimization <ref> [2] </ref>. <p> This is not necessarily an exhaustive list, but a good representative of how machine resources, program characteristics, and performance information are quantified and represented: * A machine architecture description, including features such as numbers of registers and instruction equivalences <ref> [2] </ref>. <p> Eventually, this tool will be extended to explore the use of this information in guiding optimization strategies for improved performance, particularly for ILP architectures. 5.1 Providing The Base The tool is being built as an extension of the Stanford University Intermediate Format (SUIF) [43] and the Very Portable Optimizer (vpo) <ref> [2] </ref> compilers. This combination provides a versatile research testbed for studying the interactions of both machine-independent and machine-dependent optimizations, as demonstrated by Table 1. SUIF is a flexible platform, written in C++, for research on compiler techniques for high-performance machines.
Reference: [3] <author> David A. Berson, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Resource spackling: A framework for integrating register allocation in local and global schedulers. </title> <booktitle> In Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 135-145, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [4] <author> Mickey R. Boyd and David B. Whalley. </author> <title> Graphical visualization of compiler optimizations. </title> <journal> Journal of Programming Languages, </journal> <volume> 3 </volume> <pages> 69-94, </pages> <year> 1995. </year>
Reference-contexts: The monitoring system is being designed with an X-Windows interface to provide an easy interface between the user and the functionality of the monitoring tool. The basic design of the interface window bears some resemblance to that of the xvpodb <ref> [4] </ref> interface, which uses a VCR analogy in its design. The technology that links the the user interface to the optimizers is currently based on the visualization tools that are known to work with SUIF and vpo. <p> The technology that links the the user interface to the optimizers is currently based on the visualization tools that are known to work with SUIF and vpo. The spp tool [43] generates a graphical representation of the SUIF control flow structure, and xvpodb <ref> [4] </ref> is a graphical debugging tool that interactively displays vpo optimization information. Our tool is being designed to allow the user to move forward and backward through each step of the optimization process, observing the resultant changes to the code.
Reference: [5] <author> David G. Bradlee, Susan J. Eggers, and Robert R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> In International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-131, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [6] <author> Thomas S. Brasier, Philip H. Sweany, and Steven J. Beaty. CRAIG: </author> <title> A practical framework for combining instruction scheduling and register assignment. </title> <booktitle> In Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 11-18, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [7] <author> Preston Briggs and Keith Cooper. </author> <title> Effective partial redundancy elimination. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise [31] and extended and modified by several others <ref> [7, 10, 13, 14, 26] </ref>. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths.
Reference: [8] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 53-65, </pages> <address> White Plains, New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation <ref> [8] </ref>. All have in common the goal of creating a good match between the program characteristics, such as instruction placement and register usage, and architectural features such as the availability of registers, memory access overhead, cache usage costs, and the parallelism of the target machine.
Reference: [9] <author> Steve Carr. </author> <title> Combining optimization for cache and instruction-level parallelism. In Parallel Architectures and Compilation Techniques (PACT), </title> <year> 1996. </year>
Reference-contexts: In particular, they discuss how the framework for describing optimizations can indicate whether combining the optimizations will be profitable. Other efforts include combining cache and ILP <ref> [9] </ref> and combining loop transformations [25]. There have been several efforts toward unifying transformations into a single mechanism, and applying search techniques to the transformation space. In particular, one framework for unifying loop transformations is based on unimodular matrix theory [41, 47].
Reference: [10] <author> Cliff Click. </author> <title> Global code motion global value numbering. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise [31] and extended and modified by several others <ref> [7, 10, 13, 14, 26] </ref>. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths. <p> PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths. Another example of combining optimizations is global value numbering (GVN) <ref> [10, 40] </ref>, which attempts to replace a set of instructions that each compute the same value with a single instruction, by combining constant folding and propagation, static single assignment, dead code elimination, loop invariant code motion, and common subexpression elimination. <p> In summary, the program characteristics that are used by these previous efforts for dealing with optimization interactions include: dependence analysis based on the Static Single-Assignment (SSA) form <ref> [10, 11, 12] </ref>, assigning costs to assignment statements to guide elimination of partial redundancies [26], and characteristics of loop structures [25] such as nesting level and dependences.
Reference: [11] <author> Cliff Click and Keith D. Cooper. </author> <title> Combining analyses, combining optimizations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 181-196, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Click and Cooper <ref> [11] </ref> present a framework for combining constant propagation, value numbering, and unreachable code elimination, and describe how to reason about the properties of the resulting framework. In particular, they discuss how the framework for describing optimizations can indicate whether combining the optimizations will be profitable. <p> In summary, the program characteristics that are used by these previous efforts for dealing with optimization interactions include: dependence analysis based on the Static Single-Assignment (SSA) form <ref> [10, 11, 12] </ref>, assigning costs to assignment statements to guide elimination of partial redundancies [26], and characteristics of loop structures [25] such as nesting level and dependences.
Reference: [12] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In ACM SIGPLAN Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <year> 1989. </year>
Reference-contexts: In summary, the program characteristics that are used by these previous efforts for dealing with optimization interactions include: dependence analysis based on the Static Single-Assignment (SSA) form <ref> [10, 11, 12] </ref>, assigning costs to assignment statements to guide elimination of partial redundancies [26], and characteristics of loop structures [25] such as nesting level and dependences.
Reference: [13] <author> D. M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <year> 1991. </year>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise [31] and extended and modified by several others <ref> [7, 10, 13, 14, 26] </ref>. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths.
Reference: [14] <author> K.-H. Drechsler and M. P. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <year> 1988. </year>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise [31] and extended and modified by several others <ref> [7, 10, 13, 14, 26] </ref>. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths.
Reference: [15] <author> Evelyn Duesterwald, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Demand-driven computation of interprocedu-ral data flow. </title> <booktitle> In ACM SIGPLAN Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 37-48, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This suggests a need for up 5 dating this information as optimizations are performed. This can be done by algorithms for the update that work as an incremental approach [35] or a demand-driven approach <ref> [15] </ref>. The granularity of information update can be at anywhere from the lowest level (instruction level) to the highest level (source level), and could be done after each application of an optimization for the most accurate reflection of the interactions of optimizations with resources and each other.
Reference: [16] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Stage scheduling: A technique to reduce the register requirements of a modulo schedule. </title> <booktitle> In International Symposium on Microarchitecture (MICRO), </booktitle> <month> Novem-ber </month> <year> 1995. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [17] <author> C. Eisenbeis and D. Windheiser. </author> <title> Optimal software pipelining in presence of resource constraints. In Parallel Architectures and Compilation Techniques (PACT), </title> <month> August </month> <year> 1993. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [18] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <year> 1981. </year>
Reference-contexts: This method is usually very successful, but requires a great deal of proficiency in algorithm design and a strong understanding of the features of the underlying architecture. Efforts to guide the optimization process from a higher-level include the use of profile-based optimizations <ref> [18, 29, 22] </ref>, and compile-time performance prediction [45]. These studies demonstrate how to use information about performance in optimization decision-making. They are concerned indirectly with machine and program characteristics insofar as they are reflected by the measured performance of a given program.
Reference: [19] <author> Stefan M. Freudenberger and John C. Ruttenberg. </author> <title> Phase ordering of register allocation and instruction scheduling. </title> <editor> In Robert Giegerich and Susan L. Graham, editors, </editor> <title> Code Generation Concepts, Tools, Techniques: </title> <booktitle> Proceedings of the International Workshop on Code Generation, </booktitle> <pages> pages 146-170, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [20] <author> James R. Goodman and Wei-Cheung Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 442-452, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [21] <author> R. Govindarajan, E. R. Altman, and G. R. Gao. </author> <title> Minimizing register requirements under resource-constrained rate-optimal software pipelining. In Parallel Architectures and Compilation Techniques (PACT), </title> <year> 1994. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [22] <author> Richard E. Hank, Wen mei W. Hwu, and B. Ramakr-ishna Rau. </author> <title> Region-based compilation: An introduction and motivation. </title> <booktitle> In International Symposium on Microarchitecture (MICRO), </booktitle> <year> 1995. </year>
Reference-contexts: This method is usually very successful, but requires a great deal of proficiency in algorithm design and a strong understanding of the features of the underlying architecture. Efforts to guide the optimization process from a higher-level include the use of profile-based optimizations <ref> [18, 29, 22] </ref>, and compile-time performance prediction [45]. These studies demonstrate how to use information about performance in optimization decision-making. They are concerned indirectly with machine and program characteristics insofar as they are reflected by the measured performance of a given program. <p> The sequencer provides a means for the user to specify the ordering of optimizations. The user can undo the effects of an optimization, and can also specify regions for exclusive optimization. Exclusive application of opti mizations enables experimentation into region-based optimization <ref> [22] </ref>. The controller allows the user to interact with the optimization process. In order to directly examine the effects of optimizations, and their impact on various resources and other optimizations, the ability to step forward and backward through the optimization process is needed.
Reference: [23] <author> R. A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [24] <author> Wayne Kelly and William Pugh. </author> <title> A unifying framework for iteration reordering transformations. </title> <booktitle> In Proceedings of the IEEE First International Conference on Algorithms and Architectures for Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Algorithms for handling different kinds of loop nests and loop transformations have been developed by others <ref> [24, 36, 37, 42] </ref>. Whitfield and Soffa [46] developed a framework that unifies the specification of classical and parallelizing optimizations with the goal of examining the interactions between transformations and aiding in the ordering of optimization phases.
Reference: [25] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: In particular, they discuss how the framework for describing optimizations can indicate whether combining the optimizations will be profitable. Other efforts include combining cache and ILP [9] and combining loop transformations <ref> [25] </ref>. There have been several efforts toward unifying transformations into a single mechanism, and applying search techniques to the transformation space. In particular, one framework for unifying loop transformations is based on unimodular matrix theory [41, 47]. <p> In summary, the program characteristics that are used by these previous efforts for dealing with optimization interactions include: dependence analysis based on the Static Single-Assignment (SSA) form [10, 11, 12], assigning costs to assignment statements to guide elimination of partial redundancies [26], and characteristics of loop structures <ref> [25] </ref> such as nesting level and dependences. The more unified approaches take a broader look, considering the use of an axiomatic scheme to represent and analyze characteristics [46] and a matrix-based theory of representation and analysis [41, 47].
Reference: [26] <author> Jens Knoop, Oliver Ruthing, and Bernhard Steffen. </author> <title> The power of assignment motion. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise [31] and extended and modified by several others <ref> [7, 10, 13, 14, 26] </ref>. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths. <p> In summary, the program characteristics that are used by these previous efforts for dealing with optimization interactions include: dependence analysis based on the Static Single-Assignment (SSA) form [10, 11, 12], assigning costs to assignment statements to guide elimination of partial redundancies <ref> [26] </ref>, and characteristics of loop structures [25] such as nesting level and dependences. The more unified approaches take a broader look, considering the use of an axiomatic scheme to represent and analyze characteristics [46] and a matrix-based theory of representation and analysis [41, 47].
Reference: [27] <author> J. Llosa, M. Valero, and E. Ayguade. </author> <title> Bidirectional scheduling to minimize register requirements. </title> <booktitle> In Fifth Workshop on Compilers for Parallel Computers, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [28] <author> Jack L. Lo and Susan J. Eggers. </author> <title> Improving balanced scheduling with compiler optimizations that increase instruction-level parallelism. </title> <booktitle> In ACM SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage <ref> [28] </ref>, and scalar replacement and register allocation [8].
Reference: [29] <author> S. A. Mahike, P. Change, and W. W. Hwu. </author> <title> Using profile information to assist classic code optimizations. </title> <journal> Software Practice and Experience, </journal> <year> 1991. </year>
Reference-contexts: This method is usually very successful, but requires a great deal of proficiency in algorithm design and a strong understanding of the features of the underlying architecture. Efforts to guide the optimization process from a higher-level include the use of profile-based optimizations <ref> [18, 29, 22] </ref>, and compile-time performance prediction [45]. These studies demonstrate how to use information about performance in optimization decision-making. They are concerned indirectly with machine and program characteristics insofar as they are reflected by the measured performance of a given program.
Reference: [30] <author> Soo-Mook Moon and Kemal Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> In International Symposium on Microarchitecture (MICRO), </booktitle> <year> 1992. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [31] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2), </volume> <month> February </month> <year> 1979. </year>
Reference-contexts: Partial redundancy elimination (PRE) is a global optimization introduced by Morel and Renvoise <ref> [31] </ref> and extended and modified by several others [7, 10, 13, 14, 26]. PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths.
Reference: [32] <author> Q. Ning and G. R. Gao. </author> <title> A novel framework of register allocation for software pipelining. </title> <booktitle> In ACM SIG-PLAN Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [33] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation sensitive region scheduling. In Parallel Architectures and Compilation Techniques (PACT), </title> <address> Limas-sol, Cyprus, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [34] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: A new approach. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Currently, information about the impact of optimizations on system resource utilization, and therefore on subsequent optimizations, is rarely gathered and exploited in the optimizer's strategy. There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, or software pipelining and register allocation [16, 17, 21, 23, 27, 32, 38, 44]. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation [16, 17, 21, 23, 27, 32, 38, 44], instruction scheduling and register allocation <ref> [3, 5, 6, 19, 20, 30, 33, 34] </ref>, instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [35] <author> Lori L. Pollock and Mary Lou Soffa. </author> <title> An incremental version of iterative data flow analysis. </title> <booktitle> In ACM SIG-PLAN Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 152-164, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: This suggests a need for up 5 dating this information as optimizations are performed. This can be done by algorithms for the update that work as an incremental approach <ref> [35] </ref> or a demand-driven approach [15].
Reference: [36] <author> William Pugh. </author> <title> Uniform techniques for loop optimizations. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Algorithms for handling different kinds of loop nests and loop transformations have been developed by others <ref> [24, 36, 37, 42] </ref>. Whitfield and Soffa [46] developed a framework that unifies the specification of classical and parallelizing optimizations with the goal of examining the interactions between transformations and aiding in the ordering of optimization phases.
Reference: [37] <author> J. Ramanujam. </author> <title> A linear algebraic view of loop transformations and their interaction. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1993. </year>
Reference-contexts: Algorithms for handling different kinds of loop nests and loop transformations have been developed by others <ref> [24, 36, 37, 42] </ref>. Whitfield and Soffa [46] developed a framework that unifies the specification of classical and parallelizing optimizations with the goal of examining the interactions between transformations and aiding in the ordering of optimization phases.
Reference: [38] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [39] <author> B. Ramakrishna Rau and Joseph A. Fisher. </author> <title> Instruction-level parallel processing: History, overview and perspective. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1) </volume> <pages> 9-50, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The importance of this conceptual decoupling of the program and machine has increased steadily along with the complexity of computer architectures, the size of program codes, and the desire for higher performance. Instruction-level parallelism (ILP) is a key architectural feature of high performance microprocessors <ref> [39] </ref>, and is the focus of much active research in compiler optimization.
Reference: [40] <author> Barry Rosen, Mark Wegman, and Kenneth Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In ACM SIGPLAN Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: PRE combines common subexpression elimination, loop invariant code motion, and code motion to move code from frequently used paths to less frequently used paths. Another example of combining optimizations is global value numbering (GVN) <ref> [10, 40] </ref>, which attempts to replace a set of instructions that each compute the same value with a single instruction, by combining constant folding and propagation, static single assignment, dead code elimination, loop invariant code motion, and common subexpression elimination.
Reference: [41] <author> Rafael H. Saavedra, Weihua Mao, Daeyeon Park, Jacqueline Chame, and Sungo Moon. </author> <title> The combined effectiveness of unimodular transformations, tiling, and software prefetching. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1996. </year>
Reference-contexts: Other efforts include combining cache and ILP [9] and combining loop transformations [25]. There have been several efforts toward unifying transformations into a single mechanism, and applying search techniques to the transformation space. In particular, one framework for unifying loop transformations is based on unimodular matrix theory <ref> [41, 47] </ref>. <p> The more unified approaches take a broader look, considering the use of an axiomatic scheme to represent and analyze characteristics [46] and a matrix-based theory of representation and analysis <ref> [41, 47] </ref>. Most of the program characteristics that are used are gathered through static program analysis typically performed by an optimizing compiler. 2.3 Performance Characteristics An execution profiler is a very common tool that is used in research and industry to assist in optimizing program code.
Reference: [42] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Algorithms for handling different kinds of loop nests and loop transformations have been developed by others <ref> [24, 36, 37, 42] </ref>. Whitfield and Soffa [46] developed a framework that unifies the specification of classical and parallelizing optimizations with the goal of examining the interactions between transformations and aiding in the ordering of optimization phases.
Reference: [43] <author> Stanford Compiler Group. </author> <title> The SUIF Parallelizing Compiler Guide. </title> <institution> Stanford University, </institution> <year> 1994. </year> <note> Version 1.0. </note>
Reference-contexts: Eventually, this tool will be extended to explore the use of this information in guiding optimization strategies for improved performance, particularly for ILP architectures. 5.1 Providing The Base The tool is being built as an extension of the Stanford University Intermediate Format (SUIF) <ref> [43] </ref> and the Very Portable Optimizer (vpo) [2] compilers. This combination provides a versatile research testbed for studying the interactions of both machine-independent and machine-dependent optimizations, as demonstrated by Table 1. SUIF is a flexible platform, written in C++, for research on compiler techniques for high-performance machines. <p> The technology that links the the user interface to the optimizers is currently based on the visualization tools that are known to work with SUIF and vpo. The spp tool <ref> [43] </ref> generates a graphical representation of the SUIF control flow structure, and xvpodb [4] is a graphical debugging tool that interactively displays vpo optimization information.
Reference: [44] <author> Jian Wang, Andrease Krall, and M. Anton Ertl. </author> <title> Decomposed software pipelining with reduced register requirement. </title> <booktitle> In Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 277-280, </pages> <year> 1995. </year>
Reference-contexts: There are isolated instances where this information is used to good effect, such as when combining instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], or software pipelining and register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>. While these techniques can improve program performance, they focus narrowly on the interaction of a single pair of optimizations, rather than more generally on the entire collection of optimizations to be applied to a program. <p> The most well known examples of this work focus on the interactions between software pipelining register allocation <ref> [16, 17, 21, 23, 27, 32, 38, 44] </ref>, instruction scheduling and register allocation [3, 5, 6, 19, 20, 30, 33, 34], instruction scheduling and cache usage [28], and scalar replacement and register allocation [8].
Reference: [45] <author> Ko-Yang Wang. </author> <title> Precise compile-time performance prediction for superscalar-based computers. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 73-84, </pages> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This method is usually very successful, but requires a great deal of proficiency in algorithm design and a strong understanding of the features of the underlying architecture. Efforts to guide the optimization process from a higher-level include the use of profile-based optimizations [18, 29, 22], and compile-time performance prediction <ref> [45] </ref>. These studies demonstrate how to use information about performance in optimization decision-making. They are concerned indirectly with machine and program characteristics insofar as they are reflected by the measured performance of a given program.
Reference: [46] <author> Debbie Whitfield and Mary Lou Soffa. </author> <title> An approach to ordering optimizing transformations. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 137-146, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Algorithms for handling different kinds of loop nests and loop transformations have been developed by others [24, 36, 37, 42]. Whitfield and Soffa <ref> [46] </ref> developed a framework that unifies the specification of classical and parallelizing optimizations with the goal of examining the interactions between transformations and aiding in the ordering of optimization phases. <p> The more unified approaches take a broader look, considering the use of an axiomatic scheme to represent and analyze characteristics <ref> [46] </ref> and a matrix-based theory of representation and analysis [41, 47].
Reference: [47] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> October </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Other efforts include combining cache and ILP [9] and combining loop transformations [25]. There have been several efforts toward unifying transformations into a single mechanism, and applying search techniques to the transformation space. In particular, one framework for unifying loop transformations is based on unimodular matrix theory <ref> [41, 47] </ref>. <p> The more unified approaches take a broader look, considering the use of an axiomatic scheme to represent and analyze characteristics [46] and a matrix-based theory of representation and analysis <ref> [41, 47] </ref>. Most of the program characteristics that are used are gathered through static program analysis typically performed by an optimizing compiler. 2.3 Performance Characteristics An execution profiler is a very common tool that is used in research and industry to assist in optimizing program code.
References-found: 47

