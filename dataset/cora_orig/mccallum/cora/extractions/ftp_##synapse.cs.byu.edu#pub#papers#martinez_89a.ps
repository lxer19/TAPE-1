URL: ftp://synapse.cs.byu.edu/pub/papers/martinez_89a.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: On the Pseudo Multilayer Learning of Backpropagation  
Author: Tony Martinez and Michael Lindsey 
Address: Provo, Utah 84602  
Affiliation: Computer Science Dept., Brigham Young University,  
Note: In Proceedings of the IEEE Symposium on Parallel and Distributed Computing, pp. 308-315, 1989.  
Abstract: Rosenblatt's convergence theorem for the simple perceptron initiated much excitement about iterative weight modifying neural networks. However, this convergence only holds for the class of linearly separable functions, which is vanishingly small compared to arbitrary functions. With multilayer networks of nonlinear units it is possible, though not guaranteed, to solve arbitrary functions. Backpropagation is a method of training multilayer networks to converge to the solution of arbitrary functions. This paper describes how classification takes place insingle and multilayer networks using threshold or sigmoid nodes. It then shows that the current backpropagation method can only do effective learning on one layer of a network at a time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> 1 . Lippmann, R. P., </author> <title> An Introduction to Computing with Neural Nets, </title> <journal> IEEE ASSP, </journal> <volume> Vol. 3, No.4, </volume> <pages> pp. 4-22, </pages> <year> (1987). </year> <editor> 2 . Martinez T. R., </editor> <booktitle> Models of Parallel Adaptive Logic, Proceedings of the 1987 IEEE Systems Man and Cybernetics Conference, </booktitle> <pages> pp. 290-296, </pages> <month> (October, </month> <year> 1987). </year> <note> 3 . Martinez, </note> <author> T. R. and J. J. Vidal, </author> <title> Adaptive Parallel Logic Networks, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, </volume> <pages> pp. 26-58, </pages> <year> (1988). </year> <title> 4 . Martinez, T.R., Digital Neural Networks, </title> <booktitle> Proceedings of the 1988 IEEE Systems Man and Cybernetics Conference, </booktitle> <month> (August, </month> <year> 1988). </year>
Reference-contexts: For a general classifier P is equal to 2 n . The ratio of linearly separable (LS) functions to arbitrary functions quickly approaches 0 as n increases. Although the single layer perceptron is limited to learning LS functions, it is well known <ref> [1] </ref> that a multilayer network of threshold units (where two layers is sufficient for binary classification and three for analog, though not necessarily efficient) can solve any arbitrary function.
Reference: 5. <author> Nilsson, N., </author> <title> Learning Machines, </title> <publisher> McGraw-Hill, </publisher> <year> (1965). </year>
Reference-contexts: However, the number of linearly separable (LS) functions having n inputs grows only as LS (P,n) = 2 n (P-i)! i f P &gt; n (3) where P is the number of patterns to be classified <ref> [5] </ref>. For a general classifier P is equal to 2 n . The ratio of linearly separable (LS) functions to arbitrary functions quickly approaches 0 as n increases.
Reference: 6. <author> Rosenblatt, F. </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan Books, </publisher> <address> Washington, D.C., </address> <year> (1962). </year> <title> 7 . Rumelhart, </title> <editor> D. and McClelland, J., </editor> <booktitle> Parallel Distributed Processing:, </booktitle> <volume> Vol. I, </volume> <pages> pp. 318-362, </pages> <publisher> MIT Press, </publisher> <year> (1986). </year>
Reference-contexts: Introduction Early work on iterative gradient descent algorithms by Rosenblatt <ref> [6] </ref> showed that a network of nonlinear threshold units with a single layer of weights could always converge to classify any linearly separable problem. This proof is encouraging, but the ratio of linearly separable functions to total functions becomes vanishingly small as the size of the input space increases. <p> This paper discusses how multilayer weight multiplying networks in general, and backpropagation networks in specific, do classification. It is shown that learning in a backpropagation (BP) network only effectively takes place on one layer at a time, rather than simultaneously on all layers. Linear Classification In a simple perceptron <ref> [6] </ref> learning model, each node is a binary threshold unit, such that the output of a node is O = 1 i f x i w i q where x i are real valued inputs, w i are real valued weights, and q is a threshold value. <p> Assuming that the goal function can be separated in a linear fashion, this learning model is guaranteed to converge to a proper solution in finite time <ref> [6] </ref>. The total number of possible functions which a network of n inputs can be programmed to accomplish is 2 2 n .
References-found: 3

