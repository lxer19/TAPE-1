URL: http://www.cs.gatech.edu/people/home/rosu/papers-uncompressed/jpdc-jan97.ps
Refering-URL: http://www.cs.gatech.edu/people/home/rosu/cv/cv.html
Root-URL: 
Email: bruck@systems.caltech.edu dolev@cs.huji.ac.il  fho,strongg@almaden.ibm.com rosu@cc.gatech.edu  
Phone: 116-81  
Title: Efficient Message Passing Interface (MPI) for Parallel Computing on Clusters of Workstations  
Author: Jehoshua Bruck Danny Dolev Ching-Tien Ho Marcel-Catalin Ro~su Ray Strong 
Address: Mail Code  Pasadena, CA 91125 Jerusalem, Israel  650 Harry Road College  San Jose, CA 95120 Atlanta, GA 30332-0280  
Affiliation: California Institute of Technology Institute of CS  Hebrew University  IBM Almaden Research Center Georgia Institute of Technology  of Computing  
Abstract: Parallel computing on clusters of workstations and personal computers has very high potential, since it leverages existing hardware and software. Parallel programming environments offer the user a convenient way to express parallel computation and communication. In fact, recently, a Message Passing Interface (MPI) has been proposed as an industrial standard for writing "portable" message-passing parallel programs. The communication part of MPI consists of the usual point-to-point communication as well as collective communication. However, existing implementations of programming environments for clusters are built on top of a point-to-point communication layer (send and receive) over local area networks (LANs) and, as a result, suffer from poor performance in the collective communication part. In this paper, we present an efficient design and implementation of the collective communication part in MPI that is optimized for clusters of workstations. Our system consists of two main components: the MPI-CCL layer that includes the collective communication functionality of MPI and a User-level Reliable Transport Protocol (URTP) that interfaces with the LAN Data-link layer and leverages the fact that the LAN is a broadcast medium. Our system is integrated with the operating system via an efficient kernel extension mechanism that we developed. The kernel extension significantly improves the performance of our implementation as it can handle part of the communication overhead without involving user space. We have implemented our system on a collection of IBM RS/6000 workstations connected via a 10Mbit Ethernet LAN. Our performance measurements are taken from typical scientific programs that run in a parallel mode by means of the MPI. The hypothesis behind our design is that system's performance will be bounded by interactions between the kernel and user space rather than by the bandwidth delivered by the LAN Data-Link Layer. Our results indicate that the performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as a recently published software implementation of broadcast on top of ATM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Amir, D. Dolev, S. Kramer and D. Malki, "Transis: </author> <title> A communication subs-system for high availability," </title> <booktitle> Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, IEEE, </booktitle> <pages> pp. 76-84, </pages> <year> 1992. </year>
Reference-contexts: For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as <ref> [4, 23, 1] </ref>, that provide a reliable transport protocol and other services for distributed computing. Our URTP protocol distinguishes itself from previous ones because it is targeted for supporting parallel computing using MPI programs and can take advantage of the global program semantics derived from our MPI-CCL implementation.
Reference: [2] <author> V. Bala, J. Bruck, R. Bryant, R. Cypher, P. de Jong, P. Elustondo, D. Frye, A. Ho, C.T. Ho, G. Irwin, S. Kipnis, R. Lawrence, and M. Snir, </author> <title> "The IBM External User Interface for Scalable Parallel Systems", </title> <journal> Parallel Computing, </journal> <volume> Vol. 20, No. 4, </volume> <pages> pp. 445-462, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 20, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [3] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.T. Ho, S. Kipnis, and M. Snir, </author> <title> "CCL: A portable and tunable collective communication library for scalable parallel computers", </title> <booktitle> International Parallel Processing Symposium, </booktitle> <pages> pp. 835-844, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: One of the key components of MPI is the collective communication subset that allows users to conveniently call library routines for various "global" communication operations, like broadcast, scatter and gather. All MPI collective communication routines are implicitly defined with respect to a process group <ref> [3] </ref> which specifies an ordered set of processors within which the collective communication will be performed. For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. <p> Briefly, they are: 1. Build URTP on top of the lowest available layer (LAN Data-Link Layer in our case). 2. Take advantage of the multicast/broadcast capabilities offered by the lower layers. 1 That is, the wildcard source is not allowed. See <ref> [3] </ref> for details. 5 3. Move the packets from kernel buffers into user level buffers and free kernel buffers as soon as possible. This minimizes the chances that packets are dropped due to the lack of free kernel buffers. There are two main differences between kernel and user buffers.
Reference: [4] <author> K. Birman, R. Cooper, T. A. Joseph, K. Marzullo, M. Makpangou, K. Kane, F. Schmuck and M. Wood, </author> <title> The ISIS System Manual, </title> <institution> Dept. of Computer Science, Cornell University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as <ref> [4, 23, 1] </ref>, that provide a reliable transport protocol and other services for distributed computing. Our URTP protocol distinguishes itself from previous ones because it is targeted for supporting parallel computing using MPI programs and can take advantage of the global program semantics derived from our MPI-CCL implementation.
Reference: [5] <author> G.M. Brown, M.G. Gouda, and R.E. Miller "Block Acknowledgement: </author> <title> Redesigning the Window Protocol", </title> <booktitle> In Proc. ACM SIGCOMM'89, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: A packet can be acknowledged as soon as it reaches a URTP buffer. If the number of received and unacknowledged packets reaches a threshold then an ACK packet will be sent to the sender (as in <ref> [5] </ref>). As already evident, the receive call has a rather unusual semantics: data is not returned in a buffer supplied by the MPI-CCL layer. A pointer to a buffer in the receiving buffer pool is returned instead.
Reference: [6] <author> J. Bruck, D. Dolev, C.T. Ho, R. Orni, and R. Strong, "PCODE: </author> <title> An Efficient and Reliable Collective Communication Protocol for Unreliable Broadcast Domains", </title> <journal> IBM Research Report, </journal> <volume> RJ 9895, </volume> <month> September, </month> <year> 1994. </year>
Reference-contexts: receive takes as argument one explicit source processor. 1 In the most strict definition, we say a global program is correct if, for each instruction counter, there is at most one multicast and exactly the processors in the target set issue receive with the source matching the multicast source. (See <ref> [6] </ref> for a more formal and detailed definition.) Note that the skip call, the instruction counter and the restriction to one multicast call per instruction call are all introduced for the purpose of specification. In particular, each processor will execute its program in an asynchronous and greedy manner. <p> The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. As another comparison, our broadcast of a 1 Kbyte message on 8 processors takes about 3.87 msecs as compared to 8.9 msecs based on TCP/IP measured in <ref> [6] </ref>. 4.3 MPI Allgather Let q = dM=me be the number of packets, per processor, that need to be multicast. There are two possible algorithms for MPI Allgather described as follows. <p> Note that on 8 workstations, MPI Allgather runs about 6.1 msecs and 12.7 msecs, for 32 bytes and 1K byte messages, respectively. As a comparison, a hand-coded all-to-all broadcast based on the PCODE protocol in <ref> [6] </ref> runs about 9.0 msecs and 16.7 msecs for 20 bytes and 1 Kbyte messages, respectively, on faster (100 MHz clock) workstations. Figure 7 shows the times of MPI Allgather of a 64 Kbyte message as a function of the number of workstations using slow subset. earlier) on 4 workstations.
Reference: [7] <author> J.B. Carter and W. Zwaenepoel, </author> <title> "Optimistic Implementation of Bulk Data Transfer Protocols" In Performance Evaluation Review, </title> <journal> Vol. </journal> <volume> 17, No. 1, </volume> <month> May </month> <year> 1989, </year> <pages> Pages 61-69. </pages>
Reference-contexts: buffer and second from the kernel buffer to the URTP buffer. (The assembled message will be further copied from URTP buffers into the user's buffer in the MPI program by the MPI-CCL layer.) A protocol using an average of little more than one data copy per message is described in <ref> [7] </ref> but it works only for large data transfers. URTP is intended to be used for both small and large data transfers. In addition, URTP has the following mechanisms: 1. A REQ packet is a point-to-point communication requesting a specific packet from a source.
Reference: [8] <author> D.R. Cheriton and W. Zwaenepoel, </author> <title> "Distributed Process Groups in the V Kernel", </title> <journal> In ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 2, </volume> <month> May </month> <year> 1985, </year> <pages> Pages 77-107. </pages>
Reference: [9] <author> D. Culler et al., </author> <title> the Active Message Project by U.C. Berkeley. </title> <note> See papers and related information from http://now.cs.berkeley.edu/AM/active messages.html. </note>
Reference-contexts: Note also that there have been various works in improving communication latency and bandwidth by modifying message passing protocols to facilitate efficient system implementation. Examples are the Active Message project by Culler et al. <ref> [9] </ref>, the Fast Message project by Chien et al. [19], and the Shrimp project by Li et al. [15]. In this paper, we address the same issue using a different orthorgonal approach. Thus, we do not claim that our approach should replace any work in this area.
Reference: [10] <author> M. J. Fischer, N. A. Lynch, and M. S. Paterson, </author> <title> "Impossibility of Distributed Consensus with One Faulty Process," </title> <note> JACM 32 (1985) 373-382. </note>
Reference-contexts: In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. <ref> [10] </ref>). Fortunately, we do not need to solve this problem because we are assured that all the useful work of the application has been performed if only one processor successfully returns from the MPI Barrier call.
Reference: [11] <author> H. Franke, P. Hochschild, P. Pattnaik, and M. Snir, </author> <title> MPI-F: An Efficient Implementation of MPI on IBM-SP1, </title> <booktitle> Proceedings of 1994 International Conference on Parallel Processing, </booktitle> <volume> Vol. III, </volume> <pages> pp. 197-201, </pages> <month> August, </month> <year> 1994. </year>
Reference-contexts: Note that in the MPI-CCL layer, we focus on the collective communication subset of MPI only. For MPI point-to-point communication routines, they can be easily mapped to reliable point-to-point communication provided by URTP or similar protocols (e.g. <ref> [11] </ref>). For clarity, here we define a few terms that are used throughout this paper. A message, which has no upper bound on size, is the unit of communication at the MPI applications layer.
Reference: [12] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, B. Manchek, and V. Sunderam, </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Network Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 20, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [13] <author> J. N. Gray, </author> <booktitle> "Notes on Database Operating Systems," Operating Systems: an advanced course, Lecture Notes in Computer Science 60, </booktitle> <publisher> Springer Verlag (1978) 393-481. </publisher>
Reference-contexts: This termination problem is a variant of the well-known Two Generals Problem, which is unsolvable in the presence of the possibility of message loss <ref> [13, 22] </ref>. In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. [10]).
Reference: [14] <author> C. Huang, E. P. Kasten, and P. K. McKinley, </author> <title> "Design and Implementation of Multicast Operations for ATM-Based High Performance Computing", </title> <booktitle> Proceedings of Supercomputing 94 conference, </booktitle> <pages> pp. 164-173, </pages> <address> Washington D.C., </address> <month> November </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Our results indicate that the performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. <p> (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. We note there have been many existing systems, such as [4, 23, 1], that provide a reliable transport protocol and other services for distributed computing. <p> From the figure, broadcasting one packet takes about 1 to 4 msecs. The performance of our MPI Broadcast (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. <p> (on top of Ethernet) is about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment. <p> about twice as fast as the software implementation of broadcast on top of ATM that is presented in <ref> [14] </ref>. For example, a broadcast of a 4Kbyte message on 8 machines takes about 6 msecs in our implementation compared to 15 msecs in the implementation in [14]. The hardware implementation of [14] is faster than ours as would be expected from the higher bandwidth of the ATM equipment.
Reference: [15] <editor> K. Li et al., </editor> <address> Princeton University, </address> <note> the Shrimp Project. See http://www.cs.princeton.edu/shrimp. </note>
Reference-contexts: Examples are the Active Message project by Culler et al. [9], the Fast Message project by Chien et al. [19], and the Shrimp project by Li et al. <ref> [15] </ref>. In this paper, we address the same issue using a different orthorgonal approach. Thus, we do not claim that our approach should replace any work in this area. Rather, our approach can be integrated with many proposed system designs in improving message passing protocols on networks of workstations.
Reference: [16] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Recently, a Message Passing Interface (MPI) <ref> [16] </ref> has been proposed as an industrial standard for writing "portable" message-passing parallel programs. The MPI standardization effort involved about 60 people from 40 organizations including universities, national laboratories, and most MPP vendors. Version 1 of MPI was released in May 1994. <p> The timing is very flat for up to 8 processors. 4.6 MPI Initialization and Termination According to MPI specification, MPI Init () must be called before any MPI routines and MPI Finalize () must be called after any MPI routines. Also, a system-defined communicator (see <ref> [16] </ref> for details) called MPI COMM WORLD is defined after MPI Init call. From MPI COMM WORLD, one can derive the processor group of the "MPI world", denoted MPI GROUP WORLD in the paper.
Reference: [17] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta, </author> <title> "The Packet Filter: An Efficient Mechanism for User-level Network Code", </title> <booktitle> In Proceedings of the 11th Symposium on Operating System Principles, ACM SIGOPS, </booktitle> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: As a result URTP is implemented as a combination of a kernel extension and a user-level library. Most of the protocol code is in the user-level library. This decision made the implementation easier without a significant performance degradation (see <ref> [17] </ref>). The kernel extension part enables fast processing (dropping) of multicast packets at processors that are not part of the target set 2 and also reduces the inter-processor communication overhead between processes. 3.1 Protocol Description The issues and requirements that we consider while designing the protocol are as follows: 1.
Reference: [18] <author> R. B. Morris, Y. Tsuji, and P. Carnevali, </author> <title> "Adaptive Solution Strategy for Solving Large Systems of p-type Finite Element Equations", </title> <journal> Int. J. Numer. Methods Eng., </journal> <volume> Vol 33, </volume> <pages> 2059-2071, </pages> <year> 1992. </year>
Reference-contexts: We have ported two sequential programs into parallel programs written in MPI. The first one is PolyFEM, a simulation and modeling program that uses p-type finite-element-method algorithms for elasticity modeling <ref> [18] </ref>. We have isolated and parallelized the portion of the PolyFEM solver that takes most of the running time on large problems. This subapplication involves iteratively multiplying a vector by a large sparse matrix.
Reference: [19] <author> S. Pakin, M. Lauria and A. Chien, </author> <title> "High Performance Messaging on Workstations: Illi-nois Fast Messages (FM) for Myrinet", </title> <booktitle> in Supercomputing '95, </booktitle> <address> San Diego, CA. </address> <note> See also http://www-csag.cs.uiuc.edu/projects/comm/fm.html for related papers. </note>
Reference-contexts: Note also that there have been various works in improving communication latency and bandwidth by modifying message passing protocols to facilitate efficient system implementation. Examples are the Active Message project by Culler et al. [9], the Fast Message project by Chien et al. <ref> [19] </ref>, and the Shrimp project by Li et al. [15]. In this paper, we address the same issue using a different orthorgonal approach. Thus, we do not claim that our approach should replace any work in this area.
Reference: [20] <author> Parasoft Corporation, </author> <title> Express version 1.0: A communication environment for parallel computers, </title> <year> 1988. </year>
Reference-contexts: For example, a multicast is specified as a broadcast to a particular process group. The performance of a parallel program depends on an efficient implementation of point-to-point as well as collective communication. In existing parallel programming environments, such as PVM, EXPRESS and IBM's MPL <ref> [12, 20, 2] </ref>, for Local Area Networks (LANs), collective communication routines are implemented on top of point-to-point communication. As a result, these environments suffer from poor collective communication performance.
Reference: [21] <author> D. Patterson et al., </author> <title> "A Case for Networks of Workstations (NOW)", </title> <booktitle> Symposium Record of Hot Interconnects II, </booktitle> <pages> pp. 24-39, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: In particular, we demonstrate the implementation on a traditional 10Mbit Ethernet-based LAN. We note here that the ideas presented in this paper can be easily extended to any Network of Workstations (NOW) <ref> [21] </ref> that provides an unreliable broadcast transport protocol (such as to an ATM network where the ATM switches have broadcast capability as provided by many vendors nowadays).
Reference: [22] <author> R. Strong, D. Dolev, and F. Cristian, </author> <title> "A Unified Theory of Distributed Coordination with Communication Uncertainty," IBM Research Report RJ7727, </title> <booktitle> 1990, in the Proceedings of the 28th Allerton Conference, Allerton, </booktitle> <month> September, </month> <year> 1990. </year>
Reference-contexts: This termination problem is a variant of the well-known Two Generals Problem, which is unsolvable in the presence of the possibility of message loss <ref> [13, 22] </ref>. In fact the problem is harder than the consensus problem because it cannot be solved, even when it is guaranteed that no process will fail (cf. [10]).

References-found: 22

