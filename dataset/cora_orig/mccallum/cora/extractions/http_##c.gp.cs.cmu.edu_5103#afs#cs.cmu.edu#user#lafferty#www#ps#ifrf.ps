URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lafferty/www/ps/ifrf.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lafferty/www/publications.html
Root-URL: http://www.cs.cmu.edu
Title: Inducing Features of Random Fields  
Author: Stephen Della Pietra, Vincent Della Pietra, and John Lafferty, Member, IEEE 
Keyword: Random field, Kullback-Leibler divergence, iterative scaling, maximum entropy, EM algorithm, statistical learning, clustering, word morphology, natural language processing.  
Date: 4, APRIL 1997 1  
Note: IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 19, NO.  
Abstract: We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Almeida and B. Gidas, </author> <title> A variational method for estimating the parameters of MRF from complete or incomplete data, </title> <journal> The Annals of Applied Probability, </journal> <volume> 3, No. 1, 103136, </volume> <year> 1993. </year>
Reference: [2] <author> N. Balram and J. Moura, </author> <title> Noncausal Gauss Markov random fields: Parameter structure and estimation, </title> <journal> IEEE Transactions on Information Theory 39, </journal> <volume> No. 4, 13331343, </volume> <month> July, </month> <year> 1993. </year>
Reference: [3] <author> A. Berger, V. Della Pietra, and S. Della Pietra, </author> <title> A maximum entropy approach to natural language processing, </title> <journal> Computational Linguistics, </journal> <volume> 22, No. 1, 3971, </volume> <year> 1996. </year>
Reference-contexts: For general conditional distributions p (y j x) there may be no underlying random field, but with features defined as binary functions f (x; y), the same general approach is applicable. The feature induction method for conditional exponential models is demonstrated for several problems in statistical machine translation in <ref> [3] </ref>, where it is presented in terms of the principle of maximum entropy. B. Decision trees Our feature induction paradigm also bears some resemblence to various methods for growing classification and regression trees. Like decision trees, our method builds a top-down classification that refines features.
Reference: [4] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference: [5] <author> D. Brown, </author> <title> A note on approximations to discrete probability distributions, </title> <booktitle> Information and Control 2, </booktitle> <month> 386392 </month> <year> (1959). </year>
Reference: [6] <author> P. Brown, V. Della Pietra, P. de Souza, J. Lai, and R. Mercer, </author> <title> Class-based n-gram models of natural language, </title> <journal> Computational Linguistics 18, </journal> <volume> No. 4, 467479, </volume> <year> 1992. </year>
Reference: [7] <author> P. F. Brown, J. Cocke, V. Della-Pietra, S. Della-Pietra, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. </author> <title> A statistical approach to machine translation, </title> <booktitle> Computational Linguistics, </booktitle> <address> 16(2):7985, </address> <year> 1990. </year>
Reference: [8] <author> B. Chalmond, </author> <title> An iterative Gibbsian technique for reconstruction of m-ary images, </title> <journal> Pattern Recognition, </journal> <volume> 22 No. 6, 747761, </volume> <year> 1989. </year>
Reference: [9] <author> I. Csiszar, </author> <title> I-Divergence geometry of probability distributions and minimization problems, </title> <journal> The Annals of Probability, </journal> <volume> 3, No. 1, 146158, </volume> <year> 1975. </year>
Reference: [10] <author> I. Csiszar, </author> <title> A geometric interpretation of Darroch and Ratcliff's generalized iterative scaling, </title> <journal> The Annals of Statistics, </journal> <volume> 17, No. 3, 14091413, </volume> <year> 1989. </year>
Reference: [11] <author> I. Csiszar and G. Tusnady, </author> <title> Information geometry and alternating minimization procedures,Statistics & Decisions, </title> <journal> Supplement Issue, </journal> <volume> 1, 205237, </volume> <year> 1984. </year>
Reference: [12] <author> J. Darroch and D. Ratcliff, </author> <title> Generalized iterative scaling for log-linear models, </title> <journal> Ann. Math. Statist. </journal> <volume> 43, 14701480, </volume> <year> 1972. </year>
Reference: [13] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin, </author> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> Journal of the Royal Statistical Society 39, </journal> <volume> No. B, 138, </volume> <year> 1977. </year>
Reference: [14] <author> P. Diaconis and D. Ylvisaker, </author> <title> Conjugate priors for exponential families, </title> <journal> Ann. Statist. </journal> <volume> 7, 269281, </volume> <year> 1979. </year>
Reference-contexts: This could enable a principled approach for deciding when the feature induction is complete. While there is a natural class of conjugate priors for the class of exponential models that we use <ref> [14] </ref>, the problem of incorporating prior knowledge about the set of candiate features is more challenging. APPENDIX I. DUALITY In this Appendix we prove Proposition 4 restated here. Proposition 4: Suppose that p t q 0 .
Reference: [15] <author> P. Ferrari, A. Frigessi and R. Schonmann, </author> <title> Convergence of some partially parallel Gibbs samplers with annealing, </title> <journal> The Annals of Applied Probability, </journal> <volume> 3 No. 1, 137152, </volume> <year> 1993. </year>
Reference: [16] <author> A. Frigessi, C. Hwang, and L. Younes, </author> <title> Optimal spectral structure of reversible stochastic matrices, Monte Carlo methods and the simulation of Markov random fields, </title> <journal> The Annals of Applied Probability, </journal> <volume> 2, No. 3, 610628, </volume> <year> 1992. </year>
Reference: [17] <author> S. Geman and D. Geman, </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell. </journal> <volume> 6, 721741, </volume> <year> 1984. </year>
Reference: [18] <author> C. Geyer and E. Thomson, </author> <title> Constrained Monte Carlo maximum likelihood for dependent data (with discussion), </title> <journal> J. Royal Stat. Soc. B-54, </journal> <volume> 657699, </volume> <year> 1992. </year>
Reference: [19] <author> E. T. Jaynes, </author> <title> Papers on Probability, Statistics, and Statistical Physics, </title> <editor> R. Rosenkrantz, ed., D. </editor> <publisher> Reidel Publishing Co., </publisher> <address> DordrechtHolland, </address> <year> 1983. </year>
Reference: [20] <author> J. Lafferty and R. Mercer, </author> <title> Automatic word classification using features of spellings, </title> <booktitle> Proceedings of the 9th Annual Conference of the University of Waterloo Centre for the New OED and Text Research, </booktitle> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1993. </year>
Reference: [21] <author> G. Potamianos and J. Goutsias, </author> <title> Partition function estimation of Gibbs random field images using Monte Carlo simulations, </title> <journal> IEEE Transactions on Information Theory 39, </journal> <volume> No. 4, 13221331, </volume> <month> July, </month> <year> 1993. </year>

References-found: 21

