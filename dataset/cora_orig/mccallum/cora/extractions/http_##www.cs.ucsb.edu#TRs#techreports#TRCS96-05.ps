URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-05.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Pharos: A Scalable Distributed Architecture for Locating Heterogeneous Information Sources Version 1.0.2  
Author: R. Dolin D. Agrawal L. Dillon A. El Abbadi 
Date: July 11, 1996  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: This paper presents the design of Pharos: a scalable distributed architecture for locating heterogeneous information sources. The system incorporates a hierarchical metadata structure into a multi-level retrieval system. Queries are resolved through an iterative decision-making process. The first step retrieves coarse-grain metadata, about all sources, stored on local, massively replicated, high-level servers. Further steps retrieve more detailed metadata, about a greatly reduced set of sources, stored on remote, sparsely replicated, topic-based mid-level servers. We describe the structure, distribution, and retrieval of the metadata in Pharos to enable users to locate desirable information sources over the Internet. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AltaVista, </author> <note> http://altavista.digital.com/, 1995. </note>
Reference-contexts: The reason for such a change in focus, as will hopefully be made clear later, is that we believe that locating collections by domains is a much more scalable methodology than locating particular authors, titles, etc. Current WWW indexes, such as AltaVista <ref> [1] </ref>, Lycos [23], and Yahoo [33], which are designed for locating information on the Internet, are limited in several ways. Notably, these systems do not scale to handle increasingly large numbers of search requests due to limited network bandwidth and server power.
Reference: [2] <author> Andresen, D., et al., </author> <title> "The WWW Prototype of the Alexandria Digital Library," </title> <booktitle> Proceedings of the International Symposium on Digital Libraries, </booktitle> <address> Tsukuba, Japan, </address> <year> 1995. </year>
Reference-contexts: Emerging DL systems must deal with storing, locating, cataloging, distributing, and retrieving large numbers of diverse documents in a distributed, heterogeneous environment. Several projects in the NSF/NASA/ARPA Digital Library Initiative Program [6, 24] are therefore investigating these problems. The Alexandria Digital Library (ADL) Project <ref> [2] </ref> is focusing on indexing spatial information, as well as on the storage, distribution, and retrieval of large (spatial) images. The content and form of the metadata required to classify geographically referenced data is a central research issue. <p> source: Alexandria Digital Library Src Type String Type of source: DLIB Src Loc String Geographical location of source (if relevant): Santa Barbara, California Src Sch String Availability schedule of source (if relevant): Closed Mondays Siz Col Real [3] 0.0+ Collection size in MB, pages, and items: 1.843E6,,8.91E5 Siz Cat Real <ref> [2] </ref> 0.0+ Catalog size in MB and items: 2.172E6,3.892E6 Count Tax Integer 0+ Number of taxonomies used: 3 Spec String [M] List of M Special Collections: Geography:BE16 Net Band Real 0+ Network bandwidth (Kbps): 1.5E3 Net Avg Util Real 0.0 - 1.0 Avg. network utilization: 0.47 Net Avg Delays Integer [N]
Reference: [3] <author> Berry, M.W. & Dumais, S., </author> <title> "Using Linear Algebra for Intelligent Information Retrieval," </title> <note> SIAM Review, December 1995; and http://www.cs.utk.edu/ library/TechReports/1994/ut-cs-94-270.ps.Z. </note>
Reference-contexts: SMART and gGlOSS are only two of several automatic text retrieval systems. Other notable systems include ConQuest [10], which uses a static semantic network based on dictionary definitions, and Latent Semantic Indexing (LSI) <ref> [3] </ref>, which uses a dynamic semantic network based on word co-occurrences within documents in a collection. Content Routing [28] is a system in which queries get routed to the available servers based on the expected relevance of the 2 server to the query. <p> Ver String - Metadata descriptor version number: 0.0.7 Src ID String Name of source: Alexandria Digital Library Src Type String Type of source: DLIB Src Loc String Geographical location of source (if relevant): Santa Barbara, California Src Sch String Availability schedule of source (if relevant): Closed Mondays Siz Col Real <ref> [3] </ref> 0.0+ Collection size in MB, pages, and items: 1.843E6,,8.91E5 Siz Cat Real [2] 0.0+ Catalog size in MB and items: 2.172E6,3.892E6 Count Tax Integer 0+ Number of taxonomies used: 3 Spec String [M] List of M Special Collections: Geography:BE16 Net Band Real 0+ Network bandwidth (Kbps): 1.5E3 Net Avg Util <p> This taxonomy 13 Attribute Name Type Range Description Tax ID String Name of taxonomy Tax Desc Ver String Taxonomy descriptor version number Cov Root Real <ref> [3] </ref> 0.0 - 1.0 Root node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of depth-1 nodes Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of depth-1 nodes Cov Min Real [3] 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max <p> This taxonomy 13 Attribute Name Type Range Description Tax ID String Name of taxonomy Tax Desc Ver String Taxonomy descriptor version number Cov Root Real <ref> [3] </ref> 0.0 - 1.0 Root node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of depth-1 nodes Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of depth-1 nodes Cov Min Real [3] 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max Real [3] 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata <p> taxonomy 13 Attribute Name Type Range Description Tax ID String Name of taxonomy Tax Desc Ver String Taxonomy descriptor version number Cov Root Real <ref> [3] </ref> 0.0 - 1.0 Root node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of depth-1 nodes Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of depth-1 nodes Cov Min Real [3] 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max Real [3] 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID <p> Desc Ver String Taxonomy descriptor version number Cov Root Real <ref> [3] </ref> 0.0 - 1.0 Root node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of depth-1 nodes Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of depth-1 nodes Cov Min Real [3] 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max Real [3] 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID String Node's label Cov Node Real [3] 0.0 - 1.0 Node's coverage value (MB, <p> - 1.0 Root node's coverage value (MB, pages, items) Cov Avg Real <ref> [3] </ref> 0.0 - 1.0 Avg. coverage of depth-1 nodes Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of depth-1 nodes Cov Min Real [3] 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max Real [3] 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID String Node's label Cov Node Real [3] 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of <p> coverage of depth-1 nodes Cov Min Real <ref> [3] </ref> 0.0 - 1.0 Minimum coverage of depth-1 nodes Cov Max Real [3] 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID String Node's label Cov Node Real [3] 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of children Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of children Cov Min Real [3] 0.0 - 1.0 Minimum coverage of children Cov Max Real [3] 0.0 - <p> of depth-1 nodes Cov Max Real <ref> [3] </ref> 0.0 - 1.0 Maximum coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID String Node's label Cov Node Real [3] 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of children Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of children Cov Min Real [3] 0.0 - 1.0 Minimum coverage of children Cov Max Real [3] 0.0 - 1.0 Maximum coverage of children Table 3: Metadata for Each High-Level Node in <p> coverage of depth-1 nodes Table 2: High-Level Metadata for Each Taxonomy T i Attribute Name Type Range Description Node ID String Node's label Cov Node Real <ref> [3] </ref> 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of children Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of children Cov Min Real [3] 0.0 - 1.0 Minimum coverage of children Cov Max Real [3] 0.0 - 1.0 Maximum coverage of children Table 3: Metadata for Each High-Level Node in T i 14 is a superset of that in Tables 2 <p> Attribute Name Type Range Description Node ID String Node's label Cov Node Real <ref> [3] </ref> 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of children Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of children Cov Min Real [3] 0.0 - 1.0 Minimum coverage of children Cov Max Real [3] 0.0 - 1.0 Maximum coverage of children Table 3: Metadata for Each High-Level Node in T i 14 is a superset of that in Tables 2 and 3. <p> Node Real <ref> [3] </ref> 0.0 - 1.0 Node's coverage value (MB, pages, items) Cov Avg Real [3] 0.0 - 1.0 Avg. coverage of children Cov SD Real [3] 0.0 - 1.0 Std. Dev. of coverage of children Cov Min Real [3] 0.0 - 1.0 Minimum coverage of children Cov Max Real [3] 0.0 - 1.0 Maximum coverage of children Table 3: Metadata for Each High-Level Node in T i 14 is a superset of that in Tables 2 and 3. <p> For each taxonomy included, the user selects one or more sub-domains either by entering keywords, or by traversing the tree directly and selecting particular sub-domains. The UI must deal with keyword entries differently for each taxonomy. For the subject hierarchy, techniques used in text retrieval, such as by LSI <ref> [3] </ref>, can aid in matching query terms with terms found in the hierarchy. The geography taxonomy requires a gazetteer, and the time-period taxonomy requires the equivalent of a gazetteer for time-names.
Reference: [4] <author> Bowman, </author> <title> C.M., </title> <editor> et al., </editor> <title> "Scalable Internet Resource Discovery: Research Problems and Approaches," </title> <journal> CACM, vol.37, (no.8):98-107, </journal> <volume> 114, </volume> <month> Aug. </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: WWW indexing systems that index document texts require their own indexing techniques and cannot use the metadata that a source extracts about itself. Problems of locating sources of information, or resource discovery <ref> [4] </ref>, are of particular interest to the DL community [14]. Emerging DL systems must deal with storing, locating, cataloging, distributing, and retrieving large numbers of diverse documents in a distributed, heterogeneous environment. Several projects in the NSF/NASA/ARPA Digital Library Initiative Program [6, 24] are therefore investigating these problems. <p> The above approaches do not provide solutions or information architectures that scale well in terms of the information gathering with the growing diversity of data, the information dispersion with the growing user base, and the information visualization with the growing data volume <ref> [4] </ref>. In this paper, we describe a design of a distributed architecture for locating desirable information sources entitled Pharos, which scales in all three aspects. We achieve this scalability by using a hierarchical metadata structure and a highly decentralized metadata distribution, storage and retrieval mechanism. <p> Such architectures must scale well in terms of information gathering with the increasing diversity of data, the dispersal of information among a growing user base, and the visualization of results with the growing data volume <ref> [4] </ref>. Pharos scales in all of these aspects. The use of a hierarchical metadata structure greatly enhances scalability. The Pharos distribution scheme also enhances scalability: each user begins with the metadata available at a local high-level server.
Reference: [5] <author> Bowman, </author> <title> C.M., </title> <editor> et al., </editor> <title> "The Harvest Information Discovery and Access System," </title> <booktitle> Proceedings of the Second International World-Wide Web Conference, </booktitle> <pages> pp. 763-771, </pages> <address> Chicago, Illinois, </address> <note> October 1994; and http://harvest.cs.colorado.edu/. </note>
Reference-contexts: Content Routing [28] is a system in which queries get routed to the available servers based on the expected relevance of the 2 server to the query. This system also propagates descriptions of server contents through a hierarchy of information servers. Harvest <ref> [5] </ref> automatically indexes documents within a source and distributes these indexes. It can be used to describe WWW sites, newsgroups, repositories of technical reports, etc. and includes subsystems for caching and replicating the indexes. <p> When a source sends out its mid-level metadata, it sends the different components to the corresponding servers in a point-to-point manner. The distribution of the mid-level metadata in Pharos is modeled after the distribution of indexes in Harvest <ref> [5] </ref>. The Harvest system includes efficient point-to-point transfer of metadata, server replication, and caching. It also provides for structured querying of the servers. 3 Metadata Structure The metadata must be designed to support a multi-level information system and it should match the intended queries as well as the retrieval system. <p> The mid-level servers in Pharos are responsible for keeping their metadata up-to-date. This approach minimizes network traffic and guarantees that the mid-level servers are ready to receive the metadata before it is sent. Harvest <ref> [5] </ref> provides a suitable transport mechanism for distributing and storing mid-level metadata in Pharos. Harvest is a generalized system for automatically indexing documents within a source and distributing the metadata. In Harvest terminology, sources of information are called providers. Index metadata is extracted by gatherers.
Reference: [6] <author> CACM, </author> <title> "Digital Libraries," Section on Digital Library Initiatives, </title> <journal> CACM, </journal> <volume> Vol. 38, No. 4, </volume> <pages> pp. 57-64, </pages> <month> April, </month> <year> 1995. </year>
Reference-contexts: Emerging DL systems must deal with storing, locating, cataloging, distributing, and retrieving large numbers of diverse documents in a distributed, heterogeneous environment. Several projects in the NSF/NASA/ARPA Digital Library Initiative Program <ref> [6, 24] </ref> are therefore investigating these problems. The Alexandria Digital Library (ADL) Project [2] is focusing on indexing spatial information, as well as on the storage, distribution, and retrieval of large (spatial) images.
Reference: [7] <author> Chen, H. & Lynch, K.J., </author> <title> "Automatic Construction of Networks of Concepts Characterizing Document Databases," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Volume 22, Number 5, </volume> <pages> Pages 885-902, </pages> <note> 1992; and http://ai.bpa.arizona.edu/papers/ieee91/ieee91.html. </note>
Reference-contexts: Key research topics include placement of query terms within existing classification schemes, discovery of relationships between different thesauri, and automatic classification through the analysis of document collections <ref> [7, 8, 9] </ref>. The University of Michigan Digital Library Project [11] is designing an agent-based DL system; user interface agents, mediation agents, and collection agents cooperate to allow concurrent searching of multiple collections. The gGlOSS resource discovery system [15] represents sources by vectors of term frequencies. <p> The Pharos UI bridges the gap between the words used by the user and the fixed terms in the subject hierarchy. Techniques such as those described in <ref> [12, 7] </ref> specifically address the problem of finding the relationships between query terms and potentially different document terms. We plan, therefore, to place users' concepts within appropriate sub-domains in the subject hierarchy, while at the same time fixing the categories so that it is easier to accurately compare sources.
Reference: [8] <author> Chen, H. et al., </author> <title> "Generating, Integrating, and Activating Thesauri for Concept-Based Document Retrieval," </title> <journal> IEEE Expert, Special Series on Artificial Intelligence in Text-Based Information Systems, </journal> <volume> Volume 8, Number 2, </volume> <pages> Pages 25-34, </pages> <month> April, </month> <year> 1993. </year>
Reference-contexts: Key research topics include placement of query terms within existing classification schemes, discovery of relationships between different thesauri, and automatic classification through the analysis of document collections <ref> [7, 8, 9] </ref>. The University of Michigan Digital Library Project [11] is designing an agent-based DL system; user interface agents, mediation agents, and collection agents cooperate to allow concurrent searching of multiple collections. The gGlOSS resource discovery system [15] represents sources by vectors of term frequencies.
Reference: [9] <author> Chen, H. & Ng, T., </author> <title> "An Algorithmic Approach to Concept, Exploration in a Large Knowledge Network (Automatic Thesaurus Consultation): Symbolic Branch-and-bound Search vs. Connectionist Hopfield Net Activation," </title> <journal> Journal of the American Society for Information Science, </journal> <volume> Volume 46, Number 5, </volume> <pages> Pages 348-369, </pages> <note> June 1995; and http://ai.bpa.arizona.edu/papers/snnn92/snnn92.html. </note>
Reference-contexts: Key research topics include placement of query terms within existing classification schemes, discovery of relationships between different thesauri, and automatic classification through the analysis of document collections <ref> [7, 8, 9] </ref>. The University of Michigan Digital Library Project [11] is designing an agent-based DL system; user interface agents, mediation agents, and collection agents cooperate to allow concurrent searching of multiple collections. The gGlOSS resource discovery system [15] represents sources by vectors of term frequencies.
Reference: [10] <institution> ConQuest Software, Inc., </institution> <note> "ConQuest Guide for Developers" (Proprietary), Version 4.0, </note> <institution> Columbia, MD, </institution> <year> 1994. </year>
Reference-contexts: It is similar, in this regard, to the SMART automated text retrieval system [26], replacing the two-dimensional term/document matrix by a term/collection matrix. SMART and gGlOSS are only two of several automatic text retrieval systems. Other notable systems include ConQuest <ref> [10] </ref>, which uses a static semantic network based on dictionary definitions, and Latent Semantic Indexing (LSI) [3], which uses a dynamic semantic network based on word co-occurrences within documents in a collection.
Reference: [11] <author> Crum, L., </author> <title> "University of Michigan Digital Library Project," </title> <journal> CACM, </journal> <volume> Vol. 38, No. 4, </volume> <month> April, </month> <year> 1995. </year>
Reference-contexts: Key research topics include placement of query terms within existing classification schemes, discovery of relationships between different thesauri, and automatic classification through the analysis of document collections [7, 8, 9]. The University of Michigan Digital Library Project <ref> [11] </ref> is designing an agent-based DL system; user interface agents, mediation agents, and collection agents cooperate to allow concurrent searching of multiple collections. The gGlOSS resource discovery system [15] represents sources by vectors of term frequencies.
Reference: [12] <author> Dumais, S., </author> <title> "Improving the retrieval of information from external sources," Behavior Research Methods, Instruments, </title> & <journal> Computers, </journal> <volume> Vol 23, No. 2, </volume> <pages> pp. 229-236, </pages> <year> 1991. </year>
Reference-contexts: This classification system has several advantages: it is very extensive, it is familiar to most of the library classification community, it is open-ended, and it is used by most academic libraries in the U.S.A. While there are problems with using a fixed list for query terms <ref> [12] </ref>, there are several problems with using vocabularies that are based on the terms found in each document collection. For example, it is difficult to compare sources that do not use the same terms, and we cannot expect different 7 collections to yield the same vocabularies. <p> The Pharos UI bridges the gap between the words used by the user and the fixed terms in the subject hierarchy. Techniques such as those described in <ref> [12, 7] </ref> specifically address the problem of finding the relationships between query terms and potentially different document terms. We plan, therefore, to place users' concepts within appropriate sub-domains in the subject hierarchy, while at the same time fixing the categories so that it is easier to accurately compare sources.
Reference: [13] <author> Ernst, R. & Yovits, M., </author> <title> "Information Science as an Aid to Decision-Making," Decision-Making: Creativity, Judgment [sic], and Systems, </title> <editor> ed. Brinkers, H., </editor> <publisher> Ohio State University Press, </publisher> <year> 1972. </year>
Reference-contexts: massively replicated, localized high-level servers and the sparsely replicated, remote mid-level servers, each receiving and sending appropriate metadata with very different network characteristics and hence different underlying network protocols. 4.1 User Interface and Metadata Retrieval The User Interface (UI) is designed to assist the user in an iterative decision-making process <ref> [13] </ref> in which a final small set of sources is chosen through a series of refinements on an initial large set. This process uses information about the user, called the User Profile; information about the query, called the Task Profile [29]; and the high- and mid-level metadata about the sources.
Reference: [14] <author> Fox, E., et al., </author> <title> "Digital Libraries," </title> <journal> CACM, </journal> <volume> Vol. 38, No. 4, </volume> <month> April, </month> <year> 1995. </year>
Reference-contexts: WWW indexing systems that index document texts require their own indexing techniques and cannot use the metadata that a source extracts about itself. Problems of locating sources of information, or resource discovery [4], are of particular interest to the DL community <ref> [14] </ref>. Emerging DL systems must deal with storing, locating, cataloging, distributing, and retrieving large numbers of diverse documents in a distributed, heterogeneous environment. Several projects in the NSF/NASA/ARPA Digital Library Initiative Program [6, 24] are therefore investigating these problems.
Reference: [15] <author> Gravano, L. & Garc ia-Molina, H., </author> <title> "Generalizing GlOSS to Vector-Space Databases and Broker Hierarchies", </title> <booktitle> Proceedings of the 21st VLDB Conference, </booktitle> <address> Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: The University of Michigan Digital Library Project [11] is designing an agent-based DL system; user interface agents, mediation agents, and collection agents cooperate to allow concurrent searching of multiple collections. The gGlOSS resource discovery system <ref> [15] </ref> represents sources by vectors of term frequencies. It is similar, in this regard, to the SMART automated text retrieval system [26], replacing the two-dimensional term/document matrix by a term/collection matrix. SMART and gGlOSS are only two of several automatic text retrieval systems.
Reference: [16] <author> Guyton, J. & Schwartz, M., </author> <title> "Locating Nearby Copies of Replicated Internet Servers," </title> <journal> Computer Communication Review, vol.25, </journal> <volume> No.4, </volume> <pages> pp. 288-298, </pages> <address> ACM-SIGCOMM '95, </address> <month> Oct. </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Count Tax Integer 0+ Number of taxonomies used: 3 Spec String [M] List of M Special Collections: Geography:BE16 Net Band Real 0+ Network bandwidth (Kbps): 1.5E3 Net Avg Util Real 0.0 - 1.0 Avg. network utilization: 0.47 Net Avg Delays Integer [N] 0+ Avg. network delay (ms) to N beacons <ref> [16] </ref>: ,200,,521,,,147, Net Avg Thru Integer [N] 0+ Avg. network throughput (Kbps) to N beacons [16]: ,117,,,,,246, Count Avg Hits Real 0+ Avg. number of accesses (hits/day): 1.237E5 Pol Charg String Charging Policy: See http://www.alex.ucsb.edu/doc/charging.html Pol Lend String Lending Policy: Must be UC affiliate Count Src Integer 1+ Number of sources: <p> Special Collections: Geography:BE16 Net Band Real 0+ Network bandwidth (Kbps): 1.5E3 Net Avg Util Real 0.0 - 1.0 Avg. network utilization: 0.47 Net Avg Delays Integer [N] 0+ Avg. network delay (ms) to N beacons <ref> [16] </ref>: ,200,,521,,,147, Net Avg Thru Integer [N] 0+ Avg. network throughput (Kbps) to N beacons [16]: ,117,,,,,246, Count Avg Hits Real 0+ Avg. number of accesses (hits/day): 1.237E5 Pol Charg String Charging Policy: See http://www.alex.ucsb.edu/doc/charging.html Pol Lend String Lending Policy: Must be UC affiliate Count Src Integer 1+ Number of sources: 1 Table 1: Taxonomy-Independent High-Level Metadata 12 on our estimated size, S H , of <p> Count Tax is set to the number of taxonomies used for classification by the source. Spec is the list of character strings used to describe special collections, described earlier. The set of parameters Net * are included for connectivity estimation between the users and the sources <ref> [16] </ref>. Count Avg Hits describes the average number of accesses the source receives per day. Additional attributes are available for various library policies relevant to charging, lending, etc. Finally, Count Src denotes the number of sources included in a particular metadata record to allow condensed multi-source records.
Reference: [17] <author> Horton, M., </author> <title> "Standard for Interchange of USENET Messages," </title> <institution> RFC850, IETF Net--work Working Group, </institution> <year> 1983. </year>
Reference-contexts: Given such a retrieval system, we need a corresponding metadata distribution system such that the high-level metadata is distributed widely and the mid-level metadata is distributed selectively. The wide distribution mechanism in Pharos is modeled after the distribution of USENET news via NNTP <ref> [17, 19] </ref>. There are many news servers on the Internet with basically the same replicated data. When someone posts a news article, it gets distributed to all the news servers that serve the groups or news hierarchies within which the article resides.
Reference: [18] <author> Hull, D., </author> <title> "Improving Text Retrieval for the Routing Problem using Latent Semantic Indexing," </title> <booktitle> Proc. of the 17th ACM-SIGIR Conference, </booktitle> <pages> pp. 282-291, </pages> <year> 1994. </year>
Reference-contexts: High-level automatic subject classification has been shown to be fairly successful, with over a 90% accuracy rate [22]. LSI has been shown to be an effective tool for automated classification <ref> [18] </ref>, and experiments on automatically classifying large document sets within the LC Classification system have yielded correct classifications, under some conditions, of over 80% of newly entered documents [20].
Reference: [19] <author> Kantor, B. & Lapsley, P., </author> <title> "Network News Transfer Protocol: A Proposed Standard for the Stream-Based Transmission of News," </title> <institution> RFC997, IETF Network Working Group, </institution> <year> 1986. </year>
Reference-contexts: Given such a retrieval system, we need a corresponding metadata distribution system such that the high-level metadata is distributed widely and the mid-level metadata is distributed selectively. The wide distribution mechanism in Pharos is modeled after the distribution of USENET news via NNTP <ref> [17, 19] </ref>. There are many news servers on the Internet with basically the same replicated data. When someone posts a news article, it gets distributed to all the news servers that serve the groups or news hierarchies within which the article resides.
Reference: [20] <author> Larson, R., </author> <title> "Experiments in Automatic Library of Congress Classification," </title> <journal> JASIS, vol.43, No.2, </journal> <pages> pp. 130-148, </pages> <year> 1992. </year>
Reference-contexts: LSI has been shown to be an effective tool for automated classification [18], and experiments on automatically classifying large document sets within the LC Classification system have yielded correct classifications, under some conditions, of over 80% of newly entered documents <ref> [20] </ref>. These results indicate that subject-based automatic classification is currently or soon to be sufficiently accurate to characterize sources for comparison purposes, where even order of magnitude estimates can greatly aid in filtering out most irrelevant sources.
Reference: [21] <author> Library of Congress, </author> <title> LC Classification Outline, Fifth Edition, Library of Congress, </title> <address> Washington, D.C., </address> <year> 1986. </year>
Reference-contexts: As mentioned above, the initial design of Pharos employs three information hierarchies: subject, geography, and temporal hierarchies. The subject hierarchy is modeled after the Library of Congress's LC Classification system <ref> [21] </ref>, which has a fixed list of categories. For a given query, terms must be deduced that match these categories.
Reference: [22] <author> Losee, R. & Haas, S., </author> <title> "Sublanguage Terms: Dictionaries, Usage, and Automatic Classification," </title> <journal> JASIS, vol.46, </journal> <volume> No.7, </volume> <pages> pp. 519-529, </pages> <year> 1995. </year>
Reference-contexts: High-level automatic subject classification has been shown to be fairly successful, with over a 90% accuracy rate <ref> [22] </ref>. LSI has been shown to be an effective tool for automated classification [18], and experiments on automatically classifying large document sets within the LC Classification system have yielded correct classifications, under some conditions, of over 80% of newly entered documents [20].
Reference: [23] <author> Lycos, </author> <note> http://www.lycos.com/, 1995. </note>
Reference-contexts: The reason for such a change in focus, as will hopefully be made clear later, is that we believe that locating collections by domains is a much more scalable methodology than locating particular authors, titles, etc. Current WWW indexes, such as AltaVista [1], Lycos <ref> [23] </ref>, and Yahoo [33], which are designed for locating information on the Internet, are limited in several ways. Notably, these systems do not scale to handle increasingly large numbers of search requests due to limited network bandwidth and server power.
Reference: [24] <institution> NSF, "Research on Digital Libraries," Program Guideline NSF 93-141, NSF, Septem-ber, </institution> <year> 1993, </year> <note> and http://www.nsf.gov/ftp/CISE/program/nsf93141.txt. </note>
Reference-contexts: Emerging DL systems must deal with storing, locating, cataloging, distributing, and retrieving large numbers of diverse documents in a distributed, heterogeneous environment. Several projects in the NSF/NASA/ARPA Digital Library Initiative Program <ref> [6, 24] </ref> are therefore investigating these problems. The Alexandria Digital Library (ADL) Project [2] is focusing on indexing spatial information, as well as on the storage, distribution, and retrieval of large (spatial) images.
Reference: [25] <author> Richards, J., </author> <title> Remote sensing digital image analysis: an introduction, 2nd rev. </title> <editor> and enlarged ed., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Automatically extracting subject information from maps and images, such as identifying vegetation in a map or a particular object in an image, would allow such documents to be automatically classified within, for example, the LC Classification system; such capabilities are ongoing research issues <ref> [25] </ref>. 6 http://potomac.ncsl.nist.gov/TREC/ 18 4.3 High-Level Metadata Servers Once the metadata at each site has been compiled, it needs to be distributed over the network according to the intended storage and retrieval architecture.
Reference: [26] <author> Salton, G., </author> <title> Automatic Text Processing, </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Existing WWW indexing systems utilize either small, hand-made, hierarchical lists, as in Yahoo, or word-matching on huge document-spaces, as in AltaVista. Standard techniques from the field of Automatic Text Retrieval <ref> [26] </ref>, such as relevance feedback and 1 In our present context, the main requirement for an information source to be called a Digital Library is that it contain its own catalog and query system. 2 Precision is the fraction of returned documents that are relevant to a query, while recall is <p> The gGlOSS resource discovery system [15] represents sources by vectors of term frequencies. It is similar, in this regard, to the SMART automated text retrieval system <ref> [26] </ref>, replacing the two-dimensional term/document matrix by a term/collection matrix. SMART and gGlOSS are only two of several automatic text retrieval systems. <p> This collection is processed by hand or automatically. For text-based documents, it can be processed by one of several possible automatic text analysis tools, such as LSI, SMART <ref> [26] </ref>, or others that are used, for example, in the NIST Text Retrieval Conferences (TREC). 6 Many of these systems read through a collection and build a matrix of weighted frequencies of the number of times a given term occurs in each document in the collection.
Reference: [27] <author> Schatz, B., </author> <title> "Building the Interspace: The Illinois Digital Library Project," </title> <journal> CACM, </journal> <volume> Vol. 38, No. 4, </volume> <month> April, </month> <year> 1995. </year>
Reference-contexts: The Alexandria Digital Library (ADL) Project [2] is focusing on indexing spatial information, as well as on the storage, distribution, and retrieval of large (spatial) images. The content and form of the metadata required to classify geographically referenced data is a central research issue. The Illinois Digital Library Project <ref> [27] </ref> is building a prototype of the Interspace, which presents the Internet as a single space of highly interlinked, distributed information.
Reference: [28] <author> Sheldon, M., et al., </author> <title> "Content Routing for Distributed Information Servers", </title> <booktitle> Proceedings of the 4th International Conference on Extending Database Technology, </booktitle> <year> 1994. </year>
Reference-contexts: Other notable systems include ConQuest [10], which uses a static semantic network based on dictionary definitions, and Latent Semantic Indexing (LSI) [3], which uses a dynamic semantic network based on word co-occurrences within documents in a collection. Content Routing <ref> [28] </ref> is a system in which queries get routed to the available servers based on the expected relevance of the 2 server to the query. This system also propagates descriptions of server contents through a hierarchy of information servers.
Reference: [29] <author> Shneiderman, B., </author> <title> "Designing the User Interface, Strategies for Effective Human-Computer Interaction", 2nd Ed., </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: This process uses information about the user, called the User Profile; information about the query, called the Task Profile <ref> [29] </ref>; and the high- and mid-level metadata about the sources. Although the client acquires source metadata through the network from the high- and mid-level servers, it acquires user and query information through the UI.
Reference: [30] <author> The Stanford Digital Libraries Group, </author> <title> "The Stanford Digital Library Project," </title> <journal> CACM, </journal> <volume> Vol. 38, No. 4, </volume> <month> April, </month> <year> 1995. </year>
Reference: [31] <author> White, D., </author> <title> Fundamentals of Decision Theory, </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1976. </year> <month> 27 </month>
Reference-contexts: Initially, we display each source as a point in a two-dimensional scatter plot, as depicted in Figure 6. There are several ways of assigning positions to sources. One possibility is to use a quality versus cost approach to visualize the economic value of the sources <ref> [31] </ref>. The quality and cost dimension of each source is computed as a weighted sum of the relevant attributes and each source is depicted as a point in the Quality/Cost plane.
Reference: [32] <author> Woodruff, A. & Plaunt, C., "GIPSY: </author> <title> Automated Geographic Indexing of Text Docu--ments," </title> <journal> JASIS, </journal> <volume> Vol. 45, No. 9, </volume> <pages> pp. 645-655, </pages> <year> 1994. </year>
Reference-contexts: While the TREC and related work focuses on subject-based text retrieval, one could use a gazetteer or time-name table to identify geographic or temporal references within text-based documents, or use more sophisticated techniques such as those used in GIPSY <ref> [32] </ref>. For maps, aerial photographs, and satellite images, documents are often cataloged with spatial extents, and the geographic clustering and classification process is much simpler.
Reference: [33] <author> Yahoo FAQ, </author> <note> http://www.yahoo.com/docs/info/faq.html, 1995. 28 </note>
Reference-contexts: The reason for such a change in focus, as will hopefully be made clear later, is that we believe that locating collections by domains is a much more scalable methodology than locating particular authors, titles, etc. Current WWW indexes, such as AltaVista [1], Lycos [23], and Yahoo <ref> [33] </ref>, which are designed for locating information on the Internet, are limited in several ways. Notably, these systems do not scale to handle increasingly large numbers of search requests due to limited network bandwidth and server power.
References-found: 33

