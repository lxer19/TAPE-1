URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/95-15.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: fehrlich|rapaportg@cs.buffalo.edu  
Title: A COMPUTATIONAL THEORY OF VOCABULARY EXPANSION Project Proposal  
Author: Karen Ehrlich and William J. Rapaport 
Address: NY 14260  
Affiliation: Department of Computer Science and Center for Cognitive Science State University of New York at Buffalo, Buffalo,  
Abstract: This project concerns the development and implementation of a computational theory of how human readers and other natural-language-understanding systems can automatically expand their vocabulary by determining the meaning of a word from context. The word might be unknown to the reader, familiar but misunderstood, or familiar but being used in a new sense. `Context' includes the prior and immediately surrounding text, grammatical information, and the reader's background knowledge, but no access to a dictionary or other external source of information (including a human). The fundamental thesis is that the meaning of such a word (1) can be determined from context, (2) can be revised and refined upon further encounters with the word, (3) "converges" to a dictionary-like definition if enough context has been provided and there have been enough exposures to the word, and (4) eventually "settles down" to a "steady state", which, however, is always subject to revision upon further encounters with the word. The system is being implemented in the SNePS-2.1 knowledge-representation and reasoning system, which provides a software laboratory for testing and experimenting with the theory. This research is a component of an interdisciplinary, cognitive-science project to develop 0 SUMMARY. This project concerns the development and implementation of a computational theory of how human readers and other natural-language-understanding systems can automatically increase their lexicon (i.e., expand their vocabulary) by determining the meaning of a word from context. The word might be unknown to the reader, familiar but misunderstood, or familiar but being used in a new sense. `Context' includes the prior and immediately surrounding text, grammatical information, and the reader's background knowledge, but no access to a dictionary or other external source of information (including a human). The fundamental thesis is that the meaning of such a word (1) can be determined from context, (2) can be revised and refined upon further encounters with the word, (3) "converges" to a dictionary-like definition if enough context has been provided and there have been enough exposures to the word, and (4) eventually "settles down" to a "steady state", which, however, is always subject to revision upon further encounters with the word. Each encounter with the word yields a definition|a hypothesis about meaning. Each subsequent encounter provides an opportunity to revise this hypothesis in the light of new evidence. The revision is unsupervised : There is no (human) "trainer" and no "error-correction" techniques. Finally, no domain-specific antecedent background information is required for the development and revision of the hypothesized definition (with the exception of the word's lexical category (part of speech)). a computational cognitive model of a reader of narrative text.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almeida, M.J. </author> <year> (1987), </year> <title> "Reasoning about the Temporal Structure of Narratives," </title> <type> TR 87-10 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: This system serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: <ref> [1] </ref> [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE. <p> In the case of a terminated activity, it may end because its goal is accomplished, or because the activity is no longer possible. (The related work of Almeida <ref> [1] </ref>, [2] on time in narratives may be of some use here, as would Cohen's work on textual markers [13], [14]. Sequence of action may be considered in terms of whether it suggests a causal or enabling link.
Reference: [2] <author> Almeida, M.J. </author> <year> (1995), </year> <title> "Time in Narratives," </title> <booktitle> in [19]. </booktitle>
Reference-contexts: In the case of a terminated activity, it may end because its goal is accomplished, or because the activity is no longer possible. (The related work of Almeida [1], <ref> [2] </ref> on time in narratives may be of some use here, as would Cohen's work on textual markers [13], [14]. Sequence of action may be considered in terms of whether it suggests a causal or enabling link.
Reference: [3] <author> Asker, L.; Gamback, B.; & Samuelsson, C. </author> <year> (1992), </year> <title> "EBL 2 : An Approach to Automatic Lexical Acquisition" COLING-92 : 1172-1176. </title>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: <ref> [3] </ref>, [4], [9], [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon.
Reference: [4] <author> Berwick, R.C. </author> <year> (1983), </year> <title> "Learning Word Meanings from Examples," </title> <type> IJCAI-83 : 459-461. </type>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], <ref> [4] </ref>, [9], [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon.
Reference: [5] <author> Bruder, G.A. </author> <year> (1988), </year> <title> "The Deictic Center and Sentence Interpretation in Natural Narrative" (Psychonomic Soc. </title> <booktitle> Meeting, </booktitle> <address> Chicago). </address>
Reference-contexts: This system serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] <ref> [5] </ref>, [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [6] <author> Bruder, G.A. </author> <year> (1995), </year> <title> "Psychological Evidence that Linguistic Devices Are Used by Readers to Understand Spatial Deixis in Narrative Text," </title> <booktitle> in [19]. </booktitle>
Reference: [7] <author> Bruder, G.A. & Wiebe, J.M. </author> <year> (1990), </year> <title> "A Psychological Test of an Algorithm for Recognizing Subjectivity in Narrative Text," </title> <booktitle> Proc. 12th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 947-952. </pages>
Reference-contexts: This system serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], <ref> [7] </ref>, [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [8] <author> Bruder, G.A., & Wiebe, J.M. </author> <year> (1995), </year> <title> "Recognizing Subjectivity and Identifying Subjective Characters in Third-Person Fictional Narrative," </title> <booktitle> in [19]. </booktitle>
Reference: [9] <author> Carbonell, J.G., & Hayes, P.J. </author> <year> (1984), </year> <title> "Coping with Extragrammaticality, </title> " <type> COLING-84 : 433-437. </type>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], <ref> [9] </ref>, [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon.
Reference: [10] <author> Carter, D. </author> <year> (1989), </year> <title> "Lexical Acquisition in the Core Language Engine", </title> <booktitle> Proc. 4th Conf. Euro-pean Chapter Assoc. Comp. Ling.: </booktitle> <pages> 137-144. </pages>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], <ref> [10] </ref>, [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon.
Reference: [11] <author> Choi, J. </author> <year> (1993), </year> <title> "Experience-Based Learning in Deductive Reasoning Systems," </title> <type> TR 93-20 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: This raises the problem of controlling and localizing the reasoning process. Although we do not have a panacea for this, some results have been obtained in a related project on SNePS reasoning <ref> [11] </ref>; see also xx8.2, 8.3. Cassie's input consists, in part, of information from the text being read. Currently, this, too, is input directly in the KRR formalism.
Reference: [12] <author> Chun, S.A., & Zubin, D. </author> <year> (1995), </year> <title> "Experiential vs. Agentive Constructions in Korean Narrative," </title> <booktitle> in [19]. </booktitle>
Reference: [13] <author> Cohen, R. </author> <year> (1987), </year> <title> "Analyzing the Structure of Argumentative Discourse," </title> <journal> Comp. Ling. </journal> <volume> 13: </volume> <pages> 1-10. </pages>
Reference-contexts: In the case of a terminated activity, it may end because its goal is accomplished, or because the activity is no longer possible. (The related work of Almeida [1], [2] on time in narratives may be of some use here, as would Cohen's work on textual markers <ref> [13] </ref>, [14]. Sequence of action may be considered in terms of whether it suggests a causal or enabling link. In general, an act by one agent followed by the act of a different agent is not apt to be causal or enabling unless a textual marker is present.
Reference: [14] <author> Cohen, R. </author> <year> (1990), </year> <title> "Implementing a Model for Understanding Goal-Oriented Discourse," </title> <booktitle> Int'l. J. Intelligent Systems 5: </booktitle> <pages> 1-31. </pages>
Reference-contexts: In the case of a terminated activity, it may end because its goal is accomplished, or because the activity is no longer possible. (The related work of Almeida [1], [2] on time in narratives may be of some use here, as would Cohen's work on textual markers [13], <ref> [14] </ref>. Sequence of action may be considered in terms of whether it suggests a causal or enabling link. In general, an act by one agent followed by the act of a different agent is not apt to be causal or enabling unless a textual marker is present.
Reference: [15] <author> Costello, A.M.; Bruder, G.A.; Hosenfeld, C.; & Duchan, J.F. </author> <year> (1995), </year> <title> "A Structural Analysis of A Fictional Narrative," </title> <booktitle> in [19]. </booktitle>
Reference: [16] <author> Cravo, M.R., & Martins, J.P. </author> <year> (1993), </year> <title> "SNePSwD: A Newcomer to the SNePS Family", </title> <type> JETAI 5: </type> <pages> 135-148. </pages>
Reference-contexts: We have developed algorithms for partially automating the identification and removal or modification of the offending premise, based on SNePSwD, a default belief-revision system that enables automatic revision <ref> [16] </ref>, [46]. We propose to explore techniques for fully automating it.
Reference: [17] <author> Daniels, J.H. </author> <title> (1986) "A Psychological Investigation into the Deictic Center," </title> <booktitle> Proc. 8th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 621-626. </pages>
Reference-contexts: system serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], <ref> [17] </ref>, [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [18] <author> Duchan, J.F. </author> <year> (1995), </year> <title> "Preschool Children's Introduction of Characters into Their Oral Stories: Evidence for Deictic Organization of First Narratives," </title> <booktitle> in [19]. </booktitle> <pages> 22 </pages>
Reference: [19] <editor> Duchan, J.F.; Bruder, G.A.; & Hewitt, L.E. (eds.) </editor> <year> (1995), </year> <title> Deixis in Narrative: A Cognitive Science Perspective (Erlbaum), </title> <publisher> forthcoming. </publisher>
Reference-contexts: This system serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in <ref> [19] </ref> (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE. <p> serves two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in <ref> [19] </ref> (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [20] <author> Ehrlich, K. </author> <year> (1995), </year> <title> "Automatic Vocabulary Expansion through Narrative Context," </title> <type> TR 95-09 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: 1 INTRODUCTION. We propose to continue our development of a computational theory of how readers (or natural-language-understanding (NLU) systems) can automatically increase their lexicon (i.e., expand their vocabulary) by determining the meaning of a word from context <ref> [20] </ref>. The word might be unknown to the reader, familiar but misunderstood, or familiar but being used in a new sense.
Reference: [21] <author> Elshout-Mohr, M., & van Daalen-Kapteijns, M.M. </author> <year> (1987), </year> <title> "Cognitive Processes in Learning Word Meanings," </title> <editor> in M. G. McKeown & M. E. Curtis (eds.), </editor> <booktitle> The Nature of Vocabulary Acquisition (Erlbaum): </booktitle> <pages> 53-71. </pages>
Reference-contexts: Some of these are based on psycholinguistic studies of the sort of vocabulary expansion we are modeling <ref> [21] </ref>, [39], [86]. In the absence of some or all of this information, or in the presence of potentially inconsistent information (e.g., if the text says that one brachet hunts and another doesn't), Cassie either leaves certain "slots" in her definitional framework empty, or includes information about particular brachets.
Reference: [22] <author> Galbraith, M. </author> <year> (1989), </year> <title> "What Everybody Knew Versus What Maisie Knew: The Change in Epistemological Perspective from the Prologue to the Opening of Chapter 1 in `What Maisie Knew' ", Style 23: </title> <type> 197-212. </type>
Reference-contexts: two purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], <ref> [22] </ref>, [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [23] <author> Galbraith, M. </author> <year> (1995), </year> <title> "Deictic Shift Theory and the Poetics of Involvement in Narrative," </title> <booktitle> in [19]. </booktitle>
Reference: [24] <author> Gentner, D. </author> <year> (1981), </year> <title> "Some Interesting Differences between Nouns and Verbs," </title> <journal> Cognition and Brain Theory 4: </journal> <pages> 161-178. </pages>
Reference-contexts: If only one belief has the highest level of uncertainty in the conflict set, it will be revised. If several alternatives exist with the same (highest present) kn cat, Cassie looks for a verb in the antecedent (humans more readily revise beliefs about verbs than about nouns <ref> [24] </ref>). If this is still insufficient to yield a single culprit, then, in the current 11 implementation, a human "oracle" chooses one (although this hasn't been needed in our tests to date).
Reference: [25] <author> Gleitman, L. </author> <year> (1990), </year> <title> "The Structural Sources of Verb Meanings," </title> <journal> Lang. </journal> <volume> Acquisition 1: </volume> <pages> 1-55. </pages>
Reference-contexts: We have endeavored to incorporate, or be consistent with, research in psycholinguistics on language acquisition, notably the work already cited in xx5.1, 5.4. We plan to elaborate our theory of verb acquisition in the light of research on verb meanings <ref> [25] </ref>, [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. <p> Finally, Siskind claims that his theory provides evidence for "semantic bootstrapping"| using semantics to aid in learning syntax. In contrast, our system uses syntactic bootstrapping (using syntax to aid in learning semantics <ref> [25] </ref>, [26]), which seems more reasonable for our situation. 8 NEXT STEPS. 8.1 Developing the Grammar. One significant improvement that we must undertake is the development of a generalized ATN grammar that can parse simplified forms of the sentences in [43] (cf. x6).
Reference: [26] <author> Gleitman, L. </author> <year> (1994), </year> <title> "A Picture is Worth a Thousand Words|But That's the Problem" (talk presented at the 1st Int'l. </title> <institution> Summer Inst. in Cog. Sci. (SUNY Buffalo, </institution> <note> July 1994); abstract in Proc. 16th Annual Conf. </note> <institution> Cog. Sci. Soc.: </institution> <month> 965). </month>
Reference-contexts: We have endeavored to incorporate, or be consistent with, research in psycholinguistics on language acquisition, notably the work already cited in xx5.1, 5.4. We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], <ref> [26] </ref>. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. <p> Finally, Siskind claims that his theory provides evidence for "semantic bootstrapping"| using semantics to aid in learning syntax. In contrast, our system uses syntactic bootstrapping (using syntax to aid in learning semantics [25], <ref> [26] </ref>), which seems more reasonable for our situation. 8 NEXT STEPS. 8.1 Developing the Grammar. One significant improvement that we must undertake is the development of a generalized ATN grammar that can parse simplified forms of the sentences in [43] (cf. x6).
Reference: [27] <author> Granger, R.H. </author> <year> (1977), </year> <title> "Foul-Up: A Program that Figures Out Meanings of Words from Context," </title> <address> IJCAI-77 : 67-68. </address>
Reference: [28] <author> Granger, R.H. </author> <year> (1983), </year> <title> "The NOMAD System: Expectation-Based Detection and Correction of Errors during Understanding of Syntactically and Semantically Ill-Formed Text," </title> <journal> American J. Comp. Ling. </journal> <volume> 9: </volume> <pages> 188-196. </pages>
Reference: [29] <author> Haas, N., & Hendrix, G. </author> <year> (1983), </year> <title> "Learning by Being Told: Acquiring Knowledge for Information Management," in R.S. </title> <editor> Michalski, J.G. Carbonell, & T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning (Tioga): </booktitle> <pages> 405-428. </pages>
Reference: [30] <author> Hastings, P.M. </author> <year> (1994), </year> <title> "Automatic Acquisition of Word Meaning from Context," </title> <type> Ph.D. </type> <institution> diss. (Comp. Sci. & Eng'g., Univ. of Michigan). </institution>
Reference: [31] <author> Hastings, P.M., & Lytinen, S.L. </author> <year> (1994a), </year> <title> "The Ups and Downs of Lexical Acquisition," </title> <type> AAAI-94 : 754-759. </type>
Reference: [32] <author> Hastings, P.M., & Lytinen, S.L. </author> <year> (1994b), </year> <title> "Objects, Actions, Nouns, and Verbs," </title> <booktitle> Proc. 16th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 397-402. </pages>
Reference: [33] <author> Hearst, M.A. </author> <year> (1992), </year> <title> "Automatic Acquisition of Hyponyms from Large Text Corpora," </title> <type> COLING-92 : 539-545. </type>
Reference-contexts: As long as we are using a system that requires a separate lexicon for parsing, we may as well take advantage of it.) In addition to adapting the SNePS-79 grammar to SNePS-2.1, we especially intend our new grammar to build appropriate representations of causal and hyponym-hypernym <ref> [33] </ref> relations between clauses. 16 8.2 Believing Reported Synonyms. Although Cassie does not add her definitions to her KB, she believes almost all the elements of the reported definitions. I.e., the class inclusions and assorted properties of nouns and the results of actions are all present in her KB.
Reference: [34] <author> Hewitt, L.E. </author> <year> (1995), </year> <title> "Reduced Anaphor in Subjective Contexts," </title> <booktitle> in [19]. </booktitle>
Reference: [35] <author> Hewitt, L.E., & Duchan, J.F. </author> <title> (forthcoming), </title> <booktitle> "Subjectivity in Children's Fictional Narrative," Topics in Language Disorders. </booktitle> <pages> 23 </pages>
Reference-contexts: purposes: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], <ref> [35] </ref>, [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [36] <author> Hill, R. </author> <year> (1994), </year> <title> "Issues of Semantics in a Semantic-Network Representation of Belief," </title> <type> TR 94-11 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: This is too circular and too unwieldy to be of much use. We need to limit the connections used to provide the definition. In a related project <ref> [36] </ref>, Hill proposed a SCOPE function that selects a subnetwork containing only those concepts reachable from a particular concept by a path of connections that is no longer than a given length.
Reference: [37] <author> Hosenfeld, C.; Duchan, J.; & Higginbotham, J. </author> <year> (1995), </year> <title> "Deixis in Persuasive Texts Written by Bilinguals of Differing Degrees of Expertise," </title> <booktitle> in [19]. </booktitle>
Reference: [38] <author> Jackendoff, R. </author> <year> (1987), </year> <title> "On Beyond Zebra: The Relation of Linguistic and Visual Information," </title> <journal> Cognition 26: </journal> <pages> 89-114. </pages>
Reference-contexts: This could be done in one of at least two ways: (1) by using propositional representations of the illustrations, in which case our algorithms would work more or less as they do now, or (2) by having explicit reference to imagistic representations, including these as part of the meaning <ref> [38] </ref>. Second, we have already experimented with techniques for merging illustrations and text, so we already have mechanisms in place for carrying this out [84], [85].
Reference: [39] <author> Johnson-Laird, P.N. </author> <year> (1987), </year> <title> "The Mental Representation of the Meanings of Words," in A.I. </title> <editor> Goldman (ed.), </editor> <booktitle> Readings in Philosophy and Cognitive Science (MIT, </booktitle> <year> 1993): </year> <pages> 561-583. </pages>
Reference-contexts: Some of these are based on psycholinguistic studies of the sort of vocabulary expansion we are modeling [21], <ref> [39] </ref>, [86]. In the absence of some or all of this information, or in the presence of potentially inconsistent information (e.g., if the text says that one brachet hunts and another doesn't), Cassie either leaves certain "slots" in her definitional framework empty, or includes information about particular brachets. <p> Either approach, however, would be an improvement on our current system. 8.3 Compiling Definitions. It appears that humans don't store compiled definitions <ref> [39] </ref>. Rather, we link various aspects of meaning to words, and different portions of a word's meaning "come to mind" depending on the context in which they are used.
Reference: [40] <author> Kiersey, D.M. </author> <year> (1982), </year> <title> "Word Learning with Hierarchy Guided Inference," </title> <type> AAAI-82 : 172-178. </type>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], <ref> [40] </ref>, [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon. <p> For each domain, the initial KB consisted of a taxonomy of relevant objects and actions. Camille attempts to place the unknown word appropriately in the domain hierarchy. In this respect, Camille can be viewed as an implementation of the theory of <ref> [40] </ref>.
Reference: [41] <author> Lammens, J. </author> <year> (1994), </year> <title> "A Comp. Model of Color Perception and Color Naming," </title> <type> TR 94-26 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: For some adjectives, e.g., `humongous', morphological information might be of use (`humongous' might mean "large", since it seems to be a portmanteau of `huge' and `tremendous'). Definitions of color terms might be facilitated using the techniques from related work on color recognition and color naming <ref> [41] </ref>. Proper names, of interest in applications to reading news items, ought to be handleable by extending our noun-definition algorithm (perhaps to include slots for city/state/country, function, occupation, etc.). 8.7 Malformed Input and Misused Words.
Reference: [42] <author> Li, N., & Zubin, D.A. </author> <year> (1995), </year> <title> "Perspective Taking in Mandarin Discourse," </title> <booktitle> in [19]. </booktitle>
Reference: [43] <author> Malory, Sir T. (1470), Le Morte Darthur , ed. R.M. Lumiansky (Collier, </author> <year> 1982). </year>
Reference-contexts: At present, we have developed algorithms for hypothesizing and revising meanings for nouns and verbs that are unknown, mistaken, or being used in a new way. Cassie was 7 provided with background information for understanding the King Arthur stories in the Morte Darthur <ref> [43] </ref>. <p> Such information is filled in or replaced upon further encounters with the term. In another test, Cassie was told that `to smite' meant "to kill by hitting hard" (a mistaken belief actually held by the PI before reading <ref> [43] </ref>). Passages in which various characters were smitten but then continued to act triggered SNeBR, which asks the user (i.e., the reader) which of several possible "culprit" propositions in the KB to remove in order to block inconsistencies. Ideally, the reader then decides which belief to revise. <p> One significant improvement that we must undertake is the development of a generalized ATN grammar that can parse simplified forms of the sentences in <ref> [43] </ref> (cf. x6). The grammar for a previous version of SNePS (SNePS-79) that was capable of parsing a reasonable fragment of English directly into SNePS network representations needs to be updated for SNePS-2.1, and needs a capacity to add new words to the lexicon when they are encountered.
Reference: [44] <author> Mark, </author> <title> D.M., </title> & <address> Gould, M.D. </address> <year> (1995), </year> <title> "Wayfinding Directions as Discourse: Verbal Directions in English and Spanish," </title> <booktitle> in [19]. </booktitle>
Reference: [45] <author> Mark, D.M.; Svorou, S.; & Zubin, D.A. </author> <year> (1987), </year> <title> "Spatial Terms and Spatial Concepts: Geographic, </title> <booktitle> Cognitive, and Linguistic Perspectives," Proc. Int'l. Symposium Geographic Info. Systems: The Research Agenda (Crystal City, VA), </booktitle> <volume> Vol. 2: </volume> <pages> 101-112. </pages>
Reference-contexts: (1) It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], <ref> [45] </ref>, [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [46] <author> Martins, J., & Cravo M.R. </author> <year> (1991), </year> <title> "How to Change Your Mind," </title> <booktitle> No^us 25: </booktitle> <pages> 537-551. </pages>
Reference-contexts: We have developed algorithms for partially automating the identification and removal or modification of the offending premise, based on SNePSwD, a default belief-revision system that enables automatic revision [16], <ref> [46] </ref>. We propose to explore techniques for fully automating it.
Reference: [47] <author> Martins, J., & Shapiro, S.C. </author> <year> (1988), </year> <title> "A Model for Belief Revision," </title> <journal> Artif. Intell. </journal> <volume> 35: </volume> <pages> 25-79. </pages>
Reference-contexts: When certain combinations of asserted propositions lead to a contradiction, the SNeBR belief-revision package allows the user to remove from the context in which the contradiction arose one or more of the propositions from which the contradiction was derived <ref> [47] </ref>. Once the offending premise is no longer asserted, the conclusions that depended on it also cease to be asserted in that context.
Reference: [48] <author> Nakhimovsky, A., & Rapaport, W.J. </author> <year> (1988), </year> <title> "Discontinuities in Narratives," </title> <journal> COLING-88: </journal> <pages> 465-470. </pages>
Reference-contexts: It provides a software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], [45], <ref> [48] </ref>, [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [49] <author> Neal, J.G., & Shapiro, S.C. </author> <year> (1987), </year> <title> "Knowledge Based Parsing," </title> <editor> in L. Bolc (ed.), </editor> <booktitle> Natural Language Parsing Systems (Springer-Verlag): </booktitle> <pages> 49-92. </pages>
Reference: [50] <author> Peters, S.L., & Rapaport, W.J. </author> <year> (1990), </year> <title> "Superordinate and Basic Level Categories in Discourse: Memory and Context," </title> <booktitle> Proc. 12th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 157-165. </pages>
Reference: [51] <author> Peters, S.L., & Shapiro, S.C. </author> <year> (1987a), </year> <title> "A Representation for Natural Category Systems I," </title> <booktitle> Proc. 9th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 379-390. </pages>
Reference: [52] <author> Peters, S.L., & Shapiro, S.C. </author> <year> (1987b), </year> <title> "A Representation for Natural Category Systems II," </title> <type> IJCAI-87 : 140-146. </type>
Reference: [53] <author> Peters, S.L.; Shapiro, S.C.; & Rapaport, W.J. </author> <year> (1988), </year> <title> "Flexible Natural Language Processing and Roschian Category Theory," </title> <booktitle> Proc. 10th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 125-131. </pages>
Reference: [54] <author> Pustejovsky, J. </author> <year> (1987), </year> <title> "On the Acquisition of Lexical Entries: The Perceptual Origin of Thematic Relations," </title> <booktitle> Proc. 25th Annual Meeting Assoc. Comp. Ling.: </booktitle> <pages> 172-178. 24 </pages>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], <ref> [54] </ref>, [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon. <p> We propose to explore these possibilities further. 7.2 Pustejovksy <ref> [54] </ref> has discussed defining unknown verbs, using an "Extended Aspect Calculus", in terms of their associated case-roles or arguments, using certain semantic and syntactic markers, including a notion of canonical English word order.
Reference: [55] <author> Quillian, </author> <title> M.R. (1968), "Semantic Memory," </title> <editor> in M. Minsky (ed.) </editor> <booktitle> Semantic Information Processing (MIT): </booktitle> <pages> 227-270. </pages>
Reference-contexts: I.e., a word's meaning is its (syntactic) relation to other words, and words of similar meaning will have similar connections <ref> [55] </ref>, [56]. However, even words whose meanings are very similar to one another do not have identical connections to other terms and are, therefore, distinguishable from one another in the context of the agent's experience.
Reference: [56] <author> Quillian, </author> <title> M.R. (1969), "The Teachable Language Comprehender: A Simulation Program and Theory of Language," </title> <journal> Comm. ACM 12: </journal> <pages> 459-476. </pages>
Reference-contexts: I.e., a word's meaning is its (syntactic) relation to other words, and words of similar meaning will have similar connections [55], <ref> [56] </ref>. However, even words whose meanings are very similar to one another do not have identical connections to other terms and are, therefore, distinguishable from one another in the context of the agent's experience.
Reference: [57] <author> Quine, W.V.O. </author> <year> (1951), </year> <title> "Two Dogmas of Empiricism," in From a Logical Point of View, 2/e, rev. </title> <publisher> (Harvard, </publisher> <year> 1980): </year> <pages> 20-46. </pages>
Reference-contexts: We thus adopt Quine's view that the beliefs held by a cognitive agent form an interconnected web, where a change or addition to some portion of the web can affect other portions that are linked to it <ref> [57] </ref>. Such an agent's understanding of natural-language (NL) input will, therefore, be a part of such a web or semantic network composed of internal (mental) objects.
Reference: [58] <author> Rapaport, W.J. </author> <year> (1981), </year> <title> "How to Make the World Fit Our Language: An Essay in Meinongian Semantics," </title> <journal> Grazer Phil. </journal> <volume> Studien 14: </volume> <pages> 1-21. </pages>
Reference-contexts: As different uses of a word are heard or read, new contexts come into being. Since we take meaning to be context, an agent's understanding of a word's meaning is thus revised by successive encounters with it <ref> [58] </ref>. In this (idiolectic) sense, the meaning of a word for a cognitive agent is determined by idiosyncratic experience with it. In another sense, the meaning of a word is its dictionary definition, which usually contains less information than the idiolectic meaning.
Reference: [59] <author> Rapaport, W.J. </author> <year> (1986), </year> <title> "Logical Foundations for Belief Representation," Cog. </title> <journal> Sci. </journal> <volume> 10: </volume> <pages> 371-422. </pages>
Reference: [60] <author> Rapaport, W.J. </author> <year> (1988a), </year> <title> "Syntactic Semantics: Foundations of Comp. Natural-Language Understanding," </title> <editor> in J.H. Fetzer (ed.), </editor> <booktitle> Aspects of Artificial Intelligence (Kluwer): </booktitle> <pages> 81-131. </pages>
Reference-contexts: We plan also to investigate its applicability to computational lexicography: i.e., the automatic construction of dictionary entries given samples of use-in-context of words to be defined. 3 THEORETICAL BACKGROUND. Our implemented theory at least partially tests the thesis that symbol manipulation (syntax) suffices for NLU <ref> [60] </ref>, [63]. Humans understand one another by interpreting the symbols they read or hear. This interpretation is a mapping from the speaker's (or writer's) syntax to the hearer's (or reader's) concepts (semantics). <p> If we take these mental objects to be symbols, then the interpretation of linguistic input is a syntactic operation, and formal symbol manipulation is sufficient for attaching meanings to words <ref> [60] </ref>, [63]. Objects about which we can think, speak, or write need not exist. On a purely referential, or extensional, semantics, words for such non-existent objects would be meaningless. <p> We propose to explore techniques for fully automating it. SNePS also has an English lexicon, morphological analyzer/synthesizer, and a generalized ATN parser-generator that, rather than building an intermediate parse tree, translates the input English directly into a propositional semantic network ([72], [73]; see <ref> [60] </ref>, [62], [79] for detailed examples). 5 CURRENT STATUS. "Cassie", our vocabulary-expansion system, consists of SNePS-2.1 (including SNeBR and the ATN parser-generator), SNePSwD, and a knowledge base (KB) of background information. Currently, the KB is hand-coded, because it represents Cassie's antecedent knowledge; how she acquired this knowledge is irrelevant.
Reference: [61] <author> Rapaport, W.J. </author> <year> (1988b), </year> <title> "To Think or Not to Think," </title> <booktitle> No^us 22: </booktitle> <pages> 585-609. </pages>
Reference: [62] <author> Rapaport, W.J. </author> <year> (1991), </year> <title> "Predication, Fiction, </title> <journal> and Artificial Intelligence," </journal> <volume> Topoi 10: </volume> <pages> 79-111. </pages>
Reference-contexts: We propose to explore techniques for fully automating it. SNePS also has an English lexicon, morphological analyzer/synthesizer, and a generalized ATN parser-generator that, rather than building an intermediate parse tree, translates the input English directly into a propositional semantic network ([72], [73]; see [60], <ref> [62] </ref>, [79] for detailed examples). 5 CURRENT STATUS. "Cassie", our vocabulary-expansion system, consists of SNePS-2.1 (including SNeBR and the ATN parser-generator), SNePSwD, and a knowledge base (KB) of background information. Currently, the KB is hand-coded, because it represents Cassie's antecedent knowledge; how she acquired this knowledge is irrelevant. <p> The assignment of the kn cat story could be handled automatically: Cassie would simply include it as a part of each proposition built from the parse of a sentence in a story <ref> [62] </ref>, [64]. Since story-comp is only a stop-gap measure, we need not worry about how Cassie might assign it: Either she wouldn't make any such assignment, or she would tag all derived propositions as being derived as is already done by SNeBR.
Reference: [63] <author> Rapaport, W.J. </author> <year> (1995), </year> <title> "Understanding Understanding: Syntactic Semantics and Computational Cognition," </title> <editor> in J.E. Tomberlin (ed.), </editor> <booktitle> Philosophical Perspectives, </booktitle> <volume> Vol. 9 (Ridgeview), </volume> <pages> forthcoming. </pages>
Reference-contexts: We plan also to investigate its applicability to computational lexicography: i.e., the automatic construction of dictionary entries given samples of use-in-context of words to be defined. 3 THEORETICAL BACKGROUND. Our implemented theory at least partially tests the thesis that symbol manipulation (syntax) suffices for NLU [60], <ref> [63] </ref>. Humans understand one another by interpreting the symbols they read or hear. This interpretation is a mapping from the speaker's (or writer's) syntax to the hearer's (or reader's) concepts (semantics). <p> If we take these mental objects to be symbols, then the interpretation of linguistic input is a syntactic operation, and formal symbol manipulation is sufficient for attaching meanings to words [60], <ref> [63] </ref>. Objects about which we can think, speak, or write need not exist. On a purely referential, or extensional, semantics, words for such non-existent objects would be meaningless. However, in a syntactically based semantics, the linguistic contexts in which words for such objects are found can provide meanings for them.
Reference: [64] <author> Rapaport, W.J., & Shapiro, </author> <title> S.C. (1995), </title> <journal> "Cognition and Fiction," </journal> <note> in [19]. </note>
Reference-contexts: The proposed research will be an important contribution to this and similar projects, since to fully model a reader, it is important to model the ability to learn from reading ([62], <ref> [64] </ref>), in particular, to expand one's vocabulary in a natural way while reading, without having to stop to ask someone or to consult a dictionary. This research is also of independent significance. <p> The assignment of the kn cat story could be handled automatically: Cassie would simply include it as a part of each proposition built from the parse of a sentence in a story [62], <ref> [64] </ref>. Since story-comp is only a stop-gap measure, we need not worry about how Cassie might assign it: Either she wouldn't make any such assignment, or she would tag all derived propositions as being derived as is already done by SNeBR.
Reference: [65] <author> Rapaport, W.J.; Shapiro, S.C.; & Wiebe, J.M. </author> <year> (1986), </year> <title> "Quasi-Indicators, Knowledge Reports, and Discourse," </title> <type> TR 86-15 (Comp. </type> <institution> Sci., SUNY Buffalo); rev. </institution> <note> version, "Quasi-Indicators and Knowledge Reports," Cog. Sci., forthcoming. </note>
Reference-contexts: Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group [71], [76], [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], <ref> [65] </ref>, [72]-[73], [76]-[77], [79], [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics. Each node in a SNePS network represents a concept or mental object (possibly built of other concepts), with labeled arcs linking the concepts.
Reference: [66] <author> Rosch, E. </author> <year> (1978), </year> <title> "Principles of Categorization," </title> <editor> in E. Rosch & B.B. Lloyd (eds.), </editor> <publisher> Cognition and Categorization (Erlbaum): </publisher> <pages> 27-48. </pages>
Reference-contexts: Deciding what information to look for is largely controlled by what we know about class inclusion, especially whether the target noun represents a subordinate, superordinate, or basic-level category <ref> [66] </ref>. We infer this information to make certain that any implicit class inclusions are noticed by the system. Once the deduction is made, however, we search the network to abstract the classes. All other definitional information is also abstracted by finding specific paths in the network. 5.3 Verb-Defining Algorithm. <p> One such scheme already available is Conceptual Dependency's primitive acts [67]. Another possibility would be to attempt to develop a classification of verbs analogous to Rosch's basic-level, or subordinate categories <ref> [66] </ref>. If such a classification is possible, the concepts associated with Schank's primitive acts might be seen as superordinate categories, as would English words such as `move' or `go'. `Walk' might be a basic-level verb, while `amble', `pace', `stroll', `stride', and `hobble' might be considered subordinate level.
Reference: [67] <author> Schank, R.C., & Rieger, C.J. </author> <year> (1974), </year> <title> "Inference and the Computer Understanding of Natural Language," </title> <journal> Artif. Intell. </journal> <volume> 5: </volume> <pages> 373-412 </pages>
Reference-contexts: Compared with our noun-definition algorithm, our verb-definition algorithm is sketchy. It could be improved by developing synonym-finding procedures and by the use of some scheme for classifying verbs of different types. One such scheme already available is Conceptual Dependency's primitive acts <ref> [67] </ref>. Another possibility would be to attempt to develop a classification of verbs analogous to Rosch's basic-level, or subordinate categories [66].
Reference: [68] <author> Segal, E.M. </author> <year> (1995a), </year> <title> "Narrative Comprehension and the Role of Deictic Shift Theory," </title> <booktitle> in [19]. </booktitle>
Reference: [69] <author> Segal, E.M. </author> <year> (1995b), </year> <title> "Stories, Story Worlds, and Narrative Discourse," </title> <booktitle> in [19]. </booktitle>
Reference-contexts: E.g., taxonomies (e.g., dogs are a subclass of animals), assertions about individuals (e.g., Merlin is a wizard). 4. kn cat story-comp: Information not directly present in the story, but inferred by the reader to make sense of it. Such "story completion" <ref> [69] </ref> uses background knowledge, but isn't the background knowledge itself. Few (if any) assertions should be tagged with this kn cat, since any necessary story completion should (ideally) be derived by Cassie.
Reference: [70] <author> Segal, E.M.; Duchan, J.F.; & Scott, P.J. </author> <year> (1991), </year> <title> "The Role of Interclausal Connectives in Narrative Structuring: Evidence from Adults' Interpretations of Simple Stories," </title> <booktitle> Discourse Proc. </booktitle> <volume> 14: </volume> <pages> 27-54. </pages>
Reference-contexts: software laboratory for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], <ref> [70] </ref>, [73]-[77], [85], [90]-[92], [94] [96]). 2 SIGNIFICANCE.
Reference: [71] <author> Shapiro, S.C. </author> <year> (1979), </year> <title> "The SNePS Semantic Network Processing System," </title> <editor> in N. Findler (ed.), </editor> <booktitle> Associative Networks (Academic): </booktitle> <pages> 179-203. </pages>
Reference-contexts: Our system produces descriptive definitions (cf. x8.7). 4 IMPLEMENTATION. Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group <ref> [71] </ref>, [76], [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics.
Reference: [72] <author> Shapiro, S.C. </author> <year> (1982), </year> <title> "Generalized Augmented Transition Network Grammars for Generation from Semantic Networks," </title> <journal> American J. Comp. Ling. </journal> <volume> 8: </volume> <pages> 12-25. </pages>
Reference: [73] <author> Shapiro, S.C. </author> <year> (1989), </year> <title> "The CASSIE Projects: An Approach to Natural Language Competence," in J.P. Martins & E.M. </title> <editor> Morgado (eds.), </editor> <booktitle> Proc. 4th Portugese Conf. A.I. (EPIA-89), Lecture Notes in A.I. </booktitle> <publisher> 390 (Springer-Verlag): </publisher> <pages> 362-380. 25 </pages>
Reference-contexts: We propose to explore techniques for fully automating it. SNePS also has an English lexicon, morphological analyzer/synthesizer, and a generalized ATN parser-generator that, rather than building an intermediate parse tree, translates the input English directly into a propositional semantic network ([72], <ref> [73] </ref>; see [60], [62], [79] for detailed examples). 5 CURRENT STATUS. "Cassie", our vocabulary-expansion system, consists of SNePS-2.1 (including SNeBR and the ATN parser-generator), SNePSwD, and a knowledge base (KB) of background information.
Reference: [74] <author> Shapiro, S.C. </author> <year> (1991), </year> <title> "Cables, Paths, and `Subconscious' Reasoning in Propositional Semantic Networks," </title> <editor> in J. Sowa (ed.), </editor> <booktitle> Principles of Semantic Networks (Morgan Kaufmann): </booktitle> <pages> 137-156. </pages>
Reference: [75] <author> Shapiro, S.C., & Martins, J.P. </author> <year> (1990), </year> <title> "Recent Advances and Developments: The SNePS 2.1 Report," </title> <editor> in D. Kumar (ed.), </editor> <booktitle> Current Trends in SNePS , Lecture Notes in A.I. </booktitle> <publisher> 437 (Springer-Verlag): </publisher> <pages> 1-13. </pages>
Reference: [76] <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1987), </year> <title> "SNePS Considered as a Fully Intensional Propositional Semantic Network," </title> <editor> in N. Cercone & G. McCalla (eds.), </editor> <publisher> The Knowledge Frontier (Springer-Verlag): </publisher> <pages> 262-315. </pages>
Reference-contexts: Our system produces descriptive definitions (cf. x8.7). 4 IMPLEMENTATION. Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group [71], <ref> [76] </ref>, [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics. <p> This uniqueness principle guarantees 6 that nodes will be shared whenever possible and that nodes represent intensional objects such as concepts, propositions, properties, algorithms, and objects of thought including fictional entities (e.g., Sherlock Holmes), non-existents (e.g., the golden mountain), and impossible objects (e.g., the round square) <ref> [76] </ref>, [77]. SNePS's inference package allows one to write rules for ordinary deductive reasoning as well as for default reasoning, which allows the system to infer "probable" conclusions in the absence of specific information to the contrary.
Reference: [77] <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1991), </year> <title> "Models and Minds: Knowledge Representation for Natural-Language Competence," </title> <editor> in R. Cummins & J. Pollock (eds.), </editor> <publisher> Philosophy and AI (MIT): </publisher> <pages> 215-259. </pages>
Reference-contexts: This uniqueness principle guarantees 6 that nodes will be shared whenever possible and that nodes represent intensional objects such as concepts, propositions, properties, algorithms, and objects of thought including fictional entities (e.g., Sherlock Holmes), non-existents (e.g., the golden mountain), and impossible objects (e.g., the round square) [76], <ref> [77] </ref>. SNePS's inference package allows one to write rules for ordinary deductive reasoning as well as for default reasoning, which allows the system to infer "probable" conclusions in the absence of specific information to the contrary.
Reference: [78] <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1992), </year> <title> "The SNePS Family," </title> <booktitle> Computers and Mathematics with Applications 23: </booktitle> <pages> 243-275. </pages>
Reference-contexts: Our system produces descriptive definitions (cf. x8.7). 4 IMPLEMENTATION. Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group [71], [76], <ref> [78] </ref>, [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics.
Reference: [79] <author> Shapiro, S.C., & Rapaport, W.J. </author> <year> (1995), </year> <title> "An Introduction to a Comp. Reader of Narrative," </title> <booktitle> in [19]. </booktitle>
Reference-contexts: Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group [71], [76], [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], <ref> [79] </ref>, [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics. Each node in a SNePS network represents a concept or mental object (possibly built of other concepts), with labeled arcs linking the concepts. <p> We propose to explore techniques for fully automating it. SNePS also has an English lexicon, morphological analyzer/synthesizer, and a generalized ATN parser-generator that, rather than building an intermediate parse tree, translates the input English directly into a propositional semantic network ([72], [73]; see [60], [62], <ref> [79] </ref> for detailed examples). 5 CURRENT STATUS. "Cassie", our vocabulary-expansion system, consists of SNePS-2.1 (including SNeBR and the ATN parser-generator), SNePSwD, and a knowledge base (KB) of background information. Currently, the KB is hand-coded, because it represents Cassie's antecedent knowledge; how she acquired this knowledge is irrelevant.
Reference: [80] <author> Shapiro, </author> <title> S.C., & the SNePS Implementation Group (1994), </title> <note> "SNePS-2.1 User's Manual," SNeRG Tech. Note (Comp. </note> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: Our system produces descriptive definitions (cf. x8.7). 4 IMPLEMENTATION. Our system is being implemented in the SNePS-2.1 semantic-network knowledge-representation and reasoning (KRR) system developed by S.C. Shapiro and the SNePS Research Group [71], [76], [78], <ref> [80] </ref>. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], [96]- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics.
Reference: [81] <author> Siskind, J.M. </author> <year> (1992), </year> <title> "Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition," </title> <type> Ph.D. </type> <institution> diss. (Elec. Eng'g. & Comp. Sci., MIT). </institution>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], [54], <ref> [81] </ref>- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon.
Reference: [82] <author> Siskind, J.M. </author> <year> (1994), </year> <title> "Lexical Acquisition in the Presence of Noise and Homonymy," </title> <type> AAAI-94 : 760-766. </type>
Reference-contexts: The focus of the work is on the comparative precision and accuracy of the various versions of Camille as they attempt to map unknown terms onto known nodes. For us, such a notion of "correctness" does not apply. 7.4 Siskind <ref> [82] </ref>, [83] is concerned with learning the meanings of words, as we are, but his focus is on first-language acquisition during childhood, whereas ours is on relatively mature cognitive agents who already know a large part of their language and are (merely) expanding their vocabulary.
Reference: [83] <author> Siskind, J.M. </author> <title> (forthcoming), "A Computational Study of Lexical Acquisition" (ftp://ftp.cs.toronto.edu/pub/qobi/cog94.ps.Z). </title>
Reference-contexts: We plan to elaborate our theory of verb acquisition in the light of research on verb meanings [25], [26]. In computational linguistics, there have been many language acquisition and definition projects. In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], [54], [81]- <ref> [83] </ref>. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer [98] compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon. <p> The focus of the work is on the comparative precision and accuracy of the various versions of Camille as they attempt to map unknown terms onto known nodes. For us, such a notion of "correctness" does not apply. 7.4 Siskind [82], <ref> [83] </ref> is concerned with learning the meanings of words, as we are, but his focus is on first-language acquisition during childhood, whereas ours is on relatively mature cognitive agents who already know a large part of their language and are (merely) expanding their vocabulary.
Reference: [84] <author> Srihari, R.K. </author> <year> (1991), </year> <title> "PICTION: A System that Uses Captions to Label Human Faces in Newspaper Photographs," </title> <type> AAAI-91 : 80-85. </type>
Reference-contexts: Second, we have already experimented with techniques for merging illustrations and text, so we already have mechanisms in place for carrying this out <ref> [84] </ref>, [85]. Another difference between our systems is that Siskind's begins with a mapping between a whole meaning and a whole utterance, and infers mappings between their parts.
Reference: [85] <author> Srihari, R.K. & Rapaport, W.J. </author> <year> (1989), </year> <title> "Extracting Visual Information from Text: Using Captions to Label Human Faces in Newspaper Photographs," </title> <booktitle> Proc. 11th Annual Conf. Cog. </booktitle> <publisher> Sci. Soc.: </publisher> <pages> 364-371. </pages>
Reference-contexts: for testing and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], <ref> [85] </ref>, [90]-[92], [94] [96]). 2 SIGNIFICANCE. <p> Second, we have already experimented with techniques for merging illustrations and text, so we already have mechanisms in place for carrying this out [84], <ref> [85] </ref>. Another difference between our systems is that Siskind's begins with a mapping between a whole meaning and a whole utterance, and infers mappings between their parts.
Reference: [86] <author> Sternberg, R.J. </author> <year> (1987), </year> <title> "Most Vocabulary is Learned from Context," in M.G. McKeown & M.E. </title> <editor> Curtis (eds.), </editor> <booktitle> The Nature of Vocabulary Acquisition (Erlbaum): </booktitle> <pages> 89-105. </pages>
Reference-contexts: Some of these are based on psycholinguistic studies of the sort of vocabulary expansion we are modeling [21], [39], <ref> [86] </ref>. In the absence of some or all of this information, or in the presence of potentially inconsistent information (e.g., if the text says that one brachet hunts and another doesn't), Cassie either leaves certain "slots" in her definitional framework empty, or includes information about particular brachets.
Reference: [87] <author> Talmy, L. </author> <year> (1995), </year> <title> "A Heuristic Framework for Narrative Structure," </title> <booktitle> in [19]. </booktitle>
Reference: [88] <author> Webster's New Intl. </author> <title> Dictionary of the English Language (2/e, </title> <address> Unabridged) (Merriam, </address> <year> 1937). </year>
Reference-contexts: E.g., when presented with a sequence of passages involving the hitherto unknown noun `brachet', Cassie was able to develop a theory that a brachet was a dog whose function is to hunt and that can bay and bite. (Webster's Second <ref> [88] </ref> defines it as "a hound that hunts by the scent".) However, based on the first context in which the term appeared (viz., "Right so as they sat, there came a white hart running into the hall with a white brachet next to him, and thirty couples of black hounds came
Reference: [89] <author> Wiebe, J.M. </author> <year> (1990a), </year> <title> "Recognizing Subjective Sentences: A Computational Investigation of Narrative Text," </title> <type> TR 90-03 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference: [90] <author> Wiebe, J.M. </author> <year> (1990b), </year> <title> "Identifying Subjective Characters in Narrative," </title> <booktitle> COLING-90: </booktitle> <pages> 401-408. </pages>
Reference: [91] <author> Wiebe, J.M. </author> <year> (1991), </year> <title> "References in Narrative Text," </title> <booktitle> No^us 25: </booktitle> <pages> 457-486. 26 </pages>
Reference: [92] <author> Wiebe, J.M. </author> <year> (1994), </year> <title> "Tracking Point of View in Narrative," </title> <journal> Comp. Ling. </journal> <volume> 20: </volume> <pages> 233-287. </pages>
Reference: [93] <author> Wiebe, J.M., & Rapaport, W.J. </author> <year> (1986), </year> <title> "Representing De Re and De Dicto Belief Reports in Discourse and Narrative," </title> <booktitle> Proc. IEEE 74: </booktitle> <pages> 1405-1413. </pages>
Reference: [94] <author> Wiebe, J.M., & Rapaport, W.J. </author> <year> (1988), </year> <title> "A Computational Theory of Perspective and Reference in Narrative" Proc. </title> <booktitle> 26th Annual Meeting Assoc. Comp. Ling.: </booktitle> <pages> 131-138. </pages>
Reference-contexts: and experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], <ref> [94] </ref> [96]). 2 SIGNIFICANCE.
Reference: [95] <author> Wilkins, D. </author> <year> (1995), </year> <title> "Expanding the Traditional Category of Deictic Elements: </title> <note> Interjections as Deictics," in [19]. </note>
Reference: [96] <author> Yuhan, A.H. </author> <year> (1991), </year> <title> "Dynamic Computation of Spatial Reference Frames in Narrative Understanding," </title> <type> TR 91-03 (Comp. </type> <institution> Sci., SUNY Buffalo). </institution>
Reference-contexts: experimenting with the detailed implementation of our theory. (2) It is a component of a larger project to develop a computational cognitive model of a reader of narrative text, summarized in [19] (see also: [1] [5], [7], [17], [19], [22], [35], [45], [48], [50]-[53], [60]-[62], [70], [73]-[77], [85], [90]-[92], [94] <ref> [96] </ref>). 2 SIGNIFICANCE. <p> Shapiro and the SNePS Research Group [71], [76], [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], <ref> [96] </ref>- [97]. SNePS is an appropriate KRR system for our approach to lexical semantics. Each node in a SNePS network represents a concept or mental object (possibly built of other concepts), with labeled arcs linking the concepts.
Reference: [97] <author> Yuhan, A.H., & Shapiro, </author> <title> S.C. </title> <booktitle> (1995), "Computational Representation of Space," in [19]. </booktitle>
Reference-contexts: Shapiro and the SNePS Research Group [71], [76], [78], [80]. SNePS has been and is being used for a number of research projects in NLU [1]-[2], [48]-[53], [59]-[60], [62]-[63], [65], [72]-[73], [76]-[77], [79], [89]-[94], [96]- <ref> [97] </ref>. SNePS is an appropriate KRR system for our approach to lexical semantics. Each node in a SNePS network represents a concept or mental object (possibly built of other concepts), with labeled arcs linking the concepts.
Reference: [98] <author> Zernik, U., & Dyer, M.G. </author> <year> (1987), </year> <title> "The Self-Extending Phrasal Lexicon," </title> <journal> Comp. Ling. </journal> <volume> 13: </volume> <pages> 308-327. </pages>
Reference-contexts: This research is also of independent significance. It is part of the task of lexical processing of text, dealing, in particular, with the need to process new or unknown words. As pointed out in <ref> [98] </ref>, no assumption of a "fixed complete lexicon" can be made: It could not be manually encoded, nor could it contain neologisms or new meanings given to old words in new contexts. <p> In addition to those already cited, there are: [3], [4], [9], [10], [27]-[33], [40], [54], [81]- [83]. There are, of course, many more. In this section, we can only briefly discuss a few of these. 7.1 Zernik & Dyer <ref> [98] </ref> compiles definitions of words and phrases, especially figurative phrases, from conversation into a hierarchical lexicon. Figurative phrases are linguistic patterns whose meaning cannot be understood by composition of the meanings of their component words (e.g., "to put one's foot down").
Reference: [99] <author> Zubin, D., & Hewitt, L.E. </author> <year> (1995), </year> <title> "The Deictic Center: A Theory of Deixis in Narrative," </title> <booktitle> in [19]. </booktitle>
References-found: 99

