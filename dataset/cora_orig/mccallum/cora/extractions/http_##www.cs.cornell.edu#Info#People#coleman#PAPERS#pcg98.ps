URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/pcg98.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: A Preconditioned Conjugate Gradient Approach to Linear Equality Constrained Minimization  
Author: Thomas F. Coleman Arun Verma 
Date: July 1, 1998  
Abstract: We propose a new framework for the application of preconditioned conjugate gradients in the solution of large-scale linear equality constrained minimization problems. This framework allows for the exploitation of structure and sparsity in the context of solving the reduced Newton system (despite the fact that the reduced system may be dense). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adams and J. Nazareth, </author> <title> Linear and Nonlinear Conjugate Gradient-Related Methods, </title> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: If negative curvature is discovered, i.e., fl 0, the PCG output contains a direction of infinite descent. (A good place to start for references and background on conjugate gradient methods is the proceedings edited by Adams and Nazareth <ref> [1] </ref>.) The "art" of PCG is in the preconditioning step, z P (r): effective preconditioners are often tailored to the problem at hand.
Reference: [2] <author> T. F. Coleman, </author> <title> Linearly constrained optimization and projected preconditioned conjugate gradients, </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994, </year> <pages> pp. 118-122. </pages>
Reference-contexts: There are many possible ways to choose C: most depend on sparsity or structure inherent in H. For the linearly constrained problem (1), Coleman <ref> [2] </ref> suggests that algorithm PCG can be used if the preconditioning step, z P (r), is implemented in a special way.
Reference: [3] <author> T. F. Coleman and A. Pothen, </author> <title> The null space problem I: Complexity, </title> <journal> SIAM J. Alg. & Disc. Meth., </journal> <volume> 7 (1987), </volume> <pages> pp. </pages> <month> 527-537. </month> <title> [4] , The null space problem II: Algorithms, </title> <journal> SIAM J. Alg. & Disc. Meth., </journal> <volume> 8 (1987), </volume> <pages> pp. 544-563. </pages>
Reference-contexts: For additional discussions on fundamental bases, and alternatives, see <ref> [3, 4, 7] </ref>. For simplicity of this presentation we assume P = I n in (5).
Reference: [5] <author> T. F. Coleman and A. Verma, </author> <title> Structure and efficient Jacobian calculation, in Computa tional Differentiation: Techniques, Applications, and Tools, </title> <editor> M. Berz, C. Bischof, G. Corliss, and A. Griewank, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1996, </year> <pages> pp. 149-159. </pages>
Reference-contexts: To illustrate, suppose that the Hessian matrix H has the form, H = GE 1 G T , where E is sparse, symmetric, and positive definite; matrix G is sparse and of full row rank. This structure is common, e.g., <ref> [5, 6] </ref>. <p> This is an example of how our proposed framework can be adapted to structured problems. A similar approach can be used for the general structures discussed in <ref> [5, 6] </ref>. Finally, we remark that our position in this paper is to consider a framework for the preconditioning of a conjugate gradient approach to (1). Restriction to conjugate gradient processes has the advantage that feasible descent directions are always generated this is particularily important for nonlinear problems.
Reference: [6] <author> T. F. Coleman and A. Verma, </author> <title> Structure and efficient Hessian calculation, </title> <booktitle> in Advances in Nonlinear Programming, Proceedings of the 1996 International Conference on Nonlinear Programming, </booktitle> <editor> Y.-X. Yuan, ed., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1998. </year>
Reference-contexts: To illustrate, suppose that the Hessian matrix H has the form, H = GE 1 G T , where E is sparse, symmetric, and positive definite; matrix G is sparse and of full row rank. This structure is common, e.g., <ref> [5, 6] </ref>. <p> This is an example of how our proposed framework can be adapted to structured problems. A similar approach can be used for the general structures discussed in <ref> [5, 6] </ref>. Finally, we remark that our position in this paper is to consider a framework for the preconditioning of a conjugate gradient approach to (1). Restriction to conjugate gradient processes has the advantage that feasible descent directions are always generated this is particularily important for nonlinear problems.
Reference: [7] <author> J. R. Gilbert and M. Heath, </author> <title> Computing a sparse basis for the nullspace, </title> <journal> SIAM J. Alg. & Disc. Meth., </journal> <volume> 8 (1987), </volume> <pages> pp. 446-459. </pages>
Reference-contexts: For additional discussions on fundamental bases, and alternatives, see <ref> [3, 4, 7] </ref>. For simplicity of this presentation we assume P = I n in (5).
Reference: [8] <author> P. Gill, W. Murray, D. Ponceleon, and M. Saunders, </author> <title> Preconditioners for indefinite systems arising in optimization, </title> <note> SIAM J. Matrix Anal. Appl., 13 (1992), pp. 292 -311. 10 [9] J. J. More, </note> <author> B. S. Garbow, and K. H. Hillstrom, </author> <title> Testing unconstrained optimization software, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. 17-41. </pages>
Reference-contexts: Restriction to conjugate gradient processes has the advantage that feasible descent directions are always generated this is particularily important for nonlinear problems. Another iterative approach to (1) is to consider the (symmet-ric indefinite) system of equations defining the optimality conditions and to apply symmetric indefinite iterative techniques, e.g., <ref> [8] </ref>. We have not considered such methods here.
Reference: [10] <author> S. G. Nash and A. Sofer, </author> <title> Preconditioning of reduced matrices, </title> <type> Tech. Rep. 93-01, </type> <institution> Dept. of Operations Research and Engineering, George Mason University, </institution> <year> 1993. </year>
Reference-contexts: Moreover, most traditional preconditioning strategies also require the explicit formulation of matrix H this can be a non-trivial expense. (Nash and Sofers <ref> [10] </ref> have proposed an approach based on an approximation to (4).) Our proposed method fits within the RPCG framework defined above. The output includes a vector p, an approximate solution to (3) or a feasible direction of negative curvature.
References-found: 8

