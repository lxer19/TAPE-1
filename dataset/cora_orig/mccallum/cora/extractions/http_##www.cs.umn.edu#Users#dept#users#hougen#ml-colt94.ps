URL: http://www.cs.umn.edu/Users/dept/users/hougen/ml-colt94.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/hougen/
Root-URL: http://www.cs.umn.edu
Email: hougen@cs.umn.edu jfischer@cs.umn.edu johnam@cs.umn.edu  
Title: A Neural Network Pole Balancer that Learns and Operates on a Real Robot in Real Time  
Author: Dean Hougen John Fischer Deva Johnam 
Address: Building 200 Union St. SE, Minneapolis, MN 55455  
Affiliation: Artificial Intelligence, Robotics, and Vision Laboratory Department of Computer and Information Sciences University of Minnesota 4-192 Electrical Engineering and Computer Science  
Abstract: A neural network approach to the classic inverted pendulum task is presented. This task is the task of keeping a rigid pole, hinged to a cart and free to fall in a plane, in a roughly vertical orientation by moving the cart horizontally in the plane while keeping the cart within some maximum distance of its starting position. This task constitutes a difficult control problem if the parameters of the cart-pole system are not known precisely or are variable. It also forms the basis of an even more complex control-learning problem if the controller must learn the proper actions for successfully balancing the pole given only the current state of the system and a failure signal when the pole angle from the vertical becomes too great or the cart exceeds one of the boundaries placed on its position. The approach presented is demonstrated to be effective for the real-time control of a small, self-contained mini-robot, specially outfitted for the task. Origins and details of the learning scheme, specifics of the mini-robot hardware, and results of actual learning trials are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Anderson. </author> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> Vol. 9, No. 3, </volume> <pages> 31-37, </pages> <year> 1989. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. <ref> [1] </ref>, [3], [5], [7], [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> In these authors systems (e.g. [3], [15], [19], and [21]) the input space is partitioned by the researcher, rather than by the learning system. This head start makes a significant difference. In one of the few instances where the entire pole-balancing problem is approached <ref> [1] </ref>, a single layer neural network that can learn to balance a pole quite pole shaft variable resister transmitter receiver transmitter receiver (a) wheel disk (b) (d) IR modules well when the input space partitioning is precomputed by the author, fails to learn much beyond the level of random control when
Reference: [2] <author> C. Anderson and W. Miller. </author> <title> Challenging Control Problems, </title> <booktitle> in Neural Networks for Control, </booktitle> <pages> 475-510. </pages> <editor> W. Miller, R. Sutton, and P. Werbos, eds., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: For simulation results, a standard (as presented in <ref> [2] </ref>) is generally, but not always (e.g [9]), followed. For actual cart-pole systems, the parameters are naturally quite variable. For our cart-pole system, many of the system parameters are known only very roughly and others are unknown.
Reference: [3] <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. SMC-13, </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], <ref> [3] </ref>, [5], [7], [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> Others (e.g. [5]) have been forced to give their systems hints in order for them to solve the problem. Most systems which are said to solve the pole-balancing problem without teachers or hints really do no such thing. In these authors systems (e.g. <ref> [3] </ref>, [15], [19], and [21]) the input space is partitioned by the researcher, rather than by the learning system. This head start makes a significant difference.
Reference: [4] <author> R. Cannon. </author> <title> Dynamics of Physical Systems, </title> <address> 703-710. </address> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1967. </year>
Reference: [5] <author> M. Connell and P. Utgoff. </author> <title> Learning to control a dynamic physical system, </title> <booktitle> in Proceedings AAAI-87, </booktitle> <volume> Vol. 2, </volume> <pages> 456-460. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <address> Seattle, </address> <year> 1987. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], <ref> [5] </ref>, [7], [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> There systems are completely incapable of solving the pole-balancing problem without this outside aid. Others (e.g. <ref> [5] </ref>) have been forced to give their systems hints in order for them to solve the problem. Most systems which are said to solve the pole-balancing problem without teachers or hints really do no such thing.
Reference: [6] <author> P. Donaldson. </author> <title> Error decorrelation: a technique for matching a class of functions, </title> <booktitle> in Proceedings: III International Conference on Medical Electronics, </booktitle> <pages> 173-178, </pages> <year> 1960. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago <ref> [6] </ref>, [23] and many more have been developed since (e.g. [1], [3], [5], [7], [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem.
Reference: [7] <author> A. Guez and J. Selinsky. </author> <title> A trainable neuromorphic controller. </title> <journal> Journal of Robotic Systems, </journal> <volume> Vol. 5, No. 4, </volume> <pages> 363-388, </pages> <year> 1988. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], <ref> [7] </ref>, [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> of the system to learn is not dependent on the initial weight values. 5.3 Discussion Because of the difficulty of this problem, many authors (e.g.<ref> [7] </ref>, [8], [22], and [23]) have constructed systems which learn by watching the actions of a who person controls the cart-pole system and a few (e.g. [7] and [9]) have made systems which learn by observing the cart-pole system as it is controlled by another computer program. There systems are completely incapable of solving the pole-balancing problem without this outside aid.
Reference: [8] <author> A. Guez and J. Selinsky. </author> <title> A neuromorphic contoller with a human teacher, </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <volume> Vol. 2, </volume> <pages> 595-602, </pages> <year> 1988. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], [7], <ref> [8] </ref>, [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> The fact that the average trial time stayed constant regardless of the values of the random initial weights demonstrates that the ability to of the system to learn is not dependent on the initial weight values. 5.3 Discussion Because of the difficulty of this problem, many authors (e.g.[7], <ref> [8] </ref>, [22], and [23]) have constructed systems which learn by watching the actions of a who person controls the cart-pole system and a few (e.g. [7] and [9]) have made systems which learn by observing the cart-pole system as it is controlled by another computer program.
Reference: [9] <author> D. Handelman and S. Lane. </author> <title> Fast sensorimotor skill acquisition based on rule-based training of neural networks, in Neural Networks in Robotics, </title> <editor> G. Bekey and K. Goldberg, eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], [7], [8], <ref> [9] </ref>, [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> For simulation results, a standard (as presented in [2]) is generally, but not always (e.g <ref> [9] </ref>), followed. For actual cart-pole systems, the parameters are naturally quite variable. For our cart-pole system, many of the system parameters are known only very roughly and others are unknown. <p> system to learn is not dependent on the initial weight values. 5.3 Discussion Because of the difficulty of this problem, many authors (e.g.[7], [8], [22], and [23]) have constructed systems which learn by watching the actions of a who person controls the cart-pole system and a few (e.g. [7] and <ref> [9] </ref>) have made systems which learn by observing the cart-pole system as it is controlled by another computer program. There systems are completely incapable of solving the pole-balancing problem without this outside aid.
Reference: [10] <author> R. Hecht-Nielsen. </author> <title> Counterpropagation networks. </title> <journal> Applied Optics, </journal> <volume> Vol. 26, No. 23, </volume> <pages> 4979-4984, </pages> <year> 1987. </year>
Reference-contexts: This idea has, in fact, been used to create networks which can learn to self-organize output as well as input (e.g. <ref> [10] </ref>, [17], and [18]). The limiting factor to these approaches is that they require a teacher be available to give the network a correct answer.
Reference: [11] <author> D. Hougen. </author> <title> Use of an eligibility trace to self-organize output, </title> <booktitle> in Science of Artificial Neural Networks II, </booktitle> <editor> D. Ruck, ed., </editor> <booktitle> SPIE 1966, </booktitle> <pages> 436-447, </pages> <year> 1993. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], [7], [8], [9], <ref> [11] </ref>, [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> the state vector was restricted to approximations of the cart position and pole angle. (Standard versions of the pole-balancing problem provide the controller with cart velocity and pole angular velocity as well.) 3 SONNET The learning system follows the Self-Organizing Neural Network with Eligibility Traces (SONNET) paradigm first described in <ref> [11] </ref>. <p> A SONNET system (known as PBMax after its intended task and particulars of its computations) was applied to a standard simulation of the pole-balancing problem <ref> [11] </ref>. <p> layer network takes, on the average, nearly 100 times as long to learn when the input space partitioning was not provided for it. (The only other case that we are aware of where learning pole-balancing is attempted without a pre-partitioning of the input space is in the first SONNET paper <ref> [11] </ref>.) PBMin is applied to the entire pole-balancing problem; it needs to dynamically partition the input space while learning the output space. To reduce the computational load on the processor, however, it was necessary to restrict the input vector to two dimensions.
Reference: [12] <author> T. Jervis and F. Fallside. </author> <title> Pole balancing on a real rig using a reinforcement learning controller. </title> <type> Technical Report CUED/F-INFENG/TR115, </type> <institution> Cam-bridge University Engineering Department, </institution> <address> Cam-bridge, England. </address>
Reference-contexts: What is likely the first control system to learn to balance a pole in an actual physical system without the aid of an outside teacher was only recently developed (see <ref> [12] </ref>). Yet even this system does not solve the complete problem on its own; fully half the problem is solved for it before it even begins the training process (see Subsection 5.3 Discussion). <p> These elements of the state vector must be taken into account if long-term stability is to be achieved. In the only previous paper in which researchers attempt to apply a control-learning scheme to a real cart-pole system <ref> [12] </ref>, the input space is partitioned by the researches, rather than by the learning system. Because of this, and because of differences between their cart-pole system and the one presented here, direct comparisons between results obtained with the two systems may be misleading.
Reference: [13] <author> A. Klopf. </author> <title> Brain function and adaptive systems -- a heterostatic theory, </title> <booktitle> in Proceedings of the International Conference on Systems, Man, and Cybernetics, </booktitle> <year> 1974. </year>
Reference-contexts: Kohonen Maps have been used extensively as pattern classifiers. 3.2 The Eligibility Trace Biological neurons are highly complex and their functions are only very roughly approximated by the artificial neurons in todays connectionist systems. One function found in biological neural networks, as noted in <ref> [13] </ref>, is what we refer to here as the eligibility trace. This function is the result of neurons becoming more amenable to change immediately after they fire. This plasticity reduces with time, but provides an opportunity for learning based on feedback following the neurons activity.
Reference: [14] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory, </title> <booktitle> Ch.5. 3rd. </booktitle> <publisher> ed., Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: The SONNET paradigm combines the self-organizing behavior of Kohonens Self-Organizing Topological Feature Maps (Kohonen Maps, see <ref> [14] </ref>) with the concept of an eligibility trace (see Section 3.2 The Eligibility Trace) to create a class of novel and powerful control-learning systems. A SONNET system (known as PBMax after its intended task and particulars of its computations) was applied to a standard simulation of the pole-balancing problem [11]. <p> 3.1 Self-Organizing Maps Kohonen has proposed as set of connectionist systems based the recognition that in biological neural networks (i.e. brains) there are regions (especially in the cerebral cortex) for which topology preserving relations exist between patterns of input from sensory organs and the physical arrangement of the neurons themselves <ref> [14, pp.119-122] </ref>. These areas provide efficient representations for interrelated data. Kohonen Maps, then, are a class of conceptually similar artificial maps that use several simple techniques (such as lateral inhibition) in concert to achieve the same general effects as those found in biological systems.
Reference: [15] <author> D. Michie and R. Chambers. </author> <title> Boxes: An experiment in adaptive control, </title> <booktitle> in Machine Intelligence, </booktitle> <editor> E. Dale and D. Michie, eds., </editor> <publisher> Oliver and Boyd, Edinburgh, </publisher> <year> 1968. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], [7], [8], [9], [11], <ref> [15] </ref>, and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> Others (e.g. [5]) have been forced to give their systems hints in order for them to solve the problem. Most systems which are said to solve the pole-balancing problem without teachers or hints really do no such thing. In these authors systems (e.g. [3], <ref> [15] </ref>, [19], and [21]) the input space is partitioned by the researcher, rather than by the learning system. This head start makes a significant difference.
Reference: [16] <author> K. Ogata. </author> <title> System Dynamics, </title> <address> 531-536. </address> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1978 </year>
Reference-contexts: Variously known as the inverted pendulum problem, pole-balancing, stick-balancing, or broom-balancing, this task is a classic object of study in both system dynamics (where the equations of its motion are of interest)[4], <ref> [16] </ref> and control theory (where control systems capable of balancing the pole are of interest)[20]. The dynamics are by now well understood, but they provide a difficult task nonetheless, as the system is inherently unstable.
Reference: [17] <author> H. Ritter and K. Schulten. </author> <title> Extending Kohonens self-organizing mapping algorithm to learn balistic movements, in Neural Computers, </title> <editor> R. Eckmiller and C. von der Malsberg, eds., </editor> <volume> Vol. F41, </volume> <pages> 393-406. </pages> <publisher> Springer, </publisher> <address> Heidelberg, </address> <year> 1987. </year>
Reference-contexts: This idea has, in fact, been used to create networks which can learn to self-organize output as well as input (e.g. [10], <ref> [17] </ref>, and [18]). The limiting factor to these approaches is that they require a teacher be available to give the network a correct answer.
Reference: [18] <author> H. Ritter, T. Marinetz, and K. Schulten. </author> <title> Topology conserving maps for learning visio-motor-coordination. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 2, No. 3, </volume> <year> 1989. </year>
Reference-contexts: This idea has, in fact, been used to create networks which can learn to self-organize output as well as input (e.g. [10], [17], and <ref> [18] </ref>). The limiting factor to these approaches is that they require a teacher be available to give the network a correct answer.
Reference: [19] <author> C. Sammut. </author> <title> Experimental results from an evaluation of algorithms that learn to control dynamic systems, </title> <booktitle> in Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 437-443. </pages> <publisher> Mor-gan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Systems that can learn to balance poles were first developed over thirty years ago [6], [23] and many more have been developed since (e.g. [1], [3], [5], [7], [8], [9], [11], [15], and <ref> [19] </ref>). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> Others (e.g. [5]) have been forced to give their systems hints in order for them to solve the problem. Most systems which are said to solve the pole-balancing problem without teachers or hints really do no such thing. In these authors systems (e.g. [3], [15], <ref> [19] </ref>, and [21]) the input space is partitioned by the researcher, rather than by the learning system. This head start makes a significant difference.
Reference: [20] <author> J. Schafer and R. Cannon. </author> <title> On the control of unstable mechanical systems, in Automatic and Remote Control III: </title> <booktitle> Proceedings of the Third Congress of the International Federation of Automatic Control, Paper 6C, </booktitle> <year> 1966. </year>
Reference: [21] <author> O. Selfridge, R. Sutton, and A. Barto. </author> <title> Training and tracking in robotics, </title> <booktitle> in Proceedings IJCAI-85, </booktitle> <pages> 670-672. </pages> <booktitle> International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, </address> <year> 1985. </year>
Reference-contexts: Others (e.g. [5]) have been forced to give their systems hints in order for them to solve the problem. Most systems which are said to solve the pole-balancing problem without teachers or hints really do no such thing. In these authors systems (e.g. [3], [15], [19], and <ref> [21] </ref>) the input space is partitioned by the researcher, rather than by the learning system. This head start makes a significant difference.
Reference: [22] <author> V. Tolat and B. Widrow. </author> <title> An adaptive broom balancer with visual inputs, </title> <booktitle> in IEEE International Conference on Neural Networks, </booktitle> <volume> Vol. 2, </volume> <pages> 641-647, </pages> <address> San Diego, </address> <year> 1988. </year>
Reference-contexts: The fact that the average trial time stayed constant regardless of the values of the random initial weights demonstrates that the ability to of the system to learn is not dependent on the initial weight values. 5.3 Discussion Because of the difficulty of this problem, many authors (e.g.[7], [8], <ref> [22] </ref>, and [23]) have constructed systems which learn by watching the actions of a who person controls the cart-pole system and a few (e.g. [7] and [9]) have made systems which learn by observing the cart-pole system as it is controlled by another computer program.
Reference: [23] <author> B. Widrow. </author> <title> The original adaptive neural net broom-balancer, </title> <booktitle> in International Symposium on Circuits and Systems, </booktitle> <pages> 351-357, </pages> <year> 1987. </year>
Reference-contexts: The pole-balancing task is quickly becoming a classic object of study in the theory of control-learning, as well. Systems that can learn to balance poles were first developed over thirty years ago [6], <ref> [23] </ref> and many more have been developed since (e.g. [1], [3], [5], [7], [8], [9], [11], [15], and [19]). All of these systems have fallen 4.0m-4.0m q short of fully solving the inverted pendulum problem (see Subsection 5.3 Discussion) due to the difficult nature of the problem. <p> fact that the average trial time stayed constant regardless of the values of the random initial weights demonstrates that the ability to of the system to learn is not dependent on the initial weight values. 5.3 Discussion Because of the difficulty of this problem, many authors (e.g.[7], [8], [22], and <ref> [23] </ref>) have constructed systems which learn by watching the actions of a who person controls the cart-pole system and a few (e.g. [7] and [9]) have made systems which learn by observing the cart-pole system as it is controlled by another computer program.
References-found: 23

