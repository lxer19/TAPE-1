URL: http://www.tns.lcs.mit.edu/~djw/library/sigcomm96/mosberger.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/sigcomm96/program.html
Root-URL: 
Email: fdavidm,llp,bridges,seang@cs.arizona.edu  
Title: Analysis of Techniques to Improve Protocol Processing Latency  
Author: David Mosberger Larry L. Peterson Patrick G. Bridges Sean O'Malley 
Address: Tucson, AZ 85716  
Affiliation: Department of Computer Science The University of Arizona  
Abstract: This paper describes several techniques designed to improve protocol latency, and reports on their effectiveness when measured on a modern RISC machine employing the DEC Alpha processor. We found that the memory systemwhich has long been known to dominate network throughputis also a key factor in protocol latency. As a result, improving instruction cache effectiveness can greatly reduce protocol processing overheads. An important metric in this context is the memory cycles per instructions (mCPI), which is the average number of cycles that an instruction stalls waiting for a memory access to complete. The techniques presented in this paper reduce the mCPI by a factor of 1.35 to 5.8. In analyzing the effectiveness of the techniques, we also present a detailed study of the protocol processing behavior of two protocol stacksTCP/IP and RPCon a modern RISC processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AMD. Am7990: </author> <title> Local Area Network Controller for Ethernet. </title>
Reference-contexts: Careful inlining limits the use of inlining to the cases that will result in improved performance even for latency sensitive code. This is in contrast to the blind inlining that is often used when optimizing execution in tight loops. * Fixing machine idiosyncrasies: The LANCE <ref> [1] </ref> Ethernet adapter present in the test machines employs a DMA engine that results in memory being used sparsely: every two bytes of data is followed by a 2 byte gap. The Universal Stub Compiler [24] was used to allow accessing such memory efficiently and conveniently. <p> While end-to-end latency improvements are certainly respectable, they are nevertheless fractional on the given test system. It is important to keep in mind, however, that modern high-performance network adapters have much lower latency than the LANCE Ethernet adapter present in the DEC 3000 system <ref> [1] </ref>. To put this into perspective, consider that a minimumsized Ethernet packet is 64 bytes long, to which an 8 byte long preamble is added. At the speed of a 10Mbps Ethernet, transmitting the frame takes 57:6s.
Reference: [2] <author> M. L. Bailey, B. Gopal, M. A. Pagels, L. L. Peterson, and P. Sarkar. PathFinder: </author> <title> A pattern-based packet classifier. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 115-123, </pages> <year> 1994. </year>
Reference-contexts: We have designed and implemented a new OS, called Scout, that extends the x-kernel by adding an explicit path abstraction [21]. An essential component of Scout is a packet classifier that determines which path a given packet will traverse <ref> [2, 18, 33, 9] </ref>. Thus, such a system provides exactly the information necessary to use cloning and path-inlining. 4 Evaluation This section evaluates the techniques presented in the previous section. <p> Note that path-inlining and cloning require running a packet classifier on incoming packets since the optimized code is no longer general enough to handle all possible packets. Currently, the best software classifiers add an overhead of about 1 4s per packet <ref> [2, 9] </ref>. The results presented in this section do not include the time required to classify packets. This seems justified since systems commonly use classifiers to improve functionality, flexibility, and accountability [16, 21].
Reference: [3] <author> D. Bhandarkar and D. W. Clark. </author> <title> Performance from architecture: Comparing a RISC and CISC with similar hardware organization. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 310-319. </pages>
Reference-contexts: First, consider the results for tcp input. The DEC Unix trace is roughly 50% longer than the 80386 count. Such a code inflation is not uncommon when converting CISC code to RISC code, especially considering that the traced Alpha code does not have sub-word loads and stores available <ref> [3] </ref>. Second, comparing the ipintr results, we notice that IP input processing on the Alpha appears to be more than a factor of four longer than on the 80386. We believe this to be more an artifact of how the counting was performed rather than a real difference.
Reference: [4] <author> C. Castelluccia, W. Dabbous, and S. O'Malley. </author> <title> Generating efficient protocol code from an abstract specification. </title> <booktitle> In Proceedings of SIGCOMM '96 Symposium, to appear Aug. </booktitle> <year> 1996. </year>
Reference-contexts: Note that this works focuses on networking code as currently deployed, that is, for code written in C. We do not propose a new programming language or paradigm for protocol implementation, although we observe that some of the proposed techniques have also proven useful in alternative protocol implementation languages <ref> [4] </ref>. The paper is organized as follows. Section 2 sets the context in which this research was performed. In doing so, it expands earlier studies on TCP/IP latency with results for a modern RISC machine.
Reference: [5] <author> J. B. Chen and B. N. Bershad. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <year> 1993. </year>
Reference-contexts: The bottom-line is that we evaluate protocol latency in terms of memory cycles per instruction (mCPI), a metric that will become increasingly important as improvements in memory speed lag farther behind improvements in processor speed <ref> [5, 29] </ref>. It should be clear from these three points that memory bandwidthand in particular, the memory cycles required by each instructionis a central focus of this paper.
Reference: [6] <author> D. D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overheads. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common networking protocols, such as TCP/IP <ref> [15, 6, 14] </ref> and RPC [32]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. fl O'Malley's current address is Network Appliance, 319 N. Bernardo Ave., Mountain View, CA 94043. <p> Section 3 describes and discusses the latency improvement techniques which are then evaluated in Section 4. Section 5 offers some concluding remarks. 2 Preliminaries This section sets the context in which this research was performed. It first describes the experimental testbed, and then updates earlier TCP/IP results reported in <ref> [6] </ref> with measured results for a modern RISC workstation. <p> This facility allows collecting instruction traces of actual working code. 1 Combined with execution times obtained using the CPU's cycle counter, this allows a detailed comparison of processing overheads. It also provides us with the opportunity to update <ref> [6] </ref> with results for a modern RISC workstation. The results reported in Table 1 are for the case where a one byte TCP segment arrives on a bidirectional connection. This is in contrast to [6] where the focus was on unidirectional connections (as is the case for an ftp transfer, for <p> It also provides us with the opportunity to update <ref> [6] </ref> with results for a modern RISC workstation. The results reported in Table 1 are for the case where a one byte TCP segment arrives on a bidirectional connection. This is in contrast to [6] where the focus was on unidirectional connections (as is the case for an ftp transfer, for example). <p> The distinction between uni- and bidirectional data connections matters for two reasons. First, with a bidirectional connection, both end-hosts perform sender and receiver-related housekeeping. With a unidirectional connection, each host performs one function, but not the other. Second, the DEC Unix implementation uses header-prediction <ref> [6] </ref>. This is an optimization primarily targeted at improving latency. Unfortunately, TCP header-prediction works only for unidirectional connections. The result is that for a bidirectional connectionwhich is the case for which latency is most criticalheader prediction slightly worsens latency. <p> Unfortunately, TCP header-prediction works only for unidirectional connections. The result is that for a bidirectional connectionwhich is the case for which latency is most criticalheader prediction slightly worsens latency. However, with less than a dozen additional instructions executed, the slow down is not significant. Architecture: 80386 Alpha TCP/IP implementation: <ref> [6] </ref>: Unix Improved: v3.2c: x-kernel: # of instruction executed:: : : : : in ipintr: 57 248 : : : in tcp input: 276 406 : : : from IP to TCP input: 262 437 : : : from TCP to socket input: 1188 1004 CPI: 4.3 3.3 Table 1: Comparison <p> The second row gives the number of instructions executed in tcp input after the TCP control block has been found (i.e., after function in pcblookup has returned). The IP count for the 80386 processor was taken directly from <ref> [6] </ref>. The TCP input processing count of 276 instructions was arrived at by adding both the sender side and the receiver side costs, as well as the common path cost reported in [6] (154 instructions for the common path, 15 + 17 additional instructions for the receive side processing, and 9+20+17+44 <p> The IP count for the 80386 processor was taken directly from <ref> [6] </ref>. The TCP input processing count of 276 instructions was arrived at by adding both the sender side and the receiver side costs, as well as the common path cost reported in [6] (154 instructions for the common path, 15 + 17 additional instructions for the receive side processing, and 9+20+17+44 additional instructions for the sender side processing). The DEC Unix numbers were measured as described above.
Reference: [7] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, A. Ed-wards, and J. Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <volume> 7(4) </volume> <pages> 35-43, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Using a bidirectional connection is more realistic to measure latency since if data flows in only one direction, it is usually possible to send large packets, and for large packets, processing time is dominated by data-dependent costs <ref> [7] </ref>. In contrast, with a request-response style of communication, small, latency-sensitive messages are quite common. The distinction between uni- and bidirectional data connections matters for two reasons. First, with a bidirectional connection, both end-hosts perform sender and receiver-related housekeeping. <p> Other than that, the traces are complete. For the sake of brevity, we only summarize the most important results; see [22] for a more detailed discussion. 2 Numbers is this range have been reported in the literature for FDDI and ATM controllers <ref> [7] </ref>.
Reference: [8] <author> R. P. Draves, B. N. Bershad, R. F. Rashid, and R. W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 122-36. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: These modification, which are described in detail elsewhere [22], are summarized below: * D-cache optimizations: First, data structures were reorganized to minimize compiler-introduced padding and to co-locate structure members that are accessed together. Second, the kernel was adapted to use continuations <ref> [8] </ref> and stacks that are first-class objects so as to minimize the number of stacks in use and to allow managing them in a cache-friendly last-in-first-out manner. Third, the hash-table manager was changed to allow efficient visiting of all table-elements.
Reference: [9] <author> D. R. Engler, F. Kaashoek, and J. O. Jr. Exoker-nel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 251-266, </pages> <year> 1995. </year>
Reference-contexts: We have designed and implemented a new OS, called Scout, that extends the x-kernel by adding an explicit path abstraction [21]. An essential component of Scout is a packet classifier that determines which path a given packet will traverse <ref> [2, 18, 33, 9] </ref>. Thus, such a system provides exactly the information necessary to use cloning and path-inlining. 4 Evaluation This section evaluates the techniques presented in the previous section. <p> Note that path-inlining and cloning require running a packet classifier on incoming packets since the optimized code is no longer general enough to handle all possible packets. Currently, the best software classifiers add an overhead of about 1 4s per packet <ref> [2, 9] </ref>. The results presented in this section do not include the time required to classify packets. This seems justified since systems commonly use classifiers to improve functionality, flexibility, and accountability [16, 21].
Reference: [10] <author> A. </author> <title> Eustace. </title> <type> Personal communication, </type> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: When executing straight-line code out of the b-cache, the CPU can sustain an execution rate of 8 instructions per 13 cycles <ref> [10] </ref>. Unless noted otherwise, all software was implemented in a minimal standalone version of the x-kernel [13]. The entire test runs in kernel mode (no protection domain crossings) and without virtual memory.
Reference: [11] <author> R. Gupta and C.-H. Chi. </author> <title> Improving instruction cache behavior by reducing cache pollution. </title> <booktitle> In Proceedings Supercomputing '90, </booktitle> <pages> pages 82-91. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: This hypothesis is corroborated by the fact that we have not found a single instance where aligning function entry-points or similar gap-introducing techniques would have improved end-to-end latency. Note that this is in stark contrast with the findings published in <ref> [11] </ref>, where i-cache optimization focused on functions with a very high degree of locality. So it may be that micro-positioning suffers because of the memory bandwidth wasted on loading gaps. Third, the DEC 3000/600 workstations used in the experiments employ a large second-level cache.
Reference: [12] <author> R. R. Heisch. </author> <title> Tracedirected program restructuring for AIX executables. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(9) </volume> <pages> 595-603, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: For example, error handling code could be moved to the end of the function or to the end of the program. Outlining traditionally has been associated with profile-based optimizers <ref> [12, 26] </ref>. Profile-based optimizers are aggressive rather than conservativeany code that is not covered by the collected profile will be outlined.
Reference: [13] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: When executing straight-line code out of the b-cache, the CPU can sustain an execution rate of 8 instructions per 13 cycles [10]. Unless noted otherwise, all software was implemented in a minimal standalone version of the x-kernel <ref> [13] </ref>. The entire test runs in kernel mode (no protection domain crossings) and without virtual memory. The kernel is so small that it fits entirely into the b-cache and, unless forced (as in some of the tests), there are no b-cache conflicts.
Reference: [14] <author> V. Jacobson. </author> <title> A high performance TCP/IP implementation. Presentation at the NRI Gigabit TCP Workshop, </title> <month> Mar. 18th-19th </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common networking protocols, such as TCP/IP <ref> [15, 6, 14] </ref> and RPC [32]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. fl O'Malley's current address is Network Appliance, 319 N. Bernardo Ave., Mountain View, CA 94043.
Reference: [15] <author> J. Kay and J. Pasquale. </author> <title> The importance of non-data touching processing overheads in TCP/IP. </title> <booktitle> In Proceedings of SIGCOMM '93 Symposium, </booktitle> <volume> volume 23, </volume> <pages> pages 259-268, </pages> <address> San Fransico, California, </address> <month> Oct. </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common networking protocols, such as TCP/IP <ref> [15, 6, 14] </ref> and RPC [32]. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. fl O'Malley's current address is Network Appliance, 319 N. Bernardo Ave., Mountain View, CA 94043.
Reference: [16] <author> C. Maeda and B. N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 244-255, </pages> <address> Asheville, North Carolina, </address> <month> Dec. </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Currently, the best software classifiers add an overhead of about 1 4s per packet [2, 9]. The results presented in this section do not include the time required to classify packets. This seems justified since systems commonly use classifiers to improve functionality, flexibility, and accountability <ref> [16, 21] </ref>. In a system that already makes use of a classifier, the benefits of cloning and path-inlining are truly the ones reported below.
Reference: [17] <author> H. Massalin. </author> <title> Synthesis: An Efficient Implementation of Fundamental Operating System Services. </title> <type> PhD thesis, </type> <institution> Columbia University, </institution> <address> New York, NY 10027, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: For example, if cloning is delayed until a TCP/IP connection is established, most connection state will remain constant and can be used to partially evaluate the cloned function. This achieves similar benefits as code synthesis <ref> [17] </ref>. Just as for inlining, cloning is at odds with locality of reference. Cloning at connection creation time will lead to one cloned copy per connection, while cloning at protocol stack creation time will require only one copy per protocol stack.
Reference: [18] <author> S. McCanne and V. Jacobson. </author> <title> The BSD packet filter: A new architecture for user-level packet capture. </title> <booktitle> In 1993 Winter USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1993. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: We have designed and implemented a new OS, called Scout, that extends the x-kernel by adding an explicit path abstraction [21]. An essential component of Scout is a packet classifier that determines which path a given packet will traverse <ref> [2, 18, 33, 9] </ref>. Thus, such a system provides exactly the information necessary to use cloning and path-inlining. 4 Evaluation This section evaluates the techniques presented in the previous section.
Reference: [19] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191. </pages> <publisher> ACM, </publisher> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: We experimented extensively with different layout strategies for cloned code. We thought that, ideally, it should be possible to avoid all i-cache conflicts along a critical path of execution. With a direct-mapped i-cache, the starting address of a function determines exactly which i-cache blocks it is going to occupy <ref> [19] </ref>. Consequently, by choosing appropriate addresses, it is possible to optimize i-cache behavior for a given path.
Reference: [20] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable tools for performance analysis. </title> <booktitle> In Proceedings of the USENIX 1996 Technical Conference, </booktitle> <pages> pages 120-133, </pages> <year> 1996. </year>
Reference-contexts: This is because non-blocking loads make it possible to overlap memory accesses with useful computation. The memory system interface is 128 bits wide and the lmbench <ref> [20] </ref> suite reports a memory access latency of 2, 10, and 48 cycles for a d-cache, b-cache, and main-memory accesses, respectively. When executing straight-line code out of the b-cache, the CPU can sustain an execution rate of 8 instructions per 13 cycles [10].
Reference: [21] <author> D. Mosberger and L. L. Peterson. </author> <title> Making paths explicit in the Scout operating system. </title> <type> Technical Report 96-05, </type> <institution> University of Arizona, </institution> <address> Tucson, AZ 85721, </address> <year> 1996. </year>
Reference-contexts: We have designed and implemented a new OS, called Scout, that extends the x-kernel by adding an explicit path abstraction <ref> [21] </ref>. An essential component of Scout is a packet classifier that determines which path a given packet will traverse [2, 18, 33, 9]. Thus, such a system provides exactly the information necessary to use cloning and path-inlining. 4 Evaluation This section evaluates the techniques presented in the previous section. <p> Currently, the best software classifiers add an overhead of about 1 4s per packet [2, 9]. The results presented in this section do not include the time required to classify packets. This seems justified since systems commonly use classifiers to improve functionality, flexibility, and accountability <ref> [16, 21] </ref>. In a system that already makes use of a classifier, the benefits of cloning and path-inlining are truly the ones reported below.
Reference: [22] <author> D. Mosberger, L. L. Peterson, P. G. Bridges, and S. O'Malley. </author> <title> Analysis of techniques to improve protocol processing latency. </title> <type> Technical Report 96-03, </type> <institution> University of Arizona, </institution> <address> Tucson, AZ 85721, </address> <year> 1996. </year>
Reference-contexts: All code was compiled using a version of gcc 2.6.0 that was modified to support outlining [31]. While we started with the regular x-kernel distribution, we did apply some modifications in the process of porting it to the Alpha. These modification, which are described in detail elsewhere <ref> [22] </ref>, are summarized below: * D-cache optimizations: First, data structures were reorganized to minimize compiler-introduced padding and to co-locate structure members that are accessed together. <p> The instruction traces do not cover all of the processing since the tracing facility did not allow the tracing of interrupt handling. Other than that, the traces are complete. For the sake of brevity, we only summarize the most important results; see <ref> [22] </ref> for a more detailed discussion. 2 Numbers is this range have been reported in the literature for FDDI and ATM controllers [7]. <p> The additional reduction compared to outlining or path-inlining alone is small though, on the order of 0.11 to 0.14 cycles per instruction. Of all the mCPI values, the value 0.81 for the ALL version of the RPC stack clearly stands out. Additional data presented in <ref> [22] </ref> leads us to believe that the value is an anomaly: just small changes to the code lead to mCPI values more in line with the other results.
Reference: [23] <author> S. W. O'Malley and L. L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols [28, 27]. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol <ref> [23] </ref> that routes outgoing messages to the appropriate network adapter. In BSD-derived implementations, VNET is normally part of IP. ETH is the device-independent half of the Ethernet driver, whereas LANCE is the device-dependent half for the network adapter present in the DEC 3000 machine. <p> At the top is XRPCTEST, which is the RPC-equivalent of the ping-pong test implemented in TCPTEST. MSELECT, VCHAN, CHAN, BID and BLAST together provide the desired RPC semantics. A detailed description of these protocols can be found in <ref> [23] </ref>. 2.3 Base Case We first establish that the base case used in later sections is sound. To do this, we compare the x-kernel TCP/IP stack with the one implemented in DEC Unix v3.2c. While there is always room for improvement, the Unix implementation qualifies as well-optimized code.
Reference: [24] <author> S. W. O'Malley, T. Proebsting, and A. B. Montz. </author> <title> USC: A universal stub compiler. </title> <booktitle> In Proceedings of SIG-COMM '94 Symposium, </booktitle> <pages> pages 295-306, </pages> <address> London, UK, </address> <month> Aug. </month> <note> 31st Sept. 2nd 1994. </note>
Reference-contexts: The Universal Stub Compiler <ref> [24] </ref> was used to allow accessing such memory efficiently and conveniently. Also, the Alpha architecture does not support sub-word load and store operations. The critical-path code size of TCP was dramatically reduced by changing a few type declarations to use full words instead of sub-words.
Reference: [25] <author> J. K. Ousterhout, A. Cherenson, F. Douglis, M. N. Nel-son, and B. B. Welch. </author> <title> The Sprite network operating system. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 23-35, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: TCPTEST TCP VNET ETH LANCE LANCE ETH VNET IP BLAST BID CHAN VCHAN MSELECT XRPCTEST The right side of Figure 1 shows the RPC stack. It implements a remote procedure call facility similar to Sprite RPC <ref> [25] </ref>. Since the x-kernel-paradigm encourages stacks with many small (minimal) protocols, RPC is considerably taller than TCP/IP. At the top is XRPCTEST, which is the RPC-equivalent of the ping-pong test implemented in TCPTEST. MSELECT, VCHAN, CHAN, BID and BLAST together provide the desired RPC semantics.
Reference: [26] <author> K. Pettis and R. C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 25, </volume> <pages> pages 16-27, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For example, error handling code could be moved to the end of the function or to the end of the program. Outlining traditionally has been associated with profile-based optimizers <ref> [12, 26] </ref>. Profile-based optimizers are aggressive rather than conservativeany code that is not covered by the collected profile will be outlined. <p> This layout strategy is so simple that it can be computed easily at runtimethe only dynamic information required is the order in which the functions are invoked. In essence, computing a bipartite layout consists of applying the well-known closest-is-best strategy to the library and path partition individually <ref> [26] </ref>. Establishing the performance advantage of the bipartite layout relative to the micro-positioning approach is difficult since small changes to the heuristics of the latter approach resulted in large performance variations. The micro-positioning approach usually performed somewhat worse than a bipartite layout and sometimes almost equally well, but never better.
Reference: [27] <author> J. Postel. RFC-791: </author> <title> Internet Protocol. </title> <note> Available via ftp from ftp.nisc.sri.com, </note> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: The organization of the two protocol stacks is shown in TCPTEST, a simple, ping-pong test program. Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols <ref> [28, 27] </ref>. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol [23] that routes outgoing messages to the appropriate network adapter. In BSD-derived implementations, VNET is normally part of IP.
Reference: [28] <author> J. Postel. RFC-793: </author> <title> Transmission Control Protocol. </title> <note> Available via ftp from ftp.nisc.sri.com, </note> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: The organization of the two protocol stacks is shown in TCPTEST, a simple, ping-pong test program. Below are TCP and IP which are the x-kernel versions of the corresponding Internet protocols <ref> [28, 27] </ref>. The x-kernel implementation of TCP is based on BSD source code so, except for interface changes, they are identical. VNET is a virtual protocol [23] that routes outgoing messages to the appropriate network adapter. In BSD-derived implementations, VNET is normally part of IP.
Reference: [29] <author> B. Prince. </author> <title> Memory in the fast lane. </title> <journal> IEEE Spectrum, </journal> <volume> 31(2) </volume> <pages> 38-41, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The bottom-line is that we evaluate protocol latency in terms of memory cycles per instruction (mCPI), a metric that will become increasingly important as improvements in memory speed lag farther behind improvements in processor speed <ref> [5, 29] </ref>. It should be clear from these three points that memory bandwidthand in particular, the memory cycles required by each instructionis a central focus of this paper.
Reference: [30] <author> R. L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> Burlington, Massachusetts, </address> <year> 1992. </year> <title> Order number EY-L520E-DP. </title>
Reference-contexts: These workstations use the 21064 Alpha CPU running at 175MHz <ref> [30] </ref>. The CPU is a 64-bit wide, superscalar design that can issue up to two instructions per cycle. In practice, there are very few opportunities to dual issue pure integer code. For integer-only systems code, it is therefore more accurate to view the CPU as a single-issue processor.
Reference: [31] <author> R. M. Stallman. </author> <title> Using and Porting GNU CC, 1992. Manuscript provided by the Free Software Foundation to document gcc. </title>
Reference-contexts: The kernel is so small that it fits entirely into the b-cache and, unless forced (as in some of the tests), there are no b-cache conflicts. All code was compiled using a version of gcc 2.6.0 that was modified to support outlining <ref> [31] </ref>. While we started with the regular x-kernel distribution, we did apply some modifications in the process of porting it to the Alpha.
Reference: [32] <author> C. A. Thekkath and H. M. Levy. </author> <title> Limits to low-latency communication on highspeed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Communication latency is often just as important as throughput in distributed systems, and for this reason, researchers have analyzed the latency characteristics of common networking protocols, such as TCP/IP [15, 6, 14] and RPC <ref> [32] </ref>. This paper revisits the issue of protocol latency. Our goal is not to optimize a particular protocol stack, but rather, to understand the fundamental limitations on processing overhead. fl O'Malley's current address is Network Appliance, 319 N. Bernardo Ave., Mountain View, CA 94043. <p> The LANCE overhead of 47:4s is consistent with the 51s figure reported elsewhere for the same controller in an older generation workstation <ref> [32] </ref>.
Reference: [33] <author> M. Yuhara, B. N. Bershad, C. Maeda, and J. E. B. Moss. </author> <title> Efficient packet demultiplexing for multiple endpoints and large messages. </title> <booktitle> In 1994 Winter USENIX Conference, </booktitle> <pages> pages 153-165, </pages> <year> 1994. </year>
Reference-contexts: We have designed and implemented a new OS, called Scout, that extends the x-kernel by adding an explicit path abstraction [21]. An essential component of Scout is a packet classifier that determines which path a given packet will traverse <ref> [2, 18, 33, 9] </ref>. Thus, such a system provides exactly the information necessary to use cloning and path-inlining. 4 Evaluation This section evaluates the techniques presented in the previous section.
References-found: 33

