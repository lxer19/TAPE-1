URL: http://dimacs.rutgers.edu/techps/1993/93-75.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1993.html
Root-URL: http://www.cs.rutgers.edu
Title: Interior-Point Methods for Max-Min Eigenvalue Problems  
Author: Franz Rendl Robert J. Vanderbei Henry Wolkowicz 
Note: Research support by Christian Doppler  Research support by AFOSR through grant AFOSR-91-0359.  
Date: November 8, 1993  93-75  
Address: New Brunswick, NJ 08903  Kopernikusgasse 24, A-8010 Graz, Austria.  
Affiliation: Rutgers University  Technische Universitat Graz, Institut fur Mathematik,  Laboratorium fur Diskrete Optimierung.  
Pubnum: DIMACS  Technical Report  
Abstract: The problem of maximizing the smallest eigenvalue of a symmetric matrix subject to modifications on the main diagonal that sum to zero is important since, for example, it yields the best bounds for graph-partitioning. Current algorithms for this problem work well when the multiplicity of the minimum eigenvalue at optimality is one. However, real-world applications have multiplicity at optimality that is greater than one. For such problems, current algorithms break down quickly as the multiplicity increases. We present a primal-dual interior-point z Department of Combinatorics and Optimization, University of Waterloo, Waterloo, Ont., Canada. This author thanks the Department of Civil Engineering and Operations Research, Princeton University, for their support during his stay while on research leave. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. ALIZADEH. </author> <title> Combinatorial optimization with interior point methods and semidefinite matrices. </title> <type> PhD thesis, </type> <institution> University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: Interior-point methods for problems involving matrix inequalities are studied in <ref> [6, 7, 1, 2, 12] </ref>. (See the latter for a historical overview.) Now, by substituting y = v + !e (2:2) for v, (2.1) can be reformulated as: Maximize ! subject to Diag (y) C e T y = n!: Finally, we can think of the last equation as a defining
Reference: [2] <author> F. ALIZADEH. </author> <title> Combinatorial optimization with semidefinite matrices. </title> <booktitle> In Proceedings of the Second Annual Integer Programming and Combinatorial Optimization Conference, </booktitle> <institution> Carnegie-Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Interior-point methods for problems involving matrix inequalities are studied in <ref> [6, 7, 1, 2, 12] </ref>. (See the latter for a historical overview.) Now, by substituting y = v + !e (2:2) for v, (2.1) can be reformulated as: Maximize ! subject to Diag (y) C e T y = n!: Finally, we can think of the last equation as a defining
Reference: [3] <author> J. CULLUM, W.E. DONATH, and P.WOLFE. </author> <title> The minimization of certain nondifferentiable sums of eigenvalues of symmetric matrices. </title> <journal> Mathematical Programming Study, </journal> <volume> 3 </volume> <pages> 35-55, </pages> <year> 1975. </year>
Reference-contexts: The objective function in (1.1) is not differentiable when the multiplicity of the smallest eigenvalue exceeds one. In fact, a singleton eigenvalue characterizes differentiability. Since the smallest eigenvalue is a concave function, subgradient approaches can be used to solve (1.1) (see, e.g., <ref> [3] </ref>). More recently, it has been shown that Newton-based algorithms with local quadratic convergence exist (see, e.g., [10]) but the local convergence depends on correctly identifying the multiplicity of the smallest eigenvalue. In some sense, high multiplicity is analogous to degeneracy in linear programming.
Reference: [4] <author> J. FALKNER, F. RENDL, and H. WOLKOWICZ. </author> <title> A computational study of graph partitioning. </title> <type> Technical Report CORR, </type> <institution> Department of Combinatorics and Optimization, Waterloo, Ont, </institution> <year> 1992. </year> <note> Submitted to Math. Progr. </note>
Reference-contexts: For many of these applications, it is essential to have a fast algorithm for (1.1) since it has to be solved many times within the application. For example, problem (1.1) can be used to obtain excellent bounds in branch and bound codes for graph bisection (see e.g. <ref> [4] </ref> and the survey article [5]). The objective function in (1.1) is not differentiable when the multiplicity of the smallest eigenvalue exceeds one. In fact, a singleton eigenvalue characterizes differentiability. Since the smallest eigenvalue is a concave function, subgradient approaches can be used to solve (1.1) (see, e.g., [3]). <p> BT refers to the Bundle Trust method and IP refers to our Interior-Point method. 5.1 Problems from Graph Partitioning. One of the main applications of the max-min eigenvalue problem is to obtain bounds in the graph partitioning problem (see <ref> [4] </ref>). For such problems, the matrix C is the incidence matrix of an undirected graph. We generated a few random incidence matrices and compared our interior-point method against the bundle trust method. The results are summarized in Table 1. For these problems, the interior-point method is clearly superior.
Reference: [5] <author> B. MOHAR and S. POLJAK. </author> <title> Eigenvalues in combinatorial optimization. </title> <type> Technical Report 92752, </type> <address> Charles University, Praha, Czechoslo-vakia, </address> <year> 1992. </year>
Reference-contexts: For example, problem (1.1) can be used to obtain excellent bounds in branch and bound codes for graph bisection (see e.g. [4] and the survey article <ref> [5] </ref>). The objective function in (1.1) is not differentiable when the multiplicity of the smallest eigenvalue exceeds one. In fact, a singleton eigenvalue characterizes differentiability. Since the smallest eigenvalue is a concave function, subgradient approaches can be used to solve (1.1) (see, e.g., [3]).
Reference: [6] <author> Y. E. NESTEROV and A. S. NEMIROVSKY. </author> <title> Self-concordant functions and polynomial-time methods in convex programming. </title> <institution> Book-Preprint, Central Economic and Mathematical Institute, USSR Academy of Science, Moscow, USSR, </institution> <year> 1989. </year> <note> Published in Nesterov and Nemirovsky [7]. </note>
Reference-contexts: Interior-point methods for problems involving matrix inequalities are studied in <ref> [6, 7, 1, 2, 12] </ref>. (See the latter for a historical overview.) Now, by substituting y = v + !e (2:2) for v, (2.1) can be reformulated as: Maximize ! subject to Diag (y) C e T y = n!: Finally, we can think of the last equation as a defining
Reference: [7] <author> Y. E. NESTEROV and A. S. NEMIROVSKY. </author> <title> Interior Point Polynomial Methods in Convex Programming : Theory and Algorithms. </title> <publisher> SIAM Publications. SIAM, </publisher> <address> Philadelphia, USA, </address> <year> 1993. </year>
Reference-contexts: Interior-point methods for problems involving matrix inequalities are studied in <ref> [6, 7, 1, 2, 12] </ref>. (See the latter for a historical overview.) Now, by substituting y = v + !e (2:2) for v, (2.1) can be reformulated as: Maximize ! subject to Diag (y) C e T y = n!: Finally, we can think of the last equation as a defining
Reference: [8] <author> M.L. OVERTON. </author> <title> On minimizing the maximum eigenvalue of a symmetric matrix. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 9 </volume> <pages> 256-268, </pages> <year> 1988. </year>
Reference-contexts: There are many important applications for this problem (see e.g. <ref> [8] </ref>, [9] and the references therein). For many of these applications, it is essential to have a fast algorithm for (1.1) since it has to be solved many times within the application.
Reference: [9] <author> M.L. OVERTON. </author> <title> Large-scale optimization of eigenvalues. </title> <journal> SIAM J. Optimization, </journal> <volume> 2 </volume> <pages> 88-120, </pages> <year> 1992. </year> <month> 14 </month>
Reference-contexts: There are many important applications for this problem (see e.g. [8], <ref> [9] </ref> and the references therein). For many of these applications, it is essential to have a fast algorithm for (1.1) since it has to be solved many times within the application.
Reference: [10] <author> M.L. </author> <title> OVERTON and R.S. WOMERSLEY. Second derivatives for opti-mizing eigenvalues of symmetric matrices. </title> <type> Technical Report 627, </type> <institution> Computer Science Department, NYU, </institution> <year> 1993. </year>
Reference-contexts: In fact, a singleton eigenvalue characterizes differentiability. Since the smallest eigenvalue is a concave function, subgradient approaches can be used to solve (1.1) (see, e.g., [3]). More recently, it has been shown that Newton-based algorithms with local quadratic convergence exist (see, e.g., <ref> [10] </ref>) but the local convergence depends on correctly identifying the multiplicity of the smallest eigenvalue. In some sense, high multiplicity is analogous to degeneracy in linear programming.
Reference: [11] <author> H. SCHRAMM and J. </author> <title> ZOWE. A version of the bundle idea for minimizing a nonsmooth function: Conceptual idea, convergence analysis, numerical results. </title> <journal> SIAM J. Optimization, </journal> <volume> 2 </volume> <pages> 121-152, </pages> <year> 1992. </year>
Reference: [12] <author> L. VANDENBERGHE and S. BOYD. </author> <title> Primal-dual potential reduction method for problems involving matrix inequalities. </title> <type> Technical report, </type> <institution> Electrical Engineering Department, Stanford University, Stanford, </institution> <address> CA 94305, </address> <year> 1993. </year>
Reference-contexts: Interior-point methods for problems involving matrix inequalities are studied in <ref> [6, 7, 1, 2, 12] </ref>. (See the latter for a historical overview.) Now, by substituting y = v + !e (2:2) for v, (2.1) can be reformulated as: Maximize ! subject to Diag (y) C e T y = n!: Finally, we can think of the last equation as a defining
Reference: [13] <author> R.J. VANDERBEI and T.J. CARPENTER. </author> <title> Symmetric indefinite systems for interior-point methods. </title> <journal> Mathematical Programming, </journal> <volume> 58 </volume> <pages> 1-32, </pages> <year> 1993. </year>
Reference-contexts: From this point we estimate the current value using (3.15) and divide it by ten: = 10n (Experience from linear programming indicates that this simple heuristic performs very well, even though it does not guarantee monotonic decrease in , see <ref> [13] </ref>.) We next attempt to find step directions (X; y; Z) such that the new triple (X + X; y + y; Z + Z) lies on the central trajectory at this value of .
Reference: [14] <author> H. WOLKOWICZ. </author> <title> Some applications of optimization in matrix theory. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 40 </volume> <pages> 101-118, </pages> <year> 1981. </year> <month> 15 </month>
Reference-contexts: diag (X) = trfDiag (y)Xg = trf (C Z)Xg Now, since Z and X are both positive semidefinite, we see that b T y tr (CX) and the duality gap is simply tr (ZX). 2 Strong duality holds as well, since the Slater constraint qualification is trivially satisfied, see e.g. <ref> [14] </ref>. 3.1 Derivation of the Interior-Point Algorithm Primal-dual methods for (3.2) are derived by first introducing an associated barrier problem: Maximize b T y + log det Z subject to Diag (y) + Z = C: (3:10) Here is a positive real number called the barrier parameter.
References-found: 14

