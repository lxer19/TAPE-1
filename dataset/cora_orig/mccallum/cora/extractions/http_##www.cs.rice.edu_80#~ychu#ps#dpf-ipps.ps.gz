URL: http://www.cs.rice.edu:80/~ychu/ps/dpf-ipps.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~ychu/papers.html
Root-URL: 
Email: hu@deas.harvard.edu  johnsson@cs.uh.edu  dkehagia@fir.ml.com  nadia@deas.harvard.edu  
Title: DPF: A Data Parallel Fortran Benchmark Suite  Fixed Income Sales Anal.  
Author: Yu Hu S. Lennart Johnsson Merrill Lynch Nadia Shalaby 
Affiliation: Div. Eng. and Applied Sc. Harvard University  Computer Science Dept. University of  Div. Eng. and Applied Sc. Harvard University  
Date: April 1997.  Dimitris Kehagias  
Address: Geneva, Switzerland,  Houston  
Note: 11th International Parallel Processing Symposium,  
Abstract: We present the Data Parallel Fortran (DPF) benchmark suite, a set of data parallel Fortran codes for evaluating data parallel compilers appropriate for any target parallel architecture, with shared or distributed memory. The codes are provided in basic, optimized and several library versions. The functionality of the benchmarks cover collective communication functions, scientific software library functions, and application kernels that reflect the computational structure and communication patterns in fluid dynamic simulations, fundamental physics and molecular studies in chemistry or biology. The DPF benchmark suite assumes the language model of High Performance Fortran, and provides performance evaluation metrics of busy and elapsed times and FLOP rates, FLOP count, memory usage, communication patterns, local memory access, and arithmetic efficiency as well as operation and communication counts per iteration. An instance of the benchmark suite was fully implemented in CMFortran and tested on the CM5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasin ski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, Mof fett Field, California, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Some of the objectives for the DPF benchmark suite are similar to that of several other collections of programs. The NAS parallel benchmarks <ref> [1] </ref> are paper and pencil benchmarks intended for vendors and implementors using algorithms and programming models appropriate to their particular platforms. The NAS parallel benchmarks 2.0 [2] are an MPIbased source implementation. However, to our knowledge, this suite is the first focused entirely on data parallel software environments.
Reference: [2] <author> D. Bailey, T. Harris, W. Saphir, R. Wijngaartand, A. Woo, and M. Yarrow. </author> <title> The NAS parallel benchmarks 2.0. </title> <type> Technical Report NAS-95-020, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Some of the objectives for the DPF benchmark suite are similar to that of several other collections of programs. The NAS parallel benchmarks [1] are paper and pencil benchmarks intended for vendors and implementors using algorithms and programming models appropriate to their particular platforms. The NAS parallel benchmarks 2.0 <ref> [2] </ref> are an MPIbased source implementation. However, to our knowledge, this suite is the first focused entirely on data parallel software environments. The benchmark suite is divided into two groups, the library functions, and the applications oriented codes.
Reference: [3] <author> B. Boghosian. </author> <title> CM5 performance metrics suite. </title> <note> TMC in ternal memo, </note> <year> 1993. </year>
Reference-contexts: These classes are meant to be nei ther mutually exclusive nor exhaustive, but rather, demon strate an attempt to assess the performance of different appli cations according to some inherent properties that inevitably dictate their computational structure and communication pattern <ref> [3] </ref>.
Reference: [4] <author> M. C. Chen, J. Cowie, and J. Wu. </author> <title> CRAFT: A framework for F90 compiler optimization. </title> <booktitle> In 5th Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <address> Spain, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating such high performance software suites. In particular, we target data parallel Fortran compilers; such as any of the High Performance Fortran (HPF) [5] compilers, Fortran90 [12] compilers, the FortranY or CRAFT <ref> [4] </ref> compiler, as well as the Connection Machine Fortran (CMF) [15] compilers. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available. Hence, the benchmarks were all written in CMF.
Reference: [5] <author> HPF Forum. </author> <title> High performance fortran; language specifica tion, version 1.0. </title> <journal> Scientific Prog., </journal> <volume> 2(1 - 2):1170, </volume> <year> 1993. </year>
Reference-contexts: The goal in developing the Data Parallel Fortran (DPF) benchmark suite was to produce a means for evaluating such high performance software suites. In particular, we target data parallel Fortran compilers; such as any of the High Performance Fortran (HPF) <ref> [5] </ref> compilers, Fortran90 [12] compilers, the FortranY or CRAFT [4] compiler, as well as the Connection Machine Fortran (CMF) [15] compilers. At the time the benchmarks were developed, CMF was the only data parallel Fortran language with a production quality compiler available. Hence, the benchmarks were all written in CMF.
Reference: [6] <author> J. L. Hennessy and D. A. Patterson. </author> <booktitle> Computer Architecture: </booktitle>
Reference-contexts: For instance, the factorization and solution times for qr and lu, as well as the the constituents of the kernel in diff-1D and diff-2D, are timed separately. We quantify performance by the following attributes: (1) FLOP count: In counting the FLOPs, we adopt the operation counts suggested in <ref> [6] </ref>, assuming one FLOP for real addition, subtraction and multiplication, four FLOPs for division and square root, and eight FLOPs for logarithmic and trigonometric functions.
References-found: 6

