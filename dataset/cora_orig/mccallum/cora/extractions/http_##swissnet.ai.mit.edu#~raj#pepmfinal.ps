URL: http://swissnet.ai.mit.edu/~raj/pepmfinal.ps
Refering-URL: http://www-swiss.ai.mit.edu/~raj/index.html
Root-URL: 
Email: berlin@parc.xerox.com  raj@zurich.ai.mit.edu  
Title: Partial Evaluation for Scientific Computing: The Supercomputer Toolkit Experience  
Author: Andrew A. Berlin Rajeev J. Surati 
Affiliation: Xerox Palo Alto Research Center  M.I.T. Artificial Intelligence Laboratory  
Abstract: We describe the key role played by partial evaluation in the Supercomputer Toolkit, a parallel computing system for scientific applications that effectively exploits the vast amount of parallelism exposed by partial evaluation. The Supercomputer Toolkit parallel processor and its associated partial evaluation-based compiler have been used extensively by scientists at M.I.T., and have made possible recent results in astrophysics showing that the motion of the planets in our solar system is chaotically unstable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. J. Sussman and J. </author> <title> Wisdom, "Chaotic Evolution of the Solar System," </title> <journal> Science, </journal> <volume> Volume 257, </volume> <pages> pp. 256-262, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The integration performed on the Supercomputer Toolkit confirmed that the evolution of the Solar system as a whole is chaotic with a remarkably short time scale of exponential divergence of about 4 million years. A complete analysis of the integration results appears in <ref> [1] </ref>. A novel type of symplectic integration strategy was developed by Wisdom and Holman for use in this application, and was expressed in the Scheme language using an abstract programming style.
Reference: [2] <author> H. Abelson, A. Berlin, J. Katzenelson, W. McAllister, G. Rozas, G. Sussman, </author> <title> "The Supercomputer Toolkit and its Applications," </title> <institution> MIT Artificial Intelligence Laboratory Memo 1249, Cambridge, Massachusetts. </institution>
Reference-contexts: To support inter-processor communication, each processor has two high-speed Input/Output ports attached directly to its main register files. For a more detailed description of the Supercomputer Toolkit processor architecture, see <ref> [2] </ref>.
Reference: [3] <author> R. Surati and A. </author> <title> Berlin, "Exploiting the Parallelism Exposed by Partial Evaluation", </title> <booktitle> In Proc. of the IFIP WG10.3 International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Elsevier Science, </address> <note> 1994 also available as MIT Artificial Intelligence Laboratory Technical Report no 1414, </note> <month> April </month> <year> 1993. </year>
Reference-contexts: Finally, the region boundaries are broken down, and instruction-level scheduling is performed to assign computational resources to the fine-grain operations that have been assigned to each processor. A very detailed discussion of the compiler and all of its phases can be found in <ref> [3] </ref> and [5].
Reference: [4] <author> A. Berlin and D. Weise, </author> <title> "Compiling Scientific Code using Partial Evaluation," </title> <note> IEEE Computer December 1990. </note>
Reference-contexts: the latency of inter-processor communication increases, the maximum possible speedup decreases, as some of the parallelism must be used to keep each processor busy 3 We originally chose the 9-body program as an example to ease comparison with previously published work that also studied this program, including [11], [6], and <ref> [4] </ref>. However, there are numerical discrepancies between the theoretical speedup factors published in this paper and those presented in our previously published work, due to improvements that were made to the constant-folding phase of our compiler. <p> As a result of these improvements, the data-flow graph of the 9-body program being discussed in this paper has fewer operations than the data-flow graph used in [6] and <ref> [4] </ref>. All graphs and statistics presented in this paper, including the parallelism profile, have been updated to account for this change. represents all of the parallelism available in the problem, taking into account the varying latency of numerical operations. while awaiting the arrival of results from neighboring processors.
Reference: [5] <author> R. Surati, </author> <title> "A Parallelizing Compiler Based on Partial Evaluation", </title> <type> S.B. Thesis, </type> <institution> Electrical Engineering and Computer Science Department, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1992. </year> <note> Also available as TR-1377, </note> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> July, </month> <year> 1993. </year>
Reference-contexts: Finally, the region boundaries are broken down, and instruction-level scheduling is performed to assign computational resources to the fine-grain operations that have been assigned to each processor. A very detailed discussion of the compiler and all of its phases can be found in [3] and <ref> [5] </ref>.
Reference: [6] <author> A. </author> <title> Berlin, "Partial Evaluation Applied to Numerical Computation," </title> <booktitle> Proc. 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: As the latency of inter-processor communication increases, the maximum possible speedup decreases, as some of the parallelism must be used to keep each processor busy 3 We originally chose the 9-body program as an example to ease comparison with previously published work that also studied this program, including [11], <ref> [6] </ref>, and [4]. However, there are numerical discrepancies between the theoretical speedup factors published in this paper and those presented in our previously published work, due to improvements that were made to the constant-folding phase of our compiler. <p> As a result of these improvements, the data-flow graph of the 9-body program being discussed in this paper has fewer operations than the data-flow graph used in <ref> [6] </ref> and [4]. All graphs and statistics presented in this paper, including the parallelism profile, have been updated to account for this change. represents all of the parallelism available in the problem, taking into account the varying latency of numerical operations. while awaiting the arrival of results from neighboring processors.
Reference: [7] <author> A. </author> <title> Berlin, "A compilation strategy for numerical programs based on partial evaluation," </title> <institution> MIT Artificial Intelligence Laboratory Technical Report TR-1144, </institution> <address> Cambridge, MA., </address> <month> July </month> <year> 1989. </year>
Reference-contexts: Much of the design of the Supercomputer Toolkit was based on the observation (See <ref> [7] </ref>) that numerical applications are special in that they are for the most part data-independent, meaning that the sequence of numerical operations that will be performed is independent of the actual numerical values being manipulated. <p> Note that an alternative approach would have been to use techniques for extending the placeholder-based partial eval uation strategy to allow it to generate code that contains selection-style conditional branches, as described in <ref> [7] </ref>. We did indeed add these techniques to our front-end partial evaluator, but have not extended the code generation back-end to handle conditional branches, primarily because demand for this functionality from our scientific users dropped off once the subroutine library of selection operations became available. <p> The symbolic execution technique for performing partial evaluation of data-independent programs was simple to implement and worked well. We have already developed some ways (see <ref> [7] </ref>) to extend this technique to handle certain types of data-dependent branches, and can envision extending it to permit certain data-structures to be left residual. With recent developments in partial evaluation technology, the Toolkit's partial evaluator for data-independent programs may appear somewhat primitive.
Reference: [8] <author> E. Ruf and D. Weise, </author> <title> "Opportunities for Online Partial Evaluation", </title> <type> Technical Report CSL-TR-92-516, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA. </address> <year> 1992. </year>
Reference: [9] <author> E. Ruf and D. Weise, </author> <title> "Avoiding Redundant Specialization During Partial Evaluatio," </title> <booktitle> In Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluationand Semantics-Based Program Manipulation, </booktitle> <address> New Haven, CN. </address> <month> June </month> <year> 1991. </year>
Reference: [10] <author> L. Nagel, </author> <title> SPICE2: A Computer Program to Simulate Semiconductor Circuits, </title> <institution> Electronics Research Laboratory Report No. ERL-M520, University of California, Berkeley, </institution> <month> May </month> <year> 1975. </year>
Reference: [11] <author> J. Miller, "Multischeme: </author> <title> A Parallel Processing System Based on MIT Scheme". </title> <institution> MIT Laboratory For Computer Science technical report no. TR-402. </institution> <month> September, </month> <year> 1987. </year>
Reference-contexts: As the latency of inter-processor communication increases, the maximum possible speedup decreases, as some of the parallelism must be used to keep each processor busy 3 We originally chose the 9-body program as an example to ease comparison with previously published work that also studied this program, including <ref> [11] </ref>, [6], and [4]. However, there are numerical discrepancies between the theoretical speedup factors published in this paper and those presented in our previously published work, due to improvements that were made to the constant-folding phase of our compiler.
Reference: [12] <author> M. Katz and D. Weise, </author> <title> "Towards a New Perspective on Partial Evaluation," </title> <booktitle> In Proceedings of the 1992 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <address> San Francisco, </address> <month> June </month> <year> 1992. </year>
Reference: [13] <author> J. Ellis, Bulldog: </author> <title> A Compiler for VLIW Architectures, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [14] <author> J.A. Fisher, </author> <title> "Trace scheduling: A Technique for Global Microcode Compaction." </title> <journal> IEEE Transactions on Computers, </journal> <volume> Number 7, pp.478-490. </volume> <year> 1981. </year>
Reference: [15] <author> G.J. Sussman and J. </author> <title> Wisdom, "Numerical evidence that the motion of Pluto is chaotic," </title> <note> Science, (to appear). Also available as MIT Artificial Intelligence Laboratory Memo no. 1039, </note> <month> April </month> <year> 1988. </year>
Reference: [16] <author> J. Applegate, M. Douglas, Y. Gursel, P. Hunter, C. Seitz, G.J. Sussman, </author> <title> "A Digital Orrery," </title> <journal> IEEE Trans. on Computers, </journal> <month> Sept. </month> <year> 1985. </year>
Reference: [17] <author> P. Szolovits, </author> <title> "Compilation for Fast Calculation Over Pedigrees," </title> <journal> Cytogenet Cell Genet Vol. </journal> <volume> 59, pgs 136 - 138, </volume> <year> 1992 </year>
Reference-contexts: However, if certain assumptions are made about the relative independence of some of these "unknown" nodes, partial evaluation can play an important role, significantly reducing the size of the computation, as described in more detail in <ref> [17] </ref> and [18]. For any particular program invocation this program performed well.
Reference: [18] <author> P. Szolovits and S. Pauker, </author> <title> "Pedigree Analysis for Genetic Counseling," </title> <address> Medinfo 92 </address>
Reference-contexts: However, if certain assumptions are made about the relative independence of some of these "unknown" nodes, partial evaluation can play an important role, significantly reducing the size of the computation, as described in more detail in [17] and <ref> [18] </ref>. For any particular program invocation this program performed well.
Reference: [19] <author> S. Parekh, </author> <title> "Parameter Space Scans for Chaotic Circuits" S.B. </title> <type> Thesis, </type> <institution> MIT 1992. </institution>
Reference-contexts: The Supercomputer Toolkit system was used to do parameter space scans of chaotic circuits such as the double scroll circuit. These theoretical scans were compared against actual scans performed using a real circuit. The results and implementation details of these experiments can be found in <ref> [19] </ref>. An Integration System for Ordinary Differential Equations: Sarah Ferguson built a software system on top of the toolkit compiler that takes an equation as input, and automatically generates a Scheme program to integrate it.
Reference: [20] <author> E. Bradley, </author> <type> "Taming Chaotic Circuits" Ph.D. Thesis, </type> <institution> MIT 1992. </institution>
Reference: [21] <author> T. Quinn, S. Tremaine, and M. </author> <title> Duncan "A Three Million Year Integration of the Earth's Orbit," </title> <journal> Astron. J., </journal> <volume> vol. 101, no. 6, </volume> <month> June </month> <year> 1991, </year> <pages> pp. 2287-2305. </pages>
Reference: [22] <author> C. Heinzl, </author> <title> "Functional Diagnostics for the Supercomputer Toolkit MPCU Module", </title> <type> S.B. Thesis, </type> <institution> MIT, </institution> <year> 1990. </year>
References-found: 22

