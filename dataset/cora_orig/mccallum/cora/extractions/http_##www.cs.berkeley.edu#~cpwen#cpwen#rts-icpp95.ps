URL: http://www.cs.berkeley.edu/~cpwen/cpwen/rts-icpp95.ps
Refering-URL: http://www.cs.berkeley.edu/~cpwen/
Root-URL: 
Email: cpwen@cs.berkeley.edu yelick@cs.berkeley.edu  
Title: Portable Runtime Support for Asynchronous Simulation  
Author: Chih-Po Wen and Katherine Yelick 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division, University of California  
Note: To appear in the proceedings of the Internation Conference on Parallel Processing, 1995.  
Abstract: We present library and runtime support for portable asynchronous applications, using event-driven simulation as an example. Although event-driven simulation has a natural source of parallelism between the simulated entities, real speedups have been hard to obtain because of the fine-grained, unpredictable communication patterns. Language and systems software support is also lacking for asynchronous problems. Our runtime supports makes the applications portable, eases performance tuning, and allows code re-use between applications. Our goal is to support a range of platforms with varying performance characteristics, from special-purpose multiprocessors through networks of workstations. We discuss the performance issues in the runtime support, describe a distributed event graph data structure, and present performance numbers from a par-allelized timing simulator called SWEC. 
Abstract-found: 1
Intro-found: 1
Reference: [BC90] <author> Guy E. Blelloch and Siddhartha Chat-terjee. </author> <title> Vcode: A data-parallel intermediate language. </title> <booktitle> In Frontiers '90, </booktitle> <pages> pages 471-480, </pages> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. the most difficult to parallelize [Fox91]. Data parallel languages like HPF [Hig93] and NESL [Ble93] are well-suited to synchronous problems, and with sufficient compiler <ref> [BCF + 93, BC90] </ref> and run-time support (e.g., the PARTI system [SBW91]), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications.
Reference: [BCF + 93] <author> Zeki Bozkus, Alok Choudhary, Geof-frey Fox, Tomasz Haupt, and Sanjay Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Languages and Compilers for Parallel Computing, pages h1-h23, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. the most difficult to parallelize [Fox91]. Data parallel languages like HPF [Hig93] and NESL [Ble93] are well-suited to synchronous problems, and with sufficient compiler <ref> [BCF + 93, BC90] </ref> and run-time support (e.g., the PARTI system [SBW91]), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications.
Reference: [Ble93] <author> Guy E. Blelloch. Nesl: </author> <title> A nested data-parallel language (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> CMU School of Computer Science, </institution> <address> Pitts-burgh, PA, </address> <month> April </month> <year> 1993. </year> <note> Updated version of CMU-CS-92-103, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. the most difficult to parallelize [Fox91]. Data parallel languages like HPF [Hig93] and NESL <ref> [Ble93] </ref> are well-suited to synchronous problems, and with sufficient compiler [BCF + 93, BC90] and run-time support (e.g., the PARTI system [SBW91]), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications.
Reference: [BSS91] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory multiprocessors. </title> <journal> Concurrenty: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The aggregated messages in the buffer are then sent as a single message using a bulk communication primitive. Message aggregation, a technique that is well-known for bulk-synchronous algorithms and libraries <ref> [BSS91, KB94] </ref>, reduces communication overhead by amortizing the start-up overhead over many messages. Although message aggregation reduces the amortized communication start-up, it incurs the overhead for data copying, and thus increases the overhead per byte. <p> The idea of aggregating small messages to reduce communication overhead can be found in the Fortran-D compiler [HKT91], which uses message vectorization in parallel loops to reduce overhead, and in PARTI <ref> [BSS91] </ref>, which uses runtime preprocessing to pre-allocate storage for aggregated array accesses in bulk synchronous programs. It is a common technique for regular, array-based computations and for data structures that are irregular in time space but not time. <p> It is a common technique for regular, array-based computations and for data structures that are irregular in time space but not time. There have been a number of attempts at developing application-specific distributed data structures such as irregular grids <ref> [BSS91, KB94] </ref> and oct-trees [WS93]. As with aggregation, these data structures are targeted toward loosely synchronous, rather than asynchronous applications. 7 Conclusions We have presented a runtime communication layer for a general class of irregular problems, and a distributed even graph data structure for asynchronous simulation.
Reference: [CDG + 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishna-murthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Atomicity eases programming by eliminating locking for simple read-modify-write operations. Unlike system level threads, the context of our threads are explicitly managed by the programmer, so only the required variables are passed from one thread to its continuation. The threads synchronize using counters, similar to those used in Split-C <ref> [CDG + 93] </ref>. Every split-phase operation takes a counter as input argument, which is incremented when the operation completes. A separate continuation thread can be created to wait on the value of the counter.
Reference: [CDI + 95] <author> Soumen Chakrabarti, Etienne Deprit, Eun-Jin Im, Jeff Jones, Arvind Krish-namurthy, Chih-Po Wen, and Kather-ine Yelick. Multipol: </author> <title> A distributed 8 data structure library. </title> <note> Technical Re--port To appear, </note> <institution> University of California at Berkeley, Computer Science Division, </institution> <year> 1995. </year>
Reference-contexts: In this paper, we use a conservative parallel version of the SWEC simulator, an asynchronous event-driven simulator, as a running example to study programmability, performance, and portability issues of our system. The main distributed data structure in this problem is an event-graph, a component of our Multipol library <ref> [CDI + 95] </ref>. In previous work, we described a parallel implementation of the SWEC simulator that used optimistic concurrency [WY93]. The implementation performed well on the CM5 multiprocessor, but contained machine-specific techniques and was not organized to allow for code re-use in other simulators. This work addresses those concerns.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Recent research has produced a variety of run-time support such as the Chare kernel [SK91], Nexus [FKOT91], and the compiler-controlled threaded abstract machine (TAM) <ref> [CSS + 91] </ref>. Nexus provides mechanisms similar to remote thread invocation, but is more heavyweight than our runtime system, because it supports arbitrary thread suspension and heterogeneous computing.
Reference: [FKOT91] <author> Ian Foster, Carl Kesselman, Robert Ol-son, and Steve Tuccke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Ar-gonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: In our prior work [WY93], we compared the potential of the both methods for paral-lelizing SWEC [LKMS91], and developed a CM5 specific implementation using the optimistic method. Recent research has produced a variety of run-time support such as the Chare kernel [SK91], Nexus <ref> [FKOT91] </ref>, and the compiler-controlled threaded abstract machine (TAM) [CSS + 91]. Nexus provides mechanisms similar to remote thread invocation, but is more heavyweight than our runtime system, because it supports arbitrary thread suspension and heterogeneous computing.
Reference: [Fox91] <author> G. C. Fox. </author> <title> The architecture of problems and portable parallel software systems. </title> <type> Technical Report SCCS-134, </type> <institution> Syracuse Center for Computational Science, </institution> <year> 1991. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. the most difficult to parallelize <ref> [Fox91] </ref>. Data parallel languages like HPF [Hig93] and NESL [Ble93] are well-suited to synchronous problems, and with sufficient compiler [BCF + 93, BC90] and run-time support (e.g., the PARTI system [SBW91]), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Draft, </type> <month> May </month> <year> 1993. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. the most difficult to parallelize [Fox91]. Data parallel languages like HPF <ref> [Hig93] </ref> and NESL [Ble93] are well-suited to synchronous problems, and with sufficient compiler [BCF + 93, BC90] and run-time support (e.g., the PARTI system [SBW91]), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications.
Reference: [HKT91] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Supercomputing '91, </booktitle> <address> New Mexico, </address> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: The idea of aggregating small messages to reduce communication overhead can be found in the Fortran-D compiler <ref> [HKT91] </ref>, which uses message vectorization in parallel loops to reduce overhead, and in PARTI [BSS91], which uses runtime preprocessing to pre-allocate storage for aggregated array accesses in bulk synchronous programs.
Reference: [Jef85] <author> D.R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: Chandy and Misra developed the conservative method [KC81]. Jefferson introduced the optimistic method, also known as the time-warp algorithm <ref> [Jef85] </ref>. In our prior work [WY93], we compared the potential of the both methods for paral-lelizing SWEC [LKMS91], and developed a CM5 specific implementation using the optimistic method.
Reference: [KB94] <author> Scott Kohn and Scott Baden. </author> <title> A robut parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The aggregated messages in the buffer are then sent as a single message using a bulk communication primitive. Message aggregation, a technique that is well-known for bulk-synchronous algorithms and libraries <ref> [BSS91, KB94] </ref>, reduces communication overhead by amortizing the start-up overhead over many messages. Although message aggregation reduces the amortized communication start-up, it incurs the overhead for data copying, and thus increases the overhead per byte. <p> It is a common technique for regular, array-based computations and for data structures that are irregular in time space but not time. There have been a number of attempts at developing application-specific distributed data structures such as irregular grids <ref> [BSS91, KB94] </ref> and oct-trees [WS93]. As with aggregation, these data structures are targeted toward loosely synchronous, rather than asynchronous applications. 7 Conclusions We have presented a runtime communication layer for a general class of irregular problems, and a distributed even graph data structure for asynchronous simulation.
Reference: [KC81] <author> J. Misra K.M. Chandy. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11), </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: The parallel algorithm is a typical case of distributed asynchronous simulation, where each subcircuit corresponds to a logical process, and each event corresponds to a collection of messages sent to the fanout subcircuits. We adopt the conservative approach to parallel asynchronous simulation <ref> [KC81] </ref>, scheduling a subcircuit only when all input events have been processed. Previously, we showed that conservative parallel simulation is primarily useful for combinational circuits (circuits without feedback paths); optimistic scheduling is much more effective for sequential circuits [WY93]. <p> Chandy and Misra developed the conservative method <ref> [KC81] </ref>. Jefferson introduced the optimistic method, also known as the time-warp algorithm [Jef85]. In our prior work [WY93], we compared the potential of the both methods for paral-lelizing SWEC [LKMS91], and developed a CM5 specific implementation using the optimistic method.
Reference: [LKMS91] <author> S. Lin, E. Kuh, and M. Marek-Sadowska. SWEC: </author> <title> A stepwise equivalent conductance simulator for cmos vlsi circuits. </title> <booktitle> In Proc. of European Design Automation conference, </booktitle> <month> February </month> <year> 1991. </year>
Reference-contexts: Section 6 describes previous work on portable software support and discrete event simulation. Section 7 concludes the paper. 2 Overview of SWEC The SWEC program is mainly used to perform timing simulation for digital MOS circuits <ref> [LKMS91] </ref>. It partitions a circuit into loosely coupled subcircuits (as shown in Figure 1), each of which can be simulated independently within a time step. <p> Chandy and Misra developed the conservative method [KC81]. Jefferson introduced the optimistic method, also known as the time-warp algorithm [Jef85]. In our prior work [WY93], we compared the potential of the both methods for paral-lelizing SWEC <ref> [LKMS91] </ref>, and developed a CM5 specific implementation using the optimistic method. Recent research has produced a variety of run-time support such as the Chare kernel [SK91], Nexus [FKOT91], and the compiler-controlled threaded abstract machine (TAM) [CSS + 91].
Reference: [Lun94] <author> Steve Luna. </author> <title> Implementing an efficient portable global memory layer on distributed memory multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Division, University of California at Berke-ley, </institution> <year> 1994. </year>
Reference-contexts: All times are in seconds. library MPL, and the paragon port is based on the NX communication library. We use the active message developed at Berkeley for both the SP and the NX ports <ref> [Lun94] </ref>. The SP and Paragon have higher bandwidth networks and faster processors than CM5. In general, the gap between the computation and communication performance is more pronounced for the SP and Paragon than for the CM5. Table 1 describes the benchmark circuits.
Reference: [SBW91] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrenty: Practice and Experience, </journal> <month> December </month> <year> 1991. </year>
Reference-contexts: Data parallel languages like HPF [Hig93] and NESL [Ble93] are well-suited to synchronous problems, and with sufficient compiler [BCF + 93, BC90] and run-time support (e.g., the PARTI system <ref> [SBW91] </ref>), can be used for loosely synchronous problems. In this paper we describe library and runtime support for asynchronous applications. The goal of our research is to provide software systems to make asynchronous applications easier to parallelize, using extensive runtime support and a distributed data structure library.
Reference: [SK91] <author> Wei Shu and L.V. Kale. </author> <title> Chare kernel a runtime support system for parallel computations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 198-211, </pages> <year> 1991. </year>
Reference-contexts: In our prior work [WY93], we compared the potential of the both methods for paral-lelizing SWEC [LKMS91], and developed a CM5 specific implementation using the optimistic method. Recent research has produced a variety of run-time support such as the Chare kernel <ref> [SK91] </ref>, Nexus [FKOT91], and the compiler-controlled threaded abstract machine (TAM) [CSS + 91]. Nexus provides mechanisms similar to remote thread invocation, but is more heavyweight than our runtime system, because it supports arbitrary thread suspension and heterogeneous computing.
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: It can be decomposed into the fixed start-up overhead for setting up the communication, such as allocating storage or performing kernel calls, and the overhead per byte for injecting the message. Even for machines with like the CM5 with small hardware messages and lightweight communication such as Active Messages <ref> [vECGS92] </ref>, the startup cost observed by irregular applications may be significant. Active messages avoid storage allocation by using fixed hard ware packet sizes and by requiring that the user provide an address range in memory for the message data to be written.
Reference: [WS93] <author> M.S. Warren and J.K. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 12-21, </pages> <address> Port-land, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: It is a common technique for regular, array-based computations and for data structures that are irregular in time space but not time. There have been a number of attempts at developing application-specific distributed data structures such as irregular grids [BSS91, KB94] and oct-trees <ref> [WS93] </ref>. As with aggregation, these data structures are targeted toward loosely synchronous, rather than asynchronous applications. 7 Conclusions We have presented a runtime communication layer for a general class of irregular problems, and a distributed even graph data structure for asynchronous simulation.
Reference: [WY93] <author> Chih-Po Wen and Katherine Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. 9 10 </note>
Reference-contexts: The main distributed data structure in this problem is an event-graph, a component of our Multipol library [CDI + 95]. In previous work, we described a parallel implementation of the SWEC simulator that used optimistic concurrency <ref> [WY93] </ref>. The implementation performed well on the CM5 multiprocessor, but contained machine-specific techniques and was not organized to allow for code re-use in other simulators. This work addresses those concerns. Our research targets distributed memory architectures such as the CM5, Paragon, SP1/SP2, and networks of workstations. <p> We adopt the conservative approach to parallel asynchronous simulation [KC81], scheduling a subcircuit only when all input events have been processed. Previously, we showed that conservative parallel simulation is primarily useful for combinational circuits (circuits without feedback paths); optimistic scheduling is much more effective for sequential circuits <ref> [WY93] </ref>. We are developing an optimistic analog to the conservative Parallel SWEC, but in this paper we focus on the conservative example. Parallel SWEC is irregular in computation and communication. <p> Chandy and Misra developed the conservative method [KC81]. Jefferson introduced the optimistic method, also known as the time-warp algorithm [Jef85]. In our prior work <ref> [WY93] </ref>, we compared the potential of the both methods for paral-lelizing SWEC [LKMS91], and developed a CM5 specific implementation using the optimistic method. Recent research has produced a variety of run-time support such as the Chare kernel [SK91], Nexus [FKOT91], and the compiler-controlled threaded abstract machine (TAM) [CSS + 91].
References-found: 21

