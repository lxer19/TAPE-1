URL: http://www.cs.ucsb.edu/~martin/paper/popl97.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Email: fpedro,marting@cs.ucsb.edu  
Title: Synchronization Transformations for Parallel Computing  
Author: Pedro Diniz and Martin Rinard 
Address: Engineering I Building  Santa Barbara, CA 93106-5110  
Affiliation: Department of Computer Science  University of California, Santa Barbara  
Abstract: As parallel machines become part of the mainstream computing environment, compilers will need to apply synchronization optimizations to deliver efficient parallel software. This paper describes a new framework for synchronization optimizations and a new set of transformations for programs that implement critical sections using mutual exclusion locks. These transformations allow the compiler to move constructs that acquire and release locks both within and between procedures and to eliminate acquire and release constructs. The paper also presents a new synchronization algorithm, lock elimination, for reducing synchronization overhead. This optimization locates computations that repeatedly acquire and release the same lock, then uses the transformations to obtain equivalent computations that acquire and release the lock only once. Experimental results from a parallelizing compiler for object-based programs illustrate the practical utility of this optimization. For three benchmark programs the optimization dramatically reduces the number of times the computations acquire and release locks, which significantly reduces the amount of time processors spend acquiring and releasing locks. For one of the three benchmarks, the optimization always significantly improves the overall performance. Depending on the number of processors executing the computation, the optimized version runs between 2.11 and 1.83 times faster than the unoptimized version. For one of the other benchmarks, the optimized version runs between 1.13 and 0.96 times faster than the unoptimized version, with a mean of 1.08 times faster. For the final benchmark, the optimization reduces the overall performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: We report performance results for three automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver <ref> [1] </ref>, the Water code [19] and the String code [8]. Barnes-Hut simulates the trajectories of a set of interacting bodies under Newtonian forces; it consists of approximately 1500 lines of C++ code.
Reference: [2] <author> L. Cardelli and R. Pike. Squeak: </author> <title> a language for communicating with mice. </title> <booktitle> In Proceedings of SIGGRAPH '85, </booktitle> <address> San Francisco, CA, </address> <month> July </month> <year> 1985. </year>
Reference-contexts: Parallel computing will continue to play a crucial role in delivering maximum performance for scientific and engineering computations. The increasing use of multiple threads as an effective program construction technique (used, for example, in user interface systems and multi-threaded servers <ref> [9, 2, 15] </ref>) demonstrates that parallelism is not just for performance | it can also increase the expressive power of a language. Efficient synchronization is one of the fundamental requirements of effective parallel computing.
Reference: [3] <author> J Chow and W. Harrison III. </author> <title> Compile-time analysis of parallel programs that share memory. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Other researchers have investigated the issues associated with performing standard serial compiler analyses and optimizations in the presence of explicit concurrency <ref> [3, 14] </ref>.
Reference: [4] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters [20]. Several researchers have also explored optimizations geared towards exploiting more fine-grained con-currency available within loops <ref> [4] </ref>. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences. The transformations and algorithms presented in this paper address a different problem. They are designed to optimize mutual exclusion synchronization, not barrier synchronization or post/wait synchronization.
Reference: [5] <author> P. Diniz and M. Rinard. </author> <title> Lock coarsening: Eliminating lock overhead in automatically parallelized object-based programs. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: are also exploring the possibility of using profile data or dynamic feedback to allow the compiler or generated code to automatically choose the correct false exclusion policy. 7 Related Work The closest related work is our own previous research on techniques to reduce lock overhead in automatically par-allelized object-based programs <ref> [5] </ref>. This research used a monolithic algorithm that depends heavily on the restrictions of the object-based programming paradigm and the fact that the compiler controls the placement of the acquire and release constructs. The algorithm is formulated as a set of conditions on the call graph.
Reference: [6] <author> M. Emami, R. Ghiya, and L. Hendren. </author> <title> Context-sensitive interprocedural points-to analysis in the presence of function pointers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: In general, the compiler may have to use an interproce-dural pointer or alias analysis to compute a reasonably precise read set <ref> [6, 21, 10] </ref>. In restricted contexts the compiler may be able to use simpler algorithms. Our prototype compiler, for example, is designed for object-based programs [17].
Reference: [7] <author> J. Goodman, M. Vernon, and P. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The lock cancellation transformation, of course, eliminates both the acquire and the release. 7.4 Efficient Synchronization Algorithms Other researchers have addressed the issue of synchronization overhead reduction. This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms <ref> [7, 12] </ref>. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: [8] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 82-85, </pages> <year> 1990. </year>
Reference-contexts: We report performance results for three automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver [1], the Water code [19] and the String code <ref> [8] </ref>. Barnes-Hut simulates the trajectories of a set of interacting bodies under Newtonian forces; it consists of approximately 1500 lines of C++ code. Water simulates the interaction of water molecules in the liquid state; it consists of approximately 1850 lines of C++ code.
Reference: [9] <author> C. Hauser, C. Jacobi, M. Theimer, B. Welch, and M. Weiser. </author> <title> Using threads in interactive systems: A case study. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <address> Ashville, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Parallel computing will continue to play a crucial role in delivering maximum performance for scientific and engineering computations. The increasing use of multiple threads as an effective program construction technique (used, for example, in user interface systems and multi-threaded servers <ref> [9, 2, 15] </ref>) demonstrates that parallelism is not just for performance | it can also increase the expressive power of a language. Efficient synchronization is one of the fundamental requirements of effective parallel computing.
Reference: [10] <author> W. Landi, B. Ryder, and S. Zhang. </author> <title> Interprocedural modification side effect analysis with pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: In general, the compiler may have to use an interproce-dural pointer or alias analysis to compute a reasonably precise read set <ref> [6, 21, 10] </ref>. In restricted contexts the compiler may be able to use simpler algorithms. Our prototype compiler, for example, is designed for object-based programs [17].
Reference: [11] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: We generated three instrumented versions of each application; each version uses a different false exclusion policy. We evaluated the performance of each version by running it on a 16-processor Stan-ford DASH machine <ref> [11] </ref>. 6.2 Barnes-Hut We evaluate the overhead of each false exclusion policy by running the three automatically parallelized versions on one processor and comparing the execution time with the execution time of the sequential program.
Reference: [12] <author> B-H. Lim and A. Agarwal. </author> <title> Reactive synchronization algorithms for multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The lock cancellation transformation, of course, eliminates both the acquire and the release. 7.4 Efficient Synchronization Algorithms Other researchers have addressed the issue of synchronization overhead reduction. This work has concentrated on the development of more efficient implementations of synchronization primitives using various protocols and waiting mechanisms <ref> [7, 12] </ref>. The research presented in this paper is orthogonal to and synergistic with this work.
Reference: [13] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1485-1495, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: The formulation also removes the dependence on the compiler's ability to control the placement of the synchronization constructs. 7.1 Parallel Loop Optimizations Previous synchronization optimization research has focused almost exclusively on parallel loops in scientific computations <ref> [13] </ref>. The natural implementation of a parallel loop requires two synchronization constructs: an initiation construct to start all processors executing loop iterations, and a barrier construct at the end of the loop.
Reference: [14] <author> S. Midkiff and D. Padua. </author> <title> Issues in the optimization of parallel programs. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages II-105-113, </pages> <year> 1990. </year>
Reference-contexts: Other researchers have investigated the issues associated with performing standard serial compiler analyses and optimizations in the presence of explicit concurrency <ref> [3, 14] </ref>.
Reference: [15] <author> J. Reppy. </author> <title> Higher-order Concurrency. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Cornell University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Parallel computing will continue to play a crucial role in delivering maximum performance for scientific and engineering computations. The increasing use of multiple threads as an effective program construction technique (used, for example, in user interface systems and multi-threaded servers <ref> [9, 2, 15] </ref>) demonstrates that parallelism is not just for performance | it can also increase the expressive power of a language. Efficient synchronization is one of the fundamental requirements of effective parallel computing.
Reference: [16] <author> T. Reps, S. Horowitz, and M. Sagiv. </author> <title> Precise interprocedural dataflow analysis via graph reachability. </title> <booktitle> In Proceedings of the Twenty-second Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 49-61. </pages> <publisher> ACM, </publisher> <month> Jan-uary </month> <year> 1995. </year>
Reference-contexts: The transformations and framework are independent of any such association, however. 3 Program Representation The synchronization transformations operate on the inter-procedural control flow graph (ICFG) <ref> [16] </ref>, which consists of the union of the control flow graphs of the individual procedures. In this representation a procedure call is represented by two nodes: a call node and a return node.
Reference: [17] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new analysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> (http://www.cs.ucsb.edu/~martin/pldi96.ps). </note>
Reference-contexts: Finally, this paper presents experimental results that demonstrate the practical utility of lock elimination. These experimental results come from a compiler that automatically parallelizes object-based programs written in a subset of serial C++. Because this compiler uses a new analysis technique called commutativity analysis <ref> [17] </ref>, it automatically inserts synchronization constructs into the generated parallel code to make operations execute atomically. The significant performance improvements that synchronization optimizations deliver in this context illustrates their impor- tance in achieving good parallel performance. <p> In general, the compiler may have to use an interproce-dural pointer or alias analysis to compute a reasonably precise read set [6, 21, 10]. In restricted contexts the compiler may be able to use simpler algorithms. Our prototype compiler, for example, is designed for object-based programs <ref> [17] </ref>. Because these programs use references to objects instead of pointers, it is possible to extract a reasonable read set directly from the expressions in the node. * Write Set: A conservative approximation of the set of variables that the node's computation writes. correspond to nodes of different types. <p> The compiler uses commutativity analysis <ref> [17] </ref> to extract the concurrency in the program. It views the computation as consisting of a sequence of operations on objects, then analyzes the program to determine if operations commute (two operations commute if they generate the same result regardless of the order in which they execute).
Reference: [18] <author> V. Saraswat, M. Rinard, and P. Panangaden. </author> <title> Semantic foundations of concurrent constraint programming. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 333-352, </pages> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: opportunities that appear only in explicitly parallel programs rather than on the significant challenges associated with applying standard optimizations to parallel programs. 7.3 Concurrent Constraint Programming The lock movement transformations are reminiscent of transformations from the field of concurrent constraint programming that propagate tell and ask constructs through the program <ref> [18] </ref>. The goal is to make tells and corresponding asks adjacent in the program. This adjacency enables an optimization that removes the ask construct. A difference is the asymmetry of asks and tells: the optimization that eliminates the ask leaves the tell in place.
Reference: [19] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We report performance results for three automatically parallelized scientific applications: the Barnes-Hut hierarchical N-body solver [1], the Water code <ref> [19] </ref> and the String code [8]. Barnes-Hut simulates the trajectories of a set of interacting bodies under Newtonian forces; it consists of approximately 1500 lines of C++ code. Water simulates the interaction of water molecules in the liquid state; it consists of approximately 1850 lines of C++ code. <p> The performance of the serial C++ versions of Barnes-Hut and Water is slightly better than the performance of highly optimized parallel C versions from the SPLASH-2 benchmark set <ref> [19] </ref> running on one processor. The performance of the serial C++ version of String is approximately 1% slower than the original C version. 6.1 Methodology The compiler currently supports all three false exclusion policies described in Section 5.1.
Reference: [20] <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144-155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The majority of synchronization optimization research has concentrated on removing barriers or converting barrier synchronization constructs to more efficient synchronization constructs such as counters <ref> [20] </ref>. Several researchers have also explored optimizations geared towards exploiting more fine-grained con-currency available within loops [4]. These optimizations automatically insert one-way synchronization constructs such as post and wait to implement loop-carried data dependences. The transformations and algorithms presented in this paper address a different problem.
Reference: [21] <author> R. Wilson and M. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Program Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In general, the compiler may have to use an interproce-dural pointer or alias analysis to compute a reasonably precise read set <ref> [6, 21, 10] </ref>. In restricted contexts the compiler may be able to use simpler algorithms. Our prototype compiler, for example, is designed for object-based programs [17].
References-found: 21

