URL: ftp://ftp.csd.uu.se/pub/papers/reports/0078/9-fernandez+carro+hermenegildo.ps.gz
Refering-URL: http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp94-w6.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: hermeg@dia.fi.upm.es  
Title: IDRA (IDeal Resource Allocation): A Tool for Computing Ideal Speedups  
Author: M. J. Fernandez M. Carro M. Hermenegildo fmjf, mcarro, 
Address: Boadilla del Monte, Madrid 28660|Spain  
Affiliation: Facultad de Informatica Universidad Politecnica de Madrid (UPM)  
Abstract: Performance studies of actual parallel systems usually tend to concentrate on the effectiveness of a given implementation. This is often done in the absolute, without quantitave reference to the potential parallelism contained in the programs from the point of view of the execution paradigm. We feel that studying the parallelism inherent to the programs is interesting, as it gives information about the best possible behavior of any implementation and thus allows contrasting the results obtained. We propose a method for obtaining ideal speedups for programs through a combination of sequential or parallel execution and simulation, and the algorithms that allow implementing the method. Our approach is novel and, we argue, more accurate than previously proposed methods, in that a crucial part of the data the execution times of tasks is obtained from actual executions, while speedup is computed by simulation. This allows obtaining speedup (and other) data under controlled and ideal assumptions regarding issues such as number of processor, scheduling algorithm and overheads, etc. The results obtained can be used for example to evaluate the ideal parallelism that a program contains for a given model of execution and to compare such "perfect" parallelism to that obtained by a given implementation of that model. We also present a tool, IDRA, which implements the proposed method, and results obtained with IDRA for benchmark programs, which are then compared with those obtained in actual executions on real parallel systems.
Abstract-found: 1
Intro-found: 1
Reference: [AK90] <author> K.A.M. Ali and R. Karlsson. </author> <title> The Muse Or-Parallel Prolog Model and its Performance. </title> <booktitle> In 1990 North American Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In recent years a number of parallel implementations of logic programming languages, and, in particular, of Prolog, have been proposed (some examples are <ref> [HG90, AK90, SCWY90, She92, Lus90] </ref>). Relatively extensive studies have been performed regarding the performance of these systems. However, these studies generally report only the absolute data obtained in the experiments including at most a comparison with other actual systems implementing the same paradigm.
Reference: [CGH93] <author> M. Carro, L. Gomez, and M. Hermenegildo. </author> <title> Some Paradigms for Visualizing Parallel Execution of Logic Programs. </title> <booktitle> In 1993 International Conference on Logic Programming, </booktitle> <pages> pages 184-201. </pages> <publisher> MIT Press, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: The traces used by IDRA are the same as those used by VisAndOr <ref> [CGH93] </ref>, a tool to visualize parallel execution of logic programs, and thus it can be used to calculate ideal and maximum speedups for the systems VisAndOr can visualize (namely, the independent and-parallel system &-Prolog and the or-parallel systems Muse and Aurora; the deterministic dependent and-parallel system Andorra-I is not supported yet).
Reference: [Con83] <author> J. S. Conery. </author> <title> The And/Or Process Model for Parallel Interpretation of Logic Programs. </title> <type> PhD thesis, </type> <institution> The University of California At Irvine, </institution> <year> 1983. </year> <type> Technical Report 204. </type>
Reference-contexts: Otherwise a similar representation would be recursively applied. 1 Non-restricted Independentand-parallelism allows execution structures which cannot be described by fork-join events. Such structures are generated, for example, by Conery's or Lin and Kumar's models <ref> [Con83, LK88] </ref> and by &-Prolog when wait is used. 2 Although all-solutions predicates can be depicted using this paradigm, the resulting representation is not natural. A 4 2.4 The Execution Graph Traces are converted into execution graphs, which are used by the simulator as its first structure.
Reference: [GHPC94] <author> G. Gupta, M. Hermenegildo, Enrico Pontelli, and Vtor Santos Costa. </author> <title> ACE: And/Or-parallel Copying-based Execution of Logic Programs. </title> <booktitle> In International Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <month> June </month> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: In the future, optimizations would be necessary in order to allow larger traces to be processed in a reasonable amount of time. We also plan to modify the simulator in order to support other execution paradigms, such as Andorra-I [SCWY90], ACE <ref> [GHPC94] </ref>, AKL [JH91], IDIOM [GSCYH91] etc. and study other scheduling algorithms. Finally, we believe the same approach can be used to study issues other than ideal speedup, such as memory consumption, copying overhead, etc. 8 Acknowledgments We would like to thank S.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability. W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: Unfortunately, obtaining an optimal task/processor allocation is, in general, an N P complete problem <ref> [GJ79] </ref>. Since we want to deal with sizeable, non trivial, programs, this option is too computationally expensive to be used.
Reference: [GSCYH91] <author> G. Gupta, V. Santos-Costa, R. Yang, and M. Hermenegildo. IDIOM: </author> <title> Integrating Dependent and-, Independent and-, and Or-parallelism. </title> <booktitle> In 1991 International Logic Programming Symposium, </booktitle> <pages> pages 152-166. </pages> <publisher> MIT Press, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In the future, optimizations would be necessary in order to allow larger traces to be processed in a reasonable amount of time. We also plan to modify the simulator in order to support other execution paradigms, such as Andorra-I [SCWY90], ACE [GHPC94], AKL [JH91], IDIOM <ref> [GSCYH91] </ref> etc. and study other scheduling algorithms. Finally, we believe the same approach can be used to study issues other than ideal speedup, such as memory consumption, copying overhead, etc. 8 Acknowledgments We would like to thank S.
Reference: [HB88] <author> Kai Hwang and Faye Briggs. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <year> 1988. </year>
Reference-contexts: Scheduling algorithms can be classified depending on whether they are deterministic (used when all data pertaining the execution is available [MC69, LL74, Hu61]) or non deterministic (in which random variables with known characteristic function are used to model non available data <ref> [HB88] </ref>). Our case is the first one. <p> The algorithm we implemented to find out quasi-optimal schedulings is the so-called subsets algorithm. This algorithm in fact gives optimal results under certain conditions (which are, however, not met in our more general case). The reader is referred to <ref> [HB88] </ref> for more information on this issue. Although a (quasi-)optimal scheduling gives an estimation of the maximum speedup for a given execution and number of processors, this scheduling is not likely to be found in a real system.
Reference: [Her87] <author> M. V. Hermenegildo. </author> <title> Relating Goal Scheduling, Precedence, and Memory Management in AND-Parallel Execution of Logic Programs. </title> <booktitle> In Fourth International Conference on Logic Programming, </booktitle> <pages> pages 556-575. </pages> <publisher> University of Melbourne, MIT Press, </publisher> <month> May </month> <year> 1987. </year>
Reference-contexts: Although a (quasi-)optimal scheduling gives an estimation of the maximum speedup for a given execution and number of processors, this scheduling is not likely to be found in a real system. That is why we also implemented an approximate version of the scheduling scheme found in the &-Prolog system <ref> [HG91, Her87] </ref>. We expect the comparison of the actual &-Prolog system speedups and the results obtained from IDRA to serve as an assessment of the accuracy of our technique, whereas the comparison among a (quasi-)optimal scheduling and a real one would serve to estimate the performance of the actual system.
Reference: [HG90] <author> M. Hermenegildo and K. Greene. </author> <title> &-Prolog and its Performance: Exploiting Independent And-Parallelism. </title> <booktitle> In 1990 International Conference on Logic Programming, </booktitle> <pages> pages 253-268. </pages> <publisher> MIT Press, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In recent years a number of parallel implementations of logic programming languages, and, in particular, of Prolog, have been proposed (some examples are <ref> [HG90, AK90, SCWY90, She92, Lus90] </ref>). Relatively extensive studies have been performed regarding the performance of these systems. However, these studies generally report only the absolute data obtained in the experiments including at most a comparison with other actual systems implementing the same paradigm. <p> Consider the &-Prolog <ref> [HG90] </ref> program below, where the "&" operator, in place of the comma operator, stands for and-parallel execution (a : : : g are assumed to be sequential): main:- a, c & b, g. c:- d & e & f.
Reference: [HG91] <author> M. Hermenegildo and K. Greene. </author> <title> The &-prolog System: Exploiting Independent And-Parallelism. New Generation Computing, </title> <address> 9(3,4):233-257, </address> <year> 1991. </year>
Reference-contexts: Although a (quasi-)optimal scheduling gives an estimation of the maximum speedup for a given execution and number of processors, this scheduling is not likely to be found in a real system. That is why we also implemented an approximate version of the scheduling scheme found in the &-Prolog system <ref> [HG91, Her87] </ref>. We expect the comparison of the actual &-Prolog system speedups and the results obtained from IDRA to serve as an assessment of the accuracy of our technique, whereas the comparison among a (quasi-)optimal scheduling and a real one would serve to estimate the performance of the actual system.
Reference: [Hu61] <author> T.C. Hu. </author> <title> Parallel sequencing and assembly line problems. </title> <journal> Operating Research, </journal> <volume> 9(6) </volume> <pages> 841-848, </pages> <month> November </month> <year> 1961. </year>
Reference-contexts: Scheduling algorithms can be classified depending on whether they are deterministic (used when all data pertaining the execution is available <ref> [MC69, LL74, Hu61] </ref>) or non deterministic (in which random variables with known characteristic function are used to model non available data [HB88]). Our case is the first one.
Reference: [JH91] <author> S. Janson and S. Haridi. </author> <title> Programming Paradigms of the Andorra Kernel Language. </title> <booktitle> In 1991 International Logic Programming Symposium, </booktitle> <pages> pages 167-183. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: In the future, optimizations would be necessary in order to allow larger traces to be processed in a reasonable amount of time. We also plan to modify the simulator in order to support other execution paradigms, such as Andorra-I [SCWY90], ACE [GHPC94], AKL <ref> [JH91] </ref>, IDIOM [GSCYH91] etc. and study other scheduling algorithms. Finally, we believe the same approach can be used to study issues other than ideal speedup, such as memory consumption, copying overhead, etc. 8 Acknowledgments We would like to thank S.
Reference: [LK88] <author> Y. J. Lin and V. Kumar. </author> <title> AND-Parallel Execution of Logic Programs on a Shared Memory Multiprocessor: A Summary of Results. </title> <booktitle> In Fifth International Conference and Symposium on Logic Programming, </booktitle> <pages> pages 1123-1141. </pages> <address> University of Washington, </address> <publisher> MIT Press, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: Otherwise a similar representation would be recursively applied. 1 Non-restricted Independentand-parallelism allows execution structures which cannot be described by fork-join events. Such structures are generated, for example, by Conery's or Lin and Kumar's models <ref> [Con83, LK88] </ref> and by &-Prolog when wait is used. 2 Although all-solutions predicates can be depicted using this paradigm, the resulting representation is not natural. A 4 2.4 The Execution Graph Traces are converted into execution graphs, which are used by the simulator as its first structure.
Reference: [LL74] <author> J.W. Liu and C. L. Liu. </author> <title> Bounds on scheduling algorithms for the heterogeneous computing systems. </title> <booktitle> In 1974 Proceedings IFIP Congress, </booktitle> <pages> pages 349-353, </pages> <year> 1974. </year>
Reference-contexts: Scheduling algorithms can be classified depending on whether they are deterministic (used when all data pertaining the execution is available <ref> [MC69, LL74, Hu61] </ref>) or non deterministic (in which random variables with known characteristic function are used to model non available data [HB88]). Our case is the first one.
Reference: [Lus90] <editor> E. Lusk et. al. </editor> <title> The Aurora Or-Parallel Prolog System. New Generation Computing, </title> <address> 7(2,3), </address> <year> 1990. </year>
Reference-contexts: 1 Introduction In recent years a number of parallel implementations of logic programming languages, and, in particular, of Prolog, have been proposed (some examples are <ref> [HG90, AK90, SCWY90, She92, Lus90] </ref>). Relatively extensive studies have been performed regarding the performance of these systems. However, these studies generally report only the absolute data obtained in the experiments including at most a comparison with other actual systems implementing the same paradigm.
Reference: [MC69] <author> R.R. Muntz and E.G. Coffman. </author> <title> Optimal preemptive scheduling on two processor systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1014-1020, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: Scheduling algorithms can be classified depending on whether they are deterministic (used when all data pertaining the execution is available <ref> [MC69, LL74, Hu61] </ref>) or non deterministic (in which random variables with known characteristic function are used to model non available data [HB88]). Our case is the first one.
Reference: [MH90] <author> K. Muthukumar and M. Hermenegildo. </author> <title> The CDG, UDG, and MEL Methods for Automatic Compile-time Parallelization of Logic Programs for Independent And-parallelism. </title> <booktitle> In 1990 International Conference on Logic Programming, </booktitle> <pages> pages 221-237. </pages> <publisher> MIT Press, </publisher> <month> June </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Optimal scheduling algorithms and currently implemented algorithms are clear candidates to be studied. Maximum parallelism is useful in order to find out the absolute maximum performance of a program. This would serve to compare different programs: for example, different parallelizations/sequentializations of a given program (i.e., when different annotators <ref> [MH90] </ref> for parallelism are being used) or different parallel algorithms proposed for a given problem. Ideal parallelism is useful in order to compare a given implementation against its ideal behavior for a given number of processors.
Reference: [SCWY90] <author> V. Santos-Costa, D.H.D. Warren, and R. Yang. Andorra-I: </author> <title> A Parallel Prolog System that Transparently Exploits both And- and Or-parallelism. </title> <booktitle> In Proceedings of the 3rd. ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: 1 Introduction In recent years a number of parallel implementations of logic programming languages, and, in particular, of Prolog, have been proposed (some examples are <ref> [HG90, AK90, SCWY90, She92, Lus90] </ref>). Relatively extensive studies have been performed regarding the performance of these systems. However, these studies generally report only the absolute data obtained in the experiments including at most a comparison with other actual systems implementing the same paradigm. <p> Our Prolog implementation of this phase is rather naive. In the future, optimizations would be necessary in order to allow larger traces to be processed in a reasonable amount of time. We also plan to modify the simulator in order to support other execution paradigms, such as Andorra-I <ref> [SCWY90] </ref>, ACE [GHPC94], AKL [JH91], IDIOM [GSCYH91] etc. and study other scheduling algorithms. Finally, we believe the same approach can be used to study issues other than ideal speedup, such as memory consumption, copying overhead, etc. 8 Acknowledgments We would like to thank S.
Reference: [Seq87] <institution> Sequent Computer Systems, Inc. </institution> <note> Sequent Guide to Parallel Programming, </note> <year> 1987. </year>
Reference-contexts: To obtain accurate timings we used the microsecond resolution clock available in some Sequent multiprocessors <ref> [Seq87] </ref>. This clock is not only very precise, but also memory mapped and can thus be accessed in the time corresponding to one memory access, with negligible effect on performance.
Reference: [SH91] <author> K. Shen and M. Hermenegildo. </author> <title> A Simulation Study of Or- and Independent And-parallelism. </title> <booktitle> In 1991 International Logic Programming Symposium. </booktitle> <publisher> MIT Press, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: This is understandable and appropriate in that usually what these studies try to asses is the effectiveness of a given implementation against state-of-the-art sequential Prolog implementations or against similar parallel systems. In this paper, and in line with <ref> [SH91] </ref>, we pose and try to answer a different question: given a (parallel) execution paradigm, how large is the maximum benefit that can be obtained from executing a program in parallel in a system designed according to that paradigm? What are the resources (for example, processors) needed to exploit all parallelism <p> It appears that any approach for obtaining such an answer has to resort to a greater or lesser extent to simulations. 1 There has been some previous work in the area of ideal parallel performance determination through simulation, in particular, the work of Shen <ref> [SH91] </ref> and Sehr [SK92]. These approaches are similar in spirit and objective to ours, but differ in the approach (and the results). In [SH91] a method is proposed for the evaluation of potential parallelism. <p> lesser extent to simulations. 1 There has been some previous work in the area of ideal parallel performance determination through simulation, in particular, the work of Shen <ref> [SH91] </ref> and Sehr [SK92]. These approaches are similar in spirit and objective to ours, but differ in the approach (and the results). In [SH91] a method is proposed for the evaluation of potential parallelism. The program is first executed by a high-level meta-interpreter/simulator which computes ideal speedups for independent and-parallelism, or-parallelism, and combinations thereof. Such speedups can be obtained for different numbers of processors. <p> Independent and-parallel execution is handled in a similar way by explicitly taking care of the dependencies in the program. Although this method can be more accurate than that of <ref> [SH91] </ref> it also has some drawbacks. One is the fact mentioned above that only maximal speedups are computed, although this could presumably be solved with a back-end implementing scheduling algorithms such as the ones that we will present.
Reference: [She92] <author> K. Shen. </author> <title> Exploiting Dependent And-Parallelism in Prolog: The Dynamic, Dependent And-Parallel Scheme. </title> <booktitle> In Proc. Joint Int'l. Conf. and Symp. on Logic Prog. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years a number of parallel implementations of logic programming languages, and, in particular, of Prolog, have been proposed (some examples are <ref> [HG90, AK90, SCWY90, She92, Lus90] </ref>). Relatively extensive studies have been performed regarding the performance of these systems. However, these studies generally report only the absolute data obtained in the experiments including at most a comparison with other actual systems implementing the same paradigm.
Reference: [SK92] <author> D.C. Sehr and L.V. Kale. </author> <title> Estimating the Inherent Parallelism in Logic Programs. </title> <booktitle> In Proceedings of the Fifth Generation Computer Systems, </booktitle> <pages> pages 783-790. </pages> <address> Tokio, </address> <publisher> ICOT, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: It appears that any approach for obtaining such an answer has to resort to a greater or lesser extent to simulations. 1 There has been some previous work in the area of ideal parallel performance determination through simulation, in particular, the work of Shen [SH91] and Sehr <ref> [SK92] </ref>. These approaches are similar in spirit and objective to ours, but differ in the approach (and the results). In [SH91] a method is proposed for the evaluation of potential parallelism. <p> The second drawback is that the meta-interpretive method used for running the programs limits the size of the executions which can be studied due to the time and memory consumption implied. In <ref> [SK92] </ref> a different approach was used, in order to overcome the limitations of the method presented above. The Prolog program is instrumented to count the number of WAM instructions executed at each point, assuming a constant cost for each WAM instruction. Only "maximal" speedup is provided.
References-found: 22

