URL: http://www.cs.purdue.edu/homes/sb/papers/ieee_ipps95.ps
Refering-URL: http://www.cs.purdue.edu/homes/sb/papers/LIST.html
Root-URL: http://www.cs.purdue.edu
Title: Adaptive Load Balancing Strategies for Solving Irregular Problems on Distributed Memory MIMD Systems  
Author: Ioana M. Boier Martin, Dan C. Marinescu and John R. Rice 
Address: West Lafayette, IN 47907  
Affiliation: Department of Computer Science Purdue University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M.C. Cornea-Hasegan, D.C. Marinescu, and Z. Zhang, </author> <title> Data management for a class of iterative computations on distributed memory MIMD systems, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol 6(3) (1994), </volume> <pages> pp. 205-229. </pages>
Reference-contexts: The symmetry of the crystalline structures of the molecules gives a relationship between any particular point and many other points scattered throughout the data set (and hence in different work units). See <ref> [1] </ref> for details of the schemes and how the irregularity is handled. The work units are called Data Allocation Units, DAUs, defined according to the needs of the computational algorithm. <p> Figure 2 shows the effect of varying (1 ) on the number of DAU faults. For one of the structural biology problems discussed in [3] and <ref> [1] </ref> the execution time with the "naive" load balancing method, namely dividing evenly the DAUs among the PEs, was 3,892 seconds. The hybrid load distribution led to a reduction of the execution time to 3,499 seconds. <p> Therefore, a smaller number of bits, say d, can be used to decide which DAU to be replaced when a fault occurs. The number d is called the degree of DAU fault lookahead <ref> [1] </ref>. In this case, when processing DAU m (where i k m i k+1 d), the value to be minimized is over 1 n N DAU . <p> (i) Workload (time) of the ith unit, i = 1; 2; : : : ; N . m (i) Memory size of the ith unit, i = 1; 2; : : : ; N . f (:) Probability distribution of workloads for the units, initially uniformly distributed in the range <ref> [1, w max ] </ref>. v Variation (percent) of the workload between iterations is uniformly distributed in the range [v; v] The computing system is characterized by: P Number of processors (identical).
Reference: [2] <author> J.M. Lemme and J.R. Rice, </author> <title> Speedup in parallel algorithms for adaptive quadrature, </title> <journal> J. Assoc. Comp. Mach., </journal> <volume> 26 (1979), </volume> <pages> pp. 65-71. </pages>
Reference-contexts: The static load distribution may be viewed as repar-titioning the global geometry into subdomains, each one becoming a work unit. For computations involving singular events (such as the impact of a projectile on a tank), it is known <ref> [2] </ref> in simpler situations that load balancing requires a global strategy of redistributing work; local migration approaches alone cannot maintain anything approaching optimal load balance. This restructuring of the geometry and work units is simple to visualize abstractly, but expensive to carry out.
Reference: [3] <author> D.C. Marinescu, J.R. Rice, M.A. Cornea-Hasegan, R.E. Lynch, and M.G. Rossmann, </author> <title> Macromolecular electron density averaging on distributed Memory MIMD machines, </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> vol 5(8) (1993), </volume> <pages> pp. 635-657. </pages>
Reference-contexts: The estimation of the amount of work associated with each DAU is algorithm dependent. In the structural biology application discussed in <ref> [3] </ref>, each grid point has associated with it a mask indicating if the grid-point is in the solvent, nucleic acid, or protein. <p> Figure 2 shows the effect of varying (1 ) on the number of DAU faults. For one of the structural biology problems discussed in <ref> [3] </ref> and [1] the execution time with the "naive" load balancing method, namely dividing evenly the DAUs among the PEs, was 3,892 seconds. The hybrid load distribution led to a reduction of the execution time to 3,499 seconds.
Reference: [4] <author> H.S. McFaddin and J.R. Rice, </author> <title> Collaborating PDE solvers, </title> <journal> Applied Num. Math., </journal> <volume> 10 (1992), </volume> <pages> pp. 279-215. </pages>
Reference-contexts: The solvers then adjust the boundary conditions along the domain interfaces as part of an iteration which intended to either (a) converge to a steady state solution of the problem, or (b) simulate time dependent behavior. The mathematical basis of this computation is described in <ref> [4] </ref> and [5] and is not very relevant to the current discussion. The computation thus consists of a 2-D or 3-D network of solvers connected in a somewhat irregular way.
Reference: [5] <author> M. Mu and J.R. Rice, </author> <title> Modeling with collaborating PDE solvers | Theory and practice, </title> <booktitle> Contemporary Mathematics, </booktitle> <year> (1995), </year> <note> to appear. </note>
Reference-contexts: The solvers then adjust the boundary conditions along the domain interfaces as part of an iteration which intended to either (a) converge to a steady state solution of the problem, or (b) simulate time dependent behavior. The mathematical basis of this computation is described in [4] and <ref> [5] </ref> and is not very relevant to the current discussion. The computation thus consists of a 2-D or 3-D network of solvers connected in a somewhat irregular way.
References-found: 5

