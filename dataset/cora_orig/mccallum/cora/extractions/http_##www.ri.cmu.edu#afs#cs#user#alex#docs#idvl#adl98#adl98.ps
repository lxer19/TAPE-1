URL: http://www.ri.cmu.edu/afs/cs/user/alex/docs/idvl/adl98/adl98.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/alex/www/HomePage.html
Root-URL: 
Email: Email: alex+@cs.cmu.edu  Email: witbrock@justresearch.com  
Phone: Tel: 1-412-348-8848  Tel: 1-412-683-9486  
Title: Story Segmentation and Detection of Commercials In Broadcast News Video  
Author: Alexander G. Hauptmann Michael J. Witbrock 
Keyword: Segmentation, video processing, broadcast news story analysis, closed captioning, digital library, video library creation, speech recognition.  
Address: Pittsburgh, PA 15213-3890, USA  4616 Henry St. Pittsburgh, PA 15213, USA  
Affiliation: Department of Computer Science Carnegie Mellon University  Justsystem Pittsburgh Research Center  
Abstract: The Informedia Digital Library Project [Wactlar96] allows full content indexing and retrieval of text, audio and video material. Segmentation is an integral process in the Informedia digital video library. The success of the Informedia project hinges on two critical assumptions: that we can extract sufficiently accurate speech recognition transcripts from the broadcast audio and that we can segment the broadcast into video paragraphs, or stories, that are useful for information retrieval. In previous papers [Hauptmann97, Witbrock97, Witbrock98], we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we address the issue of segmentation and demonstrate that a fully automatic system can extract story boundaries using available audio, video and closed-captioning cues. The story segmentation step for the Informedia Digital Video Library splits full-length news broadcasts into individual news stories. During this phase the system also labels commercials as separate stories. We explain how the Informedia system takes advantage of the closed captioning frequently broadcast with the news, how it extracts timing information by aligning the closed-captions with the result of the speech recognition, and how the system integrates closed-caption cues with the results of image and audio processing. 
Abstract-found: 1
Intro-found: 1
Reference: [Beeferman97] <author> Beeferman, D., Berger, A., and Lafferty. J., </author> <title> Text segmentation using exponential models. </title> <booktitle> In Proc. Empirical Methods in Natural Language Processing 2 (AAAI) '97, </booktitle> <address> Providence, RI, </address> <year> 1997. </year>
Reference: [Brown95] <author> Brown, M. G., Foote, J. T., Jones, G. J. F., Sprck-Jones, K. and Young, S. J, </author> <title> Automatic Content-Based Retrieval of Broadcast News, </title> <booktitle> ACM Multimedia-95, </booktitle> <address> p. 35 - 42, San Francisco, CA 1995. </address>
Reference-contexts: Average Human segmentation performance is given for comparison. The results for no segmentation, segmentation every second and segmentation into fixed-width blocks corresponding to the average reference story length are given for reference. DISCUSSION Unfortunately, these results cannot be directly compared with either the results in <ref> [Brown95] </ref> or [Merlino97]. Brown et al used a criterion of recall and precision for information retrieval. This was only possible with respect to a set of information retrieval queries, and given the existence of a human relevance judgement for every query against every document.
Reference: [CMUseg97] <institution> CMUseg, Carnegie Mellon University Audio Segmentation Package, ftp://jaguar.ncsl.nist.gov/pub/CMUseg_0.4a.tar.Z, </institution> <year> 1997. </year>
Reference-contexts: Alternatively, one can use the silences detected by the speech recognizer, which explicitly models and detects pauses in speech by using an acoustic model for a silence phone [Hwang94]. Silences are also conveniently detected by the CMUseg Audio Segmentation package <ref> [CMUseg97] </ref>. ~~flPHGRQfl~-fl7+( ~~flPHGRQfl~-fl.12:1 ~~flPHGRQfl~-fl$1' ~flPHGRQfl~-fl21 ~flLVfl~-~~fl+,6 ~flRQfl~-tfl21 ~flWKHfl~-fl+,6 ~flLQGRQHVLDQfl~~-fl6+28/' ~tflLVODQGfl~~~-fl,6/$1' ~tflVXPDWUDfl~~tfl2) ~tflVXPDWUDfl~fl680$75$ ~tflFLWHfl~~fl&,7 ( ~tflRIfl~t~fl$1 flPDQ"fl~t~fl$1 flRIfl~-fl,'($ flWKHfl~~fl7+$7 flILUHVfl~fl),5 (6 flWKDWfl~fl7+$7 flKDYHfl~fl+$9 ( flVHQWfl~~fl6 (17 flDfl~fl86 flVPRNHfl~t~fl+2: flFORXGfl~ttfl&/28' flRYHUfl~t-fl29 (5 flVL [fl~ttfl6,; flFRXQWULHVflfl&28175,(6 flFQQVflfl& flFQQVfl~fl1 flFQQVflfl16 flPDULDflfl0$5,$ flUHVVDflfl5 (66$ flLVfl~fl,6 flLQflfl,1 flWKHflfl7+( -flPDOD"VLDQfltfl/2: -flPDOD"VLDQflfl:$*( -flPDOD"VLDQflfl$1' -flFDSLWDOfl~fl&$3,7$/ -flNXDODfltfl48 (// ~flOXPSXUfl-~fl,1)250 (' ~flDQGfl~fl$1'
Reference: [Dempster77] <author> Dempster, A., Laird, N., Rubin, D., </author> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39, 1, </volume> <pages> pp. 1 38, </pages> <year> 1977. </year>
Reference-contexts: We would like to adapt our approach to work without the captioned text, relying entirely on the speech recognizer transcription, the audio signal and the video images. In the near term we plan to use the EM <ref> [Dempster77] </ref> algorithm to combining many features into one segmentation strategy, and to learn segmentation from data for which only a fraction has been hand-labeled. Work is also currently underway in the Informedia project to evaluate the effectiveness of the current segmentation approach when closed-captioning information is not available.
Reference: [Grice75] <author> Grice, H. P. </author> <title> Logic and Conversation. </title> <editor> In P. Cole (ed.) </editor> <booktitle> Syntax and Semantics. </booktitle> <volume> Vol. </volume> <pages> 3. </pages> <address> New York: </address> <publisher> Academic Press. </publisher> <month> 41-58 , </month> <year> 1975. </year>
Reference: [Hampapur94] <author> Hampapur, A., Jain, R., and Weymouth, T., </author> <title> Digital Video Segmentation, </title> <booktitle> ACM Multimedia 94, </booktitle> <pages> pp 357 364, </pages> <booktitle> ACM Intl Conf on Multimedia, </booktitle> <address> 15 20 Oct. 1994, San Francisco, CA. </address>
Reference: [Hauptmann95] <author> Hauptmann, A.G. and Smith, M.A. </author> <title> Text, Speech and Vision for Video Segmentation: the Informedia Project. </title> <booktitle> AAAI Fall Symposium on Computational Models for Integrating Language and Vision, </booktitle> <address> Boston MA, Nov 10-12, </address> <year> 1995. </year>
Reference-contexts: Scene breaks, on the other hand, appear, on average, at intervals of less than 15 seconds. To detect scene breaks in the Informedia Digital Video Library System, color histogram analysis and Lucas-Kanade optical flow analysis are applied to the MPEG-encoded video <ref> [Hauptmann95] </ref>. This also enables the software to identify editing effects such as cuts and pans that mark shot changes. An example of the result of this process is shown in Figure 1. A variation of this approach [Taniguchi95] uses a high rate of scene breaks to detect commercials. Black Frames.
Reference: [Hauptmann95b] <author> Hauptmann, A.G., Witbrock, M.J., Rudnicky, A.I., and Reed, S. </author> <title> Speech for Multimedia Information Retrieval, </title> <booktitle> UIST-95 Proceedings of the User Interface Software Technology Conference, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1995. </year>
Reference: [Hauptmann97] <author> Hauptmann, A.G. and Witbrock, M.J., </author> <title> Informedia News on Demand: Multimedia Information Acquisition and Retrieval, in Maybury, M.T., </title> <editor> Ed, </editor> <booktitle> Intelligent Multimedia Information Retrieval, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, </address> <year> 1997 </year>
Reference-contexts: While there are a number of ways to compute the SNR of an acoustic signal, none of them perfect, we have used the approach to SNR computation described in <ref> [Hauptmann97] </ref> with a window size of .25 seconds. To date we have only made informal attempts to include this audio signal data in our segmentation heuristics. We will report the results of this effort in a future paper. (human) segmentation. <p> The success of the Informedia project hinges on two critical assumptions: That we can extract sufficiently accurate speech recognition transcript from the broadcast audio and that we can segment the broadcast into video paragraphs (stories) that are useful for information retrieval. In previous papers <ref> [Hauptmann97, Witbrock97, Witbrock98] </ref>, we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we now have addressed the issue of segmentation and demonstrated that a fully automatic system can successfully extract story boundaries using available audio, video and closed-captioning cues.
Reference: [Wactlar96] <author> Wactlar, H.D., Kanade, T., Smith, M.A. and Stevens, </author> <title> S.M., Intelligent Access to Digital Video: Informedia Project. </title> <journal> IEEE Computer, </journal> <volume> 29 (5), </volume> <pages> 46-52, </pages> <month> May </month> <year> 1996. </year> <note> See also http://www.informedia.cs.cmu.edu/. </note>
Reference: [Hauptmann97b] <author> Hauptmann, A.G., Witbrock, M.J. and Christel, M.G. </author> <title> Artificial Intelligence Techniques in the Interface to a Digital Video Library, </title> <booktitle> Proceedings of the CHI-97 Computer-Human Interface Conference New Orleans LA, </booktitle> <month> March </month> <year> 1997. </year>
Reference: [Hearst93] <author> Hearst, M.A. and Plaunt, C., </author> <title> Subtopic structuring for full-length document access, </title> <booktitle> in Proc ACM SIGIR-93 Intl Conf. On Research and Development in Information Retrieval, </booktitle> <pages> pp. 59 68, </pages> <address> Pittsburgh PA, </address> <year> 1993. </year>
Reference: [Hwang94] <author> Hwang, M., Rosenfeld, R., Thayer, E., Mosur, R., Chase, L., Weide, R., Huang, X., and Alleva, F., </author> <title> Improving Speech Recognition Performance via PhoneDependent VQ Codebooks and Adaptive Language Models in SPHINX-II. </title> <journal> ICASSP-94, </journal> <volume> vol. I, </volume> <pages> pp. 549-552, </pages> <year> 1994. </year>
Reference-contexts: This information is not available from the closed captioning and must be derived by other means. A large vocabulary speech recognition system such as Sphinx-II can provide this information <ref> [Hwang94] </ref>. Given exact timings for a partially erroneous transcription output by the speech recognition system one can align the transcript words to the precise location where each word was spoken, within 10 milliseconds. <p> Low values of the total power over the same one second window also indicate a silence. Alternatively, one can use the silences detected by the speech recognizer, which explicitly models and detects pauses in speech by using an acoustic model for a silence phone <ref> [Hwang94] </ref>.
Reference: [Kaszkiel97] <author> Kaszkiel, M. and Zobel, J., </author> <title> Passage Retrieval Revisited, </title> <journal> pp. </journal> <volume> 178 185, </volume> <booktitle> SIGIR-97 Proceedings of the 20 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Philadelphia, PA July 27 31, </address> <year> 1997. </year>
Reference-contexts: The adequacy of segmentation depends on what you need to do with the segments. We are now in a position to evaluate the effectiveness of our segmentation process with respect to information retrieval, story tracking, or information extraction into semantic frames. Some approaches from the information retrieval literature <ref> [Kaszkiel97] </ref> claim that overlapping windows within an existing document can improve the accuracy of the information retrieval. It remains for future work to determine if a modification of this technique can circumvent the problem of static segmentation in the broadcast news video domain.
Reference: [Kobla97] <author> Kobla, V., Doermann, D., and Faloutsos, D., </author> <title> Video Trails: Representing and Visualizing Structure in Video Sequences, </title> <booktitle> ACM Multimedia 97, </booktitle> <address> Seattle, WA, </address> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: Another source of information is the similarity between different scenes. The anchor, especially, will reappear at intervals throughout a news program, and each appearance is likely to denote a segmentation boundary of some type. The notion of frame similarity across scenes is fundamental to both the Video Trails work <ref> [Kobla97] </ref> and to video storyboards [Yeung96, Yeung96b].
Reference: [Mani96] <author> Mani, I., House, D., Maybury, M. and Green, M. </author> <title> Towards Content-Based Browsing of Broadcast News Video, </title> <editor> in Maybury, M. T. (editor), </editor> <booktitle> Intelligent Multimedia Information Retrieval, </booktitle> <year> 1997. </year>
Reference: [Maybury96] <author> Maybury, M., Merlino, A., and Rayson, J., </author> <title> Segmentation, Content Extraction and Visualization of Broadcast News Video using Multistream Analysis, </title> <booktitle> in Proceedings of the ACM International Conference on Multimedia, </booktitle> <address> Boston, MA, </address> <year> 1996. </year>
Reference: [Merlino97] <author> Merlino, A., Morey, D., and Maybury, M., </author> <title> Broadcast News Navigation using Story Segmentation, </title> <booktitle> ACM Multimedia 1997, </booktitle> <month> November </month> <year> 1997 </year>
Reference-contexts: However, blank, or black frames also occur at other points during regular broadcasts. Because of the quality of the MPEG encoded analog signal, it may also not be possible to distinguish a very dark frame from a black frame. <ref> [Merlino97] </ref> also found black frames to be useful for story segmentation. Like so many of these cues, black frames are not by themselves a reliable indicator of segment boundaries. However, they provide added information to improve the segmentation process. Frame Similarity. <p> Average Human segmentation performance is given for comparison. The results for no segmentation, segmentation every second and segmentation into fixed-width blocks corresponding to the average reference story length are given for reference. DISCUSSION Unfortunately, these results cannot be directly compared with either the results in [Brown95] or <ref> [Merlino97] </ref>. Brown et al used a criterion of recall and precision for information retrieval. This was only possible with respect to a set of information retrieval queries, and given the existence of a human relevance judgement for every query against every document.
Reference: [MPEG-ISO] <institution> International Standard ISO/IEC-CD 11172 Information Technology Coding of Moving Pictures & Associated Audio for Digital Storage, International Standards Organization. </institution>
Reference-contexts: MPEG optical flow for motion estimation: In the MPEG video stream there is a direct encoding of the optical flow within the current images <ref> [MPEG-ISO] </ref>. This encoded value can be extracted and used to provide information about whether there is camera motion or motion of objects within the scene. Scenes containing movement, for example, may be less likely to occur at story boundaries.
Reference: [Nye84] <author> Nye, H. </author> <title> The Use of a One Stage Dynamic Programming Algorithm for Connected Word Recognition, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> Vol. AASP-32, No 2, </volume> <pages> pp. 262-271, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The alignment process tries to find the best way of matching up the two strings, allowing for pieces of data to be inserted, deleted or substituted, such that the resulting paired string gives the best possible match between the two streams. The well-known Dynamic Time Warping procedure (DTW) <ref> [Nye84] </ref> will accomplish with a guaranteed least cost distance for two text strings. Usually the cost is simply measured as the total number of insertions, deletions and substitutions required to make the strings identical.
Reference: [Pentland94] <author> Pentland A., Moghaddam B., and Starner T. </author> <title> View-Based and Modular Eigenspaces for Face Recognition IEEE Conference on Computer Vision & Pattern Recognition, </title> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The transcript shows only the first 73 seconds of data received for the 1-hour program. 2.fl Face similarity is computed by first using CMUs face detection algorithm [Rowley95]. Any faces in all key frames are detected, and then these faces are compared using the eigenface technique developed at MIT <ref> [Pentland94] </ref>. Once a matrix of pairwise similarity coefficients has been computed, we again select the most popular face and its closest matches as candidates for segmentation boundaries.
Reference: [Placeway96] <author> Placeway, P. and Lafferty, J., </author> <title> Cheating with Imperfect Transcripts, </title> <booktitle> ICSLP-96 Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, PA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: This performance is, however, still sufficient for alignment with a closed-captioned transcript. Methods for improving the raw speech recognition accuracy when captioned transcripts are available before recognition are outlined in <ref> [Placeway96] </ref>. The basic problem for alignment is to take two strings (or streams) or data, where sections of the data match in both strings and other sections do not.
Reference: [Rowley95] <author> Rowley, H., Baluja, S. and Kanade, T.,, </author> <title> Human Face Detection in Visual Scenes. </title> <institution> Carnegie Mellon University, School of Computer Science Technical Report CMU-CS-95-158, </institution> <address> Pittsburgh, PA, </address> <year> 1995. </year>
Reference-contexts: The transcript shows only the first 73 seconds of data received for the 1-hour program. 2.fl Face similarity is computed by first using CMUs face detection algorithm <ref> [Rowley95] </ref>. Any faces in all key frames are detected, and then these faces are compared using the eigenface technique developed at MIT [Pentland94]. Once a matrix of pairwise similarity coefficients has been computed, we again select the most popular face and its closest matches as candidates for segmentation boundaries.
Reference: [Taniguchi95] <author> Taniguchi, Y., Akutsu, A., Tonomura, Y., and Hamada, H., </author> <title> An Intuitive and Efficient Access Interface to Real-time Incoming Video based on automatic indexing, </title> <booktitle> ACM Multimedia-95, </booktitle> <address> p. 25 - 33, San Francisco, CA 1995. </address>
Reference-contexts: This also enables the software to identify editing effects such as cuts and pans that mark shot changes. An example of the result of this process is shown in Figure 1. A variation of this approach <ref> [Taniguchi95] </ref> uses a high rate of scene breaks to detect commercials. Black Frames. For technical reasons, commercials are usually preceded and followed by one or more frames that are completely black.
Reference: [Witbrock97] <author> Witbrock, M.J. and Hauptmann, </author> <title> A.G. Using Words and Phonetic Strings for Efficient Information Retrieval from Imperfectly Transcribed Spoken Documents, </title> <booktitle> DL97, The Second ACM International Conference on Digital Libraries, </booktitle> <address> Philadelphia, </address> <month> July 23 - 26, </month> <year> 1997. </year>
Reference-contexts: The success of the Informedia project hinges on two critical assumptions: That we can extract sufficiently accurate speech recognition transcript from the broadcast audio and that we can segment the broadcast into video paragraphs (stories) that are useful for information retrieval. In previous papers <ref> [Hauptmann97, Witbrock97, Witbrock98] </ref>, we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we now have addressed the issue of segmentation and demonstrated that a fully automatic system can successfully extract story boundaries using available audio, video and closed-captioning cues.
Reference: [Witbrock98] <author> Witbrock, M.J., and Hauptmann, </author> <title> A.G., Speech Recognition in a Digital Video Library, </title> <journal> Journal of the American Society for Information Science (JASIS), </journal> <note> 1998, In press. </note>
Reference-contexts: The success of the Informedia project hinges on two critical assumptions: That we can extract sufficiently accurate speech recognition transcript from the broadcast audio and that we can segment the broadcast into video paragraphs (stories) that are useful for information retrieval. In previous papers <ref> [Hauptmann97, Witbrock97, Witbrock98] </ref>, we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we now have addressed the issue of segmentation and demonstrated that a fully automatic system can successfully extract story boundaries using available audio, video and closed-captioning cues.
Reference: [Yamron97] <author> Yamron, J. </author> <title> Topic Detection and Tracking: Segmentation Task, Topic Detection and Tracking (TDT) Worksho, </title> <month> 27-28 October </month> <year> 1997, </year> <institution> College Park, </institution> <address> MD. </address> <booktitle> Also in BNTUW-98, Proceedings of the Broadcast News Transcription and Understanding Workshop, </booktitle> <address> Leesburg, VA, </address> <month> February </month> <year> 1998. </year>
Reference: [Yeung96] <author> Yeung, M., and Yeo, B.-L., </author> <title> Time constrained Clustering for Segmentation of Video into Story Units in International Conference on Pattern Recognition, </title> <month> August </month> <year> 1996. </year>
Reference-contexts: The anchor, especially, will reappear at intervals throughout a news program, and each appearance is likely to denote a segmentation boundary of some type. The notion of frame similarity across scenes is fundamental to both the Video Trails work [Kobla97] and to video storyboards <ref> [Yeung96, Yeung96b] </ref>.
Reference: [Yeung96b] <author> Yeung, M., Yeo, B.-L., and Liu, B., </author> <title> "Extracting Story Units from Long Programs for Video Browsing and Navigation" in International Conference on Multimedia Computing and Systems, </title> <month> June </month> <year> 1996. </year>
Reference-contexts: The anchor, especially, will reappear at intervals throughout a news program, and each appearance is likely to denote a segmentation boundary of some type. The notion of frame similarity across scenes is fundamental to both the Video Trails work [Kobla97] and to video storyboards <ref> [Yeung96, Yeung96b] </ref>.
Reference: [Zhang95] <author> Zhang, H.J., Low, C.Y., Smoliar S.W., and Wu, J.H., </author> <title> Video Parsing, Retrieval and Browsing: An Integrated and Content-Based Solution, </title> <booktitle> ACM Multimedia-95, </booktitle> <address> p. 15 24, San Francisco, CA 1995. </address>
References-found: 30

