URL: http://www.research.att.com/library/trs/TRs/97/97.6/97.6.1.body.ps
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient Regression Verification  
Author: R. H. Hardin* R. P. Kurshan* K. L. McMillan** J. A. Reeds* N. J. A. Sloane* 
Address: Berkeley, California 94701-1144  
Affiliation: flfl Cadence Berkeley Labs,  
Date: January 4, 1996  
Abstract: A significant problem in commercial-size development projects is to ensure that the process of fixing one design problem does not introduce another. In the context of conventional testing this is checked through regression testing. If consecutive test suites check N properties, a failure in one may require retesting all the previous suites once a fix has been made. This results in O(N 2 ) tests in all, to assure that no fix in fact breaks a previously good test. (Usually, one does not dare to defer retesting to the very end- thereby counting on nothing having been broken in the process- but retests all previous properties after each significant fix.) When formal verification is used in place of conventional testing, the analog of regression testing can be done much more simply- in some cases effectively in constant time. The key to achieving this simplification is to replace re-verification with a highly reliable "hash" check on the model parse tree, reduced relative to the verified property. If the hash value is unchanged after a change in the model, it is essentially certain that the validity of the previously verified property has not changed. fl AT&T Bell Laboratories, Murray Hill, New Jersey 07974
Abstract-found: 1
Intro-found: 1
Reference: [FIPS PUB 180-1] <institution> U.S. Department of Commerce, Federal Information Processing Standards Publication 180-1, SECURE HASH STANDARD, </institution> <year> 1995 </year> <month> April 17. </month>
Reference-contexts: To guard against the possibility of some unforeseen mechanism which systematically favors CRC hash collisions in the application at hand, and also to guard against other unforeseen factors, we also use as a second, independent, hash function the SHA-1 algorithm specified in <ref> [FIPS PUB 180-1] </ref>. This algorithm was designed to produce a "message digest" of 160 bits from a message or data file, in such a manner that it is computationally infeasible to find two different messages which produce the same message digest. <p> Its design uses principles similar to those used by R. L. Rivest of MIT when designing the MD4 message digest algorithm [MD4], and is closely modeled after that algorithm. We used an off-the-shelf implementation of SHA-1 which correctly reproduces the test values listed in <ref> [FIPS PUB 180-1] </ref>. 4. Performance Both codes together produce only 120 hexadecimal digits, so the space required to store the encoded parse tree of each reduced model is negligible. The space required to store the version of P i for which the verification succeeded, typically is .1-2 kbytes of ascii.
Reference: [HK90] <author> Z. Har'El and R. P. Kurshan. </author> <title> Software for Analytical Development of Communications Protocol. </title> <type> AT&T Tech. </type> <institution> J. </institution> <month> 69 </month> <year> (1990) </year> <month> 45-59. </month>
Reference-contexts: If the new hash value is the same as the old hash value, the validity of the property in the new model is deemed to hold, and re-verification is omitted. This scheme has been implemented in the COSPAN verification system <ref> [HK90] </ref>, with dramatic reductions in the cost of regression verification. Moreover, once one has the hash value of a verified model, that value denotes a verified model in any context (assuming the semantics of the source language does not change). <p> An extra level of safety could be obtained by having the verification tool apply a digital "hallmark" to each successful verification. The hallmark would confirm that a purportedly verified module in fact was verified. 2. Localization Reduction The COSPAN verification system <ref> [HK90] </ref> implements a localization reduction algorithm [Ku94, p. 170-172], which reduces a given model relative to a given property which is to be verified.
Reference: [Ku94] <author> R. P. Kurshan, </author> <title> Computer-aided Verification of Coordinating Processes | The Automata-Theoretic Approach, </title> <booktitle> Princeton Series in Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: An extra level of safety could be obtained by having the verification tool apply a digital "hallmark" to each successful verification. The hallmark would confirm that a purportedly verified module in fact was verified. 2. Localization Reduction The COSPAN verification system [HK90] implements a localization reduction algorithm <ref> [Ku94, p. 170-172] </ref>, which reduces a given model relative to a given property which is to be verified. The algorithm is iterative, and is based upon the monotonic nature of automata-theoretic verification, which seeks to test whether the formal language containment L (P ) L (T ) (2.1) holds. <p> It is intuitive that the variables of G (T ) are the variables upon which T ultimately depends, and that checking (2.1) can be reduced to checking (2.3), when P 0 comprises only those variables in G (T ). Formally, if P is defined in terms of component L-processes <ref> [Ku94] </ref>, as a product P = P 1 P k for a given Boolean algebra L of "system events", and T is an L-automaton, then the localiza tion reduction P 0 of P is defined as P 0 = P 0 i n where P 0 i 1 ; : : <p> fact is an *(X )-process or *(X )-automaton, respectively. (In the context of system variables, *(X) is the set of Boolean predicates (formulas) expressed in 5 terms of assignments of the variables of X.) Say an L-process P i is dependent on a subalgebra L 0 L if the projection <ref> [Ku94] </ref> L 0 P i is not a B -process, i.e., if some transition condition in the projected process is not identically 1 (true). Assume that for each i = 1; : : :; k, L (P i ) 6= . <p> For simplicity, we assume that the P i 's include no fairness constraints. That is, we assume that L (P i ) is the limit of a prefix-closed language <ref> [Ku94] </ref>. (Otherwise, the definition of dependency would need to incorporate the automaton acceptance conditions of the P i 's [Ku94].) Then, the following theorem is an easy consequence of the preceding definition. <p> For simplicity, we assume that the P i 's include no fairness constraints. That is, we assume that L (P i ) is the limit of a prefix-closed language <ref> [Ku94] </ref>. (Otherwise, the definition of dependency would need to incorporate the automaton acceptance conditions of the P i 's [Ku94].) Then, the following theorem is an easy consequence of the preceding definition. <p> The second stage of the localization reduction algorithm is initiated only if the check (2.3) runs out of memory or allotted time. In this case, P 0 is successively redefined, according to the algorithm in <ref> [Ku94, p. 172] </ref>. In this algorithm, successively P 0 = P 2P i for fT g P 1 P 2 V (G (T )). Either P 1 = fT g or P 1 is a user-defined superset of fT g. <p> Specifically, P i+1 is a smallest superset of P i such that e cannot be simulated in O P : If no such P i+1 exists, then in fact e can be extended to an error track in P <ref> [Ku94, Sec. 8.4] </ref>, and thus (2.1) fails. In terms of system variables, each successive definition of P 0 defines a section of the full model, containing the variables of T .
Reference: [MD4] <editor> The MD4 Message Digest Algorithm Advances in Cryptology-CRYPTO '90 Proceedings, </editor> <publisher> Springer-Verlag, </publisher> <pages> 1991 pp. 303-311. </pages>
Reference-contexts: Its design uses principles similar to those used by R. L. Rivest of MIT when designing the MD4 message digest algorithm <ref> [MD4] </ref>, and is closely modeled after that algorithm. We used an off-the-shelf implementation of SHA-1 which correctly reproduces the test values listed in [FIPS PUB 180-1]. 4.
Reference: [ZB69] <author> N. Zierler, J. Brillhart, </author> <title> On primitive trinomials (mod 2), II, </title> <journal> Inf. </journal> <note> Control 14 (1969) 566-569. </note>
Reference-contexts: the polynomial (1 + x)(1 + x 135 + x 316 ) = 1 + x + x 135 + x 136 + x 316 + x 317 : This incorporates the polynomial 1 + x 135 + x 316 , the highest-degree primitive trinomial of degree less than 320 <ref> [ZB69] </ref>. This limit was chosen so that this hashed value will fit into 80 hexadecimal digits.
References-found: 5

