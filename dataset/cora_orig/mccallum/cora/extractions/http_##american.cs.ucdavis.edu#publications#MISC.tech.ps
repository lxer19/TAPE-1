URL: http://american.cs.ucdavis.edu/publications/MISC.tech.ps
Refering-URL: http://www.cs.ucdavis.edu/research/tech-reports/1992.html
Root-URL: http://www.cs.ucdavis.edu
Email: (farrens@cs.ucdavis.edu) (arp@tosca.colorado.edu)  
Title: d d MISC: A Multiple Instruction Stream Computer  
Author: Gary Tyson Andrew R. Pleszkun Matthew Farrens 
Note: 1  
Address: Davis, CA 95616  (tyson@cs.ucdavis.edu) Boulder, CO 80309-0425  
Affiliation: Computer Science Department Department of Electrical and University of California, Davis Computer Engineering  University of Colorado-Boulder  
Abstract: This paper describes a single chip Multiple Instruction Stream Computer (MISC) capable of extracting instruction level parallelism from a broad spectrum of programs. The MISC architecture uses multiple asynchronous processing elements to separate a program into streams that can be executed in parallel, and integrates a conflict-free message passing system into the lowest level of the processor design to facilitate low latency intra-MISC communication. This approach allows for increased machine parallelism with minimal code expansion, and provides an alternative approach to single instruction stream multi-issue machines such as SuperScalar and VLIW. 
Abstract-found: 1
Intro-found: 1
Reference: [AlGo89] <author> G. Almasi and A. Gottlieb, </author> <title> Highly Parallel Computing, </title> <address> Benjamin/Cummings, </address> <year> (1989). </year>
Reference-contexts: These tasks can be thought of as containing explicit parallelism that can be exploited regardless of the programming model employed. Many architectures exist which exploit data parallelism (such as vector processors, dataflow machines, SIMD and MIMD designs, and many others <ref> [AlGo89] </ref>). The key difference between instruction level parallelism and data parallelism is how closely coupled the independent operations are. With instruction level parallelism, this coupling is very close, necessitating frequent bi-directional communication between operation units. This high level of communication can often be avoided in data parallel applications.
Reference: [BeDa91] <author> M. E. Benitez and J. W. Davidson, </author> <title> ``Code Generation for Streaming: an Access/Execute Mechanism'', </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA (April 8-11, </address> <year> 1991), </year> <pages> pp. 132-141. </pages>
Reference-contexts: The Lawrence Livermore Loops [McMa84] provide a convenient source for small vectoriz-able loops found in scientific code. Although originally extracted from FORTRAN code, these loops have been translated to C for purposes of this analysis. The code was then hand compiled and optimized using available vectorizing techniques <ref> [BeDa91] </ref>. - 14 - d d The C source for LLL #3 follows: main () - float z [1000], x [1000], q; int k; for ( k=0 ; k &lt; 1000 ; k++ ) - The MISC object code generated consists of 4 instruction streams.
Reference: [ClLe82] <author> D. W. Clark and H. M. Levy, </author> <title> ``Measurement and Analysis of Instruction Use in the VAX-11/780'', </title> <booktitle> Proceedings of the Ninth Annual International Symposium on Computer Architecture, </booktitle> <address> Austin, Texas (April 1982), </address> <pages> pp. 9-17. </pages>
Reference-contexts: CRAY-class computers [Inc82] focus on driving up the clock rate, while CISC machines like the VAX <ref> [ClLe82] </ref> attempt to reduce the number of instructions required to complete a task. Most current single-chip processors try to both maximize the clock rate and minimize the number of clocks per instruction (CPI) at the expense of an increase in the number of instructions required.
Reference: [CGKP87] <author> G. L. Craig, J. R. Goodman, R. H. Katz, A. R. Pleszkun, K. Ramachandran, J. Sayah and J. E. Smith, </author> <title> ``PIPE: A High Performance VLSI Processor Implementation'', </title> <journal> Journal of VLSI and Computer Systems, </journal> <volume> vol. </volume> <month> 2 </month> <year> (1987). </year>
Reference-contexts: With instruction level parallelism, this coupling is very close, necessitating frequent bi-directional communication between operation units. This high level of communication can often be avoided in data parallel applications. The MISC processor, a direct descendant of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref> will exploit both the instruction and data parallelism available in a task by combining the capabilities of traditional data parallel architectures with those found in machines designed to exploit instruction level parallelism. For example, MISC supports multiple instruction issue without sacrificing clock rate or increasing code size. <p> MISC will utilize a similar feature called processor chaining to achieve a more powerful method of chaining. 3. The MISC Design The MISC system, an outgrowth of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref>, is being developed at the University of California, Davis to study the characteristics of a broad range of C programs in order to determine the feasibility of efficiently extracting instruction level parallelism and data parallelism from a single chip (scalable) MIMD processor design.
Reference: [FaPl91a] <author> M. Farrens and A. Pleszkun, </author> <title> ``Overview of the PIPE Processor Implementation'', </title> <booktitle> Proceedings of the 24th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kapaa, Kauai (January 9-11, </address> <year> 1991), </year> <pages> pp. 433-443. </pages>
Reference-contexts: For ALU and FPU instructions, the third source field is ignored and the instructions are treated as if they were standard 3-operand instructions. In the case of control flow operations, the dest field is used as a constant to determine the number of delayed branch slots <ref> [FaPl91a] </ref> be filled. The address of the branch is calculated as the sum of the src1 and src2 operands, and the src3 operand specifies the register to be tested.
Reference: [FaPl91b] <author> M. Farrens and A. Pleszkun, </author> <title> ``Implementation of the PIPE Processor'', </title> <booktitle> Computer(January 1991), </booktitle> <pages> pp. 65-70. </pages>
Reference-contexts: With instruction level parallelism, this coupling is very close, necessitating frequent bi-directional communication between operation units. This high level of communication can often be avoided in data parallel applications. The MISC processor, a direct descendant of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref> will exploit both the instruction and data parallelism available in a task by combining the capabilities of traditional data parallel architectures with those found in machines designed to exploit instruction level parallelism. For example, MISC supports multiple instruction issue without sacrificing clock rate or increasing code size. <p> MISC will utilize a similar feature called processor chaining to achieve a more powerful method of chaining. 3. The MISC Design The MISC system, an outgrowth of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref>, is being developed at the University of California, Davis to study the characteristics of a broad range of C programs in order to determine the feasibility of efficiently extracting instruction level parallelism and data parallelism from a single chip (scalable) MIMD processor design. <p> The Delay Register is used to determine the number of delay slots that should be unconditionally executed following a branch instruction, similar to the approach used in the PIPE processor <ref> [FaPl91b] </ref>. It also contains the size of a vector loop in the VLOOP instruction discussed in section 3.2.3. Finally, each PE has its own instruction cache.
Reference: [Fish83] <author> J. A. Fisher, </author> <title> ``Very Long Instruction Word Architectures and the ELI-512'', </title> <booktitle> Proceedings of the Tenth Annual International Symposium on Computer Architecture, </booktitle> <address> Stockholm, Sweden (June 13-17, </address> <year> 1983), </year> <pages> pp. 140-149. </pages>
Reference-contexts: Similarly, an output dependency (or Write After Write (WAW) hazard) may exist between two instructions writing to the same location. Anti-dependencies and to a lesser extent output dependencies create problems for multi-issue architectures, such as SuperScalar [ Johnson 1990 ] and VLIW <ref> [Fish83] </ref>, which use shared registers to facilitate communication between the separately issued instructions. These architectures must employ hardware techniques like register renaming [Kell75] to alleviate this problem. Since MISC does not use shared registers, additional RAW and WAW hazards are avoided.
Reference: [GHLP85] <author> J. R. Goodman, J. T. Hsieh, K. Liou, A. R. Pleszkun, P. B. Schechter and H. C. Young, </author> <title> ``PIPE: a VLSI Decoupled Architecture'', </title> <booktitle> Proceedings of the Twelveth Annual International Symposium on Computer Architecture(June 1985), </booktitle> <pages> pp. 20-27. </pages>
Reference-contexts: With instruction level parallelism, this coupling is very close, necessitating frequent bi-directional communication between operation units. This high level of communication can often be avoided in data parallel applications. The MISC processor, a direct descendant of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref> will exploit both the instruction and data parallelism available in a task by combining the capabilities of traditional data parallel architectures with those found in machines designed to exploit instruction level parallelism. For example, MISC supports multiple instruction issue without sacrificing clock rate or increasing code size. <p> MISC will utilize a similar feature called processor chaining to achieve a more powerful method of chaining. 3. The MISC Design The MISC system, an outgrowth of the PIPE project <ref> [CGKP87, FaPl91b, GHLP85] </ref>, is being developed at the University of California, Davis to study the characteristics of a broad range of C programs in order to determine the feasibility of efficiently extracting instruction level parallelism and data parallelism from a single chip (scalable) MIMD processor design.
Reference: [HePa90] <author> J. Hennessy and D. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> (1990). </year>
Reference-contexts: 1. Introduction The goal of most high-performance computers is to maximize the amount of work that can be done per unit time. This quantity (work) can be expressed by the following equation <ref> [HePa90] </ref>: work = clock rate instruction count 1 hhhhhhhhhhhhhhh Clocks per Instruction 1 hhhhhhhhhhhhhhhhhhh A number of different approaches are used to increase this quantity.
Reference: [Inc82] <author> C. R. Inc, </author> <title> ``Cray-1 Computer Systems '', S Series Mainframe Reference Manual (HR-0029)(1982). </title>
Reference-contexts: This quantity (work) can be expressed by the following equation [HePa90]: work = clock rate instruction count 1 hhhhhhhhhhhhhhh Clocks per Instruction 1 hhhhhhhhhhhhhhhhhhh A number of different approaches are used to increase this quantity. CRAY-class computers <ref> [Inc82] </ref> focus on driving up the clock rate, while CISC machines like the VAX [ClLe82] attempt to reduce the number of instructions required to complete a task.
Reference: [Kell75] <author> R. M. Keller, </author> <title> ``Look-ahead Processors'', </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 7, no. </volume> <month> 4 (December </month> <year> 1975), </year> <pages> pp. 177-195. </pages>
Reference-contexts: Anti-dependencies and to a lesser extent output dependencies create problems for multi-issue architectures, such as SuperScalar [ Johnson 1990 ] and VLIW [Fish83], which use shared registers to facilitate communication between the separately issued instructions. These architectures must employ hardware techniques like register renaming <ref> [Kell75] </ref> to alleviate this problem. Since MISC does not use shared registers, additional RAW and WAW hazards are avoided. If a dependency exists between two instructions, then a certain amount of time must elapse between the execution of the two instructions.
Reference: [McMa84] <author> F. H. McMahon, </author> <title> LLNL FORTRAN KERNELS: </title> <type> MFLOPS, </type> <institution> Lawrence Livermore Laboratories, Livermore, California, </institution> <month> (March </month> <year> 1984). </year> - <title> 19 - d d </title>
Reference-contexts: Two examples have been chosen to illustrate some of the abilities of the MISC design. Of the two programs presented here, one contains highly vec-torizable code and the other displays no data parallelism. The Lawrence Livermore Loops <ref> [McMa84] </ref> provide a convenient source for small vectoriz-able loops found in scientific code. Although originally extracted from FORTRAN code, these loops have been translated to C for purposes of this analysis.
Reference: [Smit82] <author> J. E. Smith, </author> <title> ``Decoupled Access/Execute Computer Architectures'', </title> <booktitle> Proceedings of the Ninth Annual International Symposium on Computer Architecture, </booktitle> <address> Austin, Texas (April 26-29, </address> <year> 1982), </year> <pages> pp. 112-119. </pages>
Reference-contexts: This is done in the hope that such a separation will allow the read operation to precede the dependent instruction by enough cycles to hide - 3 - d d the latency (by allowing the processor to schedule unrelated work during the latency period). Decoupled Architectures <ref> [Smit82, SmWP86] </ref> use this method to mask high memory latencies without the use of data caches. Various studies have shown that the decoupling of memory access functions from the processing of the data can lead to a significant increase in processor performance.
Reference: [SmWP86] <author> J. E. Smith, S. Weiss and N. Y. Pang, </author> <title> ``A Simulation Study of Decoupled Architecture Computers'', </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-35, no. </volume> <month> 8 (August </month> <year> 1986), </year> <pages> pp. 692-702. - 20 </pages> - 
Reference-contexts: This is done in the hope that such a separation will allow the read operation to precede the dependent instruction by enough cycles to hide - 3 - d d the latency (by allowing the processor to schedule unrelated work during the latency period). Decoupled Architectures <ref> [Smit82, SmWP86] </ref> use this method to mask high memory latencies without the use of data caches. Various studies have shown that the decoupling of memory access functions from the processing of the data can lead to a significant increase in processor performance.
References-found: 14

