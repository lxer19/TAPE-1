URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/tark96.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  
Title: Planning, Learning and Coordination in Multiagent Decision Processes  
Author: Craig Boutilier 
Web: http://www.cs.ubc.ca/spider/cebly/craig.html  
Address: Vancouver, BC V6T 1Z4, CANADA  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: There has been a growing interest in AI in the design of multiagent systems, especially in multiagent cooperative planning. In this paper, we investigate the extent to which methods from single-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques from decision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe multia-gent Markov decision processes as a general model in which to frame this discussion. These are special n-person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approxima tion.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: MMDPs are a form of stochastic game [41]; but it is most closely related to the general framework for repeated games discussed by Myerson [39] (which themselves are generaliza tions of partially observable MDPs <ref> [52, 1] </ref>). We will have oc-casion to exploit both perspectives: MMDPs as a generalization of (single-agent) MDPs; and MMDPs as a specialization of n-person stochastic games. <p> process is a tuple hS; ff; fA i g i2ff ; Pr; Ri where: S and ff are finite sets of states and agents, respectively; A i is a finite set of ac tions available to agent i; Pr : S fi A 1 fi A n fi S ! <ref> [0; 1] </ref> is a transition function; and R : S ! R is a real valued reward function. For any MMDP, we dub A = fi i2ff A i the set of joint actions.
Reference: [2] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Such models have been studied quite extensively in game theory as well, including experimental work and formal analysis of convergence <ref> [2, 35, 59, 24] </ref>. The notion of fictitious play [41] offers a very simple learning model in which agents keep track of the frequency with which opponents use particular strategies, and at any point in time adopt a best response to the randomized strategy profile corresponding to the observed frequencies.
Reference: [3] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1-2):81-138, </volume> <year> 1995. </year>
Reference-contexts: Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including restricting search to local regions of the state space <ref> [17, 21, 3, 55] </ref> or reducing the state space via abstraction or clustering of states [7].
Reference: [4] <author> Richard E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: While algorithms such as modified policy iteration [43] are often used in practice, an especially simple algorithm is value iteration, based on Bellman's <ref> [4] </ref> principle of optimality. We discuss value iteration because of its simplicity, as well as the close relationship it bears to the RL techniques we describe below. We start with a random value function V 0 that assigns some value to each s 2 S.
Reference: [5] <author> D. P. Bertsekas and D. A. Castanon. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34 </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value.
Reference: [6] <author> Craig Boutilier, Thomas Dean, and Steve Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning, </booktitle> <address> Assisi, Italy, </address> <year> 1995. </year>
Reference-contexts: Typical classical planning problems can be viewed as MDPs in which actions are deterministic and there are no competing objectives only a single goal (e.g., the reward is 0-1) <ref> [6] </ref>. <p> Intuitively, this offers tremendous potential for learning coordinated policies. We might imagine, for instance, that the tree in Figure 7 represents the possible optimal joint policies. Furthermore, suppose that the region HCU (the 25 For a survey of the issues in structured computation and representation in MDPs, see <ref> [6] </ref>. 26 To simplify the presentation, we consider only binary variables. 27 This is a toy domain in which a robot is supposed to get coffee from a coffee shop across the street, can get wet if it is raining unless it has an umbrella, and is rewarded if it brings
Reference: [7] <author> Craig Boutilier and Richard Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1016-1022, </pages> <address> Seattle, </address> <year> 1994. </year> <note> (Extended version to appear, Artificial Intelligence.) </note>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) [26, 42] as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) <ref> [20, 17, 7, 55, 9] </ref>, and techniques for computing optimal policies have been adapted to AI planning tasks. Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment [27, 28]. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP <ref> [17, 7, 55, 9, 49] </ref>. In addition, reinforcement learning [28] can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work. <p> Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including restricting search to local regions of the state space [17, 21, 3, 55] or reducing the state space via abstraction or clustering of states <ref> [7] </ref>. Both approaches reduce the state space in a way that allows MDP solution techniques to be used, and generate approximately op timal solutions (whose accuracy can sometimes be bounded a 3 It is important to note that V fl depends critically on the discount factor. <p> We assume that fi is fixed in this discussion. 4 Appropriate stopping criteria are discussed in detail by Puter-man [42]. priori <ref> [7] </ref>). Certain computational techniques exploit compact representations of MDPs (we explore these in Section 5). 2.2 Reinforcement Learning One difficulty with the application of DTP methods and MDPs is the availability of an agent/system model. <p> These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value.
Reference: [8] <author> Craig Boutilier and Richard Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <type> (manuscript), </type> <year> 1996. </year>
Reference-contexts: Indeed, the coordinating agents need only have one set of beliefs per region. 29 This is an additional advantage of decomposing the coordination problem into state games. These representations can also be exploited in value function approximation and the construction of approximately optimal policies <ref> [8] </ref>. Generally speaking, value trees such as these can be pruned during computation to keep down computational costs, while sacrificing optimality. Error bounds on the resulting policies can be given as well. This offers further compaction of the value function and additional generalization in learning.
Reference: [9] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) [26, 42] as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) <ref> [20, 17, 7, 55, 9] </ref>, and techniques for computing optimal policies have been adapted to AI planning tasks. Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment [27, 28]. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP <ref> [17, 7, 55, 9, 49] </ref>. In addition, reinforcement learning [28] can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work. <p> These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value. <p> We assume that a set of atomic propositions P describes our system, inducing a state space of size 2 jPj , and use two-stage temporal or dynamic Bayesian networks to describe our actions <ref> [18, 9] </ref>. 25 For each action, we have a Bayes net with one set of nodes representing the system state prior to the action (one node for each variable), another set representing the world after the action has been performed, and directed arcs representing causal influences between the these sets. <p> Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [9] </ref> for a more detailed discussion of this representation). 26 Figure 6 (a) illustrates this representation for a single action. 27 The lack of arc from a pre-action variable X to a post-action variable Y in the network for action a reflects the independence of a's effect on Y from the <p> Thus, additional regularities in transition probabilities are used to provide a more compact representation than the usual (locally exponential) CPTs (the matrices). A similar representation can be used to represent the reward function R, as shown in Figure 6 (b). In <ref> [9] </ref>, we present an algorithm for solving MDPs that exploits such a representation. Decision trees as used as compact function representations for policies and value functions, essentially representing values for clusters, or regions of the state space, as opposed to individual states. <p> in which a robot is supposed to get coffee from a coffee shop across the street, can get wet if it is raining unless it has an umbrella, and is rewarded if it brings coffee when the user requests it, and penalized (to a lesser extent) if it gets wet <ref> [9, 10] </ref>. This network describes the action of fetching coffee. 28 See [22] for a similar approach to RL for goal-based, deterministic problems. leftmost branch) admits several PIO action choices for sev-eral agents.
Reference: [10] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <type> (manuscript), </type> <year> 1996. </year>
Reference-contexts: in which a robot is supposed to get coffee from a coffee shop across the street, can get wet if it is raining unless it has an umbrella, and is rewarded if it brings coffee when the user requests it, and penalized (to a lesser extent) if it gets wet <ref> [9, 10] </ref>. This network describes the action of fetching coffee. 28 See [22] for a similar approach to RL for goal-based, deterministic problems. leftmost branch) admits several PIO action choices for sev-eral agents.
Reference: [11] <author> Craig Boutilier and Martin L. Puterman. </author> <title> Process-oriented planning and average-reward optimality. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1096-1103, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: fl for the joint MDP to be the gold standard of performance for the multiagent system. 6 An interesting problem is that of letting the effects of each individual's actions a i be stated and providing mechanisms to determine the joint effect of a set of individual actions automatically (see <ref> [11] </ref>), since this is often the most natural way to specify a problem. <p> One final justification for the restriction to PIO choices in the state games is related to the interpretation of discounting. Recently, certain work in AI has advocated the use of the aver age reward optimality criterion as opposed to discounted total reward <ref> [33, 11] </ref>.
Reference: [12] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [13] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: : S ! A, where (s) denotes the action an agent will perform whenever it is in state s. 2 Given an MDP, an agent ought to adopt an optimal policy that maximizes the expected rewards accumulated as it 1 Partially observable processes are much more realistic in many cases <ref> [13] </ref>, but are much less tractable computationally [51]. We do not consider these here (but see the concluding section). 2 Thus we restrict attention to stationary policies. For the problems we consider, optimal stationary policies always exist. performs the specified actions.
Reference: [14] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value. <p> Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [15] <author> Peter Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: This is the basis of TD (), where a parameter captures the degree to which past states are influenced by the current sample <ref> [15] </ref>. In addition, there are variants in which truncated eligibility traces are used. Q-learning [56] is a straightforward and elegant method for combining value function learning (as in TD-methods) with policy learning.
Reference: [16] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: In a similar vein, Yanco and Stein [58] also reported experimental results with hierarchical Q-learning in order to learn cooperative communication between a leader and follower agent (see also <ref> [16] </ref>). In both works, convergence is achieved. A slightly different semi-cooperative application of Q-learning is reported in [45], where a set of agents used straightforward Q-learning in the presense of other agents with orthogonal and slightly interacting interests.
Reference: [17] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 574-579, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) [26, 42] as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) <ref> [20, 17, 7, 55, 9] </ref>, and techniques for computing optimal policies have been adapted to AI planning tasks. Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment [27, 28]. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP <ref> [17, 7, 55, 9, 49] </ref>. In addition, reinforcement learning [28] can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work. <p> Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including restricting search to local regions of the state space <ref> [17, 21, 3, 55] </ref> or reducing the state space via abstraction or clustering of states [7].
Reference: [18] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: We assume that a set of atomic propositions P describes our system, inducing a state space of size 2 jPj , and use two-stage temporal or dynamic Bayesian networks to describe our actions <ref> [18, 9] </ref>. 25 For each action, we have a Bayes net with one set of nodes representing the system state prior to the action (one node for each variable), another set representing the world after the action has been performed, and directed arcs representing causal influences between the these sets.
Reference: [19] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedingsof the Fourteenth International Joint Conferenceon Artificial Intelligence, </booktitle> <pages> pages 1121-1127, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value.
Reference: [20] <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) [26, 42] as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) <ref> [20, 17, 7, 55, 9] </ref>, and techniques for computing optimal policies have been adapted to AI planning tasks. Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment [27, 28]. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues <ref> [20] </ref>. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP [17, 7, 55, 9, 49].
Reference: [21] <author> Richard Dearden and Craig Boutilier. </author> <title> Integrating planning and execution in stochastic domains. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 162-169, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including restricting search to local regions of the state space <ref> [17, 21, 3, 55] </ref> or reducing the state space via abstraction or clustering of states [7].
Reference: [22] <author> Thomas G. Dietterich and Nicholas S. Flann. </author> <title> Explanation-based learning and reinforcement learning: A unified approach. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 176-184, </pages> <address> Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value. <p> This network describes the action of fetching coffee. 28 See <ref> [22] </ref> for a similar approach to RL for goal-based, deterministic problems. leftmost branch) admits several PIO action choices for sev-eral agents. <p> Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [23] <author> Eithan Ephrati and Jeffrey S. Rosenschein. </author> <title> Divide and conquer in multiagent planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 375-380, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: One important class of multiagent problems is that of mul-tiagent planning (or multiagent sequential decision making), that is, the problem of devising effectve action policies or strategies for a set of n agents whom share common ends <ref> [23] </ref>. The key aspect of this problem is coordinating the actions of the individual agents so that the shared goals are achieved efficiently. Of course, the problem of multiagent planning falls squarely within the setting of n-person cooperative game theory.
Reference: [24] <author> Drew Fudenberg and David K. Levine. </author> <title> Steady state learning and nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(3) </volume> <pages> 547-573, </pages> <year> 1993. </year>
Reference-contexts: Such models have been studied quite extensively in game theory as well, including experimental work and formal analysis of convergence <ref> [2, 35, 59, 24] </ref>. The notion of fictitious play [41] offers a very simple learning model in which agents keep track of the frequency with which opponents use particular strategies, and at any point in time adopt a best response to the randomized strategy profile corresponding to the observed frequencies.
Reference: [25] <author> John C. Harsanyi and Reinhard Selten. </author> <title> A General Theory of Equilibrium Selection in Games. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: However, we assume that a central controller is not feasible and that communication is not possible. 9 Thus, individual agents must determine their own policies. Treating the MMDP as an n-person game, it is easy to see that determining an optimal joint policy is a problem of equilibrium selection <ref> [39, 25] </ref>. In particular, each optimal joint policy is a Nash equilibrium of the stochastic game: once the agents adopt the individual components of an optimal policy, there is no incentive to deviate from this choice.
Reference: [26] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) <ref> [26, 42] </ref> as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) [20, 17, 7, 55, 9], and techniques for computing optimal policies have been adapted to AI planning tasks. <p> Again, MDPs form the underlying model for much of this work. We review MDPs and associated decision methods in this section, along with the application of MDPs to RL. 2.1 Markov Decision Processes We consider DTP problems that can be modeled as completely observable MDPs <ref> [26, 42] </ref>. We assume a finite set of states S of the system of interest, a finite set of actions A available to the agent, and a reward function R. <p> The expected value of a fixed policy at any given state s can be shown to satisfy <ref> [26] </ref>: X Pr (s; (s); t) V (t) (1) The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 .
Reference: [27] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment <ref> [27, 28] </ref>. The extension of MDPs to the cooperative multiagent case is straightforward. Indeed, treating the collection of agents as a single agent with joint actions at its disposal allows one to compute (or learn) optimal joint policies by standard methods, provided the agents are of one mind. <p> However, considerations of optimal exploration strategies and optimal learning have been explored, especially in relation to k-armed bandit problems <ref> [27, 28] </ref>. Model-based approaches to RL use the samples to generate both a model of the MDP (i.e., estimated transition probabilities b Pr and rewards b R) and a value function or policy.
Reference: [28] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <note> (to appear), </note> <year> 1996. </year>
Reference-contexts: Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment <ref> [27, 28] </ref>. The extension of MDPs to the cooperative multiagent case is straightforward. Indeed, treating the collection of agents as a single agent with joint actions at its disposal allows one to compute (or learn) optimal joint policies by standard methods, provided the agents are of one mind. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP [17, 7, 55, 9, 49]. In addition, reinforcement learning <ref> [28] </ref> can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work. <p> However, considerations of optimal exploration strategies and optimal learning have been explored, especially in relation to k-armed bandit problems <ref> [27, 28] </ref>. Model-based approaches to RL use the samples to generate both a model of the MDP (i.e., estimated transition probabilities b Pr and rewards b R) and a value function or policy. <p> One possible manner in which to incorporate models in RL is called the certainty equivalence approach: one treats the current estimated model as if it were accurate, and constructs an optimal policy based on the current estimate <ref> [28] </ref>. It is clear that this approach will generally be wildly infeasible, for it requires recomputation of an optimal policy after each sample (nor does it address the issue of exploration). Sutton's Dyna technique [54] adopts a less extreme approach. <p> This reduces the need to visit each state. We discuss generalization further in Section 5. The reader interested in additional details on RL is referred to <ref> [28] </ref> for a nice survey of the area. 3 Multiagent MDPs and the Coordination Problem Multiagent planning typically assumes that there are some number of (often heterogeneous) agents, each with their own set of actions, and a given task to be solved. <p> Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, [22, 14, 38, 12, 36, 50]. The survey <ref> [28] </ref> provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [29] <author> Ehud Kalai and Ehud Lehrer. </author> <title> Rational learning leads to nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(5) </volume> <pages> 1019-1045, </pages> <year> 1993. </year>
Reference-contexts: He shows conditions under which such a method will converge to a pure strategy equilibrium (which include coordination games). The work of Kalai and Lehrer <ref> [29] </ref> is also of considerable importance here. They model a repeated game as a true stochastic game so that performance during learning can be accounted for when determining a best response. <p> For instance, in our initial example, until agents A and B have some confidence in the other's choice of action, randomized choice of the actions l and r at state s will prevent deadlock. As proposed by Kalai and Lehrer <ref> [29] </ref>, at each stage of the game we would like agents to update their beliefs about other agents's policies, and then adopt a best response to the set of updated policies. <p> Thus, convergence to a globally optimal policy is assured even when the symmetric (and n fi n) conditions are dropped. convergence. One possible way of enhancing convergence is to consider the notion of "-best responses <ref> [29] </ref>. This allows agents to randomize among actions that are close to being best responses given their current beliefs. <p> However, at any point before coordination is assured at all strongly dependent states, the current policies of the agents necessarily have a lower value at some (perhaps all) states. If we were to adopt the more precise model of <ref> [29] </ref>, we would model the learning process as part of the MMDP in order to ensure optimal performance during learning. For example, agents may decide that learning to coordinate (in the sense we have described) may not be worth the risk.
Reference: [30] <author> D. Kreps and R. Wilson. </author> <title> Sequential equilibria. </title> <journal> Econometrica, </journal> <volume> 50 </volume> <pages> 863-894, </pages> <year> 1982. </year>
Reference-contexts: We do not address this issue here and assume the joint effects are given (but see Section 5). 7 Policies correspond to behavioral strategies <ref> [30] </ref> that are restricted to be stationary. 8 Not all randomized policies for the joint MDP can be modeled as a collection of randomized individual policies, since correlated action choice between two agents cannot be captured this way.
Reference: [31] <author> David K. Lewis. </author> <title> Conventions, A Philosophical Study. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <year> 1969. </year>
Reference-contexts: For example, agents might communicate in order to determine task allocation [37, 57]; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured <ref> [31, 48] </ref>; or a coordinated policy (or conventions) might be learned through repeated interaction [47, 46, 32]. We focus here primarily on imposed conventions and learned coordination of behavior, especially in sequential decision processes. <p> We elaborate on this is the next section. 4 Conventions and Learned Coordination In this section, we address the coordination problem in MMDPs. In particular, we focus on conventions as a means to coordinate the choices of agents. We adopt the view of <ref> [31, 47, 48] </ref> that conventions or social laws are restrictions on the possible action choices of agents in various circumstances. <p> In addition, we briefly discuss the application of RL techniques to MMDPs when the agents do not know the system model. 4.1 Designed Conventions and Social Laws Conventions are examined in detail by Lewis <ref> [31] </ref>. Shoham and Tennenholtz [48] address the issue from a computational perspective.
Reference: [32] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: For example, agents might communicate in order to determine task allocation [37, 57]; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured [31, 48]; or a coordinated policy (or conventions) might be learned through repeated interaction <ref> [47, 46, 32] </ref>. We focus here primarily on imposed conventions and learned coordination of behavior, especially in sequential decision processes. Ultimately, we are interested in the extent to which models, representations and computational schemes for DTP and RL can be applied to solving cooperative problems requiring coordination. <p> Of course, though it may prove useful in semi-cooperative settings, it seems clear that Q-learning will not generally converge to good (e.g., Pareto optimal) solutions. In the area of zero-sum games, little attempt has been made to apply Q-learning. One exception is the work of Littman <ref> [32] </ref> which addresses two-person zero-sum games. Each experience at a particular state results in an updated Q-value that is determined by using a small linear program to solve the analogue of a local state game.
Reference: [33] <author> Sridhar Mahadevan. </author> <title> To discount or not to discount in reinforcement learning: A case study in comparing R-learning and Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 164-172, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: One final justification for the restriction to PIO choices in the state games is related to the interpretation of discounting. Recently, certain work in AI has advocated the use of the aver age reward optimality criterion as opposed to discounted total reward <ref> [33, 11] </ref>.
Reference: [34] <editor> Maja Mataric. </editor> <title> Reward functions for accelerated learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 181-189, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: In other words, other agents are simply treated as part of the environment. Recent work in applying Q-learning to multiagent systems seems to adopt just this approach. For instance, Mataric <ref> [34] </ref> describes experiments with mobile robots in which Q-learning is applied to a cooperative task with good results. In a similar vein, Yanco and Stein [58] also reported experimental results with hierarchical Q-learning in order to learn cooperative communication between a leader and follower agent (see also [16]).
Reference: [35] <author> George Mailath Michihiro Kandori and Rafael Rob. </author> <title> Learning, mutation and long run equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61(1) </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference-contexts: Such models have been studied quite extensively in game theory as well, including experimental work and formal analysis of convergence <ref> [2, 35, 59, 24] </ref>. The notion of fictitious play [41] offers a very simple learning model in which agents keep track of the frequency with which opponents use particular strategies, and at any point in time adopt a best response to the randomized strategy profile corresponding to the observed frequencies. <p> Thus, convergence must be ensured by properties of best responses. 21 These experiments are quite similar in nature to the kind described in [47], as well as the model adopted in <ref> [35, 59] </ref>. 22 In fact, for larger values of n, faster convergence is due to the likelihood that the initial randomization is more likely to produce a unique most likely coordinated action, leading to immediate is reached, the agents cannot diverge from it. Thus, the conventions adopted will be stable.
Reference: [36] <author> Andrew W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> In Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pages 333-337, </pages> <address> Evanston, IL, </address> <year> 1991. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value. <p> Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [37] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweepingreinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: Solutions to the coordination problem can be divided into three general classes, those based on communication, those based on convention and those based on learning. For example, agents might communicate in order to determine task allocation <ref> [37, 57] </ref>; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured [31, 48]; or a coordinated policy (or conventions) might be learned through repeated interaction [47, 46, 32]. <p> This is analogous to performing k asynchronous dynamic programming steps using the current model, as opposed to computing the optimal policy for this model. A very interesting refinement of this model is prioritized sweeping <ref> [37] </ref>. Instead of updating random Q-values, the states that are updated are those that are likely to have the largest changes in value given the effect of the sample on the estimated model. <p> Then backups are performed on the k highest priority states (updating priorities as we proceed). While prioritized sweeping requires additional computational effort and bookkeeping, it generally results in considerably faster convergence (see <ref> [37] </ref> for details). Intuitively, this is so because the backups are focused on states that, under the current model, will have their values changed the most given the change in the model induced by the sample.
Reference: [38] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value. <p> Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [39] <author> Roger B. Myerson. </author> <title> Game Theory: Analysis of Conflict. </title> <publisher> Har-vard University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: But, in fact, they are nothing more than n-person stochastic games is which the payoff function is the same for all agents. MMDPs are a form of stochastic game [41]; but it is most closely related to the general framework for repeated games discussed by Myerson <ref> [39] </ref> (which themselves are generaliza tions of partially observable MDPs [52, 1]). We will have oc-casion to exploit both perspectives: MMDPs as a generalization of (single-agent) MDPs; and MMDPs as a specialization of n-person stochastic games. <p> However, we assume that a central controller is not feasible and that communication is not possible. 9 Thus, individual agents must determine their own policies. Treating the MMDP as an n-person game, it is easy to see that determining an optimal joint policy is a problem of equilibrium selection <ref> [39, 25] </ref>. In particular, each optimal joint policy is a Nash equilibrium of the stochastic game: once the agents adopt the individual components of an optimal policy, there is no incentive to deviate from this choice.
Reference: [40] <author> Radford M. Neal. </author> <title> Probabilistic inference methods using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto, Toronto, </institution> <year> 1993. </year>
Reference-contexts: This can be represented with a small number of parameters and can be updated and used quite easily. Let n be the cardinality of j's PIO set. Agent i's beliefs about j are represented by the Dirichlet parameters N j n , capturing a density function (see <ref> [40] </ref>) over such strategies. The expectation of kth ac tion being adopted by j is N j N j . Intuitively, each N j be viewed as the number of times outcome k (in this case action k) has been observed.
Reference: [41] <author> Guillermo Owen. </author> <title> Game Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: We think of MMDPs as decision processes rather than games because of the existence of a joint utility function. But, in fact, they are nothing more than n-person stochastic games is which the payoff function is the same for all agents. MMDPs are a form of stochastic game <ref> [41] </ref>; but it is most closely related to the general framework for repeated games discussed by Myerson [39] (which themselves are generaliza tions of partially observable MDPs [52, 1]). <p> Such models have been studied quite extensively in game theory as well, including experimental work and formal analysis of convergence [2, 35, 59, 24]. The notion of fictitious play <ref> [41] </ref> offers a very simple learning model in which agents keep track of the frequency with which opponents use particular strategies, and at any point in time adopt a best response to the randomized strategy profile corresponding to the observed frequencies. <p> Finally, we note the relationship of learning of this type to the solution of games by fictitious play <ref> [41] </ref>. Fictitious play is not guaranteed to converge for non-zero-sum games in general, but as noted by Young [59], will converge for coordination games under many circumstances. The use of "-best responses and experimentation is even more crucial in cases where actions are not directly observable.
Reference: [42] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) <ref> [26, 42] </ref> as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) [20, 17, 7, 55, 9], and techniques for computing optimal policies have been adapted to AI planning tasks. <p> Again, MDPs form the underlying model for much of this work. We review MDPs and associated decision methods in this section, along with the application of MDPs to RL. 2.1 Markov Decision Processes We consider DTP problems that can be modeled as completely observable MDPs <ref> [26, 42] </ref>. We assume a finite set of states S of the system of interest, a finite set of actions A available to the agent, and a reward function R. <p> We assume that fi is fixed in this discussion. 4 Appropriate stopping criteria are discussed in detail by Puter-man <ref> [42] </ref>. priori [7]). Certain computational techniques exploit compact representations of MDPs (we explore these in Section 5). 2.2 Reinforcement Learning One difficulty with the application of DTP methods and MDPs is the availability of an agent/system model.
Reference: [43] <author> Martin L. Puterman and M.C. Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: The optimal value function V fl for the MDP is the value function for any optimal policy. 3 Techniques for constructing optimal policies for discounted problems have been well-studied. While algorithms such as modified policy iteration <ref> [43] </ref> are often used in practice, an especially simple algorithm is value iteration, based on Bellman's [4] principle of optimality. We discuss value iteration because of its simplicity, as well as the close relationship it bears to the RL techniques we describe below.
Reference: [44] <author> Paul L. Schweitzer, Martin L. Puterman, and Kyle W. Kindle. </author> <title> Iterative aggregation-disaggregation procedures for discounted semi-Markov reward processes. </title> <journal> Operations Research, </journal> <volume> 33 </volume> <pages> 589-605, </pages> <year> 1985. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [44, 5, 36, 7, 9, 19, 22, 14, 38] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric, but generally assume that the states so grouped have the same optimal value.
Reference: [45] <author> Sandip Sen and Mahendra Sekaran. </author> <title> Multiagent coordination with learning classifier systems. </title> <booktitle> In Proceedings of the IJCAI Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <pages> pages 84-89, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: In a similar vein, Yanco and Stein [58] also reported experimental results with hierarchical Q-learning in order to learn cooperative communication between a leader and follower agent (see also [16]). In both works, convergence is achieved. A slightly different semi-cooperative application of Q-learning is reported in <ref> [45] </ref>, where a set of agents used straightforward Q-learning in the presense of other agents with orthogonal and slightly interacting interests.
Reference: [46] <author> Sandip Sen, Mahendra Sekaran, and John Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: For example, agents might communicate in order to determine task allocation [37, 57]; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured [31, 48]; or a coordinated policy (or conventions) might be learned through repeated interaction <ref> [47, 46, 32] </ref>. We focus here primarily on imposed conventions and learned coordination of behavior, especially in sequential decision processes. Ultimately, we are interested in the extent to which models, representations and computational schemes for DTP and RL can be applied to solving cooperative problems requiring coordination. <p> In these experiments, Q-learning also converges, but not to a optimal solution. 24 Finally, similar results are obtained in <ref> [46] </ref> with a fully cooperative setting; convergence to good policies is obtained, although learned policies were generally suboptimal due to inadequate exploration. There has been little theoretical analysis of Q-learning in multiagent settings, though it has been applied with some success in fully cooperative and noncooperative games.
Reference: [47] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: Initial experimental results and observations. </title> <booktitle> In Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 225-231, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: For example, agents might communicate in order to determine task allocation [37, 57]; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured [31, 48]; or a coordinated policy (or conventions) might be learned through repeated interaction <ref> [47, 46, 32] </ref>. We focus here primarily on imposed conventions and learned coordination of behavior, especially in sequential decision processes. Ultimately, we are interested in the extent to which models, representations and computational schemes for DTP and RL can be applied to solving cooperative problems requiring coordination. <p> We elaborate on this is the next section. 4 Conventions and Learned Coordination In this section, we address the coordination problem in MMDPs. In particular, we focus on conventions as a means to coordinate the choices of agents. We adopt the view of <ref> [31, 47, 48] </ref> that conventions or social laws are restrictions on the possible action choices of agents in various circumstances. <p> In AI, Shoham and Tennenholtz <ref> [47] </ref> have explored emergent conventions in the setting of a large population of randomly matched individuals. <p> Thus, convergence must be ensured by properties of best responses. 21 These experiments are quite similar in nature to the kind described in <ref> [47] </ref>, as well as the model adopted in [35, 59]. 22 In fact, for larger values of n, faster convergence is due to the likelihood that the initial randomization is more likely to produce a unique most likely coordinated action, leading to immediate is reached, the agents cannot diverge from it.
Reference: [48] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, </address> <year> 1992. </year>
Reference-contexts: For example, agents might communicate in order to determine task allocation [37, 57]; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured <ref> [31, 48] </ref>; or a coordinated policy (or conventions) might be learned through repeated interaction [47, 46, 32]. We focus here primarily on imposed conventions and learned coordination of behavior, especially in sequential decision processes. <p> We elaborate on this is the next section. 4 Conventions and Learned Coordination In this section, we address the coordination problem in MMDPs. In particular, we focus on conventions as a means to coordinate the choices of agents. We adopt the view of <ref> [31, 47, 48] </ref> that conventions or social laws are restrictions on the possible action choices of agents in various circumstances. <p> In addition, we briefly discuss the application of RL techniques to MMDPs when the agents do not know the system model. 4.1 Designed Conventions and Social Laws Conventions are examined in detail by Lewis [31]. Shoham and Tennenholtz <ref> [48] </ref> address the issue from a computational perspective. <p> While this general problem can be quite difficult even in more restrictive settings than MMDPs <ref> [48] </ref>, we will make three assumptions that permit a very general convention to be imposed on the agents in an MMDP.
Reference: [49] <author> Reid Simmons and Sven Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1080-1087, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP <ref> [17, 7, 55, 9, 49] </ref>. In addition, reinforcement learning [28] can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work.
Reference: [50] <author> Satinder P. Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <editor> In S. J. Han-son, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, </address> <year> 1994. </year>
Reference-contexts: Issues such as these are of great interest. Finally, we note that much research has been carried out in the RL community on generalization of learned values and action choices, as well as value function approximation (especially in continuous domains); see, for example, <ref> [22, 14, 38, 12, 36, 50] </ref>. The survey [28] provides a nice discussion of these issues. 6 Concluding Remarks We have surveyed a number of issues that arise in the application of single-agent planning and learning techniques to the setting of fully cooperative multiagent planning.
Reference: [51] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: the action an agent will perform whenever it is in state s. 2 Given an MDP, an agent ought to adopt an optimal policy that maximizes the expected rewards accumulated as it 1 Partially observable processes are much more realistic in many cases [13], but are much less tractable computationally <ref> [51] </ref>. We do not consider these here (but see the concluding section). 2 Thus we restrict attention to stationary policies. For the problems we consider, optimal stationary policies always exist. performs the specified actions.
Reference: [52] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26 </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: MMDPs are a form of stochastic game [41]; but it is most closely related to the general framework for repeated games discussed by Myerson [39] (which themselves are generaliza tions of partially observable MDPs <ref> [52, 1] </ref>). We will have oc-casion to exploit both perspectives: MMDPs as a generalization of (single-agent) MDPs; and MMDPs as a specialization of n-person stochastic games.
Reference: [53] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In model-free RL, an agent makes no attempt to directly learn Pr or R, the parts of the MDP that are unknown. Instead the agent learns the optimal value function and optimal policy (more or less) directly. Two popular (and related) methods are temporal difference (TD) learning <ref> [53] </ref> and Q-learning [56]. It is best to think of TD-methods as learning the value function for a fixed policy; thus it must be combined with another RL method that can use the value function to do policy improvement.
Reference: [54] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> Austin, </address> <year> 1990. </year>
Reference-contexts: It is clear that this approach will generally be wildly infeasible, for it requires recomputation of an optimal policy after each sample (nor does it address the issue of exploration). Sutton's Dyna technique <ref> [54] </ref> adopts a less extreme approach. After a sample hs; a; t; ri is obtained, the estimated model b Pr; b R is statistically updated and some number of Q-values are revised using the new model. <p> At each point an agent updates its model of the other agents's strategies and plays the best response to that strategy profile at any point in time. The analogy with the Dyna architecture <ref> [54] </ref> is rather compelling.
Reference: [55] <author> Jonathan Tash and Stuart Russell. </author> <title> Control strategies for a stochastic planner. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1079-1085, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Since we are interested in planning under uncertainty, with competing objectives and (potentially) indefinite or infinite horizon, we adopt Markov decision processes (MDPs) [26, 42] as our underlying (single agent) decision model. MDPs have been used as the basis for much work in decision-theoretic planning (DTP) <ref> [20, 17, 7, 55, 9] </ref>, and techniques for computing optimal policies have been adapted to AI planning tasks. Furthermore, MDPs form the foundation of most work in reinforcement learning (RL), in which agents learn optimal policies through experience with the environment [27, 28]. <p> Decision theoretic planning generalizes classical AI planning by addressing these issues [20]. In particular, the theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP <ref> [17, 7, 55, 9, 49] </ref>. In addition, reinforcement learning [28] can be viewed as a means of learning to act optimally, or incrementally constructing an optimal plan through repeated interaction with the environment. Again, MDPs form the underlying model for much of this work. <p> Much emphasis in DTP research has been placed on the issue of speeding up computation, and several solutions proposed, including restricting search to local regions of the state space <ref> [17, 21, 3, 55] </ref> or reducing the state space via abstraction or clustering of states [7].
Reference: [56] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Instead the agent learns the optimal value function and optimal policy (more or less) directly. Two popular (and related) methods are temporal difference (TD) learning [53] and Q-learning <ref> [56] </ref>. It is best to think of TD-methods as learning the value function for a fixed policy; thus it must be combined with another RL method that can use the value function to do policy improvement. <p> This is the basis of TD (), where a parameter captures the degree to which past states are influenced by the current sample [15]. In addition, there are variants in which truncated eligibility traces are used. Q-learning <ref> [56] </ref> is a straightforward and elegant method for combining value function learning (as in TD-methods) with policy learning. <p> Unfortunately, we would not expect convergence to an optimal policy should such a greedy approach be adopted: Q-learning is only guaranteed to converge to the optimal Q-function (and implicitly an optimal policy) should each state be sampled sufficiently <ref> [56] </ref>. Thus an exploration strategy must be adopted to ensure that states with low value estimates are also visited.
Reference: [57] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <address> Cham-bery, FR, </address> <year> 1993. </year>
Reference-contexts: Solutions to the coordination problem can be divided into three general classes, those based on communication, those based on convention and those based on learning. For example, agents might communicate in order to determine task allocation <ref> [37, 57] </ref>; conventions (or social laws) might be imposed by the system designed so that optimal joint action is assured [31, 48]; or a coordinated policy (or conventions) might be learned through repeated interaction [47, 46, 32].
Reference: [58] <author> Holly Yanco and Lynn Andrea Stein. </author> <title> An adaptive communication protocol for cooperating mobile robots. </title> <editor> In J. A. Meyer, H. L. Roitblat, and S.W. Wilson, editors, </editor> <booktitle> From Animals to An-imats: Proceedings of the Second International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 478-485. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Recent work in applying Q-learning to multiagent systems seems to adopt just this approach. For instance, Mataric [34] describes experiments with mobile robots in which Q-learning is applied to a cooperative task with good results. In a similar vein, Yanco and Stein <ref> [58] </ref> also reported experimental results with hierarchical Q-learning in order to learn cooperative communication between a leader and follower agent (see also [16]). In both works, convergence is achieved.
Reference: [59] <author> H. Peyton Young. </author> <title> The evolution of conventions. </title> <journal> Economet-rica, </journal> <volume> 61(1) </volume> <pages> 57-84, </pages> <year> 1993. </year>
Reference-contexts: Such models have been studied quite extensively in game theory as well, including experimental work and formal analysis of convergence <ref> [2, 35, 59, 24] </ref>. The notion of fictitious play [41] offers a very simple learning model in which agents keep track of the frequency with which opponents use particular strategies, and at any point in time adopt a best response to the randomized strategy profile corresponding to the observed frequencies. <p> Two models in particular seem especially relevant to our enterprise. Young <ref> [59] </ref> uses a matching model, but the ideas there clearly apply to our setting. In this model, agents can sample a fixed amount of past history (using incomplete sampling) in order to determine the frequency with which other agents play various strategies, and adopt appropriate best responses. <p> Thus, convergence must be ensured by properties of best responses. 21 These experiments are quite similar in nature to the kind described in [47], as well as the model adopted in <ref> [35, 59] </ref>. 22 In fact, for larger values of n, faster convergence is due to the likelihood that the initial randomization is more likely to produce a unique most likely coordinated action, leading to immediate is reached, the agents cannot diverge from it. Thus, the conventions adopted will be stable. <p> Thus, the conventions adopted will be stable. It is not too difficult to show that in this simple setting, agents will converge with probability (quickly) approaching one to coordinated action (see <ref> [59] </ref> for instance). The decomposition of the MMDP using the optimal value function therefore guarantees that the independent, local coordination of the state games ensures convergence to global coordination and a jointly optimal policy. <p> Allowing "-best responses gives the agents ample opportunity to break out of such cycles. Another possibility is to allow a small amount of experimentation by agents <ref> [59] </ref>: at any point in time, an agent will choose a random PIO-action with probability " (adopting the best response option with probability 1"). <p> Finally, we note the relationship of learning of this type to the solution of games by fictitious play [41]. Fictitious play is not guaranteed to converge for non-zero-sum games in general, but as noted by Young <ref> [59] </ref>, will converge for coordination games under many circumstances. The use of "-best responses and experimentation is even more crucial in cases where actions are not directly observable.
References-found: 59

