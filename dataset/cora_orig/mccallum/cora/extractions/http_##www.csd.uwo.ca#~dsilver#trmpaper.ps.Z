URL: http://www.csd.uwo.ca/~dsilver/trmpaper.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00447.html
Root-URL: 
Title: The Task Rehearsal Method of Sequential Learning  
Author: Daniel L. Silver (,) and Robert E. Mercer () 
Keyword: sequential learning, neural networks, knowledge transfer, inductive bias, task rehearsal, vir tual examples  
Address: Ontario, London, Ontario, Canada N6A 3K7,  Scotia, Canada B3H 3J5  
Affiliation: (1) Department of Computer Science, University of Western  Business Informatics, Faculty of Management, Dalhousie University, Halifax, Nova  
Email: email: daniel.silver@dal.ca  
Phone: (2)  
Date: February 20, 1998  
Abstract: An hypothesis of functional transfer of task knowledge is presented that requires the development of a measure of task relatedness and a method of sequential learning. The task rehearsal method (TRM) is introduced to address the issues of sequential learning, namely retention and transfer of knowledge. TRM is a knowledge based inductive learning system that uses functional domain knowledge as a source of inductive bias. The representations of successfully learned tasks are stored within domain knowledge. Virtual examples generated by domain knowledge are rehearsed in parallel with the each new task using either the standard multiple task learning (MTL) or the MTL neural network methods. The results of experiments conducted on a synthetic domain of seven tasks demonstrate the method's ability to retain and transfer task knowledge. TRM is shown to be effective in developing hypothesis for tasks that suffer from impoverished training sets. Difficulties encountered during sequential learning over the diverse domain reinforce the need for a more robust measure of task relatedness. 
Abstract-found: 1
Intro-found: 1
Reference: [AM93] <author> Yaser S. Abu-Mostafa, </author> <title> "A method for learning from Hints", </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 73-80, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In this way, the MTL framework generalizes both STL and MTL neural network learning. The above formulation of k agrees with mathematics resulting from Y. S. Abu-Mostafa's research into the use of hints in inductive learning <ref> [AM93, AM95] </ref>. This will be discussed at length in a forthcoming paper. Accuracy/Distance Measure of Relatedness. The following dynamic measure of relatedness, R k is based on the representational similarity between the developing hypotheses as well as the functional accuracy of these hypotheses.
Reference: [AM95] <author> Yaser S. Abu-Mostafa, </author> <title> "Hints", Neural Computation, </title> <journal> Massachusetts Institute of Technology, </journal> <volume> Vol. 7, </volume> <pages> pp. 639-671, </pages> <year> 1995. </year>
Reference-contexts: In contrast to representational transfer is a form we define as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the 4 use of implicit pressures from supplemental training examples <ref> [AM95, Sudd90] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Mitc93, Naik93, Thru94, Thru95a]. <p> In this way, the MTL framework generalizes both STL and MTL neural network learning. The above formulation of k agrees with mathematics resulting from Y. S. Abu-Mostafa's research into the use of hints in inductive learning <ref> [AM93, AM95] </ref>. This will be discussed at length in a forthcoming paper. Accuracy/Distance Measure of Relatedness. The following dynamic measure of relatedness, R k is based on the representational similarity between the developing hypotheses as well as the functional accuracy of these hypotheses. <p> Task domain knowledge is transferred in a functional manner using virtual examples, which can be considered a form of hint as per <ref> [AM95] </ref>. Once a new task has been learned to a desired level of generalization accuracy, its representation is retained in domain knowledge for use during future learning. based inductive learning system, KBIL. 12 The Networks. <p> One might propose that the generation of virtual examples vary dynamically as some function during the learning process (this could be seen as an extension of Mostafa's idea of adaptive minimization <ref> [AM95] </ref>).
Reference: [Baxt95] <author> Jonathan Baxter, </author> <title> "Learning internal representations", </title> <booktitle> Proceedings of the Eigth International Conference on Computational Learning Theory, </booktitle> <publisher> (to appear) ACM Press, </publisher> <address> Santa Cruz, CA, </address> <year> 1995. </year>
Reference-contexts: Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the 4 use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation <ref> [Baxt95, Caru95] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Mitc93, Naik93, Thru94, Thru95a]. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. <p> This form of transfer has its greatest value from the perspective of increased generalization performance. Certain methods of functional transfer have also been found to reduce training time (measured in number of training iterations). Chief among these methods is the parallel MTL paradigm explored recently by Caruana and Baxter <ref> [Baxt95, Caru95] </ref>. A recent paper by Caruana [Caru97] expresses plans for research into the use of MTL networks for sequential learning. <p> This general knowledge remains available for use in subsequent learning. This concept has been formalized by Baxter <ref> [Baxt95] </ref> as parallel learning and demonstrated by Caruana [Caru95] by a method called multiple task learning (MTL) which we classify as a functional form of knowledge transfer. An MTL network uses a feed-forward multi-layer network with an output for each task to be learned (see Figure 2). <p> At the point of lowest training error, the MTL network does its best to average the error across all of the output nodes. 6 2.4 Inductive Bias and Internal Representation As in <ref> [Baxt95] </ref>, consider the environment of the learner to be modeled by a pair (=; Q) where = is a set of tasks fT k g, and Q is a probability distribution over =. That is, (=; Q) defines the task domain and the probability of occurrence of any task. <p> For learning to become efficient and effective in any one environment an appropriate bias must be discovered. In particular, we are concerned with the inductive bias provided by the shared use of MTL network internal representations. Baxter <ref> [Baxt95] </ref> has proven that the number of examples required for learning any one task using an MTL network decreases as a function of the total number of tasks being learned in parallel. <p> Notice, that if R k = 1 for all k = 0; : : : ; t, then we have MTL learning as per <ref> [Baxt95, Caru95] </ref>. Alternatively, if R 0 = 1 and R k = 0 for all k = 1; : : : ; t, then we have standard single task learning (STL) for the primary function. In this way, the MTL framework generalizes both STL and MTL neural network learning.
Reference: [Caru95] <author> Richard A. Caruana, </author> <title> "Learning many related tasks at the same time with backpropagation", </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 7, pp. 657-664, </pages> <address> San Mateo, CA, </address> <year> 1995. </year>
Reference-contexts: Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the 4 use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation <ref> [Baxt95, Caru95] </ref>, or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Mitc93, Naik93, Thru94, Thru95a]. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. <p> This form of transfer has its greatest value from the perspective of increased generalization performance. Certain methods of functional transfer have also been found to reduce training time (measured in number of training iterations). Chief among these methods is the parallel MTL paradigm explored recently by Caruana and Baxter <ref> [Baxt95, Caru95] </ref>. A recent paper by Caruana [Caru97] expresses plans for research into the use of MTL networks for sequential learning. <p> This general knowledge remains available for use in subsequent learning. This concept has been formalized by Baxter [Baxt95] as parallel learning and demonstrated by Caruana <ref> [Caru95] </ref> by a method called multiple task learning (MTL) which we classify as a functional form of knowledge transfer. An MTL network uses a feed-forward multi-layer network with an output for each task to be learned (see Figure 2). <p> Therefore, to ensure the functional transfer of knowledge from several secondary tasks to the primary task, the following must be optimized: * The MTL network should have a sufficient amount of internal representation (at least as much as that required for the best single task learning (STL) network model and <ref> [Caru95] </ref> suggests k times as much representation as that required by an STL network); and * The secondary tasks should be as closely related to the primary task as possible. 2.5 Rehearsal of Task Examples In [Robi95, Robi96a] Robins discusses the concept of rehearsal and pseudo-rehearsal of task examples as a <p> Notice, that if R k = 1 for all k = 0; : : : ; t, then we have MTL learning as per <ref> [Baxt95, Caru95] </ref>. Alternatively, if R 0 = 1 and R k = 0 for all k = 1; : : : ; t, then we have standard single task learning (STL) for the primary function. In this way, the MTL framework generalizes both STL and MTL neural network learning. <p> The network is composed of an input layer of 2 nodes, a hidden layer of 14 nodes, and an output layer of 7 nodes, one for each task. The number of hidden nodes is well suited for the MTL method <ref> [Caru95] </ref>. Two linear descriminant functions, or hyperplanes are required within the internal representation of the network in order to properly separate the positive and negative examples for any one of the band tasks.
Reference: [Caru97] <author> Richard A. Caruana, </author> <title> "Multitask learning", </title> <journal> Machine Learning, </journal> <volume> Vol. 28, </volume> <pages> pp. 41-75, </pages> <year> 1997. </year>
Reference-contexts: Certain methods of functional transfer have also been found to reduce training time (measured in number of training iterations). Chief among these methods is the parallel MTL paradigm explored recently by Caruana and Baxter [Baxt95, Caru95]. A recent paper by Caruana <ref> [Caru97] </ref> expresses plans for research into the use of MTL networks for sequential learning. <p> The source of inductive bias under TRM is the virtual examples chosen for each of the domain knowledge tasks. There is great potential benefit in being able to generate virtual examples beyond those paired with the real training examples for a new task <ref> [Caru97] </ref>. Virtual examples can be selected by way of random sampling or by an ordering over the input attribute space.
Reference: [Elli65] <author> H. Ellis, </author> <title> "Transfer of Learning", </title> <publisher> MacMillan, </publisher> <address> New York, NY, </address> <year> 1965. </year>
Reference-contexts: This form of knowledge-based inductive learning is referred to elsewhere as the transfer of knowledge from one or more source tasks to a target or primary task [Utgo86, Prat93]. The transfer of task knowledge can be considered a major aspect of the problem of learning to learn <ref> [Elli65] </ref> and has close ties to analogical reasoning [Hall89]. In [Silv96b] we define the distinction between two forms of task knowledge transfer, representational and functional, and present a modified version of the multiple task learning (MTL) method of parallel functional transfer which we call MTL.
Reference: [Fahl90] <author> S.E. Fahlman and C. Lebiere, </author> <booktitle> "The cascade-correlation learning architecture", Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 2, pp. 524-532, </pages> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Gros87] <author> Stephen Grossberg, </author> <title> "Competitive learning: From interactive activation to adaptive resonance", </title> <journal> Cognitive Science, </journal> <volume> Vol. 11, </volume> <pages> pp. 23-64, </pages> <year> 1987. </year>
Reference-contexts: that required by an STL network); and * The secondary tasks should be as closely related to the primary task as possible. 2.5 Rehearsal of Task Examples In [Robi95, Robi96a] Robins discusses the concept of rehearsal and pseudo-rehearsal of task examples as a solution to the problem of catastrophic forgetting <ref> [McCl89, Gros87] </ref>. Robins considers the problem of learning one set of randomly chosen paired-associate examples by an STL ANN and then subsequently learning another set of paired-associates using the same ANN (potentially, a sequence of paired-associate tasks can be considered).
Reference: [Hall89] <author> Rogers P. Hall, </author> <title> "Computational approaches to analogical reasoning: A comparative analysis", </title> <journal> Arificial Intelligence, Elseivier Sience Publishers B.V., </journal> <volume> Vol. 39, </volume> <pages> pp. 39-120, </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning <ref> [Hall89] </ref>. In [Silv96b] we define the distinction between two forms of task knowledge transfer, representational and functional, and present a modified version of the multiple task learning (MTL) method of parallel functional transfer which we call MTL.
Reference: [Jaco88] <author> R.A. Jacobs, </author> <title> "Increased rates of convergence through learning rate adaptation", </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: It has been used for various purposes by other authors such as <ref> [Jaco88, Naik92, Vogl88] </ref>. 9 Through k , the relatedness measure should work to tailor the inductive bias from each of the developing parallel tasks such that the most related tasks will have the largest influence on weight modifications.
Reference: [Keho88] <author> E. James Kehoe, </author> <title> "A layered network model of associative learning: Learning to learn and configuration", </title> <journal> Psychological Review, </journal> <volume> Vol. 95, No. 4, </volume> <pages> pp. 411-433, </pages> <year> 1988. </year>
Reference-contexts: A recent paper by Caruana [Caru97] expresses plans for research into the use of MTL networks for sequential learning. We encourage these efforts as this is a large and exciting area of scientific discovery. 2.3 MTL Network Learning Kehoe points out in <ref> [Keho88] </ref> that psychological studies of human and animal learning suggest that besides the development of a specific discriminant function which satisfies the task at hand, there is the acquisition of general knowledge of the task domain. This general knowledge remains available for use in subsequent learning.
Reference: [McCl89] <author> Michael McCloskey and Neal J. Cohen, </author> <title> "Catastrophic interference in connectionist networks: the sequential learning problem", </title> <journal> The Psychology of Learning and Motivation, </journal> <volume> Vol. 24, </volume> , <year> 1989. </year> <month> 28 </month>
Reference-contexts: that required by an STL network); and * The secondary tasks should be as closely related to the primary task as possible. 2.5 Rehearsal of Task Examples In [Robi95, Robi96a] Robins discusses the concept of rehearsal and pseudo-rehearsal of task examples as a solution to the problem of catastrophic forgetting <ref> [McCl89, Gros87] </ref>. Robins considers the problem of learning one set of randomly chosen paired-associate examples by an STL ANN and then subsequently learning another set of paired-associates using the same ANN (potentially, a sequence of paired-associate tasks can be considered).
Reference: [McCl94] <author> James L. McClelland, Bruce L. McNaughton, and Randall C. O'Reilly, </author> <title> "Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory", </title> <type> Technical Report PDP.CNS.94.1, </type> <institution> Department of Psychology, Carnegie Mellon University, </institution> <address> Pittsurgh, PA 15213, </address> <year> 1994. </year>
Reference-contexts: The paper shows that pseudo-rehearsal is nearly as effective as rehearsal of retained examples. Robins goes on to suggest that pseudo-rehearsal is a potential model for long-term memory consolidation in the mammalian neocortex. He relates this to a recent neuroscience paper <ref> [McCl94] </ref> which discusses the complimentary roles of the hippocampus and the neocortex in human learning. 3 Theoretical Foundations The above background material has led to the development of a hypothesis of functional transfer in the context of ANNs and to the development of supportive theory. 3.1 Hypothesis of Functional Transfer The <p> Such management is likely to include a method of knowledge consolidation that is gradual and off-line to individual task learning. As suggested in <ref> [McCl94] </ref>, one possibility is that task knowledge developed in a working memory area (in humans, the hippocampus) is consolidated within long-term storage (in humans, the neocortex) using a method of inter-leaved learning very similar to that suggested by [Robi96a]. 13 Phases of Operation.
Reference: [Mich93] <author> R.S. Michalski, </author> <title> "Learning = Inferencing + Memorizing", </title> <booktitle> Foundations of Knowledge Acquistion: Machine Learning, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 1-41, </pages> <address> Boston, MA, </address> <year> 1993. </year>
Reference-contexts: In turn, new information is added to, or consolidated within the domain knowledge database following its discovery. Michalski refers to this as constructive inductive learning <ref> [Mich93] </ref>. In the extreme, where the new classification task to be learned is exactly the same as one learned at some earlier time, the inductive bias should provide rapid convergence on the optimal hypothesis with very few examples.
Reference: [Mitc80] <author> Tom. M. Mitchell, </author> <title> "The need for biases in learning generalizations", </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 184-191, </pages> <address> San Mateo, CA, </address> <year> 1980. </year>
Reference-contexts: This is unfortunate, for two reasons. First, it seems certain that human learning relies heavily upon the use of previously learned and related task knowledge. Second, for some time it has been recognized that learning by example without an appropriate inductive bias has practical limitations <ref> [Mitc80, Mitc97] </ref>. Our research investigates methods of consolidating and transferring task knowledge so as to facilitate the sequential learning of tasks. <p> Sections 6 and 7 conclude with a discussion of results and observations made during the experiment, a summary of the paper and an outline of future work. 2 Background 2.1 Knowledge Based Inductive Learning Mitchell <ref> [Mitc80] </ref> points out and Utgoff [Utgo86] iterates that inductive bias is essential for the development of a hypothesis with good generalization in a tractable amount of time and with a 3 practical number of examples.
Reference: [Mitc93] <author> Tom Mitchell and Sebastian Thrun, </author> <title> "Explanation based neural network learning for robot control", </title> <booktitle> Advances in Neural Information Processing Systems 5, Morgan Kauf-mann, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 287-294, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Mitc93, Naik93, Thru94, Thru95a] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Mitc97] <author> Tom M. Mitchell, </author> <title> "Machine Learning", </title> <publisher> McGraw Hill, </publisher> <address> New York, NY, </address> <year> 1997. </year>
Reference-contexts: This is unfortunate, for two reasons. First, it seems certain that human learning relies heavily upon the use of previously learned and related task knowledge. Second, for some time it has been recognized that learning by example without an appropriate inductive bias has practical limitations <ref> [Mitc80, Mitc97] </ref>. Our research investigates methods of consolidating and transferring task knowledge so as to facilitate the sequential learning of tasks. <p> For a recent discussion of the need for inductive bias refer to <ref> [Mitc97] </ref>. We define knowledge based inductive learning, or KBIL, as a learning method which relies on prior knowledge of the problem domain to reduce the hypothesis space which must be searched. a database of accumulated information which has been acquired from previously learned tasks. <p> The cross-entropy cost function, given by E k = p over all training examples p for task output k, seeks the maximum likelihood hypothesis under the assumption that the training example target values are a probabilistic function of their attributes <ref> [Mitc97] </ref>. Under the cross-entropy cost function, ffi k can be shown to equal (t k o k ); where t k is the desired output at node k and o k is the actual output of the network at node k.
Reference: [Naik92] <author> D. K. Naik, R. J. Mammone, and A. Agarwal, </author> <title> "Meta-Neural Network approach to learning by learning", </title> <journal> Intelligence Engineering Systems through Artificial Neural Networks, ASME Press, </journal> <volume> Vol. 2, </volume> <pages> pp. 245-252, </pages> <year> 1992. </year>
Reference-contexts: It has been used for various purposes by other authors such as <ref> [Jaco88, Naik92, Vogl88] </ref>. 9 Through k , the relatedness measure should work to tailor the inductive bias from each of the developing parallel tasks such that the most related tasks will have the largest influence on weight modifications.
Reference: [Naik93] <author> D.K. Naik and Richard J. Mammone, </author> <title> "Learning by learning in neural networks", Artificial Neural Networks for Speech and Vision; ed. </title> <editor> Richard J. Mammone, </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Mitc93, Naik93, Thru94, Thru95a] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Prat93] <author> Lorien Y. Pratt, </author> <title> "Discriminability-Based transfer between neural networks", </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 204-211, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This form of knowledge-based inductive learning is referred to elsewhere as the transfer of knowledge from one or more source tasks to a target or primary task <ref> [Utgo86, Prat93] </ref>. The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89]. <p> We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Prat96] <author> Lorien Pratt and Barbara Jennings, </author> <title> "A Survey of Transfer Between Connectionist Networks", Connection Science Special Issue: Transfer in Inductive Systems, </title> <editor> Lorien Pratt (Editor), Carfax Publishing Company, </editor> <volume> Vol. 8, No. 2, </volume> <pages> pp. 163-184, </pages> <year> 1996. </year>
Reference-contexts: Thus, the problem of selecting an appropriate bias is transformed into the problem of selecting the appropriate task knowledge for transfer. 2.2 Representational vs. Functional Transfer In <ref> [Silv96b, Prat96] </ref> the difference between two forms of task knowledge transfer is defined: representational and functional. The representational form of transfer involves the direct or indirect assignment of known task representation (weight values) to a new task.
Reference: [Ring93] <author> Mark Ring, </author> <title> "Learning sequential tasks by incrementally adding higher orders", </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <publisher> Morgan Kaufmann, Vol. </publisher> <pages> 5, pp. 155-122, </pages> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Robi95] <author> Anthony V. Robins, </author> <title> "Catastrophic forgetting, </title> <journal> rehearsal, and pseudorehearsal", Connection Science, Carfax Publishing Company, </journal> <volume> Vol. 7, </volume> <pages> pp. 123-146, </pages> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: much as that required for the best single task learning (STL) network model and [Caru95] suggests k times as much representation as that required by an STL network); and * The secondary tasks should be as closely related to the primary task as possible. 2.5 Rehearsal of Task Examples In <ref> [Robi95, Robi96a] </ref> Robins discusses the concept of rehearsal and pseudo-rehearsal of task examples as a solution to the problem of catastrophic forgetting [McCl89, Gros87]. <p> The inductive learning system is an MTL network which is capable of transferring related domain knowledge via the virtual examples. As a whole, the system of networks performs as a knowledge based inductive learning system. 3.3 Model for The Task Rehearsal Method In <ref> [Robi95] </ref> the method of pseudo-rehearsal of virtual examples was proposed as a solution to the problem of catastrophic forgetting. We apply this method to the problem of sequentially learning a series of tasks. This section presents the task rehearsal method, or TRM.
Reference: [Robi96a] <author> Anthony V. Robins, </author> <title> "Consolidation in neural metworks and in the sleeping brain", Connection Science Special Issue: Transfer in Inductive Systems, </title> <editor> Lorien Pratt (Editor), Carfax Publishing Company, </editor> <volume> Vol. 8, No. 2, </volume> <pages> pp. 259-275, </pages> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: much as that required for the best single task learning (STL) network model and [Caru95] suggests k times as much representation as that required by an STL network); and * The secondary tasks should be as closely related to the primary task as possible. 2.5 Rehearsal of Task Examples In <ref> [Robi95, Robi96a] </ref> Robins discusses the concept of rehearsal and pseudo-rehearsal of task examples as a solution to the problem of catastrophic forgetting [McCl89, Gros87]. <p> As suggested in [McCl94], one possibility is that task knowledge developed in a working memory area (in humans, the hippocampus) is consolidated within long-term storage (in humans, the neocortex) using a method of inter-leaved learning very similar to that suggested by <ref> [Robi96a] </ref>. 13 Phases of Operation. The task rehearsal method has two phases of operation, the training phase and the domain knowledge update phase. One phase is mutually dependent upon the other from the perspective of sequentially learning.
Reference: [Robi96b] <author> Anthony V. Robins, </author> <title> "Transfer in Cognition", Connection Science Special Issue: Transfer in Inductive Systems, </title> <editor> Lorien Pratt (Editor), Carfax Publishing Company, </editor> <volume> Vol. 8, No. 2, </volume> <pages> pp. 185-203, </pages> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Complexity of Relatedness. Relatedness, similarity and analogy are very difficult subjects which have been matters of philosophical debate for over 2500 years (see <ref> [Robi96b] </ref> for a detailed discussion). Experimentation with TRM suggests a fundamental reason for the complexity surrounding the issue of relatedness. Measuring task relatedness is an activity conducted within a frame of reference that is relative to those tasks which have been previously learned.
Reference: [Shar92] <author> Noel E. Sharkey and Amanda J.C. Sharkey, </author> <title> "Adaptive generalization and the transfer of knowledge", </title> <institution> Working paper Center for Connection Science, University of Exeter, UK, </institution> <year> 1992. </year>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Shav90] <author> Jude W. Shavlik and Geoffrey G. Towell, </author> <title> "An appraoch to combining explanation-based and neural learning algorithms", </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 828-839, </pages> <address> San Mateo, CA, </address> <year> 1990. </year> <month> 29 </month>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Silv95] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> "Toward a model of consolidation: The retention and transfer of neural net task knowledge", </title> <booktitle> Proceedings of the INNS World Congress on Neural Networks, </booktitle> <editor> Lawrence Erlbaun Assosciates, </editor> <volume> Vol. III, </volume> <pages> pp. 164-169, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We focus on systems of artificial neural networks which use prior task domain knowledge to decrease the training time for a new task and/or reduce the number of training examples necessary for acceptable generalization <ref> [Silv95, Silv96a] </ref>). This form of knowledge-based inductive learning is referred to elsewhere as the transfer of knowledge from one or more source tasks to a target or primary task [Utgo86, Prat93]. <p> It is the relationship between the function of the various tasks and not the relationship between their representation which is important. Subsequently, there is no need to consolidate the representations of the domain knowledge networks as was the case with the consolidation system discussed in <ref> [Silv95] </ref>. In fact, there is no requirement for any form of representational compliance between the differing domain knowledge networks.
Reference: [Silv96a] <author> Daniel L. Silver, </author> <title> "Consolidation and Transfer of Neural Network Task Knowledge, </title> <type> A PhD Proposal", </type> <institution> Department of Computer Science, University of Western Ontario, Mid-dlesex College, </institution> <address> London, Ontario N6A 5B7, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: We focus on systems of artificial neural networks which use prior task domain knowledge to decrease the training time for a new task and/or reduce the number of training examples necessary for acceptable generalization <ref> [Silv95, Silv96a] </ref>). This form of knowledge-based inductive learning is referred to elsewhere as the transfer of knowledge from one or more source tasks to a target or primary task [Utgo86, Prat93]. <p> For the domains we have investigated it has not been difficult to choose an appropriate value for c which has been shown to be reasonably robust to a wide range of values <ref> [Silv96a] </ref>. The range can be determined from a set of short preliminary runs. Typically, a value between 2 and 100 is selected.
Reference: [Silv96b] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> "The parallel transfer of task knowledge using dynamic learning rates based on a measure of relatedness", Connection Science Special Issue: Transfer in Inductive Systems, </title> <editor> Lorien Pratt (Editor), Carfax Publishing Company, </editor> <volume> Vol. 8, No. 2, </volume> <pages> pp. 277-294, </pages> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89]. In <ref> [Silv96b] </ref> we define the distinction between two forms of task knowledge transfer, representational and functional, and present a modified version of the multiple task learning (MTL) method of parallel functional transfer which we call MTL. <p> In Section 4 a TRM prototype system based on the theoretical model is detailed including important enhancements to the MTL system since <ref> [Silv96b] </ref>. Section 5 reports on experimentation with the TRM system against a synthetic domain of seven tasks. <p> Thus, the problem of selecting an appropriate bias is transformed into the problem of selecting the appropriate task knowledge for transfer. 2.2 Representational vs. Functional Transfer In <ref> [Silv96b, Prat96] </ref> the difference between two forms of task knowledge transfer is defined: representational and functional. The representational form of transfer involves the direct or indirect assignment of known task representation (weight values) to a new task. <p> The following dynamic measure of relatedness, R k is based on the representational similarity between the developing hypotheses as well as the functional accuracy of these hypotheses. The measure has been found to be effective on several task domains. An early version of the measure was reported in <ref> [Silv96b] </ref>. Assume that the weights in the task specific portion of an MTL network are initialized such that all hypotheses have an identical random starting position in weight space. As the back-propagation algorithm updates the weights in the network, each hypothesis traces out a trajectory. <p> This system calls an enhanced version of the ANN software first reported in <ref> [Silv96b] </ref>. This ANN software is capable of either single task learning (STL), multi-task learning (MTL), or MTL. The latest version of the ANN software will be reviewed followed by the details of the TRM software. 4.1 The ANN Software. <p> An additional method of preventing an unused task output from having an effect on the development of the network hypotheses is to set its learning rate, , to zero. 16 The ANN software has been previously applied to a small synthetic task domain <ref> [Silv96b] </ref> and to a more complex medical diagnostic domain [Silv97].
Reference: [Silv97] <author> Daniel L. Silver and Robert E. Mercer, </author> <title> "The functional transfer of knowledge for coronary artery disease diagnosis", </title> <type> Technical Report No. 513, </type> <institution> Department of Computer Science, University of Western Ontario, </institution> <address> Middlesex College, London, Ontario N6A 5B7, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: To discourage this from happening the accuracy factor is dampened so as to prevent the hypothesis for a simple task, T k , from being learned more quickly than the primary hypothesis <ref> [Silv97] </ref>. The following logic performs the dampening after every iteration through the training set: if 1=E k &gt; 1=E 0 then 1=E k = 1=E 0 fi E k =E 0 . <p> method of preventing an unused task output from having an effect on the development of the network hypotheses is to set its learning rate, , to zero. 16 The ANN software has been previously applied to a small synthetic task domain [Silv96b] and to a more complex medical diagnostic domain <ref> [Silv97] </ref>.
Reference: [Sing92] <author> Satinder P. Singh, </author> <title> "Transfer of learning by composing solutions for elemental sequential tasks", </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Sudd90] <author> Steven Suddarth and Y Kergoisien, </author> <title> "Rule injection hints as a means of improving network performance and learning time", </title> <booktitle> Proceedings of the EURASIP workshop on Neural Networks, </booktitle> <year> 1990. </year>
Reference-contexts: In contrast to representational transfer is a form we define as functional. Functional transfer does not involve the explicit assignment of prior task representation to a new task, rather it employs the 4 use of implicit pressures from supplemental training examples <ref> [AM95, Sudd90] </ref>, the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations [Mitc93, Naik93, Thru94, Thru95a].
Reference: [Thru94] <author> Sebastian Thrun and Tom M.Mitchell, </author> <title> "Learning one more thing", </title> <type> Technical Report CMU-CS-94-184, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1994. </year>
Reference-contexts: use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Mitc93, Naik93, Thru94, Thru95a] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru95a] <author> Sebastian Thrun, </author> <title> "LIfelong Learning: A Case Study", </title> <type> Technical Report CMU-CS-95-208, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> Nov </month> <year> 1995. </year>
Reference-contexts: use of implicit pressures from supplemental training examples [AM95, Sudd90], the parallel learning of related tasks constrained to use a common internal representation [Baxt95, Caru95], or the use of historical training information (most commonly the learning rate or gradient of the error surface) to augment the standard weight update equations <ref> [Mitc93, Naik93, Thru94, Thru95a] </ref>. These pressures serve to reduce the effective hypothesis space in which the learning system performs its search. This form of transfer has its greatest value from the perspective of increased generalization performance.
Reference: [Thru95b] <author> Sebastian Thrun and J. O'Sullivan, </author> <title> "Clustering learning tasks and the selective cross-task transfer of knowledge", </title> <type> Technical Report CMU-CS-95-209, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> Nov </month> <year> 1995. </year>
Reference-contexts: A more general discussion of a measure of relatedness will be the subject of a forthcoming paper. In addition we refer the reader to a recent article that discusses task relatedness and knowledge transfer in the context of a nearest neighbour memory based learning system <ref> [Thru95b] </ref>. 3.2 A Dynamic Measure of Relatedness Relaxing the MTL Parallel Learning Constraint. Being the common learning rate across all outputs employed by the back-propagation algorithm, is normally brought outside the summations of the weight update equation. However, this does not have to be the case.
Reference: [Towe90] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier, </author> <title> "Refinement of approximate domain theories by knowledge-based neural networks", </title> <booktitle> Proceedings of the Eigth National Conference on Artificial Intelligence (AAAI-90), AAAI Press/MIT Press, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 861-866, </pages> <address> Menlo Park, CA, </address> <year> 1990. </year>
Reference-contexts: We consider this to be an explicit form of knowledge transfer from a source task to a target task. Since 1990 numerous authors have discussed methods of representational transfer <ref> [Fahl90, Prat93, Ring93, Shar92, Shav90, Sing92, Towe90] </ref> which often results in substantially reduced training time with no loss in generalization performance. In contrast to representational transfer is a form we define as functional.
Reference: [Utgo86] <author> Paul E. Utgoff, </author> <title> "Machine Learning of Inductive Bias", </title> <publisher> Kluwer Academc Publisher, </publisher> <address> Boston, MA, </address> <year> 1986. </year>
Reference-contexts: This form of knowledge-based inductive learning is referred to elsewhere as the transfer of knowledge from one or more source tasks to a target or primary task <ref> [Utgo86, Prat93] </ref>. The transfer of task knowledge can be considered a major aspect of the problem of learning to learn [Elli65] and has close ties to analogical reasoning [Hall89]. <p> Sections 6 and 7 conclude with a discussion of results and observations made during the experiment, a summary of the paper and an outline of future work. 2 Background 2.1 Knowledge Based Inductive Learning Mitchell [Mitc80] points out and Utgoff <ref> [Utgo86] </ref> iterates that inductive bias is essential for the development of a hypothesis with good generalization in a tractable amount of time and with a 3 practical number of examples. In other words, inductive bias is required for efficient and effective learning of most real-world tasks.

References-found: 38

