URL: ftp://ftp.mpce.mq.edu.au/pub/comp/papers/raychaudhuri.phd_thesis.97.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00305.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: SEEKING THE VALUABLE DOMAIN QUERY LEARNING IN A COST-OPTIMAL PERSPECTIVE  
Author: Tirthankar RayChaudhuri, B.E.E., M.E.E. 
Degree: Thesis for the degree of Doctor of Philosophy  
Note: c 1997 Tirthankar RayChaudhuri Research sponsored by an Australian Postgraduate Award (Industry) grant and by Arnott's Limited.  
Date: January 1997  
Address: Sydney  
Affiliation: School of Mathematics, Physics, Computing and Electronics Macquarie University,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> (1987), </year> <title> `Learning regular sets from queries and examples', </title> <booktitle> Information and Computation 75, </booktitle> <pages> 87-106. </pages>
Reference-contexts: Some of the earlier machine learning literature contains theoretical work on `membership querying' of an environment (or `oracle') for learning certain classes of functions that cannot be learnt efficiently by random passive training data, e.g., (Valiant 1984) and <ref> (Angluin 1987, Angluin 1988) </ref>. In these methods the learner uses heuristical methods to construct queries on the basis of an existing training set but does not perceive the distribution of the entire input space.
Reference: <author> Angluin, D. </author> <year> (1988), </year> <title> `Queries and concept learning', </title> <booktitle> Machine Learning 2(4), </booktitle> <pages> 319-342. </pages>
Reference: <author> Atkinson, A. C. & Donev, A. N. </author> <year> (1992), </year> <title> Optimum Experimental Designs, </title> <booktitle> Oxford Statistical Science Series, </booktitle> <publisher> Clarendon Press. </publisher>
Reference: <author> Bar-Shalom, Y. & Tse, E. </author> <year> (1974), </year> <title> `Dual effect, certainty equivalence and separation in stochastic control', </title> <journal> IEEE Transactions on Automatic Control 19(5), </journal> <pages> 494-500. </pages>
Reference: <author> Barto, A. G., Sutton, R. S. & Anderson, C. W. </author> <year> (1983), </year> <title> `Neuronlike adaptive elements that can solve difficult learning control problems', </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 13(5), </journal> <pages> 834-846. </pages>
Reference-contexts: A primary contribution to this area has been the adaptive heuristic critic (AHC) or temporal difference (TD) philosophy of Barto et al <ref> (Barto et al. 1983) </ref>. The name TD is used because one of the primary considerations is that an action might have far reaching effects in time so that one requires to account for such temporal differences by means of some form of temporal credit assignment to each action.
Reference: <author> Baum, E. B. </author> <year> (1991), </year> <title> `Neural net algorithms that learn in polynomial time from examples and queries', </title> <journal> IEEE Transactions on Neural Networks 2(1), </journal> <pages> 5-19. </pages>
Reference-contexts: In these methods the learner uses heuristical methods to construct queries on the basis of an existing training set but does not perceive the distribution of the entire input space. Heuristic construction of queries for connectionist learning has been proposed by Baum <ref> (Baum 1991, Baum & Lang 1991) </ref> who suggests an algorithm for identifying hyperplanes in input space that will separate examples with training output 0 from examples with output 1.
Reference: <author> Baum, E. B. & Lang, K. J. </author> <year> (1991), </year> <title> Constructing hidden units using examples and queries, </title> <editor> in R. L. et al, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 3, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo,CA. </address> <note> 181 182 BIBLIOGRAPHY Bellman, </note> <author> R. </author> <year> (1957), </year> <title> Dynamic Programming, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: In these methods the learner uses heuristical methods to construct queries on the basis of an existing training set but does not perceive the distribution of the entire input space. Heuristic construction of queries for connectionist learning has been proposed by Baum <ref> (Baum 1991, Baum & Lang 1991) </ref> who suggests an algorithm for identifying hyperplanes in input space that will separate examples with training output 0 from examples with output 1.
Reference: <author> Bellman, R. </author> <year> (1961), </year> <title> Adaptive Control Processes, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference: <author> Berger, J. O. </author> <year> (1980), </year> <title> Statistical Decision Theory Foundations, Concepts and Methods, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: In Oehlmann, Sleeman and Edwards' IULIAN (Oehlmann, Sleeman & Ed-wards 1992) questions and experimentation interact in an exploratory discovery process applied to electrical circuits. desJardins' PAGODA (desJardins 1992b, desJardins 1992a) employs decison theory principles. A comprehensive treatment of the principles of statistical decision theory can be found in <ref> (Berger 1980) </ref>. In desJardins' work knowledge acquisition goals represent concepts which, if learned, would maximise the expected utility of the system. The learning agent's inductive mechanism is endowed with the ability of forming theories to make predictions about environment features. <p> (y) of the dollar value of an output label y is conceptually similar to the notion of a reward function R of a state s in reinforcement learning (see Section 1.2.1). 3 This may be looked upon as a distinct case of an expected utility function in Statistical Decision Theory <ref> (Berger 1980) </ref>. 5.3. QUERYING HIGH VALUE 83 5.3 Querying High Value ....Our wasted oil unprofitably burns.... WILLIAM COWPER From the point of view of cost-effectiveness it is argued that it is more useful to have a querying method using an objective function of expected value.
Reference: <author> Berry, D. A. & Fristedt, B. </author> <year> (1985), </year> <title> Bandit Problems: Sequential Allocation of Experiments, </title> <publisher> Chapman and Hall, </publisher> <address> London, UK. </address>
Reference-contexts: Exploration occurs when a strategy for improving the expected future performance is determined. Action is taken based on the anticipated gain of information by future actions. This Bayesian perspective <ref> (Berry & Fristedt 1985) </ref> is significant. Dayan and Sejnowski (Dayan & Sejnowski 1995) have used a Bayesian approach to finding a suboptimal balance between exploration and exploitation in a dual control approach to the problem of a two-dimensional maze with movable barriers.
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <year> (1989), </year> <title> Parallel and Distributed Computation: Numerical Methods, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Bertsekas <ref> (Bertsekas & Tsitsiklis 1989) </ref> has made extensive studies on some of the issues of value iteration. Another angle of investigation in reinforcement learning is the so-called model-free approach.
Reference: <author> Box, G. E. P. & Draper, N. R. </author> <year> (1969), </year> <title> Evolutionary Operation | A Statistical Method for Process Improvement, </title> <publisher> John Wiley and Sons, Inc. </publisher>
Reference-contexts: Much of the work in query-learning appears to have evolved from seminal contributions to the statistical Optimal Experimental Design methods by Fedorov (Fedorov 1972). In this context it is worth mentioning that the statistical experimental design literature also includes methods of evolutionary operation (EVOP) of industrial processes <ref> (Box & Draper 1969, Box, Hunter & Hunter 1978) </ref> which employ response surface methodology (RSM) to obtain plant parameters corresponding to optimal process yield. Such response surfaces are obtained by experimentation on pre-gathered data.
Reference: <author> Box, G. E. P., Hunter, W. G. & Hunter, J. S. </author> <year> (1978), </year> <title> Statistics for Experimenters An Introduction to Design, Data Analysis and Model Building, </title> <publisher> John Wiley and Sons, Inc. </publisher>
Reference: <author> Boyan, J. A. & Moore, A. W. </author> <year> (1995), </year> <title> Dynamic programming and function approximation, </title> <type> Technical report, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: It has been demonstrated in (Storck et al. 1995) that RDIA (whose basic idea is to take those actions that improved performance in similar situations) significantly outperforms random exploration. Boyan and Moore <ref> (Boyan & Moore 1995) </ref> have developed an algorithm called ROUT which uses dynamic programming to adaptively construct a training data set. The hierarchical approach (treating the environment as a hierarchy of learning problems) has been suggested as a means of dealing with large state spaces.
Reference: <author> Brent, R. J. </author> <year> (1996), </year> <title> Applied Cost-Benefit Analysis, </title> <type> Edward Elgar, </type> <institution> Chel-tenham, UK. </institution>
Reference-contexts: WILLIAM COWPER From the point of view of cost-effectiveness it is argued that it is more useful to have a querying method using an objective function of expected value. If B is the benefit and C is the cost of an action then the general model of cost-benefit analysis <ref> (Brent 1996) </ref> seeks to maximise the difference B C. In context of the querying model a simple cost-benefit perspective is adopted as follows. The expected dollar value defined in equation 5.1 is looked upon as the benefit of querying a label.
Reference: <author> Cohn, D. A. </author> <year> (1994), </year> <title> Neural network exploration using optimal experiment design, </title> <type> Technical Report AIM-1491, </type> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology. </institution>
Reference-contexts: Neural net learning models are, however, highly data (i.e., information)-dependent and the early algorithms were based upon reception of pre-gathered information (Plutowski 1994). While using them researchers soon realised that there were serious limitations to such passive algorithms used for `training' the nets <ref> (Cohn 1994) </ref>. The main limitation was a gen-eralisation 1 problem. In an attempt to address this issue active learning algorithms began to appear (Barto, Sutton & Anderson 1983, Cohn, Atlas & Ladner 1990, MacKay 1992) in the literature. <p> Cohn's `query-filtering' paradigm (Cohn et al. 1990, Cohn 1994) addresses this issue most effectively. 1.1.1 Usefulness of Active Learning Typically the improvement in generalisation performance by active learning is obtained at an increased computational expense <ref> (Cohn 1994) </ref>. However the purpose of such increased computation is to concentrate gathered information in the more significant regions of the knowledge-space. <p> In (Cohn et al. 1990) he has defined a region of uncertainty as the space where a `specific' and a more `general' hypothesis disagree. Active query filtering occurs when selective sampling from an environment is restricted to such regions. In <ref> (Cohn 1994) </ref> he has used an Optimum Experiment Design approach on the lines of the work of (Fedorov 1972). Cohn's basic querying criterion for future sampling in (Cohn 1994) is the expected value of the learner's mean squared error. <p> Active query filtering occurs when selective sampling from an environment is restricted to such regions. In <ref> (Cohn 1994) </ref> he has used an Optimum Experiment Design approach on the lines of the work of (Fedorov 1972). Cohn's basic querying criterion for future sampling in (Cohn 1994) is the expected value of the learner's mean squared error. The mean squared error has been decoupled into bias and variance terms. Assuming a neural network estimator to be unbiased (or having a bias inde 5 The term `bias' is used here in a purely statistical sense. <p> In (Fukumizu 1996) a method of active querying that also simultaneously trims the learning architecture (multilayer perceptron) is demonstrated. The method uses an inverted Fisher Information matrix (a Hessian) as a component of the objective function as does Cohn in <ref> (Cohn 1994) </ref>. Clearly it is difficult to evaluate such an objective function at points where the matrix inverse does not exist. Fukumizu has shown that such a circumstance indicates the presence of redundant hidden units in the network and therefore 1.2. <p> Desirable criteria for actively choosing such training data are 1. The training examples contain maximum information about environ ment characteristics (MacKay 1992). As a learning model grows closer to fulfilling this condition there exists a greater possibility of improved generalisation performance <ref> (Cohn 1994) </ref>. 2. Data collection costs are optimised. This is not simply a case of the quantity m being optimal. In a real-world situation costs of labeling individual data points differ. 1 Backpropagation is known to work efficiently when residuals yi f w (x i ) are normally distributed. 2.2. <p> In contrast a globally optimal querying strategy is to choose the next step that is optimal over an entire expected trajectory of a known finite number of queries (DeGroot 1962). Previous theoretical (Sollich 1995) and experimental <ref> (Cohn 1994) </ref> studies upon querying methods seeking to maximise information gain, have not demonstrated any major benefit in the globally-optimal method to justify the additional computational expense involved 2 . <p> In a finite boundary of activity such value-based queries are experimentally shown to give cost-effective results in Chapter 7. 2.4 Batch and One-Point Resampling The learning framework described involves retraining neural nets for designing each query, Hence computational costs are hardly negligible. In <ref> (Cohn 1994) </ref> a `semi-batch approach', i.e., retraining after sampling a small batch of points that appear most useful, has been suggested in order to amortize retraining costs. <p> QUERYING FOR INFORMATION 3.2 Model Uncertainty and Query-by-Committee A popular trend in active learning appears to be exploitation of some estimate of model uncertainty. Data collection guided by estimated neural network model variance using a Fisher Information matrix <ref> (Cohn 1994) </ref> and by measures of model bias (Plutowski 1994, Cohn 1995b) have been examined (see Section 1.2.3 for reviews of these studies). Recently in (Schneider 1997) Bayesian locally-weighted regression has been profitably used to estimate model unreliabilty. <p> QUERYING FOR INFORMATION 3.2 Model Uncertainty and Query-by-Committee A popular trend in active learning appears to be exploitation of some estimate of model uncertainty. Data collection guided by estimated neural network model variance using a Fisher Information matrix (Cohn 1994) and by measures of model bias <ref> (Plutowski 1994, Cohn 1995b) </ref> have been examined (see Section 1.2.3 for reviews of these studies). Recently in (Schneider 1997) Bayesian locally-weighted regression has been profitably used to estimate model unreliabilty.
Reference: <author> Cohn, D. A. </author> <year> (1995a), </year> ` <title> "What is the best thing to do right now?" : getting beyond greedy exploration', </title> <note> AAAI 95 Fall Symposium on Active Learning. Working Notes. BIBLIOGRAPHY 183 Cohn, </note> <author> D. A. </author> <year> (1995b), </year> <title> Minimising statistical bias with queries, </title> <type> Technical Report AI Memo 1552, </type> <institution> CBCL 124, Centre for Biological and Computational Learning, Massachusetts Institute of Technology. </institution>
Reference-contexts: Globally-optimal value-based querying will be investigated in future research (see Chapter 9). 44CHAPTER 2. A QUERYING FRAMEWORK: ISSUES AND DIRECTIONS 2.3. FINITE, RECEDING AND INFINITE HORIZONS 45 1995a). It has been mentioned in <ref> (Cohn 1995a) </ref> that it would be highly useful to have a querying strategy that takes into account the cost differences of individual queries.
Reference: <author> Cohn, D. A., Ghahramani, Z. & Jordan, M. I. </author> <year> (1995), </year> <title> Active learning with statistical models, </title> <editor> in G.Tesauro, D.Touretzky & T. Leen, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 7', </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: A greedy (one-step) optimisation approach is used and it has been discussed that globally optimal queries would be computationally expensive to the extent that they may not be worthwhile. In his subsequent work <ref> (Cohn, Ghahramani & Jordan 1995) </ref> there is a paradigm shift | he has migrated to using statistically-based learning architectures (mixtures of Gaussians and locally-weighted regression) in place of the neural-network models of his earlier work so as to reduce the computational costs of repeatedly retraining neural network models.
Reference: <author> Cohn, D., Atlas, L. & Ladner, L. </author> <year> (1990), </year> <title> Training connectionist networks with queries and selective sampling, </title> <editor> in D. Touretzky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems 2', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Sollich (Sollich 1994, Sollich & Saad 1995, Sollich 1995) has shown, using a Bayesian analysis, that generalisation performance is greatly improved by choosing each new training example such that it will be `maximally useful', whereas random training examples contain progressively less information as training proceeds. Cohn's `query-filtering' paradigm <ref> (Cohn et al. 1990, Cohn 1994) </ref> addresses this issue most effectively. 1.1.1 Usefulness of Active Learning Typically the improvement in generalisation performance by active learning is obtained at an increased computational expense (Cohn 1994). <p> This is a truly active method because every data point is sampled from the environment in a principled and directed manner. Cohn has studied the approach in some detail, employing feedforward neural network models initially. In <ref> (Cohn et al. 1990) </ref> he has defined a region of uncertainty as the space where a `specific' and a more `general' hypothesis disagree. Active query filtering occurs when selective sampling from an environment is restricted to such regions. <p> INTRODUCTION The `Query-by-Committee' Approach Another way to query-filter an environment for the most informative examples is the `query-by-committee' (QBC) method. In reality this is a particular kind of query-filter that uses Cohn's general notion of selectively sampling from a region of uncertainty <ref> (Cohn et al. 1990) </ref>.
Reference: <author> Dayan, P. </author> <year> (1992), </year> <title> `The convergence of TD() for general ', Machine Learning 8(3), </title> <type> 341-362. </type>
Reference-contexts: The reward r is obtained from a real world state transition. If the learning rate is adjusted to decrease slowly while the policy remains fixed, TD (0) eventually converges to optimal value (Sutton 1988). A more general version of TD (0), the TD (), has been developed by Dayan <ref> (Dayan 1992, Dayan & Sejnowski 1994) </ref>. <p> When = 1 then approximately all the states are updated according to the number of times they were visited by the end of a run. In general TD () is more compu-tationally expensive than TD (0) but converges faster <ref> (Dayan 1992, Dayan & Sejnowski 1994) </ref>. Recently in (Stensmo & Sejnowski 1997) TD () has been successfully used to determine decision-theoretic utilities of outcomes of tests in medical diagnosis. Convergence properties of AHC-related algorithms have been explored in (Williams & Baird III 1993).
Reference: <author> Dayan, P. & Sejnowski, T. J. </author> <year> (1994), </year> <title> `TD() converges with probability 1', </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Dayan, P. & Sejnowski, T. J. </author> <year> (1995), </year> <title> `Exploration bonuses and dual control', </title> <journal> Machine Learning. </journal> <note> also available on the Internet from ftp.ai.mit.edu as pub/dayan/exp.ps.gz. </note>
Reference-contexts: In <ref> (Dayan & Sejnowski 1995) </ref> a non-stationary MDP is learned using Bayes' rule to update the expected transition probability of an approximate value function. <p> Exploration occurs when a strategy for improving the expected future performance is determined. Action is taken based on the anticipated gain of information by future actions. This Bayesian perspective (Berry & Fristedt 1985) is significant. Dayan and Sejnowski <ref> (Dayan & Sejnowski 1995) </ref> have used a Bayesian approach to finding a suboptimal balance between exploration and exploitation in a dual control approach to the problem of a two-dimensional maze with movable barriers. <p> Various approaches to exploration are recognised in the reinforcement learning literature, e.g., the RDIA method of Storck et al, Moore and Atkeson's `Prioritized Sweeping', etc. (see Section 1.2.2). In <ref> (Dayan & Sejnowski 1995) </ref> it is suggested that the key to a well-directed exploration strategy is a model of the learning agent's uncer 5.5. A CONFIDENCE INTERVAL: EXPLORATION 85 tainty about its world.
Reference: <author> DeGroot, M. H. </author> <year> (1962), </year> <title> `Uncertainty, information and sequential experiments', </title> <journal> Annals of Mathematical Statistics 3, </journal> <pages> 105-124. </pages>
Reference-contexts: This is shown in the flowchart of Figure 2.2 and can be called a locally-optimal strategy for selecting the next query. In contrast a globally optimal querying strategy is to choose the next step that is optimal over an entire expected trajectory of a known finite number of queries <ref> (DeGroot 1962) </ref>. Previous theoretical (Sollich 1995) and experimental (Cohn 1994) studies upon querying methods seeking to maximise information gain, have not demonstrated any major benefit in the globally-optimal method to justify the additional computational expense involved 2 .
Reference: <author> Demuth, H. & Beale, M. </author> <year> (1994), </year> <title> Neural Network Toolbox for Use with MAT-LAB, The Math Works Inc. User's Guide. </title> <editor> desJardins, M. </editor> <year> (1992a), </year> <title> Goal-directed learning: A decision-theoretic model for deciding what to do next, </title> <booktitle> in `Proceedings of Machine Discovery Workshop', </booktitle> <address> Aberdeen, Scotland. </address> <note> 184 BIBLIOGRAPHY desJardins, </note> <author> M. </author> <year> (1992b), </year> <title> PAGODA: A Model for Autonomous Learning in Probabilistic Domains, </title> <type> PhD thesis, </type> <institution> U C Berkeley. </institution>
Reference-contexts: Nodes in the hidden layer had sigmoidal activation functions and the output node was linear. The architectures of the ten identifying networks were identical with that used to generate the plant. Using the Matlab Neural Network Toolbox, fast backpropagation with the Levenberg-Marquardt algorithm <ref> (Demuth & Beale 1994) </ref> (see Appendix B for a theoretical discussion) performed quick training. Training to convergence and random weight initialisation techniques were employed as in earlier described experiments (Section 3.3.3). The initial data consisted of ten points (input-output pairs) collected randomly from the plant.
Reference: <author> Duff, M. O. </author> <year> (1995), </year> <title> Q-learning for bandit problems, </title> <type> Technical Report 95-26, </type> <institution> Department of Computer Science, University of Massachusetts, Amherst, Massachusetts, </institution> <address> MA 2109. </address>
Reference-contexts: Q-learning is a very popular model-free algorithm for learning from delayed reinforcement. It has been adapted to an average-reward framework (R-learning) in (Schwartz 1993) and (Jaakkola, Singh & Jordan 1995). Duff has used Gittins indices and Watkins' Q-learning in a model-free approach <ref> (Duff 1995) </ref> to addressing multi-armed bandit problems. The model-free methods demonstrate that it is possible to learn an optimal policy fl without bothering to learn models of T (s; a; r; s 0 ) and R (s; a).
Reference: <author> Efron, B. </author> <year> (1979), </year> <title> `Bootstrap methods: Another look at the jackknife', </title> <journal> The Annals of Statistics 7, </journal> <pages> 1-26. </pages>
Reference-contexts: A theoretical foundation more rigorous than the preliminary analysis in Section 3.3.1 will be established. The basis will be a jack-knife technique applied to the available sample S for obtaining data for training the committee nets. 4.1 A Jack-knifed Committee Method Jack-knifing (Quenouille 1949, Tukey 1958) and bootstrapping <ref> (Efron 1979) </ref> are well-known statistical methods of data-resampling in order to approximate sampling distribution. In this work the jackknife approach is employed on account of its similarity with the random subsampling technique used earlier (see Section 4.3 for a discussion).
Reference: <author> Eisenberg, B. & Rivest, R. </author> <year> (1990), </year> <title> On the sample complexity of PAC-learning using random and chosen examples, </title> <editor> in M. Fulk & J. Case, eds, </editor> <booktitle> `Proceedings of the Third Annual workshop on Computational learning Theory', </booktitle> <address> San Mateo, CA, </address> <pages> pp. 154-162. </pages>
Reference-contexts: Heuristic construction of queries for connectionist learning has been proposed by Baum (Baum 1991, Baum & Lang 1991) who suggests an algorithm for identifying hyperplanes in input space that will separate examples with training output 0 from examples with output 1. It has been shown in <ref> (Eisenberg & Rivest 1990) </ref> that such query construction does not significantly reduce the number of data samples needed for learning the final model. More recently, in (Kulkarni, Mitter & Tsitsiklis 1993) a similar conclusion has been reached following a study using general binary-valued queries.
Reference: <author> Fedorov, V. </author> <year> (1972), </year> <title> Theory of Optimal Experiments, Probability and Mathematical Statistics, </title> <publisher> Academic Press. </publisher>
Reference-contexts: The purpose of asking questions is to gather the most informative/useful training examples for the student. Much of the work in query-learning appears to have evolved from seminal contributions to the statistical Optimal Experimental Design methods by Fedorov <ref> (Fedorov 1972) </ref>. In this context it is worth mentioning that the statistical experimental design literature also includes methods of evolutionary operation (EVOP) of industrial processes (Box & Draper 1969, Box, Hunter & Hunter 1978) which employ response surface methodology (RSM) to obtain plant parameters corresponding to optimal process yield. <p> Active query filtering occurs when selective sampling from an environment is restricted to such regions. In (Cohn 1994) he has used an Optimum Experiment Design approach on the lines of the work of <ref> (Fedorov 1972) </ref>. Cohn's basic querying criterion for future sampling in (Cohn 1994) is the expected value of the learner's mean squared error. The mean squared error has been decoupled into bias and variance terms. <p> REVIEW OF ACTIVE LEARNING RESEARCH 29 adequate reduction of the size of the hidden layer removes the singularity. The Entropy Minimisation Approach Optimal Experimental Design is a field that is concerned with the statistics of gathering new data. It has been studied in <ref> (Fedorov 1972, Atkinson & Donev 1992) </ref> wherein several options of exploring a data space in a principled way have been proposed. In (Fedorov 1972) and (MacKay 1992), it has been suggested that the optimal experiment (query) is the one that contains maximum Shannon information. <p> The Entropy Minimisation Approach Optimal Experimental Design is a field that is concerned with the statistics of gathering new data. It has been studied in (Fedorov 1972, Atkinson & Donev 1992) wherein several options of exploring a data space in a principled way have been proposed. In <ref> (Fedorov 1972) </ref> and (MacKay 1992), it has been suggested that the optimal experiment (query) is the one that contains maximum Shannon information. <p> This is maximal when all the system states are equiprobable (all the p i 's are equal), i.e., there is no order in the system. Minimum entropy, conversely, is tantamount to the most orderly state (maximum information). Fedorov <ref> (Fedorov 1972) </ref> has suggested optimal experiments (queries) based on entropy minimisation. In (MacKay 1992) several objective functions for query selection, based upon information gain (i.e., entropy) have been proposed.
Reference: <author> Fel'dbaum, A. </author> <year> (1960), </year> <title> `Theory of dual control, </title> <booktitle> i-ii', Automation and Remote Control 21(9 and 11), </booktitle> <pages> 1240-1249 and 1453-1464. </pages>
Reference: <author> Fikes, R., Hart, P. & Nilsson, N. </author> <year> (1972), </year> <title> `Learning and executing generalised robot tasks', </title> <booktitle> Artificial Intelligence 3, </booktitle> <pages> 251-288. </pages>
Reference-contexts: INTRODUCTION A common approach to modeling goals is by descriptions of desired results or states in an external world. This corresponds to `task goals' in the previously-discussed model. Such task goals were evident in early planning programs, e.g., STRIPS <ref> (Fikes, Hart & Nilsson 1972) </ref> which specified desired outcomes from a performance task in the outside world. Schank and Abelson (Schank & Abelson 1977) described a category of knowledge goals to determine needed information.
Reference: <author> Freund, Y., Seung, H., Shamir, E. & Tishby, N. </author> <year> (1993), </year> <title> Information, prediction, and query by committee, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: INTRODUCTION 1.2.3 Learning by Querying: Query Construction, Query Filtering All kinds of arguments and questions deep. SHAKESPEARE, Sonnets and other Poetry The idea of the query paradigm is simply that the learner (student) has the power to ask questions of the environment (teacher) <ref> (Freund, Seung, Shamir & Tishby 1993) </ref>. The purpose of asking questions is to gather the most informative/useful training examples for the student. Much of the work in query-learning appears to have evolved from seminal contributions to the statistical Optimal Experimental Design methods by Fedorov (Fedorov 1972). <p> In reality this is a particular kind of query-filter that uses Cohn's general notion of selectively sampling from a region of uncertainty (Cohn et al. 1990). QBC was first proposed by Seung (Seung, Opper & Sompolinsky 1992) and analysed further by Fre-und <ref> (Freund et al. 1993) </ref> who showed in particular that an increase in the number of QBC queries on thresholded smooth functions led to an exponential decrease in the overall generalisation error. <p> The presence of noise in data samples appears to lead to increased querying 4 . More experimental studies using the subsampling QBC algorithm are reported in Chapter 6 where noise effects are further studied. 4 According to <ref> (Freund et al. 1993) </ref> adapting QBC techniques to learning noisy environments is an `important open problem'. 70 CHAPTER 3. QUERYING FOR INFORMATION Random Subsampling Approach Each colour determines an experimental run Random Subsampling Approach 3.3.
Reference: <author> Fukumizu, K. </author> <year> (1996), </year> <title> Active learning in multilayer perceptrons, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 8. </pages>
Reference-contexts: A Bayesian model selection strategy was employed for model complexity regularisation (ensuring that the neural network model had enough hidden nodes). A similar framework has been used in (Sung & Niyogi 1995) who studied problems in one-dimensional input space. In <ref> (Fukumizu 1996) </ref> a method of active querying that also simultaneously trims the learning architecture (multilayer perceptron) is demonstrated. The method uses an inverted Fisher Information matrix (a Hessian) as a component of the objective function as does Cohn in (Cohn 1994). <p> In real life dimensionality of the student-space is approximated from incomplete knowledge of the teacher-space and architecture-adjustment methods are thereafter used to adjust the dimension of the student for an optimal fit <ref> (Fukumizu 1996) </ref> 2 . Using such architecture-adjustment, however, will increase computational expense. 3. The following variations of the objective function v may be interesting to examine * Confidence interval estimates of the uncertainty of longterm expected value.
Reference: <author> Gage, W. </author> <year> (1967), </year> <title> Value Analysis, </title> <publisher> McGraw-Hill, </publisher> <address> London. BIBLIOGRAPHY 185 Gittins, J. C. </address> <year> (1989), </year> <title> Multi-armed bandit allocation indices, Wiley-Interscience series in systems and optimisation, </title> <publisher> Wiley, </publisher> <address> Chichester, New York. </address>
Reference: <author> Gullapalli, V. </author> <year> (1992), </year> <title> Reinforcement Learning and its Application to Control, </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: Reinforcement learning is usually carried out in non-deterministic environments. The learning policy is often derived as a function of discrete finite state-action pairs (s; a), although algorithms in continuous spaces have also been proposed <ref> (Gullapalli 1992, Moore 1994) </ref>. Actions produce state transitions. A state transition T (s; a; s 0 ) probabilistically specifies the next state s 0 of the environment as a function of its current state s and the agent's action a.
Reference: <author> Heskes, T. </author> <year> (1997), </year> <title> Practical confidence and prediction intervals, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 9. </pages> <note> to appear. </note>
Reference-contexts: However the usefulness of the statistical parameters 2 y and y will be apparent in the next Chapter. 1 This work was first reported in (RayChaudhuri & Hamey 1996a). Similar concepts also appear in a recent publication of Heskes <ref> (Heskes 1997) </ref> where the `variation' in an ensemble of neural networks, each trained and stopped early on bootstrap replicates of the original data set, has been derived.
Reference: <author> Hillier, F. S. & Lieberman, G. J. </author> <year> (1980), </year> <title> Introduction to Operations Research, third edn, </title> <publisher> Holden-Day Inc, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: The relationship between such objectives and long-term profit maximisation is, as yet, obscure <ref> (Hillier & Lieberman 1980) </ref>. 1.3 Aim of the Current Investigation (For a List of Contributions of this work please refer to page xxii) In this section the principal objective of the present investigation is made clear, first by a description of a typical `real world' scenario that the work is designed
Reference: <author> Howard, R. A. </author> <year> (1960), </year> <title> Dynamic Programming and Markov Processes, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This method suffers from the so-called `curse of dimensionality' and is model-dependent. Many other model-dependent contributions have been made. Howard's `policy it 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 13 Table 1.1: Milestones in Reinforcement Learning Research 14 CHAPTER 1. INTRODUCTION eration' <ref> (Howard 1960) </ref> consists essentially of computing the value function V (s) of an arbitrary learning policy : S ! A and using this value function as a basis, improving the policy at each state until no further improvement is possible.
Reference: <author> Jaakkola, T., Singh, S. P. & Jordan, M. I. </author> <year> (1995), </year> <title> Monte-Carlo reinforcement learning in non-Markovian decision problems, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 7. </pages>
Reference-contexts: Q-learning is a very popular model-free algorithm for learning from delayed reinforcement. It has been adapted to an average-reward framework (R-learning) in (Schwartz 1993) and <ref> (Jaakkola, Singh & Jordan 1995) </ref>. Duff has used Gittins indices and Watkins' Q-learning in a model-free approach (Duff 1995) to addressing multi-armed bandit problems.
Reference: <author> Kaelbling, L. P. </author> <year> (1993), </year> <title> Learning in Embedded Systems, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The method has been shown to significantly improve upon Dyna, both in terms of computational effort as well as the number of steps of experience required. Another method of directed exploration is Kaelbling's `interval estimation' <ref> (Kaelbling 1993) </ref> algorithm. This is based on the philosophy that uncertainty-based exploration seems more efficient. The steps are, briefly 1. Store statistics for each action a i : the number of successes s i and the number of trials n i . 2. <p> REVIEW OF ACTIVE LEARNING RESEARCH 21 ub (a i ) = n i s n i s i (1 n i 3. choose the action with the highest ub. Kaelbling's `interval estimation' <ref> (Kaelbling 1993) </ref> algorithm is thus hinged around choosing the highest upper bound of the confidence interval of success probability of an action. Yet another method of directed exploration of Markov environments is the RDIA (Reinforcement Driven Information Acquistion) approach of (Storck, Hochreiter & Schmidhuber 1995). <p> The uncertainty of expected reward will be estimated using confidence intervals. This is different from the uncertainty of the success probability of an action defined in the interval estimation algorithm <ref> (Kaelbling 1993) </ref>. The majority of known query learning methods aim at the goal of function approximation and generalisation. In contrast the objective pursued herein is ultimately process yield improvement.
Reference: <author> Kaelbling, L. P., Littman, M. L. & Moore, A. W. </author> <year> (1996), </year> <title> `Reinforcement learning: A survey', </title> <journal> Journal of Artificial Intelligence Research 4, </journal> <pages> 237-285. </pages>
Reference-contexts: The term `reinforcement learning', borrowed from animal learning 10 CHAPTER 1. INTRODUCTION literature, essentially refers to all problems where an agent learns behaviour through trial-and-error interactions with its environment <ref> (Kaelbling, Littman & Moore 1996) </ref>. The emphasis of the definition is not upon characterising learning algorithms, but upon the learning problem (Sutton & Barto 1997). Reinforcement learning is a computational approach to the study of learning from interaction. <p> The method also differs from supervised learning in that on-line performance is more important how well the agent performs at a particular stage often determines how well it has learned so far. Detailed tutorial reviews of reinforcement learning research can be found in (Keerthi & Ravindran 1995) and <ref> (Kaelbling et al. 1996) </ref>. For an understanding of the philosophical and real-world perspective (Sutton & Barto 1997) is highly useful. The literature is reasonably extensive and spans more than three decades, although the last decade has attracted rather more contributions from AI researchers. <p> If the optimal action is set well apart from others then this method is found to work well, but when values of actions are close, Boltzmann learning is not so reliable <ref> (Kaelbling et al. 1996) </ref>. The `value iteration' approach (as different from policy iteration) is based upon the idea that the optimal policy fl will be the one which finds the 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 15 optimal value function V fl . <p> However such techniques compute best estimates only for the states the system is actually in and do not always effectively utilise available data. Thus they often need to gather a great deal of experience before beginning to 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 19 perform well <ref> (Kaelbling et al. 1996) </ref>. Model-free learning, therefore, is controversial yet and many researchers are of the opinion that the importance of deriving a learned model cannot be ignored.
Reference: <author> Keerthi, S. S. & Ravindran, B. </author> <year> (1995), </year> <title> `A tutorial survey of reinforcement learning', </title> <note> Sadhana (published by the Indian Academy of Sciences). </note>
Reference-contexts: The method also differs from supervised learning in that on-line performance is more important how well the agent performs at a particular stage often determines how well it has learned so far. Detailed tutorial reviews of reinforcement learning research can be found in <ref> (Keerthi & Ravindran 1995) </ref> and (Kaelbling et al. 1996). For an understanding of the philosophical and real-world perspective (Sutton & Barto 1997) is highly useful. The literature is reasonably extensive and spans more than three decades, although the last decade has attracted rather more contributions from AI researchers.
Reference: <author> Krogh, A. & Vedelsby, J. </author> <year> (1995), </year> <title> Neural network ensembles, cross validation and active learning, </title> <editor> in G. Tesauro, D. Touretzky & T. Leen, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 7', </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address> <note> 186 BIBLIOGRAPHY Kulkarni, </note> <author> S. R., Mitter, S. K. & Tsitsiklis, J. N. </author> <year> (1993), </year> <title> `Active learning using arbitrary binary-valued queries', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 23-35. </pages>
Reference-contexts: Matan (Matan 1995) has made studies using the QBC method for real-world OCR tasks using feedforward neural network models. Krogh and Vedelsby <ref> (Krogh & Vedelsby 1995) </ref> have examined the problem of learning a square-wave environment with an active querying committee of five feedforward neural network models. Although there is mention in (Krogh & Vedelsby 1995) of using different training sets for increasing `ensemble ambiguity', the QBC experiments reported (Section 5 of (Krogh & <p> Krogh and Vedelsby <ref> (Krogh & Vedelsby 1995) </ref> have examined the problem of learning a square-wave environment with an active querying committee of five feedforward neural network models. Although there is mention in (Krogh & Vedelsby 1995) of using different training sets for increasing `ensemble ambiguity', the QBC experiments reported (Section 5 of (Krogh & Vedelsby 1995)) deliberately use the same data set to train each committee member with a view to reducing both error and ambiguity. <p> Vedelsby <ref> (Krogh & Vedelsby 1995) </ref> have examined the problem of learning a square-wave environment with an active querying committee of five feedforward neural network models. Although there is mention in (Krogh & Vedelsby 1995) of using different training sets for increasing `ensemble ambiguity', the QBC experiments reported (Section 5 of (Krogh & Vedelsby 1995)) deliberately use the same data set to train each committee member with a view to reducing both error and ambiguity. <p> This was shown to lead to generalisation error decreasing exponentially with the number of examples. The analysis was performed under the assumption that the training set was not corrupted by noise. Experimental studies were performed using a querying committee of feed-forward neural networks trained by backpropagation in <ref> (Krogh & Vedelsby 1995) </ref> and (RayChaudhuri & Hamey 1995a, RayChaudhuri & Hamey 1995b, 3.3. SUBSAMPLING FOR COMMITTEE DISAGREEMENT 51 RayChaudhuri & Hamey 1996b). <p> It is supposed that a small set S of data consisting of a m labeled points has already been sampled at random, i.e., S = f (x i ; y i ); i = 1; : : : ; mg. In Krogh and Vedelsby's paper <ref> (Krogh & Vedelsby 1995) </ref> and our earlier work (RayChaudhuri & Hamey 1995a, Ray-Chaudhuri & Hamey 1995b, RayChaudhuri & Hamey 1996b) the statistical variance function (see relation 3.3) of the different hypotheses of y for each x propounded by a committee of feedforward neural networks, is computed. <p> This function (see the discussion on `A Criterion for Agreement' in the next subsection) is defined to quantify the level of disagreement | hence the point corresponding to the global maximum of this variance is chosen for querying. Krogh and Vedelsby's approach in <ref> (Krogh & Vedelsby 1995) </ref> was to train a committee of feedforward neural networks on the entire set of data in S and, based on such trained committee hypotheses of the output labels y, to compute the the variance (disagreement level) function over the input space X. <p> QUERYING FOR INFORMATION data is likely to be sampled. The problem is therefore one of ensuring adequate data querying. In <ref> (Krogh & Vedelsby 1995) </ref> it has been dealt with by using a much larger-than-necessary number of hidden nodes (i.e., over-fitting). <p> The test of agreement takes place according to either of the conditions 3.3 or 3.4 in the previous subsection. 3.3.3 Experimentation: Concentration of Sampling in Significant Regions A previous active learning algorithm based upon querying-by-committee using feedforward neural nets was proposed by Krogh and Vedelsby <ref> (Krogh & Vedelsby 1995) </ref> who also conducted experimental studies to test the method. 3.3. SUBSAMPLING FOR COMMITTEE DISAGREEMENT 57 58 CHAPTER 3. QUERYING FOR INFORMATION The technique, however, did not include the definition of a stopping criterion, nor was a study made of the amount of data collected. <p> SUBSAMPLING FOR COMMITTEE DISAGREEMENT 57 58 CHAPTER 3. QUERYING FOR INFORMATION The technique, however, did not include the definition of a stopping criterion, nor was a study made of the amount of data collected. The studies reported <ref> (Krogh & Vedelsby 1995) </ref> concentrated upon the difference in gen-eralisation error in the cases of active and passive learning. In contrast a stopping criterion for the iterative querying has been specified herein, viz., the maximum model variance equalling or dropping below the threshold value *. <p> Sufficient model disagreement has also been ensured by the approach of randomly subsampling half the collected data to train each network of the committee, thereby resulting in an adequate number of data points being collected. In <ref> (Krogh & Vedelsby 1995) </ref> all the collected data was given to all the nets at each iteration. As previously discussed, in such a case all the nets of the committee agree as soon as sufficient data points have been collected to define the weight and bias parameters of each network. <p> Setup Comparison experiments have been performed to illustrate the above 3.3. SUBSAMPLING FOR COMMITTEE DISAGREEMENT 59 claims. Both Krogh and Vedelsby's approach <ref> (Krogh & Vedelsby 1995) </ref> and the proposed random subsampling committte method (RayChaudhuri & Hamey 1995a, RayChaudhuri & Hamey 1995b, RayChaudhuri & Hamey 1996b) were employed to perform querying experiments. First a square wave step was used for the environment to be identified. <p> To reduce overfitting possibilites the nets were chosen to have a minimal 1-1-1 architecture each. Every net had a sigmoidal hidden node and a linear output node. The training data began with two labeled points selected randomly from the environment. The point corresponding to highest "ensemble ambiguity" <ref> (Krogh & Vedelsby 1995) </ref> or model variance among 1500 randomly chosen input points was queried from the environment. Following Krogh and Vedelsby, in the first lot of experiments all the networks were trained upon the entire set of collected data samples in each iteration. <p> The querying criterion in <ref> (Krogh & Vedelsby 1995) </ref> therefore has scope for improvement, especially if the parameter space of the learner is relatively small, i.e., when little or no overfitting occurs. The presence of noise in data samples appears to lead to increased querying 4 . <p> The algorithm converged, but as is only to be expected, more points were collected than in the experiment with active learning. Figure 6.7 shows the final sampled points and Figure 6.8 the corresponding plot of final variance. This is a case of `passive' learning <ref> (Krogh & Vedelsby 1995) </ref> | the selection of the additional labeled sample each time is arbitrary. 6.1.3 Experiments with Noisy Data Investigations so far employed clean data which is mathematically easier to handle. Noise was now added to the output of the plant (Figure 6.9).
Reference: <author> Littman, M., Cassandra, A. & Kaelbling, L. </author> <year> (1995), </year> <title> Learning policies for partially observable environments: Scaling up, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Proceedings of the Twelfth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San francisco,CA, </address> <pages> pp. 362-370. </pages>
Reference-contexts: Much of the work in Reinforcement learning is based on the assumption that the learning agent is able to perceive the exact state of the environment. This may not be the case in a real world situation. Reported in <ref> (Littman, Cassandra & Kaelbling 1995) </ref> is work upon the use of so-called `Partially Observable Markov Decision Processes' (POMDPs) which employ belief state observers to estimate states that cannot be directly perceived.
Reference: <author> MacKay, D. </author> <year> (1992), </year> <title> `Information-based objective functions for active data selection', </title> <booktitle> Neural Computation 4(4), </booktitle> <pages> 590-604. </pages>
Reference-contexts: It has been studied in (Fedorov 1972, Atkinson & Donev 1992) wherein several options of exploring a data space in a principled way have been proposed. In (Fedorov 1972) and <ref> (MacKay 1992) </ref>, it has been suggested that the optimal experiment (query) is the one that contains maximum Shannon information. Shannon (Shannon 1948) first proposed the concept of the entropy measure of information in the theory of communications and the idea has had widespread application in many areas of science. <p> Minimum entropy, conversely, is tantamount to the most orderly state (maximum information). Fedorov (Fedorov 1972) has suggested optimal experiments (queries) based on entropy minimisation. In <ref> (MacKay 1992) </ref> several objective functions for query selection, based upon information gain (i.e., entropy) have been proposed. <p> Desirable criteria for actively choosing such training data are 1. The training examples contain maximum information about environ ment characteristics <ref> (MacKay 1992) </ref>. As a learning model grows closer to fulfilling this condition there exists a greater possibility of improved generalisation performance (Cohn 1994). 2. Data collection costs are optimised. This is not simply a case of the quantity m being optimal. <p> In a real-world situation costs of labeling individual data points differ. 1 Backpropagation is known to work efficiently when residuals yi f w (x i ) are normally distributed. 2.2. QUERYING STRATEGY 43 Methods of meeting the first objective have been suggested by other researchers <ref> (MacKay 1992, Plutowski & White 1993, Cohn 1994, Sollich 1995) </ref>. Most commonly used objective functions in this regard are expected gener-alisation error and entropy (information level). For an in-depth treatment of such objective functions, please refer to (MacKay 1992). <p> Most commonly used objective functions in this regard are expected gener-alisation error and entropy (information level). For an in-depth treatment of such objective functions, please refer to <ref> (MacKay 1992) </ref>. One of the primary issues addressed in the present work is the cost aspect, although a contribution to the first objective is also made. 2.2 Querying Strategy It will be assumed that the controlled input space x is entirely visible deterministically to the learner.
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to coordinate behaviors, </title> <booktitle> in `Proceedings of Eighth National Conference on Artificial Intelligence', </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <pages> pp. 796-802. </pages>
Reference-contexts: The hierarchical approach (treating the environment as a hierarchy of learning problems) has been suggested as a means of dealing with large state spaces. An example of such work can be found in <ref> (Maes & Brooks 1990) </ref> where the hierarchical learners are looked upon as a collection of gated be-haviours. Behaviours map environment states into low-level actions and a gating function decides the choice of actions. In the work of (Maes & Brooks 1990) individual behaviours were fixed a priori and the gating function <p> An example of such work can be found in <ref> (Maes & Brooks 1990) </ref> where the hierarchical learners are looked upon as a collection of gated be-haviours. Behaviours map environment states into low-level actions and a gating function decides the choice of actions. In the work of (Maes & Brooks 1990) individual behaviours were fixed a priori and the gating function was learned from reinforcement. The `Parti-game' method of Moore (Moore 1994) addresses learning in continuous action high-dimensional state spaces.
Reference: <author> Matan, O. </author> <year> (1995), </year> <title> On-site learning, </title> <type> Technical Report LOGIC-95-4, </type> <institution> The Logic Group, Stanford University. </institution>
Reference-contexts: The basic principle is that if a committee of hypotheses are in disagreement over an unlabeled data point then it is necessary to query the environment for its label and to add it to the training set. Matan <ref> (Matan 1995) </ref> has made studies using the QBC method for real-world OCR tasks using feedforward neural network models. Krogh and Vedelsby (Krogh & Vedelsby 1995) have examined the problem of learning a square-wave environment with an active querying committee of five feedforward neural network models.
Reference: <author> McCallum, A. K. </author> <year> (1997), </year> <title> Efficient exploration of reinforcement learning with hidden state, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 9. </volume> <pages> preprint. </pages>
Reference: <author> Miles, L. </author> <year> (1961), </year> <title> Techniques of Value Analysis and Engineering, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The goal-driven learning literature advises definitely against the effort of obtaining knowledge that has zero contribution to the learner's ultimate objective (Ram & Leake 1995). In the operations research literature the notion of value analysis as an organised approach to challenging unnecessary cost is well-known <ref> (Miles 1961, Gage 1967) </ref>.
Reference: <author> Mitchell, T., Keller, R. & Kedar-Cabelli, S. </author> <year> (1986), </year> <title> `Explanation-based gen-eralisation: A unifying view', </title> <booktitle> Machine Learning 1(1), </booktitle> <pages> 47-80. </pages>
Reference-contexts: However they do not specify the motivation for learning like the learning goal notion of (Ram & Hunter 1992) discussed previously. In the EBG algorithm of <ref> (Mitchell, Keller & Kedar-Cabelli 1986) </ref> a target concept or a `goal concept' is taken as an input in order to learn an operational description of that concept. In (Ram 1990) the use of knowledge goals is proposed as a basis for focussing attention in understanding and learning.
Reference: <author> Moore, A. W. </author> <year> (1994), </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 6. </pages>
Reference-contexts: Behaviours map environment states into low-level actions and a gating function decides the choice of actions. In the work of (Maes & Brooks 1990) individual behaviours were fixed a priori and the gating function was learned from reinforcement. The `Parti-game' method of Moore <ref> (Moore 1994) </ref> addresses learning in continuous action high-dimensional state spaces. This is an adaptive resolution method and is based upon learning a good discretization of the state-space while learning the model and the optimal value function. The algorithm 22 CHAPTER 1. <p> Parti-game is a fast algorithm and claims have been made of learning policies in spaces upto nine dimensions within a minute <ref> (Moore 1994) </ref>. Much of the work in Reinforcement learning is based on the assumption that the learning agent is able to perceive the exact state of the environment. This may not be the case in a real world situation.
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993), </year> <title> `Prioritized Sweeping: Reinforcement learning with less data and less real time', Machine Learning. BIBLIOGRAPHY 187 Mosteller, </title> <editor> F. & Tukey, J. W. </editor> <year> (1977), </year> <title> Data Analysis and Regression A Second Course in Statistics, </title> <publisher> Addison-Wesley Publishing Co, Inc, Philippines. </publisher>
Reference-contexts: However it is based on random exploration and does not direct the learner to examine more `useful' or `interesting' regions of the state-space. Moore and Atkeson's `Prioritized Sweeping' <ref> (Moore & Atkeson 1993) </ref> is 20 CHAPTER 1. INTRODUCTION a major improvement upon Sutton's `Dyna' in terms of directed exploration. In prioritized sweeping each state is assigned a priority. Instead of updating k random state-action pairs as in Dyna, the algorithm updates k states with the highest priority.
Reference: <author> Narendra, K. S. & Thatchachar, M. A. L. </author> <year> (1989), </year> <title> Learning Automata: An Introduction, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: The method is guaranteed to terminate with optimal policy fl . Narendra and Thatchachar <ref> (Narendra & Thatchachar 1989) </ref> suggest a `linear reward-penalty' probabilistic scheme which assumes that internal state is a probability distribution over possible actions. If, chosen from such a distribution an action succeeds then its probability is increased linearly; if it fails then its probability is linearly decreased. <p> By making small the numerical value of the coefficient of increment/decrement of probability of action choice, the possibility of choosing the wrong action can be considerably reduced in this algorithm, but convergence to the correct action is not guaranteed <ref> (Narendra & Thatchachar 1989) </ref>. Gittins (Gittins 1989) uses an `allocation index' for finding the choice of action at each step. The action with the largest index value is chosen.
Reference: <author> Oehlmann, R., Sleeman, D. & Edwards, P. </author> <year> (1992), </year> <title> Self-questioning and experimentation in an exploratory discovery system, </title> <booktitle> in `Proceedings of the ML-92 Workshop on Machine Discovery', </booktitle> <pages> pp. 41-50. </pages> <booktitle> Ninth International Machine Learning Conference, </booktitle> <institution> University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: In (Ram 1990) the use of knowledge goals is proposed as a basis for focussing attention in understanding and learning. A theory of `interestingness' is described. In Oehlmann, Sleeman and Edwards' IULIAN <ref> (Oehlmann, Sleeman & Ed-wards 1992) </ref> questions and experimentation interact in an exploratory discovery process applied to electrical circuits. desJardins' PAGODA (desJardins 1992b, desJardins 1992a) employs decison theory principles. A comprehensive treatment of the principles of statistical decision theory can be found in (Berger 1980).
Reference: <author> Paass, G. & Kindermann, J. </author> <year> (1995), </year> <title> Bayesian query construction for neural network models, </title> <editor> in D. T. et al, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 7, </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Recently in (Cohn 1995b) he demonstrates that for a locally-weighted regression query learner, bias minimisation outperforms variance minimisation. Such bias is estimated either by cross-validation or by bootstrapping residuals. Other investigations in active querying methods to obtain data selectively have been carried out by Paass and Kindermann <ref> (Paass & Kindermann 1995) </ref> who queried data so as to minimise an average Bayesian risk decision criterion. To assist computation of average risk a Markov Chain Monte Carlo computational strategy was used to approximate the Bayesian future posterior distribution of learning parameters.
Reference: <author> Plutowski, M. </author> <year> (1994), </year> <title> Selected Training Exemplars for Neural Network Learning, </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: Neural net learning models are, however, highly data (i.e., information)-dependent and the early algorithms were based upon reception of pre-gathered information <ref> (Plutowski 1994) </ref>. While using them researchers soon realised that there were serious limitations to such passive algorithms used for `training' the nets (Cohn 1994). The main limitation was a gen-eralisation 1 problem. <p> QUERYING FOR INFORMATION 3.2 Model Uncertainty and Query-by-Committee A popular trend in active learning appears to be exploitation of some estimate of model uncertainty. Data collection guided by estimated neural network model variance using a Fisher Information matrix (Cohn 1994) and by measures of model bias <ref> (Plutowski 1994, Cohn 1995b) </ref> have been examined (see Section 1.2.3 for reviews of these studies). Recently in (Schneider 1997) Bayesian locally-weighted regression has been profitably used to estimate model unreliabilty.
Reference: <author> Plutowski, M. & White, H. </author> <year> (1993), </year> <title> `Selecting concise training sets from clean data', </title> <journal> IEEE Transactions on Neural Networks 4(2), </journal> <pages> 305-318. </pages>
Reference: <author> Poggio, T. & Girosi, F. </author> <year> (1989), </year> <title> A theory of networks for approximation and learning, </title> <type> Technical Report AIM-1140, </type> <institution> Artificial Intelligence Laboratory and Center for Biological Information Processing, Whitaker College, Massachusetts Institute of Technology. </institution>
Reference-contexts: This is chiefly because of the `automatic' learning feature of neural nets and the fact that they serve as function ap-proximators for nonlinear systems to a high degree of accuracy <ref> (Poggio & Girosi 1989) </ref>. Neural net learning models are, however, highly data (i.e., information)-dependent and the early algorithms were based upon reception of pre-gathered information (Plutowski 1994). While using them researchers soon realised that there were serious limitations to such passive algorithms used for `training' the nets (Cohn 1994).
Reference: <author> Quenouille, M. </author> <year> (1949), </year> <title> `Approximation tests of correlation in time series', </title> <journal> J. </journal>
Reference-contexts: A theoretical foundation more rigorous than the preliminary analysis in Section 3.3.1 will be established. The basis will be a jack-knife technique applied to the available sample S for obtaining data for training the committee nets. 4.1 A Jack-knifed Committee Method Jack-knifing <ref> (Quenouille 1949, Tukey 1958) </ref> and bootstrapping (Efron 1979) are well-known statistical methods of data-resampling in order to approximate sampling distribution. In this work the jackknife approach is employed on account of its similarity with the random subsampling technique used earlier (see Section 4.3 for a discussion).
Reference: <author> R. </author> <booktitle> Statist Soc. </booktitle> <volume> B 11, </volume> <pages> 18-84. </pages>
Reference: <author> Ram, A. </author> <year> (1990), </year> <title> Knowledge goals: A theory of interestingness, </title> <booktitle> in `Proceedings of the Twelfth Annual Conference of the Cognitive Science Society', </booktitle> <pages> pp. 206-214. </pages> <note> 188 BIBLIOGRAPHY Ram, </note> <author> A. & Hunter, L. </author> <year> (1992), </year> <title> `The use of explicit goals for knowledge to guide inference and learning', </title> <booktitle> Applied Intelligence 2(1), </booktitle> <pages> 47-73. </pages>
Reference-contexts: In the EBG algorithm of (Mitchell, Keller & Kedar-Cabelli 1986) a target concept or a `goal concept' is taken as an input in order to learn an operational description of that concept. In <ref> (Ram 1990) </ref> the use of knowledge goals is proposed as a basis for focussing attention in understanding and learning. A theory of `interestingness' is described.
Reference: <author> Ram, A. & Leake, D. </author> <year> (1995), </year> <title> Goal-Driven Learning, </title> <publisher> MIT Press. </publisher>
Reference-contexts: A goal-driven learner determines what to learn by reasoning about the information it needs, and determines how to learn by reasoning about the relative merits of alternative learning strategies available. A primary goal may give rise to sub-goals. A common real-world example <ref> (Ram & Leake 1995) </ref> is that of learning to purchase a good product (the main goal). Two apparent subgoals are to learn of the best market sources for the product and to learn criteria for judging the relative merits of competing products. <p> An example of such a resource is stored information 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 7 needed for learning. The computational complexity of a search procedure is simplified if information is stored selectively according to priority determined by a goal. In their book on Goal-Driven Learning, Ram and Leake <ref> (Ram & Leake 1995) </ref> describe a framework of such learning as An active (explicitly goal-driven) and strategic (rational and deliberative) process in which a reasoner, human or machine, explicitly identifies its goals in learning and attempts to learn by determining and pursuing appropriate learning actions via explicit reasoning about its goals, <p> For a more thorough coverage <ref> (Ram & Leake 1995) </ref> may be consulted. 1.2.2 The Reinforcement Learning Approach A common real-world example of learning by interaction is that of an infant's motor development (Sutton & Barto 1997). The infant has a direct senso-rimotor connection to its environment but no explicit teacher. <p> A greedy (one-step) optimisation approach is used and it has been discussed that globally optimal queries would be computationally expensive to the extent that they may not be worthwhile. In his subsequent work <ref> (Cohn, Ghahramani & Jordan 1995) </ref> there is a paradigm shift | he has migrated to using statistically-based learning architectures (mixtures of Gaussians and locally-weighted regression) in place of the neural-network models of his earlier work so as to reduce the computational costs of repeatedly retraining neural network models. <p> The goal-driven learning literature advises definitely against the effort of obtaining knowledge that has zero contribution to the learner's ultimate objective <ref> (Ram & Leake 1995) </ref>. In the operations research literature the notion of value analysis as an organised approach to challenging unnecessary cost is well-known (Miles 1961, Gage 1967).
Reference: <author> Rasmussen, C. E. </author> <year> (1996), </year> <title> `Multilayer perceptron ensembles trained with early stopping', </title> <note> available at http://www.cs.utoronto.ca/ delve/methods/mlp-ese-1/mlp-ese-1.html. included within the DELVE project. </note>
Reference-contexts: A theoretical treatment of random subsampling from a statistical viewpoint is available in Chapter 5 of (Shao & Tu 1995). The inherent power of randomly subsampling a data set has been used subsequently in studies upon multilayer perceptron ensembles trained by early stopping <ref> (Rasmussen 1996) </ref>. This approach is directed towards avoidance of overfitting.
Reference: <author> RayChaudhuri, T. & Hamey, L. G. </author> <year> (1996a), </year> <title> Cost-effective querying leading to dual control, </title> <type> Technical Report 96-07, </type> <month> June </month> <year> 1996, </year> <institution> Department of Computing, Macquarie University. </institution>
Reference-contexts: For such an approach the actual computation of pseudovalues and their statistics appears somewhat unnecessary at first glance. However the usefulness of the statistical parameters 2 y and y will be apparent in the next Chapter. 1 This work was first reported in <ref> (RayChaudhuri & Hamey 1996a) </ref>. Similar concepts also appear in a recent publication of Heskes (Heskes 1997) where the `variation' in an ensemble of neural networks, each trained and stopped early on bootstrap replicates of the original data set, has been derived.
Reference: <author> RayChaudhuri, T. & Hamey, L. G. C. </author> <year> (1995a), </year> <title> An algorithm for active data collection for learning|feasibility study with neural networks, </title> <type> Technical Report 95-173C, </type> <institution> Department of Computing, Macquarie University, </institution> <address> NSW 2109 Australia. URL: ftp://ftp.mpce.mq.edu.au/pub/comp/ techreports/950173.raychaudhuri.ps. </address>
Reference-contexts: Two approaches to query filtering (see Table 1.2) should be recognised. The first one consists of filtering an abundant, already-labeled data-set for the most useful examples. This approach, which we have chosen to call active data subset selection <ref> (RayChaudhuri & Hamey 1995a) </ref>, will reduce the computational activity in the ultimate learning process but will not, clearly, reduce the cost of data collection. Such filtering methods lack repeated active interaction with the actual environment in question. Therefore these are in reality passive learning techniques that use iterative experimental design. <p> In <ref> (RayChaudhuri & Hamey 1995a) </ref> we have proposed a QBC algorithm with a well-defined stopping criterion for the learning process and a method of ensuring disagreement among the committee members by having each committee member trained on subsets of the already-labeled data (a detailed discussion appears in Section 3.3). 1.2.
Reference: <author> RayChaudhuri, T. & Hamey, L. G. C. </author> <year> (1995b), </year> <title> Minimisation of data collection by active learning, </title> <booktitle> in `IEEE Int. Conf. Neural Networks', </booktitle> <volume> Vol. 3, </volume> <booktitle> IEEE, </booktitle> <pages> pp. 1338-1341. </pages>
Reference-contexts: A random subsampling method has been proposed as an alternative in <ref> (RayChaudhuri & Hamey 1995b, RayChaudhuri & Hamey 1996b) </ref>. A theoretical treatment of random subsampling from a statistical viewpoint is available in Chapter 5 of (Shao & Tu 1995).
Reference: <author> RayChaudhuri, T. & Hamey, L. G. C. </author> <year> (1996b), </year> <title> Accurate modelling with minimised data collection|an active learning algorithm, </title> <editor> in P. Bartlett, A. Burkitt & R. C. Williamson, eds, </editor> <booktitle> `Proc. Australian Conf. Art. Neural Networks', </booktitle> <publisher> Australian Nat. Univ., </publisher> <pages> pp. 11-15. </pages>
Reference-contexts: The global maximum of this variance function corresponded to the input selected for querying. As pointed out in <ref> (RayChaudhuri & Hamey 1996b) </ref> this approach may result merely in the querying of a number of points that is sufficient to define all the parameters of the learner, e.g., the weights and biases of a neural net.
Reference: <author> RayChaudhuri, T., Hamey, L. G. C. & Bell, R. D. </author> <year> (1995), </year> <title> Neural network control using active learning, </title> <booktitle> in `Control 95', </booktitle> <volume> Vol. </volume> <pages> 2, </pages> <institution> Inst. Eng. </institution> <address> Australia, </address> <pages> pp. 369-373. </pages> <editor> -Astrom, K. J. & Wittenmark, B. </editor> <year> (1989), </year> <title> Adaptive Control, Addison-Wesley. BIBLIOGRAPHY 189 Rumelhart, </title> <editor> D., Hinton, G. & Williams, R. </editor> <year> (1986), </year> <title> `Learning representations by back-propagating errors', </title> <booktitle> Nature 323, </booktitle> <pages> 533-536. </pages>
Reference-contexts: Thus instead of completely replacing a conventional control method one could use complementary controller levels. Such a scheme (see Figure 8.2) has been proposed in one of the author's previous publications <ref> (RayChaudhuri, Hamey & Bell 1995) </ref>. In (Schneider 1997) one finds a very recent example of such an approach where the active learning component is implemented using Bayesian locally weighted regression and stochastic dynamic programming. 162 CHAPTER 8.
Reference: <author> Savage, L. J. </author> <year> (1977), </year> <title> The Foundations of Statistics, second revised edn, </title> <publisher> Dover. </publisher>
Reference: <author> Schank, R. & Abelson, R. </author> <year> (1977), </year> <title> Scripts, Plans, Goals and Understanding, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: This corresponds to `task goals' in the previously-discussed model. Such task goals were evident in early planning programs, e.g., STRIPS (Fikes, Hart & Nilsson 1972) which specified desired outcomes from a performance task in the outside world. Schank and Abelson <ref> (Schank & Abelson 1977) </ref> described a category of knowledge goals to determine needed information. In their model called D-KNOW, goals of this type arise when a planner requres knowledge of particular facts (such as the location of a desired object) in order to achieve other goals.
Reference: <author> Schneider, J. G. </author> <year> (1997), </year> <title> Exploiting model uncertainty estimates for safe dynamic control learning, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 9. </pages> <note> to appear. </note>
Reference-contexts: Data collection guided by estimated neural network model variance using a Fisher Information matrix (Cohn 1994) and by measures of model bias (Plutowski 1994, Cohn 1995b) have been examined (see Section 1.2.3 for reviews of these studies). Recently in <ref> (Schneider 1997) </ref> Bayesian locally-weighted regression has been profitably used to estimate model unreliabilty. In this dissertation the approach used for model uncertainty estimation is based upon the disagreement between different hypotheses proposed by a committee of models. This method is often called query by committee (QBC). <p> Thus instead of completely replacing a conventional control method one could use complementary controller levels. Such a scheme (see Figure 8.2) has been proposed in one of the author's previous publications (RayChaudhuri, Hamey & Bell 1995). In <ref> (Schneider 1997) </ref> one finds a very recent example of such an approach where the active learning component is implemented using Bayesian locally weighted regression and stochastic dynamic programming. 162 CHAPTER 8. DUAL CONTROL experiments in order to improve the estimate of the controlled input setting that produces cost-effective output.
Reference: <author> Schwartz, A. </author> <year> (1993), </year> <title> A reinforcement learning method for maximising undis-counted rewards, </title> <booktitle> in `Proceedings of the Tenth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Amherst, MA, </address> <pages> pp. 298-305. </pages>
Reference-contexts: Q-learning is a very popular model-free algorithm for learning from delayed reinforcement. It has been adapted to an average-reward framework (R-learning) in <ref> (Schwartz 1993) </ref> and (Jaakkola, Singh & Jordan 1995). Duff has used Gittins indices and Watkins' Q-learning in a model-free approach (Duff 1995) to addressing multi-armed bandit problems.
Reference: <author> Scott, D. </author> <year> (1992), </year> <title> Multivariate Density Estimation. Theory, Practice and Visualization, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: ALEXANDER POPE The results are shown in the form of histograms of collected data. Choice of bin width and number of bins in a histogram were in accordance with a lower bound of the number of bins and an upper bound of the bin width recommended in <ref> (Scott 1992) </ref>. According to (Scott 1992) the number of bins of a histogram should be at least 3 p 2n where n is the number of samples while the bin width should not exceed 2:61Rn 1=3 , R being the inter-quartile range. <p> Choice of bin width and number of bins in a histogram were in accordance with a lower bound of the number of bins and an upper bound of the bin width recommended in <ref> (Scott 1992) </ref>. According to (Scott 1992) the number of bins of a histogram should be at least 3 p 2n where n is the number of samples while the bin width should not exceed 2:61Rn 1=3 , R being the inter-quartile range. Figures 6.13, 6.14, 6.15, 6.16, 6.17 and 6.18 depict these histograms. <p> of higher value is initially created around such regions leading to higher initial data gathering in these zones. 3 For all experimental results reported in this Chapter the histogram parameters, i.e., bin-width and number of bins were chosen in accordance with recommended upper and lower bounds for such parameters in <ref> (Scott 1992) </ref>. Formulae for these bounds appear earlier in Section 6.2 of the previous Chapter. 118 CHAPTER 7. COST-EFFECTIVE QUERYING EXPERIMENTS ff = 0 time, the querying frequencies at other zones of the input space progressively increase.
Reference: <author> Seung, H., Opper, M. & Sompolinsky, H. </author> <year> (1992), </year> <title> Query by committee, </title> <booktitle> in `Proceedings of the Fifth Workshop on Computational Learning Theory', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <pages> pp. 287-294. </pages>
Reference-contexts: In reality this is a particular kind of query-filter that uses Cohn's general notion of selectively sampling from a region of uncertainty (Cohn et al. 1990). QBC was first proposed by Seung <ref> (Seung, Opper & Sompolinsky 1992) </ref> and analysed further by Fre-und (Freund et al. 1993) who showed in particular that an increase in the number of QBC queries on thresholded smooth functions led to an exponential decrease in the overall generalisation error. <p> In this dissertation the approach used for model uncertainty estimation is based upon the disagreement between different hypotheses proposed by a committee of models. This method is often called query by committee (QBC). The QBC algorithm was first proposed by Seung et al <ref> (Seung et al. 1992) </ref>. <p> Theoretical methods have been developed to address multidimensional environments. It has been discussed that the dollar value of a particular output label is always a scalar irrespective of the dimensionality of the output. The approach makes prolific use of Seung's `query-by-committee' algorithm <ref> (Seung et al. 1992) </ref> for estimating model uncertainty. Strength has been added to the method by randomly subsampling or statistically jackknifing the sampled data before training the committee members. Consequently a reliable estimate of the learner's variance of a label is obtained.
Reference: <author> Shannon, C. </author> <year> (1948), </year> <title> `A mathematical theory of communication', </title> <institution> Bell Syst. </institution>
Reference-contexts: In (Fedorov 1972) and (MacKay 1992), it has been suggested that the optimal experiment (query) is the one that contains maximum Shannon information. Shannon <ref> (Shannon 1948) </ref> first proposed the concept of the entropy measure of information in the theory of communications and the idea has had widespread application in many areas of science.
Reference: <editor> Tech. J. </editor> <volume> 27, </volume> <pages> 379-423. </pages>
Reference: <author> Shao, J. & Tu, D. </author> <year> (1995), </year> <title> The Jackknife and Bootstrap, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: A random subsampling method has been proposed as an alternative in (RayChaudhuri & Hamey 1995b, RayChaudhuri & Hamey 1996b). A theoretical treatment of random subsampling from a statistical viewpoint is available in Chapter 5 of <ref> (Shao & Tu 1995) </ref>. The inherent power of randomly subsampling a data set has been used subsequently in studies upon multilayer perceptron ensembles trained by early stopping (Rasmussen 1996). This approach is directed towards avoidance of overfitting. <p> THE USE OF THE JACK-KNIFE so that no two subsets are exactly identical, although much overlap of data, clearly, will exist between these subsets. Normally the jack-knife is used to obtain an unbiased (or O ( 1 n 2 ) bias-reduced) <ref> (Shao & Tu 1995) </ref> estimate of some function fi (a) and an estimate of the variance of fi (a). For a full treatment of the theory of the jack-knife (Shao & Tu 1995) may be consulted. <p> Normally the jack-knife is used to obtain an unbiased (or O ( 1 n 2 ) bias-reduced) <ref> (Shao & Tu 1995) </ref> estimate of some function fi (a) and an estimate of the variance of fi (a). For a full treatment of the theory of the jack-knife (Shao & Tu 1995) may be consulted. An estimate fi (a j ) based on each subset and fi (a all ) based on the entire data set are first determined. These estimates are based on much the same data and are therefore statistically dependent. <p> Clearly, as the size of the data-set gets larger the amount of data left out each time increases. This is more general than the deleted jackknife <ref> (Shao & Tu 1995) </ref> where a fixed number of points is left out each time. It is proved in the next section that the variance of the pseudovalues fi (a (j) ) is directly proportional to the variance of the estimates fi (a j ). <p> As the bias reduction is O ( 1 n 2 ) <ref> (Shao & Tu 1995) </ref> a chosen committee size of n = 5 ought to reduce bias significantly. This is in fact the size of the committee in the majority of the experiments in this 79 80 CHAPTER 5. QUERYING FOR COST-EFFECTIVENESS work.
Reference: <author> Singh, S. P. </author> <year> (1992), </year> <title> Reinforcement learning with a hierarchy of abstract models, </title> <booktitle> in `Proceedings of the Tenth International Conference on Artificial Intelligence', </booktitle> <publisher> AAAI Press, </publisher> <address> San Jose, CA. 190 BIBLIOGRAPHY Sollich, P. </address> <year> (1994), </year> <title> `Query construction, entropy and generalisation in neural network models', </title> <journal> Physical Review E 49, </journal> <pages> 4637-4651. </pages>
Reference: <author> Sollich, P. </author> <year> (1995), </year> <title> Asking Intelligent Questions the Statistical Mechanics of Query Learning, </title> <type> PhD thesis, </type> <institution> University of Edinburgh. </institution>
Reference-contexts: A smaller training data set gathered by an active learner produces generalisation performance equal to and often better than that of a passive learner using a much larger preselected data set containing many redundant examples <ref> (Sollich & Saad 1995) </ref>. It follows that active learning methods are best recommended in situations where the cost of obtaining training examples far outweighs the additional computational cost of an automated data-gathering strategy. <p> Minimum entropy, conversely, is tantamount to the most orderly state (maximum information). Fedorov (Fedorov 1972) has suggested optimal experiments (queries) based on entropy minimisation. In (MacKay 1992) several objective functions for query selection, based upon information gain (i.e., entropy) have been proposed. This has been studied further by Sollich <ref> (Sollich & Saad 1995, Sollich 1995) </ref> whose analysis has led to the conclusion that minimising an entropy-based objective function in student space (the space of parameters that characterise a learning model) as a querying criterion produces far better generalisation than do minimum entropy queries in teacher space (the space of variables <p> In contrast a globally optimal querying strategy is to choose the next step that is optimal over an entire expected trajectory of a known finite number of queries (DeGroot 1962). Previous theoretical <ref> (Sollich 1995) </ref> and experimental (Cohn 1994) studies upon querying methods seeking to maximise information gain, have not demonstrated any major benefit in the globally-optimal method to justify the additional computational expense involved 2 .
Reference: <author> Sollich, P. & Krogh, A. </author> <year> (1996), </year> <title> Learning with ensembles: How over-fitting can be useful, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 8. </pages>
Reference-contexts: The problem is therefore one of ensuring adequate data querying. In (Krogh & Vedelsby 1995) it has been dealt with by using a much larger-than-necessary number of hidden nodes (i.e., over-fitting). Analysis of the `usefulness of overfitting' for learning with large ensembles has been carried out in <ref> (Sollich & Krogh 1996) </ref> where it has been shown that it is often advantageous for networks within an ensemble to over-fit the training data. A random subsampling method has been proposed as an alternative in (RayChaudhuri & Hamey 1995b, RayChaudhuri & Hamey 1996b). <p> Bellman-equation-type formulations (see Section 1.2.2) will be necessary. Again, increased computational cost 2 The approach of using overfitting is a ready and pragmatic alternative. As discussed briefly in Section 3.3 overfitting (having more weights than `necessary') has been shown to be useful in ensemble methods <ref> (Sollich & Krogh 1996) </ref>. 172 CHAPTER 9. CONCLUSION is a factor that must be justified against improved performance (if any) of such methods. * A v function (see Section 5.2) based on a multimodal value characteristic v (y).
Reference: <author> Sollich, P. & Saad, D. </author> <year> (1995), </year> <title> Learning from queries for maximum information gain in imperfectly learnable problems, </title> <editor> in G. Tesauro, D. Touretzky & T. Leen, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 7', </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: A smaller training data set gathered by an active learner produces generalisation performance equal to and often better than that of a passive learner using a much larger preselected data set containing many redundant examples <ref> (Sollich & Saad 1995) </ref>. It follows that active learning methods are best recommended in situations where the cost of obtaining training examples far outweighs the additional computational cost of an automated data-gathering strategy. <p> Minimum entropy, conversely, is tantamount to the most orderly state (maximum information). Fedorov (Fedorov 1972) has suggested optimal experiments (queries) based on entropy minimisation. In (MacKay 1992) several objective functions for query selection, based upon information gain (i.e., entropy) have been proposed. This has been studied further by Sollich <ref> (Sollich & Saad 1995, Sollich 1995) </ref> whose analysis has led to the conclusion that minimising an entropy-based objective function in student space (the space of parameters that characterise a learning model) as a querying criterion produces far better generalisation than do minimum entropy queries in teacher space (the space of variables <p> In contrast a globally optimal querying strategy is to choose the next step that is optimal over an entire expected trajectory of a known finite number of queries (DeGroot 1962). Previous theoretical <ref> (Sollich 1995) </ref> and experimental (Cohn 1994) studies upon querying methods seeking to maximise information gain, have not demonstrated any major benefit in the globally-optimal method to justify the additional computational expense involved 2 .
Reference: <author> Stensmo, M. & Sejnowski, T. J. </author> <year> (1997), </year> <title> Learning decision theoretc utilities through reinforcement learning, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. </volume> <pages> 9. </pages> <note> to appear. </note>
Reference-contexts: When = 1 then approximately all the states are updated according to the number of times they were visited by the end of a run. In general TD () is more compu-tationally expensive than TD (0) but converges faster (Dayan 1992, Dayan & Sejnowski 1994). Recently in <ref> (Stensmo & Sejnowski 1997) </ref> TD () has been successfully used to determine decision-theoretic utilities of outcomes of tests in medical diagnosis. Convergence properties of AHC-related algorithms have been explored in (Williams & Baird III 1993). A more unified approach to AHC has been 18 CHAPTER 1.
Reference: <author> Storck, J., Hochreiter, S. & Schmidhuber, J. </author> <year> (1995), </year> <title> Reinforcement driven information acquisition in non-deterministic environments, </title> <booktitle> in `Proceedings of ICANN'95', </booktitle> <pages> pp. 159-164. </pages>
Reference-contexts: Kaelbling's `interval estimation' (Kaelbling 1993) algorithm is thus hinged around choosing the highest upper bound of the confidence interval of success probability of an action. Yet another method of directed exploration of Markov environments is the RDIA (Reinforcement Driven Information Acquistion) approach of <ref> (Storck, Hochreiter & Schmidhuber 1995) </ref>. In this method the top-level algorithm uses a combination of information theory and Q-learning concepts to provide reinforcement to a learner seeking to obtain useful data. <p> In this method the top-level algorithm uses a combination of information theory and Q-learning concepts to provide reinforcement to a learner seeking to obtain useful data. It has been demonstrated in <ref> (Storck et al. 1995) </ref> that RDIA (whose basic idea is to take those actions that improved performance in similar situations) significantly outperforms random exploration. Boyan and Moore (Boyan & Moore 1995) have developed an algorithm called ROUT which uses dynamic programming to adaptively construct a training data set.
Reference: <author> Sung, K. K. & Niyogi, P. </author> <year> (1995), </year> <title> Active learning for function approximation, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 7, </volume> <pages> pp. 593-600. </pages>
Reference-contexts: A Bayesian model selection strategy was employed for model complexity regularisation (ensuring that the neural network model had enough hidden nodes). A similar framework has been used in <ref> (Sung & Niyogi 1995) </ref> who studied problems in one-dimensional input space. In (Fukumizu 1996) a method of active querying that also simultaneously trims the learning architecture (multilayer perceptron) is demonstrated.
Reference: <author> Sutton, R. & Barto, A. G. </author> <year> (1997), </year> <title> An Introduction to Reinforcement Learning. </title> <note> to be published. draft available at http://www-anw.cs.umass.edu/People/sutton/the-book.html. </note>
Reference-contexts: For a more thorough coverage (Ram & Leake 1995) may be consulted. 1.2.2 The Reinforcement Learning Approach A common real-world example of learning by interaction is that of an infant's motor development <ref> (Sutton & Barto 1997) </ref>. The infant has a direct senso-rimotor connection to its environment but no explicit teacher. Exercising this connection assists in producing information about cause and effect, action consequences and what should be done to achieve a particular objective. <p> INTRODUCTION literature, essentially refers to all problems where an agent learns behaviour through trial-and-error interactions with its environment (Kaelbling, Littman & Moore 1996). The emphasis of the definition is not upon characterising learning algorithms, but upon the learning problem <ref> (Sutton & Barto 1997) </ref>. Reinforcement learning is a computational approach to the study of learning from interaction. Algorithms that address such reinforcement learning problems generally require a combination of search and memory. <p> In reinforcement learning the concept of a goal (c.f. Goal-Driven learning) is modeled by a scalar signal called the reward. The agent's goal is to maximise the total reward it receives, usually some long-run measure <ref> (Sutton & Barto 1997) </ref>. An action a can be associated with immediate reward r. A reward function R (s; a) maps states to rewards and is usually fixed. Thus R : S fi A ! &lt;. <p> Thus R : S fi A ! &lt;. The reward function R specifies expected instantaneous reward as a function of the current state and action. Rewards may be looked upon as the immediate, intrinsic desirability of states <ref> (Sutton & Barto 1997) </ref>. The longterm desirability of a state is defined as its value V (s). The value of a state takes into account the effect of the current state upon states that follow. <p> Detailed tutorial reviews of reinforcement learning research can be found in (Keerthi & Ravindran 1995) and (Kaelbling et al. 1996). For an understanding of the philosophical and real-world perspective <ref> (Sutton & Barto 1997) </ref> is highly useful. The literature is reasonably extensive and spans more than three decades, although the last decade has attracted rather more contributions from AI researchers. The chart in Table 1.1 contains brief descriptions of important milestones.
Reference: <author> Sutton, R. S. </author> <year> (1988), </year> <title> `Learning to predict by the method of temporal credit differences', </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages> <note> BIBLIOGRAPHY 191 Sutton, </note> <author> R. S. </author> <year> (1990), </year> <title> Integrated architectures for learning, planning and reacting based on approximating dynamic programming, </title> <booktitle> in `Proceedings of the Seventh International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Austin, TX. </address>
Reference-contexts: Often both components operate simultaneously. The critic learns the value function, not by solving a set of linear equations, but by an algorithm called TD (0), first suggested by Sutton <ref> (Sutton 1988) </ref>. It is assumed that the experience tuple (s; a; r; s 0 ) summarises a single transition within an environment, i.e., s is the agent's state before transition, s 0 is the state after transition, a is the choice of action and r is the immediate reward. <p> The reward r is obtained from a real world state transition. If the learning rate is adjusted to decrease slowly while the policy remains fixed, TD (0) eventually converges to optimal value <ref> (Sutton 1988) </ref>. A more general version of TD (0), the TD (), has been developed by Dayan (Dayan 1992, Dayan & Sejnowski 1994).
Reference: <author> Tamburini, F. & Davoli, R. </author> <year> (1994), </year> <title> An algorithmic method to build good training sets for neural-network classifiers, </title> <type> Technical Report UBLCS-94-18, </type> <institution> Laboratory for Computer Science, University of Bologna. </institution>
Reference-contexts: Therefore these are in reality passive learning techniques that use iterative experimental design. Some work using this approach can be found in (Plutowski & White 1993, 26 CHAPTER 1. INTRODUCTION Table 1.2: Querying: Approaches and Contributions 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 27 Plutowski 1994) and in <ref> (Tamburini & Davoli 1994) </ref>. From an existing data set Plutowski selected those exemplars that corresponded to the maximum of the objective function of the differential of the integrated squared bias 5 of a least squares estimator.
Reference: <author> Thrun, S. B. </author> <year> (1992), </year> <title> The role of exploration in learning control, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York. </address>
Reference-contexts: However, such experimentation costs, even if reduced to a few seconds, can be excessive in certain real time applications, such as the control of a moving environment like a robot arm <ref> (Thrun & Moller 1992, Cohn 1994) </ref>. 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 5 1.2 Review of Active Learning Research In the previous section we have seen that active learning consists of the learning agent being able to autonomously decide about what, when and how it should learn. <p> A discounted expected reward criterion is used for estimating payoff. The approach so far has been shown to be particularly useful in balancing exploration levels within problem domains where immediate reward is important. Thrun <ref> (Thrun 1992) </ref> has employed a randomized strategy in his `Boltzmann learning'. The expected reinforcement of each action is first estimated.
Reference: <author> Thrun, S. B. & Moller, K. </author> <year> (1992), </year> <title> Active exploration in dynamic environments, </title> <editor> in J. E. Moody, S. J. Hanson & R. P. Lippman, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 4', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: However, such experimentation costs, even if reduced to a few seconds, can be excessive in certain real time applications, such as the control of a moving environment like a robot arm <ref> (Thrun & Moller 1992, Cohn 1994) </ref>. 1.2. REVIEW OF ACTIVE LEARNING RESEARCH 5 1.2 Review of Active Learning Research In the previous section we have seen that active learning consists of the learning agent being able to autonomously decide about what, when and how it should learn. <p> A discounted expected reward criterion is used for estimating payoff. The approach so far has been shown to be particularly useful in balancing exploration levels within problem domains where immediate reward is important. Thrun <ref> (Thrun 1992) </ref> has employed a randomized strategy in his `Boltzmann learning'. The expected reinforcement of each action is first estimated.
Reference: <author> Tukey, J. </author> <year> (1958), </year> <title> `Bias and confidence in not quite large samples', </title> <journal> Ann. Math. Statist 29, </journal> <volume> 614. </volume>
Reference-contexts: If y all is the hypothesis obtained using the entire data set then according to jack-knifing theory <ref> (Tukey 1958) </ref> one obtains y (r) = y all + (n 1)(y all y r ) (4.1) From the above expression a summation on both sides yields n X y (r) = ny all + (n 1) r=1 or, r=1 n X y r Upon further simplification one derives n X
Reference: <author> Valiant, L. G. </author> <year> (1984), </year> <title> `A theory of the learnable', </title> <journal> Communications of the ACM 27(11), </journal> <pages> 1134-1142. </pages>
Reference-contexts: Some of the earlier machine learning literature contains theoretical work on `membership querying' of an environment (or `oracle') for learning certain classes of functions that cannot be learnt efficiently by random passive training data, e.g., <ref> (Valiant 1984) </ref> and (Angluin 1987, Angluin 1988). In these methods the learner uses heuristical methods to construct queries on the basis of an existing training set but does not perceive the distribution of the entire input space.
Reference: <author> Watkins, C. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> Cambridge University. </institution>
Reference-contexts: Convergence properties of AHC-related algorithms have been explored in (Williams & Baird III 1993). A more unified approach to AHC has been 18 CHAPTER 1. INTRODUCTION taken by Watkins in his `Q-learning' <ref> (Watkins 1989, Watkins & Dayan 1992) </ref> where the adaptive heuristic critic and the reinforcement learner have been merged. A brief description follows. <p> The theorem of Q-learing states that if each action is executed in each state an infinite number of times and is decayed, the Q values will converge to Q fl , yielding an optimal policy <ref> (Watkins 1989) </ref>. In discrete state space neural networks may be used to estimate Q values | either a distinct network may be used for each action or a network with a distinct output may be used for each action.
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992), </year> <title> `Q-learning', </title> <booktitle> Machine Learning 8(3), </booktitle> <pages> 279-292. </pages>

References-found: 92

