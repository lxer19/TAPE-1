URL: http://www.cs.berkeley.edu/~johnw/papers/jnn93dig.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~johnw/publications.html
Root-URL: 
Email: email: johnw@cs.berkeley.edu  
Title: The Design of a Neuro-Microprocessor  
Phone: phone: (510)-643-9434 fax: (510)-642-5775  
Author: John Wawrzynek Evans Hall John Wawrzynek Krste Asanovic and Nelson Morgan 
Address: Berkeley, CA 94704  Berkeley CA  
Affiliation: EECS, Computer Science Division University of California  EECS Department, University of California at Berkeley International Computer Science Institute,  
Note: Corresponding author:  
Abstract: This paper presents the architecture of a neuro-microprocessor. This processor was designed using the results of careful analysis of our set of applications and extensive simulation of moderate-precision arithmetic for back-propagation networks. We present simulated performance results and test-chip results for the processor. This work is an important intermediate step in the development of a connectionist network supercomputer. 
Abstract-found: 1
Intro-found: 1
Reference: [AMW92] <author> K. Asanovic, N. Morgan, and J. Wawrzynek. </author> <title> Using Simulations of Reduced Precision Arithmetic to Design a Neuro-Microprocessor. </title> <journal> Journal of VLSI Signal Processing, </journal> <note> 1992. to appear. </note>
Reference-contexts: An earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme [MBKB92]. We have used the RAP to simulate variable precision arithmetic and guide us in the design of the neuro-microprocessor <ref> [AMW92] </ref>. The RAP system played a critical role in this study, enabling us to experiment 2 with much larger networks than would otherwise be possible. Our study shows that, for the speech recognition training problems we have examined, back-propagation training algorithms only require moderate precision.
Reference: [Ham90] <author> D. Hammerstrom. </author> <title> A VLSI architecture for High-Performance, Low-Cost, On-Chip Learning. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, </booktitle> <pages> pages II-537-543, </pages> <year> 1990. </year>
Reference-contexts: Fabrication of the entire SPERT processor is planned for fall 1992. 5 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions <ref> [Ham90] </ref> and the MA-16 chip from Siemens [RBR + 91]. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer.
Reference: [MB90] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition using Multilayer Perceptrons with Hidden Markov models. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 413-416, </pages> <address> Albuquerque, New Mexico, USA, </address> <year> 1990. </year>
Reference-contexts: Special emphasis has been placed on the support of variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks [RHW86, Wer74]. This class of networks is of interest in the speech recognition task that is the focus of our applications work <ref> [MB90] </ref>. An earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme [MBKB92]. We have used the RAP to simulate variable precision arithmetic and guide us in the design of the neuro-microprocessor [AMW92].
Reference: [MBKB92] <author> N. Morgan, J. Beck, P. Kohn, and J. Bilmes. </author> <title> Neurocomputing on the RAP. </title> <editor> In K. W. Przytula and V. K. Prasanna, editors, </editor> <title> Digital Parallel Implementations of Neural Networks. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year> <note> In Press. </note>
Reference-contexts: This class of networks is of interest in the speech recognition task that is the focus of our applications work [MB90]. An earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme <ref> [MBKB92] </ref>. We have used the RAP to simulate variable precision arithmetic and guide us in the design of the neuro-microprocessor [AMW92]. The RAP system played a critical role in this study, enabling us to experiment 2 with much larger networks than would otherwise be possible.
Reference: [RBR + 91] <author> U. Ramacher, J. Beichter, W. Raab, J. Anlauf, N. Bruls, M. Hachmann, and M. Wes-seling. </author> <title> Design of a 1st Generation Neurocomputer. In VLSI Design of Neural Networks. </title> <publisher> Kluwer Academic, </publisher> <year> 1991. </year>
Reference-contexts: Fabrication of the entire SPERT processor is planned for fall 1992. 5 Related Work Several related efforts are underway to construct programmable digital neurocomputers, most notably the CNAPS chip from Adaptive Solutions [Ham90] and the MA-16 chip from Siemens <ref> [RBR + 91] </ref>. Adaptive Solutions provides a SIMD array with 64 processing elements per chip, in a system with four chips on a board controlled by a common microcode sequencer. As with SPERT, processing elements are similar to general purpose DSPs with reduced precision multipliers.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning Internal Representations by Error Propagation. In Parallel Distributed Processing. </title> <journal> Exploration of the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The architecture is fully programmable, and efficiently executes a wide range of connectionist computations. Special emphasis has been placed on the support of variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks <ref> [RHW86, Wer74] </ref>. This class of networks is of interest in the speech recognition task that is the focus of our applications work [MB90]. An earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme [MBKB92].
Reference: [Wer74] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Harvard University, </institution> <year> 1974. </year> <type> 14 15 16 17 18 19 </type>
Reference-contexts: The architecture is fully programmable, and efficiently executes a wide range of connectionist computations. Special emphasis has been placed on the support of variants of the commonly used backpropagation training algorithm for multi-layer feed-forward networks <ref> [RHW86, Wer74] </ref>. This class of networks is of interest in the speech recognition task that is the focus of our applications work [MB90]. An earlier system, the Ring Array Processor (RAP), was a multiprocessor based on commercial DSPs with a low-latency ring interconnection scheme [MBKB92].
References-found: 7

