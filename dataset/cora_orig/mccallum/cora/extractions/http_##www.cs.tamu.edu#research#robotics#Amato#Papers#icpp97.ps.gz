URL: http://www.cs.tamu.edu/research/robotics/Amato/Papers/icpp97.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/amato/dsmft/publications.html
Root-URL: http://www.cs.tamu.edu
Email: fyuehow,amato,frieseng@cs.tamu.edu  
Title: Hindsight Helps: Deterministic Task Scheduling with Backtracking  
Author: Yueh-O Wang Nancy M. Amato D. K. Friesen 
Address: College Station, Texas 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Date: August 1997  
Note: To appear in the Proc. of the 1997 International Conference on Parallel Processing,  
Abstract: This paper considers the problem of scheduling a set of precedence-related tasks on a nonpreemptive homogeneous message-passing multiprocessor system in order to minimize the makespan, that is, the completion time of the last task relative to start time of the first task. We propose a family of scheduling algorithms, called IPR for immediate predecessor rescheduling, which utilize one level of backtracking. We also develop a unifying framework to facilitate the comparison between our results and the various models and algorithms that have been previously studied. We show, both theoretically and experimentally, that the IPR algorithms outperform previous algorithms in terms of both time complexity and the makespans of the resulting schedules. Moreover, our simulation results indicate that the relative advantage of the IPR algorithms increases as the communication constraint is relaxed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Anger, J. Hwang, and Y. Chow. </author> <title> Scheduling with sufficient loosely coupled processors. </title> <journal> J. of Parallel and Dist. Comp., </journal> <volume> 9 </volume> <pages> 87-92, </pages> <year> 1990. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14]. <p> The correctness of this approach follows 2 Table 2: Summary of Most Relevant Previous Work Algorithm Model Time Makespan ETF [6] M (G0; P 0; C:0) O (n 2 m) (2 1 JLP <ref> [1] </ref> M (G1; P 1; C3) O (n) OP T (I) T [10] M (G1; P 1; C2) O (n) OP T (I) DSC [16] M (G1; P 1; C2) O ((e + n) log n) OP T (I) Table 3: Summary of the IPR Algorithms Algorithm Model Time Makespan IPR <p> None of the other algorithms have been theoretically analyzed for models with C &lt; C2. The simulation was performed for two models M (G1; P 1; C0) and M (G1; P 1; C1). The algorithms we selected to compare with IPR were JLP <ref> [1] </ref>, DSC [16], DCP [8], and MCCP [16]. We chose DSC and JLP as representatives of the cluster and list scheduling algorithms, respectively. DSC was selected since it is the only cluster scheduling algorithm that has been both theoretically and experimentally analyzed. <p> All algorithms were coded in C++ and run on a Unix system. We generated 500 test cases for each model. The number of tasks in each test case was randomly generated in the range <ref> [1; 500] </ref>, and the execution costs and communication delays were randomly generated in the range [1; 100]. The communication constraint C1 was enforced by setting the communication delay to be the remainder after dividing by the sender's execution cost. <p> All algorithms were coded in C++ and run on a Unix system. We generated 500 test cases for each model. The number of tasks in each test case was randomly generated in the range [1; 500], and the execution costs and communication delays were randomly generated in the range <ref> [1; 100] </ref>. The communication constraint C1 was enforced by setting the communication delay to be the remainder after dividing by the sender's execution cost.
Reference: [2] <author> S. Darbha and D. P. Agrawal. SDBS: </author> <title> A task duplication based optimal scheduling algorithm. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <pages> pages 756-763, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [3] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitary target machines. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 138-153, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [4] <author> A. Gerasoulis and T. Yang. </author> <title> A comparison of clustering heuristics for scheduling directed acyclic graphs on multiprocessors. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 276-291, </pages> <year> 1992. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [5] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 17 </volume> <pages> 416-429, </pages> <year> 1969. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [6] <author> J. Hwang, Y. Chow, F. Angers, and C. Lee. </author> <title> Scheduling precedence graphs in systems with interprocessor communication times. </title> <journal> SIAM J. Comput., </journal> <volume> 18(2) </volume> <pages> 244-257, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14]. <p> The correctness of this approach follows 2 Table 2: Summary of Most Relevant Previous Work Algorithm Model Time Makespan ETF <ref> [6] </ref> M (G0; P 0; C:0) O (n 2 m) (2 1 JLP [1] M (G1; P 1; C3) O (n) OP T (I) T [10] M (G1; P 1; C2) O (n) OP T (I) DSC [16] M (G1; P 1; C2) O ((e + n) log n) OP T
Reference: [7] <author> A. A. Khan, C. L. McCreary, and M. S. Jones. </author> <title> A comparison of multiprocessor scheduling heuristics. </title> <booktitle> In Proc. of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 243-250, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [8] <author> Y.-K. Kwok and Ishfaq Ahmad. </author> <title> A static scheduling algorithm using dynamic critical path for assigning parallel algorithms onto multiprocessors. </title> <booktitle> In Proc. of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 155-159, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14]. <p> None of the other algorithms have been theoretically analyzed for models with C &lt; C2. The simulation was performed for two models M (G1; P 1; C0) and M (G1; P 1; C1). The algorithms we selected to compare with IPR were JLP [1], DSC [16], DCP <ref> [8] </ref>, and MCCP [16]. We chose DSC and JLP as representatives of the cluster and list scheduling algorithms, respectively. DSC was selected since it is the only cluster scheduling algorithm that has been both theoretically and experimentally analyzed.
Reference: [9] <author> T. G. Lewis and H. El-Rewini. </author> <title> Parallax: A tool for parallel program scheduling. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 62-72, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: If we further assume that all tasks have identical execution costs, then polynomial time algorithms are known if either there are only two processors in the system, or the precedence relation of the tasks forms a tree <ref> [9] </ref>. Nevertheless, the complexity of the scheduling problem varies according to the constraints we place on the following factors: (i) the relative magnitudes of the execution costs and the communication delays (costs), (ii) the structure of the precedence graph, and (iii) the number of processors in the system. <p> Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [10] <author> D. R. Lopez. </author> <title> Models and Algorithms for Task Allocation in a Parallel Environment. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Texas A&M University, </institution> <year> 1992. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14]. <p> The correctness of this approach follows 2 Table 2: Summary of Most Relevant Previous Work Algorithm Model Time Makespan ETF [6] M (G0; P 0; C:0) O (n 2 m) (2 1 JLP [1] M (G1; P 1; C3) O (n) OP T (I) T <ref> [10] </ref> M (G1; P 1; C2) O (n) OP T (I) DSC [16] M (G1; P 1; C2) O ((e + n) log n) OP T (I) Table 3: Summary of the IPR Algorithms Algorithm Model Time Makespan IPR M (G1,P1,C2) O (n log n) OP T (I) 5 OP T
Reference: [11] <author> C. McCreary and H. Gill. </author> <title> Automatic determination of grain size for efficient parallel processing. </title> <journal> Communication of the ACM, </journal> <volume> 32(9) </volume> <pages> 1073-1078, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [12] <author> V. J. Rayward-Smith. </author> <title> UET scheduling with interproces-sor communications delays. </title> <type> Technical Report SYS-C86-06, </type> <institution> School of Information Systems, University of East Anglia, Norwich, NR4 7TJ, </institution> <year> 1986. </year>
Reference-contexts: This problem is NP-complete, and few polynomial time scheduling algorithms are known even if strong restrictions are placed on the problem. For example, Rayward-Smith <ref> [12] </ref> showed that the problem of optimally scheduling a set of tasks whose precedence relation forms a directed acyclic graph (DAG) on m &gt; 1 processors is NP-complete even when all tasks have unit time execution cost and unit time communication delays. <p> Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [13] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [14] <author> Y. Wang, N. M. Amato, and D. K. Friesen. </author> <title> Hindsight helps: A backtracking scheme for scheduling concurrent tasks. </title> <type> Technical Report 97005, </type> <institution> Department of Computer Science, Texas A&M University, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in <ref> [14] </ref>. <p> The diagram shows the relationships between the models; an arrow from a to b indicates a is more restricted than b. 1 Details omitted here due to space constraints can be found in <ref> [14] </ref>. 2 The IPR algorithms In addition to the makespan of the schedule produced, the time required to compute the schedule is also a critical concern. <p> Using an appropriate data structure, it can be implemented in O (e i + log e i ) time. Details can be found in <ref> [14] </ref>. The running time of IPR is dictated by the time spent in Steps 2 (b) and 2 (c) of the while loop. Recall that e i is the number of immediate predecessors of task i, the task being considered in the current iteration. <p> can be shown that for M (G1; P 1; C), IPR constructs schedules with optimal makespans when C C2, and when C1 C &lt; C2, it constructs schedules that are at most 6 5 OP T (I), that is, makespans at most 6 5 the length of the optimal makespan <ref> [14] </ref>. 2.1 Other versions of the IPR algorithm We have designed and analyzed several variations of the basic IPR algorithm discussed above (see Table 3).
Reference: [15] <author> M. Wu and D. D. Gajski. </author> <title> Hypertool:a programming aid for message-passing systems. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 330-343, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
Reference: [16] <author> T. Yang and A. Gerasoulis. </author> <title> A fast static scheduling algorithm for dags on an unbounded number of processors. </title> <booktitle> In Proc. of Supercomputing'91, </booktitle> <pages> pages 633-642, </pages> <year> 1991. </year>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms [12, 5, 6, 17, 1, 10, 2] or cluster scheduling algorithms <ref> [3, 4, 7, 8, 9, 11, 13, 15, 16] </ref>. Due to space constraints, we do not describe the various approaches here; details can be found in [14]. <p> Table 2: Summary of Most Relevant Previous Work Algorithm Model Time Makespan ETF [6] M (G0; P 0; C:0) O (n 2 m) (2 1 JLP [1] M (G1; P 1; C3) O (n) OP T (I) T [10] M (G1; P 1; C2) O (n) OP T (I) DSC <ref> [16] </ref> M (G1; P 1; C2) O ((e + n) log n) OP T (I) Table 3: Summary of the IPR Algorithms Algorithm Model Time Makespan IPR M (G1,P1,C2) O (n log n) OP T (I) 5 OP T (I) M (G1,P1,C1) O (n) 6 IPR/D M (G0,P1,C2) O (n 2 <p> None of the other algorithms have been theoretically analyzed for models with C &lt; C2. The simulation was performed for two models M (G1; P 1; C0) and M (G1; P 1; C1). The algorithms we selected to compare with IPR were JLP [1], DSC <ref> [16] </ref>, DCP [8], and MCCP [16]. We chose DSC and JLP as representatives of the cluster and list scheduling algorithms, respectively. DSC was selected since it is the only cluster scheduling algorithm that has been both theoretically and experimentally analyzed. <p> The simulation was performed for two models M (G1; P 1; C0) and M (G1; P 1; C1). The algorithms we selected to compare with IPR were JLP [1], DSC <ref> [16] </ref>, DCP [8], and MCCP [16]. We chose DSC and JLP as representatives of the cluster and list scheduling algorithms, respectively. DSC was selected since it is the only cluster scheduling algorithm that has been both theoretically and experimentally analyzed.
Reference: [17] <author> C. Yen, S. S. Tseng, and C.-T. Yang. </author> <title> Scheduling of precedence constrained tasks on multiprocessor systems. </title> <booktitle> In Proc. IEEE First International Conference on Algorithms and Architectures for Parallel Processing, </booktitle> <pages> pages 379-382, </pages> <month> April </month> <year> 1995. </year> <month> 4 </month>
Reference-contexts: Most previous work can be classified as either list scheduling algorithms <ref> [12, 5, 6, 17, 1, 10, 2] </ref> or cluster scheduling algorithms [3, 4, 7, 8, 9, 11, 13, 15, 16]. Due to space constraints, we do not describe the various approaches here; details can be found in [14].
References-found: 17

