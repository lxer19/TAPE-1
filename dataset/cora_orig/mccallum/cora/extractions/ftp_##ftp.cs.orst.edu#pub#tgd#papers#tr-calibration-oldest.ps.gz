URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-calibration-oldest.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/change-history.html
Root-URL: 
Email: chown@cs.orst.edu  tgd@cs.orst.edu  
Title: Learning in the Presence of Prior Knowledge: A Case Study Using Model Calibration  
Author: Eric Chown Thomas G. Dietterich 
Keyword: Learning from prior knowledge, model calibration, gradient descent search, simulated annealing, Powell's method.  
Date: January, 1996.  
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: Computational models of natural systems often contain free parameters that must be set to optimize the predictive accuracy of the models. This process|called calibration|can be viewed as a form of supervised learning in the presence of prior knowledge. In this view, the fixed aspects of the model constitute the prior knowledge, and the goal is to learn correct values for the free parameters. We report on a series of attempts to learn parameter values for a global vegetation model called MAPSS (Mapped Atmosphere-Plant-Soil System) developed by our collaborator, Ron Neilson. Unfortunately, attempts to apply standard machine learning methods|specifically global error functions and gradient descent search|do not work with MAPSS, because the constraints introduced by the structure of the model (the prior knowledge) create a very difficult non-linear optimization problem. Successful calibration of MAPSS required taking a divide-and-conquer approach in which subsets of the parameters were calibrated while others were held constant. This approach was made possible by carefully selecting training sets that exercised only portions of the model and by designing error functions for each part that had desirable properties. The automated calibration tool that we have developed is currently being applied to calibrate MAPSS against a global climate data set. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fahlman, S. E., & Lebiere, C. </author> <year> (1990). </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This produces a high branching factor for any search algorithm. In neural networks, this same phenomenon arises, and Scott Fahlman has termed it "mob behavior" <ref> (Fahlman & Lebiere, 1990) </ref>.
Reference: <author> Ingber, L. </author> <year> (1995). </year> <title> Adaptive simulated annealing [software package]. </title> <type> Tech. rep., </type> <institution> California Institute of Technology, </institution> <note> http://alumni.caltech.edu/~ingber/. </note>
Reference: <author> Kuchler, A. W. </author> <year> (1964). </year> <title> The potential natural vegetation of the conterminous United States. </title> <journal> Special Publication 36, American Geographical Society. </journal>
Reference: <author> Mahadevan, S., & Tadepalli, P. </author> <year> (1994). </year> <title> Quantifying prior determination knowledge using the PAC learning model. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <pages> 69-105. </pages>
Reference: <author> Neilson, R. P. </author> <year> (1996). </year> <title> A model for predicting continental-scale vegetation distribution and water balance. </title> <journal> Ecological Applications, </journal> <volume> 6. </volume>
Reference-contexts: The primary goal of our research is to automate and improve this calibration process by applying ideas and methods from machine learning. 1.2 The MAPSS Vegetation Model We are working with Dr. Ron Neilson to automate the calibration of a global vegetation model called MAPSS <ref> (Mapped Atmosphere-Plant-Soil System Neilson, 1996) </ref>. The goal of MAPSS is to model the feedbacks between the atmosphere and plant ecosystems relevant to global climate change. Existing models of global climate change capture only the behavior of the atmosphere and the oceans.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference: <author> Press, W. H. </author> <year> (1992). </year> <title> Numerical recipes in C : The art of scientific computing, 2nd Edition. </title> <publisher> Cam bridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: However, with these numerically-computed gradients, it is possible to apply gradient descent algorithms to minimize J. The second approach we pursued was to apply Powell's method <ref> (Press, 1992) </ref>. This is a search technique that can be applied when no gradient information is available. Like the conjugate-gradient algorithm, Powell's method is a direction set algorithm that tries to identify good directions along which to minimize J.
Reference: <author> Simard, P., Le Cun, Y., & Denker, J. </author> <year> (1993). </year> <title> Efficient pattern recognition using a new transfor mation distance. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pp. 50-58. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Simard, P., Victorri, B., Le Cun, Y., & Denker, J. </author> <year> (1992). </year> <title> Tangent Prop|A formalism for specifying selected invariances in an adaptive network. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. P. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 895-903. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address> <month> 13 </month>
References-found: 9

