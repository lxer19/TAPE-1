URL: http://www.cs.cmu.edu/~har/proposal.ps.gz
Refering-URL: http://www.cs.cmu.edu/~har/faces.html
Root-URL: 
Title: A Trainable View-Based Object Detection System  
Author: Henry A. Rowley Shumeet Baluja Dean Pomerleau Manuela Veloso Tomaso Poggio, 
Degree: Thesis Proposal  Thesis Committee: Takeo Kanade, Chair  
Affiliation: MIT  
Abstract: Object detection is a fundamental problem in computer vision. Techniques for addressing this problem range from those matching three-dimensional geometric models with images to using two-dimensional view-based representations. My previous work has shown that the latter technique can be effectively implemented using standard pattern recognition techniques (such as clustering or small neural networks). This approach has been successfully used to detect faces and eyes in cluttered backgrounds. In developing a view-based object detector, three main issues arise. First, to what extent should the problem be partitioned into sets of views or sets of features of the object, so that a simple pattern recognition algorithm will provide sufficient accuracy? Second, what type of pattern classifier should be used, and how should it be trained best detect an individual view or feature? Third, how should the outputs from multiple detectors be combined into a single decision about the presence of an object? I propose to study the view-based approach to object detection. This will include experimental studies of how views and features should be selected, and how this selection effects detection performance. Some preliminary results in face and car detection show that partitioning the problem is effective, but more systematic study is needed. Other factors that effect detector accuracy include the classifier architecture itself (initially, I will use neural networks, but other tools are available), and on how the training examples are generated. Finally, I will study how the detections of individual views or features should be combined to give a final detection result. For view partitioning, I have found that simple arbitration strategies work well, while for feature partitioning, other researchers have used graph matching techniques. The system will be tested in at least the face detection domain, and in other domains to illustrate parts of the algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [ Baker and Nayar, 1996 ] <author> S. Baker and S. K. Nayar. </author> <title> A theory of pattern rejection. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: He argued that if the errors made by a detector are independent, then by having a set of networks vote on the result, the number of overall errors will be reduced. <ref> [ Baker and Nayar, 1996 ] </ref> used the converse idea, that of pattern rejectors, for recognition. Each classifier eliminates a set of potential classifications of an example, until only the example's class is left. In training a detector, obtaining a sufficient number of examples is an important problem.
Reference: [ Baluja, 1996 ] <author> Shumeet Baluja. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: One problem with the ORing approach is that each detector will have some (hopefully small) false detection rate, and thus the OR of the network outputs will add their false detections. An alternative, suggested by <ref> [ Baluja, 1996 ] </ref> , is to train a system that, given a window of an image containing an object, decides which viewpoint the object belongs to. This router can then be applied at every pixel location, to determine which viewpoint-specific network should be applied.
Reference: [ Belhumeur and Kriegman, 1996 ] <author> P. N. Belhumeur and D. J. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions? In Computer Vision and Pattern Recogition, page 270, </title> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: Based on experience with the face detector, it is probably not necessary to partition the frontal views of faces into subsets for detection. A potentially useful technique for understanding the space of object appearances under different lighting conditions was presented in <ref> [ Belhumeur and Kriegman, 1996 ] </ref> . This work showed that the shape of the set of images of an object which has a Lambertian reflectance model is a convex cone in image space. <p> One major problem with the current face detector is its sensitivity to lighting conditions. The training examples are faces with good lighting, so training examples without this feature are desirable. Using ideas from <ref> [ Belhumeur and Kriegman, 1996 ] </ref> , it may be possible to synthesize training images of a face under a large variety of lighting conditions, given just three images of that face with different illumination conditions.
Reference: [ Bobick and Pinhanez, 1995 ] <author> A. Bobick and C. Pinhanez. </author> <title> Divide and conquer: Using approxi-mate models to control view-based algorithms. </title> <type> Perceptual Computing Technical Report 357, </type> <institution> MIT, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: The remaining problems are in tracking and motion segmentation, and removing moving artifacts such as shadows. The use of such cues is not restricted only to car detection. For instance, <ref> [ Bobick and Pinhanez, 1995 ] </ref> used contextual information to guide vision algorithms used to control video cameras, in order to frame camera shots in an instructional video. 6 of faces detected correctly, and the number of false detections.
Reference: [ Cortes and Vapnik, 1995 ] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <booktitle> Machine Learning, </booktitle> <address> 20:125, </address> <year> 1995. </year>
Reference-contexts: This and other variations on the network architecture will be evaluated, along with the possibility of using other pattern classification techniques (for instance, the Support Vector Network <ref> [ Cortes and Vapnik, 1995 ] </ref> ). Related to the issue of classifier type is the speed of the classifier.
Reference: [ Dempster et al., 1977 ] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum-likelihood from incom--plete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:138, </volume> <year> 1977. </year>
Reference-contexts: In the face detector described in Section 2, multiple neural networks were trained with the same data, and they implicitly partitioned the problem so that each network ruled out some portion of the non-face images. The EM (Expectation-Maximization) statistical framework might provide a way to explicitly do this partitioning <ref> [ Dempster et al., 1977 ] </ref> . EM is a method for fitting a model to data, some of which may be unknown.
Reference: [ Dubuisson and Jain, 1995 ] <author> Marie-Pierre Dubuisson and Anil K. Jain. </author> <title> Contour extraction of mov-ing objects in complex outdoor scenes. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14:83105, </volume> <year> 1995. </year>
Reference-contexts: Most algorithms in this domain make specific assumptions about the context in which they will be appliedfor instance, that the camera is always pointed at a road, so any moving object will be a car <ref> [ Dubuisson and Jain, 1995, Koller et al., 1993 ] </ref> . The remaining problems are in tracking and motion segmentation, and removing moving artifacts such as shadows. The use of such cues is not restricted only to car detection.
Reference: [ Frankel et al., 1996 ] <author> Charles Frankel, Michael J. Swain, and Vassilis Athitsos. WebSeer: </author> <title> An image search engine for the world wide web. </title> <institution> TR-96-14, University of Chicago, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Finally, the WebSeer project has used this face detector to allow searches for images containing faces on the World Wide Web <ref> [ Frankel et al., 1996 ] </ref> . 2.1 The Detector The detector is only concerned with frontal views of faces, and the face is not being split into sub-features. Thus there is no explicit partitioning of the detection problem into smaller pieces.
Reference: [ Koller et al., 1993 ] <author> D. Koller, J. Weber, and J. Malik. </author> <title> Robust multiple car tracking with occlusion reasoning. </title> <institution> CSD-93-780, University of California, Berkeley, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Most algorithms in this domain make specific assumptions about the context in which they will be appliedfor instance, that the camera is always pointed at a road, so any moving object will be a car <ref> [ Dubuisson and Jain, 1995, Koller et al., 1993 ] </ref> . The remaining problems are in tracking and motion segmentation, and removing moving artifacts such as shadows. The use of such cues is not restricted only to car detection.
Reference: [ Le Cun et al., 1989 ] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. </author> <title> Backpropogation applied to handwritten zip code recognition. </title> <booktitle> Neural Computation, </booktitle> <address> 1:541551, </address> <year> 1989. </year>
Reference-contexts: Although the figure shows a single hidden unit for each subregion of the input, we found experimentally that two or three copies give the best performance. Similar input connection patterns are commonly used in speech and character recognition tasks <ref> [ Waibel et al., 1989, Le Cun et al., 1989 ] </ref> . The network has a single, real-valued output, which indicates whether or not the window contains a face. Examples of output from a single network are shown in Figure 2.
Reference: [ Leung et al., 1995 ] <author> T. K. Leung, M. C. Burl, and P. Perona. </author> <title> Finding faces in cluttered scenes using random labeled graph matching. </title> <booktitle> In Fifth International Conference on Computer Vision, </booktitle> <pages> pages 637644, </pages> <address> Cambridge, Massachusetts, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Third, how should one combine the outputs from multiple detectors into a final decision about the presence of an object? Previous work suggests that graph matching or other geometric constraints can successfully merge individual feature detections <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> , while efficiently merging multiple view detectors requires further research. The problems of object detection and object recognition are closely related. <p> Some of them use matching of hand-coded templates [ Yang and Huang, 1994, Seitz and Lang, 1992 ] , others use geometric models <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> , while others use a view-based approach, treating the whole face as a pattern to be recognized [ Row- ley et al., 1995, Sung and Poggio, 1994, Vaillant et al., 1994, Lin et al., 1996 ] . <p> An approach that has proven useful in the face detection domain is to detect smaller features of the face, and then to match them using a statistical or graph model which accounts for changes in the relative positions of the features <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> . However, 2 Since implementing a real time demonstration of the face detector, this assumption has proven false.
Reference: [ Lin et al., 1996 ] <author> Shang-Hung Lin, S. Y. Kung, and Long-Ji Lin. </author> <title> Face recognition/detection by probabilistic decision-based neural network. </title> <journal> Submitted to IEEE Transactions on Neural Networks, </journal> <year> 1996. </year>
Reference: [ Nayar et al., 1996 ] <author> Shree K. Nayar, Sameer A. Nene, and Hiroshi Murase. </author> <title> Real-time 100 object recognition system. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <pages> pages 12231227, </pages> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Assuming the router is accurate, such a system will have the same number of false detections as a single classifier. The router can be implemented using a neural network, or using a nearest neighbor approach like the ones described in <ref> [ Nayar et al., 1996, Neiberg et al., 1996 ] </ref> . Another challenge is how to merge the detection of sub-features into the detection of the entire object. <p> One useful approach has been to compute eigenfeatures from the set of images of the object under different poses, thus reducing the dimensionality of the space so that a nearest- neighbor search can be effective <ref> [ Nayar et al., 1996, Neiberg et al., 1996 ] </ref> . These techniques have been applied to object and pose recognition rather than object detection; in many cases, the object is already extracted from the background.
Reference: [ Neiberg et al., 1996 ] <author> Leonard Neiberg, David Casasent, Robert Fontana, and Jeffrey E. </author> <title> Cade. Feature space trajectory neural net classifier: 8-class distortion-invarient tests. </title> <booktitle> In SPIE Applications and Science of Artificial Neural Networks, </booktitle> <volume> volume 2760, </volume> <pages> pages 540555, </pages> <address> Orlando, FL, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Assuming the router is accurate, such a system will have the same number of false detections as a single classifier. The router can be implemented using a neural network, or using a nearest neighbor approach like the ones described in <ref> [ Nayar et al., 1996, Neiberg et al., 1996 ] </ref> . Another challenge is how to merge the detection of sub-features into the detection of the entire object. <p> One useful approach has been to compute eigenfeatures from the set of images of the object under different poses, thus reducing the dimensionality of the space so that a nearest- neighbor search can be effective <ref> [ Nayar et al., 1996, Neiberg et al., 1996 ] </ref> . These techniques have been applied to object and pose recognition rather than object detection; in many cases, the object is already extracted from the background.
Reference: [ Pomerleau, 1992 ] <author> Dean Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <note> Feburary 1992. Available as CS Technical Report CMUCS-92-115. </note>
Reference-contexts: One commonly used technique is that of virtual views, in which new example images are created from real images. In my work, this has taken the form of randomly rotating, translating, and scaling example images by small amounts. <ref> [ Pomerleau, 1992 ] </ref> made extensive use of this in training a neural network for autonomous driving. The network learns from watching an experienced driver staying on the road, but needs examples of what to do should the vehicle start to leave the road.
Reference: [ Rowley et al., 1995 ] <author> Henry A. Rowley, Shumeet Baluja, and Takeo Kanade. </author> <title> Human face detec-tion in visual scenes. </title> <institution> CMU-CS-95-158R, Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year> <note> Also available at http://www.cs.cmu.edu/har/faces.html. </note>
Reference-contexts: This section briefly presents my face detection system (from <ref> [ Rowley et al., 1995 ] </ref> ), which 1 locates frontal, upright faces at any location or scale in grayscale images. It is a view-based tech-nique, implemented using neural networks.
Reference: [ Sajda et al., 1996 ] <author> Paul Sajda, Clay D. Spence, and John C. Pearson. </author> <title> Learning image context for improced computer-aided diagnosis. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <pages> pages 1375 1380, </pages> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The remaining differences might be due to the relatively smaller training set used for the tire detector. A rather different network architecture, which has shown good results for the tire detection domain, is based on the hierarchical neural-network model proposed in <ref> [ Sajda et al., 1996 ] </ref> . The idea is that some portion of the tire detection task can be solved easily using a lower resolution image, and the remaining distinctions can be made with a separate (and therefore simpler) decision process at a higher resolution.
Reference: [ Sato, 1996 ] <author> Shin'ichi Sato. </author> <type> Personal communication, </type> <year> 1996. </year> <month> 17 </month>
Reference-contexts: The first two are part of the Informedia project at CMU: using faces as one feature for selecting important frames in a video sequence [ Smith and Kanade, 1996 ] , and using located faces which are automatically matched with names news broadcasts <ref> [ Sato, 1996 ] </ref> .
Reference: [ Seitz and Lang, 1992 ] <author> P. Seitz and G. K. Lang. </author> <title> Pattern matching and object recognition employ--ing local orientation and hierarchical spatial feature matching. </title> <booktitle> In First Swiss Symposium on Pattern Recognition and Computer Vision, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Some of them use matching of hand-coded templates <ref> [ Yang and Huang, 1994, Seitz and Lang, 1992 ] </ref> , others use geometric models [ Leung et al., 1995, Yow and Cipolla, 1996 ] , while others use a view-based approach, treating the whole face as a pattern to be recognized [ Row- ley et al., 1995, Sung and Poggio,
Reference: [ Smith and Kanade, 1996 ] <author> M. Smith and T. Kanade. </author> <title> Skimming for quick browsing based on audio and image characterization. </title> <institution> CMU-CS-96-186R, Carnegie Mellon University, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: The system described in this section has already been incorporated into three projects. The first two are part of the Informedia project at CMU: using faces as one feature for selecting important frames in a video sequence <ref> [ Smith and Kanade, 1996 ] </ref> , and using located faces which are automatically matched with names news broadcasts [ Sato, 1996 ] . <p> There are a number of other objects which might be useful to detect, and might help define parts of the detection system. For instance, text has been detected by searching for high horizontal spatial frequencies in the image <ref> [ Smith and Kanade, 1996 ] </ref> . Such features should be straightforward for a neural network based detector to learn. Detection of fruit is an application important in the automated harvesting industry, and would provide an opportunity to explore the use of color in object detection.
Reference: [ Spence et al., 1996 ] <author> Clay Spence, John C. Pearson, and Paul Sajda. </author> <title> Learning hierarchical rep-resentations of objects. </title> <booktitle> In ARPA Image Understanding Workshop, </booktitle> <pages> pages 14151427, </pages> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: With this information, it should be straightforward to determine how the views of the object should be partitioned. A related problem is whether smaller features of an object should be used for detection rather than the whole object, and if so how these sub-features should be selected. <ref> [ Spence et al., 1996 ] </ref> has some work on automatically making this selection, but it has not been demonstrated in a real domain. Some feature selection choices may be obviousin the face detection domain, for example, the eyes, nose, and mouth are clearly useful. <p> Also, these systems have no explicit model of variation other than that caused by changes in object pose. An interesting question is whether sub-features for a parts-based detector can be selected completely automatically. Some preliminary ideas presented in <ref> [ Spence et al., 1996 ] </ref> sound promising, although they have not yet been completely implemented or applied to real domains. In recent work, [ Viola, 1996 ] has developed a mechanism to find good sub-features of a set of example images.
Reference: [ Sung and Poggio, 1994 ] <author> Kah-Kay Sung and Tomaso Poggio. </author> <title> Example-based learning for viewbased human face detection. </title> <journal> A.I. </journal> <note> Memo 1521, CBCL Paper 112, </note> <institution> MIT, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: To detect faces larger than the window size, the input image is repeatedly reduced in size (by subsampling), and the filter is applied at each size, as shown in The window is then preprocessed, using a method developed for another face detection system <ref> [ Sung and Poggio, 1994 ] </ref> . The preprocessing first attempts to equalize the intensity values in across the window. We fit a function which varies linearly across the window to the intensity values. <p> However, collecting a representative set of non-faces is difficult. Instead of collecting the images before training is started, the images are collected during training, in the following manner (a similar idea was used in <ref> [ Sung and Poggio, 1994 ] </ref> , although the details of the implementation are probably quite different): 1. Create an initial set of non-face images by generating 1000 images with random pixel inten <br>- sities. Apply the preprocessing steps to each of these images. 2. <p> Related to the issue of classifier type is the speed of the classifier. For example, it is much faster to apply a linear classifier to an image than a neural network, but a linear classifier is not as flexible. <ref> [ Sung and Poggio, 1994 ] </ref> used a linear filter as a preprocessing step, to eliminate a large number of candidate faces before applying the more accurate (and more time consuming) classifier.
Reference: [ Sung, 1996 ] <author> Kah-Kay Sung. </author> <title> Learning and Example Selection for Object nad Pattern Detection. </title> <type> PhD thesis, </type> <institution> MIT AI Lab, </institution> <month> January </month> <year> 1996. </year> <note> Available as AI Technical Report 1572. </note>
Reference-contexts: and may lead to a way to directly compensate for lighting variation as a preprocessing step. 4.4 Background Variation In his thesis, Sung suggested that with current pattern recognition techniques, the view-based approach to object detection is only applicable for objects that have highly predictable image bound <p>- 11 aries <ref> [ Sung, 1996 ] </ref> . When an object has a predictable boundary, it is possible to extract a window which contains only pixels within the boundary, and to ignore the background. <p> In recent work, [ Viola, 1996 ] has developed a mechanism to find good sub-features of a set of example images. This technique was applied to select features in oriented energy images, but may have applicability in intensity images as well. <ref> [ Sung, 1996 ] </ref> provided some formalization of how a set of identically trained detectors can be used together to improve accuracy.
Reference: [ Umezaki, 1995 ] <author> Tazio Umezaki. </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: A practical concern is getting the detector to run quickly. I have performed some preliminary experiments with the technique used by Umezaki for car license plate detection <ref> [ Umezaki, 1995 ] </ref> . Instead of evaluating windows of the image sampled at each pixel location, the classifier is trained to have some invariance to translation, and the window is moved in larger steps over the image.
Reference: [ Vaillant et al., 1994 ] <author> R. Vaillant, C. Monrocq, and Y. Le Cun. </author> <title> Original approach for the localisation of objects in images. </title> <booktitle> IEE Proceedings on Vision, Image, and Signal Processing, </booktitle> <volume> 141(4), </volume> <month> August </month> <year> 1994. </year>
Reference: [ Vetter et al., 1992 ] <author> T. Vetter, T. Poggio, and H. Bulthoff. </author> <title> 3D object recognition: Symmetry and virtual views. </title> <journal> A.I. </journal> <volume> Memo 1409, </volume> <publisher> MIT, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: The network learns from watching an experienced driver staying on the road, but needs examples of what to do should the vehicle start to leave the road. Examples for this latter case are generated synthetically. The work of <ref> [ Vetter et al., 1992 ] </ref> suggests that for bilaterally symmetric objects (such as faces and cars), a large number of views of the object can be generated from a single view, given information about which points in the image are symmetric points on the object.
Reference: [ Viola, 1996 ] <author> Paul A. Viola. </author> <title> Complex feature recognition: A bayesian approach for learning to recognize objects. </title> <journal> A.I. </journal> <volume> Memo No. 1591, </volume> <publisher> MIT, </publisher> <month> November </month> <year> 1996. </year>
Reference-contexts: An interesting question is whether sub-features for a parts-based detector can be selected completely automatically. Some preliminary ideas presented in [ Spence et al., 1996 ] sound promising, although they have not yet been completely implemented or applied to real domains. In recent work, <ref> [ Viola, 1996 ] </ref> has developed a mechanism to find good sub-features of a set of example images.
Reference: [ Waibel et al., 1989 ] <author> Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 393404, </pages> <year> 1989. </year>
Reference-contexts: Although the figure shows a single hidden unit for each subregion of the input, we found experimentally that two or three copies give the best performance. Similar input connection patterns are commonly used in speech and character recognition tasks <ref> [ Waibel et al., 1989, Le Cun et al., 1989 ] </ref> . The network has a single, real-valued output, which indicates whether or not the window contains a face. Examples of output from a single network are shown in Figure 2.
Reference: [ Yang and Huang, 1994 ] <author> Gaungzheng Yang and Thomas S. Huang. </author> <title> Human face detection in a complex background. </title> <journal> Pattern Recognition, </journal> <volume> 27(1):5363, </volume> <year> 1994. </year>
Reference-contexts: Some of them use matching of hand-coded templates <ref> [ Yang and Huang, 1994, Seitz and Lang, 1992 ] </ref> , others use geometric models [ Leung et al., 1995, Yow and Cipolla, 1996 ] , while others use a view-based approach, treating the whole face as a pattern to be recognized [ Row- ley et al., 1995, Sung and Poggio,
Reference: [ Yow and Cipolla, 1996 ] <author> Kin Choong Yow and Roberto Cipolla. </author> <title> Feature-based human face detec-tion. </title> <type> CUED/F-INFENG/TR 249, </type> <institution> Department of Engineering, University of Cambridge, </institution> <address> England, </address> <year> 1996. </year> <month> 18 </month>
Reference-contexts: Third, how should one combine the outputs from multiple detectors into a final decision about the presence of an object? Previous work suggests that graph matching or other geometric constraints can successfully merge individual feature detections <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> , while efficiently merging multiple view detectors requires further research. The problems of object detection and object recognition are closely related. <p> Some of them use matching of hand-coded templates [ Yang and Huang, 1994, Seitz and Lang, 1992 ] , others use geometric models <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> , while others use a view-based approach, treating the whole face as a pattern to be recognized [ Row- ley et al., 1995, Sung and Poggio, 1994, Vaillant et al., 1994, Lin et al., 1996 ] . <p> An approach that has proven useful in the face detection domain is to detect smaller features of the face, and then to match them using a statistical or graph model which accounts for changes in the relative positions of the features <ref> [ Leung et al., 1995, Yow and Cipolla, 1996 ] </ref> . However, 2 Since implementing a real time demonstration of the face detector, this assumption has proven false. <p> Some feature selection choices may be obviousin the face detection domain, for example, the eyes, nose, and mouth are clearly useful. In fact, there is evidence that such simple features as corners may be sufficient, given intelligent methods for merging detections of those features <ref> [ Yow and Cipolla, 1996 ] </ref> . In other domains, such as car detection, useful features may not be so obvious. Tires seem to be the most uniform features, but they are not visible from all viewpoints, and so therefore are not sufficient by themselves.
References-found: 30

