URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/umsi-98-118.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/
Root-URL: http://www.cs.umn.edu
Title: BILUTM: A Domain-Based Multi-Level Block ILUT Preconditioner for General Sparse Matrices  
Author: Yousef Saad and Jun Zhang 
Keyword: Key words: Incomplete LU factorization, ILUT, multi-level ILU preconditioner, Krylov subspace methods, multi-elimination ILU factorization.  
Note: AMS subject classifications: 65F10, 65N06.  
Date: June 24, 1998  
Address: 4-192 EE/CS Building, 200 Union Street S.E., Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering, University of Minnesota,  
Abstract: This paper describes a domain-based multi-level block ILU preconditioner (BILUTM) for solving general sparse linear systems. This preconditioner combines a high accuracy incomplete LU factorization with an algebraic multi-level recursive reduction. Thus, in the first level the matrix is permuted into a block form using (block) independent set ordering and an ILUT factorization for the reordered matrix is performed. The reduced system is the approximate Schur complement associated with the partitioning and it is obtained implicitly as a by-product of the partial ILUT factorization with respect to the complement of the independent set. The incomplete factorization process is repeated with the reduced systems recursively. The last reduced system is factored approximately using ILUT again. The successive reduced systems are not stored. This implementation is efficient in controlling the fill-in elements during the multi-level block ILU factorization, especially when large size blocks are used in domain decomposition type implementations. Numerical experiments are used to show the robustness and efficiency of the proposed technique for solving some difficult problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. C. Anderson and Y. Saad. </author> <title> Solving sparse triangular systems on parallel computers. </title> <journal> Internat. J. High Speed Comput., </journal> <volume> 1 </volume> <pages> 73-96, </pages> <year> 1989. </year>
Reference: [2] <author> O. Axelsson and M. Neytcheva. </author> <title> Algebraic multilevel iteration method for Stieltjes matrices. </title> <journal> Numer. Linear Algebra Appl., </journal> <volume> 1(3) </volume> <pages> 216-236, </pages> <year> 1994. </year> <month> 16 </month>
Reference: [3] <author> O. Axelsson and P. S. Vassilevski. </author> <title> A survey of multilevel preconditioned iterative methods. </title> <journal> BIT, </journal> <volume> 29(4) </volume> <pages> 769-793, </pages> <year> 1989. </year>
Reference: [4] <author> O. Axelsson and P. S. Vassilevski. </author> <title> Algebraic multilevel preconditioning methods. II. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 27(6) </volume> <pages> 1569-1590, </pages> <year> 1990. </year>
Reference: [5] <author> V. A. Bandy. </author> <title> Black Box Multigrid for Convection-Diffusion Equations on Advanced Computers. </title> <type> PhD thesis, </type> <institution> University of Colorado at Denver, </institution> <address> Denver, CO, </address> <year> 1996. </year>
Reference: [6] <author> R. E. Bank and C. Wagner. </author> <title> Multilevel ILU decomposition. </title> <type> Technical report, </type> <institution> Department of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1997. </year>
Reference: [7] <author> R. E. Bank and J. Xu. </author> <title> The hierarchical basis multigrid method and incomplete LU decomposition. </title> <editor> In D. Keyes and J. Xu, editors, </editor> <booktitle> Seventh International Symposium on Domain Decomposition Methods for Partial Differential Equations, </booktitle> <pages> pages 163-173, </pages> <address> Providence, RI, 1994. </address> <publisher> AMS. </publisher>
Reference: [8] <author> M. Benzi and M. Tuma. </author> <title> A sparse approximate inverse preconditioner for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 19(3) </volume> <pages> 968-994, </pages> <year> 1998. </year> <month> 17 </month>
Reference: [9] <author> E. F. F. Botta, K. Dekker, Y. Notay, A. van der Ploeg, C. Vuik, F. W. Wubs, and P. M. de Zeeuw. </author> <title> How fast the laplace equation was solved in 1995. </title> <journal> Appl. Numer. Math., </journal> <volume> 32 </volume> <pages> 439-455, </pages> <year> 1997. </year>
Reference: [10] <author> E. F. F. Botta, A. van der Ploeg, and F. W. Wubs. </author> <title> A fast linear system solver for large unstructured problems on a shared-memory computer. </title> <editor> In O. Axelsson and B. Polman, editors, </editor> <booktitle> Proceedings of the Conference on Algebraic Multilevel Methods with Applications, </booktitle> <pages> pages 105-116, </pages> <year> 1996. </year>
Reference-contexts: Implementations of grid-based multi-level methods on parallel and vector computers can be found in [39] (for structured matrices), those of domain-based (two-level) methods in [51, 52]. Those and other implementations on shared-memory machines <ref> [10] </ref> demonstrate the advantage of the inherent parallelism of the multi-level preconditioning methods. On the other hand, implementing multi-level preconditioning methods on distributed-memory machines requires the consideration of cost trade-off between communications and computations. It is obviously not advantageous to have too small blocks and too many levels.
Reference: [11] <author> E. F. F. Botta and F. W. Wubs. MRILU: </author> <title> an effective algebraic multi-level ILU-preconditioner for sparse matrices. </title> <note> SIAM J. Matrix Anal. Appl. to appear. </note>
Reference: [12] <author> D. Braess. </author> <title> Towards algebraic multigrid for elliptic problems of second order. </title> <journal> Computing, </journal> <volume> 55(4) </volume> <pages> 379-393, </pages> <year> 1995. </year>
Reference: [13] <author> W. L. Briggs. </author> <title> A Multigrid Tutorial. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: The preconditioned iteration process is reminiscent of a multigrid V-cycle algorithm <ref> [13] </ref>, see Figure 3. A Krylov subspace iteration is performed on the finest level acting as a smoother, the residual is then transfered level-by-level to the coarsest level, where one sweep of ILUT is used to yield an approximate solution.
Reference: [14] <author> T. F. Chan, S. Go, and J. Zou. </author> <title> Multilevel domain decomposition and multigrid methods for unstructured meshes: algorithms and theory. </title> <type> Technical Report CAM 95-24, </type> <institution> Department of Mathematics, UCLA, </institution> <address> Los Angeles, CA, </address> <year> 1995. </year>
Reference: [15] <author> T. F. Chan, W. P. Tang, and W. L. Wan. </author> <title> Wavelet sparse approximate inverse preconditioners. </title> <journal> BIT, </journal> <volume> 37(3) </volume> <pages> 644-660, </pages> <year> 1997. </year>
Reference: [16] <author> Q. S. Chang, Y. S. Wong, and L. Z. Feng. </author> <title> New interpolation formulas of using geometric assumptions in the algebraic multigrid method. </title> <journal> Appl. Math. Comput., </journal> <volume> 50(2-3):223-254, </volume> <year> 1992. </year>
Reference: [17] <author> Q. S. Chang, Y. S. Wong, and H. Q. Fu. </author> <title> On the algebraic multigrid method. </title> <journal> J. Comput. Phys., </journal> <volume> 125 </volume> <pages> 279-292, </pages> <year> 1996. </year>
Reference: [18] <author> A. Chapman, Y. Saad, and L. Wigton. </author> <title> High-order ILU preconditioners for CFD problems. </title> <type> Technical Report UMSI 96/14, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year>
Reference-contexts: WIGTO966 Matrix. The WIGTO966 matrix 4 has 3; 864 unknowns and 238; 252 nonzeros. It comes from an Euler equation model and was supplied by L. Wigton from Boeing. It is solvable by ILUT with large values of p <ref> [18] </ref>. This matrix was also used to compare BILUM with ILUT in [52] and to test point and block preconditioning techniques in [19, 21]. BILUM (with GMRES (10)) was shown to be 6 times faster than ILUT with only one-third of the memory required by ILUT [52]. <p> The four BARTH matrices are described in Table 8. Note that in order for ILUT and BILUTM to work properly, zero diagonals are added. The BARTH matrices have been used as test matrices for other ILU type techniques in <ref> [18] </ref>, but none of them have been solved by enhanced BILUM techniques [55], partly because of the prohibitive computation and memory costs associated with the use of very large size blocks (on the given computer).
Reference: [19] <author> E. Chow and M. A. Heroux. </author> <title> An object-oriented framework for block preconditioning. </title> <journal> ACM Trans. Math. </journal> <note> Softr., 1998. to appear. </note>
Reference-contexts: It comes from an Euler equation model and was supplied by L. Wigton from Boeing. It is solvable by ILUT with large values of p [18]. This matrix was also used to compare BILUM with ILUT in [52] and to test point and block preconditioning techniques in <ref> [19, 21] </ref>. BILUM (with GMRES (10)) was shown to be 6 times faster than ILUT with only one-third of the memory required by ILUT [52].
Reference: [20] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse techniques for block-partitioned matrices. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18 </volume> <pages> 1657-1675, </pages> <year> 1997. </year>
Reference-contexts: 2pm L = p (2 ff=0 L1 X r ff ): (10) Since the nodes of all independent sets and the last reduced system constitute the order of the matrix, we have L X m ff = n: (11) 1 For this, we may employ a sparse approximate inverse technique <ref> [20] </ref> or a multicoloring strategy [50] to solve the last reduced system. 9 Note that r ff = m ff+1 + r ff+1 for 0 ff &lt; L 1, and r L = 0.
Reference: [21] <author> E. Chow and Y. Saad. </author> <title> Experimental study of ILU preconditioners for indefinite matrices. </title> <journal> J. Comput. Appl. Math., </journal> <volume> 86(2) </volume> <pages> 387-414, </pages> <year> 1997. </year>
Reference-contexts: It comes from an Euler equation model and was supplied by L. Wigton from Boeing. It is solvable by ILUT with large values of p [18]. This matrix was also used to compare BILUM with ILUT in [52] and to test point and block preconditioning techniques in <ref> [19, 21] </ref>. BILUM (with GMRES (10)) was shown to be 6 times faster than ILUT with only one-third of the memory required by ILUT [52].
Reference: [22] <author> E. Chow and Y. Saad. </author> <title> Approximate inverse preconditioners via sparse-sparse iterations. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 19(3) </volume> <pages> 995-1023, </pages> <year> 1998. </year>
Reference: [23] <author> T. Davis. </author> <title> University of Florida sparse matrix collection, http://www.cise.ufl.edu/~davis/sparse. NA Digest, </title> <type> 97(23), </type> <month> June 7, </month> <year> 1997. </year>
Reference-contexts: Left: different block size. Right: different dropping threshold t . left part of Figure 5 shows that BILUTM with block size 100 gave best results. Larger and smaller block 3 The RAEFSKY matrices are available online from the University of Florida sparse matrix collection <ref> [23] </ref> at http://www.cise.ufl.edu/~davis/sparse. 13 sizes resulted in deterioration of convergence. The right part of Figure 5 shows that t = 10 4 was the best among the three values tested for this parameter. <p> Recall that there is a big difference between one level of reduction and no 4 The WIGTO966 matrix is available from the authors. 5 The OLAFU matrix is available online from the University of Florida sparse matrix collection <ref> [23] </ref> at http://www.cise.ufl.edu/~davis/sparse. 14 OLAFU matrix. reduction at all, since Figure 6 shows that BILUTM without reduction, (actually equivalent to ILUT) failed to converge. matrix. Left: different block size. Right: different level of reductions. BARTH Matrices. The BARTH matrices 6 were supplied by T. Barth of NASA Ames.
Reference: [24] <author> E. F. D'Azevedo, F. A. Forsyth, and W. P. Tang. </author> <title> Ordering methods for preconditioned conjugate gradient methods applied to unstructured grid problems. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 944-961, </pages> <year> 1992. </year>
Reference: [25] <author> E. F. D'Azevedo, F. A. Forsyth, and W. P. Tang. </author> <title> Towards a cost effective ILU preconditioner with high level fill. </title> <journal> BIT, </journal> <volume> 31 </volume> <pages> 442-463, </pages> <year> 1992. </year>
Reference: [26] <author> P. M. de Zeeuw. </author> <title> Matrix-dependent prolongations and restrictions in a blackbox multigrid solver. </title> <journal> J. Comput. Appl. Math., </journal> <volume> 33 </volume> <pages> 1-25, </pages> <year> 1990. </year>
Reference: [27] <author> J. E. Dendy, Jr. </author> <title> Black box multigrid. </title> <journal> J. Comput. Phys., </journal> <volume> 48(3) </volume> <pages> 366-386, </pages> <year> 1982. </year>
Reference: [28] <author> I. S. Duff and G. A. Meurant. </author> <title> The effect of reordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference: [29] <author> L. C. Dutto. </author> <title> The effect of reordering on the preconditioned GMRES algorithm for solving the compressible Navier-Stokes equations. </title> <journal> Internat. J. Numer. Methods Engrg., </journal> <volume> 36(3) </volume> <pages> 457-497, </pages> <year> 1993. </year>
Reference: [30] <author> H. C. Elman. </author> <title> Approximate Schur complement preconditioners on serial and parallel computers. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(3) </volume> <pages> 581-605, </pages> <year> 1989. </year>
Reference: [31] <author> H. C. Elman and E. Agron. </author> <title> Ordering techniques for the preconditioned conjugate gradient method on parallel computers. </title> <journal> Comput. Phys. Commun., </journal> <volume> 53 </volume> <pages> 253-269, </pages> <year> 1989. </year>
Reference: [32] <author> K. Gallivan, A. Sameh, and Z. Zlatev. </author> <title> A parallel hybrid sparse linear system solver. </title> <journal> Comput. Systems Engrg., </journal> <volume> 1 </volume> <pages> 183-195, </pages> <year> 1990. </year>
Reference: [33] <author> J. A. George and J. W. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference: [34] <author> N. I. M. Gould and J. A. Scott. </author> <title> Sparse approximate-inverse preconditioners using norm-minimization techniques. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 19(2) </volume> <pages> 605-625, </pages> <year> 1998. </year>
Reference: [35] <author> M. Grote and T. Huckle. </author> <title> Parallel preconditioning with sparse approximate inverses. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 18 </volume> <pages> 838-853, </pages> <year> 1998. </year>
Reference: [36] <author> M. T. Jones and P. E. Plassman. </author> <title> A parallel graph coloring heuristic. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14(3) </volume> <pages> 654-669, </pages> <year> 1993. </year>
Reference-contexts: The construction of the lower part factorization is fully parallelizable relative to individual rows, as processing each row only needs information from (access to) the upper part. In addition, parallel algorithms for finding independent sets are available <ref> [36, 37] </ref>. 4 Multi-Level Block ILUT The multi-level block ILUT preconditioner (BILUTM) is based on the restricted ILUT Algorithm 3.4. On each level ff, an incomplete LU factorization is performed and an approximate reduced system A ff+1 7 is formed as in Algorithm 3.4.
Reference: [37] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM J. Comput., </journal> <volume> 4 </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: The construction of the lower part factorization is fully parallelizable relative to individual rows, as processing each row only needs information from (access to) the upper part. In addition, parallel algorithms for finding independent sets are available <ref> [36, 37] </ref>. 4 Multi-Level Block ILUT The multi-level block ILUT preconditioner (BILUTM) is based on the restricted ILUT Algorithm 3.4. On each level ff, an incomplete LU factorization is performed and an approximate reduced system A ff+1 7 is formed as in Algorithm 3.4.
Reference: [38] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comput., </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference: [39] <author> M. Neytcheva, A. Padiy, M. Mellaard, K. Georgiev, and O. Axelsson. </author> <title> Scalable and optimal iterative solvers for linear and nonlinear problems. </title> <type> Technical Report MRI 9613, </type> <institution> Mathematical Research Institute, University of Nijmegen, </institution> <address> The Netherlands, </address> <year> 1996. </year>
Reference-contexts: Implementations of grid-based multi-level methods on parallel and vector computers can be found in <ref> [39] </ref> (for structured matrices), those of domain-based (two-level) methods in [51, 52]. Those and other implementations on shared-memory machines [10] demonstrate the advantage of the inherent parallelism of the multi-level preconditioning methods.
Reference: [40] <author> Y. Notay and Z. Ould Amar. </author> <title> Incomplete factorization preconditioning may lead to multigrid like speed of convergence. </title> <editor> In A. S. Alekseev and N. S. Bakhvalov, editors, </editor> <booktitle> Advanced Mathematics: Computation and Applications, </booktitle> <pages> pages 435-446, </pages> <address> Novosibirsk, Russia, 1996. </address> <publisher> NCC Publisher. </publisher>
Reference: [41] <author> Y. Notay and Z. Ould Amar. </author> <title> A nearly optimal preconditioning based on recursive red-black orderings. </title> <journal> Numer. Linear Algebra Appl., </journal> <volume> 4 </volume> <pages> 369-391, </pages> <year> 1997. </year>
Reference: [42] <author> O. Orterby and Z. Zlatev. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference: [43] <author> A. A. Reusken. </author> <title> Multigrid with matrix-dependent transfer operators for convection-diffusion problems. </title> <editor> In P. W. Hemker and P. Wesseling, editors, </editor> <title> Multigrid Method IV, </title> <booktitle> Proceedings of Fourth European Multigrid Conference, </booktitle> <pages> pages 269-280, </pages> <address> Basel, 1994. </address> <publisher> Brikhauser Verlag. </publisher>
Reference: [44] <author> A. A. Reusken. </author> <title> Approximate cyclic reduction preconditioning. </title> <type> Technical Report RANA 97-02, </type> <institution> Department of Mathematics and Computing Science, Eindhoven University of Technology, The Nether-lands, </institution> <year> 1997. </year>
Reference: [45] <author> J. W. Ruge and K. Stuben. </author> <title> Algebraic multigrid. </title> <editor> In S. McCormick, editor, </editor> <title> Multigrid Methods, Frontiers in Appl. </title> <journal> Math., </journal> <volume> chapter 4, </volume> <pages> pages 73-130. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference: [46] <author> Y. Saad. </author> <title> Krylov subspace methods on supercomputers. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1200-1232, </pages> <year> 1989. </year>
Reference: [47] <author> Y. Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <editor> In G. Golub, M. Luskin, and A. Greenbaum, editors, </editor> <booktitle> Recent Advances in Iterative Methods, volume 60 of IMA Volumes in Mathematics and Its Applications, </booktitle> <pages> pages 165-199. </pages> <publisher> Springer Verlag, </publisher> <address> New York, NY, </address> <year> 1994. </year> <month> 19 </month>
Reference: [48] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU preconditioner. </title> <journal> Numer. Linear Algebra Appl., </journal> <volume> 1(4) </volume> <pages> 387-402, </pages> <year> 1994. </year>
Reference-contexts: In other words, BILUM can be viewed as a naturally defined algebraic multigrid technique. 3 Gaussian Elimination and ILUT ILUT is a high-order (high accuracy) preconditioner based on Incomplete LU factorization. It uses a dual dropping strategy to control the storage cost (the amount of fill-ins) <ref> [48] </ref>. Its implementation is based on the IKJ variant of Gaussian elimination, which we recall next. Algorithm 3.1 Gaussian elimination - IKJ variant. 1. For i = 2; n; Do 3. a i;k := a i;k =a k;k 5. a i;j := a i;j a i;k fl a k;j 7. <p> The accuracy of ILUT (t; p) is controlled by two dropping parameters, t and p. In Algorithm 3.2, w is a work array, a i;fi and u k;fi denote the ith and kth rows of A and U , respectively. Algorithm 3.2 Standard ILUT (t; p) factorization <ref> [48, 50] </ref>. 1. For i = 2; n; Do: 3. For k = 1; i 1 and when w k 6= 0; Do: 4. w k := w k =a k;k 5. Set w k := 0 if jw k j &lt; t fl nzavg (a i;fi ) 6.
Reference: [49] <author> Y. Saad. ILUM: </author> <title> a multi-elimination ILU preconditioner for general sparse matrices. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 17(4) </volume> <pages> 830-847, </pages> <year> 1996. </year>
Reference-contexts: On the other hand, we may permute the matrices on each level in the construction phase. In this case, only the global permutation is needed before and after the application of the preconditioner <ref> [49] </ref>. <p> Note that m 0 is not in the second term and the factor ff grows as the level increases. It is therefore advantageous to have large block independent sets in the first few levels. 5 Numerical Experiments Implementations of ILUM and BILUM have been described in detail in <ref> [49, 53] </ref>. One significant difference between BILUTM and BILUM and ILUM, is that we do not use an inner iteration to solve the last reduced system.
Reference: [50] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS Publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: The accuracy of ILUT (t; p) is controlled by two dropping parameters, t and p. In Algorithm 3.2, w is a work array, a i;fi and u k;fi denote the ith and kth rows of A and U , respectively. Algorithm 3.2 Standard ILUT (t; p) factorization <ref> [48, 50] </ref>. 1. For i = 2; n; Do: 3. For k = 1; i 1 and when w k 6= 0; Do: 4. w k := w k =a k;k 5. Set w k := 0 if jw k j &lt; t fl nzavg (a i;fi ) 6. <p> The ILUT implementation gives an easy representation of the residual matrix. Proposition 3.2 The elements of the residual matrix R as in Equation (7) are those elements dropped in Algorithm 3.4. Proof. The proof can be formulated from the arguments in <ref> [50, p. 274] </ref> and [54]. ? ? ? Clearly, Algorithm 3.4 will fail when any individual ILUT fails on at least one of the blocks due to zero pivots. There are at least three strategies to deal with this situation. <p> There are at least three strategies to deal with this situation. First, one can use pivoting as in ILUTP <ref> [50] </ref>, a variant of ILUT which incorporates column pivoting. Second, we may use a diagonal threshold strategy as was done in ILUM [54]. In this technique nodes with small absolute diagonal values are put in the vertex cover. This strategy may reduce the size of the independent set. <p> ff=0 L1 X r ff ): (10) Since the nodes of all independent sets and the last reduced system constitute the order of the matrix, we have L X m ff = n: (11) 1 For this, we may employ a sparse approximate inverse technique [20] or a multicoloring strategy <ref> [50] </ref> to solve the last reduced system. 9 Note that r ff = m ff+1 + r ff+1 for 0 ff &lt; L 1, and r L = 0.
Reference: [51] <author> Y. Saad and M. Sosonkina. </author> <title> Distributed Schur complement techniques for general sparse linear systems. </title> <type> Technical Report UMSI 97/159, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: Implementations of grid-based multi-level methods on parallel and vector computers can be found in [39] (for structured matrices), those of domain-based (two-level) methods in <ref> [51, 52] </ref>. Those and other implementations on shared-memory machines [10] demonstrate the advantage of the inherent parallelism of the multi-level preconditioning methods. On the other hand, implementing multi-level preconditioning methods on distributed-memory machines requires the consideration of cost trade-off between communications and computations.
Reference: [52] <author> Y. Saad, M. Sosonkina, and J. Zhang. </author> <title> Domain decomposition and multi-level type techniques for general sparse linear systems. </title> <editor> In J. Mandel, C. Farhat, and X.-C. Cai, editors, </editor> <title> Domain Decomposition Methods 10, </title> <booktitle> number 218 in Contemporary Mathematics, </booktitle> <pages> pages 200-216, </pages> <address> Providence, RI, 1998. </address> <publisher> AMS. </publisher>
Reference-contexts: The WIGTO966 matrix 4 has 3; 864 unknowns and 238; 252 nonzeros. It comes from an Euler equation model and was supplied by L. Wigton from Boeing. It is solvable by ILUT with large values of p [18]. This matrix was also used to compare BILUM with ILUT in <ref> [52] </ref> and to test point and block preconditioning techniques in [19, 21]. BILUM (with GMRES (10)) was shown to be 6 times faster than ILUT with only one-third of the memory required by ILUT [52]. <p> This matrix was also used to compare BILUM with ILUT in <ref> [52] </ref> and to test point and block preconditioning techniques in [19, 21]. BILUM (with GMRES (10)) was shown to be 6 times faster than ILUT with only one-third of the memory required by ILUT [52]. In our current tests, we chose several values for t and p for BILUTM and ILUT, and the size of the blocks in case of BILUTM. We tabulate the results in Table 7. Amazingly BILUTM converged for this problem with a sparsity ratio of 1:60. <p> Implementations of grid-based multi-level methods on parallel and vector computers can be found in [39] (for structured matrices), those of domain-based (two-level) methods in <ref> [51, 52] </ref>. Those and other implementations on shared-memory machines [10] demonstrate the advantage of the inherent parallelism of the multi-level preconditioning methods. On the other hand, implementing multi-level preconditioning methods on distributed-memory machines requires the consideration of cost trade-off between communications and computations.
Reference: [53] <author> Y. Saad and J. Zhang. BILUM: </author> <title> block versions of multi-elimination and multi-level ILU precondi-tioner for general sparse linear systems. </title> <type> Technical Report UMSI 97/126, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1997. </year>
Reference-contexts: The approximate solution on the last level is obtained by applying one sweep of ILUT of the last reduced system using the factors L L U L . This is different from the implementation of BILUM <ref> [53] </ref>, where the last reduced system is solved to certain accuracy by a Krylov subspace method preconditioned by ILUT. The advantage of BILUTM includes the added flexibility in controlling the amount of fill-ins (and the computation costs during the construction), especially when large size blocks are used. <p> The above discussion of the solution procedure, omitted the permutation and inverse permutation that must be performed before and after each operations on each level. This is also the approach that we used in our current implementation (and that of BILUM <ref> [53] </ref>). On the other hand, we may permute the matrices on each level in the construction phase. In this case, only the global permutation is needed before and after the application of the preconditioner [49]. <p> Note that m 0 is not in the second term and the factor ff grows as the level increases. It is therefore advantageous to have large block independent sets in the first few levels. 5 Numerical Experiments Implementations of ILUM and BILUM have been described in detail in <ref> [49, 53] </ref>. One significant difference between BILUTM and BILUM and ILUM, is that we do not use an inner iteration to solve the last reduced system.
Reference: [54] <author> Y. Saad and J. Zhang. </author> <title> Diagonal threshold techniques in robust multi-level ILU preconditioners for general sparse linear systems. </title> <type> Technical Report UMSI 98/7, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1998. </year>
Reference-contexts: The ILUT implementation gives an easy representation of the residual matrix. Proposition 3.2 The elements of the residual matrix R as in Equation (7) are those elements dropped in Algorithm 3.4. Proof. The proof can be formulated from the arguments in [50, p. 274] and <ref> [54] </ref>. ? ? ? Clearly, Algorithm 3.4 will fail when any individual ILUT fails on at least one of the blocks due to zero pivots. There are at least three strategies to deal with this situation. <p> There are at least three strategies to deal with this situation. First, one can use pivoting as in ILUTP [50], a variant of ILUT which incorporates column pivoting. Second, we may use a diagonal threshold strategy as was done in ILUM <ref> [54] </ref>. In this technique nodes with small absolute diagonal values are put in the vertex cover. This strategy may reduce the size of the independent set. Third, we may replace a small (absolute) diagonal value by a larger one and proceed with the normal ILUT. <p> Simon from Lawrence Berkeley National Laboratory (originally created by A. Raefsky from Centric Engineering). This is probably the hardest one in the total of 6 RAEFSKY matrices. (BILUM with diagonal threshold techniques was able to solve the other 5 but this one RAEFSKY matrices <ref> [54] </ref>.) In order for BILUTM to converge fast, we found it necessary to use a larger restart value (100) for GMRES. Figure 4 shows the convergence history of BILUTM and ILUT with p = 150 and 200, respectively. In both tests, the block size was 200 for BILUTM.
Reference: [55] <author> Y. Saad and J. Zhang. </author> <title> Enhanced multi-level block ILU preconditioning strategies for general sparse linear systems. </title> <type> Technical Report UMSI 98/98, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1998. </year>
Reference-contexts: Thus some heuristic approaches have been proposed to drop small elements from the exactly or approximately inverted blocks to recover sparsity. Obviously this approach cannot reduce the cost of inverting these blocks. The link between the algebraic multigrid methods and BILUM has been discussed briefly in <ref> [55] </ref>. If we define the grid transfer operators naturally based on the matrix, the reduced system based on the Schur complement technique as in (4) also satisfies the Galerkin condition (2). <p> In a two-level analysis, we may define I ff+1 ff L 1 ff+1 = U 1 ff F ff as the projection and interpolation operators, respectively. Then the following results linking BILUTM with the algebraic multigrid methods can be verified directly, see <ref> [55] </ref>. 8 Proposition 4.1 Suppose the factorization (9) exists and exact, then 1. The reduced system A ff+1 = I ff+1 ff A ff I ff ff+1 satisfying the Galerkin condition (2); 2. If, in addition, A ff is symmetric, then I ff+1 ff = I ff T . <p> Note that in order for ILUT and BILUTM to work properly, zero diagonals are added. The BARTH matrices have been used as test matrices for other ILU type techniques in [18], but none of them have been solved by enhanced BILUM techniques <ref> [55] </ref>, partly because of the prohibitive computation and memory costs associated with the use of very large size blocks (on the given computer). We only present in Figure 8 one set of comparisons of BILUTM and ILUT by solving the BARTH matrices using large size blocks and GMRES (100).
Reference: [56] <author> H. A. van der Vorst. </author> <title> A vectorizable version of some ICCG methods. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 3 </volume> <pages> 350-356, </pages> <year> 1982. </year>
Reference: [57] <author> P. Vanek, J. Mandel, and M. Brezina. </author> <title> Algebraic multigrid by smoothed aggregation for second and fourth order elliptic problems. </title> <journal> Computing, </journal> <volume> 56 </volume> <pages> 179-196, </pages> <year> 1996. </year>
Reference: [58] <author> D. M. Young, R. G. Melvin, F. T. Johnson, J. B. Bussoletti, L. B. Wigton, and S. S. Samant. </author> <title> Application of sparse matrix solvers as effective preconditioners. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1186-1199, </pages> <year> 1989. </year>
Reference: [59] <author> Z. Zlatev. </author> <title> Use of iterative refinement in the solution of sparse linear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 </volume> <pages> 381-399, </pages> <year> 1982. </year> <month> 20 </month>
References-found: 59

