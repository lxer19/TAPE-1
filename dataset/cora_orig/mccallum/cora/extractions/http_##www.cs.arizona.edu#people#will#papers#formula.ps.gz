URL: http://www.cs.arizona.edu/people/will/papers/formula.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/will/papers.html
Root-URL: http://www.cs.arizona.edu
Title: Signal Propagation, with Application to a Lower Bound on the Depth of Noisy Formulas  
Author: William Evans Leonard J. Schulman 
Address: Berkeley CA 94720  
Affiliation: Department of Computer Science University of California at Berkeley  
Abstract: We study the decay of an information signal propagating through a series of noisy channels. We obtain exact bounds on such decay, and as a result provide a new lower bound on the depth of formulas with noisy components. This improves upon previous work of Pippenger and significantly decreases the gap between his lower bound and the classical upper bound of von Neumann. We also discuss connections between our work and the study of mixing rates of Markov chains. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In particular, p Y (0) = p X (0)a + p X (1)(1 b) and p Y (1) = p X (0)(1 a) + p X (1)b. For background on information theory the texts of Gallager [4] and Cover and Thomas <ref> [1] </ref> as well as Shan non's original paper [8] are recommended. 3 Reduction to Weak Signal Case Our first step relies upon a geometric interpretation of mutual information. <p> Our first step in determining these parameters relies on a very general fact about maximizing the ratio between two discrete second derivatives. For any function f, any two values x; y in the domain of f , and any p 2 <ref> [0; 1] </ref>, let f 2 (x; y; p) = f (px + (1 p)y) pf (x) (1 p)f (y) denote the discrete second derivative of f. Lemma 1 For any strictly concave functions f, g on the interval [0; 1], and any p 2 [0; 1], the ratio r (x; y) <p> x; y in the domain of f , and any p 2 <ref> [0; 1] </ref>, let f 2 (x; y; p) = f (px + (1 p)y) pf (x) (1 p)f (y) denote the discrete second derivative of f. Lemma 1 For any strictly concave functions f, g on the interval [0; 1], and any p 2 [0; 1], the ratio r (x; y) = g 2 (x; y; p)=f 2 (x; y; p) is maximized in the limit jx yj ! 0. <p> f , and any p 2 <ref> [0; 1] </ref>, let f 2 (x; y; p) = f (px + (1 p)y) pf (x) (1 p)f (y) denote the discrete second derivative of f. Lemma 1 For any strictly concave functions f, g on the interval [0; 1], and any p 2 [0; 1], the ratio r (x; y) = g 2 (x; y; p)=f 2 (x; y; p) is maximized in the limit jx yj ! 0. The lemma holds for more general functions f and g but for brevity we restrict ourselves to the above statement. <p> Proof: Let x fl and y fl be a closest pair of points which achieve the maximum ratio r. We obtain a contradiction by finding a closer pair x,y which achieve at least ratio r. Say x fl &lt; y fl (otherwise reflect the interval <ref> [0; 1] </ref> about 1=2). The function g is bounded since it is a continuous function on a closed and bounded interval. Since f and g are strictly concave it follows that 0 &lt; r &lt; 1. <p> The distance function is induced from the interval.) Proof: Fix any weights p X (0) and p X (1). Then I (X; Y ) and I (X; Z) are the discrete second derivatives of strictly concave functions, namely the restrictions of the entropy function to various subintervals of <ref> [0; 1] </ref>. 2 Observe also that unless the channel is either perfectly noiseless or perfectly noisy, that is unless the entries of A are all 0's and 1's, the corollary will hold strictly; which is to say that the maximum ratio is achieved only in the limit of very close distributions.
Reference: [2] <author> M. Dyer and A. Frieze. </author> <title> Computing the volume of convex bodies: A case where randomness provably helps. </title> <booktitle> Proceedings of Symposia in Applied Mathematics, </booktitle> <volume> 44 </volume> <pages> 123-169, </pages> <year> 1991. </year>
Reference-contexts: Mixing rates of large Markov chains have been studied extensively, in terms of combinatorial properties related to the connectivity of the chains. (For background see the survey papers of Vazirani [9] and of Dyer and Frieze <ref> [2] </ref>.) By contrast we focus on the detailed properties of small, connected chains. 2 Definitions We use p X to denote a probability distribution on random variable X. Similarly, p Y jX=x or p Y jx denotes a probability distribution on random variable Y conditioned on X = x.
Reference: [3] <author> T. Feder. </author> <title> Reliable computation by networks in the presence of noise. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(3) </volume> <pages> 569-571, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Finally, Pippenger showed through an elegant information-theoretic argument that both features were necessary, at least in noisy formulas (circuits with out-degree 1) [7]. Shortly afterward Feder extended Pippenger's bound to general noisy circuits <ref> [3] </ref>. Using more precise information theoretic bounds developed in this paper, we improve Pippenger's result to show: Theorem 1 Let f be a function which depends essentially on n inputs.
Reference: [4] <author> R. G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: In particular, p Y (0) = p X (0)a + p X (1)(1 b) and p Y (1) = p X (0)(1 a) + p X (1)b. For background on information theory the texts of Gallager <ref> [4] </ref> and Cover and Thomas [1] as well as Shan non's original paper [8] are recommended. 3 Reduction to Weak Signal Case Our first step relies upon a geometric interpretation of mutual information.
Reference: [5] <author> B. Hajek and T. Weller. </author> <title> On the maximum tolerable noise for reliable computation by formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(2) </volume> <pages> 388-391, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: log (k~ 2 ) * If ~ 2 1=k then n 1= This result is the best known except in the case of k = 3 where, by different methods, Hajek and Weller have shown that to achieve ffi &lt; 1=2 for arbitrary n, ~ must be greater than 2=3 <ref> [5] </ref>. This matches the threshold for ~ in von Neumann's construction. The application of our information theoretic analysis to the lower bound on formula depth follows the outline of Pippenger's argument which, very briefly, has the following structure.
Reference: [6] <author> R. W. </author> <title> Hamming. Error detecting and error correcting codes. </title> <journal> Bell Syst. Tech. J., </journal> <volume> 29 </volume> <pages> 147-160, </pages> <month> April </month> <year> 1950. </year> <title> Also in Key Papers in the Development of Coding Theory, </title> <editor> E. R. Berlekamp (Ed), </editor> <publisher> IEEE Press, N.Y., </publisher> <pages> pages 9-12, </pages> <year> 1974. </year>
Reference-contexts: The problem of signal decay is not restricted to communication: that it plagues long computations, as well, was all too apparent to the first users of electronic computers, and was for example the spur for Hamming's interest in coding theory <ref> [6] </ref>. In this case the computation is a signal propagating through time. Von Neumann recognized that, rather than being technological and passing, this signal decay was an essential difficulty for large-scale computations.
Reference: [7] <author> N. Pippenger. </author> <title> Reliable computation by formulas in the presence of noise. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34(2) </volume> <pages> 194-197, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: The first inequality of this type on the ratio I (X; Z)=I (X; Y ) was derived by Pippenger (for binary channels) as a key step in his method for showing a lower bound on the depth, and an upper bound on the maximum tolerable component noise, of noisy formu-las <ref> [7] </ref>. In this paper we improve Pippenger's inequality, and obtain the exact upper bound on the maximum achievable "signal strength ratio" I (X; Z)=I (X; Y ), for every binary channel. <p> For a long time it was not known whether these features were necessary, or were artifacts of von Neuman-n's construction. Finally, Pippenger showed through an elegant information-theoretic argument that both features were necessary, at least in noisy formulas (circuits with out-degree 1) <ref> [7] </ref>. Shortly afterward Feder extended Pippenger's bound to general noisy circuits [3]. Using more precise information theoretic bounds developed in this paper, we improve Pippenger's result to show: Theorem 1 Let f be a function which depends essentially on n inputs.
Reference: [8] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Tech. J., </journal> <volume> 27 </volume> <pages> 379-423; 623-656, </pages> <year> 1948. </year>
Reference-contexts: In particular, p Y (0) = p X (0)a + p X (1)(1 b) and p Y (1) = p X (0)(1 a) + p X (1)b. For background on information theory the texts of Gallager [4] and Cover and Thomas [1] as well as Shan non's original paper <ref> [8] </ref> are recommended. 3 Reduction to Weak Signal Case Our first step relies upon a geometric interpretation of mutual information.
Reference: [9] <author> U. Vazirani. </author> <title> Rapidly mixing markov chains. </title> <booktitle> Proceedings of Symposia in Applied Mathematics, </booktitle> <volume> 44 </volume> <pages> 99-121, </pages> <year> 1991. </year>
Reference-contexts: Mixing rates of large Markov chains have been studied extensively, in terms of combinatorial properties related to the connectivity of the chains. (For background see the survey papers of Vazirani <ref> [9] </ref> and of Dyer and Frieze [2].) By contrast we focus on the detailed properties of small, connected chains. 2 Definitions We use p X to denote a probability distribution on random variable X.
Reference: [10] <author> J. von Neumann. </author> <title> Probabilistic logics and the synthesis of reliable organisms from unreliable components. </title> <editor> In C. E. Shannon and J. McCarthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <pages> pages 43-98. </pages> <publisher> Princeton University Press, </publisher> <year> 1956. </year>
Reference-contexts: Von Neumann recognized that, rather than being technological and passing, this signal decay was an essential difficulty for large-scale computations. Consequently he was interested in whether, and at what cost, a computer with noisy components might simulate one with ideal, noiseless components <ref> [10] </ref>. In this paper we investigate the propagation of information signals in noisy media. We study a basic question which is relevant to any such propagation, whether in communication or in computation. To set the framework we first recall the well known "data processing lemma" for information.
References-found: 10

