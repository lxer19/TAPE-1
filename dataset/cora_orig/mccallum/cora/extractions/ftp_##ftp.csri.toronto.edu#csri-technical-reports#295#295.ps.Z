URL: ftp://ftp.csri.toronto.edu/csri-technical-reports/295/295.ps.Z
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3549/node10.html
Root-URL: 
Title: Parallel Sorting by Overpartitioning  
Author: Hui Li and Kenneth C. Sevcik 
Address: Toronto, Canada  
Affiliation: Computer Systems Research Institute University of Toronto  
Pubnum: M5S 1A1  
Abstract: Technical Report CSRI-295 February 1994 The Computer Systems Research Institute (CSRI) is an interdisciplinary group formed to conduct research and development relevant to computer systems and their application. It is an Institute within the Faculty of Applied Science and Engineering, and the Faculty of Arts and Science, at the University of Toronto, and is supported in part by the Natural Sciences and Engineering Research Council of Canada. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B.-H. Lim, G. Maa, and D. Nussbaum. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. In Scalable Shared Memory Architectures. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Each processor has a 256K data subcache, a 256K instruction subcache, and a 32M cache memory. The subcache and cache memory on the KSR1 are similar to cache and local memory on other multiprocessors (e.g., Stanford DASH [19], MIT Alewife <ref> [1] </ref>, and University of Toronto Hector).
Reference: [2] <author> M. Ajtai, J. Komolos, and E. Szemeredi. </author> <title> Sorting in clog n parallel steps. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [3] <author> S. G. Akl. </author> <title> Parallel Sorting Algorithms. </title> <publisher> Academic Press, </publisher> <address> Toronto, </address> <year> 1985. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [4] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. Simon. </author> <title> NAS parallel benchmark results. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(1) </volume> <pages> 43-52, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application <ref> [4] </ref>, and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network.
Reference: [5] <author> K. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proc. of the AFIPS Spring Joint Computer Conference, </booktitle> <volume> volume 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher <ref> [5] </ref> proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. <p> Sciences and Engineering Research Council of Canada and from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort <ref> [5] </ref> based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17]
Reference: [6] <author> D. T. Blackston and A. Ranade. Snakesort: </author> <title> A family of simple optimal randomized sorting algorithms. </title> <booktitle> In Proc. of 22nd International Conference on Parallel Processing, </booktitle> <pages> pages III-201-III-204, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort <ref> [6] </ref> quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing.
Reference: [7] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort <ref> [7, 20, 13] </ref> flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing.
Reference: [8] <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Proc. of Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <address> Hilton Head, SC., </address> <month> July </month> <year> 1991. </year>
Reference-contexts: one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort <ref> [15, 8] </ref> B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. <p> The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) [30, 21], Parallel Sample Sort (PSS) <ref> [15, 8] </ref>, and Parallel Radix Sort [31] on these machines. The performance obtained is very close to that predicted by the analytical model developed in Section 4. <p> Two metrics can be used to measure the load balancing: (i) the sublist expansion <ref> [8] </ref> and (ii) the load expansion. The sublist expansion is defined as the ratio of the maximum sublist size to the mean sublist size: max j=1;q jS j j j=1 jS j j=q where jS j j is the size of the sublist S j . <p> The regular sampling guarantees to balance the load among processors within a factor of two of optimal in theory, and within a few percent of optimal in practice. * Parallel Sample Sort (PSS) <ref> [15, 8] </ref> does not sort the portions in Phase 1 and uses oversampling to select pivots. It picks p 1 pivots by randomly choosing ps candidates from the entire list `, where s is the oversampling ratio, and then selecting p 1 equally spaced pivots from the sorted candidates. <p> However, the time spent in Phase 1 increases with the oversampling ratio and the overpartitioning ratio. Therefore, it is necessary to choose the appropriate values for the two parameters in order to achieve good performance. 3.2.1 Maximum Sublist Size Blelloch et al. <ref> [8] </ref> have shown that the sublist sizes can be reduced by increasing the oversampling ratio. <p> Thus the time for remote memory increases more quickly when p &gt; 32. Comparison. Two other parallel sorting algorithms PSS and PSRS were also implemented on the KSR1 in order to compare their performance with PQOP. It has been shown that PSS performs well on the SIMD machine CM-2 <ref> [8] </ref>, and PSRS achieves good performance on a variety of MIMD machines [21]. Figure 10 shows the speedup curves for the three algorithms for sorting one million integers. Although PSS has low communication overhead, it does not balance load well even with an oversampling ratio as large as 32.
Reference: [9] <author> A. Borodin and J. Hopcroft. </author> <title> Routing, merging and sorting on parallel models of computation. </title> <journal> J. Computer Systems and Science, </journal> <volume> 30 </volume> <pages> 130-145, </pages> <year> 1985. </year> <month> 19 </month>
Reference-contexts: from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort <ref> [9, 10] </ref> smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The
Reference: [10] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort <ref> [9, 10] </ref> smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The
Reference: [11] <author> E. Felten, S. Karlin, and S. Otto. </author> <title> Sorting on a hypercube. </title> <type> Technical report, Hm 244, </type> <institution> Cal-tech/JPL, </institution> <year> 1986. </year>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [12] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors, Volumne I, General Techniques aand Regular Problems. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [13] <author> R. S. Francis and L. J. H. Pannan. </author> <title> A parallel partition for enhanced parallel quicksort. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 543-550, </pages> <year> 1992. </year>
Reference-contexts: the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort <ref> [7, 20, 13] </ref> flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing.
Reference: [14] <author> W. L. Hightower, J. F. Prins, and J. H. Reif. </author> <title> Implementation of randomized sorting on large parallel machines. </title> <booktitle> In Proc. of Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-167, </pages> <address> San Diego, CA., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort <ref> [14] </ref> hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. Single-step algorithms incur low communication cost, because they move each element at most once.
Reference: [15] <author> J. S. Huang and Y. C. Chow. </author> <title> Parallel sorting and data partitioning by sampling. </title> <booktitle> In Proc. of the IEEE Computer Society's 7th International Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <year> 1983. </year>
Reference-contexts: one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort <ref> [15, 8] </ref> B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. <p> The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) [30, 21], Parallel Sample Sort (PSS) <ref> [15, 8] </ref>, and Parallel Radix Sort [31] on these machines. The performance obtained is very close to that predicted by the analytical model developed in Section 4. <p> The regular sampling guarantees to balance the load among processors within a factor of two of optimal in theory, and within a few percent of optimal in practice. * Parallel Sample Sort (PSS) <ref> [15, 8] </ref> does not sort the portions in Phase 1 and uses oversampling to select pivots. It picks p 1 pivots by randomly choosing ps candidates from the entire list `, where s is the oversampling ratio, and then selecting p 1 equally spaced pivots from the sorted candidates.
Reference: [16] <author> C. Kaklamanis, D. Kraizanc, L. Narayanan, and T. Tsantilas. </author> <title> Randomized sorting and selection on mesh-connected processor arrays. </title> <booktitle> In Proc. of Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 17-28, </pages> <address> Hilton Head, SC., </address> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [17] <author> L. V. Kale and S. Krishnan. </author> <title> A comparison based parallel sorting algorithm. </title> <booktitle> In Proc. of 22nd International Conference on Parallel Processing, </booktitle> <pages> pages III-196-III-200, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort <ref> [17] </ref> Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. Single-step algorithms incur low communication cost, because they move each element at most once. <p> complexities depend on p. * The total number of remote memory accesses is n W in the worst case and n W p on average. 14 Algorithms Machine Processors Time (Sec.) PQOP KSR1 64 3.78 PQOP KSR1 32 7.49 PSRS [21] TC-2000 64 4.29 PSRS [21] iPSC/860 64 2.46 K&K <ref> [17] </ref> iPSC/860 64 3.87 K&K [17] nCUBE/2 64 12.30 T&B [32] CM-5 32 5.48 Table 6: Execution time for sorting 8 million integers. Thus, as the number of processors increases, the fraction of the time involving remote memory accesses increases. <p> The total number of remote memory accesses is n W in the worst case and n W p on average. 14 Algorithms Machine Processors Time (Sec.) PQOP KSR1 64 3.78 PQOP KSR1 32 7.49 PSRS [21] TC-2000 64 4.29 PSRS [21] iPSC/860 64 2.46 K&K <ref> [17] </ref> iPSC/860 64 3.87 K&K [17] nCUBE/2 64 12.30 T&B [32] CM-5 32 5.48 Table 6: Execution time for sorting 8 million integers. Thus, as the number of processors increases, the fraction of the time involving remote memory accesses increases.
Reference: [18] <author> F. T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(4):344-354, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort <ref> [18] </ref> Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication
Reference: [19] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M.S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Each processor has a 256K data subcache, a 256K instruction subcache, and a 32M cache memory. The subcache and cache memory on the KSR1 are similar to cache and local memory on other multiprocessors (e.g., Stanford DASH <ref> [19] </ref>, MIT Alewife [1], and University of Toronto Hector).
Reference: [20] <author> P. P. Li and Y.-W. Tung. </author> <title> Parallel sorting on Symult 2010. </title> <booktitle> In Proc. of 5th Distributed Memory Computing Conference, </booktitle> <pages> pages 224-229, </pages> <address> Charleston, SC., </address> <month> April </month> <year> 1990. </year>
Reference-contexts: the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort <ref> [7, 20, 13] </ref> flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing.
Reference: [21] <author> X. Li, P. Lu, J. Schaeffer, J. Shillington, P. S. Wong, and H. Shi. </author> <title> On the versatility of parallel sorting by regular sampling. </title> <journal> Parallel Computing, </journal> 19(10) 543-550, October 1993. 
Reference-contexts: Research Council of Canada and from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS <ref> [30, 21] </ref> parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification <p> The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) <ref> [30, 21] </ref>, Parallel Sample Sort (PSS) [15, 8], and Parallel Radix Sort [31] on these machines. The performance obtained is very close to that predicted by the analytical model developed in Section 4. <p> by i f (jS j j)] P q 2.2 Examples of Single-step Sorting We can obtain different flavors of single-step algorithms based on whether the ` i 's are sorted in Phase 1, and the way the pivots are picked in Phase 2. * Parallel Sorting by Regular Sampling (PSRS) <ref> [30, 21] </ref> sorts the portions in Phase 1 and uses regular sampling to select pivots. It picks p 1 pivots by first choosing p equally spaced candidates from each portion, and then selecting p 1 equally spaced pivots from the sorted p 2 candidates. <p> queue grow with the number of processors because their time complexities depend on p. * The total number of remote memory accesses is n W in the worst case and n W p on average. 14 Algorithms Machine Processors Time (Sec.) PQOP KSR1 64 3.78 PQOP KSR1 32 7.49 PSRS <ref> [21] </ref> TC-2000 64 4.29 PSRS [21] iPSC/860 64 2.46 K&K [17] iPSC/860 64 3.87 K&K [17] nCUBE/2 64 12.30 T&B [32] CM-5 32 5.48 Table 6: Execution time for sorting 8 million integers. Thus, as the number of processors increases, the fraction of the time involving remote memory accesses increases. <p> of processors because their time complexities depend on p. * The total number of remote memory accesses is n W in the worst case and n W p on average. 14 Algorithms Machine Processors Time (Sec.) PQOP KSR1 64 3.78 PQOP KSR1 32 7.49 PSRS <ref> [21] </ref> TC-2000 64 4.29 PSRS [21] iPSC/860 64 2.46 K&K [17] iPSC/860 64 3.87 K&K [17] nCUBE/2 64 12.30 T&B [32] CM-5 32 5.48 Table 6: Execution time for sorting 8 million integers. Thus, as the number of processors increases, the fraction of the time involving remote memory accesses increases. <p> Comparison. Two other parallel sorting algorithms PSS and PSRS were also implemented on the KSR1 in order to compare their performance with PQOP. It has been shown that PSS performs well on the SIMD machine CM-2 [8], and PSRS achieves good performance on a variety of MIMD machines <ref> [21] </ref>. Figure 10 shows the speedup curves for the three algorithms for sorting one million integers. Although PSS has low communication overhead, it does not balance load well even with an oversampling ratio as large as 32. This results in relatively poor speedups.
Reference: [22] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generlized connection network. </title> <journal> Journal of the ACM, </journal> <volume> 29(3) </volume> <pages> 642-667, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort <ref> [22] </ref> column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined
Reference: [23] <author> M.H. Nodine and J.S. Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proc. of Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Velen, Germany, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [24] <author> C. G. Plaxton. </author> <title> Efficient computation on sparse interconnection networks. </title> <type> Technical Report STAN-CS-89-1283, </type> <institution> Stanford University, Department of Computer Scienece, Stanford, </institution> <address> CA, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort <ref> [24] </ref> Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a
Reference: [25] <author> M. J. Quinn. </author> <title> Analysis and benchmarking of two parallel sorting algorithms: </title> <journal> hyperquicksort and quickmerge. BIT, </journal> <volume> 29(2) </volume> <pages> 239-250, </pages> <year> 1989. </year> <month> 20 </month>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> grants from the Natural Sciences and Engineering Research Council of Canada and from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge <ref> [25] </ref> Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale <p> merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort <ref> [34, 25] </ref> Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. Single-step algorithms incur low communication cost, because they move each element at most once.
Reference: [26] <author> S. Rajasekaran and J. H. Reif. </author> <title> Optimal and sublogrithmic time randomized parallel sorting algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18(3) </volume> <pages> 594-607, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [27] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 60-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Sorting is an important symbolic application [4], and parallel algorithms for sorting have been studied intensively ever since Batcher [5] proposed the bitonic-sorting network. Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms <ref> [2, 3, 18, 27, 10, 26, 16, 23] </ref>, empirical studies [12, 11, 7, 34, 25, 29, 35] started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort <ref> [27] </ref> based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing.
Reference: [28] <author> Kendall Squre Research. </author> <title> KSR1 Principles of Operation. </title> <address> Waltham, MA, </address> <year> 1991. </year>
Reference-contexts: The resulting algorithms are called Parallel Quicksort by OverPartitioning (PQOP) and Parallel Radix sort by OverPartitioning (PROP) respectively. We have implemented these algorithms on two shared memory multiprocessors, namely the Kendall Square Research KSR1 <ref> [28] </ref>, and the University of Toronto Hector system [33]. <p> We thus choose a value for H such that most sublists have size smaller than the cache size. 5 Experimental Results The PQOP and PROP algorithms have been implemented on both the KSR1 <ref> [28] </ref> and Hector [33] multiprocessors. The measured timing does not include the times for initializing the list and confirming the correct order of the final list. The cost of handling page faults is not included in the timing because the current KSR OS does not handle page faults efficiently. <p> We believe that this cost will be negligible with the future version of the KSR OS. 5.1 The KSR1 and Hector Multiprocessors Both the KSR1 and Hector have a hierarchical ring interconnection network. The KSR1 <ref> [28] </ref> is a scalable Cache-Only-Memory Architecture (COMA) system composed of a hierarchy of rings. The lowest level, ring:0, consists of 32 processor cells and two cells for routing to the higher level ring - ring:1. The system we used consists of 64 cells on two ring:0's connected by a ring:1.
Reference: [29] <author> S. R. Seidel and W. L. George. </author> <booktitle> Binsorting on hypercubes with d-port communication. </booktitle> <pages> pages 1455-1461, </pages> <year> 1988. </year>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
Reference: [30] <author> H. Shi and J. Schaeffer. </author> <title> Parallel sorting by regular sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 362-372, </pages> <year> 1992. </year>
Reference-contexts: Research Council of Canada and from the Information Technology Research Centre of Ontario. 1 A single-step algorithm will merge the portions in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS <ref> [30, 21] </ref> parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification <p> The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) <ref> [30, 21] </ref>, Parallel Sample Sort (PSS) [15, 8], and Parallel Radix Sort [31] on these machines. The performance obtained is very close to that predicted by the analytical model developed in Section 4. <p> by i f (jS j j)] P q 2.2 Examples of Single-step Sorting We can obtain different flavors of single-step algorithms based on whether the ` i 's are sorted in Phase 1, and the way the pivots are picked in Phase 2. * Parallel Sorting by Regular Sampling (PSRS) <ref> [30, 21] </ref> sorts the portions in Phase 1 and uses regular sampling to select pivots. It picks p 1 pivots by first choosing p equally spaced candidates from each portion, and then selecting p 1 equally spaced pivots from the sorted p 2 candidates.
Reference: [31] <author> K. Thearling and S. Smith. </author> <title> An improved supercomputer sorting benchmark. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <pages> pages 307-314, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) [30, 21], Parallel Sample Sort (PSS) [15, 8], and Parallel Radix Sort <ref> [31] </ref> on these machines. The performance obtained is very close to that predicted by the analytical model developed in Section 4. <p> Overhead Analysis. Since PROP does not involve pivoting, the main overhead is in the cost of remote memory access. Figure 13 shows that the overhead of remote memory accesses increases 16 17 Machine KSR1 CM-5 Algorithm PROP Parallel Radix Sort <ref> [31] </ref> Time (sec) 10.02 16.7 Table 7: Execution times for sorting 64 million 32-bit integers on 64 processors. with the number of processors up to about 20% on 64 processors. However, the speedup is not reduced substantially, because of the increase in the cache hit ratio. of Parallel Radix Sort.
Reference: [32] <author> A. Tridgell and R. P. Brent. </author> <title> An implementation of a general-purpose parallel sorting algorithm. </title> <type> Technical Report TR-CS-93-01, </type> <institution> Computer Scienece laboratory, Australian National University, Australia, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: in all the processors or partition the sublists and assign to all the processors in one step. 1 Single-step Multi-step merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort <ref> [32] </ref> snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort [34, 25] Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of <p> memory accesses is n W in the worst case and n W p on average. 14 Algorithms Machine Processors Time (Sec.) PQOP KSR1 64 3.78 PQOP KSR1 32 7.49 PSRS [21] TC-2000 64 4.29 PSRS [21] iPSC/860 64 2.46 K&K [17] iPSC/860 64 3.87 K&K [17] nCUBE/2 64 12.30 T&B <ref> [32] </ref> CM-5 32 5.48 Table 6: Execution time for sorting 8 million integers. Thus, as the number of processors increases, the fraction of the time involving remote memory accesses increases.
Reference: [33] <author> Z. G. Vranesic, M. Stumm, D. M. Lewis, and R. White. Hector: </author> <title> A hierarchically structured shared memory multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 72-79, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The resulting algorithms are called Parallel Quicksort by OverPartitioning (PQOP) and Parallel Radix sort by OverPartitioning (PROP) respectively. We have implemented these algorithms on two shared memory multiprocessors, namely the Kendall Square Research KSR1 [28], and the University of Toronto Hector system <ref> [33] </ref>. The experiments in Section 5 show that these algorithms achieve nearly linear speedup and perform better than (our implementation of) Parallel Sorting by Regular Sampling (PSRS) [30, 21], Parallel Sample Sort (PSS) [15, 8], and Parallel Radix Sort [31] on these machines. <p> We thus choose a value for H such that most sublists have size smaller than the cache size. 5 Experimental Results The PQOP and PROP algorithms have been implemented on both the KSR1 [28] and Hector <ref> [33] </ref> multiprocessors. The measured timing does not include the times for initializing the list and confirming the correct order of the final list. The cost of handling page faults is not included in the timing because the current KSR OS does not handle page faults efficiently. <p> ring:0, and 530 cycles on ring:1. 11 n 100K 256K 512K 800K 1M 8M Hector 2.70 7.34 16.40 25.67 Table 4: Sequential execution times KSR1 Hector C 1.32 1.32 Q 18.06 (cycles) 18.78 cycles W 32 4 t r depends on p depends on p Table 5: Parameter values Hector <ref> [33] </ref> is a scalable NUMA shared memory multiprocessor system consisting of sets of processor-memory pairs connected by buses to form a station, and several stations connected by local rings, and several local rings connected by a global ring.
Reference: [34] <author> B. A. Wagar. </author> <title> Practical Sorting Algorithms for Hypercube Computers. </title> <type> PhD thesis, </type> <institution> Depatrment of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted. <p> merge- quickmerge [25] Batcher's bitonic sort [5] based PSRS [30, 21] parallel merge sort [9, 10] smoothsort [24] Nassimi and Sahni's sort [22] column sort [18] Tridgell and Brent's sort [32] snakesort [6] quicksort- parallel quicksort [7, 20, 13] flashsort [27] based parallel sample sort [15, 8] B-flashsort [14] hyperquicksort <ref> [34, 25] </ref> Kale and Krishnan sort [17] Table 1: Classification of parallel sorting algorithms The performance of a parallel sorting algorithm is determined by the communication cost and the degree of load balancing. Single-step algorithms incur low communication cost, because they move each element at most once.
Reference: [35] <author> Y. Won and S. Sahni. </author> <title> A balanced bin sort for hypercube multicomputers. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <year> 1988. </year> <month> 21 </month>
Reference-contexts: Although the theoretical community has devoted considerable effort to the design of parallel sorting algorithms [2, 3, 18, 27, 10, 26, 16, 23], empirical studies <ref> [12, 11, 7, 34, 25, 29, 35] </ref> started appearing only in the late 1980's when multiprocessors became widely available. In a typical parallel sorting algorithm, each processor contains a portion of the list of n elements to be sorted.
References-found: 35

