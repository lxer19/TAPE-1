URL: http://www-poleia.lip6.fr/~moulinie/sdair.ps.gz
Refering-URL: 
Root-URL: 
Title: Text Categorization: a Symbolic Approach  
Author: Isabelle Moulinier Gailius Raskinis yz and Jean-Gabriel Ganascia LAFORIA-IBP-CNRS Vtautas Magnus 
Address: 4 place Jussieu F-75252 Paris Cedex 05 FRANCE  Vileikos 8 3035 Kaunas LITHUANIA  
Affiliation: Universite Paris VI  University Information Computer Science Faculty  
Abstract: Recent research in Machine Learning has been concerned with scaling-up to large datasets. Since Information Retrieval is an domain where such datasets are widespread, it provides an ideal application area for machine learning. This paper studies the ability of symbolic learning algorithms to perform a text categorization task. This ability depends on both text representation and feature filtering. We present a unified view of text categorization systems, focusing on the selection of features. A new selection technique, SCAR, is proposed for k-DNF learners and evaluated on the Reuters financial dataset. Even though our experimental results do not outperform earlier approaches, they give rise to promising perspectives. 
Abstract-found: 1
Intro-found: 1
Reference: [ADW94] <author> C. Apte, F. Damerau, and S. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <month> July </month> <year> 1994. </year>
Reference-contexts: Symbolic feature selection is covered in Section 4. After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction [LR94, FHL + 91], rule induction <ref> [ADW94, MG95] </ref>, neural networks [WPW95] and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. <p> Several recent text categorization studies have been evaluated on the Reuters financial corpus <ref> [LR94, WPW95, ADW94] </ref>. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. In the following, we present a unified view of text categorization methods, especially [LR94, Text Categorization: a Symbolic Approach ADW94, WPW95]. <p> Another characteristic is that each approach presents a means of filtering these features and adopt some text representation. Finally, additional knowledge can be provided to the learner. This unifying view is summarized in Figure 1. We now describe several studies along this framework. Apte, Damerau and Weiss <ref> [ADW94] </ref> use an algorithm called Swap-1 that induces production rules by performing heuristic search using local optimization techniques. Firstly, they empirically show that a frequency-based representation performs better than a boolean-based model. Moreover, frequency-based representation can be enhanced by additional knowledge. <p> Production rule learners, like Swap-1 <ref> [ADW94] </ref> or CHARADE [Gan91], are typical k-DNF learners. To complete the definition of k-DNF learners, we give an intuition of what a symbolic learner is. Ideally, a symbolic learner induces concepts from symbolic data, using symbolic operators [Mic83]. <p> As mentioned in Section 2, earlier studies in text categorization (and more generally in information retrieval approaches) are based on the filter model, where feature selection is done in a preprocessing step. Mutual information [LR94], relevancy score [WPW95] and frequency <ref> [ADW94] </ref> are instances of the filter model. However, this model is limited, since the effects of the selected feature subset on the performance of the induction algorithm cannot be predicted. The wrapper model is an approach to remedy this restriction. <p> The performance of the learner, both in overall effectiveness and learning complexity, was enhanced. However, the accuracy reported in earlier experiments outperforms our effectiveness by several percents. Using a boolean representation, [WPW95] reported a break-even point of 77.5%, while under similar constraints, <ref> [ADW94] </ref> obtained 78,5%. <p> Finally, machine learning sometimes considers representation as additional knowledge, which is used to enhance learning. This has been been applied to text categorization to a smaller extent; for instance, using more complex representations <ref> [ADW94, WPW95] </ref> or hierarchical neural nets [WPW95] is a means of providing domain-knowledge to the learner. Our future work will follow this lead, in the context of machine learning. Adapting machine learning methods to text categorization provides means to evaluate both the strengths and weaknesses of ML approaches.
Reference: [Car93] <author> C. Cardie. </author> <title> Using decison trees to improve case-based learning. </title> <booktitle> In Proceeding of the 10th International Conference on Machine Learning, </booktitle> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: When redundancy is set to one, local space k need not be explored if subspaces i and j have already been searched. generated by SCAR are relevant to the assignment of the category modeled by that formula. A similar idea can be found in <ref> [Car93] </ref>, where Cardie uses a decision tree as an approach to feature selection. A straightforward approach would be to run CHARADE on the training set and consider all features in the resulting production rules as relevant. However, CHARADE can not be used over the initial description space.
Reference: [CMSW92] <author> R. H. Creecy, B. M. Masand, S. J. Smith, and D. L. Waltz. </author> <title> Trad-inf mips and memory for knowledge engineering: Clssifying census returns on the connection machine. </title> <journal> Communication of the ACM, </journal> <year> 1992. </year>
Reference-contexts: Typical data mining applications deal with over 100,000 instances described using several hundreds of features. Text categorization is another challenging application for well-known algorithms, mainly because of the size of data involved: tens of thousands of texts However, feature selection does not always imply dimensionality reduction <ref> [CMSW92] </ref>. described by tens or hundreds of thousands of features. In this paper, we study whether symbolic ML algorithms can be used to induce text cat-egorizers. <p> Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners <ref> [Yan94, CMSW92] </ref>. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83]. <p> After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction [LR94, FHL + 91], rule induction [ADW94, MG95], neural networks [WPW95] and k nearest neighbors <ref> [Yan94, CMSW92] </ref>. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus.
Reference: [CN89] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: The stopping criterion can be consistency, like in AQ [Mic83], or some statistical validity measurement, as in CN2 <ref> [CN89] </ref>. The CHARADE algorithm, given in Figure 2, is closely related to these symbolic k-DNF learners. However, it differs in several ways that we briefly discuss. Let us call descriptor a pair hfeature, valuei. A description is a conjunction of descriptors.
Reference: [Coh95] <author> W. Cohen. </author> <title> Text categorization and relational learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Lake Tahoe, California, </address> <year> 1995. </year>
Reference-contexts: There has been a recent outbreak of applications of text categorization, such as text extraction, filtering and routing to topic-specific processing mechanisms [HANS90, RL94]. On the other hand, text categorization has emerged as an application domain in machine learning (ML) <ref> [LC94, Coh95] </ref>. Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments. A pioneering work to automate text categorization can be found in [Mar61].
Reference: [FHL + 91] <author> N. Fuhr, S. Hartmann, G. Lustig, M. Schwantner, and K. Tzeras. </author> <title> AIR/X | a Rule-Based Multistage Indexing System for Large Subject Fields. </title> <booktitle> In Proc. of RIAO'91, </booktitle> <address> Barcelona, Spain, </address> <year> 1991. </year>
Reference-contexts: Symbolic feature selection is covered in Section 4. After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction <ref> [LR94, FHL + 91] </ref>, rule induction [ADW94, MG95], neural networks [WPW95] and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94].
Reference: [Gan91] <author> J.-G. Ganascia. </author> <title> Deriving the learning bias from rule properties. </title> <editor> In J. E. Hayes, D. Mitchie, and E. Tyngu, editors, </editor> <booktitle> Machine Intelligence 12, </booktitle> <pages> pages 151-167. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1991. </year>
Reference-contexts: More precisely, we are interested in k-DNF learners, i.e. learners that express their results as a logical formula in disjunctive normal form, and will present our experimental results with such a learner, the CHARADE system <ref> [Gan91, Gan93] </ref>. Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83]. <p> Production rule learners, like Swap-1 [ADW94] or CHARADE <ref> [Gan91] </ref>, are typical k-DNF learners. To complete the definition of k-DNF learners, we give an intuition of what a symbolic learner is. Ideally, a symbolic learner induces concepts from symbolic data, using symbolic operators [Mic83].
Reference: [Gan93] <author> J.-G. Ganascia. TDIS: </author> <title> an Algebraic Formalization. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, </address> <year> 1993. </year>
Reference-contexts: More precisely, we are interested in k-DNF learners, i.e. learners that express their results as a logical formula in disjunctive normal form, and will present our experimental results with such a learner, the CHARADE system <ref> [Gan91, Gan93] </ref>. Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83].
Reference: [HANS90] <author> P. Hayes, P. Andersen, I. Niren-burg, and L. Schmandt. </author> <title> TCS: A Shell for Content-Based Text Categorization. </title> <booktitle> In Proceeding of the Sixth IEEE CAIA, </booktitle> <pages> pages 321-325, </pages> <year> 1990. </year>
Reference-contexts: There has been a recent outbreak of applications of text categorization, such as text extraction, filtering and routing to topic-specific processing mechanisms <ref> [HANS90, RL94] </ref>. On the other hand, text categorization has emerged as an application domain in machine learning (ML) [LC94, Coh95]. Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments.
Reference: [HS94] <author> M. Holsheimer and A. Siebes. </author> <title> Data mining. the search for knowledge in databases. </title> <type> Technical report, </type> <institution> CWI, </institution> <year> 1994. </year>
Reference-contexts: Until recently, ML has mostly been concerned with the design of algorithms and their applications to toy learning sets, consisting of a few hundreds of instances described by few dozens of features. Recent applications of ML algorithms to molecular biology [HSS93] or data mining <ref> [HS94] </ref> have introduced scaling up into ML research. Typical data mining applications deal with over 100,000 instances described using several hundreds of features.
Reference: [HSS93] <editor> L. Hunter, D. Searls, and J. Shav-lik, editors. </editor> <booktitle> First International Conference on Intelligent Systems for Molecular Biology. </booktitle> <publisher> The AAAI Press, </publisher> <year> 1993. </year>
Reference-contexts: Until recently, ML has mostly been concerned with the design of algorithms and their applications to toy learning sets, consisting of a few hundreds of instances described by few dozens of features. Recent applications of ML algorithms to molecular biology <ref> [HSS93] </ref> or data mining [HS94] have introduced scaling up into ML research. Typical data mining applications deal with over 100,000 instances described using several hundreds of features.
Reference: [HW90] <author> P. Hayes and S. Weinstein. CONSTRUE/TIS: </author> <title> a system for content-based indexing of a database of news stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: On one hand, information processing needs have increased with the rapid growth of textual information sources. Subject assignment to documents in order to support information retrieval as well as aid to human indexers in this assignment task are the major applications of text categorization systems <ref> [HW90] </ref>. There has been a recent outbreak of applications of text categorization, such as text extraction, filtering and routing to topic-specific processing mechanisms [HANS90, RL94]. On the other hand, text categorization has emerged as an application domain in machine learning (ML) [LC94, Coh95]. <p> Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments. A pioneering work to automate text categorization can be found in [Mar61]. The effectiveness of the hand-crafted, knowledge engineering system CONSTRUE <ref> [HW90] </ref> has set a landmark in text categorization research. CONSTRUE design, however, was rather expensive and time-consuming. ML approaches to classification (indeed, text categorization is a classification task) suggest the construction of decision means using induction over pre-classified samples.
Reference: [JKP94] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Recent studies in ML have tackled the problem of selecting relevant features from a feature set. These studies were only applied to datasets where instances were described by little more than a score of features. In <ref> [JKP94] </ref>, John, Kohavi and Pfleger introduce two models, that characterize feature selection algorithms, namely the filter and the wrapper model. <p> Candidate sets are then evaluated in the following manner: an inductive algorithm is run on the training set, and its prediction accuracy is tested on a test set. The candidate set that obtains the best accuracy is used to represent instances for further induction. The results reported in <ref> [JKP94] </ref> suggest that feature selection using the wrapper model improves the predictive accuracy of the classifier. Since this model requires repeatedly running the inductive algorithm, its cost for large feature sets prohibits its use in text categorization. Nevertheless, adapting feature selection to the inductive algorithm is appealing.
Reference: [KR92] <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: We propose a selection that is adapted to k-DNF learners and compare it with two statistical feature selection techniques, the mutual information criterion found in [Lew92] and a modified version of the RELIEF algorithm <ref> [KR92] </ref>. In Section 2, we analyze related approaches to text categorization. k-DNF learners and the CHARADE system are briefly presented in Section 3. Symbolic feature selection is covered in Section 4. <p> Learning is performed using CHARADE with redundancy set to 2 and no noise constraint. we do not learn to assign a category on the absence of a word. We evaluated SCAR using two alternative statistical filtering techniques: standard mutual information (MI) and RELIEF <ref> [KR92] </ref>, which was re-implemented to take our representational hypothesis into account. We ran CHARADE using several parameter settings for each filtering technique. During these experiments, varying size of feature sets were used for both MI and RELIEF.
Reference: [LC94] <author> D. Lewis and J. Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: There has been a recent outbreak of applications of text categorization, such as text extraction, filtering and routing to topic-specific processing mechanisms [HANS90, RL94]. On the other hand, text categorization has emerged as an application domain in machine learning (ML) <ref> [LC94, Coh95] </ref>. Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments. A pioneering work to automate text categorization can be found in [Mar61].
Reference: [Lew92] <author> D. Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> Graduate School of the University of Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: We propose a selection that is adapted to k-DNF learners and compare it with two statistical feature selection techniques, the mutual information criterion found in <ref> [Lew92] </ref> and a modified version of the RELIEF algorithm [KR92]. In Section 2, we analyze related approaches to text categorization. k-DNF learners and the CHARADE system are briefly presented in Section 3. Symbolic feature selection is covered in Section 4. <p> Therefore, we designed a suboptimal k-DNF learner called SCAR and used it to select relevant features to train other k-DNF learners. 4.1 Representational hypothesis Choosing a text representation depends on what is considered to be an elementary but meaningful component of textual data. For instance, Lewis <ref> [Lew92] </ref> did a comparison between various representations based on words, phrases, word clusters and phrase clusters. In the context of text categorization, his results showed that the best text representation was word-based. Following this recommendation, we choose a word-based representation of documents. <p> This list of categories differs from that used in [WPW95]. We decided to overlook stories without category assignment, since we could not possibly learn from them. This left us with 7789 learning and 3875 testing examples described by 22791 words provided by Lewis' representation <ref> [Lew92, p. 99] </ref>. 5.2 Evaluation measures In order to assess the performance of our approach, we consider two standard measures used 4 We used Lewis' transaction files [Lew92, p. 99], included with the Reuters dataset which can be obtained by anonymous ftp from /pub/reuters1 on ciir-ftp.cs.umass.edu. in information retrieval: recall and <p> This left us with 7789 learning and 3875 testing examples described by 22791 words provided by Lewis' representation <ref> [Lew92, p. 99] </ref>. 5.2 Evaluation measures In order to assess the performance of our approach, we consider two standard measures used 4 We used Lewis' transaction files [Lew92, p. 99], included with the Reuters dataset which can be obtained by anonymous ftp from /pub/reuters1 on ciir-ftp.cs.umass.edu. in information retrieval: recall and precision. Recall is the part of correct category assignments actually made out of the total number of assignments that should have been made.
Reference: [Lew95] <author> D. Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <booktitle> In Proceedings of SIGIR-95, </booktitle> <year> 1995. </year>
Reference-contexts: Graphical plots of recall versus precision curves can be summarized using the break-even point, i.e. the point where recall equals precision. Although this measure has been widely used in text categorization, we use the F fi -measure as a summary of categorization effectiveness <ref> [Lew95] </ref>: F fi = fi 2 P + R where R denotes recall, P precision and fi varies from 0 to infinity. Text categorization systems have usually been evaluated on the Reuters-22173 data set using the break-even point.
Reference: [LR94] <author> D. Lewis and M. Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1994. </year> <title> Text Categorization: a Symbolic Approach </title>
Reference-contexts: Symbolic feature selection is covered in Section 4. After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification <ref> [LR94] </ref>, decision tree construction [LR94, FHL + 91], rule induction [ADW94, MG95], neural networks [WPW95] and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. <p> Symbolic feature selection is covered in Section 4. After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction <ref> [LR94, FHL + 91] </ref>, rule induction [ADW94, MG95], neural networks [WPW95] and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. <p> Several recent text categorization studies have been evaluated on the Reuters financial corpus <ref> [LR94, WPW95, ADW94] </ref>. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. In the following, we present a unified view of text categorization methods, especially [LR94, Text Categorization: a Symbolic Approach ADW94, WPW95]. <p> Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. In the following, we present a unified view of text categorization methods, especially <ref> [LR94, Text Categorization: a Symbolic Approach ADW94, WPW95] </ref>. This unification is made possible because they all make use of induction to build categorizers. Even though inductive learners have been adapted for overlapping classes 1 , they are typically used for single-class prediction. <p> Next, two feature selection techniques are proposed: global and local filtering. While global filtering selects words according to their information on the whole corpus, local filtering restricts the selection to those words that occur with a given category. Local filtering is reported to be more efficient. In <ref> [LR94] </ref>, Lewis and Ringuette study two types of induction algorithms, Bayesian classifiers and decision trees. With a given text representation, boolean vectors, and a filtering method, a local selection using a mutual information criterion, they run several experiments on feature selection, as well as induction. <p> As mentioned in Section 2, earlier studies in text categorization (and more generally in information retrieval approaches) are based on the filter model, where feature selection is done in a preprocessing step. Mutual information <ref> [LR94] </ref>, relevancy score [WPW95] and frequency [ADW94] are instances of the filter model. However, this model is limited, since the effects of the selected feature subset on the performance of the induction algorithm cannot be predicted. The wrapper model is an approach to remedy this restriction. <p> For example, a word is removed from further consideration, when its conditional probability given the local category is not larger than its probability, given all other categories. We conclude by stressing SCAR most salient property. While the number of selected features is usually set by an external source (see <ref> [LR94] </ref> for instance) and remains the same for all categories, the proposed algorithm automatically tunes the number of features chosen per category: for example, a category, which can easily be separated from others, will require less features than frequently overlapping categories.
Reference: [Mar61] <author> M. E. Maron. </author> <title> Automatic indexing: an experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery, </journal> (8):404-417, 1961. 
Reference-contexts: Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments. A pioneering work to automate text categorization can be found in <ref> [Mar61] </ref>. The effectiveness of the hand-crafted, knowledge engineering system CONSTRUE [HW90] has set a landmark in text categorization research. CONSTRUE design, however, was rather expensive and time-consuming. ML approaches to classification (indeed, text categorization is a classification task) suggest the construction of decision means using induction over pre-classified samples.
Reference: [MG95] <author> I. Moulinier and J.-G. Ganascia. </author> <title> Confronting an existing machine learning algorithm to the text categorization task. </title> <booktitle> In Working notes, IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: Symbolic feature selection is covered in Section 4. After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction [LR94, FHL + 91], rule induction <ref> [ADW94, MG95] </ref>, neural networks [WPW95] and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus.
Reference: [Mic83] <author> R. Michalski. </author> <title> A theory and methodoly of inductive learning. </title> <journal> Artificial Intelligence, </journal> (20):111-161, 1983. 
Reference-contexts: Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ <ref> [Mic83] </ref>. Therefore we focus our attention on feature selection, a processing step that aims at reducing the size of the initial feature set in order to enable such learners to process large datasets. <p> Production rule learners, like Swap-1 [ADW94] or CHARADE [Gan91], are typical k-DNF learners. To complete the definition of k-DNF learners, we give an intuition of what a symbolic learner is. Ideally, a symbolic learner induces concepts from symbolic data, using symbolic operators <ref> [Mic83] </ref>. Symbolic operators are mainly derived from logical operators, like dropping a conjunct, adding a disjunct or climbing up a conceptual hierarchy. Symbolic data typically comprise nominal, ordered or hierarchical features as input features and logical formul or decision trees as output structure. <p> A generalizing term is generated from this subset; all examples that do not satisfy this term (a logical formula) are put into a pot, which is fed back to the inductive learner. The stopping criterion can be consistency, like in AQ <ref> [Mic83] </ref>, or some statistical validity measurement, as in CN2 [CN89]. The CHARADE algorithm, given in Figure 2, is closely related to these symbolic k-DNF learners. However, it differs in several ways that we briefly discuss. Let us call descriptor a pair hfeature, valuei. A description is a conjunction of descriptors.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 <ref> [Qui86] </ref> or AQ [Mic83]. Therefore we focus our attention on feature selection, a processing step that aims at reducing the size of the initial feature set in order to enable such learners to process large datasets. <p> These means are often presented as searching some hypothesis space. This search can be conducted using statistical and symbolic operators. For instance, decision trees (e.g. Quinlan's ID3 <ref> [Qui86] </ref> algorithm) are constructed using a divide and conquer strategy, that recursively divides the learning set until each subset has a unique class label. This division is often based on an entropy criterion, i.e. uses statistic information.
Reference: [RL94] <author> E. Riloff and W. Lehnert. </author> <title> Information extraction as a basis for high-precision text classification. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 296-333, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: There has been a recent outbreak of applications of text categorization, such as text extraction, filtering and routing to topic-specific processing mechanisms <ref> [HANS90, RL94] </ref>. On the other hand, text categorization has emerged as an application domain in machine learning (ML) [LC94, Coh95]. Indeed, manual categorization is known to be an expensive and time-consuming task, which results is dependent on variations in experts' judgments.
Reference: [WPW95] <author> E. Wiener, J. Pedersen, and A. Weigend. </author> <title> A neural network approach to topic spotting. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1995. </year>
Reference-contexts: Learning with tens of thousands of features has been achieved with regression methods <ref> [YC94, WPW95] </ref> or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83]. <p> After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction [LR94, FHL + 91], rule induction [ADW94, MG95], neural networks <ref> [WPW95] </ref> and k nearest neighbors [Yan94, CMSW92]. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. <p> Several recent text categorization studies have been evaluated on the Reuters financial corpus <ref> [LR94, WPW95, ADW94] </ref>. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. In the following, we present a unified view of text categorization methods, especially [LR94, Text Categorization: a Symbolic Approach ADW94, WPW95]. <p> Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus. In the following, we present a unified view of text categorization methods, especially <ref> [LR94, Text Categorization: a Symbolic Approach ADW94, WPW95] </ref>. This unification is made possible because they all make use of induction to build categorizers. Even though inductive learners have been adapted for overlapping classes 1 , they are typically used for single-class prediction. <p> The authors show that these two inductive techniques are subject to over-fitting, when the number of features increases. In the end, they report that decision trees slightly outperform Bayesian classifiers and are easier to interpret. Wiener, Pedersen and Weigend <ref> [WPW95] </ref> investigate text representation and induction using neural networks. They study term selection and Latent Semantic Indexing (LSI) as two approaches to dimensionality reduction in document representation. <p> As mentioned in Section 2, earlier studies in text categorization (and more generally in information retrieval approaches) are based on the filter model, where feature selection is done in a preprocessing step. Mutual information [LR94], relevancy score <ref> [WPW95] </ref> and frequency [ADW94] are instances of the filter model. However, this model is limited, since the effects of the selected feature subset on the performance of the induction algorithm cannot be predicted. The wrapper model is an approach to remedy this restriction. <p> Among 689 subjects of interest, including topics, places or company names, we worked on a set of 135 categories, that were provided with the corpus 4 . This list of categories differs from that used in <ref> [WPW95] </ref>. We decided to overlook stories without category assignment, since we could not possibly learn from them. <p> The performance of the learner, both in overall effectiveness and learning complexity, was enhanced. However, the accuracy reported in earlier experiments outperforms our effectiveness by several percents. Using a boolean representation, <ref> [WPW95] </ref> reported a break-even point of 77.5%, while under similar constraints, [ADW94] obtained 78,5%. <p> Finally, machine learning sometimes considers representation as additional knowledge, which is used to enhance learning. This has been been applied to text categorization to a smaller extent; for instance, using more complex representations <ref> [ADW94, WPW95] </ref> or hierarchical neural nets [WPW95] is a means of providing domain-knowledge to the learner. Our future work will follow this lead, in the context of machine learning. Adapting machine learning methods to text categorization provides means to evaluate both the strengths and weaknesses of ML approaches. <p> Finally, machine learning sometimes considers representation as additional knowledge, which is used to enhance learning. This has been been applied to text categorization to a smaller extent; for instance, using more complex representations [ADW94, WPW95] or hierarchical neural nets <ref> [WPW95] </ref> is a means of providing domain-knowledge to the learner. Our future work will follow this lead, in the context of machine learning. Adapting machine learning methods to text categorization provides means to evaluate both the strengths and weaknesses of ML approaches.
Reference: [Yan94] <author> Y. Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. </title> <booktitle> In Proceedings of SIGIR-94, </booktitle> <year> 1994. </year>
Reference-contexts: Learning with tens of thousands of features has been achieved with regression methods [YC94, WPW95] or k nearest neighbors learners <ref> [Yan94, CMSW92] </ref>. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83]. <p> After reporting some experimental results, we draw some conclusions and discuss future issues. 2 Related work Text categorization has been studied using various learning techniques including, for instance, Bayesian classification [LR94], decision tree construction [LR94, FHL + 91], rule induction [ADW94, MG95], neural networks [WPW95] and k nearest neighbors <ref> [Yan94, CMSW92] </ref>. Several recent text categorization studies have been evaluated on the Reuters financial corpus [LR94, WPW95, ADW94]. This corpus has thus become an interesting benchmark of comparison between learning algorithms; we ran our experiments on this corpus.
Reference: [YC94] <author> Y. Yang and C. G. Chute. </author> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transactions on Information Systems: </journal> <note> Special Issue on Text Categorization, 1994. Isabelle Moulinier, Gailius Raskinis, Jean-Gabriel Ganascia </note>
Reference-contexts: Learning with tens of thousands of features has been achieved with regression methods <ref> [YC94, WPW95] </ref> or k nearest neighbors learners [Yan94, CMSW92]. To our knowledge, there have been no such large-scale application of standard ML algorithms such as ID3 [Qui86] or AQ [Mic83].
References-found: 26

