URL: http://www-csag.cs.uiuc.edu/papers/dags94.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: The Illinois Concert System: Programming Support for Irregular Parallel Applications (Extended Abstract)  
Author: Andrew A. Chien flfl Julian Dolby 
Date: June 6, 1994  
Abstract: Irregular applications are critical to supporting grand challenge applications on massively parallel machines and extending the utility of those machines beyond the scientific computing domain. The dominant parallel programming models, data parallel and explicit message passing, provide little support for programming irregular applications. We articulate a set of requirements for supporting irregular computations on massively parallel machines: a shared object namespace, integrated task and data parallelism, dynamic thread creation, and aggressive compiler and runtime support. We believe that any successful programming tools for irregular applications must provide these services. The Illinois Concert System provides these essential services in a concurrent object-oriented programming model, supporting data abstraction and convenient expression of irregular concurrency. Concert also provides program development and debugging tools helpful for developing explicitly concurrent programs. The basic features of the Concert system, programming model and tools, are described. Two application programs are used as examples, highlighting the utility of the system support for irregular applications. Preliminary performance numbers are reported.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha and C. Hewitt. </author> <title> Object-Oriented Concurrent Programming, chapter on Actors: </title> <booktitle> A Conceptual Foundation for Concurrent Object-Oriented Programming, </booktitle> <pages> pages 49-74. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year> <month> 19 </month>
Reference-contexts: While many concurrent object-oriented programming systems have been built, several deserve specific mention because of their prominence. The basic Actor model, upon which concurrent object-oriented programming is based, was developed by Hewitt, Clinger and Agha <ref> [1] </ref>. The ABCL programming system pioneered many ideas on concurrent object-oriented programming [24]. The feature that distinguishes Concurrent Aggregates from ABCL is the support for data parallelism and its integration with task parallelism. pC++ is a well-known data parallel extension of C++ based on the aggregate model [15].
Reference: [2] <author> American National Standard Institute. </author> <title> Draft Standard for Information Systems Programming Language Fortran, volume 8 of SIGPLAN Special Interest Publication on Fortran. </title> <publisher> ACM Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: The data parallel model aligns data into large collections and operates on all elements of these collections in parallel. Extant examples include Fortran 90 <ref> [2] </ref>, CM Fortran [23], Fortran D [13], and of course High Performance Fortran (HPF). This programming model has been successful because it exposes the parallelism in regular problems, allowing it to be exploited by vector and parallel machines.
Reference: [3] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Limitations of the data parallel model are described above. More interesting is the combination of pC++ and Compositional C++ (CC++) <ref> [3] </ref> into a language called HPC++, supporting both task and data parallelism. Unfortunately, to our knowledge, no implementation of HPC++ yet exists. 2.2 Parallel Programming Tools We built a complete set of programming tools because few tools exist for true multiple-instruction multiple-data programming.
Reference: [4] <author> A. A. Chien and W. J. Dally. </author> <title> Concurrent Aggregates (CA). </title> <booktitle> In Proceedings of Second Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The programming model provides a shared object namespace, expression of both task and data parallelism, and dynamic thread creation. This allows the programmer to express parallelism conveniently and portably. We give a brief description of the programming model here, more detailed discussions can be found in <ref> [8, 9, 4] </ref>. 3.2 Programming Model The Concert programming model is embodied in the language, Concurrent Aggregates [9]. The language provides a simple concurrent object-oriented programming model. Program data is divided into objects, and the computation is expressed as operations on objects, much as in a C++ program. <p> Data parallelism is supported by aggregates (parallel collections of objects). Aggregates can be identified by a single name and distributed over multiple nodes in the machine. Though they can be used to implemented a general form of multi-access data abstractions <ref> [4] </ref>, a simpler use implements data-parallel collections. A dp-aggregate class can be defined, encapsulating data parallel operations, such as do-all, do-range, and do-sequential. Such a class is a standard user-level library; user-defined aggregates can inherit these data-parallel operations.
Reference: [5] <author> A. A. Chien, W. Feng, V. Karamcheti, and J. Plevyak. </author> <title> Techniques for efficient execution of fine-grained concurrent programs. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <pages> pages 103-13, </pages> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Other parts of the Concert system include an interpreter which supports incremental program development and debugging, a source-level symbolic debugger, and performance tools to evaluate parallel program performance. These tools are described in greater detail in the next section. More information on the Concert System can be found in <ref> [7, 18, 14, 19, 5] </ref>. 1 3.3 Concert Tools for Program Development and Tuning The Concert System supports the development of irregular parallel applications. <p> This comparable to the maximum floating performance achievable by sequential C++ programs. The Concert compiler performs aggressive type inference and inlining. However, the effectiveness of these optimizations is limited by locality and synchronization constraints. 3 As posited in <ref> [5, 7] </ref>, locality analysis and grain size tuning is essential to achieving higher levels of efficiency. At present the Concert system does not automatically do any locality optimization.
Reference: [6] <author> A. A. Chien, M. Straka, J. Dolby, V. Karamcheti, J. Plevyak, and X. Zhang. </author> <title> A case study in irregular parallel programming. </title> <booktitle> In DIMACS Workshop on Specification of Parallel Algorithms, </booktitle> <month> May </month> <year> 1994. </year> <note> Also available as Springer-Verlag LNCS. </note>
Reference-contexts: Unfortunately, this both reduces the modularity and clarity of the program, and incurs significant overhead in data movement to align data for parallel operation. We have described a study which exposed these shortcomings in <ref> [6] </ref>. The other popular programming model for massively-parallel machines (and networks of workstations) is explicit message passing and C or Fortran [20]. This can be achieved via a number of message passing substrates such as PVM [12], MPI [10], the Crystalline OS [11], or any number of proprietary messaging systems. <p> In Sections 4 and 5, we describe two example applications which require both task and data parallelism to achieve good performance. Systems that support data parallelism alone increase the programming effort and reduce performance <ref> [6] </ref> because the programmer must force the irregular application structure into the regular parallelism model. Task parallelism alone unnecessarily complicates the programming for programs which do conform to the data parallel model.
Reference: [7] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The concert system compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Technical Report UIUCDCS-R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Other parts of the Concert system include an interpreter which supports incremental program development and debugging, a source-level symbolic debugger, and performance tools to evaluate parallel program performance. These tools are described in greater detail in the next section. More information on the Concert System can be found in <ref> [7, 18, 14, 19, 5] </ref>. 1 3.3 Concert Tools for Program Development and Tuning The Concert System supports the development of irregular parallel applications. <p> This comparable to the maximum floating performance achievable by sequential C++ programs. The Concert compiler performs aggressive type inference and inlining. However, the effectiveness of these optimizations is limited by locality and synchronization constraints. 3 As posited in <ref> [5, 7] </ref>, locality analysis and grain size tuning is essential to achieving higher levels of efficiency. At present the Concert system does not automatically do any locality optimization.
Reference: [8] <author> Andrew A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Though explicit parallelism can increase programming difficulty, using an object-oriented model allows data abstraction to be used via modularity and information hiding to manage the complexity. Our concurrent object-oriented model is extended with aggregates, parallel collections of objects, which enable the convenient expression of data parallelism as task parallelism <ref> [8] </ref>. The underlying compiler and runtime are responsible for providing a global object namespace that spans the machine, distributing the parallel collections of objects, and scheduling the dynamically created threads on the nodes of the parallel machine. <p> The programming model provides a shared object namespace, expression of both task and data parallelism, and dynamic thread creation. This allows the programmer to express parallelism conveniently and portably. We give a brief description of the programming model here, more detailed discussions can be found in <ref> [8, 9, 4] </ref>. 3.2 Programming Model The Concert programming model is embodied in the language, Concurrent Aggregates [9]. The language provides a simple concurrent object-oriented programming model. Program data is divided into objects, and the computation is expressed as operations on objects, much as in a C++ program.
Reference: [9] <author> Andrew A. Chien, Vijay Karamcheti, John Plevyak, and Xingbin Zhang. </author> <title> Concurrent aggregates language report 2.0. </title> <note> Available via anonymous ftp from cs.uiuc.edu in /pub/csag or from http://www-csag.cs.uiuc.edu/, September 1993. </note>
Reference-contexts: The programming model provides a shared object namespace, expression of both task and data parallelism, and dynamic thread creation. This allows the programmer to express parallelism conveniently and portably. We give a brief description of the programming model here, more detailed discussions can be found in <ref> [8, 9, 4] </ref>. 3.2 Programming Model The Concert programming model is embodied in the language, Concurrent Aggregates [9]. The language provides a simple concurrent object-oriented programming model. Program data is divided into objects, and the computation is expressed as operations on objects, much as in a C++ program. <p> This allows the programmer to express parallelism conveniently and portably. We give a brief description of the programming model here, more detailed discussions can be found in [8, 9, 4]. 3.2 Programming Model The Concert programming model is embodied in the language, Concurrent Aggregates <ref> [9] </ref>. The language provides a simple concurrent object-oriented programming model. Program data is divided into objects, and the computation is expressed as operations on objects, much as in a C++ program. However, operations on objects can be invoked as sequential or concurrent, giving rise to task parallelism.
Reference: [10] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: We have described a study which exposed these shortcomings in [6]. The other popular programming model for massively-parallel machines (and networks of workstations) is explicit message passing and C or Fortran [20]. This can be achieved via a number of message passing substrates such as PVM [12], MPI <ref> [10] </ref>, the Crystalline OS [11], or any number of proprietary messaging systems. The basic idea is that concurrency is expressed as concurrent tasks which communicate and synchronize via message passing. These concurrent tasks can be written in a sequential language of your choice - C and Fortran are most popular.
Reference: [11] <author> G. Fox, et. al. </author> <title> Solving Problems on Concurrent Processors, volume I and II. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: The other popular programming model for massively-parallel machines (and networks of workstations) is explicit message passing and C or Fortran [20]. This can be achieved via a number of message passing substrates such as PVM [12], MPI [10], the Crystalline OS <ref> [11] </ref>, or any number of proprietary messaging systems. The basic idea is that concurrency is expressed as concurrent tasks which communicate and synchronize via message passing. These concurrent tasks can be written in a sequential language of your choice - C and Fortran are most popular. <p> While separate address spaces can be managed by the programmer (witness the number of irregular applications the Fox's group built using the Crystalline operating system <ref> [11] </ref>), the complexity of this task only complicates programming. Essentially, the programmer must anticipate all such remote references and encode send/receives to transfer the data. Not only is this very difficult in most irregular programs; the distributed address space implies that data cannot be accessed uniformly.
Reference: [12] <author> G. Geist and V. Sunderam. </author> <title> The pvm system: Supercomputer level concurrent computation on a heterogeneous network of workstations. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computers Conference, </booktitle> <pages> pages 258-61, </pages> <year> 1991. </year>
Reference-contexts: We have described a study which exposed these shortcomings in [6]. The other popular programming model for massively-parallel machines (and networks of workstations) is explicit message passing and C or Fortran [20]. This can be achieved via a number of message passing substrates such as PVM <ref> [12] </ref>, MPI [10], the Crystalline OS [11], or any number of proprietary messaging systems. The basic idea is that concurrency is expressed as concurrent tasks which communicate and synchronize via message passing.
Reference: [13] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for FORTRAN D on mimd distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: The data parallel model aligns data into large collections and operates on all elements of these collections in parallel. Extant examples include Fortran 90 [2], CM Fortran [23], Fortran D <ref> [13] </ref>, and of course High Performance Fortran (HPF). This programming model has been successful because it exposes the parallelism in regular problems, allowing it to be exploited by vector and parallel machines.
Reference: [14] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: Other parts of the Concert system include an interpreter which supports incremental program development and debugging, a source-level symbolic debugger, and performance tools to evaluate parallel program performance. These tools are described in greater detail in the next section. More information on the Concert System can be found in <ref> [7, 18, 14, 19, 5] </ref>. 1 3.3 Concert Tools for Program Development and Tuning The Concert System supports the development of irregular parallel applications.
Reference: [15] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: The ABCL programming system pioneered many ideas on concurrent object-oriented programming [24]. The feature that distinguishes Concurrent Aggregates from ABCL is the support for data parallelism and its integration with task parallelism. pC++ is a well-known data parallel extension of C++ based on the aggregate model <ref> [15] </ref>. Limitations of the data parallel model are described above. More interesting is the combination of pC++ and Compositional C++ (CC++) [3] into a language called HPC++, supporting both task and data parallelism.
Reference: [16] <editor> Lomdahl, et. al. </editor> <booktitle> 50 gflops molecular dynamics on the connection machine 5. In Proceedings of Supercomputing '93, </booktitle> <pages> page 520. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: We describe these in greater detail. Neighbor List Generation The neighbor list generation is the most time consuming and complex phase. The code in original CEDAR is quite inefficient. Therefore, we used an adaptation of the Multi-Cell algorithm <ref> [16] </ref> for this phase. A new neighbor list is generated every 50 iteration. In CEDAR and IC-CEDAR, neighbor list operates on the level of groups of atoms. Groups are clusters atoms (one to five) that together have a net neutral charge.
Reference: [17] <author> F. Muller-Plathe and D. Brown. </author> <title> Multi-colour algorithms in molecular simulation. </title> <journal> Computer Physics Communications, </journal> <volume> 64:7, </volume> <year> 1991. </year>
Reference-contexts: Both bond lengths and bond angles are constrained for water molecules. The SHAKE algorithm in CEDAR converges rapidly, but the algorithm is inherently sequential. To achieve good parallel speedup, a parallel algorithm for SHAKE is required. We use a multi-color algorithm to parallelize SHAKE <ref> [22, 17] </ref>. In this method constraints can be evaluated and corrected concurrently. In addition, the multi-color approach retains the advantage of always using the most up to date atom positions which assures rapid convergence (fewer iterations).
Reference: [18] <author> John Plevyak and Andrew Chien. </author> <title> Incremental inference of concrete types. </title> <type> Technical Report UIUCDCS-R-93-1829, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Other parts of the Concert system include an interpreter which supports incremental program development and debugging, a source-level symbolic debugger, and performance tools to evaluate parallel program performance. These tools are described in greater detail in the next section. More information on the Concert System can be found in <ref> [7, 18, 14, 19, 5] </ref>. 1 3.3 Concert Tools for Program Development and Tuning The Concert System supports the development of irregular parallel applications.
Reference: [19] <author> John Plevyak, Vijay Karamcheti, and Andrew Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Other parts of the Concert system include an interpreter which supports incremental program development and debugging, a source-level symbolic debugger, and performance tools to evaluate parallel program performance. These tools are described in greater detail in the next section. More information on the Concert System can be found in <ref> [7, 18, 14, 19, 5] </ref>. 1 3.3 Concert Tools for Program Development and Tuning The Concert System supports the development of irregular parallel applications.
Reference: [20] <author> D. Reed and R. Fujimoto. </author> <title> Multicomputer Networks: Message-based Parallel Processing. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: We have described a study which exposed these shortcomings in [6]. The other popular programming model for massively-parallel machines (and networks of workstations) is explicit message passing and C or Fortran <ref> [20] </ref>. This can be achieved via a number of message passing substrates such as PVM [12], MPI [10], the Crystalline OS [11], or any number of proprietary messaging systems. The basic idea is that concurrency is expressed as concurrent tasks which communicate and synchronize via message passing.
Reference: [21] <author> Daniel A. Reed, Ruth A. Aydt, Roger J. Noe, Philip C. Roth, Keith A. Shields, Bradley W. Schwartz, and Luis F. Tavera. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <editor> In Anthony Skjellum, editor, </editor> <booktitle> Proceedings of the Scalable Parallel Libraries Conference. IEEE Computer Society, </booktitle> <year> 1993. </year>
Reference-contexts: The Concert system provides a basic set of performance tools which allow programmers to profile and trace program execution. An application program can execute his program and collect traces of activity, memory usage, available work, etc. These statistics can be displayed using using the Pablo performance environment <ref> [21] </ref> or a number of publicly available graphical display tools. These tools provide essential feedback, allowing irregular applications to be tuned for both concurrency and load balance.
Reference: [22] <author> S. L. Lin, et. al. </author> <title> Molecular dynamics on a distributed-memory multiprocessor. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 13:1022, </volume> <year> 1992. </year>
Reference-contexts: Both bond lengths and bond angles are constrained for water molecules. The SHAKE algorithm in CEDAR converges rapidly, but the algorithm is inherently sequential. To achieve good parallel speedup, a parallel algorithm for SHAKE is required. We use a multi-color algorithm to parallelize SHAKE <ref> [22, 17] </ref>. In this method constraints can be evaluated and corrected concurrently. In addition, the multi-color approach retains the advantage of always using the most up to date atom positions which assures rapid convergence (fewer iterations).
Reference: [23] <author> Thinking Machines Corporation. </author> <title> Getting Started in CM Fortran, </title> <year> 1990. </year>
Reference-contexts: The data parallel model aligns data into large collections and operates on all elements of these collections in parallel. Extant examples include Fortran 90 [2], CM Fortran <ref> [23] </ref>, Fortran D [13], and of course High Performance Fortran (HPF). This programming model has been successful because it exposes the parallelism in regular problems, allowing it to be exploited by vector and parallel machines.
Reference: [24] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: While many concurrent object-oriented programming systems have been built, several deserve specific mention because of their prominence. The basic Actor model, upon which concurrent object-oriented programming is based, was developed by Hewitt, Clinger and Agha [1]. The ABCL programming system pioneered many ideas on concurrent object-oriented programming <ref> [24] </ref>. The feature that distinguishes Concurrent Aggregates from ABCL is the support for data parallelism and its integration with task parallelism. pC++ is a well-known data parallel extension of C++ based on the aggregate model [15]. Limitations of the data parallel model are described above.
References-found: 24

