URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/review.text.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract-found: 0
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> 1994. </year> <note> "Hints." Submitted to Neural Computation. </note>
Reference: <author> Akaike, H. </author> <year> 1974. </year> <title> "A New Look at the Statistical Model Identiflcation." </title> <journal> IEEE Trans. Auto. Control 19: </journal> <pages> 716-723. </pages>
Reference: <author> Bishop, C. M. </author> <year> 1994. </year> <title> "Training with Noise is equivalent to Tikhonov Regularization." </title> <note> Accepted for publication in Neural Computation. </note>
Reference: <author> Beck, C. </author> <year> 1990. </year> <title> "Upper and Lower Bounds on the Renyi Dimensions and the Uniformity of Multifractals." </title> <journal> Physica D 41: </journal> <pages> 67-78. </pages>
Reference: <author> Bonnlander, B. V., and A. S. Weigend. </author> <year> 1994. </year> <title> "Selecting Input Variables Using Kernel Density Estimation." </title> <booktitle> In Proceedings of the International Symposium on Artiflcial Neur al Networks (ISANN'94), </booktitle> <address> Tainan, Taiwan, R.O.C. </address>
Reference-contexts: Computationally more expensive but more data e-cient are methods that use kernels to estimate Time Series Analysis and Prediction 21 information theoretic quantities; we have used them in the context of time series prediction and information theory for the selection of the most relevant subset of inputs <ref> (Bonnlander & Weigend, 1994) </ref>. <p> There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection <ref> (Bonnlander & Weigend, 1994) </ref>, wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when
Reference: <author> Box, G. E. P., and F. M. Jenkins. </author> <year> 1976. </year> <title> Time Series Analysis: Forecasting and Control, 2nd ed. </title> <address> Oakland, CA: </address> <publisher> Holden-Day. </publisher>
Reference-contexts: If the input to a MA model is an impulse (which has a at power spectrum), the discrete Fourier transform of the output is given by N X b n exp (i2nf ) (2) 6 Andreas S. Weigend <ref> (see, for example, Box & Jenkins, 1976, p.69) </ref>. <p> Here, however|due to the feedback coupling of previous steps|we obtain a set of linear equations rather than just a single equation for each autocorrelation coe-cient. By multiplying Eq. (6) by x t , taking expectation values, and normalizing <ref> (see Box & Jenkins, 1976, p.54) </ref>, Time Series Analysis and Prediction 7 the autocorrelation coe-cients of an AR model are found by solving this set of linear equations, traditionally called the Yule-Walker equations, = m=1 Unlike the MA case, the autocorrelation coe-cient need not vanish after M steps.
Reference: <author> Bradley, E. </author> <year> 1992. </year> <title> "Taming Chaotic Circuits." </title> <type> Ph.D. Thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference: <author> Brown, R., P. Bryant, and H. D. I. Abarbanel. </author> <year> 1991. </year> <title> "Computing the Lyapunov Spectrum of a Dynamical System from an Observed Time Series." </title> <journal> Phys. Rev. </journal> <volume> A 43: </volume> <pages> 2787-2806. </pages>
Reference-contexts: They can be found from the Jacobian (either directly, if the Jacobian is known, or indirectly from an emulation of the system by a neural network, for example), or by following trajectories <ref> (Brown, Bryant, & Abarbanel, 1991) </ref>. Diverging trajectories reveal information about the system which is initially hidden by the measurement quantization. The amount of this information is proportional to the expansion rate of the volume, which is given by the sum of the positive exponents.
Reference: <author> Cacciatore, T. W., and S. J. Nowlan. </author> <year> 1994. </year> <title> "Mixtures of Controllers for Jump Linear and Nonlinear Plants." </title> <booktitle> In Advances in Neural Information Processing Systems 6 (NIPS*93), </booktitle> <editor> edited by J. D. Cowen, G. Tesauro, and J. Alspector, </editor> <address> 719-726. San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Casdagli, M. C. </author> <year> 1991. </year> <title> "Chaos and Deterministic versus Stochastic Nonlinear Modeling." </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 54: </volume> <month> 303-328. </month> <title> Time Series Analysis and Prediction 51 Casdagli, </title> <editor> M., S. Eubank, J. D. Farmer, and J. Gibson. </editor> <year> 1991. </year> <title> "State Space Reconstruction in the Presence of Noise." </title> <journal> Physica D 51D: </journal> <pages> 52-98. </pages>
Reference: <author> Casdagli, M. C., and A. S. Weigend. </author> <year> 1994. </year> <title> "Exploring the Continuum Between Deterministic and Stochastic Modeling." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 347-366. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Chaitin, G. J. </author> <year> 1966. </year> <title> "On the Length of Programs for Computing Finite Binary Sequences." </title> <journal> J. Assoc. Comp. Mach. </journal> <volume> 13: </volume> <pages> 547-569. </pages>
Reference: <author> Chaitin, G. J. </author> <year> 1990. </year> <title> Information, Randomness & Incompleteness. </title> <booktitle> Series in Computer Science, </booktitle> <volume> Vol. 8, </volume> <editor> 2nd ed. </editor> <address> Singapore: World-Scientiflc. </address>
Reference-contexts: In algorithmic information theory, information is measured within a single string of symbols by the number of bits needed to specify the shortest algorithm that can generate them. This has led to signiflcant extensions of the results by Godel (1931) and Turing (1936) <ref> (see Chaitin, 1990) </ref> and, through the Minimum Description Length principle, it has been used as the basis for a general theory of statistical inference (Wallace & Boulton, 1968; Rissanen, 1987).
Reference: <author> Chatfleld, C. </author> <year> 1989. </year> <title> The Analysis of Time Series, 4th ed. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1989. </year>
Reference: <author> Cover, T. M., and J. A. Thomas. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: We close this section by bridging back to the section on information theory. In Section 3.2, we established the source entropy rate of a series as the lower bound for any model. For a linear model, the entropy rate is given by <ref> (Cover & Thomas, 1991, p. 274) </ref>: 2 1 Z log S ()d ; (46) where S () is the power spectral density; i.e., the Fourier transform of the autocorrelation function (see Section 2.1 for a discussion why a linear model is completely described by its spectrum or, equivalently, by its autocorrelation <p> However, there can be no universal algorithm to flnd the shortest program to generate an observed sequence because we cannot determine whether an arbitrary candidate program will continue to produce symbols or will halt <ref> (e.g., see Cover & Thomas, 1991, p.162) </ref>. Although there are deep theoretical limitations on time series prediction, the constraints associated with speciflc domains of application can nevertheless permit strong useful results, and can leave room for signiflcant future development.
Reference: <author> Drucker, H. , C. Cortes, L. D. Jackel, Y. LeCun, V. Vapnik. </author> <year> 1994. </year> <title> "Boosting and Other Machine Learning Algorithms." </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference (ML'94), Rutgers, </booktitle> <address> NJ, </address> <publisher> edited by W. W. </publisher>
Reference-contexts: There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set <ref> (Drucker et al., 1994) </ref>, variable subset selection (Bonnlander & Weigend, 1994), wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory,
Reference: <author> Cohen, and H. </author> <booktitle> Hirsh, </booktitle> <pages> 53-61. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufimann. </publisher>
Reference: <author> Engle, R. F., and C. W. J. Granger. </author> <year> 1987. </year> <title> "Cointegration and Error-Correcting Representation, Estimation and Testing." </title> <type> Econometrica 55: </type> <pages> 251-276. </pages>
Reference-contexts: First, taking additional information into account can make the problem appear more stationary. Second, in some cases, there are sets of variables that wander as a group. In the econometric literature, this property is called cointegration <ref> (Engle & Granger, 1987) </ref>.
Reference: <author> Farmer, J. D., and J. J. Sidorowich. </author> <year> 1987. </year> <title> "Predicting Chaotic Time Series." </title> <journal> Phys. Rev. Lett. </journal> <volume> 59(8): </volume> <pages> 845-848. </pages>
Reference: <author> Farmer, J. D., and J. J. Sidorowich. </author> <year> 1988. </year> <title> "Exploiting Chaos to Predict the Future and Reduce Noise." Evolution, Learning, and Cognition, edited by Y. </title> <editor> C. Lee. </editor> <publisher> Singapore: World Scientiflc. </publisher>
Reference: <author> Fraser, A. M., and H. L. Swinney. </author> <year> 1986. </year> <title> "Independent Coordiates for Strange At-tractors from Mutual Information." </title> <journal> Phys. Rev. </journal> <volume> A 33: </volume> <pages> 1134-1140. </pages>
Reference-contexts: Equal-probability data structures can be used (at the expense of computational complexity) to generate more reliable unbiased entropy estimates <ref> (Fraser & Swinney, 1986) </ref>.
Reference: <author> Fraser, A. M. </author> <year> 1989. </year> <title> "Information and Entropy in Strange Attractors." </title> <journal> IEEE Trans. Info. Theory IT-35: </journal> <pages> 245-262. </pages>
Reference: <author> Friedman, J. H. </author> <year> 1991. </year> <title> "Multivariate Adaptive Regression Splines." </title> <journal> Ann. Stat. </journal> <volume> 19: </volume> <pages> 1-142. </pages> <note> With discussion. </note>
Reference: <author> Geman, S., E. Bienenstock, and R. Doursat. </author> <year> 1992. </year> <title> "Neural Networks and the Bias/ Variance Dilemma." </title> <booktitle> Neural Computation 5: </booktitle> <pages> 1-58. </pages>
Reference-contexts: Let us consider noisy observations. It is likely that we are able to improve the prediction quality by averaging over the continuation of a few nearest neighbors (rather than only taking the closest one). Here a trade-ofi important in all statistical modeling appears: the bias-variance dilemma <ref> (e.g., Geman, Bienenstock, & Doursat, 1992) </ref>. On the one hand, if we take too few neighbors into account, our predictions will still be very noisy (i.e., have a large variance). At the same time, they are very exible (i.e., have a small bias).
Reference: <author> Gencay, R., and W. D. Dechert. </author> <year> 1992. </year> <title> "An Algorithm for the n Lyapunov Exponents of an n-Dimensional Unknown Dynamical System." </title> <journal> Physics D 59 </journal> <note> 142-157. </note> <author> 52 Andreas S. Weigend Gershenfeld, N. A. </author> <year> 1989. </year> <title> "An Experimentalist's Introduction to the Observation of Dynamical Systems." In Directions in Chaos, edited by B.-L. </title> <journal> Hao, </journal> <volume> Vol. 2, </volume> <pages> 310-384. </pages> <address> Singapore: </address> <publisher> World Scientiflc. </publisher>
Reference: <author> Gershenfeld, N. A. </author> <year> 1993. </year> <title> "Information in Dynamics." </title> <booktitle> In Proceedings of the Workshop on Physics of Computation, edited by D. Matzke, </booktitle> <pages> 276-280. </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Press. </publisher>
Reference-contexts: figure indicates that three past values are sufficient to retrieve most of the predictable structure and that the system has lost the memory of its initial conditions after roughly 100 steps. e-ciently computed with an O (N ) algorithm by sorting the measured values on a simple flxed-resolution binary tree <ref> (Gershenfeld, 1993) </ref>. 10,000 data points were used, sorted on the three most signiflcant bits. Note that three lags su-ce to retrieve most of the predictable structure. This is in agreement with Figure 3, where the 2 + 1 dimensions plotted appear to be su-cient for an embedding.
Reference: <author> Gershenfeld, N. A., and A. S. Weigend. </author> <year> 1994. </year> <title> "The Future of Time Series: Learning and Understanding." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 1-70. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The results of the competition and of the follow-up NATO Advanced Research Workshop are collected in a book <ref> (Weigend & Gershenfeld, eds., 1994) </ref>. FIGURE 1 Examples of some time series. The first series, the laser data, is used as example for all models discussed in this article. [1] The data sets are available in the directory ftp://ftp.cs.colorado.edu/pub/Time-Series/SantaFe, or via http://www.cs.colorado.edu/andreas/TSWelcome.html. 4 Andreas S.
Reference: <author> Godel, K. </author> <year> 1931. </year> " <title> Uber formal unentscheidbare Satze der Principia Mathematica und verwandter Systeme, </title> <type> I." </type> <institution> Monatshefte fur Mathematik und Physik 38: </institution> <month> 173-198. </month> <title> An English translation of this paper is found in On Formally Undecidable Propositions by K. </title> <address> Godel (New York: </address> <publisher> Basic Books, </publisher> <year> 1962). </year>
Reference: <author> Granger, C. W. J., and A. P. Andersen. </author> <year> 1978. </year> <title> An Introduction to Bilinear Time Series Models. </title> <type> Gottingen: </type> <institution> Vandenhoek and Ruprecht. </institution>
Reference: <author> Granger, C. W. J., and T. Terasvirta. </author> <year> 1993. </year> <title> Modelling Nonlinear Economic Relationships. </title> <publisher> Oxford, UK: Oxford University Press. </publisher>
Reference: <author> Grassberger, P. </author> <year> 1988. </year> <title> "Finite Sample Corrections to Entropy and Dimension Estimates." </title> <journal> Phys. Lett A 128: </journal> <pages> 369-373. </pages>
Reference-contexts: D 1 is a constant 1: Therefore, the dependence of H 1 on N provides information about the resolution of the observable. (D 1 is called the [9] Note that there can be corrections to such estimates if one is interested in the expectation value of functions of the probability <ref> (Grassberger, 1988) </ref>.
Reference: <author> Grebogi, C., S. M. Hammel, J. A. Yorke, and T. Sauer. </author> <year> 1990. </year> <title> "Shadowing of Physical Trajectories in Chaotic Dynamics: Containment and Reflnement." </title> <journal> Phys. Rev. Lett. </journal> <volume> 65: </volume> <pages> 1527. </pages>
Reference: <author> Guillemin, V., and A. Pollack. </author> <year> 1974. </year> <title> Difierential Topology. </title> <address> Englewood Clifis, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This last restriction may be unfamiliar, but it is surprisingly unimportant: the embedded data su-ce as a representation for characterizing the essential features of the dynamics that produced the time series, as well as for predicting its future behavior. [8] The Whitney embedding theorem from the 1930s <ref> (see Guillemin & Pollack, 1974, p. 48) </ref> guarantees that the number of independent observations d required to embed an arbitrary manifold (in the absence of noise) into a Euclidean embedding space will be no more than twice the dimension of the manifold.
Reference: <author> Gutowitz, H., ed. </author> <year> 1991. </year> <title> Cellular Automata, Theory and Experiment. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The transition to turbulence in a large aspect-ratio convection cell is an experimental example of a spatio-temporal structure (Swinney, 1994), and cellular automata and coupled map lattices have been extensively investigated to explore the theoretical relationship between temporal and spatial ordering <ref> (Gutowitz, 1991) </ref>. A promising approach is to extend time series embedding (in which the task is to flnd a rule to map past observations to future values) to spatial embedding (which aims to flnd a map between one spatial, or spatio-temporal, region and another).
Reference: <author> Hentschel, H. G. E., and I. Procaccia. </author> <year> 1983. </year> <title> "The Inflnite Number of Generalized Dimensions of Fractals and Strange Attractors." </title> <journal> Physica D 8: </journal> <pages> 435-444. </pages>
Reference: <author> Hu, M. J. C. </author> <year> 1964. </year> <title> "Application of the Adaline System to Weather Forecasting." </title> <publisher> E. </publisher>
Reference: <author> E. </author> <type> Degree Thesis. Technical Report 6775-1, </type> <institution> Stanford Electronic Laboratories, Stanford University. </institution>
Reference: <author> Hubler, A. </author> <year> 1989. </year> <title> "Adaptive Control of Chaotic Systems." </title> <journal> Helv. Phys. </journal> <volume> Acta 62: </volume> <pages> 343-346. </pages>
Reference: <author> Hubner, U., C. O. Weiss, N. B. Abraham, and D. Tang. </author> <year> 1994. </year> <title> "Lorenz-Like Chaos in NH 3 -FIR Lasers." In Time Series Prediction: Forecasting the Future and Time Series Analysis and Prediction 53 Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 73-104. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Since both approaches are data-driven (rather than theory-driven), we illustrate the ideas presented in this article with a "real-world" data set, recorded from a far-infrared laser in chaotic state. It was used in the Santa Fe Times Series Prediction and Analysis Competition as Data Set A <ref> (Hubner et al., 1994) </ref>.
Reference: <author> Jacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <year> 1991. </year> <title> "Adaptive Mixtures of Local Experts." </title> <booktitle> Neural Computation 3: </booktitle> <pages> 79-87. </pages>
Reference-contexts: Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output (Weigend & Zimmermann, 1994). Experts for prediction. A Gaussian mixture model <ref> (Jacobs et al., 1991) </ref> can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics. After learning, the sub-networks identify the difierent dynamical regimes.
Reference: <author> Kazlas, P. T., and A. S. Weigend. </author> <year> 1995. </year> <title> "Settling Temporal Difierences: Time Series Prediction Using TD()." </title> <booktitle> In Advances in Neural Information Processing Systems 7 (NIPS*94). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More modestly|narrowing the scope to neural networks for time series prediction| some examples of recent work are: Temporal Difierence learning (TD). The TD algorithm (Sutton, 1988) has traditionally been used for Markovian and game-like situations (Tesauro, 1992). We applied TD learning to the prediction of real-value time series <ref> (such as the laser data, Kazlas & Weigend, 1995) </ref>. In some cases the network trained with TD (where the target is the prediction made at the next time step) outperforms the network trained with standard supervised learning (where the target is always the observed value). Cleaning the data.
Reference: <author> Kevrekidis, I. G., R. Rico-Martinez, R. E. Ecke, R. M. Farber, and A. S. La-pedes. </author> <year> 1993. </year> <title> "Global Bifurcations in Rayleigh-Benard Convection." </title> <type> Los Alamos preprint, </type> <institution> LA-UR-93-2922, </institution> <month> May </month> <year> 1993. </year> <note> Submitted to Physica D. </note>
Reference-contexts: A parametrized network allows us to characterize the system in terms of its qualitative behavior <ref> (Kevrekidis et al., 1993) </ref>. Will it be possible to prove a theorem similar to Takens' theorem, but not for past values of the observable but for past values of the parameters? Controlling nonlinear systems.
Reference: <author> Kolmogorov, A. </author> <year> 1941. </year> <title> "Interpolation und Extrapolation von stationaren zufalligen Folgen." </title> <journal> Bull. Acad. Sci. </journal> <volume> (Nauk) 5: </volume> <pages> 3-14. </pages> <address> U.S.S.R., </address> <publisher> Ser. Math. </publisher>
Reference: <author> Kolmogorov, A. N. </author> <year> 1965. </year> <title> "Three Approaches to the Quantitative Deflnition of Information." </title> <journal> Prob. Infor. Trans. </journal> <volume> 1: </volume> <pages> 4-7. </pages>
Reference: <author> Landauer, R. </author> <year> 1991. </year> <title> "Information is Physical." </title> <journal> Physics Today 44: </journal> <volume> 23. </volume>
Reference-contexts: Weigend Although the connection between information theory and ergodic theory has long been appreciated, Shaw (1981) helped point out the connection between dissipative dynamics and information theory, and Fraser and Swinney (1986) flrst used information-theoretic measures to flnd optimal embedding lags. This example of the physical meaning of information <ref> (Landauer, 1991) </ref> can be viewed as an application of information theory back to its roots in dynamics: Shannon (1948) built his theory of information on the analysis of the single-molecule Maxwell Demon by Szilard in 1929, which in turn was motivated by Maxwell and Boltzmann's efiort to understand the microscopic dynamics
Reference: <author> Lang, K. J., A. H. Waibel, and G. E. Hinton. </author> <year> 1990. </year> <title> "A Time-Delay Neural Network Architecture for Isolated Word Recognition." </title> <booktitle> Neural Networks 3: </booktitle> <pages> 23-43. </pages>
Reference: <author> Lapedes, A., and R. Farber. </author> <year> 1987. </year> <title> "Nonlinear Signal Processing Using Neural Networks." </title> <type> Technical Report No. </type> <institution> LA-UR-87-2662, Los Alamos National Laboratory, Los Alamos, NM. le Cun, Y. </institution> <year> 1989. </year> <title> "Generalization and Network Design Strategies." In Connectionism in Perspective, edited by R. </title> <editor> Pfeifer, Z. Schreter, F. Fogelman, and L. Steels. </editor> <publisher> Amsterdam: North Holland. </publisher> <editor> le Cun, Y., J. S. Denker, and S. A. Solla. </editor> <year> 1990. </year> <title> "Optimal Brain Damage." </title> <booktitle> In Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <editor> edited by D. S. Touretzky, </editor> <address> 598-605. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lewis, P. A. W., B. K. Ray, and J. G. Stevens. </author> <year> 1994. </year> <title> "Modeling Time Series Using Multivariate Adaptive Regression Splines (MARS)." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <publisher> S. </publisher>
Reference: <author> Weigend and N. A. </author> <month> Gershenfeld, </month> <pages> 296-318. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Lorenz, E. N. </author> <year> 1963. </year> <title> "Deterministic Non-Periodic Flow." </title> <journal> J. Atmos. Sci. </journal> <volume> 20: </volume> <pages> 130-141. </pages>
Reference-contexts: For example, in some regimes the inflnite physical degrees of freedom of a convect-ing uid reduce to a small set of coupled ordinary difierential equations for a mode expansion <ref> (Lorenz, 1963) </ref>.
Reference: <author> Lorenz, E. N. </author> <year> 1989. </year> <title> "Computational Chaos|A Prelude to Computational Instability." </title> <journal> Physica D 35: </journal> <pages> 299-317. </pages>
Reference: <author> Lutkepohl, H., and A. S. Weigend. </author> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: In the worst case (i.e., if the VAR matrix is an identity matrix) this reduces to training the network on the individual returns. Any improvement beyond this standard approach is due to the exploitation of the cointegration relation between the price series <ref> (Lutkepohl & Weigend, 1994) </ref>. Recurrent networks. In this article, we did not cover recurrent networks. One important advantage they have over feed-forward networks is that they can recover state in the present of noise, similar to a Hidden Markov Model.
Reference: <author> Mangeas, M., and A. S. Weigend. </author> <year> 1994. </year> <title> "Experts for Prediction." In preparation, </title> <institution> Computer Science Department, University of Colorado at Boulder. </institution> <note> 54 Andreas S. Weigend May, </note> <author> R. M. </author> <year> 1976. </year> <title> "Simple Mathematical Models with Very Complicated Dynamics." </title> <booktitle> Nature 261: </booktitle> <pages> 459. </pages>
Reference-contexts: Inspired by Cacciatore and Nowlan (1994), we gave the gating network some recurrence, acting as capacitance to match the expected time scale of the gating output <ref> (Mangeas & Weigend, 1994) </ref>. Multivariate and non-stationary time series. Throughout this article, we have considered only univariate time series, i.e., a single variable as a function of time.
Reference: <author> Miller, W. T., R. S. Sutton, and P. J. Werbos, eds. </author> <year> 1990. </year> <title> Neural Networks for Control. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Moody, J. </author> <year> 1994. </year> <title> "Prediction Risk and Architecture Selection for Neural Networks." In From Statistics to Neural Networks: Theory and Pattern Recognition Applications, edited by V. Cherkassky, </title> <editor> J. H. Friedman and H. Wechsler, </editor> <booktitle> NATO ASI Series F, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Moore, C. </author> <year> 1991. </year> <title> "Generalized Shifts: Unpredictability and Undecidability in Dynamical Systems." </title> <type> Nonlinearity 4: </type> <pages> 199-230. </pages>
Reference-contexts: This map can be implemented in a simple physical system consisting of a classical billiard ball and reecting surfaces, where the x t are the successive positions at which the ball crosses a given line <ref> (Moore, 1991) </ref>. Both systems are completely deterministic (their evolutions are entirely determined by the initial condition x 0 ), yet they can easily generate time series with broadband power spectra.
Reference: <author> Mozer, M. C. </author> <year> 1994. </year> <title> "Neural Net Architectures for Temporal Sequence Processing." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 243-264. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Narendra, K. S., and S.-M. Li. </author> <year> 1995. </year> <title> "Neural Networks in Control Systems." In Mathematical Perspective on Neural Networks, edited by P. </title> <editor> Smolensky, M. C. Mozer, and D. E. Rumelhart. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Nix, D. A., and A. S. Weigend. </author> <year> 1995. </year> <title> "Local Error Bars for Nonlinear Regression and Time Series Prediction." </title> <booktitle> In Advances in Neural Information Processing Systems 7 (NIPS*94). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is often desirable and important to also know how sure we can be about a prediction. At the Santa Fe competition, none of the entries estimated the error bars (required for the laser data) in a principled way. Before presenting our solution to the problem <ref> (Nix & Weigend, 1995) </ref>, we make explicit the three assumptions that minimizing sum squared errors implies in a maximum likelihood framework (which we need not accept): 1. The errors of difierent data points are independent of each other.
Reference: <author> Nowlan, S. J., and G. E. Hinton. </author> <year> 1992. </year> <title> "Simplifying Neural Networks by Soft Weight-Sharing." </title> <booktitle> Neural Computation 4: </booktitle> <pages> 473-493. </pages>
Reference-contexts: Any such sum over individual weights does not take interactions between the weights into account.) The sunspots time series has served as an example for a number of algorithms trying to produce small networks, e.g., soft weight-sharing <ref> (Nowlan & Hinton, 1992) </ref>, and optimal brain damage (developed by le Cun, Denker, & Solla, 1990, and applied to the sunspots series by Svarer, Hansen, & Larsen, 1993). The latter method is an example of deleting unimportant weights (pruning) based on the Hessian.
Reference: <author> Nychka, D., S. Ellner, D. McCafirey, and A. R. Gallant. </author> <year> 1992. </year> <title> "Finding Chaos in Noisy Systems." </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 54(2): </volume> <pages> 399-426. </pages>
Reference: <author> Oppenheim, A. V., and R. W. Schafer. </author> <year> 1989. </year> <title> Discrete-Time Signal Processing. </title> <address> En-glewood Clifis, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: The next step in complexity is to allow both AR and MA parts in the model; this is called an ARMA (M; N ) model: x t = m=1 N X b n e tn : (10) Its output is most easily understood in terms of the z-transform <ref> (Oppenheim & Schafer, 1989) </ref>, which generalizes the discrete Fourier transform to the complex plane: X (z) t=1 On the unit circle, z = exp (i2f ), the z-transform reduces to the discrete Fourier transform.
Reference: <author> Ott, E., C. Grebogi, and J. A. Yorke. </author> <year> 1990. </year> <title> "Controlling Chaos." </title> <journal> Phys. Rev. Lett. </journal> <volume> 64: </volume> <pages> 1196. </pages>
Reference: <author> Packard, N. H., J. P. Crutchfeld, J. D. Farmer, and R. S. Shaw. </author> <year> 1980. </year> <title> "Geometry from a Time Series." </title> <journal> Phys. Rev. Lett. </journal> <volume> 45(9): </volume> <pages> 712-716. </pages>
Reference: <author> Palus, M. </author> <year> 1994. </year> <title> "Identifying and Quantifying Chaos Using Information-Theoretic Functionals." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 387-413. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Perrone, M. P. </author> <year> 1994. </year> <title> "General Averaging Results for Complex Optimization." </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <editor> edited by M. C. </editor> <title> Time Series Analysis and Prediction 55 Mozer, </title> <editor> P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. </editor> <booktitle> Weigend, </booktitle> <pages> 364-371. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The interested reader is referred to the review by Mozer (1994) that provides a unifled treatment and classiflcation scheme for recurrent networks. There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts <ref> (Perrone, 1994) </ref>, "boosting" of the training set (Drucker et al., 1994), variable subset selection (Bonnlander & Weigend, 1994), wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series
Reference: <author> Petersen, K. </author> <year> 1989. </year> <title> Ergodic Theory, </title> <booktitle> 2nd ed. Cambridge Studies in Advanced Mathematics, </booktitle> <volume> Vol. </volume> <pages> 2. </pages> <address> Cambridge, MA: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: resolution: h (; N ) = lim lim H d (; N ) H d1 (; N ) : (24) The limit of inflnite resolution is usually not needed in practice: the source entropy reaches its maximum asymptotic value once the resolution is su-ciently flne to produce a generating partition <ref> (Petersen, 1989, p. 243) </ref>.
Reference: <author> Pi, H., and C. Peterson. </author> <year> 1994. </year> <title> "Finding the Embedding Dimension and Variable Dependences in Time Series." </title> <booktitle> Neural Computation 6: </booktitle> <pages> 509-520. </pages>
Reference-contexts: used to generate Figure 4 is related to the frequently rediscovered fact that box-counting algorithms (such as are needed for estimating dimensions and entropies) can be implemented in high-dimensional space with an O (N log N ) algorithm requiring no auxiliary storage by sorting the appended indices of lagged vectors <ref> (Pineda & Sommerer, 1994) </ref>. Equal-probability data structures can be used (at the expense of computational complexity) to generate more reliable unbiased entropy estimates (Fraser & Swinney, 1986).
Reference: <author> Pineda, F. J., and J. C. Sommerer. </author> <year> 1994. </year> <title> "Estimating Generalized Dimensions and Choosing Time Delays: A Fast Algorithm." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 367-385. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: used to generate Figure 4 is related to the frequently rediscovered fact that box-counting algorithms (such as are needed for estimating dimensions and entropies) can be implemented in high-dimensional space with an O (N log N ) algorithm requiring no auxiliary storage by sorting the appended indices of lagged vectors <ref> (Pineda & Sommerer, 1994) </ref>. Equal-probability data structures can be used (at the expense of computational complexity) to generate more reliable unbiased entropy estimates (Fraser & Swinney, 1986).
Reference: <author> Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <year> 1992. </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientiflc Computing, 2nd ed. </booktitle> <address> Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: ARMA models have dominated all areas of time series analysis and discrete-time signal processing for more than half a century. For example, in speech recognition and synthesis, Linear Predictive Coding <ref> (Press et al., 1992, p. 571) </ref> compresses speech by transmitting the slowly varying coe-cients for a linear model (and possibly the remaining error between the linear forecast and the desired signal) rather than the original signal.
Reference: <author> Prichard, D., and J. Theiler. </author> <year> 1995. </year> <title> Generalized Redundancies for Time Series Analysis. </title> <journal> Physica D, </journal> <note> in press. </note>
Reference: <author> Priestley, M. </author> <year> 1981. </year> <title> Spectral Analysis and Time Series. </title> <publisher> London: Academic Press. </publisher>
Reference: <author> Principe, J. C., B. de Vries, and P. Oliveira. </author> <year> 1993. </year> <title> "The Gamma Filter|A New Class of Adaptive IIR Filters with Restricted Feedback." </title> <journal> IEEE Trans. Sig. Proc. </journal> <volume> 41: </volume> <pages> 649-656. </pages>
Reference-contexts: In the connectionist community, the term "teacher forcing" is used when the inputs are set to the true values, and the term "trajectory learning" <ref> (Principe, de Vries, & Oliveira, 1993) </ref> when the predicted output is fed back to the inputs. 36 Andreas S. Weigend since its longer prediction time will be closer to the dynamics of the system.
Reference: <author> Rico-Martinez, R., I. G. Kevrekidis, and R. A. Adomaitis. </author> <year> 1993. </year> <title> "Noninvertibility in Neural Networks." </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco (ICNN), 382-386. Piscataway, NJ: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Rissanen, J. </author> <year> 1986. </year> <title> "Stochastic Complexity and Modeling." </title> <journal> Ann. Stat. </journal> <volume> 14: </volume> <pages> 1080-1100. </pages>
Reference: <author> Rissanen, J. </author> <year> 1987. </year> <title> "Stochastic Complexity." </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 49: </volume> <pages> 223-239. </pages> <booktitle> With discussion: </booktitle> <pages> 252-265. </pages>
Reference: <author> Rissanen, J. </author> <year> 1995. </year> <title> "Information Theory and Neural Nets." In Mathematical Perspective on Neural Networks, edited by P. </title> <editor> Smolensky, M. C. Mozer, and D. E. Rumel-hart. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Rissanen, J., and G. G. Langdon. </author> <year> 1981. </year> <title> "Universal Modeling and Coding." </title> <journal> IEEE Trans. Info. Theory IT-27: </journal> <pages> 12-23. </pages>
Reference: <author> Ruelle, D., and J. P. Eckmann. </author> <year> 1985. </year> <title> "Ergodic Theory of Chaos and Strange At-tractors." </title> <journal> Rev. Mod. Phys. </journal> <volume> 57: </volume> <pages> 617-656. </pages>
Reference-contexts: The source entropy is important because the Pesin identity relates it to the sum of the positive Lyapunov exponents <ref> (Ruelle & Eckmann, 1985) </ref>: h ( ) = h (1) = i i : (25) The Lyapunov exponents i are the eigenvalues of the local linearization of the dynamics, measuring the average rate of divergence of the principal axes of an ensemble of nearby trajectories.
Reference: <author> Rumelhart, D. E., R. Durbin, R. Golden, and Y. Chauvin. </author> <year> 1994. </year> <title> "Backpropagation: The Basic Theory." In Backpropagation: Theory, Architectures, and Applications, edited by Y. </title> <editor> Chauvin and D. E. Rumelhart. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher> <editor> Also in: </editor> <booktitle> In Mathematical Perspective on Neural Networks, </booktitle> <editor> 56 Andreas S. Weigend edited by P. Smolensky, M. C. Mozer, and D. E. Rumelhart. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Weigend is assumed to be constant, and constant terms drop out after difierentiation <ref> (see Rumelhart, Durbin, Golden, & Chauvin, 1994) </ref>. In contrast, we here allow the variance to depend on the input, and explicitly keep these terms in C. Given any network architecture and any error model, the appropriate weight-update equations for gradient descent learning can be derived straightforwardly.
Reference: <author> Rumelhart, D. E., G. E. Hinton, and R. J. Williams. </author> <year> 1986. </year> <title> "Learning Internal Representations by Error Propagation." In Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume I: Foundations, </title> <editor> edited by D. E. Rumelhart and J. L. McClelland, </editor> <address> 318-362. Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: The parameters are then adjusted to have the smallest error after the h iterations, i.e., at the top of the unfolded network <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. Time Series Analysis and Prediction 37 The third network has a difierent task: it projects directly from the present to the desired point h steps in the future|it does not involve any intermediate predictions or feed-back.
Reference: <author> Sauer, T. </author> <year> 1994. </year> <title> "Time Series Prediction Using Delay Coordinate Embedding." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 175-193. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We will return to this issue in Sections 5.4 and 5.5. In Figures 7 and 8, we compare the predictions for the laser data obtained with a local linear model <ref> (Sauer, 1994) </ref> and with a feed-forward network (Wan, 1994). The network did very well over the prediction interval of the flrst 100 points after the end of the training set (Figure 7), but notice how it fails dramatically thereafter (Figure 8) while the local linear forecast continues on. <p> advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection (Bonnlander & Weigend, 1994), wavelets for flltered embedding, a new embedding theorem for interspike time series <ref> (Tim Sauer, personal communication, 1994) </ref>, ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when an apparently complicated time series has been produced by a low-dimensional nonlinear system and to characterize
Reference: <author> Sauer, T., J. A. Yorke, and M. Casdagli. </author> <year> 1991. </year> <title> "Embedology." </title> <journal> J. Stat. Phys. </journal> <volume> 65(3/4): </volume> <pages> 579-616. </pages>
Reference: <author> Saund, E. </author> <year> 1989. </year> <title> "Dimensionality-Reduction Using Connectionist Networks." </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-11: </journal> <pages> 304-314. </pages>
Reference: <author> Schuster, A. </author> <title> 1898. "On the Investigation of Hidden Periodicities with Applications to a Supposed 26-Day Period of Meteorological Phenomena." </title> <journal> Terr. Mag. </journal> <volume> 3: </volume> <pages> 13-41. </pages>
Reference: <author> Shannon, C. E. </author> <year> 1948. </year> <title> "A Mathematical Theory of Communication." </title> <institution> Bell Syst. </institution>
Reference: <editor> Tech. J. </editor> <volume> 27: </volume> <pages> 379-423, 623-656. </pages> <booktitle> Reprinted in Key Papers in the Development of Information Theory, edited by D. Slepian, </booktitle> <pages> 5-18. </pages> <address> New York: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Shaw, R. S. </author> <year> 1981. </year> <title> "Strange Attractors, Chaotic Behavior and Information Flow." </title> <journal> Z. Naturforsch. </journal> <volume> 36A: </volume> <pages> 80-112. </pages>
Reference: <author> Solomonofi, R. J. </author> <year> 1964. </year> <title> "A Formal Theory of Induction Inference, Parts I and II." </title> <booktitle> Information & Control 7: </booktitle> <pages> 1-22, 221-254. </pages>
Reference: <author> Srivastava, A. N., and A. S. Weigend. </author> <year> 1994. </year> <title> "Computing the Probability Density in Connectionist Regression." </title> <booktitle> In Proceedings of the International Conference on Artiflcial Neural Networks (ICANN), </booktitle> <address> Sorrento, Italy, 685-688. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We now show a connectionist method to estimate the entire conditional target distribution <ref> (Srivastava & Weigend, 1994) </ref>. Such nonparametric estimates of the shape of a conditional target distribution require large quantities of data. We flrst introduce a representation appropriate for conditional probability distributions. The idea is to perform fractional binning, using the following procedure: 1.
Reference: <author> Sutton, R.S. </author> <year> 1988. </year> <title> "Learning to Predict by the Methods of Temporal Difierences." </title> <booktitle> Machine Learning 3: </booktitle> <pages> 9-44. </pages>
Reference-contexts: Unfortunately, the mathematical framework underlying time-delay embedding (such as the uniqueness of state-space trajectories) does not simply carry over to spatial structures. More modestly|narrowing the scope to neural networks for time series prediction| some examples of recent work are: Temporal Difierence learning (TD). The TD algorithm <ref> (Sutton, 1988) </ref> has traditionally been used for Markovian and game-like situations (Tesauro, 1992). We applied TD learning to the prediction of real-value time series (such as the laser data, Kazlas & Weigend, 1995).
Reference: <author> Svarer, C., L. K. Hansen, and J. Larsen. </author> <year> 1993. </year> <title> "On Design and Evaluation of Tapped-Delay Neural Network Architectures." </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco (ICNN), 46-51. Piscataway, NJ: </address> <note> IEEE Press. Time Series Analysis and Prediction 57 Swinney, </note> <author> H. L. </author> <year> 1994. </year> <title> "Spatio-Temporal Patterns: Observations and Analysis." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 557-567. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Takens, F. </author> <year> 1981. </year> <title> "Detecting Strange Attractors in Turbulence." In Dynamical Systems and Turbulence, </title> <editor> edited by D. A. Rand and L.-S. Young. </editor> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> Vol. 898, </volume> <pages> 336-381. </pages> <address> Warwick, 1980. Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Temam, R. </author> <year> 1988. </year> <title> Inflnite-Dimensional Dynamical Systems in Mechanics and Physics. </title> <journal> Applied Mathematical Sciences, </journal> <volume> Vol. </volume> <pages> 68. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Dimensionality reduction from the conflgura-tion space to the solution manifold is a common feature of dissipative systems: dissipation in a system will reduce its dynamics onto a lower dimensional sub space <ref> (Temam, 1988) </ref>. [4] If the governing equations and the functional form of the observable are known in advance, then a Kalman fllter is the optimal linear estimator of the state of the system. [5] The flrst point (conflguration space and potentially accessible degrees of freedom) will not be used again in
Reference: <author> Tesauro, G. </author> <year> 1992. </year> <title> "Practical Issues in Temporal Difierence Learning." </title> <booktitle> Machine Learning 8: </booktitle> <pages> 257-277. </pages>
Reference-contexts: More modestly|narrowing the scope to neural networks for time series prediction| some examples of recent work are: Temporal Difierence learning (TD). The TD algorithm (Sutton, 1988) has traditionally been used for Markovian and game-like situations <ref> (Tesauro, 1992) </ref>. We applied TD learning to the prediction of real-value time series (such as the laser data, Kazlas & Weigend, 1995).
Reference: <author> Theiler, J. </author> <year> 1990. </year> <title> "Estimating Fractal Dimension." </title> <journal> J. Opt. Soc. Am. </journal> <volume> A 7(6): </volume> <pages> 1055-1073. </pages>
Reference: <author> Theiler, J., P. S. Linsay, and D. M. Rubin. </author> <year> 1994. </year> <title> "Detecting Nonlinearity in Data with Long Coherence Times." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershen-feld, </booktitle> <pages> 439-455. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Tong, H., and K. S. Lim. </author> <year> 1980. </year> <title> "Threshold Autoregression, Limit Cycles and Cyclical Data." </title> <journal> J. Roy. Stat. Soc. </journal> <volume> B 42: </volume> <pages> 245-292. </pages>
Reference: <author> Turing, A. M. </author> <year> 1936. </year> <title> "On Computable Numbers, with an Application to the Entschei-dungsproblem." </title> <journal> Proc. London Math. Soc. </journal> <volume> 42: </volume> <pages> 230-265. </pages>
Reference: <author> Ulam, S. M, and J. v. Neumann. </author> <year> 1947. </year> <journal> Bulletin Amer. Math. Soc. </journal> <volume> 53: </volume> <pages> 1120. </pages>
Reference: <author> Wallace, C. S., and D. M. Boulton. </author> <year> 1968. </year> <title> "An Information Measure for Classiflca-tion." </title> <journal> Comp. J. </journal> <volume> 11: </volume> <pages> 185-195. </pages>
Reference: <author> Wan, E. A. </author> <year> 1994. </year> <title> "Times Series Prediction Using a Connectionist Network with Internal Delay Lines." In Time Series Prediction: Forecasting the Future and Understanding the Past, edited by A. </title> <editor> S. Weigend and N. A. </editor> <booktitle> Gershenfeld, </booktitle> <pages> 195-217. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We will return to this issue in Sections 5.4 and 5.5. In Figures 7 and 8, we compare the predictions for the laser data obtained with a local linear model (Sauer, 1994) and with a feed-forward network <ref> (Wan, 1994) </ref>. The network did very well over the prediction interval of the flrst 100 points after the end of the training set (Figure 7), but notice how it fails dramatically thereafter (Figure 8) while the local linear forecast continues on.
Reference: <author> Weigend, A. S. </author> <year> 1991. </year> <title> "Connectionist Architectures for Time Series Prediction of Dynamical Systems." </title> <type> Ph.D. Thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: This architecture can extract the linearly predictable part early in the learning process and free up the nonlinear resources to be employed where they are really needed. It can be advantageous to choose difierent learning rates for difierent parts of the architecture, and thus not follow the gradient exactly <ref> (Weigend, 1991) </ref>. A network that is to predict the future must know about the past. The simplest approach is to provide time-delayed samples as its inputs. A network without (nonlinear) hidden units is equivalent to an AR model (one linear fllter).
Reference: <author> Weigend, A. S. </author> <year> 1994. </year> <title> "On Overfltting and the Efiective Number of Hidden Units." </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <editor> edited by M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. </editor> <booktitle> Weigend, </booktitle> <pages> 335-342. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The results of the competition and of the follow-up NATO Advanced Research Workshop are collected in a book <ref> (Weigend & Gershenfeld, eds., 1994) </ref>. FIGURE 1 Examples of some time series. The first series, the laser data, is used as example for all models discussed in this article. [1] The data sets are available in the directory ftp://ftp.cs.colorado.edu/pub/Time-Series/SantaFe, or via http://www.cs.colorado.edu/andreas/TSWelcome.html. 4 Andreas S. <p> Computationally more expensive but more data e-cient are methods that use kernels to estimate Time Series Analysis and Prediction 21 information theoretic quantities; we have used them in the context of time series prediction and information theory for the selection of the most relevant subset of inputs <ref> (Bonnlander & Weigend, 1994) </ref>. <p> We now show a connectionist method to estimate the entire conditional target distribution <ref> (Srivastava & Weigend, 1994) </ref>. Such nonparametric estimates of the shape of a conditional target distribution require large quantities of data. We flrst introduce a representation appropriate for conditional probability distributions. The idea is to perform fractional binning, using the following procedure: 1. <p> obtain the manifold dimension as expressed by sigmoids, which is an upper limit to the true manifold dimension. [19] Second, a small size of the bottleneck layer can make the search (via gradient descent in backpropagation) hard: overfltting even occurs for small networks, before they have reached their full potential <ref> (Weigend, 1994) </ref>. There are two approaches to this problem: to penalize network complexity, or to use an oversized network and analyze it. <p> We picked the task of predicting daily trading volume on the New York Stock Exchange, from 1962 until the 1987 stock market crash, from past volume, past returns and their absolute values, and past volatility <ref> (Weigend & LeBaron, 1994) </ref>. 44 Andreas S. <p> Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output <ref> (Weigend & Zimmermann, 1994) </ref>. Experts for prediction. A Gaussian mixture model (Jacobs et al., 1991) can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics. <p> Inspired by Cacciatore and Nowlan (1994), we gave the gating network some recurrence, acting as capacitance to match the expected time scale of the gating output <ref> (Mangeas & Weigend, 1994) </ref>. Multivariate and non-stationary time series. Throughout this article, we have considered only univariate time series, i.e., a single variable as a function of time. <p> In the worst case (i.e., if the VAR matrix is an identity matrix) this reduces to training the network on the individual returns. Any improvement beyond this standard approach is due to the exploitation of the cointegration relation between the price series <ref> (Lutkepohl & Weigend, 1994) </ref>. Recurrent networks. In this article, we did not cover recurrent networks. One important advantage they have over feed-forward networks is that they can recover state in the present of noise, similar to a Hidden Markov Model. <p> There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection <ref> (Bonnlander & Weigend, 1994) </ref>, wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when
Reference: <author> Weigend, A. S., and N. A. Gershenfeld, eds. </author> <year> 1994. </year> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <booktitle> Santa Fe Institute Studies in the Sciences of Complexity, Proc. </booktitle> <volume> Vol. </volume> <pages> XV. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher> <editor> 58 Andreas S. Weigend Weigend, A. S., B. A. Huberman, and D. E. Rumelhart. </editor> <year> 1990. </year> <title> "Predicting the Future: A Connectionist Approach." </title> <journal> International Journal of Neural Systems 1: </journal> <pages> 193-209. </pages>
Reference-contexts: The results of the competition and of the follow-up NATO Advanced Research Workshop are collected in a book <ref> (Weigend & Gershenfeld, eds., 1994) </ref>. FIGURE 1 Examples of some time series. The first series, the laser data, is used as example for all models discussed in this article. [1] The data sets are available in the directory ftp://ftp.cs.colorado.edu/pub/Time-Series/SantaFe, or via http://www.cs.colorado.edu/andreas/TSWelcome.html. 4 Andreas S. <p> Computationally more expensive but more data e-cient are methods that use kernels to estimate Time Series Analysis and Prediction 21 information theoretic quantities; we have used them in the context of time series prediction and information theory for the selection of the most relevant subset of inputs <ref> (Bonnlander & Weigend, 1994) </ref>. <p> We now show a connectionist method to estimate the entire conditional target distribution <ref> (Srivastava & Weigend, 1994) </ref>. Such nonparametric estimates of the shape of a conditional target distribution require large quantities of data. We flrst introduce a representation appropriate for conditional probability distributions. The idea is to perform fractional binning, using the following procedure: 1. <p> obtain the manifold dimension as expressed by sigmoids, which is an upper limit to the true manifold dimension. [19] Second, a small size of the bottleneck layer can make the search (via gradient descent in backpropagation) hard: overfltting even occurs for small networks, before they have reached their full potential <ref> (Weigend, 1994) </ref>. There are two approaches to this problem: to penalize network complexity, or to use an oversized network and analyze it. <p> We picked the task of predicting daily trading volume on the New York Stock Exchange, from 1962 until the 1987 stock market crash, from past volume, past returns and their absolute values, and past volatility <ref> (Weigend & LeBaron, 1994) </ref>. 44 Andreas S. <p> Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output <ref> (Weigend & Zimmermann, 1994) </ref>. Experts for prediction. A Gaussian mixture model (Jacobs et al., 1991) can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics. <p> Inspired by Cacciatore and Nowlan (1994), we gave the gating network some recurrence, acting as capacitance to match the expected time scale of the gating output <ref> (Mangeas & Weigend, 1994) </ref>. Multivariate and non-stationary time series. Throughout this article, we have considered only univariate time series, i.e., a single variable as a function of time. <p> In the worst case (i.e., if the VAR matrix is an identity matrix) this reduces to training the network on the individual returns. Any improvement beyond this standard approach is due to the exploitation of the cointegration relation between the price series <ref> (Lutkepohl & Weigend, 1994) </ref>. Recurrent networks. In this article, we did not cover recurrent networks. One important advantage they have over feed-forward networks is that they can recover state in the present of noise, similar to a Hidden Markov Model. <p> There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection <ref> (Bonnlander & Weigend, 1994) </ref>, wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when
Reference: <author> Weigend, A., B. A. Huberman, and D. E. Rumelhart. </author> <year> 1992. </year> <title> "Predicting Sunspots and Exchange Rates with Connectionist Networks." In Nonlinear Modeling and Forecasting, edited by M. </title> <editor> Casdagli and S. </editor> <booktitle> Eubank, </booktitle> <pages> 395-432. </pages> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Weigend, A. S., and B. LeBaron. </author> <year> 1994. </year> <title> "Evaluating Neural Network Predictors by Bootstrapping." </title> <booktitle> In Proceedings of International Conference on Neural Information Processing (ICONIP'94), </booktitle> <address> Seoul, Korea. </address> <note> Also Technical Report CU-CS-725-94, </note> <institution> Computer Science Department, University of Colorado at Boulder. </institution>
Reference-contexts: The results of the competition and of the follow-up NATO Advanced Research Workshop are collected in a book <ref> (Weigend & Gershenfeld, eds., 1994) </ref>. FIGURE 1 Examples of some time series. The first series, the laser data, is used as example for all models discussed in this article. [1] The data sets are available in the directory ftp://ftp.cs.colorado.edu/pub/Time-Series/SantaFe, or via http://www.cs.colorado.edu/andreas/TSWelcome.html. 4 Andreas S. <p> Computationally more expensive but more data e-cient are methods that use kernels to estimate Time Series Analysis and Prediction 21 information theoretic quantities; we have used them in the context of time series prediction and information theory for the selection of the most relevant subset of inputs <ref> (Bonnlander & Weigend, 1994) </ref>. <p> We now show a connectionist method to estimate the entire conditional target distribution <ref> (Srivastava & Weigend, 1994) </ref>. Such nonparametric estimates of the shape of a conditional target distribution require large quantities of data. We flrst introduce a representation appropriate for conditional probability distributions. The idea is to perform fractional binning, using the following procedure: 1. <p> obtain the manifold dimension as expressed by sigmoids, which is an upper limit to the true manifold dimension. [19] Second, a small size of the bottleneck layer can make the search (via gradient descent in backpropagation) hard: overfltting even occurs for small networks, before they have reached their full potential <ref> (Weigend, 1994) </ref>. There are two approaches to this problem: to penalize network complexity, or to use an oversized network and analyze it. <p> We picked the task of predicting daily trading volume on the New York Stock Exchange, from 1962 until the 1987 stock market crash, from past volume, past returns and their absolute values, and past volatility <ref> (Weigend & LeBaron, 1994) </ref>. 44 Andreas S. <p> Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output <ref> (Weigend & Zimmermann, 1994) </ref>. Experts for prediction. A Gaussian mixture model (Jacobs et al., 1991) can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics. <p> Inspired by Cacciatore and Nowlan (1994), we gave the gating network some recurrence, acting as capacitance to match the expected time scale of the gating output <ref> (Mangeas & Weigend, 1994) </ref>. Multivariate and non-stationary time series. Throughout this article, we have considered only univariate time series, i.e., a single variable as a function of time. <p> In the worst case (i.e., if the VAR matrix is an identity matrix) this reduces to training the network on the individual returns. Any improvement beyond this standard approach is due to the exploitation of the cointegration relation between the price series <ref> (Lutkepohl & Weigend, 1994) </ref>. Recurrent networks. In this article, we did not cover recurrent networks. One important advantage they have over feed-forward networks is that they can recover state in the present of noise, similar to a Hidden Markov Model. <p> There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection <ref> (Bonnlander & Weigend, 1994) </ref>, wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when
Reference: <author> Weigend, A. S., and D. E. Rumelhart. </author> <year> 1991. </year> <title> "Generalization Through Minimal Networks with Application to Forecasting." </title> <booktitle> In 23rd Symposium on the Interface: Computing Science and Statistics (INTERFACE'91), </booktitle> <address> Seattle, </address> <note> edited by E. </note> <author> M. Keramidas, </author> <month> 362-370. </month> <title> Interface Foundation of North America. </title>
Reference-contexts: This architecture can extract the linearly predictable part early in the learning process and free up the nonlinear resources to be employed where they are really needed. It can be advantageous to choose difierent learning rates for difierent parts of the architecture, and thus not follow the gradient exactly <ref> (Weigend, 1991) </ref>. A network that is to predict the future must know about the past. The simplest approach is to provide time-delayed samples as its inputs. A network without (nonlinear) hidden units is equivalent to an AR model (one linear fllter).
Reference: <author> Weigend, A. S., D. E. Rumelhart and B. A. Huberman. </author> <year> 1991. </year> <title> "Generalization by Weight-Elimination with Application to Forecasting." </title> <booktitle> In Advances in Neural Information Processing Systems 3 (NIPS*90), </booktitle> <editor> edited by R. P. Lippmann, </editor> <publisher> J. </publisher>
Reference-contexts: This architecture can extract the linearly predictable part early in the learning process and free up the nonlinear resources to be employed where they are really needed. It can be advantageous to choose difierent learning rates for difierent parts of the architecture, and thus not follow the gradient exactly <ref> (Weigend, 1991) </ref>. A network that is to predict the future must know about the past. The simplest approach is to provide time-delayed samples as its inputs. A network without (nonlinear) hidden units is equivalent to an AR model (one linear fllter).
Reference: <editor> Moody, and D. S. Touretzky, </editor> <address> 875-882. Redwood City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weigend, A. S., and H. G. Zimmermann. </author> <year> 1994. </year> <note> In preparation, </note> <institution> Computer Science Department, University of Colorado at Boulder. </institution>
Reference-contexts: The results of the competition and of the follow-up NATO Advanced Research Workshop are collected in a book <ref> (Weigend & Gershenfeld, eds., 1994) </ref>. FIGURE 1 Examples of some time series. The first series, the laser data, is used as example for all models discussed in this article. [1] The data sets are available in the directory ftp://ftp.cs.colorado.edu/pub/Time-Series/SantaFe, or via http://www.cs.colorado.edu/andreas/TSWelcome.html. 4 Andreas S. <p> Computationally more expensive but more data e-cient are methods that use kernels to estimate Time Series Analysis and Prediction 21 information theoretic quantities; we have used them in the context of time series prediction and information theory for the selection of the most relevant subset of inputs <ref> (Bonnlander & Weigend, 1994) </ref>. <p> We now show a connectionist method to estimate the entire conditional target distribution <ref> (Srivastava & Weigend, 1994) </ref>. Such nonparametric estimates of the shape of a conditional target distribution require large quantities of data. We flrst introduce a representation appropriate for conditional probability distributions. The idea is to perform fractional binning, using the following procedure: 1. <p> obtain the manifold dimension as expressed by sigmoids, which is an upper limit to the true manifold dimension. [19] Second, a small size of the bottleneck layer can make the search (via gradient descent in backpropagation) hard: overfltting even occurs for small networks, before they have reached their full potential <ref> (Weigend, 1994) </ref>. There are two approaches to this problem: to penalize network complexity, or to use an oversized network and analyze it. <p> We picked the task of predicting daily trading volume on the New York Stock Exchange, from 1962 until the 1987 stock market crash, from past volume, past returns and their absolute values, and past volatility <ref> (Weigend & LeBaron, 1994) </ref>. 44 Andreas S. <p> Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output <ref> (Weigend & Zimmermann, 1994) </ref>. Experts for prediction. A Gaussian mixture model (Jacobs et al., 1991) can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics. <p> Inspired by Cacciatore and Nowlan (1994), we gave the gating network some recurrence, acting as capacitance to match the expected time scale of the gating output <ref> (Mangeas & Weigend, 1994) </ref>. Multivariate and non-stationary time series. Throughout this article, we have considered only univariate time series, i.e., a single variable as a function of time. <p> In the worst case (i.e., if the VAR matrix is an identity matrix) this reduces to training the network on the individual returns. Any improvement beyond this standard approach is due to the exploitation of the cointegration relation between the price series <ref> (Lutkepohl & Weigend, 1994) </ref>. Recurrent networks. In this article, we did not cover recurrent networks. One important advantage they have over feed-forward networks is that they can recover state in the present of noise, similar to a Hidden Markov Model. <p> There are many other recent advances that can be used for connectionist time series analysis and prediction|just to name a few examples: combination of forecasts (Perrone, 1994), "boosting" of the training set (Drucker et al., 1994), variable subset selection <ref> (Bonnlander & Weigend, 1994) </ref>, wavelets for flltered embedding, a new embedding theorem for interspike time series (Tim Sauer, personal communication, 1994), ...|but let us try to regain perspective: where once, less than a decade ago, time series analysis was shaped by linear systems theory, it is now possible to recognize when
Reference: <author> White, D. A., and D. A. Sofge, eds. </author> <year> 1992. </year> <title> Handbook of Intelligent Control. </title> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Wiener, N. </author> <year> 1949. </year> <title> The Extrapolation, Interpolation and Smoothing of Stationary Time Series with Engineering Applications. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Yule, G. </author> <year> 1927. </year> <title> "On a Method of Investigating Periodicity in Disturbed Series with Special Reference to Wolfer's Sunspot Numbers." </title> <journal> Phil. Trans. Roy. Soc. </journal> <volume> Lon-don A 226: </volume> <pages> 267-298. </pages>
Reference: <author> Zimmermann, H. G. </author> <year> 1994. </year> <editor> "Neuronale Netze in der Okonometrie." (In German.) In Neuronale Netze als Entscheidungskalkul, edited by H. Rehkugler, and H. </editor> <publisher> G. </publisher>
Reference-contexts: Other pruning algorithms derive the signiflcance of a weight from the ratio of the size of the weight to the standard deviation of its pattern-by-pattern changes, i.e., the square root of the variance of the w's in one pass through the data in backpropagation <ref> (Zimmermann, 1994) </ref>. In general, it seems useful to combine some of the methods, e.g., to flrst use early stopping (i.e., to train to a minimum on a cross-validation set), and then crank up one of the complexity penalizers or pruning algorithms. <p> Second, we use the prediction network itself by backpropagating the activations through the network and moving the (input) data towards those values that would have generated the target output <ref> (Weigend & Zimmermann, 1994) </ref>. Experts for prediction. A Gaussian mixture model (Jacobs et al., 1991) can be used for the prediction of time series which exhibit regime switching and structure breaks: the gating network learns to partition the data between competing sub-networks according to similarities in the internal dynamics.
Reference: <author> Zimmermann, </author> <month> 1-87. </month> <title> Munchen: </title> <publisher> Vahlen Verlag. </publisher>
References-found: 116

