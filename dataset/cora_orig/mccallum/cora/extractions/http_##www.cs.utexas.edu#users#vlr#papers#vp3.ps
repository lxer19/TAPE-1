URL: http://www.cs.utexas.edu/users/vlr/papers/vp3.ps
Refering-URL: http://www.cs.utexas.edu/users/vlr/pub.html
Root-URL: 
Title: Parallel Implementation of Algorithms for Finding Connected Components in Graphs (Preprint)  
Author: TSAN-SHENG HSU, VIJAYA RAMACHANDRAN, AND NATHANIEL DEAN 
Date: June 10, 1996  
Note: DIMACS Series in Discrete Mathematics and Theoretical Computer Science Volume 00, 0000  c fl0000 American Mathematical Society 0000-0000/00 $1.00 $.25 per page  
Abstract: In this paper, we describe our implementation of several parallel graph algorithms for finding connected components. Our implementation, with virtual processing, is on a 16,384-processor MasPar MP-1 using the language MPL. We present extensive test data on our code. In our previous projects [21, 22, 23], we reported the implementation of an extensible parallel graph algorithms library. We developed general implementation and fine-tuning techniques without expending too much effort on optimizing each individual routine. We also handled the issue of implementing virtual processing. In this paper, we describe several algorithms and fine-tuning techniques that we developed for the problem of finding connected components in parallel; many of the fine-tuning techniques are of general interest, and should be applicable to code for other problems. We present data on the execution time and memory usage of our various implementations. 1991 Mathematics Subject Classification. Primary 68-04; Secondary 05-04, 05C85, 68Q22. Key words and phrases. parallel algorithms, graph algorithms, connected components, im plementation, MasPar. The first author was supported in part by NSC of Taiwan, ROC, Grants 84-2213-E-001-005 and 85-2213-E-001-003. The second author was supported in part by NSF Grant CCR-90-23059 and Texas Advanced Research Projects Grant 003658386. This paper is in final form and no version of it will be submitted for publication elsewhere. y This paper will appear in DIMACS Series in Discrete Mathematics and Theoretical Com puter Science, volume on the 3rd DIMACS Challenge, American Mathematical Societ. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Anderson and J. Setubal, </author> <title> On the parallel implementation of Goldberg's maximum flow algorithm, </title> <booktitle> Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 168-177. </pages>
Reference: 2. <author> B. Awerbuch and Y. Shiloach, </author> <title> New connectivity and MSF algorithms for shu*e-exchange network and PRAM, </title> <booktitle> IEEE Tran. on Computers (1987), </booktitle> <pages> 1258-1263. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. <p> Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) <ref> [2, 5, 6] </ref>, ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. <p> An algorithm for finding connected components by Awerbuch and Shiloach <ref> [2] </ref>. component number can thus be assigned on the roots and the component number of each vertex is the component number of its root. The number of connected component is equal to the number of tree loops. <p> The use of CRCW PRAM algorithms involved dealing with concurrent memory accesses; we discuss our implementation of concurrent memory accesses in Sections 3.4 and 3.7. 2.1. Awerbuch and Shiloach. The algorithm by Awerbuch and Shiloach <ref> [2] </ref> is shown in Algorithm 1. In each hooking-and-pointer-jumping iteration of this algorithm, two hooks are performed. The first hook, which is called conditional star hooking, makes the root of a rooted star point to a tree loop. <p> He also reported an implementation on a vector super computer Cray C-90 using one processor. Greiner implemented the algorithms of Shiloach and Vishkin [50] and Awerbuch and Shiloach <ref> [2] </ref>, and the simple randomized algorithm that we implemented. Greiner did not use the system pseudo number generator for generating random bits in his randomized code. Instead, the ith "random" bit for a vertex is the (i mod log 2 n)th bit of the vertex number.
Reference: 3. <author> G. E. Blelloch, </author> <title> Scan primitives and parallel vector models, </title> <type> Ph.D. thesis, </type> <institution> M.I.T., </institution> <month> October </month> <year> 1989. </year>
Reference: 4. <author> G. E. Blelloch, C. E. Leiserson, B. M. Maggs, C. G. Plaxton, S. J. Smith, and M. Zagha, </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2, </title> <booktitle> Proc. 3th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1991, </year> <pages> pp. 3-16. </pages>
Reference: 5. <author> K. W. Chong and T. W. Lam, </author> <title> Finding connected components in O(log n log log n) time on the EREW PRAM, </title> <booktitle> Proc. 4th Annual ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 11-20. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. <p> Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) <ref> [2, 5, 6] </ref>, ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 6. <author> R. Cole, P. N. Klein, and R. E. Tarjan, </author> <title> A linear-work parallel algorithm for finding minimum spanning trees, </title> <booktitle> Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 11-15. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) <ref> [2, 5, 6] </ref>, ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 7. <author> R. Cole and U. Vishkin, </author> <title> Approximate parallel scheduling. Part II: Applications to logarithmic-time optimal graph algorithms, </title> <booktitle> Information and Computation 92 (1991), </booktitle> <pages> 1-47. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. <p> We implemented the following four algorithms, all of which run in O (log n) time (with high probability for the randomized algorithm) using a linear number of processors on a CRCW PRAM. Although techniques are known to reduce to the number of processors used in the algorithms <ref> [7] </ref>, we chose not to implement them because the associated algorithms are quite complicated and the overhead is likely to be too large. The use of CRCW PRAM algorithms involved dealing with concurrent memory accesses; we discuss our implementation of concurrent memory accesses in Sections 3.4 and 3.7. 2.1.
Reference: 8. <author> E. Dekel, D. Nassimi, and S. Sahni, </author> <title> Parallel matrix and graph algorithms, </title> <journal> SIAM J. Com-put. </journal> <volume> 10 (1981), </volume> <pages> 657-675. </pages>
Reference: 9. <author> B. Dixon and A. K. Lenstra, </author> <title> Factoring integers using SIMD sieves, </title> <type> Manuscript, </type> <year> 1992. </year> <title> 10. , Massively parallel elliptic curve factoring, </title> <type> Manuscript, </type> <year> 1992. </year>
Reference: 11. <author> T. Feder, A. G. Greenberg, V. Ramachandran, M. Rauch, and L.-C. Wang, </author> <title> Circuit switched link simulation: Algorithms, complexity and implementation, </title> <type> Draft manuscript, </type> <year> 1992. </year>
Reference: 12. <author> D. Fussel, V. Ramachandran, and R. Thurimella, </author> <title> Finding triconnected components by local replacements, </title> <journal> SIAM J. Comput. </journal> <volume> 22 (1993), no. 3, </volume> <pages> 587-616. </pages>
Reference-contexts: been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity <ref> [12, 36] </ref> and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 13. <author> H. Gazit, </author> <title> An optimal randomized parallel algorithm for finding connected components in a graph, </title> <journal> SIAM J. Comput. </journal> <volume> 20 (1991), no. 6, </volume> <pages> 1046-1067. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44].
Reference: 14. <author> P. B. Gibbons, Y. Matias, and V. Ramachandran, </author> <title> The QRQW PRAM: Accounting for contention in parallel algorithms, </title> <booktitle> Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <year> 1994, </year> <pages> pp. 638-648, </pages> <note> SIAM J. Comput., to appear. </note>
Reference-contexts: The system-provided routine rsend in MPL language can be used to directly implement arbitrary write operations. The execution time of an arbitrary concurrent write operation increases when the maximum number of concurrent write requests per physical processor increases. This is the `Queue-Write' model that is addressed in <ref> [14] </ref>. Another way of implementing concurrent write in MPL is to use the routine sendwith. This routine executes the standard simulation of a concurrent write step on an exclusive write PRAM [27], and requires the use of sorting and additional working memory space. <p> The number of hooks performed tends to be smaller after the first iteration. Thus we can use priority concurrent write (with the sendwith routine) in the first iteration and arbitrary concurrent write (with the rsend routine) (i.e., queue-write <ref> [14] </ref>) in the remaining iterations. We will show in Section 5 that this hybrid implementation improves the performance of some of our algorithms. 3.5. Edge Condensation. During the execution of the algorithm, vertices in a tree loop can be viewed as a super vertex and can be collapsed. <p> The MPL language provides system routine rfetch to directly implement it. The execution time of an concurrent read operation increases when the maximum number of concurrent read requests per physical processor increases [21, 40]. This is the `Queue-Read' model that is addressed in <ref> [14] </ref>. As indicated in the experiments performed in [21], an exclusive read implementation of concurrent read operations using sorting outperforms the rfetch concurrent read implementation when the maximum number of concurrent read requests on one processor is larger than 256.
Reference: 15. <author> A. G. Greenberg, B. D. Lubachevsky, and L.-C. Wang, </author> <title> Experience in massively parallel discrete event simulation, </title> <booktitle> Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1993, </year> <pages> pp. 193-202. </pages>
Reference: 16. <author> J. Greiner, </author> <title> A comparison of data-parallel algorithms for connected components, </title> <booktitle> Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 16-25. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. <p> Related work on implementing combinatorial algorithms on massively parallel machines can be found in [1, 3, 4, 8, 9, 10, 11, 15, 16, 18, 19, 30, 38, 39, 41, 48]. Also there has been work reported on implementing combinatorial algorithms on a vector super computer <ref> [16, 45, 49] </ref> and on a distributed memory machine [29]. The rest of the paper is organized as follows. Section 2 describes the algorithms implemented which includes an algorithm that we devised for this project. Section 3 gives general fine-tuning techniques for our code. Section 4 describes the testing scheme. <p> For each size and sparsity, we generated four different test graphs. We ran each program on each test graph for 10 iterations and recorded the average of the 40 trials. We also used the following special classes of graphs that are reported in the experiments conducted in <ref> [16] </ref>. * Two-dimensional wrap-around grids that are squares with 30% and 60% of their edges (chosen randomly). <p> Our randomized code used only half of the amount of memory because of the simplicity of the underlying algorithm. As a result of our fine-tuning, our current deterministic code uses 25% less memory than our earlier version in [23]. 5.5. Comparison to Related Work. In <ref> [16] </ref>, Greiner reported the implementation of several parallel algorithms for finding connected components on a massively parallel computer CM-2 using a quarter of the processors (8,192 processors) and all 32 kilo-bytes of memory in each processor. <p> It should be noted that the CM-2 is a more expensive machine than the MasPar MP-1, and the Cray C-90 is a much more expensive machine than the MP-1. The issue of memory usage is not addressed in <ref> [16] </ref>. Using four times the amount of total memory that we used, Greiner shows CM-2 performance data on random graphs with about 0.52 millions edges. He shows CM-2 performance data on grids and tertiary graphs with twice as many edges.
Reference: 17. <author> S. Halperin and U. Zwick, </author> <title> An optimal randomized logarithmic time connectivity algorithm for the EREW PRAM, </title> <booktitle> Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 1-10. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44].
Reference: 18. <author> W. Hightower, J. Prins, and J. Reif, </author> <title> Implementations of randomized sorting on large parallel machines, </title> <booktitle> Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 158-167. </pages>
Reference: 19. <author> W. D. Hillis and G. L. Steele Jr., </author> <title> Data parallel algorithms, </title> <booktitle> Communications of the ACM 29 (1986), </booktitle> <pages> 1170-1183. </pages>
Reference: 20. <author> D. S. Hirschberg, A. K. Chandra, and D. V. Sarwate, </author> <title> Computing connected components on parallel computers, </title> <journal> Communications of the ACM 22 (1979), </journal> <volume> no. 8, </volume> <pages> 461-464. </pages>
Reference-contexts: during the execution is a tree loop, where each vertex in the set has an outgoing pointer that points to another vertex in the set with the constraint that exactly one vertex has a pointer that points to itself. (Note that this is sometimes called a `zero-tree-loop' in the literature <ref> [20] </ref>.) Let the height of a tree loop T be the number of vertices in a longest simple directed path in T . A tree loop whose height is 2 is a rooted star. The vertex with self loop in a tree loop is the root. <p> He implemented fine-tuning techniques which include routines similar to our first iteration of hooking, check of live edges, and edge condensation. He also implemented hybrid algorithms that combine features in the above three algorithms and an algorithm in Hirchberg, Chandra, and Sarwate <ref> [20] </ref>. His hybrid algorithm has the best performance for the classes of graphs tested. Greiner did not implement the compressed data structure for edges, various implementation of concurrent write operations, or deferred pointer jumping, all of which have been implemented in our work.
Reference: 21. <author> T.-s. Hsu and V. Ramachandran, </author> <title> Efficient massively parallel implementation of some combinatorial algorithms, </title> <note> Theoretical Computer Science (1996, to appear). </note>
Reference-contexts: Thus an implementation of parallel algorithms for undirected graphs would have to proceed in a bottom-up fashion, starting with an implementation of basic primitives, and successively building up to more complex algorithms. Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems <ref> [21, 22, 23] </ref>. Our implementations have been on the MasPar MP-1 in the parallel language MPL [34, 35], which is an extension of the C language [28]. In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in <p> Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems <ref> [21, 22, 23] </ref>. Our implementations have been on the MasPar MP-1 in the parallel language MPL [34, 35], which is an extension of the C language [28]. In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in the previous paragraph: We first built a kernel of basic parallel primitives and then implemented parallel graph algorithms in order of increasing complexity. <p> Since the MasPar MP-1 does not support virtual processing in MPL, in [23] we handled the issue of implementing virtual processing. We then went into the basic routines we implemented in the kernel, and performed extensive fine-tuning; this is reported in <ref> [21] </ref>. In this paper, we present our work on fine-tuning the first parallel graph algorithm we implemented, that for finding connected components in an undirected graph. <p> Thus it usually took a very large number of iterations and a very long time for the algorithm to terminate. (The same problem is also reported in our implementation of a randomized list ranking algorithm <ref> [21] </ref>.) To avoid the above problem, we revised our algorithm as follows. We execute our randomized algorithm until the number of live edges left is less than half of the number of physical processors. <p> Implementation of Concurrent Read. In implementing pointer jumping, we need to use concurrent read operations. The MPL language provides system routine rfetch to directly implement it. The execution time of an concurrent read operation increases when the maximum number of concurrent read requests per physical processor increases <ref> [21, 40] </ref>. This is the `Queue-Read' model that is addressed in [14]. <p> The execution time of an concurrent read operation increases when the maximum number of concurrent read requests per physical processor increases [21, 40]. This is the `Queue-Read' model that is addressed in [14]. As indicated in the experiments performed in <ref> [21] </ref>, an exclusive read implementation of concurrent read operations using sorting outperforms the rfetch concurrent read implementation when the maximum number of concurrent read requests on one processor is larger than 256.
Reference: 22. <author> T.-s. Hsu, V. Ramachandran, and N. Dean, </author> <title> Implementation of parallel graph algorithms on the MasPar, </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> vol. 15, </volume> <publisher> American Mathematical Society, </publisher> <year> 1994, </year> <pages> pp. 165-198. </pages> <month> 23. </month> , <title> Implementation of parallel graph algorithms on a massively parallel SIMD computer with virtual processing, </title> <booktitle> Proc. 9th International Parallel Processing Symp., </booktitle> <year> 1995, </year> <pages> pp. 106-112. </pages> <note> 18 HSU, RAMACHANDRAN, AND DEAN </note>
Reference-contexts: Thus an implementation of parallel algorithms for undirected graphs would have to proceed in a bottom-up fashion, starting with an implementation of basic primitives, and successively building up to more complex algorithms. Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems <ref> [21, 22, 23] </ref>. Our implementations have been on the MasPar MP-1 in the parallel language MPL [34, 35], which is an extension of the C language [28]. In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in <p> Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems <ref> [21, 22, 23] </ref>. Our implementations have been on the MasPar MP-1 in the parallel language MPL [34, 35], which is an extension of the C language [28]. In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in the previous paragraph: We first built a kernel of basic parallel primitives and then implemented parallel graph algorithms in order of increasing complexity. <p> In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in the previous paragraph: We first built a kernel of basic parallel primitives and then implemented parallel graph algorithms in order of increasing complexity. In <ref> [22] </ref> we described general implementation and fine-tuning techniques used in the implementation of our parallel graph algorithms library; in this implementation we did not expend too much effort on optimizing each individual routine. <p> A simple randomized algorithm for finding con nected components (see, e.g., Chapter 4.3 in [46]). (without virtual processing) as described in <ref> [22] </ref>. Since it is possible for a very dense graph to have a small number of vertices, but a very large number of edges, the problem with the system pseudo random bits remained for dense graphs even after this modification. <p> The MasPar computer [33] is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. A description of the hardware architecture and the software environment of MasPar MP-1 can be found in <ref> [22] </ref>. We used 4 kilo-bytes of memory per physical processor to test our various implementations for connected components. We show the performance of our code running on the largest data that we could fit into the system. 4.2. Test Inputs.
Reference: 24. <author> K. Iwama and Y. Kambayashi, </author> <title> A simpler parallel algorithm for graph connectivity, </title> <booktitle> Journal of Algorithms 16 (1994), </booktitle> <pages> 190-217. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44].
Reference: 25. <author> J. JaJa, </author> <title> An introduction to parallel algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: 1. Introduction Over the past decade there has been a large amount of work in the theory of efficient, highly parallel graph algorithm design <ref> [25, 27, 31, 46] </ref>. <p> An algorithm for finding connected components by Shiloach and Vishkin [50] as described in Chapter 5.1.3 of JaJa <ref> [25] </ref>. parent of its parent). For each vertex whose grandparent is different from its parent, we mark (concurrent write) a flag f for its grandparent. Any vertex that is marked is not in a rooted star. Every unmarked vertex reads the flag f from its grandparent. <p> Every unmarked vertex reads the flag f from its grandparent. Unmarked vertices whose grandparents are marked are also not in rooted stars. 2.2. Shiloach and Vishkin. The next algorithm we implemented (Algorithm 2) is by Shiloach and Vishkin [50] and also appears in Chapter 5.1.3 of JaJa <ref> [25] </ref>. In each of the hooking-and-pointer-jumping iteration of this algorithm, two hooks are performed as in Algorithm 1.
Reference: 26. <author> D. R. Karger, N. Nisan, and M. Parnas, </author> <title> Fast connected components algorithms for the EREW PRAM, </title> <booktitle> Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 373-381. </pages>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44].
Reference: 27. <author> R. M. Karp and V. Ramachandran, </author> <title> Parallel algorithms for shared-memory machines, </title> <booktitle> Handbook of Theoretical Computer Science (J. </booktitle> <editor> van Leeuwen, ed.), </editor> <publisher> North Holland, </publisher> <year> 1990, </year> <pages> pp. 869-941. </pages>
Reference-contexts: 1. Introduction Over the past decade there has been a large amount of work in the theory of efficient, highly parallel graph algorithm design <ref> [25, 27, 31, 46] </ref>. <p> This is the `Queue-Write' model that is addressed in [14]. Another way of implementing concurrent write in MPL is to use the routine sendwith. This routine executes the standard simulation of a concurrent write step on an exclusive write PRAM <ref> [27] </ref>, and requires the use of sorting and additional working memory space. The execution time of a concurrent write operation when sendwith is used depends only on the total number of write requests, and not on the maximum number of concurrent write requests per physical processor.
Reference: 28. <author> B. W. Kernighan and D. M. Ritchie, </author> <title> The C programming language, </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1988, </year> <note> Second Edition. </note>
Reference-contexts: Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems [21, 22, 23]. Our implementations have been on the MasPar MP-1 in the parallel language MPL [34, 35], which is an extension of the C language <ref> [28] </ref>. In our previous papers [21, 22, 23], we reported on the implementation of an extensible parallel graph algorithms library using the approach outlined in the previous paragraph: We first built a kernel of basic parallel primitives and then implemented parallel graph algorithms in order of increasing complexity.
Reference: 29. <author> A. Krishnamurthy, S. Lumetta, D. E. Culler, and K. Yelick, </author> <title> Connected components on distributed memory machines, </title> <booktitle> Presented at the 3rd DIMACS Implementation Challenge Workshop, </booktitle> <month> October, </month> <year> 1994. </year>
Reference-contexts: Also there has been work reported on implementing combinatorial algorithms on a vector super computer [16, 45, 49] and on a distributed memory machine <ref> [29] </ref>. The rest of the paper is organized as follows. Section 2 describes the algorithms implemented which includes an algorithm that we devised for this project. Section 3 gives general fine-tuning techniques for our code. Section 4 describes the testing scheme. Section 5 gives performance data. <p> We also show performance data for our deterministic code on graphs with 0.26 millions edges. Since we use only quarter the amount of memory as Greiner's implementation on the CM-2, it appears that our code uses less space than his. In <ref> [29] </ref>, a distributed memory implementation of the algorithm by Shiloach and Vishkin [50] is reported. After fine-tuning, they obtain a speedup of 20 using a 32 processor CM-5 on grid graphs and obtain virtually no speedup on sparse random graphs.
Reference: 30. <author> S. Kumar, S. M. Goddard, and J. F. Prins, </author> <title> Connected-components algorithms for mesh-connected parallel computers, </title> <booktitle> Presented at the 3rd DIMACS Implementation Challenge Workshop, </booktitle> <month> October, </month> <year> 1994. </year>
Reference-contexts: After fine-tuning, they obtain a speedup of 20 using a 32 processor CM-5 on grid graphs and obtain virtually no speedup on sparse random graphs. The performance of our massively parallel implementation seems to be more adaptable to different classes of graphs. In <ref> [30] </ref>, a mesh implementation of hooking-and-pointer-jumping type algorithms is reported on a MasPar MP-1 using 8,192 processors. <p> By using the underlying mesh architecture and the fact that the mesh communication is more than 100 times faster than the global router communication, their implementation is generally faster than ours on dense graphs. Our implementation runs in about the same speed than theirs on very sparse graphs. In <ref> [30] </ref>, the input graph is represented by an adjacency linked list on sparse graphs and is represented by an adjacency matrix on dense graphs while we use an arbitrary edge list to represent any input graph. It appears that our input data format is more flexible.
Reference: 31. <author> F. T. Leighton, </author> <title> Introduction to parallel algorithms and architectures: Arrays, trees, </title> <publisher> hy-percubes, Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: 1. Introduction Over the past decade there has been a large amount of work in the theory of efficient, highly parallel graph algorithm design <ref> [25, 27, 31, 46] </ref>.
Reference: 32. <author> Y. Maon, B. Schieber, and U. Vishkin, </author> <title> Parallel ear decomposition search (EDS) and st-numbering in graphs, </title> <type> Theoret. </type> <institution> Comput. Sci. </institution> <year> (1986), </year> <pages> 277-298. </pages>
Reference-contexts: that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity <ref> [32, 37, 43] </ref>, open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. <p> sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity <ref> [32, 37, 43, 52] </ref>, triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 33. <institution> MasPar Computer Co., MasPar system overview, </institution> <note> version 2.0 ed., </note> <month> March </month> <year> 1991. </year>
Reference-contexts: We will show in Section 5 this hybrid implementation of concurrent read operations improves the performance of some of our algorithms. 4. Testing Scheme 4.1. Computing Platform. All of our parallel implementations are on a MasPar MP-1 with 16,384 processors. The MasPar computer <ref> [33] </ref> is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. A description of the hardware architecture and the software environment of MasPar MP-1 can be found in [22].
Reference: 34. <institution> MasPar Computer Co., </institution> <note> MasPar parallel application language (MPL) reference manual, version 3.0, rev. </note> <editor> a3 ed., </editor> <month> July </month> <year> 1992. </year>
Reference-contexts: Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems [21, 22, 23]. Our implementations have been on the MasPar MP-1 in the parallel language MPL <ref> [34, 35] </ref>, which is an extension of the C language [28].
Reference: 35. <author> MasPar Computer Co., </author> <title> MasPar parallel application language (MPL) user guide, </title> <note> version 3.1, rev. </note> <editor> a3 ed., </editor> <month> November </month> <year> 1992. </year>
Reference-contexts: Using this strategy, we have implemented efficient parallel algorithms for several combinatorial and graph problems [21, 22, 23]. Our implementations have been on the MasPar MP-1 in the parallel language MPL <ref> [34, 35] </ref>, which is an extension of the C language [28].
Reference: 36. <author> G. L. Miller and V. Ramachandran, </author> <title> A new triconnectivity algorithm and its applications, </title> <booktitle> Combinatorica 12 (1992), </booktitle> <pages> 53-76. </pages> <month> 37. </month> , <title> Efficient parallel ear decomposition with applications, </title> <type> Manuscript, </type> <address> MSRI, Berke-ley, CA, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity <ref> [12, 36] </ref> and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 38. <author> B. Narendran and P. Tiwari, </author> <title> Polynomial root-finding: Analysis and computational investigation of a parallel algorithm, </title> <booktitle> Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 178-187. </pages>
Reference: 39. <author> P. M. Pardalos, M. G.C. Resende, and K.G. Ramakrishnan (eds.), </author> <title> Parallel processing of discrete optimization problems, </title> <booktitle> DIMACS series in discrete mathematics and theoretical computer science, </booktitle> <volume> vol. 22, </volume> <publisher> American Mathematical Society, </publisher> <year> 1995. </year>
Reference: 40. <author> L. Prechelt, </author> <title> Measurements of MasPar MP-1216A communication operations, </title> <type> Tech. Report 01/93, </type> <institution> Institute fur Programmstrukturen und Datenorganisation, Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Implementation of Concurrent Read. In implementing pointer jumping, we need to use concurrent read operations. The MPL language provides system routine rfetch to directly implement it. The execution time of an concurrent read operation increases when the maximum number of concurrent read requests per physical processor increases <ref> [21, 40] </ref>. This is the `Queue-Read' model that is addressed in [14].
Reference: 41. <author> J. F. Prins and J. A. Smith, </author> <title> Parallel sorting of large arrays on the MasPar MP-1, </title> <booktitle> Proc. 3rd Symp. on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1990, </year> <pages> pp. 59-64. </pages>
Reference-contexts: We observe that we do not occupy all 4 kilo-bytes of space by storing 16 vertices and 16 edges in a physical processor, and we should have enough space to pack 24 vertices and 24 edges. However, the sorting program that we used <ref> [41] </ref> requires that the number of data allocated in each physical processor to be power of 2. We use sorting to implement priority concurrent write operations (using the sendwith routine) and to remove duplicated edges.
Reference: 42. <author> T. Radzik, </author> <title> Computing connected components on EREW PRAM, </title> <type> Tech. report, </type> <institution> King's College, </institution> <address> London, </address> <year> 1994, </year> <type> Tech. Rep. 94/02. </type>
Reference-contexts: Parallel algorithms that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x <ref> [2, 5, 7, 13, 16, 17, 24, 26, 42] </ref>, minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44].
Reference: 43. <author> V. Ramachandran, </author> <title> Parallel open ear decomposition with applications to graph biconnec-tivity and triconnectivity, Synthesis of Parallel Algorithms (J. </title> <editor> H. Reif, ed.), </editor> <publisher> Morgan-Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 275-340. </pages>
Reference-contexts: that run in polylog time with linear or sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity <ref> [32, 37, 43] </ref>, open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. <p> sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity <ref> [32, 37, 43, 52] </ref>, triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms.
Reference: 44. <author> V. Ramachandran and J. Reif, </author> <title> Planarity testing in parallel, </title> <journal> Jour. Comput. and Sys. Sci. </journal> <volume> 49 (1994), no. 3, </volume> <pages> 517-561, </pages> <note> Special Issue for FOCS '89. </note>
Reference-contexts: fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity [32, 37, 43, 52], triconnectivity [12, 36] and planarity <ref> [44] </ref>. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. However, these algorithms are quite different from earlier linear time algorithms based on depth-first search [51] in that they are very modular in structure.
Reference: 45. <author> M. Reid-Miller, </author> <title> List ranking and list scan on the CRAY C-90, </title> <booktitle> Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1994, </year> <pages> pp. 104-113. </pages>
Reference-contexts: Related work on implementing combinatorial algorithms on massively parallel machines can be found in [1, 3, 4, 8, 9, 10, 11, 15, 16, 18, 19, 30, 38, 39, 41, 48]. Also there has been work reported on implementing combinatorial algorithms on a vector super computer <ref> [16, 45, 49] </ref> and on a distributed memory machine [29]. The rest of the paper is organized as follows. Section 2 describes the algorithms implemented which includes an algorithm that we devised for this project. Section 3 gives general fine-tuning techniques for our code. Section 4 describes the testing scheme.
Reference: 46. <author> J. H. Reif (ed.), </author> <title> Synthesis of parallel algorithms, </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: 1. Introduction Over the past decade there has been a large amount of work in the theory of efficient, highly parallel graph algorithm design <ref> [25, 27, 31, 46] </ref>. <p> A revised deterministic algorithm for finding connected components. the number of tree loops reduced in each iteration. There is only one hook per iteration. 2.4. A Simple Randomized Algorithm. Algorithm 4 is a simple randomized algorithm (see, e.g., Chapter 4.3 in <ref> [46] </ref>) that avoids the checking of rooted stars by making sure that each tree loop is a rooted star at the beginning of each hooking-and-pointer-jumping iteration. In each iteration, two hooks between rooted stars are performed. <p> A simple randomized algorithm for finding con nected components (see, e.g., Chapter 4.3 in <ref> [46] </ref>). (without virtual processing) as described in [22]. Since it is possible for a very dense graph to have a small number of vertices, but a very large number of edges, the problem with the system pseudo random bits remained for dense graphs even after this modification.
Reference: 47. <author> B. Schieber and U. Vishkin, </author> <title> On finding lowest common ancestors: Simplification and parallelization, </title> <journal> SIAM J. Comput. </journal> <volume> 17 (1988), no. 6, </volume> <pages> 1253-1262. </pages>
Reference-contexts: For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees [52], least common ancestors in trees <ref> [47, 52] </ref> and range minima [47, 52]. More complex algorithms, such as those for triconnectivity and planarity call subroutines for open ear decomposition, in addition to subroutines for more basic problems. <p> For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees [52], least common ancestors in trees <ref> [47, 52] </ref> and range minima [47, 52]. More complex algorithms, such as those for triconnectivity and planarity call subroutines for open ear decomposition, in addition to subroutines for more basic problems.
Reference: 48. <author> J. T. Schwartz, </author> <title> Ultracomputers, </title> <journal> ACM Trans. on Programming Languages and Systems 2 (1980), </journal> <month> 484-521. </month> <title> PARALLEL ALGORITHMS FOR CONNECTED COMPONENTS 19 </title>
Reference: 49. <author> T. J. She*er, </author> <title> Implementing the multiprefix operation on parallel and vector computers, </title> <booktitle> Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <year> 1993, </year> <pages> pp. 377-386. </pages>
Reference-contexts: Related work on implementing combinatorial algorithms on massively parallel machines can be found in [1, 3, 4, 8, 9, 10, 11, 15, 16, 18, 19, 30, 38, 39, 41, 48]. Also there has been work reported on implementing combinatorial algorithms on a vector super computer <ref> [16, 45, 49] </ref> and on a distributed memory machine [29]. The rest of the paper is organized as follows. Section 2 describes the algorithms implemented which includes an algorithm that we devised for this project. Section 3 gives general fine-tuning techniques for our code. Section 4 describes the testing scheme.
Reference: 50. <author> Y. Shiloach and U. Vishkin, </author> <title> An o(log n) parallel connectivity algorithm, </title> <journal> Journal of Algorithms (1982), </journal> <pages> 57-67. </pages>
Reference-contexts: An algorithm for finding connected components by Shiloach and Vishkin <ref> [50] </ref> as described in Chapter 5.1.3 of JaJa [25]. parent of its parent). For each vertex whose grandparent is different from its parent, we mark (concurrent write) a flag f for its grandparent. Any vertex that is marked is not in a rooted star. <p> Any vertex that is marked is not in a rooted star. Every unmarked vertex reads the flag f from its grandparent. Unmarked vertices whose grandparents are marked are also not in rooted stars. 2.2. Shiloach and Vishkin. The next algorithm we implemented (Algorithm 2) is by Shiloach and Vishkin <ref> [50] </ref> and also appears in Chapter 5.1.3 of JaJa [25]. In each of the hooking-and-pointer-jumping iteration of this algorithm, two hooks are performed as in Algorithm 1. <p> He also reported an implementation on a vector super computer Cray C-90 using one processor. Greiner implemented the algorithms of Shiloach and Vishkin <ref> [50] </ref> and Awerbuch and Shiloach [2], and the simple randomized algorithm that we implemented. Greiner did not use the system pseudo number generator for generating random bits in his randomized code. <p> Since we use only quarter the amount of memory as Greiner's implementation on the CM-2, it appears that our code uses less space than his. In [29], a distributed memory implementation of the algorithm by Shiloach and Vishkin <ref> [50] </ref> is reported. After fine-tuning, they obtain a speedup of 20 using a 32 processor CM-5 on grid graphs and obtain virtually no speedup on sparse random graphs. The performance of our massively parallel implementation seems to be more adaptable to different classes of graphs.
Reference: 51. <author> R. E. Tarjan, </author> <title> Depth-first search and linear graph algorithms, </title> <journal> SIAM J. Comput. </journal> <volume> 1 (1972), </volume> <pages> 146-160. </pages>
Reference-contexts: All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. However, these algorithms are quite different from earlier linear time algorithms based on depth-first search <ref> [51] </ref> in that they are very modular in structure. For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees [52], least common ancestors in trees [47, 52] and range minima [47, 52].
Reference: 52. <author> R. E. Tarjan and U. Vishkin, </author> <title> An efficient parallel biconnectivity algorithm, </title> <journal> SIAM J. Com-put. </journal> <volume> 14 (1985), </volume> <pages> 862-874. </pages> <institution> Inst. of Information Science, Academia Sinica, Nankang 115, Taipei, Taiwan, ROC E-mail address: tshsu@iis.sinica.edu.tw Dept. of Computer Sciences, Univ. of Texas at Austin, Austin, TX 78712, USA E-mail address: vlr@cs.utexas.edu S/W Production Research, AT&T Bell Labs., </institution> <address> Murray Hill, NJ 07960, USA E-mail address: nate@research.att.com </address>
Reference-contexts: sub-linear number of processors have been developed for several fundamental problems on undirected graphs including connected components and spanning forest x [2, 5, 7, 13, 16, 17, 24, 26, 42], minimum spanning forest (MSF) [2, 5, 6], ear decomposition and 2-edge connectivity [32, 37, 43], open ear decomposition and biconnectiv-ity <ref> [32, 37, 43, 52] </ref>, triconnectivity [12, 36] and planarity [44]. All of these algorithms (with the exception of some algorithms for MSF) have the additional feature that they serialize into linear-time sequential algorithms. <p> However, these algorithms are quite different from earlier linear time algorithms based on depth-first search [51] in that they are very modular in structure. For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees <ref> [52] </ref>, least common ancestors in trees [47, 52] and range minima [47, 52]. More complex algorithms, such as those for triconnectivity and planarity call subroutines for open ear decomposition, in addition to subroutines for more basic problems. <p> For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees [52], least common ancestors in trees <ref> [47, 52] </ref> and range minima [47, 52]. More complex algorithms, such as those for triconnectivity and planarity call subroutines for open ear decomposition, in addition to subroutines for more basic problems. <p> For instance, the algorithm for ear decomposition calls subroutines for several basic problems such as connected components, spanning forest, the Euler tour technique on trees [52], least common ancestors in trees <ref> [47, 52] </ref> and range minima [47, 52]. More complex algorithms, such as those for triconnectivity and planarity call subroutines for open ear decomposition, in addition to subroutines for more basic problems.
References-found: 49

