URL: http://www.cs.toronto.edu/~frey/papers/mfafr.ps.Z
Refering-URL: http://www.cs.toronto.edu/~frey/papers/mfafr.abs.html
Root-URL: http://www.cs.toronto.edu
Title: Mixtures of Local Linear Subspaces for Face Recognition  
Author: Brendan J. Frey (www.cs.utoronto.ca/frey), Antonio Colmenarez, Thomas S. Huang 
Affiliation: Beckman Institute for Advanced Science and Technology University of Illinois at Urbana-Champaign  
Abstract: Traditional subspace methods for face recognition compute a measure of similarity between images after projecting them onto a fixed linear subspace that is spanned by some principal component vectors (a.k.a. "eigenfaces") of a training set of images. By supposing a parametric Gaussian distribution over the subspace and a symmetric Gaussian noise model for the image given a point in the subspace, we can endow this framework with a probabilistic interpretation so that Bayes-optimal decisions can be made. However, we expect that different image clusters (corresponding, say, to different poses and expressions) will be best represented by different subspaces. In this paper, we study the recognition performance of a mixture of local linear subspaces model that can be fit to training data using the expectation maximization algorithm. The mixture model outperforms a nearest-neighbor classifier that operates in a PCA subspace. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Turk and A Pentland, </author> <title> "Eigenfaces for recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> vol. 3, no. 1, </volume> <year> 1991. </year>
Reference-contexts: In one approach to visual face modeling, normalized N -pixel face images are projected onto a subset of D eigenvectors or "eigenfaces" of the covariance matrix estimated from a training set of images <ref> [1] </ref>. The D-dimensional subspace spanned by these orthogonal eigenfaces is the subspace in which the training data has the greatest variance. In fact, these eigenfaces are equal to the first D principal components obtained from principal components analysis [2]. <p> The method of principal components analysis (PCA) finds the first D components having the D largest eigenvalues. Because of this eigenvalue formulation of PCA, the principal components of face images were dubbed "eigenfaces" in <ref> [1] </ref>. 2.1 Sensitivity of Eigenfaces to Variation in Pixel Noise We expect different regions of input images to have different levels of pixel noise that cannot be explained by our model.
Reference: [2] <author> I. T. Jolliffe, </author> <title> Principal Component Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> New York NY., </address> <year> 1986. </year>
Reference-contexts: The D-dimensional subspace spanned by these orthogonal eigenfaces is the subspace in which the training data has the greatest variance. In fact, these eigenfaces are equal to the first D principal components obtained from principal components analysis <ref> [2] </ref>. The distance of a new input image from this linear subspace has been used quite successfully to detect faces [3]. Moghaddam and Pentland [4] recently extended the eigenface framework to include a distance measure within the eigenspace.
Reference: [3] <author> A. Pentland, B. Moghaddam, and T. Starner, </author> <title> "View-based and modular eigenspaces for face recognition," </title> <booktitle> in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. </booktitle> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: In fact, these eigenfaces are equal to the first D principal components obtained from principal components analysis [2]. The distance of a new input image from this linear subspace has been used quite successfully to detect faces <ref> [3] </ref>. Moghaddam and Pentland [4] recently extended the eigenface framework to include a distance measure within the eigenspace. Although the eigenface method is currently one of the best algorithms for face recognition, it seems natural that different types of images ought to be better represented by different subspaces.
Reference: [4] <author> B. Moghaddam and A. Pentland, </author> <title> "Probabilistic visual learning for object recognition," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 19, no. 7, </volume> <pages> pp. 696-710, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: In fact, these eigenfaces are equal to the first D principal components obtained from principal components analysis [2]. The distance of a new input image from this linear subspace has been used quite successfully to detect faces [3]. Moghaddam and Pentland <ref> [4] </ref> recently extended the eigenface framework to include a distance measure within the eigenspace. Although the eigenface method is currently one of the best algorithms for face recognition, it seems natural that different types of images ought to be better represented by different subspaces. <p> We can add a second principal component (short line) and even try to model the data within the 2-dimensional subspace using a mixture of Gaussians (e.g., <ref> [4] </ref>). Although this will work for the toy data, we expect the manifold of faces to twist and turn so that each of many directions in pixel-space is significant somewhere on the manifold. As a result, PCA must ignore some directions of local variability. <p> Moghaddam and Pentland <ref> [4] </ref> append a Gaussian in-subspace model and a single off-subspace noise variance parameter to account for these deficiencies in PCA. Although this model can be estimated directly from a training set, the "noise" is actually structured since it lies in the null space of the linear subspace. <p> The application we have in mind is highly robust face recognition in a relatively closed environment, such as an office area. In this section, we compare the recognition performances of the mixture of local linear subspaces model and a method that performs nearest neighbor classification in a PCA subspace <ref> [4] </ref>. In these experiments, temporal structure is not modeled. Of course, treating the video as a time-series is expected to greatly improve performance and we are currently investigating temporal mixture models.
Reference: [5] <author> C. Bregler and S. M. Omohundro, </author> <title> "Surface learning with applications to lip-reading," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> Eds., </booktitle> <pages> pp. 43-50. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Fran-cisco CA., </address> <year> 1994. </year>
Reference-contexts: In such a model, the orientation of the subspace depends on the input image. Local dimensionality reduction of this type has been considered by Bregler and Omohundro <ref> [5] </ref>, Kambhatla and Leen [6], Sung and Poggio [7] and Hinton et al. [8]. These models can represent input images in locally linear, globally nonlinear subspaces, but they do not include a distance within the subspace.
Reference: [6] <author> N. Kambhatla and T. K. Leen, </author> <title> "Fast non-linear dimension reduction," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> Eds., </booktitle> <pages> pp. 152-159. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco CA., </address> <year> 1994. </year>
Reference-contexts: In such a model, the orientation of the subspace depends on the input image. Local dimensionality reduction of this type has been considered by Bregler and Omohundro [5], Kambhatla and Leen <ref> [6] </ref>, Sung and Poggio [7] and Hinton et al. [8]. These models can represent input images in locally linear, globally nonlinear subspaces, but they do not include a distance within the subspace.
Reference: [7] <author> K.-K. Sung and T. Poggio, </author> <title> "Example-based learning for view-based human face detection," MIT AI Memo 1521, </title> <type> CBCL Paper 112, </type> <year> 1994. </year>
Reference-contexts: In such a model, the orientation of the subspace depends on the input image. Local dimensionality reduction of this type has been considered by Bregler and Omohundro [5], Kambhatla and Leen [6], Sung and Poggio <ref> [7] </ref> and Hinton et al. [8]. These models can represent input images in locally linear, globally nonlinear subspaces, but they do not include a distance within the subspace.
Reference: [8] <author> G. E. Hinton, M. Revow, and P. Dayan, </author> <title> "Recognizing handwritten digits using mixtures of linear models," </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. Touretzky, and T. Leen, Eds. </editor> <year> 1995, </year> <pages> pp. 1015-1022, </pages> <publisher> MIT Press. </publisher>
Reference-contexts: In such a model, the orientation of the subspace depends on the input image. Local dimensionality reduction of this type has been considered by Bregler and Omohundro [5], Kambhatla and Leen [6], Sung and Poggio [7] and Hinton et al. <ref> [8] </ref>. These models can represent input images in locally linear, globally nonlinear subspaces, but they do not include a distance within the subspace.
Reference: [9] <author> B. J. Frey, </author> <title> Graphical Models for Machine Learning and Digital Communication, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA., </address> <year> 1998, </year> <note> See http://www.cs.utoronto.ca/frey. </note>
Reference-contexts: Using this approach, a variety of nonlinear latent variable models have been successfully applied to the task of pattern classification <ref> [9] </ref>. Recently, Hinton et al. [10] applied a mixture of linear subspaces to the task of handwritten character recognition. In this paper, we begin with a description of the eigenface model and its shortcomings, and then develop a mixture of local subspaces model for face recognition.
Reference: [10] <author> G. E. Hinton, P. Dayan, and M. Revow, </author> <title> "Modeling the manifolds of images of handwritten digits," </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: Using this approach, a variety of nonlinear latent variable models have been successfully applied to the task of pattern classification [9]. Recently, Hinton et al. <ref> [10] </ref> applied a mixture of linear subspaces to the task of handwritten character recognition. In this paper, we begin with a description of the eigenface model and its shortcomings, and then develop a mixture of local subspaces model for face recognition.
Reference: [11] <author> Z. Ghahramani and G. E. Hinton, </author> <title> "The EM algorithm for mixtures of factor analyzers," </title> <institution> University of Toronto Technical Report CRG-TR-96-1, </institution> <year> 1997. </year>
Reference-contexts: In this paper, we begin with a description of the eigenface model and its shortcomings, and then develop a mixture of local subspaces model for face recognition. We review a variation on the expectation maximization algorithm described in <ref> [11] </ref> and then give results on a new database that is being compiled at the Beckman Institute, University of Illinois at Urbana-Champaign. <p> They can be computed easily using linear algebra (all likelihoods are Gaussian). See <ref> [11] </ref> for details. 10 iterations of this algorithm were used to fit a mixture of 2 1-dimensional subspaces to the scatter-plot data shown in Fig. 3. The mixture model is clearly a better fit than the single 1-dimensional PCA model.
Reference: [12] <author> B. S. Everitt, </author> <title> An Introduction to Latent Variable Models, </title> <publisher> Chapman and Hall, </publisher> <address> New York NY., </address> <year> 1984. </year>
Reference-contexts: Although this model can be estimated directly from a training set, the "noise" is actually structured since it lies in the null space of the linear subspace. In contrast, factor analysis (FA) <ref> [12] </ref> is a probabilistic model where the noise on each pixel is independent.
Reference: [13] <author> B. J. Frey and G. E. Hinton, </author> <title> "Variational learning in non-linear Gaussian belief networks," </title> <note> Submitted to Neural Computation, </note> <year> 1998. </year>
Reference-contexts: This allows FA to model variation in pixel noise across the image. Notice that the columns of fl are not required to be orthogonal. That is, the factors may introduce variability in non-orthogonal directions in input space. This property is useful in nonlinear extensions of FA <ref> [13] </ref>. We fit a FA model with D = 10; 20; and 30 factors to a training set of 181 normalized front-view FERET face images, using 10 iterations of the EM algorithm [14]. Such a model with D factors for N - dimensional data has (D + 1)N parameters.
Reference: [14] <author> D. Rubin and D. Thayer, </author> <title> "EM algorithms for ML factor analysis," </title> <journal> Psychometrika, </journal> <volume> vol. 47, no. 1, </volume> <pages> pp. 69-76, </pages> <year> 1982. </year>
Reference-contexts: This property is useful in nonlinear extensions of FA [13]. We fit a FA model with D = 10; 20; and 30 factors to a training set of 181 normalized front-view FERET face images, using 10 iterations of the EM algorithm <ref> [14] </ref>. Such a model with D factors for N - dimensional data has (D + 1)N parameters. <p> We then apply Bayes' rule (2) to make a recognition decision. 4.1 Maximum Likelihood Parameter Es timation via the EM Algorithm The EM algorithm for this mixture model is similar to the EM algorithm for a single factor analysis model <ref> [14] </ref>, except that the E-step must now also fill in the subspace model identity k for each input image. The identity k of the subspace can be represented as a K-element binary "subspace indicator" vector s that has a 1 in the kth position and zeros in all other positions.
Reference: [15] <author> A. Colmenarez, B. J. Frey, and T. S. Huang, </author> <title> "Face detection and tracking using probability trees," </title> <note> in preparation, </note> <month> April </month> <year> 1998. </year>
Reference-contexts: The training set is completely separate from the test set, and each contains a total of 4000 images of the sort shown in Fig. 1. In this figure, the picture-in-picture images show the intensity-normalized output of our real-time face tracking system <ref> [15] </ref>. Fig. 5 shows the error rates for the nearest neighbor method and the mixture model as a function of recognition algorithm complexity (the number of multiplies required to recognize each input pattern).
References-found: 15

