URL: http://www.sdsc.edu/~tbailey/papers/clnl92.final2.ps
Refering-URL: 
Root-URL: 
Title: From: Computational Learning Theory and Natural Systems, Chapter 18, "Cross-validation and Modal Theories", Cross-Validation and
Author: Timothy L. Bailey Charles Elkan 
Address: San Diego 1  
Affiliation: Department of Computer Science and Engineering University of California,  
Date: 1995  October 1993  
Note: Vol. 3, MIT Press,  
Abstract: Cross-validation is a frequently used, intuitively pleasing technique for estimating the accuracy of theories learned by machine learning algorithms. During testing of a machine learning algorithm (foil) on new databases of prokaryotic RNA transcription promoters which we have developed, cross-validation displayed an interesting phenomenon. One theory is found repeatedly and is responsible for very little of the cross-validation error, whereas other theories are found very infrequently which tend to be responsible for the majority of the cross-validation error. It is tempting to believe that the most frequently found theory (the "modal theory") may be more accurate as a classifier of unseen data than the other theories. However, experiments showed that modal theories are not more accurate on unseen data than the other theories found less frequently during cross-validation. Modal theories may be useful in predicting when cross-validation is a poor estimate of true accuracy. We offer explanations 1 For correspondence: Department of Computer Science and Engineering, University of California, San 
Abstract-found: 1
Intro-found: 1
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, California, </address> <year> 1984. </year>
Reference-contexts: An obviously desirable quality for the learned rules is that they perform well (i.e. "generalize") on unseen examples. The generalization accuracy of the learned rules must be estimated. One popular and intuitively attractive method for estimating the accuracy of learned rules on unseen examples is cross-validation <ref> [ Breiman et al., 1984 ] </ref> . Cross-validation repeatedly splits the training examples into two subsets and learns a theory (synonymously, a rule) from the first subset and tests the theory ("cross-validates" it) by seeing how well it classifies the other subset.
Reference: [ Bucher, 1991 ] <author> Philipp Bucher. </author> <title> The Eukaryotic Promoter Database of the Weizmann Institute of Science. EMBL Nucleotide Sequence Data Library Release 29. </title> <institution> Weizmann Institute of Science, </institution> <year> 1991. </year>
Reference-contexts: A section of a DNA molecule can thus be represented as a string over the alphabet "A C G T". Table 1 shows a portion of a typical promoter dataset. We have prepared a number of new data sets by extracting sequences from a database of eukaryotic promoters <ref> [ Bucher, 1991 ] </ref> .
Reference: [ Efron, 1983 ] <author> Bradley Efron. </author> <title> Estimating the error rate of a prediction rule: Improvement on cross-validation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 78(382) </volume> <pages> 316-331, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: To quantify the concept of true error rate, we make the following definitions which are essentially the same as those in <ref> [ Efron, 1983 ] </ref> . An example x = (t; y) where t stands for the features of the example and y is the true classification of the example. Suppose a learning algorithm constructs prediction rule (t; X) from training set X.
Reference: [ Haussler, 1988 ] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: The Datalog theories discovered by foil on the promoter databases tended to consist of at least s = 2 clauses each with at least k = 2 literals. So, the hypothesis space has Vapnik-Chervonenkis dimension (VCdim) as defined in <ref> [ Haussler, 1988 ] </ref> of at least VCdim (H) j n k Since the number of attributes is n = 106, this gives a lower bound for VCdim of approximately 20. The sample complexity of foil on this problem is then given in [ Haussler, 1988 ] as (4 log (2=ffi) <p> has Vapnik-Chervonenkis dimension (VCdim) as defined in <ref> [ Haussler, 1988 ] </ref> of at least VCdim (H) j n k Since the number of attributes is n = 106, this gives a lower bound for VCdim of approximately 20. The sample complexity of foil on this problem is then given in [ Haussler, 1988 ] as (4 log (2=ffi) + 8 VCdim (H) log (13=*))=* ; So for * = ffi = :1, the sample complexity is at least 11,370 samples, far more than the actual sample sizes of (order) 100 samples. 18
Reference: [ Kononenko and Bratko, 1991 ] <author> Igor Kononenko and Ivan Bratko. </author> <title> Information-based evaluation criterion for classifiers' performance. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 67-80, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: By this argument, other methods which use smaller subsets for training can be expected to be poorer estimates of Err when the number of training examples available is small. v-fold cross-validation where v &lt; n falls into this category, as does the random-split method discussed in <ref> [ Kononenko and Bratko, 1991 ] </ref> . 2 Modal theories Each time v-fold cross-validation is used to estimate the error of a theory learned by a learning algorithm from a dataset, the learning algorithm is run v times.
Reference: [ Quinlan, 1990 ] <author> John R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Table 2 lists the number of positive and negative examples in each of the datasets. 4 The learning algorithm|FOIL The foil <ref> [ Quinlan, 1990 ] </ref> machine learning algorithm learns from data encoded as relations and outputs concepts in the form of simple Datalog programs.
Reference: [ Towell et al., 1990 ] <author> G. G. Towell, J. W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year> <month> 19 </month>
Reference-contexts: In most cases, the syntactically modal theory had a frequency near or above 50%. 3 The promoter datasets Several researchers have used the problem of recognizing RNA transcription promoters in DNA sequences as a test of various learning algorithms <ref> [ Towell et al., 1990 ] </ref> . An RNA transcription promoter is a section of a DNA molecule which binds with a particular protein which "promotes" the transcription of a gene from DNA to RNA. DNA molecules consist of sequences of four different nucleotides called bases.
References-found: 7

