URL: ftp://cns.brown.edu/nin/papers/bariconip.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: nin@cns.brown.edu  
Title: Neuronal Goals: Efficient Coding and Coincidence Detection  
Author: Nathan Intrator 
Keyword: Key words: Sparse coding, Non-Gaussian distributions, BCM Theory, Minimal Entropy  
Affiliation: School of Mathematical Sciences Tel Aviv University  
Abstract: Barlow's seminal work on minimal entropy codes and unsupervised learning is reiterated. In particular, the need to transmit the probability of events is put in a practical neuronal framework for detecting suspicious events. A variant of the BCM learning rule [15] is presented together with some mathematical results suggesting optimal minimal entropy coding. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. Atick. </author> <title> Could information theory provide an ecological theory of sensory processing? Network, </title> <booktitle> 3 </booktitle> <pages> 213-251, </pages> <year> 1992. </year>
Reference-contexts: We derive these assertions from basic principles of information theory, from energy conservation considerations, and from some assumptions about neuronal goals. Several other researchers have been interested in these questions. Atick <ref> [1] </ref> studied information coding patterns in flys and mammalians retinal coding and supports the notion of redundancy reduction through effective information coding. Field et al. [10, 21] inferred about the goal of visual sensory coding from properties of the statistics of natural images.
Reference: [2] <author> H. B. Barlow. </author> <title> Possible principles underlying the transfomations of sensory messages. </title> <editor> In W. Rosen-blith, editor, </editor> <booktitle> Sensory Communication, </booktitle> <pages> pages 217-234. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: We conclude with a motivation from the principle of maximum entropy to the optimality of the code. 2 Neuronal goal: Suspicious coincidences detection Barlow has been arguing for a long time that suspicious coincidences is the basic type of event to which the cerebral cortex must attune itself <ref> [2, 3, 6, 7] </ref>. Assuming that a major task of the brain is to form a statistical model of the world, Barlow asked what kind of events would be worth noting and keeping a record of.
Reference: [3] <author> H. B. Barlow. </author> <title> Cerebral cortex as model builder. </title> <editor> In D. Rose and V. G. Dobson, editors, </editor> <booktitle> Models of the visual cortex, </booktitle> <pages> pages 37-46. </pages> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: We conclude with a motivation from the principle of maximum entropy to the optimality of the code. 2 Neuronal goal: Suspicious coincidences detection Barlow has been arguing for a long time that suspicious coincidences is the basic type of event to which the cerebral cortex must attune itself <ref> [2, 3, 6, 7] </ref>. Assuming that a major task of the brain is to form a statistical model of the world, Barlow asked what kind of events would be worth noting and keeping a record of.
Reference: [4] <author> H. B. Barlow. </author> <title> Single units and sensation. </title> <journal> Perception, </journal> <volume> 1 </volume> <pages> 371-394, </pages> <year> 1989. </year>
Reference-contexts: This is because each BCM neuron conveys events with an activity that is inversely proportional to their probability of occurrence [15]. Thus, in accordance with Barlows predictions <ref> [4] </ref>, events that occur with high probability are conveyed by a less active neuron than events which occur less frequently. Such events will be conveyed by a neuron that is quiet most of the time, but fires strongly when the event is detected.
Reference: [5] <author> H. B. Barlow. </author> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 295-311, </pages> <year> 1989. </year>
Reference-contexts: There is an apparent contradiction between the desire to have redundant coding which is related to the simplicity of the code and the need for redundancy reduction as a mean for efficient transmission. Barlow stresses that a completely non redundant stimuli is indistinguishable from random noise <ref> [5] </ref>, thus, requiring a highly sophisticated scheme (probably complex and slow) to decode the signal. Since neuronal code seems highly structured, one can infer that its encoding is highly redundant.
Reference: [6] <author> H. B. Barlow. </author> <title> Conditions for versatile learning, helmholtz's unconscious inference, and the task of perception. </title> <journal> Vision Research, </journal> <volume> 30 </volume> <pages> 1561-1571, </pages> <year> 1990. </year>
Reference-contexts: We conclude with a motivation from the principle of maximum entropy to the optimality of the code. 2 Neuronal goal: Suspicious coincidences detection Barlow has been arguing for a long time that suspicious coincidences is the basic type of event to which the cerebral cortex must attune itself <ref> [2, 3, 6, 7] </ref>. Assuming that a major task of the brain is to form a statistical model of the world, Barlow asked what kind of events would be worth noting and keeping a record of. <p> the prior probability of what has been signaled. 2.1 Selfridge's Pandemonium and Barlow's Probabilistic Pandemonium The probabilistic line of reasoning suggests that sensory coding is "... the process of preparing a representation of the current sensory scene in a form that enables subsequent learning mechanisms to be versatile and reliable" <ref> [6] </ref>. Specifically, a representation is useful for learning if it includes records of recurring and co-occurring events. As noted by Barlow, a convenient substrate for such a representation is provided by Selfridge's Pandemonium [23].
Reference: [7] <author> H. B. Barlow. </author> <title> What is the computational goal of the neocortex. </title> <editor> In C. Koch and J. L. Davis, editors, </editor> <title> Large Scale Neuronal Theories of the Brain. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: We conclude with a motivation from the principle of maximum entropy to the optimality of the code. 2 Neuronal goal: Suspicious coincidences detection Barlow has been arguing for a long time that suspicious coincidences is the basic type of event to which the cerebral cortex must attune itself <ref> [2, 3, 6, 7] </ref>. Assuming that a major task of the brain is to form a statistical model of the world, Barlow asked what kind of events would be worth noting and keeping a record of.
Reference: [8] <author> E. L. Bienenstock, L. N Cooper, and P. W. Munro. </author> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48, </pages> <year> 1982. </year>
Reference-contexts: The dimensionality reduction is achieved by replacing each image (or its high dimensional equivalent vector) by a low dimensional vector in which each element represents a projection of the image onto a vector of synaptic weights. with a two-cluster data (right). The BCM feature extraction <ref> [8, 15] </ref> seeks multi-modality in the projected distribution of these high dimensional vectors. A simple example is illustrated in Figure 1.
Reference: [9] <author> S. Edelman. </author> <title> Class similarity and viewpoint invariance in the recognition of 3D objects. </title> <type> CS-TR 92-17, </type> <institution> Weizmann Institute of Science, </institution> <year> 1992. </year>
Reference-contexts: In particular, two events that are close to one another in measurement space, e.g. two views of the same person, should have an internal representation that preserves this correspondence <ref> [9, 25] </ref>. This requirement nullifies the use of classical coding theory, since we no longer can use a look-up table that translates the code into symbols to be conveyed from layer to layer, as the symbol representation does not preserve the metric of the original space.
Reference: [10] <author> D. J. </author> <title> Field. What is the goal of sensory coding. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 559-601, </pages> <year> 1994. </year>
Reference-contexts: Several other researchers have been interested in these questions. Atick [1] studied information coding patterns in flys and mammalians retinal coding and supports the notion of redundancy reduction through effective information coding. Field et al. <ref> [10, 21] </ref> inferred about the goal of visual sensory coding from properties of the statistics of natural images. Their main conclusions are the need to extract higher order statistics (i.e., more than linear and pairwise) and the need for sparse coding as a mean to achieve efficient information relay.
Reference: [11] <author> P. Foldiak. </author> <title> Forming sparse representations by local anti-Hebbian learning. </title> <journal> Biological Cybernetics, </journal> <volume> 64 </volume> <pages> 165-170, </pages> <year> 1990. </year>
Reference-contexts: It follows that in the BCM coding case, sparse coding is an outcome of the other constraints and not a direct goal by itself. It will be interesting to compare the resulting code with methods that maximize sparsity or kurtosis as a goal for neuronal coding and feature detection <ref> [11, 12, 21] </ref>. 6 Optimal neuronal code When seeking optimal neuronal code, we have to bear in mind that the code should preserve spatial relations between the inputs.
Reference: [12] <author> C. Fyfe and R. Baddeley. </author> <title> Finding compact and sparse-distributed representations of visual images. </title> <journal> Network, </journal> <volume> 6 </volume> <pages> 333-344, </pages> <year> 1995. </year>
Reference-contexts: It follows that in the BCM coding case, sparse coding is an outcome of the other constraints and not a direct goal by itself. It will be interesting to compare the resulting code with methods that maximize sparsity or kurtosis as a goal for neuronal coding and feature detection <ref> [11, 12, 21] </ref>. 6 Optimal neuronal code When seeking optimal neuronal code, we have to bear in mind that the code should preserve spatial relations between the inputs.
Reference: [13] <author> S. Geman and E. </author> <title> Bienenstock. Compositional vision, 1995. Talk given at the Object Features for Visual Shape Representation workshop, </title> <booktitle> NIPS. </booktitle>
Reference-contexts: Coincidence detection is also a key idea in the Compositional Machine framework presented by Geman and Bienenstock <ref> [13] </ref>. Consider the statistical problem of learning which tries to determine whether a compound event such as C followed by U is a random co-occurrence or a significant association.
Reference: [14] <author> N. Intrator. </author> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 98-107, </pages> <year> 1992. </year>
Reference-contexts: The potential importance of these features is related to their invariance properties, or their ability to generalize. Invariance properties of features extracted by this method have been demonstrated previously in various recognition tasks <ref> [14, 16, 17] </ref>. From a mathematical viewpoint, extracting features from gray level images is related to dimensionality reduction in a high dimensional vector space, in which an n fi k pixel image is considered to be a vector of length n fi k.
Reference: [15] <author> N. Intrator and L. N Cooper. </author> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17, </pages> <year> 1992. </year>
Reference-contexts: The dimensionality reduction is achieved by replacing each image (or its high dimensional equivalent vector) by a low dimensional vector in which each element represents a projection of the image onto a vector of synaptic weights. with a two-cluster data (right). The BCM feature extraction <ref> [8, 15] </ref> seeks multi-modality in the projected distribution of these high dimensional vectors. A simple example is illustrated in Figure 1. <p> As is seen in Figure 1 (right), this leads to a bimodal, or, in general, multi-modal, projected distribution. The mathematical results concerning the type of feature detection and coding is given in <ref> [15] </ref>. One of the results roughly says: Theorem With n clusters in an n-dimensional space, the only stable solutions are projections which are orthogonal to all but one of the clusters. <p> Clearly, if a neuron becomes tuned to an event with vanishing probability (although the probability of such case is going to zero as well) its activity may grow unbounded. This fact had motivated us in the past to apply a saturating sigmoidal transfer function to the neuronal activity <ref> [15] </ref>. Such saturation function should have an upper bound that is larger than 1, and in fact the upper bound will determine the smallest probability of events that the neuron can become tuned to. <p> This is because each BCM neuron conveys events with an activity that is inversely proportional to their probability of occurrence <ref> [15] </ref>. Thus, in accordance with Barlows predictions [4], events that occur with high probability are conveyed by a less active neuron than events which occur less frequently. Such events will be conveyed by a neuron that is quiet most of the time, but fires strongly when the event is detected. <p> This formulation lets us compare between different coding schemes which have the same mean activity. It can be extended to networks of neurons, and it follows that a network of BCM neurons as described in <ref> [15] </ref> maximizes entropy under the additional (independent) constraint of sparse coding. 7 Summary Motivated by Barlow's seminal work, we have presented a theory that includes feature detection and efficient feature coding. One possible application is a fundamental neuronal task of suspicious coincidence detection.
Reference: [16] <author> N. Intrator and J. I. Gold. </author> <title> Three-dimensional object recognition of gray level images: The usefulness of distinguishing features. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 61-74, </pages> <year> 1993. </year>
Reference-contexts: The potential importance of these features is related to their invariance properties, or their ability to generalize. Invariance properties of features extracted by this method have been demonstrated previously in various recognition tasks <ref> [14, 16, 17] </ref>. From a mathematical viewpoint, extracting features from gray level images is related to dimensionality reduction in a high dimensional vector space, in which an n fi k pixel image is considered to be a vector of length n fi k.
Reference: [17] <author> N. Intrator, D. Reisfeld, and Y. Yeshurun. </author> <title> Face recognition using a hybrid supervised/unsupervised neural network. </title> <journal> Pattern Recognition Letters, </journal> <volume> 17 </volume> <pages> 67-76, </pages> <year> 1996. </year>
Reference-contexts: The potential importance of these features is related to their invariance properties, or their ability to generalize. Invariance properties of features extracted by this method have been demonstrated previously in various recognition tasks <ref> [14, 16, 17] </ref>. From a mathematical viewpoint, extracting features from gray level images is related to dimensionality reduction in a high dimensional vector space, in which an n fi k pixel image is considered to be a vector of length n fi k.
Reference: [18] <author> E. T. Jaynes. </author> <title> Information theory and statistical mechanics I. </title> <journal> Phys. Rev., </journal> <volume> 106 </volume> <pages> 620-530, </pages> <year> 1957. </year>
Reference-contexts: We additionally assume that cell activity is non negative, so an inactive cell which dissipates the least amount of energy has zero activity. It turns out that the principle of maximum entropy <ref> [18, 19, For discussion] </ref>, is directly applicable in this case. It gives an explicit relation between neuronal activity (possibly firing rate) and the corresponding probability for this activity level, so as to maximize information capacity subject to a fixed mean activity.
Reference: [19] <author> E. T. Jaynes. </author> <title> On the rationale of maximum entropy methods. </title> <journal> Proc. IEEE, </journal> <volume> 70 </volume> <pages> 939-952, </pages> <year> 1982. </year>
Reference-contexts: We additionally assume that cell activity is non negative, so an inactive cell which dissipates the least amount of energy has zero activity. It turns out that the principle of maximum entropy <ref> [18, 19, For discussion] </ref>, is directly applicable in this case. It gives an explicit relation between neuronal activity (possibly firing rate) and the corresponding probability for this activity level, so as to maximize information capacity subject to a fixed mean activity.
Reference: [20] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: use a suboptimal distribution given by q i 's, then the entropy of the new code is less or equal that given in (4), more precisely [24] H q = i X q i log (q i =p i ); (5) namely the two entropies differ by the Kullback-Leibler divergence <ref> [20] </ref> between the given distribution q and the optimal one p. The latter term is non-negative and is zero if and only if q p. Thus, better distributions will have a small K-L divergence and smaller differences between the distributions are more desirable.
Reference: [21] <author> B. A. Olshausen and D. J. </author> <title> Field. Natural image statistics and efficient coding. </title> <note> Network (to appear), </note> <year> 1996. </year>
Reference-contexts: Several other researchers have been interested in these questions. Atick [1] studied information coding patterns in flys and mammalians retinal coding and supports the notion of redundancy reduction through effective information coding. Field et al. <ref> [10, 21] </ref> inferred about the goal of visual sensory coding from properties of the statistics of natural images. Their main conclusions are the need to extract higher order statistics (i.e., more than linear and pairwise) and the need for sparse coding as a mean to achieve efficient information relay. <p> It follows that in the BCM coding case, sparse coding is an outcome of the other constraints and not a direct goal by itself. It will be interesting to compare the resulting code with methods that maximize sparsity or kurtosis as a goal for neuronal coding and feature detection <ref> [11, 12, 21] </ref>. 6 Optimal neuronal code When seeking optimal neuronal code, we have to bear in mind that the code should preserve spatial relations between the inputs.
Reference: [22] <author> G. Sebestyen. </author> <title> Decision Making Processes in Pattern Recognition. </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: This space of solutions is minimal and sufficient for distinguishing between n clusters. In contrast, a discriminant analysis method for separation between n clusters <ref> [22, for review] </ref>, has n possible solutions, i.e., on the order of n 2 , and is thus, more likely to find correlated solutions.
Reference: [23] <author> O. G. Selfridge. Pandemonium: </author> <title> a paradigm for learning. </title> <booktitle> In The mechanisation of thought processes. </booktitle> <address> H.M.S.O., London, </address> <year> 1959. </year>
Reference-contexts: Specifically, a representation is useful for learning if it includes records of recurring and co-occurring events. As noted by Barlow, a convenient substrate for such a representation is provided by Selfridge's Pandemonium <ref> [23] </ref>. In Barlow's Probabilistic Pandemonium, the response strength of a feature-detector demon would be proportional to log P , where P is the probability of occurrence of the feature the demon detects.
Reference: [24] <author> C. J. Thompson. </author> <title> Classical Equilibrium Statistical Mechanics. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1988. </year>
Reference-contexts: If instead of using the optimal probability distribution given by (3), we use a suboptimal distribution given by q i 's, then the entropy of the new code is less or equal that given in (4), more precisely <ref> [24] </ref> H q = i X q i log (q i =p i ); (5) namely the two entropies differ by the Kullback-Leibler divergence [20] between the given distribution q and the optimal one p. The latter term is non-negative and is zero if and only if q p.
Reference: [25] <author> Y. Weiss and S. Edelman. </author> <title> Representation of similarity as a goal of early visual processing. </title> <journal> Network, </journal> <volume> 6 </volume> <pages> 19-41, </pages> <year> 1995. </year>
Reference-contexts: In particular, two events that are close to one another in measurement space, e.g. two views of the same person, should have an internal representation that preserves this correspondence <ref> [9, 25] </ref>. This requirement nullifies the use of classical coding theory, since we no longer can use a look-up table that translates the code into symbols to be conveyed from layer to layer, as the symbol representation does not preserve the metric of the original space.
References-found: 25

