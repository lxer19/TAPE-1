URL: http://www.cs.purdue.edu/research/cse/pythia/papers/conf-journal/pythia-tnn.ps.gz
Refering-URL: http://www.cs.purdue.edu/research/cse/pythia/pythia-papers.html
Root-URL: http://www.cs.purdue.edu
Title: On Neurobiological, Neuro-Fuzzy, Machine Learning and Statistical Pattern Recognition Techniques  
Author: Anupam Joshi, Member, IEEE, Narendran Ramakrishnan, Member, IEEE, Elias N. Houstis and John R. Rice 
Keyword: Pattern Recognition, Classification, Clustering, Neuro-Fuzzy Systems, Multiresolution, Vision Systems, Overlapping Classes, Comparative Experiments.  
Abstract: In this paper, we propose two new neuro-fuzzy schemes, one for classification and one for clustering problems. The classification scheme is based on Simpson's Fuzzy Min Max method, and relaxes some assumptions he makes. This enables our scheme to handle mutually non exclusive classes. The neuro-fuzzy clustering scheme is a multires-olution algorithm that is modeled after the mechanics of human pattern recognition. We also present data from an exhaustive comparison of these techniques with neural, statistical, machine learning and other traditional approaches to pattern recognition applications. The data sets used for comparisons include those from the machine learning repository at the University of California, Irvine. We find that our proposed schemes compare quite well with the existing techniques, and in addition offer the advantages of one pass learning and on-line adaptation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.J. </author> <title> Marks II, </title> <journal> "Intelligence: Computational versus artificial," IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 5, </volume> <year> 1993. </year>
Reference: [2] <author> D.O. Hebb, </author> <title> The Organisation of Behaviour : a neuropsychological theory, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, USA, </address> <year> 1949. </year>
Reference: [3] <author> W.S. McCulloch and W. Pitts, </author> <title> "A logical calculus of ideas immanent in nervous activity," </title> <journal> Bulletin of Mathematical BioPhysics, </journal> <volume> vol. 5, </volume> <pages> pp. 115-133, </pages> <year> 1943. </year>
Reference: [4] <author> F. Rosenblatt, </author> <title> Principles of Neurodynamics, </title> <publisher> Spartan Press, </publisher> <address> New York, </address> <year> 1962. </year>
Reference: [5] <author> M. </author> <title> Boden, </title> <booktitle> The Philosophy of Artificial Intelligence, </booktitle> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1990. </year>
Reference: [6] <author> D.E. Rumelhart and J.L. McClelland, </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference: [7] <author> A.K. Jain and J. Mao, </author> <title> "Neural networks and pattern recognition," in Computational Intelligence for Imitating Life, Zurada, </title> <editor> Marks and Robinson (eds.). </editor> <year> 1994, </year> <pages> pp. 194-212, </pages> <publisher> IEEE Press. </publisher>
Reference: [8] <author> I.K. Sethi and A.K. Jain, </author> <title> Artificial Neural Networks and Statistical Pattern Recognition, </title> <publisher> North Holland, </publisher> <address> Netherlands, </address> <year> 1991. </year>
Reference: [9] <author> B. Cheng and D.M. Titterington, </author> <title> "Neural networks: A review from a statistical perspective," </title> <journal> Statistical Science, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 2-54, </pages> <year> 1994. </year>
Reference: [10] <author> B.D. Ripley, </author> <title> "Neural networks: A review from a statistical perspective comment," </title> <journal> Statistical Science, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 45-48, </pages> <year> 1994. </year>
Reference: [11] <author> S. Amari, </author> <title> "Neural networks: A review from a statistical perspective comment," </title> <journal> Statistical Science, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 31-32, </pages> <year> 1994. </year>
Reference: [12] <author> J. L. McClelland, </author> <title> "Comment: </title> <journal> Neural networks and cognitive science: Motivations and applications," Statistical Science, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 42-45, </pages> <year> 1994. </year>
Reference: [13] <author> W. S. Sarle, </author> <title> "Neural networks and statistical models," </title> <booktitle> in Nineteenth Annual SAS Users Group International Conference, </booktitle> <year> 1994. </year>
Reference: [14] <author> B.D. Ripley, </author> <title> "Statistical aspects of neural networks," </title> <booktitle> in Proc. Neural Networks and Chaos Statistical and Probabilistic Aspects. </booktitle> <year> 1993, </year> <pages> pp. 40-123, </pages> <publisher> Chapman and Hall, London. </publisher>
Reference: [15] <author> B.D. Ripley, </author> <title> "Neural networks and related methods for classification," </title> <journal> J. Roy. Statistical Society, </journal> <volume> vol. 56, </volume> <year> 1994. </year>
Reference: [16] <author> P.V. Balakrishnan, M.C. Cooper, </author> <title> V.S. Jacob, and P.A. Lewis, "A study of the classification capabilities of neural networks using unsupervised learning: A comparison with k-means clustering," </title> <journal> Psy-chometrika, </journal> <volume> vol. 59, no. 4, </volume> <pages> pp. 509-525, </pages> <year> 1994. </year>
Reference: [17] <author> R.P.W. Duin, </author> <title> "A note on comparing classifiers," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 1, </volume> <pages> pp. 529-536, </pages> <year> 1996. </year>
Reference-contexts: The performance of these algorithms has been evaluated by applying them to several real world data sets. More information about these data sets is given in the next section. Some interesting observations on techniques used to compare classifiers can be found in <ref> [17] </ref>. A. Traditional method We started out with a traditional naive heuristic, which represented a pattern class as the centroid of all the known exemplars of the class.
Reference: [18] <author> R.M. Nosofsky, </author> <title> "Tests of a Generalized MDS-Choice Model of Stimulus Identification," </title> <type> Tech. Rep. 83, </type> <institution> Indiana University Cognitive Science Program, </institution> <year> 1992. </year>
Reference: [19] <author> J.M. Jolion and A. Rosenfel, </author> <title> A Pyramid Framework for Early Vision, </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: [20] <author> C. Enroth-Cugell and J. Robson, </author> <title> "The contrast sensitivity of retinal ganglion cells of the cat," </title> <journal> Journal Physiol. (London), </journal> <volume> vol. 187, </volume> <pages> pp. 517-522, </pages> <year> 1966. </year>
Reference: [21] <author> D. Marr and E. Hilderth, </author> <title> "The theory of edge detection," </title> <journal> Proc. Royal Soc. London, B, </journal> <volume> vol. 207, </volume> <pages> pp. 187-217, </pages> <year> 1980. </year>
Reference: [22] <author> A. Joshi and C.H. Lee, </author> <title> "Backpropagation learns marr's operator," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 70, </volume> <year> 1993. </year>
Reference: [23] <author> J. Daugman, </author> <title> "Pattern and motion vision without laplacian zero crossings," </title> <journal> Journal Opt. Soc. Am. A, </journal> <volume> vol. 5, </volume> <pages> pp. 1142-1148, </pages> <year> 1988. </year>
Reference: [24] <author> M. Livingstone and D.O. Hubel, </author> <title> "Segregation of form, color, movement and depth: anatomy, </title> <journal> physio logy and perception," Science, </journal> <volume> vol. 240, </volume> <pages> pp. 740-749, </pages> <year> 1988. </year>
Reference: [25] <author> D.O. Hubel, </author> <title> Eye, Brain and Vision, </title> <publisher> Scientific American Library, </publisher> <address> New York, USA, </address> <year> 1988. </year>
Reference: [26] <author> Z. Pizlo, A. Rosenfeld, and J. Epelboim, </author> <title> "An exponential pyramid model of the time course of size processing," </title> <journal> Vision Research, </journal> <volume> vol. 35, </volume> <pages> pp. 1089-1107, </pages> <year> 1995. </year>
Reference: [27] <author> P.K. Simpson, </author> <title> "Fuzzy Min-Max Neural Networks-Part 1: Classification," </title> <journal> IEEE Transactions on Neural Networks,, </journal> <volume> vol. vol.3, no. 5, </volume> <pages> pp. </pages> <address> pp.776-786, </address> <year> 1992. </year>
Reference: [28] <author> E. Gallopoulos, E. Houstis, and J.R. Rice, </author> <title> "Computer as Thinker/Doer: Problem-Solving Environments for Computational Science," </title> <journal> IEEE Computational Science and Enginerring,, </journal> <volume> vol. vol.1, no. 2, </volume> <pages> pp. </pages> <address> pp.11-23, </address> <year> 1994. </year>
Reference: [29] <author> A. Joshi, S. Weerawarana, N. Ramakrishnan, E.N. Houstis, and J.R. Rice, </author> <title> "Neuro-fuzzy support for problem solving environments," </title> <booktitle> IEEE Computational Science & Engg., Spring, </booktitle> <year> 1996. </year>
Reference-contexts: In this section, we therefore, concentrate on the PYTHIA dataset which comes from our work in scientific computing the efficient numerical solution of partial differential equations (PDEs) [55], [30], [56], <ref> [29] </ref>. PYTHIA is an intelligent computational assistant that prescribes an optimal strategy to solve a given PDE. This includes the method to use, the discretization to be employed and the hardware/software configuration of the computing environment.
Reference: [30] <author> N. Ramakrishnan, A. Joshi, S. Weerawarana, E.N. Houstis, and J.R. Rice, </author> <title> "Neuro-Fuzzy Systems for Intelligent Scientific Computing," </title> <booktitle> in Proc. Artificial Neural Networks in Engineering ANNIE '95, </booktitle> <year> 1995, </year> <pages> pp. 279-284. </pages>
Reference-contexts: In this section, we therefore, concentrate on the PYTHIA dataset which comes from our work in scientific computing the efficient numerical solution of partial differential equations (PDEs) [55], <ref> [30] </ref>, [56], [29]. PYTHIA is an intelligent computational assistant that prescribes an optimal strategy to solve a given PDE. This includes the method to use, the discretization to be employed and the hardware/software configuration of the computing environment.
Reference: [31] <author> P.K. Simpson, </author> <title> "Fuzzy Min-Max Neural Networks-Part 2: Clustering," </title> <journal> IEEE Transactions on Fuzzy Systems,, </journal> <volume> vol. vol.1, no. 1, </volume> <pages> pp. </pages> <address> pp.32-45, </address> <year> 1993. </year> <month> 13 </month>
Reference: [32] <author> R. Kohavi, G. John, R. Long, D. Manley, and K. Pfleger, "MLC++: </author> <title> A machine learning library in C++," </title> <booktitle> in Tools with Artificial Intelligence. </booktitle> <year> 1994, </year> <pages> pp. 740-743, </pages> <publisher> IEEE Computer Society Press, ftp://starry.stanford.edu/pub/ronnyk/mlc/toolsmlc.ps. </publisher>
Reference-contexts: B. Classical Machine Learning Algorithms Several algorithms that have been proposed by the AI community are described next. These include classical decision tree algorithms, native inducers and classical Bayesian classifiers. The implementations used are available in public domain in the MLC++ <ref> [32] </ref> (Machine Learning Library in C++). In addition to directly using the techniques presented next, we also tested their performance by combining them with other inducers to improve their behavior etc., We found the most useful of such "wrappers" to be the Feature Subset Selection (FSS) inducer. <p> 1 and k k 2 appear to perform better than k k 1 as they do a more reasonable task of `encapsulating' the information in the characteristic vector by a scalar. * CLASSICAL AI ALGORITHMS As described earlier, these algorithms are implemented in the Machine Learning library in C++ (MLC++) <ref> [32] </ref>. Table I shows the performance of these methods on each of the seven data sets. The values of accuracy indicate the performance when tarining with the larger training set, and with an FSS wrapper inducer.
Reference: [33] <author> J.R. Quinlan, </author> <title> "Induction of decision trees," </title> <journal> Machine Learning, </journal> <volume> vol. 1, </volume> <pages> pp. </pages> <address> pp.81-106, </address> <year> 1986. </year>
Reference-contexts: The FSS inducer operates by selecting a "good" subset of features to present to the algorithm for improved accuracy and performance. The effectiveness of this wrapper inducer is dealt with in a future section. * ID3 This is a classical iterative algorithm for constructing decision tress from examples <ref> [33] </ref>. The simplicity of the resulting decision trees is a characteristic of ID3's attribute selection heuristic.
Reference: [34] <author> R. Kohavi, </author> <title> "Bottom-up induction of oblivious, read-once decision graphs: Strengths and limitations," </title> <booktitle> in Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994, </year> <pages> pp. 613-618, </pages> <month> ftp://starry.stanford.edu/pub/ronnyk/aaai94.ps. </month>
Reference-contexts: This algorithm is based on the idea that it is less profitable to consider the training set, in its entirety, than an appropriately chosen part of it. * HOODG This is a greedy hill-climbing inducer for building decision graphs <ref> [34] </ref>. It does this in a bottom up manner. It was originally proposed to overcome the disadvantages of decision trees duplication of subtrees in disjunctive concepts (replication) and partitioning of data into fragments, where a high-arity attribute is tested at each node (fragmentation).
Reference: [35] <author> J. Dougherty, R. Kohavi, and M. Sahami, </author> <title> "Supervised and unsupervised discretization of continuous features," </title> <booktitle> in Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <year> 1995, </year> <month> ftp://starry.stanford.edu/pub/ronnyk/disc.ps. </month>
Reference-contexts: It however, does not cater to unknown values. HOODG suffers from irrelevant or weakly relevant features and also requires dis-cretized data. Thus, it must be used with another inducer and requires procedures like disc-filtering <ref> [35] </ref>. * Const This inducer just predicts a constant class for all the exemplars. The majority class present in the training set is chosen as this constant class. <p> More details about this algorithm can be obtained from [36]. * Disc-Bayes Better results to the Bayes inducer are provided by this algorithm. It achieves this by discretizing the continuous features. This preprocessing step is provided by chaining the disc-filter inducer to the naive-bayes inducer <ref> [35] </ref>, [41]. * OC1-Inducer This system is used for the induction of multivariate decision trees [42]. Such trees classify examples by testing linear combinations of the features at each non-leaf node in the decision tree.
Reference: [36] <author> D.W. Aha, </author> <title> "Tolerating noisy, irrelevant attributes in instance-based learning algorithms," </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 36, no. 1, </volume> <pages> pp. 267-287, </pages> <year> 1992. </year>
Reference-contexts: The majority class present in the training set is chosen as this constant class. Though this approach is very naive, its accuracy is very useful as the baseline accuracy. * IB Aha's Instance Based algorithms generate class predictions based only on specific instances <ref> [36] </ref>, [37]. These methods, thus, do not maintain any set of abstractions for the classes. The disadvantage is that these methods have large storage requirements, but these can be significantly reduced with minor sacrifices in learning rate and classification accuracy. <p> It is basically used for tolerating noisy, irrelevant and novel attributes in conventional instance based learning. It is still a research system and is not very robust. More details about this algorithm can be obtained from <ref> [36] </ref>. * Disc-Bayes Better results to the Bayes inducer are provided by this algorithm. It achieves this by discretizing the continuous features.
Reference: [37] <author> D. Wettschreck, </author> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> Ph.D. thesis, </type> <institution> Oregon State University, </institution> <year> 1994. </year>
Reference-contexts: The majority class present in the training set is chosen as this constant class. Though this approach is very naive, its accuracy is very useful as the baseline accuracy. * IB Aha's Instance Based algorithms generate class predictions based only on specific instances [36], <ref> [37] </ref>. These methods, thus, do not maintain any set of abstractions for the classes. The disadvantage is that these methods have large storage requirements, but these can be significantly reduced with minor sacrifices in learning rate and classification accuracy.
Reference: [38] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1993. </year>
Reference-contexts: The performance also degrades rapidly with attribute noise in the exemplars and hence, it becomes necessary to dis tinguish noisy instances. * C4.5 C4.5 is a decision tree cum rule-based system <ref> [38] </ref>. C4.5 has several options which can be tuned to suit a particular learning environment. Some of these options include varying the amount of pruning of the decision tree, choosing among n "best" trees, windowing, using noisy data and several options for the rule induction program.
Reference: [39] <author> P. Langley, W. Iba, and K. Thompson, </author> <title> "An analysis of bayesian classifiers," </title> <booktitle> in Tenth National Conference on Artificial Intelligence. </booktitle> <year> 1992, </year> <pages> pp. 223-228, </pages> <publisher> AAAI Press and MIT Press. </publisher>
Reference-contexts: The most used of these features are windowing and allowing C4.5 to build several trees and retaining the best. * Bayes The Bayes Inducer <ref> [39] </ref> computes conditional probabilities of the classes given the instance and picks the class with the highest posterior. Features are assumed to be independent but the algorithm is nevertheless robust in cases where this condition is not met.
Reference: [40] <author> R.C. Holte, </author> <title> "Very simple classification rules perform well on most commonly used datasets," </title> <journal> Machine Learning, </journal> <volume> vol. 11, </volume> <pages> pp. 63-90, </pages> <year> 1993. </year>
Reference-contexts: This involves considering the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. * oneR Holte's one-R <ref> [40] </ref> is a simple classifier that makes a "one-rule" which is a rule based on the value of a single attribute. It is based on the idea that very simple classification rules perform well on most commonly used datasets. It is most commonly implemented as a base inducer.
Reference: [41] <author> P. Langley and S. Sage, </author> <title> "Induction of selective bayesian classifiers," </title> <booktitle> in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence. </booktitle> <year> 1994, </year> <pages> pp. 399-406, </pages> <publisher> Morgan Kaufman, </publisher> <address> Seattle, WA. </address>
Reference-contexts: More details about this algorithm can be obtained from [36]. * Disc-Bayes Better results to the Bayes inducer are provided by this algorithm. It achieves this by discretizing the continuous features. This preprocessing step is provided by chaining the disc-filter inducer to the naive-bayes inducer [35], <ref> [41] </ref>. * OC1-Inducer This system is used for the induction of multivariate decision trees [42]. Such trees classify examples by testing linear combinations of the features at each non-leaf node in the decision tree. OC1 uses a combination of deterministic and randomized algorithms to heuristically "search" for a good tree.
Reference: [42] <author> S. K. Murthy, S. Kasif, and S. Salzberg, </author> <title> "A system for the induction of oblique decision trees," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 2, </volume> <pages> pp. 1-33, </pages> <year> 1994. </year>
Reference-contexts: It achieves this by discretizing the continuous features. This preprocessing step is provided by chaining the disc-filter inducer to the naive-bayes inducer [35], [41]. * OC1-Inducer This system is used for the induction of multivariate decision trees <ref> [42] </ref>. Such trees classify examples by testing linear combinations of the features at each non-leaf node in the decision tree. OC1 uses a combination of deterministic and randomized algorithms to heuristically "search" for a good tree.
Reference: [43] <institution> SAS Institute Inc., Cary, NC, </institution> <note> SAS/STAT User's Guide : Version 6., </note> <year> 1990. </year>
Reference-contexts: It has been experimentally observed that OC1 consistently finds much smaller trees than comparable methods using univariate tests. 5 C. Statistical Techniques The two basic statistical techniques commonly used for pattern classification are regression and discriminant analysis. We used the SAS/STAT routines <ref> [43] </ref> which implement these algorithms. Below, we describe briefly the basic ideas of these two techniques. * Regression Models Regression Analysis [44], [45] determines the relationship between one variable (also called the dependent or response variable) and another set of variables (called the independent variables). <p> See Table II for the performance of these methods. It is seen that the DISCRIM and LOGISTIC procedures consistently outperform the REG procedure. This can be explained as follows <ref> [43] </ref>: DISCRIM obeys a canonical discriminant analysis methodology in which canonical variables are derived from the quantitative data, which are linear combinations of the given variables. These canonical variables summarize "between-class" variation in the same manner in which Principal Components Analysis (PCA) performs total variation.
Reference: [44] <author> N. Draper and H. Smith, </author> <title> Applied Regression Analysis, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Statistical Techniques The two basic statistical techniques commonly used for pattern classification are regression and discriminant analysis. We used the SAS/STAT routines [43] which implement these algorithms. Below, we describe briefly the basic ideas of these two techniques. * Regression Models Regression Analysis <ref> [44] </ref>, [45] determines the relationship between one variable (also called the dependent or response variable) and another set of variables (called the independent variables). This relationship is often described in the form of several parameters. These parameters are adjusted until a reasonable measure of fit is attained.
Reference: [45] <author> S. Weisberg, </author> <title> Applied Linear Regression, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Statistical Techniques The two basic statistical techniques commonly used for pattern classification are regression and discriminant analysis. We used the SAS/STAT routines [43] which implement these algorithms. Below, we describe briefly the basic ideas of these two techniques. * Regression Models Regression Analysis [44], <ref> [45] </ref> determines the relationship between one variable (also called the dependent or response variable) and another set of variables (called the independent variables). This relationship is often described in the form of several parameters. These parameters are adjusted until a reasonable measure of fit is attained.
Reference: [46] <author> R. Fisher, </author> <title> "The Use of Multiple Measurements in Taxonomic Problems," </title> <journal> Annals of Eugenics,, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. </pages> <address> pp.179-188, </address> <year> 1936. </year>
Reference-contexts: The SAS/STAT REG procedure serves as a general purpose tool for regression by least squares and supports a diverse range of models. For methods of regression using logistic models, we used the SAS/STAT LOGISTIC procedure. * Discriminant Analysis Discriminant Analysis <ref> [46] </ref>, [47], [48] uses a function called a discriminant function to determine the class to which a given observation belongs, based on knowledge of the quantitative variables. This is also known as "Classificatory Discriminant Analysis". The SAS/STAT DISCRIM procedure computes discriminant functions to classify observations into two or more groups. <p> V. Description of clustering techniques Clustering is another fundamental procedure in pattern recognition. It can be regarded as a form of unsupervised inductive learning that looks for regularities in training exemplars. The clustering problem [58], <ref> [46] </ref> can be formally described as follows: Input A set of patterns X = fx 1 ; x 2 ; x 3 ; : : : ; x n g Output A cpartition of X that exhibits categorically homogeneous subsets, where 2 c n Different clustering methods have been proposed that
Reference: [47] <author> W.W. Cooley and P.R. Lohnes, </author> <title> Multivariate Data Analysis, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The SAS/STAT REG procedure serves as a general purpose tool for regression by least squares and supports a diverse range of models. For methods of regression using logistic models, we used the SAS/STAT LOGISTIC procedure. * Discriminant Analysis Discriminant Analysis [46], <ref> [47] </ref>, [48] uses a function called a discriminant function to determine the class to which a given observation belongs, based on knowledge of the quantitative variables. This is also known as "Classificatory Discriminant Analysis". The SAS/STAT DISCRIM procedure computes discriminant functions to classify observations into two or more groups.
Reference: [48] <editor> M.M. Tatsouka, </editor> <title> Multivariate Analysis, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The SAS/STAT REG procedure serves as a general purpose tool for regression by least squares and supports a diverse range of models. For methods of regression using logistic models, we used the SAS/STAT LOGISTIC procedure. * Discriminant Analysis Discriminant Analysis [46], [47], <ref> [48] </ref> uses a function called a discriminant function to determine the class to which a given observation belongs, based on knowledge of the quantitative variables. This is also known as "Classificatory Discriminant Analysis". The SAS/STAT DISCRIM procedure computes discriminant functions to classify observations into two or more groups.
Reference: [49] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams, </author> <title> "Learning Internal Representations by Error Propagation," in in Parallel Distributed Processing : Explorations in the Microstructure of Cognition, </title> <editor> D.E. Rumelhart and McClelland J.L., Eds., </editor> <volume> vol. </volume> <editor> I. </editor> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This is essentially using gradient descent on the error surface with respect to the weight values. For more details, see the classic text by Rumelhart & McClelland <ref> [49] </ref>. * BackProp with Momentum The second algorithm we consider modifies backpropagation by adding a fraction (the momentum parameter, ff) of the previous weight change during the computation of the new weight change [50].
Reference: [50] <author> A. Zell, N. Mache, R. Hubner, G. Mamier, M. Vogt, K. Herrmann, M. Schmalzl, T. Sommer, A. Hatzigeorgiou, S. Doring, and D. Pos-selt, </author> <title> "SNNS : Stuttgart Neural Network Simulator," </title> <type> Tech. Rep. 3/93, </type> <institution> Institute for Parallel and Distributed High Performance Systems, University of Stuutgart, Fed. </institution> <type> Rep. </type> <institution> of Germany, </institution> <year> 1993. </year>
Reference-contexts: For more details, see the classic text by Rumelhart & McClelland [49]. * BackProp with Momentum The second algorithm we consider modifies backpropagation by adding a fraction (the momentum parameter, ff) of the previous weight change during the computation of the new weight change <ref> [50] </ref>. This simple artifice helps moderate changes in the search direction, reduce the notorious oscillation problems common with gradient descent. To take care of the "plateaus", a "flat spot elimination constant" is added to the derivative of f . <p> Each network was trained till the weights converged i.e., when subsequent iterations did not cause any significant changes to the weight vector. Again, as mentioned previously, training was done with both the larger training set and the smaller set. All simulations were performed using the Stuttgart Neural Network Simulator <ref> [50] </ref>. The only "free" parameter in the simple backpropagation paradigm was the learning rate and it was varied in the range [0:1 : : : 0:9]. It was observed that the best performance, in terms of classification accuracy, was achieved at values of 0:8 0:95.
Reference: [51] <author> S.E. Fahlman, </author> <title> "Faster-learning Variations on Backpropagation : An Empirical Study," in in 1988 Connectionist Models Summer School, T.J. </title> <editor> Sejnowski, G.E. Hinton, and D.S. Touretzky, Eds. </editor> <publisher> Mor-gan Kaufmann, </publisher> <year> 1988. </year>
Reference: [52] <author> H. Braun and M. Riedmiller, </author> <title> "Rprop : A Fast and Robust Backpropagation Learning Strategy," </title> <booktitle> in Proceedings of the ACNN, </booktitle> <year> 1993. </year>
Reference: [53] <author> T. Kohonen, J. Kangas, J. Laaksoonen, and K. Torkolla, </author> <title> "LVQ-PAK Learning Vector Quantization Program Package," </title> <type> Tech. Rep. 2C, </type> <institution> Laboratory of Computer and Information Science, Rakentajanaukio, </institution> <year> 1992. </year>
Reference-contexts: The accuracy and time needed for learning depend on an appropriately chosen set of codebook vectors and the exact algorithm that modifies the codebook vectors. We have utilized four different implementations of the LVQ algorithm - LVQ1, OLVQ1, LVQ2 and LVQ3. LVQ PAK <ref> [53] </ref>, a LVQ program training package was used in the experiments. IV. Classification Results We evaluated the performance of the various classification algorithms described above by applying them to real world data sets.
Reference: [54] <author> P.M. Murphy and D.W. </author> <type> Aha, </type> <institution> "University of California, Irvine, </institution> <note> Repository of machine learning databases [http://www.ics.uci.edu/ mlearn/MLRepository.html]," 1994, </note> <institution> (Irvine, </institution> <address> CA). </address>
Reference-contexts: The PYTHIA data set contains classes that are not mutually-exclusive, the Soybean data set contains data that have missing features etc., These data sets, with the exception of PYTHIA, were obtained from the Machine Learning Repository at the University of Califor-nia, Irvine <ref> [54] </ref>, which also contains details about the information contained in these datasets and their characteristics. In this section, we therefore, concentrate on the PYTHIA dataset which comes from our work in scientific computing the efficient numerical solution of partial differential equations (PDEs) [55], [30], [56], [29].
Reference: [55] <author> S. Weerawarana, E.N. Houstis, J.R. Rice, and A. Joshi, </author> <title> "PYTHIA: </title>
Reference-contexts: In this section, we therefore, concentrate on the PYTHIA dataset which comes from our work in scientific computing the efficient numerical solution of partial differential equations (PDEs) <ref> [55] </ref>, [30], [56], [29]. PYTHIA is an intelligent computational assistant that prescribes an optimal strategy to solve a given PDE. This includes the method to use, the discretization to be employed and the hardware/software configuration of the computing environment.
References-found: 55

