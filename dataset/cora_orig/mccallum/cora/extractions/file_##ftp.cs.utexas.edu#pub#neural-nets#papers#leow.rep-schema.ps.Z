URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/leow.rep-schema.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: leow@cs.utexas.edu, risto@cs.utexas.edu  
Title: Representing Visual Schemas in Neural Networks for Scene Analysis  
Author: Wee Kheng Leow and Risto Miikkulainen 
Address: Austin, Texas 78712, USA  
Affiliation: Department of Computer Sciences, University of Texas at Austin,  
Abstract: Using object recognition in simple scenes as the task, this research focuses on two fundamental problems in neural network systems: (1) processing large amounts of input with limited resources, and (2) the representation and use of structured knowledge. The first problem arises because no practical neural network can process all the visual input simultaneously and efficiently. The solution is to process a small amount of the input in parallel, and successively focus on other parts of the input. This strategy requires that the system maintains structured knowledge for describing and interpreting successively gathered information. The proposed system, VISOR, consists of two main modules. The Low-Level Visual Module (simulated using procedural programs) extracts featural and positional information from the visual input. The Schema Module (implemented with neural networks) encodes structured knowledge about possible objects, and provides top-down information for the Low-Level Visual Module to focus attention at different parts of the scene. Working cooperatively with the Low-Level Visual Module, it builds a globally consistent interpretation of successively gathered visual information. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Tsotsos. </author> <title> How does human vision beat the computational complexity of visual perception? In Z. </title> <editor> W. Pylyshyn, editor, </editor> <booktitle> Computational Processes in Human Vision. </booktitle> <publisher> Ablex, </publisher> <address> Norwood, New Jersey, </address> <year> 1988. </year> <month> 5 </month>
Reference-contexts: Even if the network can capture a large part of the scene at once, it may not be able to process all the information in parallel unless it has an exponential amount of units and connections <ref> [1] </ref>. The only viable option is to process a small amount of visual input in parallel, and successively focus on different parts of the scene. This strategy also seems to be in use in biological vision systems [2].
Reference: [2] <author> J. E. Hochberg. </author> <title> Perception, 2nd Ed. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1978. </year>
Reference-contexts: The only viable option is to process a small amount of visual input in parallel, and successively focus on different parts of the scene. This strategy also seems to be in use in biological vision systems <ref> [2] </ref>. Since the network is fixed and finite, it may not have enough storage space for the indefinitely large amounts of input information. It will have to build and maintain a partial interpretation of the information gathered so far. <p> Each partial interpretation corresponds to an intermediate stable state of the network, and the globally consistent interpretation corresponds to the final stable state. A system that adopts this strategy requires an internal model, generally known as schema in psychological research, for making the interpretations <ref> [2] </ref>. Thus, the solution to the first problem requires that neural networks encode schemas, or in general, structured knowledge; that is, it requires addressing the second problem. One approach is to represent such knowledge symbolically in neural networks [3, 4, 5].
Reference: [3] <author> D. S. Touretzky. BoltzCONS: </author> <title> Reconciling connectionism with the recursive nature of stacks and trees. </title> <booktitle> In Proceedings of 8th Annual Conference of Cognitive Science Society, </booktitle> <pages> pages 522-530, </pages> <year> 1986. </year>
Reference-contexts: Thus, the solution to the first problem requires that neural networks encode schemas, or in general, structured knowledge; that is, it requires addressing the second problem. One approach is to represent such knowledge symbolically in neural networks <ref> [3, 4, 5] </ref>. The approach works in simple cases but does not generalize well to more complex tasks. Neural networks are not very good at manipulating symbols explicitly. However, they are good at feature extraction, association, constraint satisfaction, pattern classification, and making other fuzzy decisions.
Reference: [4] <author> J. Pollack. </author> <title> Recursive auto-associative memory: Devising compositional distributed representation. </title> <booktitle> In Proceedings of 10th Annual Conference of Cognitive Science Society, </booktitle> <pages> pages 33-39, </pages> <year> 1988. </year>
Reference-contexts: Thus, the solution to the first problem requires that neural networks encode schemas, or in general, structured knowledge; that is, it requires addressing the second problem. One approach is to represent such knowledge symbolically in neural networks <ref> [3, 4, 5] </ref>. The approach works in simple cases but does not generalize well to more complex tasks. Neural networks are not very good at manipulating symbols explicitly. However, they are good at feature extraction, association, constraint satisfaction, pattern classification, and making other fuzzy decisions.
Reference: [5] <author> D. S. Touretzky and G. E. Hinton. </author> <title> A distributed connectionist production system. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 423-466, </pages> <year> 1988. </year>
Reference-contexts: Thus, the solution to the first problem requires that neural networks encode schemas, or in general, structured knowledge; that is, it requires addressing the second problem. One approach is to represent such knowledge symbolically in neural networks <ref> [3, 4, 5] </ref>. The approach works in simple cases but does not generalize well to more complex tasks. Neural networks are not very good at manipulating symbols explicitly. However, they are good at feature extraction, association, constraint satisfaction, pattern classification, and making other fuzzy decisions.
Reference: [6] <author> David E. Rumelhart, P. Smolensky, James L. McClelland, and Geoffrey E. Hinton. </author> <title> Schemata and sequential thought processings in PDP models. </title> <editor> In James L. McClelland and David E. Rumelhart, editors, </editor> <booktitle> Parallel Distributed Processings. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Such knowledge can be conveniently encoded in terms of maps and connections among units. Despite the simplified task, this research aims at deriving general solutions that are applicable to more complex scenes and other tasks. II. Related Work Rumelhart et al. <ref> [6] </ref> suggested a general method for encoding conceptual schemas in a PDP model. Individual components of schemas, such as sofa, bed, bathtub, and toilet are represented as units in a network.
Reference: [7] <author> Geoffrey E. Hinton. </author> <title> Representing part-whole hierarchies in connectionist networks. </title> <booktitle> In Proceedings of 10th Annual Conference of Cognitive Science Society, </booktitle> <pages> pages 48-54, </pages> <year> 1988. </year>
Reference-contexts: The weight of the connection between two units represents how likely the two components are to be present in a schema, and the activity pattern of the network encodes a schema in-stantiation. The network does not encode hierarchical relationships among the schemas. Hinton <ref> [7] </ref> described three methods for representing hierarchical knowledge. The second method is similar to the one used in VISOR. The units in the network are organized into different levels. The higher the level, the more complex is the object that the unit represents.
Reference: [8] <author> D. A. Norman and T. Shallice. </author> <title> Attention to action: Willed and automatic control of behavior. </title> <type> Technical Report 99, </type> <institution> Center for Human Information Processing, Univ. of California, </institution> <address> San Diego, La Jolla, California, </address> <year> 1980. </year>
Reference-contexts: The higher the level, the more complex is the object that the unit represents. Lower-level units representing components of objects are connected to one or more higher-level units representing the objects themselves. The cognitive model of Norman and Shallice focuses on the activation and control of schemas <ref> [8, 9] </ref>. In this model, domain-specific action schemas and thought schemas can be activated independently of each other. A small subset of schemas to be "run" are selected by two distinct processes known as Contention Scheduling and Supervisory Attentional System.
Reference: [9] <author> T. Shallice. </author> <title> Specific impairments of planning. </title> <journal> Philosophical Transactions of the Royal Society of London B, </journal> <volume> 298 </volume> <pages> 199-209, </pages> <year> 1982. </year>
Reference-contexts: The higher the level, the more complex is the object that the unit represents. Lower-level units representing components of objects are connected to one or more higher-level units representing the objects themselves. The cognitive model of Norman and Shallice focuses on the activation and control of schemas <ref> [8, 9] </ref>. In this model, domain-specific action schemas and thought schemas can be activated independently of each other. A small subset of schemas to be "run" are selected by two distinct processes known as Contention Scheduling and Supervisory Attentional System.
Reference: [10] <author> David C. Van Essen and C. H. Anderson. </author> <title> Information processing strategies and pathways in the primate retina and visual cortex. </title> <editor> In S. F. Zornetzer, J. L. Davis, and C. Lau, editors, </editor> <title> Introduction to Neural and Electronic Networks. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1990. </year>
References-found: 10

