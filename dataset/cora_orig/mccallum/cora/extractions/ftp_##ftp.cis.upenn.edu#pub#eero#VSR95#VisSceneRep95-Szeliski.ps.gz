URL: ftp://ftp.cis.upenn.edu/pub/eero/VSR95/VisSceneRep95-Szeliski.ps.gz
Refering-URL: 
Root-URL: 
Title: Direct Methods for Visual Scene Reconstruction  
Author: Richard Szeliski and Sing Bing Kang 
Address: One Kendall Square, Bldg. 700 Cambridge, MA 02139  
Affiliation: Digital Equipment Corporation Cambridge Research Lab  
Note: To appear at the IEEE Workshop on Representations of Visual Scenes, June 24, 1995, Cambridge, MA 1  
Abstract: There has been a lot of activity recently surrounding the reconstruction of photorealistic 3-D scenes and high-resolution images from video sequences. In this paper, we present some of our recent work in this area, which is based on the registration of multiple images (views) in a projective framework. Unlike most other techniques, we do not rely on special features to form a projective basis. Instead, we directly solve a least-squares estimation problem in the unknown structure and motion parameters, which leads to statistically optimal estimates. We discuss algorithms for both constructing planar and panoramic mosaics, and for projective depth recovery. We also speculate about the ultimate usefulness of projective approaches to visual scene reconstruction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <journal> Computer Graphics (SIGGRAPH'93), </journal> <pages> pages 279-288, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The resulting depth-map is shown in Figure 5b as intensity-coded range values. 3.1 View Interpolation Once a dense depth map has been recovered for the scene, we can use this information to synthesize (interpolate or extrapolate) novel views <ref> [1, 11, 13] </ref>. When a Euclidean depth map is available, regular 3-D graphics can be used for the view synthesis [1]. <p> When a Euclidean depth map is available, regular 3-D graphics can be used for the view synthesis <ref> [1] </ref>. In other situations, corresponding points must be found between the original views and the novel view in order to compute the required transformations [11], or the projective depth description must be converted to a Euclidean one [2]. <p> While this may be adequate for certain tasks such as navigation, it is not that useful for tasks such as view-based recognition or virtual reality. The dense range maps available from multiframe stereo techniques are more interesting. They can be used to synthesize novel views using view interpolation <ref> [1] </ref>, even in the absence of full metric information [11]. For true virtual environments, however, multiple depth maps must be combined into a richer structure, which may require segmentation. Several alternatives exist for the representation of such environments. <p> One possibility would be to reduce the world to a collection of (hopefully continuous) planar surfaces [17], which could then be texture mapped. Another possibility is to have a collection of contiguous depth maps and images, which could then be rendered using either conventional graphics or multi-frame view interpolation <ref> [1] </ref>. The question of how to merge such multiple depth maps is an active research area. Such systems would also have to include multiresolution representations, at least if a large range of viewing positions or virtual camera settings were permitted.
Reference: [2] <author> O. Faugeras. </author> <title> Three-dimensional computer vision: A geometric viewpoint. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: When the camera motion is known, the problem of depth map recovery is called stereo reconstruction (or multi-frame stereo if more than two views are used). When the camera motion is unknown, we have the more difficult structure from motion problem <ref> [2, 15] </ref>. In this section, we present our solution to this latter problem based on recovering projective depth, which is particularly simple and robust and fits in well with the methods already developed in this paper. <p> ) to screen coordinates To appear at the IEEE Workshop on Representations of Visual Scenes, June 24, 1995, Cambridge, MA 5 u = (x; y; w) as where V is the upper triangular viewing matrix, and R and t are the usual rotational and translational components of the camera motion <ref> [2] </ref>. Without loss of generality, we can set R = I and t = 0 in the first frame. <p> Once the projective depth values are recovered, they can be used directly in viewpoint interpolation (using a new M and ~ t), or they can be converted to true Euclidean depth using at least 4 known depth measurements <ref> [2] </ref>. <p> In other situations, corresponding points must be found between the original views and the novel view in order to compute the required transformations [11], or the projective depth description must be converted to a Euclidean one <ref> [2] </ref>. A simpler approach, which often produces results of acceptable quality, is to simply re-scale the projective depths by an amount which yields a sensible 3-D scene when viewed from moderate viewing angles. This is the approach we used to generate the pictures in Figure 5. <p> In some situations, however, it may be necessary to bootstrap the dense depth recovery algorithm by first estimating the camera mo tion using a feature-based structure from motion algorithm. Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world <ref> [2] </ref>. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine [5, 16] or projective [3, 9, 11] description.
Reference: [3] <author> O. D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Second European Conference on Computer Vision (ECCV'92), </title> <address> pages 563-578, Santa Margherita Liguere, Italy, May 1992. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world [2]. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine [5, 16] or projective <ref> [3, 9, 11] </ref> description. The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11]. <p> The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates <ref> [3] </ref>. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11].
Reference: [4] <author> M. Hansen, P. Anandan, K. Dana, G. van der Wal, and P. Burt. </author> <title> Real-time scene stabilization and mosaic construction. </title> <booktitle> In IEEE Workshop on Applications of Computer Vision (WACV'94), </booktitle> <pages> pages 54-62, </pages> <address> Sarasota, Florida, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Building aerial photomosaics has long been a staple of photogrammetry, but only recently have fully automated techniques for building mosaics been developed. Most techniques still only estimate pure translations or affine transformations <ref> [4] </ref>, but some recent work has dealt with the full projective case [8]. Our approach is, to our knowledge, the first to combine full projective warping with near real-time performance.
Reference: [5] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> Affine structure from motion. </title> <journal> Journal of the Optical Society of America A, </journal> <volume> 8 </volume> <pages> 377-385538, </pages> <year> 1991. </year>
Reference-contexts: Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world [2]. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine <ref> [5, 16] </ref> or projective [3, 9, 11] description. The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11].
Reference: [6] <author> C. D. Kuglin and D. C. Hines. </author> <title> The phase correlation image alignment method. </title> <booktitle> In IEEE 1975 Conference on Cybernetics and Society, </booktitle> <pages> pages 163-165, </pages> <address> New York, </address> <month> September </month> <year> 1975. </year>
Reference-contexts: If the motion between successive frames is large, we use hierarchical matching, which first registers smaller, subsampled versions of the images where the apparent motion is smaller. For even larger displacements, we use phase correlation, which is a technique based on 2-D Fourier transforms <ref> [6] </ref>. To demonstrate the performance of our algorithm, we digitized an image sequence with a camera panning over a whiteboard. Figure 1 shows the final mosaic of the whiteboard, with the location of a constituent image shown as a white outline.
Reference: [7] <author> R. Kumar, P. Anandan, and K. Hanna. </author> <title> Direct recovery of shape from multiple views: A parallax based approach. </title> <booktitle> In Twelfth International Conference on Pattern Recognition (ICPR'94), volume A, </booktitle> <pages> pages 685-688, </pages> <address> Jerusalem, Israel, October 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This formulation has formed the basis of both our projective structure from motion algorithms [15] and our projective dense depth estimation algorithm [14]. More recently, it been used by other researchers under the names of affine depth [12] and planar parallax <ref> [10, 7] </ref> (see Section 4.2 for a more detailed discussion of projective depth). <p> This three-parameter ambiguity corresponds to choosing the plane relative to which the projectives depths d i are defined. Planar parallax techniques <ref> [10, 7] </ref> assume that this plane is the one with the dominant motion.
Reference: [8] <author> S. Mann and R. W. </author> <title> Picard. Virtual bellows: Constructing high-quality images from video. </title> <booktitle> In First IEEE International Conference on Image Processing (ICIP-94), </booktitle> <volume> volume I, </volume> <pages> pages 363-367, </pages> <address> Austin, Texas, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Building aerial photomosaics has long been a staple of photogrammetry, but only recently have fully automated techniques for building mosaics been developed. Most techniques still only estimate pure translations or affine transformations [4], but some recent work has dealt with the full projective case <ref> [8] </ref>. Our approach is, to our knowledge, the first to combine full projective warping with near real-time performance. Our techniques for automatically aligning images into pho-tomosaics exploit the particularly simple form of the motion field resulting from two specific imaging situations. <p> This is the approach used to composite a large wide-angle mosaic of Bryce Canyon, as shown in Figure 2. A third approach is to use a cylindrical viewing surface to represent the image mosaic <ref> [8] </ref>. In this approach, we map world coordinates p = (x; y; z) onto 2-D cylindrical screen locations u = (; v), with = tan 1 (x=z) and v = y= x 2 + z 2 .
Reference: [9] <author> R. Mohr, L. Veillon, and L. Quan. </author> <title> Relative 3D reconstruction using multiple uncalibrated images. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 543-548, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world [2]. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine [5, 16] or projective <ref> [3, 9, 11] </ref> description. The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11].
Reference: [10] <author> H. S. Sawhney. </author> <title> 3D geometry from planar parallax. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), </booktitle> <pages> pages 929-934, </pages> <address> Seattle, Washington, June 1994. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: This formulation has formed the basis of both our projective structure from motion algorithms [15] and our projective dense depth estimation algorithm [14]. More recently, it been used by other researchers under the names of affine depth [12] and planar parallax <ref> [10, 7] </ref> (see Section 4.2 for a more detailed discussion of projective depth). <p> This three-parameter ambiguity corresponds to choosing the plane relative to which the projectives depths d i are defined. Planar parallax techniques <ref> [10, 7] </ref> assume that this plane is the one with the dominant motion.
Reference: [11] <author> A. Shashua. </author> <title> Projective depth: A geometric invariant for 3D reconstruction from two perspective/orthographic views and for visual recognition. </title> <booktitle> In Fourth International Conference on Computer Vision (ICCV'93), </booktitle> <pages> pages 583-590, </pages> <address> Berlin, Ger-many, May 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The resulting depth-map is shown in Figure 5b as intensity-coded range values. 3.1 View Interpolation Once a dense depth map has been recovered for the scene, we can use this information to synthesize (interpolate or extrapolate) novel views <ref> [1, 11, 13] </ref>. When a Euclidean depth map is available, regular 3-D graphics can be used for the view synthesis [1]. <p> When a Euclidean depth map is available, regular 3-D graphics can be used for the view synthesis [1]. In other situations, corresponding points must be found between the original views and the novel view in order to compute the required transformations <ref> [11] </ref>, or the projective depth description must be converted to a Euclidean one [2]. A simpler approach, which often produces results of acceptable quality, is to simply re-scale the projective depths by an amount which yields a sensible 3-D scene when viewed from moderate viewing angles. <p> Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world [2]. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine [5, 16] or projective <ref> [3, 9, 11] </ref> description. The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11]. <p> The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition <ref> [11] </ref>. <p> The dense range maps available from multiframe stereo techniques are more interesting. They can be used to synthesize novel views using view interpolation [1], even in the absence of full metric information <ref> [11] </ref>. For true virtual environments, however, multiple depth maps must be combined into a richer structure, which may require segmentation. Several alternatives exist for the representation of such environments.
Reference: [12] <author> A. Shashua and N. Navab. </author> <title> Relative affine structure: Theory and applications to 3D reconstruction from perspective views. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), </booktitle> <pages> pages 483-489, </pages> <address> Seattle, Washington, June 1994. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: This formulation has formed the basis of both our projective structure from motion algorithms [15] and our projective dense depth estimation algorithm [14]. More recently, it been used by other researchers under the names of affine depth <ref> [12] </ref> and planar parallax [10, 7] (see Section 4.2 for a more detailed discussion of projective depth).
Reference: [13] <author> R. Szeliski. </author> <title> Image mosaicing for tele-reality applications. </title> <booktitle> In IEEE Workshop on Applications of Computer Vision (WACV'94), </booktitle> <pages> pages 44-53, </pages> <address> Sarasota, Florida, </address> <month> December </month> <year> 1994. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Traditionally, 3-D scene reconstruction has been the focus of both stereo and structure from motion, two subfields with complementary sets of assumptions and techniques. In this paper, we present some of our recent techniques in this area, which blend aspects of both stereo and structure from motion <ref> [14, 15, 13] </ref>. We call our techniques direct, since they both directly minimize an image-based misregistration measure (without special algebraic or geometric transformations), and because they are (usually) based on the direct minimization of intensity errors. <p> Finally, we discuss possible visual scene representations based on our techniques, and some potential applications. 2 Video Mosaics The first technique we describe automatically aligns and composites multiple images into high-resolution mosaics <ref> [13] </ref>. Building aerial photomosaics has long been a staple of photogrammetry, but only recently have fully automated techniques for building mosaics been developed. Most techniques still only estimate pure translations or affine transformations [4], but some recent work has dealt with the full projective case [8]. <p> Under To appear at the IEEE Workshop on Representations of Visual Scenes, June 24, 1995, Cambridge, MA 2 either of these two conditions, the inter-frame motion can be represented by a homography, i.e., a linear function of projective image coordinates u 0 = Mu (see <ref> [13] </ref> for a simple proof). <p> To reduce visible artifacts, we weight images being blended together more heavily towards the center, using a bilinear weighting function. To perform the minimization, we use the Levenberg-Marquardt iterative non-linear minimization algorithm (see <ref> [14, 13] </ref> for details). The advantage of using Levenberg-Marquardt over straightforward gradient descent is that it converges in fewer iterations. Unfortunately, both gradient descent and Levenberg-Mar-quardt only find locally optimal solutions. <p> This mosaic is 1300fi2046 pixels, based on compositing 39 NTSC (640fi480) resolution images. 2.2 Panoramic Mosaics In order to build a panoramic image mosaic, we rotate a camera around its optical center. Images taken in this manner are related by 2-D projective transformations, just as in the planar case <ref> [13] </ref>. Intuitively, we cannot tell the relative depth of points in the scene as we rotate (there is no motion parallax), so they could be located on a plane. To appear at the IEEE Workshop on Representations of Visual Scenes, June 24, 1995, Cambridge, MA 3 whole mosaic. <p> To appear at the IEEE Workshop on Representations of Visual Scenes, June 24, 1995, Cambridge, MA 4 More formally, the 2-D transformation denoted by M is related to the 3 fi 3 viewing matrices V and V 0 and the interview rotation matrix R by <ref> [13] </ref> M = V 0 RV 1 (3) (see Section 3 for definitions of V and R). In the case of a completely calibrated camera, M is a pure rotation matrix (only three unknowns). <p> How do we represent a panoramic scene composited using our techniques? One approach is to divide the viewing sphere into several large, potentially overlapping regions, and to represent each region with a plane onto which we paste the images. Examples of such mosaics are given in <ref> [13] </ref>. Another approach is to compute the relative position of each frame relative to some base frame, and to periodically choose a new base frame for doing the alignment. <p> The resulting depth-map is shown in Figure 5b as intensity-coded range values. 3.1 View Interpolation Once a dense depth map has been recovered for the scene, we can use this information to synthesize (interpolate or extrapolate) novel views <ref> [1, 11, 13] </ref>. When a Euclidean depth map is available, regular 3-D graphics can be used for the view synthesis [1].
Reference: [14] <author> R. Szeliski and J. Coughlan. </author> <title> Hierarchical spline-based image registration. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'94), </booktitle> <pages> pages 194-201, </pages> <address> Seattle, Washington, June 1994. </address> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Traditionally, 3-D scene reconstruction has been the focus of both stereo and structure from motion, two subfields with complementary sets of assumptions and techniques. In this paper, we present some of our recent techniques in this area, which blend aspects of both stereo and structure from motion <ref> [14, 15, 13] </ref>. We call our techniques direct, since they both directly minimize an image-based misregistration measure (without special algebraic or geometric transformations), and because they are (usually) based on the direct minimization of intensity errors. <p> The approach we have taken is to directly minimize the discrepancy in intensities between pairs of images after applying the transformation we are recovering. Our technique does not require the location and correspondence of feature points, and is statistically optimal in the vicinity of the true solution <ref> [14] </ref>. <p> To reduce visible artifacts, we weight images being blended together more heavily towards the center, using a bilinear weighting function. To perform the minimization, we use the Levenberg-Marquardt iterative non-linear minimization algorithm (see <ref> [14, 13] </ref> for details). The advantage of using Levenberg-Marquardt over straightforward gradient descent is that it converges in fewer iterations. Unfortunately, both gradient descent and Levenberg-Mar-quardt only find locally optimal solutions. <p> This formulation has formed the basis of both our projective structure from motion algorithms [15] and our projective dense depth estimation algorithm <ref> [14] </ref>. More recently, it been used by other researchers under the names of affine depth [12] and planar parallax [10, 7] (see Section 4.2 for a more detailed discussion of projective depth). <p> In our current implementation, in order to reduce the total number of parameters being estimated, we represent the depth map using a tensor-product spline, and only recover the depth estimates at the spline control vertices (the complete depth map is available by interpolation) <ref> [14] </ref>. the camera up and over the scene of a table with stacks of papers (Figure 5a).
Reference: [15] <author> R. Szeliski and S. B. Kang. </author> <title> Recovering 3D shape and motion from image streams using nonlinear least squares. </title> <journal> Journal of Visual Communication and Image Representation, </journal> <volume> 5(1) </volume> <pages> 10-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Traditionally, 3-D scene reconstruction has been the focus of both stereo and structure from motion, two subfields with complementary sets of assumptions and techniques. In this paper, we present some of our recent techniques in this area, which blend aspects of both stereo and structure from motion <ref> [14, 15, 13] </ref>. We call our techniques direct, since they both directly minimize an image-based misregistration measure (without special algebraic or geometric transformations), and because they are (usually) based on the direct minimization of intensity errors. <p> When the camera motion is known, the problem of depth map recovery is called stereo reconstruction (or multi-frame stereo if more than two views are used). When the camera motion is unknown, we have the more difficult structure from motion problem <ref> [2, 15] </ref>. In this section, we present our solution to this latter problem based on recovering projective depth, which is particularly simple and robust and fits in well with the methods already developed in this paper. <p> The world coordinates corresponding to an optical ray (in the first image) passing through u are therefore p = V 1 u where d is the projective depth of the world point <ref> [15] </ref>. <p> This formulation has formed the basis of both our projective structure from motion algorithms <ref> [15] </ref> and our projective dense depth estimation algorithm [14]. More recently, it been used by other researchers under the names of affine depth [12] and planar parallax [10, 7] (see Section 4.2 for a more detailed discussion of projective depth). <p> The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11]. Our structure from motion algorithm <ref> [15] </ref> directly minimizes (using Levenberg-Marquardt) the squared difference between predicted and measured screen coordinates E = j i ij [(u ij x 0 ij ) 2 ]; (8) where (u ij ; v ij ) is the screen location of the ith feature in the jth frame, and (x 0 ij <p> Furthermore, this approach provides explicit measures of uncertainty in the estimates, which can be used to great advantage when processing sequences of data. Carefully choosing the coordinate frame for the structure reconstruction, i.e., using an object-centered representation, can dramatically improve the quality of Euclidean reconstruction <ref> [16, 15] </ref>. This advantage is shared by many projective reconstruction techniques, which often choose the reconstruction plane to be located near the interesting structure. Many structure from motion algorithms are also restricted to using only a few points or frames.
Reference: [16] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Traditional structure from motion algorithms attempt to recover a Euclidean reconstruction of the world [2]. More re cent algorithms, motivated by the difficulty of obtaining metrically accurate 3-D reconstructions, have attacked the prob lem of recovering an affine <ref> [5, 16] </ref> or projective [3, 9, 11] description. The advantage of this approach is that it does not require camera calibration and can lead to more reliable estimates [3]. It may also be sufficient for many vision-based tasks such as re-projection and object recognition [11]. <p> It turns out that in the orthographic case, i.e., for affine structure from motion (where the denominators in (7) are unity), this two step approach results in an exact solution (in the noise-free case), and is equivalent to singular value decomposition <ref> [16] </ref> but at a lower computational cost. For perspective projection, the planar motion computed by the first step may not correspond to the motion of an actual plane, but this will be corrected during the iterative minimization, which often converges in just a single step. <p> Furthermore, this approach provides explicit measures of uncertainty in the estimates, which can be used to great advantage when processing sequences of data. Carefully choosing the coordinate frame for the structure reconstruction, i.e., using an object-centered representation, can dramatically improve the quality of Euclidean reconstruction <ref> [16, 15] </ref>. This advantage is shared by many projective reconstruction techniques, which often choose the reconstruction plane to be located near the interesting structure. Many structure from motion algorithms are also restricted to using only a few points or frames.
Reference: [17] <author> J. Y. A. Wang and E. H. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'93), </booktitle> <pages> pages 361-366, </pages> <address> New York, New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For true virtual environments, however, multiple depth maps must be combined into a richer structure, which may require segmentation. Several alternatives exist for the representation of such environments. One possibility would be to reduce the world to a collection of (hopefully continuous) planar surfaces <ref> [17] </ref>, which could then be texture mapped. Another possibility is to have a collection of contiguous depth maps and images, which could then be rendered using either conventional graphics or multi-frame view interpolation [1]. The question of how to merge such multiple depth maps is an active research area.
References-found: 17

