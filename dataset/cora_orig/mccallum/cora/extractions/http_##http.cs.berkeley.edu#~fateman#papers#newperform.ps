URL: http://http.cs.berkeley.edu/~fateman/papers/newperform.ps
Refering-URL: http://http.cs.berkeley.edu/~fateman/algebra.html
Root-URL: 
Title: Speeding up Lisp-based Symbolic Mathematics  
Author: Richard J. Fateman and Mark Hayden 
Address: Berkeley  
Affiliation: Computer Science Division, EECS Dep't, University of California at  
Abstract: Two techniques for speeding up a traditional Lisp-based symbolic manipulation system (Macsyma) were outlined in a previous paper [2]. These were: using unique representations for equivalent "kernel" expressions (where a kernel here is basically anything but a sum), and using hash tables for an unordered representation of sums. New tests in the context of a complete version of Macsyma suggest that a speedup of | in some cases | orders of magnitude can be obtained. In some cases this appears to match or exceed the speed of Mathematica, a recently written system written in a lower-level language (e.g. C). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Fateman. </author> <title> MACSYMA's General Simplifier: Philosophy and Operation. </title> <editor> In V. E. Lewis, ed. </editor> <booktitle> Proc. of the 1979 MACSYMA Users' Conference (MUC-79), </booktitle> <address> Washington, D.C., </address> <month> June 20-22, </month> <year> 1979, </year> <institution> MIT Lab. for Computer Science, </institution> <month> 563-582. </month>
Reference-contexts: This notion of "contract" is entirely informal, and appears nowhere in the original Macsyma code except (rarely) in comments. It is therefore non-trivial to enforce such contracts on new non-conforming data <ref> [1] </ref>. Our approach was to establish a global flag that is tested to see whether we must return an old-style list data-type (ret2list = t) or not. <p> Furthermore, subsystems that do not use the general algebraic list form for their major computation will not be affected at all. Some of the more efficient code in Macsyma already uses a rather different form, canonical rational expression (CRE) form <ref> [1] </ref>. The only changes needed for integration of these changes with CRE form have to do with conversion from a hash table into CRE form.
Reference: [2] <author> R. Fateman. </author> <title> "Canonical Representations in Lisp and Applications to Computer Algebra Systems," </title> <publisher> ISSAC-91. </publisher>
Reference-contexts: Each time we compute a Lisp cons in constructing an algebraic expression, we check first to see if another cons cell exists with the same car and cdr. This checking is done by installing all such "unique" cons cell in a hash table (see Fateman <ref> [2] </ref>). This is a substantial additional overhead on a per-cons basis, in the hope of reaping returns in more efficient processing. <p> The earlier benchmarks in section 2 benefitted from this. As we said earlier <ref> [2] </ref> citing a a 1981 paper [3], we found that in a typical run of Macsyma, some 60% of the execution time is spent executing code in the kernel of Lisp.
Reference: [3] <author> J. Foderaro and R. Fateman. </author> <title> Characterization of VAX Macsyma. </title> <editor> In P. S. Wang, (ed.) </editor> <booktitle> Proc. of 1981 ACM Symp. on Symbolic and Algebraic Comp. </booktitle> <address> Snow-bird, Utah, </address> <month> Aug, </month> <year> 1981 </year> <month> 14-19. </month>
Reference-contexts: The earlier benchmarks in section 2 benefitted from this. As we said earlier [2] citing a a 1981 paper <ref> [3] </ref>, we found that in a typical run of Macsyma, some 60% of the execution time is spent executing code in the kernel of Lisp.
Reference: [4] <author> Mark Hayden. </author> <title> "Fast structural comparisons in Lisp," term paper for CS282, </title> <booktitle> Fall, </booktitle> <year> 1991. </year> <institution> (R. Fateman), EECS Dept. Univ. Calif. Berkeley. </institution>
Reference: [5] <author> E. Goto and Y. Kanada. </author> <title> Hashing Lemmas on Time Complexities with Applications to Formula Manipulation. </title> <editor> In R. D. Jenks, (ed.) </editor> <booktitle> Proc. of 1976 ACM Symp. on Symbolic and Algebraic Comp. </booktitle> <address> Yorktown Heights, N. Y., </address> <month> August, </month> <year> 1976. </year> <pages> 154-158. </pages>
Reference-contexts: That is, all references to this expression must be pointers to the same location in memory. The basic idea behind the "hashing cons" we use was expressed in 1976 by Goto and Kanada <ref> [5] </ref>. Each time we compute a Lisp cons in constructing an algebraic expression, we check first to see if another cons cell exists with the same car and cdr. This checking is done by installing all such "unique" cons cell in a hash table (see Fateman [2]).
Reference: [6] <author> Donald Michie. </author> <title> "Memo" Functions and Machine Learning, </title> <journal> Nature, </journal> <volume> 218, </volume> <year> (1968), </year> <pages> 19-22. </pages>
Reference-contexts: Though some structural comparisons are fast, there is nothing to prevent structural comparisons from having an average behavior close to worst-case. 3.3 A Naive Speedup An obvious way to speed up structural comparisons is to "memoize" the comparison function <ref> [6] </ref>. Memoization involves saving the results of all comparisons. Before comparing two structures, a check is made to see if that comparison has already been made. If it has, then the old result can be returned.
Reference: [7] <author> Tak W. Yan. </author> <title> "A rational function arithmetic and simplification system in Common Lisp," </title> <journal> SIGSAM Bull. </journal> <volume> 25, no. </volume> <month> 4 (Oct., </month> <year> 1991), </year> <pages> 4-6. </pages>
Reference-contexts: Lisp representation for lists tends to interfere with parallel programming efforts| divide-and-conquer approaches are slowed if one must count half-way down a list for the "divide" step! The new system routines we have developed are vector, rather than list based, and should be much easier to use for parallel processing <ref> [7] </ref>. While we can encourage speculation on the use of parallel machines for hash-table based processing as in this paper, we are skeptical at the moment that current operating systems and processors have too high an overhead for process dispatch.
References-found: 7

