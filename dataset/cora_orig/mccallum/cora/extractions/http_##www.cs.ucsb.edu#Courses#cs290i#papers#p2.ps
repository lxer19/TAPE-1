URL: http://www.cs.ucsb.edu/Courses/cs290i/papers/p2.ps
Refering-URL: http://www.cs.ucsb.edu/Courses/cs290i/papers/index.html
Root-URL: http://www.cs.ucsb.edu
Title: Designing Programming Languages for Analyzability: A Fresh Look at Pointer Data Structures  
Author: Laurie J. Hendren and Guang R. Gao 
Address: Montreal, Canada  
Affiliation: School of Computer Science, McGill University  
Abstract: In the Proceedings of the 1992 International Conference on Computer Languages, pp. 242-251, Oakland, California, April 20-23, 1992. c fl1992 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE. Abstract In this paper we propose a programming language mechanism and associated compiler techniques which significantly enhance the analyzability of pointer-based data structures frequently used in non-scientific programs. Our approach is based on exploiting two important properties of pointer data structures: structural inductivity and speculative traversability. Structural inductivity facilitates the application of a static interference analysis method for such pointer data structures based on path matrices, and speculative traversability is utilized by a novel loop unrolling technique for while loops that exploit fine-grain parallelism by speculatively traversing such data structures. The effectiveness of this approach is demonstrated by applying it to a collection of loops found in typical nonscientific C programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Randy Allen, David Callahan, and Ken Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> Conference Record of the 14th POPL, </booktitle> <pages> pages 63-76, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: For example, the index expression is often an affine function of the loop indices. This mathematical structure of the arrays and the regularity of their accesses in embedded loops has lead to the development of a variety of dependence analysis, loop transformation, and parallelization techniques <ref> [1, 2, 3, 12, 16] </ref>. Although many of these techniques were pioneered in the areas of vectorizing and paral-lelizing compilers, these techniques are also being applied to architectures supporting instruction-level parallelism. Unfortunately, the analysis and optimization of non-scientific programs has not been so successful.
Reference: [2] <author> Todd Allen, Michael Burke, and Ron Cytron. </author> <title> A practical and powerful algorithm for subscript dependence testing. </title> <type> Technical report, </type> <institution> IBM, </institution> <year> 1986. </year> <type> Internal Report. </type>
Reference-contexts: For example, the index expression is often an affine function of the loop indices. This mathematical structure of the arrays and the regularity of their accesses in embedded loops has lead to the development of a variety of dependence analysis, loop transformation, and parallelization techniques <ref> [1, 2, 3, 12, 16] </ref>. Although many of these techniques were pioneered in the areas of vectorizing and paral-lelizing compilers, these techniques are also being applied to architectures supporting instruction-level parallelism. Unfortunately, the analysis and optimization of non-scientific programs has not been so successful.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: For example, the index expression is often an affine function of the loop indices. This mathematical structure of the arrays and the regularity of their accesses in embedded loops has lead to the development of a variety of dependence analysis, loop transformation, and parallelization techniques <ref> [1, 2, 3, 12, 16] </ref>. Although many of these techniques were pioneered in the areas of vectorizing and paral-lelizing compilers, these techniques are also being applied to architectures supporting instruction-level parallelism. Unfortunately, the analysis and optimization of non-scientific programs has not been so successful.
Reference: [4] <author> J.J. Dongarra and A.R. Hinds. </author> <title> Unrolling loops in FORTRAN. </title> <journal> Software-Practice and Experience, </journal> <volume> 9 </volume> <pages> 219-226, </pages> <year> 1979. </year>
Reference-contexts: RISC schedule ? A ? (3) (8) (6)(5) (2) r2 &lt;-- b (3) r2 &lt;-- r1 + r2 (4) 0 [a] &lt;-- r2 nop nop (e) Bad dependence analysis (f ) Bad RISC schedule transformation technique that was developed for par-allelizing and optimizing compilers for scientific pro grams and arrays <ref> [4] </ref>.
Reference: [5] <author> Vincent A. Guarna Jr. </author> <title> A technique for analyzing pointer and structure references in parallel restructuring compilers. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 212-220, </pages> <year> 1988. </year>
Reference-contexts: Inductive structures include linked lists, nested lists, and trees, while non-inductive structures include DAGs and cyclic graphs. Inductive structures have nice properties for analysis, and techniques for alias analysis, dependence analysis, and par-allelizing transformations for this class of structures have been developed <ref> [5, 8, 11] </ref>.
Reference: [6] <author> Laurie J. Hendren. </author> <title> Parallelizing Programs with Recursive Data Structures. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: item of the pair corresponds to "nilness" of the handle itself, and the second item corresponds to the "nilness" of the next field of the handle (* means definitely not nil, ffi means definitely nil, and fi means 3 This is an interprocedural analysis tool that is written in SML <ref> [6, 8] </ref>. /* build a list with n items */ orig_lp = build_list (n); /* orig_lp points to original head of list */ lp = orig_lp; prev = LIST_NULL; while (lp != LIST_NULL) - /* get next node */ next_lp = lp-&gt;next; /* reverse link of lp */ lp-&gt;next = prev;
Reference: [7] <author> Laurie J. Hendren and Guang R. Gao. </author> <title> Designing programming languages for analyzability: A fresh look at pointer data structures. </title> <type> ACAPS Technical Memo 28, </type> <institution> McGill University, </institution> <year> 1991. </year>
Reference-contexts: However, a 242 fresh look at the study by Wall actually shows im-pressive instruction-level parallelism assuming perfect alias analysis for all types of data structures <ref> [7, 14] </ref>. Given the increasing importance of fine-grain parallelism and the evidence that good alias analysis aids in the exploitation of such parallelism, it is clear that we must have programs that are analyzable.
Reference: [8] <author> Laurie J. Hendren and Alex Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1), </volume> <year> 1990. </year>
Reference-contexts: Inductive structures include linked lists, nested lists, and trees, while non-inductive structures include DAGs and cyclic graphs. Inductive structures have nice properties for analysis, and techniques for alias analysis, dependence analysis, and par-allelizing transformations for this class of structures have been developed <ref> [5, 8, 11] </ref>. <p> Thus, an analysis best suited to inductive structures can be used for each rectype which has been labeled as inductive. Just as dependence analysis is a natural choice for analyzing array references, path matrix analysis is a natural choice for inductive structures <ref> [8] </ref>. Path matrix analysis is an interprocedural analy sis technique that was specifically developed for in ductive data structures. <p> item of the pair corresponds to "nilness" of the handle itself, and the second item corresponds to the "nilness" of the next field of the handle (* means definitely not nil, ffi means definitely nil, and fi means 3 This is an interprocedural analysis tool that is written in SML <ref> [6, 8] </ref>. /* build a list with n items */ orig_lp = build_list (n); /* orig_lp points to original head of list */ lp = orig_lp; prev = LIST_NULL; while (lp != LIST_NULL) - /* get next node */ next_lp = lp-&gt;next; /* reverse link of lp */ lp-&gt;next = prev;
Reference: [9] <author> N. P. Jouppi and D. W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proceedings of the ASPLOS III, </booktitle> <pages> pages 272-282, </pages> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: In fact, some researchers have reported pessimistic results indicating that the exploitable instruction-level parallelism in non-scientific programs is so low as to question the merit of advanced advanced processor architectures such as superscalar/VLIW machines and superpipelined machines <ref> [9, 13, 14] </ref>. However, a 242 fresh look at the study by Wall actually shows im-pressive instruction-level parallelism assuming perfect alias analysis for all types of data structures [7, 14].
Reference: [10] <author> S. M. Krishnamurthy. </author> <title> A brief survey of papers on scheduling for pipelined processors. </title> <journal> SIGPLAN Notices, </journal> <volume> 25(7) </volume> <pages> 97-106, </pages> <year> 1990. </year>
Reference-contexts: Instruction scheduling is commonly performed for pipelined RISC architectures <ref> [10] </ref>. In order to fully utilize such instruction scheduling techniques, it is necessary to have accurate dependency analysis. Consider the small example program given in Figure 1 (a).
Reference: [11] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Restructuring Lisp programs for concurrent execution. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988, </booktitle> <pages> pages 100-110, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Inductive structures include linked lists, nested lists, and trees, while non-inductive structures include DAGs and cyclic graphs. Inductive structures have nice properties for analysis, and techniques for alias analysis, dependence analysis, and par-allelizing transformations for this class of structures have been developed <ref> [5, 8, 11] </ref>.
Reference: [12] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: For example, the index expression is often an affine function of the loop indices. This mathematical structure of the arrays and the regularity of their accesses in embedded loops has lead to the development of a variety of dependence analysis, loop transformation, and parallelization techniques <ref> [1, 2, 3, 12, 16] </ref>. Although many of these techniques were pioneered in the areas of vectorizing and paral-lelizing compilers, these techniques are also being applied to architectures supporting instruction-level parallelism. Unfortunately, the analysis and optimization of non-scientific programs has not been so successful.
Reference: [13] <author> M. D. Smith, M. Johnson, and M. A. Horowitz. </author> <title> Limits of instruction issue. </title> <booktitle> Proceedings of ASPLOS III, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: In fact, some researchers have reported pessimistic results indicating that the exploitable instruction-level parallelism in non-scientific programs is so low as to question the merit of advanced advanced processor architectures such as superscalar/VLIW machines and superpipelined machines <ref> [9, 13, 14] </ref>. However, a 242 fresh look at the study by Wall actually shows im-pressive instruction-level parallelism assuming perfect alias analysis for all types of data structures [7, 14].
Reference: [14] <author> D. W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <booktitle> In Proceedings of ASPLOS IV, </booktitle> <pages> pages 176-188, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In fact, some researchers have reported pessimistic results indicating that the exploitable instruction-level parallelism in non-scientific programs is so low as to question the merit of advanced advanced processor architectures such as superscalar/VLIW machines and superpipelined machines <ref> [9, 13, 14] </ref>. However, a 242 fresh look at the study by Wall actually shows im-pressive instruction-level parallelism assuming perfect alias analysis for all types of data structures [7, 14]. <p> However, a 242 fresh look at the study by Wall actually shows im-pressive instruction-level parallelism assuming perfect alias analysis for all types of data structures <ref> [7, 14] </ref>. Given the increasing importance of fine-grain parallelism and the evidence that good alias analysis aids in the exploitation of such parallelism, it is clear that we must have programs that are analyzable.
Reference: [15] <author> Shlomo Weiss and James E. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> In Proceedings of ASPLOS II, </booktitle> <pages> pages 105-109. </pages> <publisher> ACM, </publisher> <year> 1987. </year>
Reference-contexts: More recently it has also been applied in conjunction with instruction scheduling for pipelined and RISC architectures <ref> [15] </ref>. By increasing the size the body of the loop, the instruction scheduler can often produce a shorter schedule for the unrolled loop. Now let us consider the problem of performing a similar transformation on while loops with pointer data structures.
Reference: [16] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman, London and MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1989. </year> <month> 251 </month>
Reference-contexts: For example, the index expression is often an affine function of the loop indices. This mathematical structure of the arrays and the regularity of their accesses in embedded loops has lead to the development of a variety of dependence analysis, loop transformation, and parallelization techniques <ref> [1, 2, 3, 12, 16] </ref>. Although many of these techniques were pioneered in the areas of vectorizing and paral-lelizing compilers, these techniques are also being applied to architectures supporting instruction-level parallelism. Unfortunately, the analysis and optimization of non-scientific programs has not been so successful.
References-found: 16

