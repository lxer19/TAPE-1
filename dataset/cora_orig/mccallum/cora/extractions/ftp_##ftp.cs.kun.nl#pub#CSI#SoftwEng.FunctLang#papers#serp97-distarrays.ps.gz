URL: ftp://ftp.cs.kun.nl/pub/CSI/SoftwEng.FunctLang/papers/serp97-distarrays.ps.gz
Refering-URL: http://www.cs.kun.nl/~clean/Clean.Papers.html
Root-URL: 
Email: email: pascalrs@cs.kun.nl  
Title: Distributed Arrays in the Functional Language Concurrent Clean  
Author: Pascal R. Serrarens 
Address: Netherlands  
Affiliation: Computer Science Institute University of Nijmegen, The  
Abstract: In this paper, we show how distributed arrays can be implemented in Concurrent Clean without extensions to the language. We introduce the notion of remote values, associating a value at a remote processor with its location. By combining remote values with arrays we can build distributed arrays, having the same flexibility as standard arrays. With the example of matrix-vector multiplication, we show that our distributed arrays are well suited to implement parallel algorithms with little overhead.
Abstract-found: 1
Intro-found: 1
Reference: [BCH + 93] <author> G.E. Blelloch, S. Chatterjee, J.C. Hardiwck, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 102-101, </pages> <year> 1993. </year>
Reference-contexts: Network objects do not differ from normal objects for the programmer, while we need the special apply function rap for remote values. Various other functional languages provide some kind of distributed array, but they all provide it as a primitive language construction. The primitive parallel datatype of NESL <ref> [BCH + 93] </ref> are sequences. It uses a construction similar to ZF-expressions for the application of a function over a sequence. Skil [BK96] tries to have maximal flexibility with its pardata construct.
Reference: [BK96] <author> G.H. Botorog and H. Kuchen. Skil: </author> <title> An imperative language with algorithmic skeletons for efficient distributed programming. </title> <booktitle> In Proceedings of the Fifth International Symposium on High Performance Distributed Computing (HPDC-5), </booktitle> <pages> pages 243-252. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Various other functional languages provide some kind of distributed array, but they all provide it as a primitive language construction. The primitive parallel datatype of NESL [BCH + 93] are sequences. It uses a construction similar to ZF-expressions for the application of a function over a sequence. Skil <ref> [BK96] </ref> tries to have maximal flexibility with its pardata construct. It allows any distributed data structure, as long as it is composed of identical data structures place on each processor. Nested parallelism is not possible. SCL [DGTY95] give the ParArray for data-parallelism.
Reference: [BNOW94] <author> A. Birrell, G. Nelson, S. Owicki, and E. Wobber. </author> <title> Network objects. </title> <type> Technical Report Digital SRC 115, </type> <address> DEC, Palo Alto, USA, </address> <year> 1994. </year>
Reference-contexts: Our times are slightly better, because the explicit movement of data gives less parallel overhead. 6 Related Work The remote values in this paper are similar to network objects <ref> [BNOW94] </ref>. A network object consists of a local and a remote object. Invocation of a method of the local object will send a message to the remote object which then invokes its corresponding method.
Reference: [Col89] <author> M. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Skil [BK96] tries to have maximal flexibility with its pardata construct. It allows any distributed data structure, as long as it is composed of identical data structures place on each processor. Nested parallelism is not possible. SCL [DGTY95] give the ParArray for data-parallelism. It also provides configuration skeletons <ref> [Col89] </ref> to determine the distribution and alignment of the data. No direct control over locations of values is possible in NESL and Skil. Kuchen and Geiler [KG91] suggest distributed arrays in favor of list- and treelike structures. Every processor has an array with links to non-local distributed arrays elements.
Reference: [DGTY95] <author> J. Darlington, Y. Guo, H.W. To, and J. Yang. </author> <title> Functional skeletons for parallal coordination. </title> <booktitle> In Proceedings of Europar 95, </booktitle> <year> 1995. </year>
Reference-contexts: Skil [BK96] tries to have maximal flexibility with its pardata construct. It allows any distributed data structure, as long as it is composed of identical data structures place on each processor. Nested parallelism is not possible. SCL <ref> [DGTY95] </ref> give the ParArray for data-parallelism. It also provides configuration skeletons [Col89] to determine the distribution and alignment of the data. No direct control over locations of values is possible in NESL and Skil. Kuchen and Geiler [KG91] suggest distributed arrays in favor of list- and treelike structures.
Reference: [Kes94] <author> M.H.G Kesseler. </author> <title> Uniqueness and lazy graph copying copyright for the unique. </title> <booktitle> In Proceedings of the 6th International Workshop on the Implementation of Functional Languages, </booktitle> <address> Norwich, UK, </address> <year> 1994. </year> <institution> University of East Anglia. </institution>
Reference-contexts: Section 5 shows some results obtained with distributed arrays. Finally, Section 6 describes related work and Section 7 concludes the paper. 2 Remote Values In Concurrent Clean it is possible to introduce parallelism by annotating expressions with the P-annotation [PvE93] <ref> [Kes94] </ref>. In the expression fj P at p jg f x, the expression f x is evaluated on the processor identified with p, where p has type ProcId. If the expression is not completely evaluated, the result is only partially returned to the root processor.
Reference: [Kes95] <author> M.H.G Kesseler. </author> <title> Constructing skeletons in clean the bare bones. </title> <booktitle> In Proceedings of High Performance Functional Computing (HPFC '95), </booktitle> <pages> pages 182-192, </pages> <address> Denver, Colorado, </address> <year> 1995. </year> <institution> Lawrence Livermore National Laboratory. CONF-9504126. </institution>
Reference-contexts: With Concurrent Clean, these arrays can be built efficiently without new language constructions. We continue with the work initiated by Kesseler <ref> [Kes95] </ref> and present a toolkit for working with distributed arrays. In Section 2 we introduce remote values: values located on other processors. Section 3 then describes how distributed arrays can be built with remote values and gives some functions for them. <p> In his paper Kesseler <ref> [Kes95] </ref> used a new primitive function arg id to solve the first problem: it returns the ProcId of its argument. For the second problem he used an ordinary datatype for representing remote values. <p> When we have n elements in our array, a map will need n messages. In the future we plan a broadcast system to reduce this to a single message. 3.2 Rearranging Distributed Arrays and Communication Kesseler <ref> [Kes95] </ref> did not use an explicit copy function to move data between processors.
Reference: [KG91] <author> H. Kuchen and G. Geiler. </author> <title> Distributed applicative arrays. </title> <type> Technical Report AIB 91-5, </type> <institution> RWTH Aachen, </institution> <year> 1991. </year>
Reference-contexts: Nested parallelism is not possible. SCL [DGTY95] give the ParArray for data-parallelism. It also provides configuration skeletons [Col89] to determine the distribution and alignment of the data. No direct control over locations of values is possible in NESL and Skil. Kuchen and Geiler <ref> [KG91] </ref> suggest distributed arrays in favor of list- and treelike structures. Every processor has an array with links to non-local distributed arrays elements.
Reference: [KGGK94] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing, Design and Analysis of Algorithms. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> California, </address> <year> 1994. </year>
Reference-contexts: a foldlr Dist f dx = foldlr 1 dx.[0] where foldlr i r j i &gt;= size dx = r j otherwise = foldlr (i + 1) (rap2snd f dx.[i] r) 4 An Example: Matrix-Vector Multiplication In this section we will implement the matrix-vector multiplication algorithm by Kumar et al. <ref> [KGGK94] </ref>. The algorithm assumes a mesh-shaped processor network. The matrix is distributed blockwise over the network, while the vector is distributed blockwise over the rightmost column of the network.
Reference: [PvE93] <author> M.J. Plasmeijer and M.C.J.D. van Eekelen. </author> <title> Functional Programming and Parallel Graph Rewriting. </title> <publisher> Addison-Wesley Publishers Ltd., </publisher> <year> 1993. </year>
Reference-contexts: Still, parallel functional programming is not very popular. Perhaps some features for effective parallel programming are still missing. In this paper we present an important construction for parallel programming in the functional language Concurrent Clean <ref> [PvE93] </ref>, namely distributed arrays: arrays of which the elements are distributed over a number of processors. With Concurrent Clean, these arrays can be built efficiently without new language constructions. We continue with the work initiated by Kesseler [Kes95] and present a toolkit for working with distributed arrays. <p> Section 5 shows some results obtained with distributed arrays. Finally, Section 6 describes related work and Section 7 concludes the paper. 2 Remote Values In Concurrent Clean it is possible to introduce parallelism by annotating expressions with the P-annotation <ref> [PvE93] </ref> [Kes94]. In the expression fj P at p jg f x, the expression f x is evaluated on the processor identified with p, where p has type ProcId. If the expression is not completely evaluated, the result is only partially returned to the root processor.
Reference: [Ser96] <author> P.R. Serrarens. </author> <title> A clean conjugate gradient algorithm. </title> <booktitle> In Proceedings of the 8th International Workshop on Implementation of Functional Languages, </booktitle> <pages> pages 367-376, </pages> <address> Kiel, Germany, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: We compared the code against sequential code with no overheads for parallelism. Table 1 shows good speedups, because the local matrix-vector multiplications take most of the time. Another test case the conjugate gradient algorithm. The sequential version compared well to C and Haskell <ref> [Ser96] </ref>. The parallel implementation ran on a network of 4 Macintosh II computers connected by Localtalk and Ethernet. Using many map-like operations, we had lots of communication, so worse speedups could be expected, but table 2 shows that the results were encouraging. We Table 2.
Reference: [vG96] <author> J.H.G. van Groningen. </author> <title> The implementation and efficiency of arrays in Clean 1.1. </title> <booktitle> In Proceedings of the 8th International Workshop on Implementation of Functional Languages, </booktitle> <pages> pages 131-154, </pages> <address> Kiel, Germany, </address> <month> September </month> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Selecting an element from position i of array a is denoted as a.[i]. 1 In fact, Clean provides three kinds of arrays: lazy, strict and unboxed, with types f x g, f !x g and f #x g respectively <ref> [vG96] </ref> 3.1 Basic Operations on Distributed Arrays Distributed arrays are created by combining two arrays: an array with the elements and an array for the locations of those elements.
References-found: 12

