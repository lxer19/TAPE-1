URL: ftp://ftp.cs.washington.edu/pub/orca/papers/hpf.ps
Refering-URL: http://www.cs.washington.edu/research/zpl/papers/abstracts/hpf.html
Root-URL: 
Title: vs HPF: A Comparison of Performance and Programming Style  
Author: Calvin Lin Lawrence Snyder Ruth E. Anderson Bradford L. Chamberlain Sung-Eun Choi George Forman E Christopher Lewis W. Derrick Weathersby 
Date: August 1, 1995  
Address: Box 352350  Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering  University of Washington  
Note: ZPL  
Abstract: University of Washington Technical Report #95-11-05, 1995. Abstract This paper compares two data parallel languages, ZPL and HPF, in terms of programming style and performance. The results show that for eight programs from a number of standard benchmark suites, ZPL generally outperforms HPF, and ZPL expresses problems at higher levels of abstraction, yielding programs that are shorter, less error prone and easier to maintain. ZPL's better performance comes from its clean expression of parallelism that allows for better compiler analysis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.A. Abramson and R. Sosic. </author> <title> A debugging tool for software evolution. In (to appear) CASE-95, </title> <booktitle> 7 th International Workshop on Computer-Aided Software Engineering, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: After this effort, the HPF approach is finished, if the performance is satisfactory, while the ZPL program must be validated to show it equivalent to the original Fortran. Tools can assist in this validation <ref> [1] </ref>, and the additional effort is generally repaid in better performance and a cleaner, more easily maintained program. 2 A Brief Introduction to HPF HPF was designed by the High Performance Fortran Forum in 1993 [4]. <p> ZPL also supports dynamic regions 3 1 program Jacobi; 2 config var n: integer = 512; -- Configuration defaults 3 epsilon: real = 0.000001; 4 5 region R = [1..n, 1..n]; -- Declarations 6 7 direction north = [-1, 0]; 8 east = <ref> [ 0, 1] </ref>; 9 west = [ 0,-1]; 10 south = [ 1, 0]; 11 12 procedure Jacobi (); 13 var A, Temp: [R] real; 14 err: real; 15 begin 16 [R] A := 0.0; -- Initialization 17 [north of R] A := 0.0; 18 [east of R] A := 0.0; <p> regions 3 1 program Jacobi; 2 config var n: integer = 512; -- Configuration defaults 3 epsilon: real = 0.000001; 4 5 region R = [1..n, 1..n]; -- Declarations 6 7 direction north = [-1, 0]; 8 east = [ 0, 1]; 9 west = [ 0,-1]; 10 south = <ref> [ 1, 0] </ref>; 11 12 procedure Jacobi (); 13 var A, Temp: [R] real; 14 err: real; 15 begin 16 [R] A := 0.0; -- Initialization 17 [north of R] A := 0.0; 18 [east of R] A := 0.0; 19 [west of R] A := 0.0; 20 [south of R] <p> I = [1..m, 1..n] and direction east = [0,1], the statement [east of I] wrap U; -- copy first column into last column assigns to the region [east of I] (that is, [1..m, n+1]) the data from the same-sized region on the opposite end of the array, namely, the region <ref> [1..m, 1] </ref>. The programmer's intent is evident from the text, reducing the chance for error. Thus, ZPL raises the level of abstraction by providing a direct solution for periodic (and mirrored, using reflect) boundary computations.
Reference: [2] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS parallel benchmarks (94). </title> <type> RNR Technical Report RNR-94-007, </type> <month> March </month> <year> 1994. </year>
Reference-contexts: How difficult is each error to find? Embar. Embar is a NAS parallel benchmark kernel that "generates pairs of Gaussian random deviates (according to a specific scheme) and tabulates the number of pairs in successive square annuli" <ref> [2] </ref>. Embar is considered "embarrassingly parallel" because it requires very little communication. This benchmark is interesting as an upper bound on FLOPS rates for parallel computers. Of interest here is the way in which ZPL and HPF specify the computation's "unlimited" concurrency. <p> ZPL is unambiguously superior on both machines for half of the benchmarks: Embar, Swm256, PDE1 and X42 (Figure 5 (a)-(d)). Embar is, perhaps, most significant since it was developed to bound the performance of parallel computations <ref> [2] </ref>. For Shallow (Figure 5 (e))|a larger single precision version of Swm256 in which only the inner loops are timed|both ZPL and HPF match until P=32, where there is an advantage for HPF on the T3D and where the HPF Paragon entry is unexpectedly superlinear, suggesting a possible transcriptional error.
Reference: [3] <author> Richard Friedman, John Levesque, and Gene Wagenbreth. </author> <title> Fortran Parallelization Handbook, </title> <note> Preliminary Edition. Applied Parallel Research, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: Moreover, after adding HPF directives, considerable effort can be required to restructure a Fortran program to get good parallel performance <ref> [3] </ref>. After this effort, the HPF approach is finished, if the performance is satisfactory, while the ZPL program must be validated to show it equivalent to the original Fortran. <p> This is not the case in HPF, where obscure details of the source program can disable parallelization by the compiler <ref> [3] </ref>. It is for this reason that Fortran compilers must often be accompanied by parallelization tools designed to help the programmer restructure their code for parallelism. Such parallelization tools are not needed in ZPL because the parallelism is clear from the source program. The Context.
Reference: [4] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification. </title> <month> November </month> <year> 1994. </year>
Reference-contexts: HPF was recently defined by the High Performance Fortran Forum to provide "data parallel programming" and "top performance on MIMD and SIMD computers" <ref> [4, p.1] </ref>. With such similar ambitions, it is natural to compare the two languages. This paper compares ZPL and HPF, concentrating on performance and programming style. <p> This change has been made to avoid confusion with a similarly named language for distributed computing. HPF Design Goal: "to develop extensions to Fortran that provide support for high performance programming on a wide variety of machines : : : " <ref> [4, p.vii] </ref>. The HPF Forum included groups with large installed Fortran bases, vendors intending to support Fortran on their parallel hardware, and researchers developing compilation technology for Fortran sources. <p> Tools can assist in this validation [1], and the additional effort is generally repaid in better performance and a cleaner, more easily maintained program. 2 A Brief Introduction to HPF HPF was designed by the High Performance Fortran Forum in 1993 <ref> [4] </ref>. HPF extends sequential Fortran by adding data decomposition directives that do not change the semantics of the program, but serve as hints to the compiler to improve performance. Thus HPF provides data parallelism by adding data decomposition information to sequential programs.
Reference: [5] <author> Calvin Lin. </author> <title> ZPL language reference manual. </title> <type> Technical Report 94-10-06, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Using the direction vector north, defined to be [-1, 0] in line 7, [north of R] is a region whose indices are disjoint from R and offset from it by north, i.e. [north of R] = [0, 1..n]. (More precise definitions can be found in the literature <ref> [5] </ref>.) Line 17 not only initializes the northern border of A, but also implicitly allocates storage for this border region. The program body computes the Jacobi iteration using a repeat statement (lines 22-26). <p> Many interesting and powerful features of the language are omitted for the sake of brevity. For more details, see the ZPL Programmer's Guide [11] and the ZPL Reference Manual <ref> [5] </ref>. Parallel Execution. Arrays declared over regions are called parallel arrays because they are the mechanism for achieving parallelism. In the ZPL programming model, parallelism is not expressed via loops [10]. Instead, ZPL derives concurrency from parallel array and parallel prefix operations.
Reference: [6] <author> Calvin Lin and Lawrence Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year>
Reference-contexts: Indeed, ZPL is the data parallel subset of a more general parallel language, Advanced ZPL, 1 which is based on an efficient parallel machine model and programming model [10] with demonstrated performance and portability <ref> [6] </ref>. fl This research was supported in part by ARPA Grant N00014-92-J-1824 and NSF Grant CDA-9211095 1 Advanced ZPL is the new name for Orca C. This change has been made to avoid confusion with a similarly named language for distributed computing.
Reference: [7] <author> Calvin Lin and Lawrence Snyder. ZPL: </author> <title> An array sublanguage. </title> <editor> In Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 96-114. </pages> <address> SpringerVerlag, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction ZPL is a new data parallel array language designed at the University of Washington to provide high per <p>- formance parallel computation for all MIMD parallel computers <ref> [7] </ref>. HPF was recently defined by the High Performance Fortran Forum to provide "data parallel programming" and "top performance on MIMD and SIMD computers" [4, p.1]. With such similar ambitions, it is natural to compare the two languages. This paper compares ZPL and HPF, concentrating on performance and programming style. <p> The evidence presented will favor ZPL over HPF, for the programs will likely run faster and be easier to understand and maintain. As a crude measure of readability, the SIMPLE benchmark is 2400 lines of Fortran 77 without HPF directives but only 500 lines in ZPL <ref> [7] </ref>. The discussion in Section 4 illustrates this point more clearly. In cases where a Fortran program exists, our experience suggests that the effort to understand it well enough to select the proper HPF annotations can be comparable to the effort required to rewrite it in ZPL.
Reference: [8] <author> Calvin Lin and Lawrence Snyder. </author> <title> SIMPLE performance results in ZPL. </title> <editor> In Keshav Pingali, Uptal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 361-375. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Performance that matches hand-coded, explicitly parallel C code has been demonstrated on both shared and distributed memory parallel computers <ref> [8] </ref>. Program Walk-through. Perhaps the quickest introduction is to walk-through a ZPL program. Figure 2 shows the ZPL code for the 4-point Jacobi computation [11]. The program begins by defining two problem- specific configuration variables, n and epsilon, and their default values (line 2-3).
Reference: [9] <institution> Applied Parallel Research. </institution> <type> XHPF benchmark results. Technical report, </type> <month> March </month> <year> 1995. </year>
Reference-contexts: This section compares ZPL and HPF code fragments of three programs from the Applied Parallel Research (APR) xHPF Benchmark Suite <ref> [9] </ref>: * Relaxation loop from PDE1|illustrates very similar approaches, but ZPL expresses it more mnemon <br>- ically. * Invocation of vrand () from Embar|ZPL uses direct data parallelism, while in HPF, a serial loop is parallelized. * Boundary updates from Shallow|another case where ZPL raises the conceptual level above Fortran 90's <p> Shallow. The Shallow benchmark, a finite-difference calculation to predict weather using the shallow- water equations, illustrates how ZPL and HPF handle periodic boundary conditions. Here, the ZPL code is compared to a Fortran 90 style HPF program from APR <ref> [9] </ref>. <p> to ZPL as directly as possible, in each case timing the same portions of code, consuming the same inputs, and producing the same outputs as the original HPF programs. 3 The HPF performance numbers are those published in March, 1995 by Applied Parallel Research (APR) for their commercial HPF compiler <ref> [9] </ref>. The ZPL results were gathered on the San Diego Supercomputing Center's (SDSC) Paragon and the Arctic Region Supercomputing Center's (ARSC) Cray T3D, while the HPF results used the Paragon at NASA's Ames Research Center (Ames) and the Cray T3D at the Pittsburgh Supercomputing Center (PSC).
Reference: [10] <author> Lawrence Snyder. </author> <booktitle> Foundations of practical parallel programming languages. In Proceedings of the Second International Conference of the Austrian Center for Parallel Computation. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Indeed, ZPL is the data parallel subset of a more general parallel language, Advanced ZPL, 1 which is based on an efficient parallel machine model and programming model <ref> [10] </ref> with demonstrated performance and portability [6]. fl This research was supported in part by ARPA Grant N00014-92-J-1824 and NSF Grant CDA-9211095 1 Advanced ZPL is the new name for Orca C. This change has been made to avoid confusion with a similarly named language for distributed computing. <p> For more details, see the ZPL Programmer's Guide [11] and the ZPL Reference Manual [5]. Parallel Execution. Arrays declared over regions are called parallel arrays because they are the mechanism for achieving parallelism. In the ZPL programming model, parallelism is not expressed via loops <ref> [10] </ref>. Instead, ZPL derives concurrency from parallel array and parallel prefix operations. By default, parallel arrays are distributed in a block allocation for 1D and 2D arrays; higher dimensional arrays are distributed across two dimensions. <p> Advanced ZPL provides full control over memory allocation, so this default can be overridden. (b) Advanced ZPL has been created from first principles, and has a well specified machine model (the CTA) and a well specified programming model (Phase Abstractions) <ref> [10] </ref>. These foundations have simplified the development of ZPL and the construction of its machine independent compiler. Clearly, advantage (a) awaits completion of the Advanced ZPL compiler.

References-found: 10

