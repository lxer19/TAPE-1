URL: http://ini.cs.tu-berlin.de/~ao/pubs/metric.ps.gz
Refering-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Root-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Email: fao, asyncg@cs.tu-berlin.de  
Phone: 5-9,  
Title: An Analysis of the Metric Structure of the Weight Space of Feedforward Networks and its
Author: Arnfried Ossen and Stefan M. Ruger 
Address: Sekr. FR  Franklinstr. 28/29, 10 587 Berlin, Germany  
Affiliation: Informatik,  Technische Universitat Berlin  
Abstract: We study symmetries of feedforward networks in terms of their corresponding groups. We find that these groups naturally act on and partition weight space into disjunct domains. We derive an algorithm to generate representative weight vectors in a fundamental domain. The analysis of the metric structure of the fundamental domain leads to improved evaluation procedures of learning results, such as local error bars estimated using maximum-likelihood and bootstrap methods. It can be implemented efficiently even for large networks. We demonstrate the approach in the area of nonlinear time series modeling and prediction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> An Mei Chen, Haw-minn Lu, and Robert Hecht-Nielsen. </author> <title> On the geometry of feedforward neural network error surfaces. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 910-927, </pages> <year> 1993. </year>
Reference-contexts: Figure 1 displays an example for this type of network. 3.2 Symmetries There have been several attempts to analyze the symmetries of the network function <ref> [3, 7, 1] </ref>. We present an independent approach, in which the arising symmetries are described and analyzed in terms of their corresponding groups, see e. g. [4]. Let (M ) denote the permutation group of a set M . <p> So far the symmetry group S of the weight space has been identified as a certain subgroup of GL (E; R). As pointed out by <ref> [1] </ref> no analytic function other than an element of S can represent a symmetry in this context. However, there exist a lot of discontinous functions that give rise to a symmetry: Fix two hidden nodes a; b from the same hidden layer.
Reference: [2] <author> Bradley Efron and Robert J. Tibshirani. </author> <title> An Introduction to the Bootstrap, </title> <booktitle> volume 57 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction Feedforward networks can be interpreted as a form of nonlinear regression. They offer great flexibility at the price of a complicated structure. It is possible to use classical maximum-likelihood procedures or modern computational approaches, e. g. bootstrap <ref> [2] </ref> to evaluate learning results. However, the usual gradient-based parameter estimation, or, in the language of neural networks, learning procedures, may get stuck in local extrema. In the case of maximum-likelihood estimation of error bars, estimates at local maxima of the likelihood can be completely wrong. <p> The maximum-likelihood estimation ^w n may be obtained, e. g., by gradient ascent in the likelihood, or in the logarithm of the likelihood. 2.2 Bootstrap Approach The bootstrap method <ref> [2] </ref> is based on re-estimations of the parameter vector on B bootstrap samples of the training set. <p> Let ^w fl (ff) B be the empirical ff quantile of the cumulative distribution function of ^w fl . The approximate 1 2ff confidence interval is [ ^w B ; ^w B ] However, to achieve good accuracy, many more bootstrap samples and elaborate post-processing may be required. See <ref> [2] </ref> for details. 3 Symmetry Group of R E Every transformation t: R E ! R E of the weight space, which leaves the network function invariant, i. e., out w = out t (w) , indicates a symmetry of the weight space. <p> The method of least squares can still be applied to es-timate network weights. However, in contrast to ordinary nonlinear regression, it is not directly equivalent to the maximum-likelihood method. Therefore, the estimation was done according to the moving blocks bootstrap method <ref> [2] </ref>. The empirical distribution of network function outputs was used to estimate a number of quantiles around the median of the distribution. These in turn were evaluated to estimate an empirical predictive density.
Reference: [3] <author> Robert Hecht-Nielsen. </author> <title> On the algebraic structure of feedforward network weight spaces. </title> <editor> In R. Eck-miller, editor, </editor> <booktitle> Advanced Neural Computers, Ams-terdam, 1990. </booktitle> <publisher> Elsevier. </publisher>
Reference-contexts: Figure 1 displays an example for this type of network. 3.2 Symmetries There have been several attempts to analyze the symmetries of the network function <ref> [3, 7, 1] </ref>. We present an independent approach, in which the arising symmetries are described and analyzed in terms of their corresponding groups, see e. g. [4]. Let (M ) denote the permutation group of a set M .
Reference: [4] <author> Ian D. Macdonald. </author> <title> The theory of groups. </title> <publisher> Oxford University Press, </publisher> <year> 1975. </year>
Reference-contexts: We present an independent approach, in which the arising symmetries are described and analyzed in terms of their corresponding groups, see e. g. <ref> [4] </ref>. Let (M ) denote the permutation group of a set M . Bear in mind that every permutation can be written in terms of transpositions.
Reference: [5] <author> Arnfried Ossen and Martin Schnauss. </author> <title> Practical tools for derivative instruments based on nonlinear time series prediction. </title> <booktitle> Neural Network World, </booktitle> <volume> 5(4) </volume> <pages> 525-536, </pages> <year> 1995. </year> <booktitle> Proceedings | International Workshop on Parallel Applications in Statistics and Economics. </booktitle>
Reference-contexts: The smaller the error bars are, the bigger the profit will be on average. The actual time series we selected is a short term in terest rate. Its prediction is used to support traders in the forward rate agreement market <ref> [5] </ref>. 5.2 Modeling, Refinement and Results The time series was modeled indirectly as the difference between certain market expectations in the past and actual rates at present; see [5] for details. We used two-layer feedforward networks in a nonlinear auto-regressive setting. <p> Its prediction is used to support traders in the forward rate agreement market <ref> [5] </ref>. 5.2 Modeling, Refinement and Results The time series was modeled indirectly as the difference between certain market expectations in the past and actual rates at present; see [5] for details. We used two-layer feedforward networks in a nonlinear auto-regressive setting. Network inputs were past values of the series at specific time lags. Network outputs directly predicted values in the future.
Reference: [6] <author> Stefan M. Ruger and Arnfried Ossen. </author> <title> Performance evaluation of feedforward networks using computational methods. </title> <booktitle> In Proceedings of NEURAP'95. </booktitle> <address> NEURAP, </address> <year> 1996. </year>
Reference-contexts: In the case of maximum-likelihood estimation of error bars, estimates at local maxima of the likelihood can be completely wrong. For bootstrap, local extrema lead to unnecessary large error bars <ref> [6] </ref>. In order to exclude suboptimal maximum-likelihood and bootstrap estimations we propose to use the location information of the weight vectors instead. However, owing to a canonical symmetry group (Section 3), the space of weight vectors has a nontrivial metric structure that is studied in Section 4.
Reference: [7] <author> H. J. Sussmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 589-593, </pages> <year> 1992. </year>
Reference-contexts: Figure 1 displays an example for this type of network. 3.2 Symmetries There have been several attempts to analyze the symmetries of the network function <ref> [3, 7, 1] </ref>. We present an independent approach, in which the arising symmetries are described and analyzed in terms of their corresponding groups, see e. g. [4]. Let (M ) denote the permutation group of a set M .
Reference: [8] <author> Halbert White. </author> <title> Artificial Neural Networks | Approximation & Learning Theory. </title> <publisher> Blackwell, Oxford, </publisher> <address> Cambridge, </address> <year> 1992. </year> <title> blocks bootstrap results in multi-modal and wide distribution for 21 May, 1993. procedure (see text). </title>
Reference-contexts: L D (w) describes how likely the data have been generated by w. Under quite general assumptions, the weight vector ^w n = argsup w that maximizes the likelihood is asymptotically (w. r. t. n) unbiased, consistent, asymptotically efficient and asymptotically normal-distributed <ref> [8] </ref>. The maximum-likelihood estimation ^w n may be obtained, e. g., by gradient ascent in the likelihood, or in the logarithm of the likelihood. 2.2 Bootstrap Approach The bootstrap method [2] is based on re-estimations of the parameter vector on B bootstrap samples of the training set. <p> This class of networks is quite universal: choosing the activation functions of the output layer as identity and using one hidden layer makes this type of network an universal approx-imator <ref> [8] </ref>. Figure 1 displays an example for this type of network. 3.2 Symmetries There have been several attempts to analyze the symmetries of the network function [3, 7, 1].
References-found: 8

