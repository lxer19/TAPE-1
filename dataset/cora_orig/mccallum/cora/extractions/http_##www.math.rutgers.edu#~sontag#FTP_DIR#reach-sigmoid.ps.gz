URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/reach-sigmoid.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: sontag@control.rutgers.edu  sussmann@hamilton.rutgers.edu  
Title: Complete Controllability of Continuous-Time Recurrent Neural Networks  
Author: Eduardo Sontag Hector Sussmann 
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Dept. of Mathematics, Rutgers University  Dept. of Mathematics, Rutgers University  
Abstract: This paper presents a characterization of controllability for the class of control systems commonly called (continuous-time) recurrent neural networks. The characterization involves a simple condition on the input matrix, and is proved when the activation function is the hyperbolic tangent.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albertini, F., and Dai Pra, P., </author> <title> "Forward accessibility for recurrent neural networks," </title> <journal> IEEE Trans. Automat. Control 40 (1995): </journal> <pages> 1962-1968 </pages>
Reference-contexts: Here we focus on problems of controllability. The fundamental contribution in this area was a recent paper by Albertini and Dai Pra, cf. <ref> [1] </ref>. This paper dealt with the study of the forward accessibility property. <p> Proposition 1.5 Let = arctan. Then the 4-dimensional, single-input system _x 1 = (x 1 + x 2 + x 3 + x 4 + 2u) _x 3 = (3u) is not controllable. Observe that this system is forward accessible, because it satisfies the conditions in <ref> [1] </ref>. Indeed, the nonlinearity = arctan satisfies the "independence property" (cf. [2]), and clearly the matrix B = col (2; 12; 3; 4) belongs to B n;1 . 2 Proof of the Main Results We first prove Lemma 1.2. Proof.
Reference: [2] <author> Albertini, F., E.D. Sontag, and V. Maillot, </author> <title> "Uniqueness of weights for neural networks," in Artificial Neural Networks for Speech and Vision (R. Mammone, </title> <editor> ed.), </editor> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1993, </year> <pages> pp. 115-125. </pages>
Reference-contexts: Albertini and Dai Pra showed that forward accessibility holds provided that * the "independence property" (cf. [16], <ref> [2] </ref>) holds for , and * B is in a certain class B n;m of matrices which was introduced in [2] and [4] and is reviewed below. (For the special but most important case of single input systems, m = 1, the condition B 2 B n;m means that the entries <p> Albertini and Dai Pra showed that forward accessibility holds provided that * the "independence property" (cf. [16], <ref> [2] </ref>) holds for , and * B is in a certain class B n;m of matrices which was introduced in [2] and [4] and is reviewed below. (For the special but most important case of single input systems, m = 1, the condition B 2 B n;m means that the entries of the vector B are all nonzero and have different absolute values. <p> Observe that this system is forward accessible, because it satisfies the conditions in [1]. Indeed, the nonlinearity = arctan satisfies the "independence property" (cf. <ref> [2] </ref>), and clearly the matrix B = col (2; 12; 3; 4) belongs to B n;1 . 2 Proof of the Main Results We first prove Lemma 1.2. Proof. The first three properties are clear, with 1 = 1, so we need to prove the limit property (3).
Reference: [3] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "For neural networks, function determines form," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 975-990. </pages>
Reference: [4] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "State observability in recurrent neural networks," </title> <journal> Systems & Control Letters 22(1994): </journal> <pages> 235-244. </pages>
Reference-contexts: Albertini and Dai Pra showed that forward accessibility holds provided that * the "independence property" (cf. [16], [2]) holds for , and * B is in a certain class B n;m of matrices which was introduced in [2] and <ref> [4] </ref> and is reviewed below. (For the special but most important case of single input systems, m = 1, the condition B 2 B n;m means that the entries of the vector B are all nonzero and have different absolute values.
Reference: [5] <author> Back, A.D., and A.C. Tsoi, </author> <title> "FIR and IIR synapses, a new neural network architecture for time-series modeling," </title> <booktitle> Neural Computation 3 (1991): </booktitle> <pages> 375-385. </pages>
Reference-contexts: The coefficients A ij ; B ij denote the weights, intensities, or "synaptic strengths," of the various connections. The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], <ref> [5] </ref>, [10]), control (see e.g. [14], [18], [11], [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [6] <author> Bengio, Y., </author> <title> Neural Networks for Speech and Sequence Recognition, </title> <publisher> Thompson Computer Press, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: The coefficients A ij ; B ij denote the weights, intensities, or "synaptic strengths," of the various connections. The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance <ref> [6] </ref>, [5], [10]), control (see e.g. [14], [18], [11], [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [7] <author> Dasgupta, B., and E.D. Sontag, </author> <title> "Sample complexity for learning recurrent perceptron mappings," </title> <journal> IEEE Trans. Inform. Theory 42 (1996): </journal> <pages> 1479-1487. </pages>
Reference: [8] <author> Koiran, P., and E.D. Sontag, </author> <title> "Vapnik-Chervonenkis dimension of recurrent neural networks," </title> <note> submitted. </note>
Reference-contexts: In past work we have studied, for such models, questions of parameter identifiability ([3]), observability ([4]), system approximation ([13]), computability ([12]), parameter reconstruction ([9]), and sample complexity for learning and generalization ([7], <ref> [8] </ref>). Here we focus on problems of controllability. The fundamental contribution in this area was a recent paper by Albertini and Dai Pra, cf. [1]. This paper dealt with the study of the forward accessibility property.
Reference: [9] <author> Koplon, R., and E.D. Sontag, </author> <title> "Using Fourier-neural recurrent networks to fit sequential input/output data," </title> <journal> Neurocomputing, </journal> <note> to appear. </note>
Reference: [10] <author> Matthews, M., </author> <title> "A state-space approach to adaptive nonlinear filtering using recurrent neural networks," </title> <booktitle> Proc. 1990 IASTED Symp. on Artificial Intelligence Applications and Neural Networks, </booktitle> <address> Zurich, </address> <pages> pp. 197-200, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The coefficients A ij ; B ij denote the weights, intensities, or "synaptic strengths," of the various connections. The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], [5], <ref> [10] </ref>), control (see e.g. [14], [18], [11], [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [11] <author> Polycarpou, </author> <title> M.M., and P.A. Ioannou, "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], [5], [10]), control (see e.g. [14], [18], <ref> [11] </ref>, [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [12] <editor> Siegelmann, H.T., and E.D. Sontag, </editor> <booktitle> "On the computational power of neural nets," </booktitle> <institution> J. Comp. Syst. Sci. </institution> <month> 50( </month> <year> 1995): </year> <pages> 132-150. </pages>
Reference: [13] <author> Sontag, E.D., </author> <title> "Neural nets as systems models and controllers," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [14] <author> Sontag, E.D., </author> <title> "Neural networks for control," in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. </pages>
Reference-contexts: The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], [5], [10]), control (see e.g. <ref> [14] </ref>, [18], [11], [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [15] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year> <month> 10 </month>
Reference-contexts: The system (1) is controllable if every ~ 2 R n can be steered to every 2 R n . (See <ref> [15] </ref> for generalities and basic facts about control systems.) For each pair of positive integers n and m, we let B n;m := B 2 R nfim ; (8 i) row i (B) 6= 0 and (8 i 6= j) row i (B) 6= row j (B) where row i () <p> Similarly, there is a control u 3 which steers some 2 2 U to ~ 2 . (Proof: we must show that ~ 2 can be controlled to some 2 2 U with respect now to the time-reversed system _x = (y); _y = (u), cf. Lemma 2.6.8 in <ref> [15] </ref>. With the new variable z := y, the equations become _x = (z); _z = (u), which coincide with those of the original system.) Finally, there is a control u 2 taking 1 to 2 .
Reference: [16] <author> Sussmann, H.J., </author> <title> "Uniqueness of the weights for minimal feedforward nets with a given input-output map," </title> <booktitle> Neural Networks 5 (1992): </booktitle> <pages> 589-593. </pages>
Reference-contexts: Albertini and Dai Pra showed that forward accessibility holds provided that * the "independence property" (cf. <ref> [16] </ref>, [2]) holds for , and * B is in a certain class B n;m of matrices which was introduced in [2] and [4] and is reviewed below. (For the special but most important case of single input systems, m = 1, the condition B 2 B n;m means that the
Reference: [17] <author> Zbikowski, R., </author> <title> "Lie algebra of recurrent neural networks and identifiability," </title> <booktitle> Proc. Amer. Automatic Control Conference, </booktitle> <address> San Francisco, </address> <year> 1993, </year> <pages> pp. 2900-2901. </pages>
Reference-contexts: The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], [5], [10]), control (see e.g. [14], [18], [11], <ref> [17] </ref>), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
Reference: [18] <editor> Zbikowski, R., and K.J. Hunt, eds., </editor> <booktitle> Neural Adaptive Control Technology World Scientific Publishing, </booktitle> <year> 1996. </year> <month> 11 </month>
Reference-contexts: The transformation : R ! R is called the "activation function". Systems of this type have been employed in areas as varied as digital signal processing (see for instance [6], [5], [10]), control (see e.g. [14], <ref> [18] </ref>, [11], [17]), the design of associative memories ("Hopfield nets"), language inference, and sequence extrapolation for time series prediction.
References-found: 18

