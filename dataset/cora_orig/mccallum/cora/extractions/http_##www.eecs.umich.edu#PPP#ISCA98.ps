URL: http://www.eecs.umich.edu/PPP/ISCA98.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: fgabandah,davidsong@eecs.umich.edu  
Title: Effects of Architectural and Technological Advances on the HP/Convex Exemplar's Memory and Communication Performance  
Author: Gheith A. Abandah Edward S. Davidson 
Keyword: Index Terms: Performance evaluation, memory performance, communication performance, microbenchmarking, distributed shared memory, HP/Convex Exemplar.  
Address: Ann Arbor  
Affiliation: Advanced Computer Architecture Laboratory, University of Michigan,  
Note: In the 25th Ann. Int'l Symp. on Computer Architecture, (ISCA'98), June 1998, pp 318-329. 1  
Abstract: Advances in microarchitecture, packaging, and manufacturing processes enable designers to build new systems with higher performance and scalability. Using mi-crobenchmark techniques, we contrast the memory and communication performance of two generations of the HP/Convex Exemplar scalable parallel processing system. The SPP1000 and SPP2000 have significant architectural and implementation differences, but maintain upward binary compatibility. The SPP2000 employs manufacturing and packaging advances to obtain shorter system interconnects with wider data paths and improved functionality, thereby reducing the latency and increasing the bandwidth of remote communication. Although the memory latency is not significantly improved, newer out-of-order execution processors coupled with nonblocking caches achieve much higher memory bandwidth. The SPP2000 has a richer system interconnect topology that allows scalability to a larger number of processors. The SPP2000 also employs innovations in its coherence protocols to improve synchronization and communication performance. This paper characterizes the performance effects of these changes, and identifies some remaining inefficiencies, in the cache coherence protocol and the node configuration, that future systems should address. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Abandah and E. Davidson. </author> <title> A Comparative Study of Cache-Coherent Nonuniform Memory Access Systems. In High Performance Computing Systems and Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1998. </year> <booktitle> 12th Ann. Int'l Symp. High Performance Computing Systems and Applications (HPCS'98). </booktitle>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [15], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [1, 9, 10] </ref>.
Reference: [2] <author> G. Abandah and E. Davidson. </author> <title> Characterizing Distributed Shared Memory Performance: A Case Study of the Convex SPP1000. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 9(2):206216, </volume> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: This paper evaluates the effects of these advances on the memory and communication performance. We use a suite of microbenchmarks that is designed to characterize the performance of DSM systems <ref> [2] </ref>. Similar to other multiprocessor microbenchmarks [11, 19, 12], these microbench-marks measure the latency and bandwidth of memory accessing. <p> Each kernel accesses the l-byte elements of an array of size w bytes with stride s bytes. Thus, one call time corresponds to the w=s accesses in a kernel call. The memory kernels need to be carefully designed to accurately measure the latency and bandwidth of modern processors <ref> [18, 2, 12] </ref>. A kernel that measures latency must expose the access latency and cannot overlap or hide the latencies of individual memory accesses; a kernel that measures bandwidth must maximize access overlap to achieve the highest possible bandwidth. <p> The array size is varied to characterize cases where the array fits in the processor data cache through cases where all accesses are satisfied from the local memory. The 2 For a more detailed description of these microbenchmarks, see <ref> [2] </ref>. access stride is varied to expose processor and system bottlenecks. Remote memory: the latency of accessing a shared array that is allocated in a remote node. The array size is varied to characterize the performance of the interconnect cache, which is described in Section 5.1. <p> The hit region ends at w = 1024 KB and the transition region ends at w = 2048 KB, indicating that the data cache is a direct-mapped 1024-KB cache <ref> [2] </ref>. In the hit region, every access hits in the cache, while in the miss region (w 2048 KB), every access to a new cache line is a miss. 3 transition region (1024 &lt; w &lt; 2048 KB), and miss region (w 2048 KB). <p> Therefore, the SPP2000 ring controller can process a line request concurrently with collapsing the sharing list of a replaced line. Notice that the SPP2000's curve shape differs slightly from the curve of the 512-MB direct-mapped cache model shown <ref> [2] </ref>. In fact, it changes from one experiment to another according to the OS mapping of the array's virtual pages to the physical memory pages.
Reference: [3] <author> T. Asprey, G. Averill, E. DeLano, R. Mason, B. Weiner, and J. Yetter. </author> <title> Performance Features of the PA7100 Microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2234, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Yet, due to improved packaging and device density, a 2-node SPP2000 tower is physically the same size as a 2-node SPP1000 tower. The SPP1000 uses the Hewlett-Packard PA 7100 <ref> [3] </ref>, a two-way superscalar processor, running at 100 MHz. The SPP2000 uses the PA 8000 [13], a four-way superscalar processor, running at 180 MHz. In addition to the PA 8000's higher frequency and larger number of functional units, it supports out-of-order instruction execution, and memory latency overlapping and hiding.
Reference: [4] <author> G. Astfalk and T. Brewer. </author> <title> An Overview of the HP/Convex Exemplar Hardware. </title> <type> Tech. paper, </type> <institution> Hewlett-Packard Co., </institution> <month> June </month> <year> 1997. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: Each node that has a copy of a memory line, maintains pointers to the next forward and next backward nodes in that line's sharing list. Most of the glue logic that coherently interconnects the distributed processors and memory banks is in custom-designed gate arrays <ref> [4] </ref>. The SPP1000 uses 250-K gate gallium arsenide technology, which provided the speed to pump a 16-bit flit onto the ring each 3.33 nanoseconds. The SPP2000 uses 0.35- CMOS technology, which has evolved to provide competitive speeds in addition to its lower power consumption and higher integration. <p> This improved performance is achieved by supporting up to 32 outstanding requests in each ring interface, rather than only one <ref> [4] </ref>. Therefore, the SPP2000 ring controller can process a line request concurrently with collapsing the sharing list of a replaced line. Notice that the SPP2000's curve shape differs slightly from the curve of the 512-MB direct-mapped cache model shown [2].
Reference: [5] <author> G. Astfalk, T. Brewer, and G. Palmer. </author> <title> Cache Coherence in the Convex MPP. </title> <type> Tech. paper, </type> <institution> Hewlett-Packard Co., </institution> <month> Feb. </month> <year> 1994. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: Thus, the measured far WAR (or far RAW) latency is the average of four cases according to the location of the home node. The way that the internode coherence protocol satisfies a cache miss does in fact depend on the home node of the missed line <ref> [5, 14] </ref>; the three main cases are: Local home: the line's home node is the local node. Remote home: the line's home node is the remote node.
Reference: [6] <author> T. Brewer. </author> <title> A Highly Scalable System Utilizing up to 128 PA-RISC Processors. </title> <booktitle> In Digest of papers, COMPCON'95, </booktitle> <pages> pages 133140, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: The distributed memory is shared through a global address space, thus providing a natural and convenient programming model [17, 10]. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture <ref> [6] </ref>. In 1997, HP/Convex launched its second Exemplar generation, the SPP2000 [7]. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability. The SPP2000 uses a modern, superscalar processor that features out-of-order execution and nonblocking caches.
Reference: [7] <author> T. Brewer and G. Astfalk. </author> <title> The Evolution of the HP/Convex Exemplar. </title> <booktitle> In Digest of papers, COMPCON'97, </booktitle> <pages> pages 81 86, </pages> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: The distributed memory is shared through a global address space, thus providing a natural and convenient programming model [17, 10]. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [6]. In 1997, HP/Convex launched its second Exemplar generation, the SPP2000 <ref> [7] </ref>. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability. The SPP2000 uses a modern, superscalar processor that features out-of-order execution and nonblocking caches. <p> As visible from the measured data and the curve fits, the SPP2000 has much improved performance. The SPP2000 uses the new coherent increment primitive to reduce ring traffic and to lower barrier times <ref> [7] </ref>. In the SPP1000, when a thread reaches the barrier, it increments a counter and waits on a flag. When the last thread reaches the barrier, it updates the flag to signal to the other threads to go on.
Reference: [8] <author> W. Bryg, K. Chan, and N. Fiduccia. </author> <title> A High-Performance, Low-Cost Multiprocessor Bus for Workstations and Midrange Servers. </title> <journal> Hewlett-Packard J., </journal> <volume> 47(1):18 24, </volume> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The store transfer rate for strides 16 and 32 bytes is limited by a bus bandwidth bottleneck. Recall that the store kernel transfers two lines for each miss, a fetched line and a dirty replaced line. The SPP2000 uses the Runway bus <ref> [8] </ref>, which is an 8-byte wide bus clocked at 120 MHz. This Table 1. Accessed SPP2000 memory banks for strides 8 through 1024 bytes.
Reference: [9] <author> D. Chaiken, C. Fields, K. Kurihara, and A. Agarwal. </author> <title> Directory-Based Cache Coherence in Larg-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 1224, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [15], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [1, 9, 10] </ref>.
Reference: [10] <author> D. Culler, J. Singh, and A. Gupta. </author> <title> Parallel Computer Architecture: A Hardware/Software Approach. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1998. </year>
Reference-contexts: 1. Introduction Distributed Shared Memory is becoming the prevailing approach for building scalable parallel computers. DSM systems use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes that contain processors and memory. The distributed memory is shared through a global address space, thus providing a natural and convenient programming model <ref> [17, 10] </ref>. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [6]. In 1997, HP/Convex launched its second Exemplar generation, the SPP2000 [7]. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability. <p> Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as [15], linked-list protocols fundamentally require more directory accesses than other protocols <ref> [1, 9, 10] </ref>.
Reference: [11] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wi-jshoff. </author> <title> Experimentally Characterizing the Behavior of Multiprocessor Memory Systems: A Case Study. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 16(2):216223, </volume> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: This paper evaluates the effects of these advances on the memory and communication performance. We use a suite of microbenchmarks that is designed to characterize the performance of DSM systems [2]. Similar to other multiprocessor microbenchmarks <ref> [11, 19, 12] </ref>, these microbench-marks measure the latency and bandwidth of memory accessing. However, they also offer wide coverage of the DSM memory and communication performance, including characterizations of local and shared accesses as functions of access pattern and distance, and overheads due to cache coherence and concurrent accessing.
Reference: [12] <author> C. Hristea, D. Lenoski, and J. Keen. </author> <title> Measuring Memory Hierarchy Performance of Cache-Coherent Multiprocessors Using Micro Benchmarks. </title> <booktitle> In Supercomputing, </booktitle> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: This paper evaluates the effects of these advances on the memory and communication performance. We use a suite of microbenchmarks that is designed to characterize the performance of DSM systems [2]. Similar to other multiprocessor microbenchmarks <ref> [11, 19, 12] </ref>, these microbench-marks measure the latency and bandwidth of memory accessing. However, they also offer wide coverage of the DSM memory and communication performance, including characterizations of local and shared accesses as functions of access pattern and distance, and overheads due to cache coherence and concurrent accessing. <p> Each kernel accesses the l-byte elements of an array of size w bytes with stride s bytes. Thus, one call time corresponds to the w=s accesses in a kernel call. The memory kernels need to be carefully designed to accurately measure the latency and bandwidth of modern processors <ref> [18, 2, 12] </ref>. A kernel that measures latency must expose the access latency and cannot overlap or hide the latencies of individual memory accesses; a kernel that measures bandwidth must maximize access overlap to achieve the highest possible bandwidth.
Reference: [13] <author> D. Hunt. </author> <title> Advanced Performance Features of the 64-bit PA-8000. </title> <booktitle> In Digest of papers, COMPCON'95, </booktitle> <pages> pages 123128, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Yet, due to improved packaging and device density, a 2-node SPP2000 tower is physically the same size as a 2-node SPP1000 tower. The SPP1000 uses the Hewlett-Packard PA 7100 [3], a two-way superscalar processor, running at 100 MHz. The SPP2000 uses the PA 8000 <ref> [13] </ref>, a four-way superscalar processor, running at 180 MHz. In addition to the PA 8000's higher frequency and larger number of functional units, it supports out-of-order instruction execution, and memory latency overlapping and hiding.
Reference: [14] <institution> IEEE Computer Society. IEEE Standard for Scalable Coherent Interface (SCI), </institution> <month> Aug. </month> <year> 1993. </year> <note> IEEE Std 1596-1992. </note>
Reference-contexts: The largest SPP2000 configuration is 4 by 8, comprised of 32 nodes interconnected with 96 rings. The two-dimensional topology provides lower latency and higher bisection bandwidth. The two systems use variants of the IEEE Scalable Coherent Interface <ref> [14] </ref> to achieve internode cache coherence. The SCI protocol uses a distributed doubly-linked list to keep track of which nodes share each memory line. Each node that has a copy of a memory line, maintains pointers to the next forward and next backward nodes in that line's sharing list. <p> Thus, the measured far WAR (or far RAW) latency is the average of four cases according to the location of the home node. The way that the internode coherence protocol satisfies a cache miss does in fact depend on the home node of the missed line <ref> [5, 14] </ref>; the three main cases are: Local home: the line's home node is the local node. Remote home: the line's home node is the remote node.
Reference: [15] <author> S. Kaxiras and J. Goodman. </author> <title> The GLOW Cache Coherence Protocol Extension for Widely Shared Data. </title> <booktitle> In Proc. Int'l Conf. on Supercomputing, </booktitle> <pages> pages 3543, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Additionally, the invalidation time is linear with the number of sharing nodes. Although some of these issues are addressed in research work such as <ref> [15] </ref>, linked-list protocols fundamentally require more directory accesses than other protocols [1, 9, 10].
Reference: [16] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proc. 24th ISCA, </booktitle> <pages> pages 241251, </pages> <year> 1997. </year>
Reference-contexts: The addition of this glue logic, however, raises the local memory and communication latencies, which penalizes sequential and small-scale parallel programs. Many modern processors are incorporating interfaces for small-scale multiprocessor buses, which can be exploited to build economic small nodes with low local latency <ref> [16] </ref>. However, for a system based on small nodes to succeed with large-scale parallel programs, the remote latency should not increase prohibitively.
Reference: [17] <author> D. Lenoski and W.-D. Weber. </author> <title> Scalable Shared-Memory Multiprocessing. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: 1. Introduction Distributed Shared Memory is becoming the prevailing approach for building scalable parallel computers. DSM systems use high-bandwidth, low-latency interconnection networks to connect powerful processing nodes that contain processors and memory. The distributed memory is shared through a global address space, thus providing a natural and convenient programming model <ref> [17, 10] </ref>. The SPP1000 was introduced in 1994 as Convex's first implementation of its Exemplar DSM architecture [6]. In 1997, HP/Convex launched its second Exemplar generation, the SPP2000 [7]. Although the new system is upwardly binary compatible with older systems, it has significant differences that achieve higher performance and scalability.
Reference: [18] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable Tools for Performance Analysis. </title> <booktitle> In Proc. USENIX'96 Ann. Technical Conf., </booktitle> <pages> pages 279294, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Each kernel accesses the l-byte elements of an array of size w bytes with stride s bytes. Thus, one call time corresponds to the w=s accesses in a kernel call. The memory kernels need to be carefully designed to accurately measure the latency and bandwidth of modern processors <ref> [18, 2, 12] </ref>. A kernel that measures latency must expose the access latency and cannot overlap or hide the latencies of individual memory accesses; a kernel that measures bandwidth must maximize access overlap to achieve the highest possible bandwidth.
Reference: [19] <author> R. Saavedra, R. Gaines, and M. Carlton. </author> <title> Micro Benchmark Analysis of the KSR1. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 202213, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: This paper evaluates the effects of these advances on the memory and communication performance. We use a suite of microbenchmarks that is designed to characterize the performance of DSM systems [2]. Similar to other multiprocessor microbenchmarks <ref> [11, 19, 12] </ref>, these microbench-marks measure the latency and bandwidth of memory accessing. However, they also offer wide coverage of the DSM memory and communication performance, including characterizations of local and shared accesses as functions of access pattern and distance, and overheads due to cache coherence and concurrent accessing.
Reference: [20] <author> K. Shaw and G. Astfalk. </author> <title> Four-State Cache-Coherence in the Convex Exemplar System. Internal memo, </title> <institution> Convex Computer Corp., </institution> <month> Oct. </month> <year> 1995. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: Each processor has two off-chip data and instruction caches that are virtually addressed. The data caches within one node are kept coherent using directory-based cache coherence protocols. The SPP1000 uses a three-state protocol, and the SPP2000 uses a four-state protocol <ref> [20] </ref>. Each memory line has an associated coherence tag to keep track of which processors have cached copies of this line. In the SPP1000, each memory controller has a ring interface that connects it to one ring.
Reference: [21] <editor> SPEC CPU95 Benchmarks Results. </editor> <title> See the Standard Performance Evaluation Corp., </title> <note> web page http://www.spec.org/. </note>
Reference-contexts: On HP workstations that use these processors, 1 the PA 8000 achieves 4.7x floating-point performance and 3.6x integer performance on SPEC95, relative to the PA 7100 <ref> [21] </ref>. Each processor has two off-chip data and instruction caches that are virtually addressed. The data caches within one node are kept coherent using directory-based cache coherence protocols. The SPP1000 uses a three-state protocol, and the SPP2000 uses a four-state protocol [20].
Reference: [22] <author> Z. Zhang and J. Torrellas. </author> <title> Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA. </title> <booktitle> In Proc. HPCA-3, </booktitle> <pages> pages 272281, </pages> <year> 1997. </year> <month> 12 </month>
Reference-contexts: In the SPP2000, although giving an exclusive copy of a modified line hurts repetitive near producer-consumer communication, it is profitable for migratory lines (lines that are accessed largely by one processor at a time <ref> [22] </ref>). When a processor gets exclusive ownership of a migratory line, it does not need to request ownership when it subsequently updates this line. When the consumer performs a RAW access, the valid copy of the accessed line is in the producer's cache.
References-found: 22

