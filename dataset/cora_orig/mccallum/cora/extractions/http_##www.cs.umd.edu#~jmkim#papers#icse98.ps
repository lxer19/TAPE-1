URL: http://www.cs.umd.edu/~jmkim/papers/icse98.ps
Refering-URL: http://www.cs.umd.edu/users/jmkim/research.html
Root-URL: 
Title: An Empirical Study of Regression Test Selection Techniques  
Author: Todd L. Graves Mary Jean Harrold Jung-Min Kim Adam Porter Gregg Rothermel 
Keyword: regression testing, selective retest, empirical study  
Abstract: Regression testing is an expensive maintenance process directed at validating modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting tests from a program's existing test suite. Many regression test selection techniques have been proposed. Although there have been some analytical and empirical evaluations of individual techniques, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed. We conducted an experiment to examine the relative costs and benefits of several regression test selection techniques. The experiment examined five techniques for reusing tests, focusing on their relative abilities to reduce regression testing effort and uncover faults in modified programs. Our results highlight several differences between the techniques, and expose essential tradeoffs that should be considered when choosing a technique for practical application. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agrawal, J. Horgan, E. Krauser, and S. </author> <title> London. Incremental regression testing. </title> <booktitle> In Proc. of the Conf. on Softw. Maint., </booktitle> <pages> pages 348-357, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16].
Reference: [2] <author> M. Balcer, W. Hasling, and T. </author> <title> Ostrand. Automatic generation of test scripts from formal test specifications. </title> <booktitle> In Proc. of the 3rd Symp. on Softw. Testing, Analysis, and Verification, </booktitle> <pages> pages 210-218, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Hutchins et al. constructed tests for these programs by a process described in [10]. For each base program, they created a test pool of black-box tests, using the category partition method and Siemens Test Specification Language tool <ref> [2, 14] </ref>. They then created additional white-box tests by hand and added them to the test pool, to ensure that each exercisable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 tests.
Reference: [3] <author> J. M. Chambers, W. S. Cleveland, B. Kleiner, and P. A. Tukey. </author> <title> Graphical Methods for Data Analysis. </title> <booktitle> Wadsworth Int'l. Group, </booktitle> <address> Belmont, CA, </address> <year> 1983. </year>
Reference-contexts: The dashed vertical lines attached to the box indicate the tails of the distribution; they extend to the standard range of the data (1.5 times the inter-quartile range). All other detached points are "outliers". We also use arrays of boxplots (a type of Trellis display <ref> [3] </ref>) to show data distributions that are conditioned on one or more other variables (e.g., Figure 2.) By conditioned, we mean that data are partitioned into subsets, such that the data in each subset have the same 1 Readers who wish to examine the data should contact Adam Porter. value for
Reference: [4] <author> Y. Chen, D. Rosenblum, and K. Vo. TestTube: </author> <title> A system for selective regression testing. </title> <booktitle> In Proc. of the 16th Int'l. Conf. on Softw. Eng., </booktitle> <pages> pages 211-222, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., [1, 4, 8, 9, 12, 15, 21]). Although there have been some analytical and empirical evaluations of individual techniques <ref> [4, 18, 20, 21] </ref>, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> One such technique requires that every program statement added to or modified for P 0 be executed (if possible) by at least one test in T . 2.2.2 Safe Techniques. These techniques (e.g., <ref> [4, 11, 21] </ref>) select, under certain conditions, every test in T that can expose one or more faults in P 0 . <p> However, because dataflow-coverage-based techniques require at least as much analysis as the two most efficient safe techniques <ref> [4, 21] </ref>, we saw little reason to recommend dataflow if test selection alone is the goal. However, dataflow techniques can be useful in other parts of regression testing, such as in identification of portions of T 0 that are not adequately tested by T .
Reference: [5] <author> K. Fischer, F. Raji, and A. Chruscicki. </author> <title> A methodology for retesting modified software. </title> <booktitle> In Proc. of the Nat'l. Tele. Conf. B-6-3, </booktitle> <pages> pages 1-6, </pages> <month> Nov. </month> <year> 1981. </year>
Reference-contexts: We here describe these families and approaches, and give a representative example of each we utilize these representative examples in our experimentation. 2.2.1 Minimization Techniques. These techniques (e.g., <ref> [5, 9] </ref>) attempt to select minimal sets of tests from T , that yield coverage of modified or affected portions of P .
Reference: [6] <author> M. Harrold, J. A. Jones, and J. Lloyd. </author> <title> Design and implementation of an interprocedural data-flow tester. </title> <type> Technical report, </type> <institution> The Ohio State University, </institution> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: As a dataflow-coverage-based technique, we simulated a tool by manually inspecting program modifications, and generating a list of tuples that represent the definition-use pairs that are affected by the modifications. We then used a data-flow testing tool <ref> [6] </ref> to identify the tests in the test suite that satisfy the definition-use pairs. As a random (n) technique we created a tool that randomly selects n% of the tests from the suite. The retest-all technique required no implementation. 3.4 Experimental Design 3.4.1 Variables.
Reference: [7] <author> M. Harrold and G. Rothermel. Aristotle: </author> <title> A system for research on and development of program analysis based tools. </title> <type> Technical Report OSU-CISRC-3/97-TR17, </type> <institution> The Ohio State University, </institution> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: As a safe technique, we utilized an implementation of Rothermel and Harrold's regression test selection algorithm, implemented as a tool called DejaVu, and available as a component of the Aristotle program analysis system: the system and tool are described in <ref> [7, 19] </ref>. As a dataflow-coverage-based technique, we simulated a tool by manually inspecting program modifications, and generating a list of tuples that represent the definition-use pairs that are affected by the modifications.
Reference: [8] <author> M. Harrold and M. Soffa. </author> <title> An incremental approach to unit testing during maintenance. </title> <booktitle> In Proc. of the Conf. on Softw. Maint., </booktitle> <pages> pages 362-367, </pages> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> One such technique selects every test in T that, when executed on P , exercised at least one statement that has been deleted from P , or at least one statement that is new in or modified for P 0 . 2.2.3 Dataflow-Coverage-Based Techniques. These techniques (e.g., <ref> [8, 15, 22] </ref>) select tests that exercise data interactions that have been affected by modifications.
Reference: [9] <author> J. Hartmann and D. Robson. </author> <title> Techniques for selective revalidation. </title> <journal> IEEE Software, </journal> <volume> 16(1) </volume> <pages> 31-38, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> We here describe these families and approaches, and give a representative example of each we utilize these representative examples in our experimentation. 2.2.1 Minimization Techniques. These techniques (e.g., <ref> [5, 9] </ref>) attempt to select minimal sets of tests from T , that yield coverage of modified or affected portions of P .
Reference: [10] <author> M. Hutchins, H. Foster, T. Goradia, and T. Os-trand. </author> <title> Experiments on the effectiveness of dataflow-and controlflow-based test adequacy criteria. </title> <booktitle> In Proc. of the 16th Int'l. Conf. on Softw. Eng., </booktitle> <pages> pages 191-200, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The subjects were collected and constructed initially by Hutchins et al. <ref> [10] </ref> for use in experiments with dataflow- and controlflow-based test adequacy criteria; we modified some of the programs and versions slightly to enable their use with our tools. Table 1 describes the subjects. <p> We describe the other data in the table in the following paragraphs. 3.3.2 Tests, Test Pools, Versions, and Test Suites. Hutchins et al. constructed tests for these programs by a process described in <ref> [10] </ref>. For each base program, they created a test pool of black-box tests, using the category partition method and Siemens Test Specification Language tool [2, 14].
Reference: [11] <author> J. Laski and W. Szermer. </author> <title> Identification of program modifications and its applications in software maintenance. </title> <booktitle> In Proc. of the Conf. on Softw. Maint., </booktitle> <pages> pages 282-290, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: One such technique requires that every program statement added to or modified for P 0 be executed (if possible) by at least one test in T . 2.2.2 Safe Techniques. These techniques (e.g., <ref> [4, 11, 21] </ref>) select, under certain conditions, every test in T that can expose one or more faults in P 0 .
Reference: [12] <author> H. Leung and L. White. </author> <title> A study of integration testing and software regression at the integration level. </title> <booktitle> In Proc. of the Conf. on Softw. Maint., </booktitle> <pages> pages 290-300, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16].
Reference: [13] <author> H. Leung and L. White. </author> <title> A cost model to compare regression test strategies. </title> <booktitle> In Proc. of the Conf. on Softw. Maint., </booktitle> <pages> pages 201-208, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: We here restrict our attention to the costs and benefits defined by these models, but there are many other costs and benefits these models do not capture. Some possible additions to the models are discussed in Section 5. 3.2.1 Modeling Cost-Effectiveness. Leung and White <ref> [13] </ref> present a cost model for selective retest techniques. Their model considers both test selection and identification of inadequately tested components; we adapt it to consider just the cost-effectiveness of a regression test selection technique relative to that of the retest-all approach. <p> This model makes several simplifying assumptions. It assumes that the cost of executing tests is the same under test selection and retest all, and that all test cases have uniform costs <ref> [13] </ref>. It also assumes that all sub-costs can be expressed in equivalent units, whereas, in practice, they are often a mixture of CPU time, human effort, and equipment costs [18].
Reference: [14] <author> T. Ostrand and M. Balcer. </author> <title> The category-partition method for specifying and generating functional tests. </title> <journal> Commun. ACM, </journal> <volume> 31(6), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: Hutchins et al. constructed tests for these programs by a process described in [10]. For each base program, they created a test pool of black-box tests, using the category partition method and Siemens Test Specification Language tool <ref> [2, 14] </ref>. They then created additional white-box tests by hand and added them to the test pool, to ensure that each exercisable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 tests.
Reference: [15] <author> T. Ostrand and E. Weyuker. </author> <title> Using dataflow analysis for regression testing. </title> <booktitle> In Sixth Annual Pacific Northwest Softw. Qual. Conf., </booktitle> <pages> pages 233-247, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> One such technique selects every test in T that, when executed on P , exercised at least one statement that has been deleted from P , or at least one statement that is new in or modified for P 0 . 2.2.3 Dataflow-Coverage-Based Techniques. These techniques (e.g., <ref> [8, 15, 22] </ref>) select tests that exercise data interactions that have been affected by modifications.
Reference: [16] <author> D. Rosenblum and G. Rothermel. </author> <title> A comparative study of regression test selection techniques. </title> <booktitle> In Proc. of the 2nd Int'l. Workshop on Empir. Studies of Softw. </booktitle> <address> Maint., </address> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed <ref> [16] </ref>. We hypothesize that different regression test selection techniques create different tradeoffs between the costs of selecting and executing tests, and the need to achieve sufficient fault detection ability. Because there have been few controlled experiments to quantify these tradeoffs, we conducted such a study. <p> Thus, although our understanding of the issue is incomplete, there is some evidence to suggest that test selection can provide savings. Thus, further empirical investigation of test selection is warranted. To our knowledge, the only existing comparative study of regression test selection techniques <ref> [16] </ref>, by Rosen-blum and Rothermel, compared the test selection results of TestTube and DejaVu. The study showed that TestTube was frequently competitive with DejaVu in terms of its ability to reduce the number of test cases selected, but that DejaVu sometimes substantially outperformed TestTube.
Reference: [17] <author> D. Rosenblum and E. J. Weyuker. </author> <title> Lessons learned from a regression testing case study. </title> <journal> Empir. Softw. Eng. Journal, </journal> <volume> 2(2), </volume> <year> 1997. </year>
Reference-contexts: Therefore, cost-effectiveness is one of the first questions researchers in this area have studied. Rosenblum and Weyuker <ref> [17, 18] </ref> and Rothermel and Harrold [21] conducted empirical studies to determine whether certain safe regression testing techniques are cost-effective relative to retest all. Rosenblum and Weyuker applied their regression test selection algorithm, implemented in a tool called TestTube, to 31 versions of the KornShell and its associated test suites.
Reference: [18] <author> D. Rosenblum and E. J. Weyuker. </author> <title> Using coverage information to predict the cost-effectiveness of regression testing strategies. </title> <journal> IEEE Transactions on Softw. Eng., </journal> <volume> 23(3) </volume> <pages> 146-156, </pages> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., [1, 4, 8, 9, 12, 15, 21]). Although there have been some analytical and empirical evaluations of individual techniques <ref> [4, 18, 20, 21] </ref>, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> Therefore, cost-effectiveness is one of the first questions researchers in this area have studied. Rosenblum and Weyuker <ref> [17, 18] </ref> and Rothermel and Harrold [21] conducted empirical studies to determine whether certain safe regression testing techniques are cost-effective relative to retest all. Rosenblum and Weyuker applied their regression test selection algorithm, implemented in a tool called TestTube, to 31 versions of the KornShell and its associated test suites. <p> It also assumes that all sub-costs can be expressed in equivalent units, whereas, in practice, they are often a mixture of CPU time, human effort, and equipment costs <ref> [18] </ref>. Given this model, we needed to measure two things: the reduction in the cost of executing and validating tests, and the average analysis cost. Given our assumptions, however, the former can be measured in terms of test suite size reduction, as ( jT 0 j jTj ).
Reference: [19] <author> G. Rothermel. </author> <title> Efficient, effective regression testing using safe test selection techniques. </title> <type> Technical Report 96-101, </type> <institution> Clemson University, </institution> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: As a safe technique, we utilized an implementation of Rothermel and Harrold's regression test selection algorithm, implemented as a tool called DejaVu, and available as a component of the Aristotle program analysis system: the system and tool are described in <ref> [7, 19] </ref>. As a dataflow-coverage-based technique, we simulated a tool by manually inspecting program modifications, and generating a list of tuples that represent the definition-use pairs that are affected by the modifications.
Reference: [20] <author> G. Rothermel and M. Harrold. </author> <title> Analyzing regression test selection techniques. </title> <journal> IEEE Transactions on Softw. Eng., </journal> <volume> 22(8) </volume> <pages> 529-551, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., [1, 4, 8, 9, 12, 15, 21]). Although there have been some analytical and empirical evaluations of individual techniques <ref> [4, 18, 20, 21] </ref>, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> Although each of these steps involves important problems, in this article we restrict our attention to step 1, which involves the regression test selection problem. 2.2 Regression Test Selection Techniques A variety of regression test selection techniques have been described in the research literature. Rothermel and Harrold <ref> [20] </ref> describe several families of techniques; we consider three such families, along with two additional approaches often used in practice. We here describe these families and approaches, and give a representative example of each we utilize these representative examples in our experimentation. 2.2.1 Minimization Techniques.
Reference: [21] <author> G. Rothermel and M. Harrold. </author> <title> A safe, efficient regression test selection technique. </title> <journal> ACM Transactions on Softw. Eng. and Methodology, </journal> <volume> 6(2) </volume> <pages> 173-210, </pages> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: This tradeoff between the time required to select and run tests and the fault detection ability of the tests that are run is central to regression test selection. Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., <ref> [1, 4, 8, 9, 12, 15, 21] </ref>). Although there have been some analytical and empirical evaluations of individual techniques [4, 18, 20, 21], to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> Because there are many ways in which to approach this tradeoff, a number of test selection techniques have been proposed (e.g., [1, 4, 8, 9, 12, 15, 21]). Although there have been some analytical and empirical evaluations of individual techniques <ref> [4, 18, 20, 21] </ref>, to our knowledge only one comparative study, focusing on one aspect of two of these techniques, has been performed [16]. <p> One such technique requires that every program statement added to or modified for P 0 be executed (if possible) by at least one test in T . 2.2.2 Safe Techniques. These techniques (e.g., <ref> [4, 11, 21] </ref>) select, under certain conditions, every test in T that can expose one or more faults in P 0 . <p> Therefore, cost-effectiveness is one of the first questions researchers in this area have studied. Rosenblum and Weyuker [17, 18] and Rothermel and Harrold <ref> [21] </ref> conducted empirical studies to determine whether certain safe regression testing techniques are cost-effective relative to retest all. Rosenblum and Weyuker applied their regression test selection algorithm, implemented in a tool called TestTube, to 31 versions of the KornShell and its associated test suites. <p> However, because dataflow-coverage-based techniques require at least as much analysis as the two most efficient safe techniques <ref> [4, 21] </ref>, we saw little reason to recommend dataflow if test selection alone is the goal. However, dataflow techniques can be useful in other parts of regression testing, such as in identification of portions of T 0 that are not adequately tested by T .
Reference: [22] <author> A. B. Taha, S. M. Thebaut, and S. S. Liu. </author> <title> An approach to software fault localization and revalidation based on incremental data flow analysis. </title> <booktitle> In Proc. of the 13th Annual Intl. Comp. Softw. and Appl. Conf., </booktitle> <pages> pages 527-534, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: One such technique selects every test in T that, when executed on P , exercised at least one statement that has been deleted from P , or at least one statement that is new in or modified for P 0 . 2.2.3 Dataflow-Coverage-Based Techniques. These techniques (e.g., <ref> [8, 15, 22] </ref>) select tests that exercise data interactions that have been affected by modifications.
References-found: 22

