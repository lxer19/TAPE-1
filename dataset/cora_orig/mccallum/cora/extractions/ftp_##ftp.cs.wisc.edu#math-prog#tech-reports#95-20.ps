URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps
Refering-URL: http://www.cs.wisc.edu/~paulb/research.html
Root-URL: 
Title: Machine Learning via Polyhedral Concave Minimization  
Author: O. L. Mangasarian 
Abstract: Mathematical Programming Technical Report 95-20 November 1995 Dedicated to Klaus Ritter on the Occasion of his Sixtieth Birthday Abstract Two fundamental problems of machine learning, misclassification minimization [10, 24, 18] and feature selection, [25, 29, 14] are formulated as the minimization of a concave function on a polyhedral set. Other formulations of these problems utilize linear programs with equilibrium constraints [18, 1, 4, 3] which are generally intractable. In contrast, for the proposed concave minimization formulation, a successive linearization algorithm without stepsize terminates after a maximum average of 7 linear programs on problems with as many as 4192 points in 14-dimensional space. The algorithm terminates at a stationary point or a global solution to the problem. Preliminary numerical results indicate that the proposed approach is quite effective and more efficient than other approaches.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. </title> <note> Department of Mathematical Sciences Math Report No. 217, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1994. </year> <note> ORSA Journal on Computing, submitted. </note>
Reference-contexts: But, effective methods for its solution have been proposed in [18] and implemented in <ref> [1] </ref>. An approximate technique [6] has also been implemented. The formulation that we propose in this work terminates in a finite number of linear programs (typically less than seven) at a vertex solution or stationary point of the problem. We outline the contents of the paper now. <p> A desirable objective for such a case <ref> [24, 18, 1, 6] </ref> is to minimize the number of points of A lying in the complement of the closed halfspace reserved for it, that is, minimize the number of elements of A in: 2 as well as the number of points of B lying in the complement of the closed <p> ? u a &gt; = u ? r + e &gt; Proof The constraints of the minimization problem constitute the Karush-Kuhn-Tucker conditions for the dual linear programs: max fa T r j 0 r &lt; u = a; u &gt; which are solved by: r i = r i 2 <ref> [0; 1] </ref> for a i = 0 ; u = a + (15) The objective function e T r minimized in (13) renders the solution r of (15) unique by making r i = 0 for a i = 0; thus giving r = a fl : By using Lemma 2.3, <p> sets arising from real world problems [21], and motivate our formulation by the perturbation results of linear programming [20] to suppress as many of the coefficients w of the separating plane fx j x T w = flg as possible, we obtain the following problem for a suitably chosen 2 <ref> [0; 1] </ref> : min 8 : m e T z ! fi fi fi Aw efl + y &gt; = e; Bw + efl + z &gt; = e; y &gt; = 0; z &gt; = 0 9 ; For = 0; we obtain the robust linear program of [2]. <p> We thus can state the following result. Proposition 2.5 The feature selection problem (17) has a solution to each 2 <ref> [0; 1] </ref>: We turn our attention now to algorithmic considerations by first approximating the step function () fl , which appears in both problems (5) and (17), by a smooth concave approximation. 3 Concave Approximation of the Step Function One of the most common and useful approximations in neural networks [28, <p> Otherwise the minimized objective of (20) approximates from below (for moderate values of ff) the smallest number of misclassified points by any plane x T w = fl: 3.2 Smooth Concave Feature Selection Problem (17) Let 2 <ref> [0; 1] </ref> and ff &gt; 0: min 8 : m e T z ! fi fi fi Aw efl + y &gt; = e; Bw + efl + z &gt; = e; y &gt; = 0; z &gt; = 0 9 ; Again for this problem, the concave objective function is <p> For various values of the parameter 2 <ref> [0; 1] </ref>; emphasis of separation by the plane x T w = fl is balanced against suppression of as many coefficients of w as possible, with the term (n e T " ffv ) giving an approximation (from below) to the number of nonzero coefficients of w: By making use of <p> 5.9 Star/Galaxy (Bright) 957 266.13 69.48 14 3.2 Tic Tac Toe 332 46.45 6.44 168 98.82 96.9 Votes 267 14.76 1.56 16 3.4 Total Times 1599.97 896.11 Table 1: Comparison of Successive Linearization Algorithm (SLA) Algorithm 4.1 for the Smooth Misclassification Minimization Problem (20) with the Parametric Minimization Method (PMM) <ref> [18, 1] </ref>. SLA was coded in GAMS [5] utilizing the CPLEX solver [7]. PMM was coded was coded in AMPL [8] utilizing the MINOS LP solver [23]. exponential approximation t (; ff) = 1 " ff to the step function fl , was five. <p> Table 1 gives the percent of correctly separated points as well as CPU times using an average of ten SLA runs on the smooth misclassification minimization problem (20). These quantities are compared with those of a parametric minimization method (PMM) applied to an LPEC associated with the misclassification minimization <ref> [18, 1] </ref>. Table 1 shows that the much simpler SLA algorithm obtained a separation that was almost as good as the parametric method for solving the LPEC at considerably less computing cost. Each problem was solved using no more than a maximum average of 7 LPs over ten runs.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: Thus in the exceptional case when the convex hulls of A and B do not intersect, a single linear program <ref> [2] </ref> will generate a plane P that strictly separates the sets A and B as follows: Aw &gt; = efl e (2) Our concern here is with the usually occurring case when no plane P exists satisfying (2). <p> the misclassification minimization problem can be stated as follows: min fe T y fl + e T z fl j y &gt; = 0; z &gt; = 0g (5) Note that without the step function () fl in (5), the problem becomes a linear program (essentially the robust linear program <ref> [2, Equation (2.11)] </ref>, but without averaging over m and k), in which case y and z of (5) become: y = (Aw + efl + e) + ; z = (Bw efl + e) + (6) Thus, problem (5) without the step function () fl is equivalent to: min fl fl <p> The problem again is to separate the finite point sets A and B in R n ; but with the additional requirement of using as few of the dimensions of R n as possible. If we take as our point of departure the robust linear program <ref> [2, Equation (2.11)] </ref>, which is very effective in discriminating between sets arising from real world problems [21], and motivate our formulation by the perturbation results of linear programming [20] to suppress as many of the coefficients w of the separating plane fx j x T w = flg as possible, we <p> 2 [0; 1] : min 8 : m e T z ! fi fi fi Aw efl + y &gt; = e; Bw + efl + z &gt; = e; y &gt; = 0; z &gt; = 0 9 ; For = 0; we obtain the robust linear program of <ref> [2] </ref>. For = 1; all components of w are suppressed yielding no useful result. For sufficiently small, the program (17) selects those solutions of the robust linear program, that is (17) with = 0, that minimize e T j w j fl .
Reference: [3] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Problem 1.2 Feature Selection <ref> [4, 3] </ref> Given two finite point sets A and B in R n ; select a sufficiently small number of dimensions of R n such that a plane, constructed in the smaller dimensional space, optimizes some separation criterion between the sets A and B: We immediately note that the misclassification minimization
Reference: [4] <author> E. J. Bredensteiner and K. P. Bennett. </author> <title> Feature minimization within decision trees. </title> <note> Department of Mathematical Sciences Math Report No. 218, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1995. </year>
Reference-contexts: Problem 1.2 Feature Selection <ref> [4, 3] </ref> Given two finite point sets A and B in R n ; select a sufficiently small number of dimensions of R n such that a plane, constructed in the smaller dimensional space, optimizes some separation criterion between the sets A and B: We immediately note that the misclassification minimization
Reference: [5] <author> A. Brooke, D. Kendrick, and A. Meeraus. </author> <title> GAMS: A User's Guide. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, CA, </address> <year> 1988. </year>
Reference-contexts: SLA was coded in GAMS <ref> [5] </ref> utilizing the CPLEX solver [7]. PMM was coded was coded in AMPL [8] utilizing the MINOS LP solver [23]. exponential approximation t (; ff) = 1 " ff to the step function fl , was five.
Reference: [6] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <type> Technical Report 95-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wiscon-sin, </institution> <month> February </month> <year> 1995. </year> <note> Advances in Computational Mathematics, to appear. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: finite point sets A and B in R n ; select a sufficiently small number of dimensions of R n such that a plane, constructed in the smaller dimensional space, optimizes some separation criterion between the sets A and B: We immediately note that the misclassification minimization problem is NP-complete <ref> [6, Proposition 2] </ref>. But, effective methods for its solution have been proposed in [18] and implemented in [1]. An approximate technique [6] has also been implemented. <p> But, effective methods for its solution have been proposed in [18] and implemented in [1]. An approximate technique <ref> [6] </ref> has also been implemented. The formulation that we propose in this work terminates in a finite number of linear programs (typically less than seven) at a vertex solution or stationary point of the problem. We outline the contents of the paper now. <p> A desirable objective for such a case <ref> [24, 18, 1, 6] </ref> is to minimize the number of points of A lying in the complement of the closed halfspace reserved for it, that is, minimize the number of elements of A in: 2 as well as the number of points of B lying in the complement of the closed
Reference: [7] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Optimizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <year> 1992. </year>
Reference-contexts: SLA was coded in GAMS [5] utilizing the CPLEX solver <ref> [7] </ref>. PMM was coded was coded in AMPL [8] utilizing the MINOS LP solver [23]. exponential approximation t (; ff) = 1 " ff to the step function fl , was five.
Reference: [8] <author> R. Fourer, D. Gay, and B. Kernighan. </author> <title> AMPL. </title> <publisher> The Scientific Press, </publisher> <address> South San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: SLA was coded in GAMS [5] utilizing the CPLEX solver [7]. PMM was coded was coded in AMPL <ref> [8] </ref> utilizing the MINOS LP solver [23]. exponential approximation t (; ff) = 1 " ff to the step function fl , was five.
Reference: [9] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: We now prescribe a simple finite successive linearization algorithm (essentially a Frank-Wolfe algorithm <ref> [9] </ref> without a stepsize) for solving (22) that appears to give good computational results. (See Section 5.) Other more complex computational schemes for this problem are given in [13, 12]. 4.1 Successive Linearization Algorithm (SLA) Start with a random x 0 2 R n : Having x i determine x i+1
Reference: [10] <author> David Heath. </author> <title> A geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Johns Hopkins University-Baltimore, Maryland, </institution> <year> 1992. </year>
Reference-contexts: Our second test consisted of solving the smooth concave feature selection problem (21) by SLA 4.1. The test problem consisted of the Wisconsin Breast Cancer Database WBCD tested in the above set of tests, with one modification. Two new random features, uniformly distributed on the interval <ref> [0; 10] </ref> were added to the problem, so that the problem space was R 11 instead of the original R 9 : With = 0:05 in problem (21), and by solving 6 successive linear programs, the SLA was able to suppress the effect of the random components x 10 and x
Reference: [11] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: We then introduce in Section 3 a simple concave exponential approxi-mation of the step function, similar to the classical sigmoid function of neural networks <ref> [28, 11, 17] </ref>, but with the significant difference of concavity of the proposed approximation which is not shared by the sigmoid function. This concavity is possible, because the step function is applied here to nonnegative variables. <p> [0; 1]: We turn our attention now to algorithmic considerations by first approximating the step function () fl , which appears in both problems (5) and (17), by a smooth concave approximation. 3 Concave Approximation of the Step Function One of the most common and useful approximations in neural networks <ref> [28, 11] </ref> is the sigmoid function approximation of the step function fl defined as s (; ff) := 1 + " ff ; ff &gt; 0 (18) Here " is the base of the natural logarithm.
Reference: [12] <author> R. Horst, P. Pardalos, and N. V. Thoai. </author> <title> Introduction to Global Optimization. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dodrecht, Netherlands, </address> <year> 1995. </year>
Reference-contexts: We now prescribe a simple finite successive linearization algorithm (essentially a Frank-Wolfe algorithm [9] without a stepsize) for solving (22) that appears to give good computational results. (See Section 5.) Other more complex computational schemes for this problem are given in <ref> [13, 12] </ref>. 4.1 Successive Linearization Algorithm (SLA) Start with a random x 0 2 R n : Having x i determine x i+1 as follows: 6 x i+1 2 arg vertex min x2X rf (x i )(x x i ) X = fx j Ax &lt; = b; x &gt; =
Reference: [13] <author> R. Horst and H. Tuy. </author> <title> Global Optimization. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year> <note> Second, Revised Edition. </note>
Reference-contexts: We now prescribe a simple finite successive linearization algorithm (essentially a Frank-Wolfe algorithm [9] without a stepsize) for solving (22) that appears to give good computational results. (See Section 5.) Other more complex computational schemes for this problem are given in <ref> [13, 12] </ref>. 4.1 Successive Linearization Algorithm (SLA) Start with a random x 0 2 R n : Having x i determine x i+1 as follows: 6 x i+1 2 arg vertex min x2X rf (x i )(x x i ) X = fx j Ax &lt; = b; x &gt; =
Reference: [14] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [15] <author> Z.-Q. Luo, J.-S. Pang, and D. Ralph. </author> <title> Mathematical Programs with Equilibrium Constraints. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1996. </year> <month> 10 </month>
Reference-contexts: that e T y fl + e T z fl = min e T (Aw + efl + e) fl + e T (Bw efl + e) fl (12) To establish the existence of solution to problem (5) and to relate it to a linear program with equilibrium constraints (LPEC) <ref> [18, 19, 16, 15] </ref>, we state the following lemma.
Reference: [16] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. </author> <title> Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. </title> <type> Technical Report 275, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton, </institution> <address> Ontario, Hamilton, Ontario L8S 4K1, Canada, </address> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: that e T y fl + e T z fl = min e T (Aw + efl + e) fl + e T (Bw efl + e) fl (12) To establish the existence of solution to problem (5) and to relate it to a linear program with equilibrium constraints (LPEC) <ref> [18, 19, 16, 15] </ref>, we state the following lemma.
Reference: [17] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: We then introduce in Section 3 a simple concave exponential approxi-mation of the step function, similar to the classical sigmoid function of neural networks <ref> [28, 11, 17] </ref>, but with the significant difference of concavity of the proposed approximation which is not shared by the sigmoid function. This concavity is possible, because the step function is applied here to nonnegative variables.
Reference: [18] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction We shall consider the following two fundamental problems of machine learning: Problem 1.1 Misclassification Minimization <ref> [24, 18] </ref> Given two finite point sets A and B in the n-dimensional real space R n , construct a plane that minimizes the number of points of A falling in one of the closed halfspaces determined by the plane and the number of points of B falling in the other <p> But, effective methods for its solution have been proposed in <ref> [18] </ref> and implemented in [1]. An approximate technique [6] has also been implemented. The formulation that we propose in this work terminates in a finite number of linear programs (typically less than seven) at a vertex solution or stationary point of the problem. <p> A desirable objective for such a case <ref> [24, 18, 1, 6] </ref> is to minimize the number of points of A lying in the complement of the closed halfspace reserved for it, that is, minimize the number of elements of A in: 2 as well as the number of points of B lying in the complement of the closed <p> that e T y fl + e T z fl = min e T (Aw + efl + e) fl + e T (Bw efl + e) fl (12) To establish the existence of solution to problem (5) and to relate it to a linear program with equilibrium constraints (LPEC) <ref> [18, 19, 16, 15] </ref>, we state the following lemma. <p> 5.9 Star/Galaxy (Bright) 957 266.13 69.48 14 3.2 Tic Tac Toe 332 46.45 6.44 168 98.82 96.9 Votes 267 14.76 1.56 16 3.4 Total Times 1599.97 896.11 Table 1: Comparison of Successive Linearization Algorithm (SLA) Algorithm 4.1 for the Smooth Misclassification Minimization Problem (20) with the Parametric Minimization Method (PMM) <ref> [18, 1] </ref>. SLA was coded in GAMS [5] utilizing the CPLEX solver [7]. PMM was coded was coded in AMPL [8] utilizing the MINOS LP solver [23]. exponential approximation t (; ff) = 1 " ff to the step function fl , was five. <p> Table 1 gives the percent of correctly separated points as well as CPU times using an average of ten SLA runs on the smooth misclassification minimization problem (20). These quantities are compared with those of a parametric minimization method (PMM) applied to an LPEC associated with the misclassification minimization <ref> [18, 1] </ref>. Table 1 shows that the much simpler SLA algorithm obtained a separation that was almost as good as the parametric method for solving the LPEC at considerably less computing cost. Each problem was solved using no more than a maximum average of 7 LPs over ten runs.
Reference: [19] <author> O. L. Mangasarian. </author> <title> The ill-posed linear complementarity problem. </title> <type> Technical Report 95-15, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> August </month> <year> 1995. </year> <booktitle> Proceedings of the International Conference on Complementarity Problems, </booktitle> <institution> Johns Hopkins University, </institution> <month> November 1-4, </month> <title> 1995, </title> <publisher> SIAM Publishers, </publisher> <address> Philadelphia, PA, </address> <note> submitted. </note>
Reference-contexts: that e T y fl + e T z fl = min e T (Aw + efl + e) fl + e T (Bw efl + e) fl (12) To establish the existence of solution to problem (5) and to relate it to a linear program with equilibrium constraints (LPEC) <ref> [18, 19, 16, 15] </ref>, we state the following lemma.
Reference: [20] <author> O. L. Mangasarian and R. R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17(6) </volume> <pages> 745-752, </pages> <month> November </month> <year> 1979. </year>
Reference-contexts: If we take as our point of departure the robust linear program [2, Equation (2.11)], which is very effective in discriminating between sets arising from real world problems [21], and motivate our formulation by the perturbation results of linear programming <ref> [20] </ref> to suppress as many of the coefficients w of the separating plane fx j x T w = flg as possible, we obtain the following problem for a suitably chosen 2 [0; 1] : min 8 : m e T z ! fi fi fi Aw efl + y &gt;
Reference: [21] <author> O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <journal> Operations Research, </journal> <volume> 43(4) </volume> <pages> 570-577, </pages> <month> July-August </month> <year> 1995. </year>
Reference-contexts: If we take as our point of departure the robust linear program [2, Equation (2.11)], which is very effective in discriminating between sets arising from real world problems <ref> [21] </ref>, and motivate our formulation by the perturbation results of linear programming [20] to suppress as many of the coefficients w of the separating plane fx j x T w = flg as possible, we obtain the following problem for a suitably chosen 2 [0; 1] : min 8 : m
Reference: [22] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine, California, </institution> <note> http://www.ics.uci.edu/AI/ML/MLDBRepository.html., 1992. </note>
Reference-contexts: We turn our attention to some computational results. 7 5 Numerical Tests The proposed approach was tested numerically on publicly available databases from the University of California Repository of Machine Learning Databases <ref> [22] </ref> as well as the Star/Galaxy database collected by Odewahn [26].
Reference: [23] <author> B. A. Murtagh and M. A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year> <note> MINOS 5.4 Release Notes, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: SLA was coded in GAMS [5] utilizing the CPLEX solver [7]. PMM was coded was coded in AMPL [8] utilizing the MINOS LP solver <ref> [23] </ref>. exponential approximation t (; ff) = 1 " ff to the step function fl , was five. This value of ff allows t (; ff) to capture the essence of of the step function fl with sufficient smoothness to make the proposed algorithm work effectively without overflow or underflow.
Reference: [24] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Cambridge, MA 02142, 1993. </address> <publisher> The AAAI Press/The MIT Press. </publisher>
Reference-contexts: 1 Introduction We shall consider the following two fundamental problems of machine learning: Problem 1.1 Misclassification Minimization <ref> [24, 18] </ref> Given two finite point sets A and B in the n-dimensional real space R n , construct a plane that minimizes the number of points of A falling in one of the closed halfspaces determined by the plane and the number of points of B falling in the other <p> A desirable objective for such a case <ref> [24, 18, 1, 6] </ref> is to minimize the number of points of A lying in the complement of the closed halfspace reserved for it, that is, minimize the number of elements of A in: 2 as well as the number of points of B lying in the complement of the closed
Reference: [25] <author> P. M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(9):917-922, </volume> <month> September </month> <year> 1977. </year>
Reference: [26] <author> S. Odewahn, E. Stockwell, R. Pennington, R. Hummphreys, and W. Zumach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: We turn our attention to some computational results. 7 5 Numerical Tests The proposed approach was tested numerically on publicly available databases from the University of California Repository of Machine Learning Databases [22] as well as the Star/Galaxy database collected by Odewahn <ref> [26] </ref>.
Reference: [27] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: 1]; emphasis of separation by the plane x T w = fl is balanced against suppression of as many coefficients of w as possible, with the term (n e T " ffv ) giving an approximation (from below) to the number of nonzero coefficients of w: By making use of <ref> [27, Corollary 32.3.3] </ref> which implies that a concave function, bounded from below on a nonempty polyhedral set, attains its minimum on that set, we can state the following existence results for the two smooth problems above. <p> be transformed to the following concave minimization problem: min ff (x) j Ax &lt; = 0g; (22) where f : R ` ! R, is a differentiable, concave function bounded below on the nonempty polyhedral feasible region of (22), A 2 R pfi` and b 2 R p : By <ref> [27, Corollary 32.3.4] </ref> it follows that f attains its minimum at a vertex of the feasible region of (22).
Reference: [28] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: We then introduce in Section 3 a simple concave exponential approxi-mation of the step function, similar to the classical sigmoid function of neural networks <ref> [28, 11, 17] </ref>, but with the significant difference of concavity of the proposed approximation which is not shared by the sigmoid function. This concavity is possible, because the step function is applied here to nonnegative variables. <p> [0; 1]: We turn our attention now to algorithmic considerations by first approximating the step function () fl , which appears in both problems (5) and (17), by a smooth concave approximation. 3 Concave Approximation of the Step Function One of the most common and useful approximations in neural networks <ref> [28, 11] </ref> is the sigmoid function approximation of the step function fl defined as s (; ff) := 1 + " ff ; ff &gt; 0 (18) Here " is the base of the natural logarithm.
Reference: [29] <author> W. Siedlecki and J. Sklansky. </author> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 197-220, </pages> <year> 1988. </year>
Reference: [30] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year> <month> 11 </month>
Reference-contexts: This in effect suppresses as many components of w as possible. Computationally, one obviously varies until some "best" value of (w; fl) is obtained as evinced by a cross-validating procedure <ref> [30] </ref>. By using an identical technique to that used to establish the existence of a solution to problem (5), we can similarly replace problem (17) by an LPEC and establish existence of a solution to it. We thus can state the following result.
References-found: 30

