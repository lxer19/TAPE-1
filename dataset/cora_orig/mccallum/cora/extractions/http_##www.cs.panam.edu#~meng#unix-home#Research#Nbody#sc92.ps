URL: http://www.cs.panam.edu/~meng/unix-home/Research/Nbody/sc92.ps
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/Research/Nbody/
Root-URL: http://www.cs.panam.edu
Title: Astrophysical N-body Simulations Using Hierarchical Tree Data Structures  
Author: Michael S. Warren John K. Salmon 
Address: Los Alamos, NM 87545 Pasadena, CA 91125  
Affiliation: Theoretical Astrophysics Physics Department Los Alamos National Laboratory California Institute of Technology  
Date: Sep. 11, 1992  
Note: 1992 Bell Prize Finalist. Submitted to Proceedings of Supercomputing '92,  
Abstract: We report on recent large astrophysical N-body simulations executed on the Intel Touchstone Delta system. We review the astrophysical motivation, and the numerical techniques, and discuss steps taken to parallelize these simulations. The methods scale as O(N log N ), for large values of N , and also scale linearly with the number of processors. The performance, sustained for a duration of 67 hours was between 5.1 and 5.4 Gflop/sec on a 512 processor system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles. </title> <address> New York: </address> <publisher> Mcgraw-Hill International, </publisher> <year> 1981. </year>
Reference-contexts: It is common to treat such systems on computers by 1 particle methods or N-body methods <ref> [1] </ref>, rather than finite-element or finite-difference methods, which cannot cope with the high dimensionality of the problem domain.
Reference: [2] <author> A. W. Appel, </author> <title> "An efficient program for many-body simulation," </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> vol. 6, </volume> <editor> p. </editor> <volume> 85, </volume> <year> 1985. </year>
Reference-contexts: Methods employing an adaptive tree data structure have been popular in recent years because the resulting time complexity is O (N log N ) or O (N ), and is relatively insensitive to the spatial distribution of particles <ref> [2, 3, 4] </ref>. N-body simulations which use adaptive tree data structures are referred to as treecodes.
Reference: [3] <author> J. E. Barnes and P. Hut, </author> <title> "A hierarchical O(NlogN) force-calculation algorithm," </title> <journal> Nature, </journal> <volume> vol. 324, </volume> <pages> pp. 446-449, </pages> <year> 1986. </year>
Reference-contexts: Methods employing an adaptive tree data structure have been popular in recent years because the resulting time complexity is O (N log N ) or O (N ), and is relatively insensitive to the spatial distribution of particles <ref> [2, 3, 4] </ref>. N-body simulations which use adaptive tree data structures are referred to as treecodes.
Reference: [4] <author> L. Greengard and V. I. Rokhlin, </author> <title> "A fast algorithm for particle simulations," </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 73, </volume> <pages> pp. 325-348, </pages> <year> 1987. </year>
Reference-contexts: Methods employing an adaptive tree data structure have been popular in recent years because the resulting time complexity is O (N log N ) or O (N ), and is relatively insensitive to the spatial distribution of particles <ref> [2, 3, 4] </ref>. N-body simulations which use adaptive tree data structures are referred to as treecodes. <p> HOT can accommodate independent timestep integration schemes as well as an important new class of cell opening criteria [17] for which it is difficult to predict, a priori, which cells are required in a rectangular domain. HOT will also support O (N ) methods <ref> [4] </ref>, which involve interactions between cells, in addition to the cell-body and body-body interactions which prevail in the O (N log N ) methods we have used until now.
Reference: [5] <author> S. B. Baden, </author> <title> Run-time Partitioning of Scientific Continuum Calculations Running on Multiprocessors. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1987. </year>
Reference-contexts: Instead, we adopted the technique of orthogonal recursive bisection, ORB <ref> [5] </ref>, whereby space is recursively divided in two, and half the processors are assigned to each domain until there is one processor associated with each rectangular domain.
Reference: [6] <author> J. K. Salmon, P. J. Quinn, and M. S. Warren, </author> <title> "Using parallel computers for very large N-body simulations: Shell formation using 180k particles," in Heidelberg Conference on Dynamics and Interactions of Galaxies (R. </title> <editor> Wielen, ed.), </editor> <address> (New York), </address> <pages> pp. 216-218, </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [7] <author> M. S. Warren, P. J. Quinn, J. K. Salmon, and W. H. Zurek, </author> <title> "Dark haloes formed via dissipationless collapse: I. shapes and alignment of angular momentum," </title> <journal> Astrophysical Journal, </journal> <note> 1992. (accepted for publication). </note>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [8] <author> M. S. Warren and J. K. Salmon, </author> <title> "A parallel treecode for gravitational N-body simulations with up to 20 million particles," </title> <journal> Bulletin of the American Astronomical Society, </journal> <volume> vol. 23, no. 4, </volume> <editor> p. </editor> <volume> 1345, </volume> <year> 1991. </year>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [9] <author> M. S. Warren, W. H. Zurek, P. J. Quinn, and J. K. Salmon, </author> <title> "The shape of the invisible halo: N-body simulations on parallel supercomputers," in After the First Three Minutes Workshop Proceedings (S. </title> <editor> Holt, V. Trimble, and C. Bennett, eds.), </editor> <address> (New York), </address> <publisher> AIP Press, </publisher> <year> 1991. </year>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [10] <author> D. P. Fullagar, P. J. Quinn, and J. K. Salmon, </author> <title> "N-body simulations of a 4 isophote deviations in elliptical galaxies," in Proceedings of ESO/EIPC Workshop on the Structure, Dynamics and Chemical Evolution of Early-Type Galaxies (I. </title> <editor> J. Danziger, ed.), </editor> <address> (Munich), </address> <institution> European Southern Observatory, </institution> <year> 1992. </year>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [11] <author> C. Grillmair, </author> <title> Dynamics of Globular Cluster Systems. </title> <type> PhD thesis, </type> <institution> Australia National University, </institution> <year> 1992. </year>
Reference-contexts: At press time, the fastest available platform is the Intel Touchstone Delta at Caltech, on which the simulations reported here were run. We have also run significant simulations on an Intel ipsc/860, Ncube machines, and the Caltech/JPL Mark III <ref> [6, 7, 8, 9, 10, 11] </ref>. The statistics quoted below are based on internal diagnostics compiled by our program. Essentially, we keep track of the number of interactions computed.
Reference: [12] <author> C. L. Bennett et al., </author> <title> "Preliminary separation of galactic and cosmic microwave emissions for the COBE DMR," </title> <type> COBE preprint 92-05, </type> <institution> Goddard Space Flight Center, </institution> <year> 1992. </year>
Reference-contexts: In June 1992, in response to the recently announced measurement of the microwave background anisotropy by the COBE satellite <ref> [12, 13] </ref>, we ran two large simulations of the Cold Dark Matter model of the Universe.
Reference: [13] <author> G. F. Smoot et al., </author> <title> "Structure in the COBE DMR first year maps," </title> <type> COBE preprint 92-04, </type> <institution> Goddard Space Flight Center, </institution> <year> 1992. </year>
Reference-contexts: In June 1992, in response to the recently announced measurement of the microwave background anisotropy by the COBE satellite <ref> [12, 13] </ref>, we ran two large simulations of the Cold Dark Matter model of the Universe.
Reference: [14] <author> E. Bertschinger and J. M. Gelb, </author> <title> "Large cosmological N-body simulations," </title> <journal> Computers in Physics, </journal> <volume> vol. 5, no. 2, </volume> <editor> p. </editor> <volume> 164, </volume> <year> 1991. </year>
Reference-contexts: Thus, there is no highly tuned commercial software available for carrying out the inner loops of our calculations. In the quest for 1 A simulation with 17 million bodies has been reported <ref> [14] </ref> using a different approximation which suffers from limited spatial resolution. It ran for over 600 hours on an IBM vector supercomputer.
Reference: [15] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Pren-tice Hall, </publisher> <year> 1988. </year>
Reference-contexts: The Delta is neither the first nor the last parallel supercomputer that will run our N-body code. We have found that we can achieve an acceptable degree of portability by disciplined use of a few very simple communication primitives (essentially the cshift routine from CrosIII <ref> [15] </ref>) and a well-defined model of I/O (essentially cubix [16] with some modifications). We have developed our own ANSI C implementation of these primitives, and we have ported them to all of the parallel processors at our disposal. Other systems are available which ostensibly provide the same level of portability.
Reference: [16] <author> J. K. Salmon, "CUBIX: </author> <title> Programming hypercubes without programming hosts," in Hypercube MultiProcessors 1987 (M. </title> <editor> Heath, ed.), </editor> <booktitle> (Philadelphia), </booktitle> <pages> pp. 3-9, </pages> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: We have found that we can achieve an acceptable degree of portability by disciplined use of a few very simple communication primitives (essentially the cshift routine from CrosIII [15]) and a well-defined model of I/O (essentially cubix <ref> [16] </ref> with some modifications). We have developed our own ANSI C implementation of these primitives, and we have ported them to all of the parallel processors at our disposal. Other systems are available which ostensibly provide the same level of portability.
Reference: [17] <author> J. K. Salmon and M. S. Warren, </author> <title> "Skeletons from the treecode closet," </title> <journal> Journal of Computational Physics, </journal> <note> 1992. (submitted). 7 </note>
Reference-contexts: The use of independent timestep integration methods which advance particles according to the local timescale is problematical with the old code. HOT can accommodate independent timestep integration schemes as well as an important new class of cell opening criteria <ref> [17] </ref> for which it is difficult to predict, a priori, which cells are required in a rectangular domain. <p> The critical value of N at which O (N ) methods outperform O (N log N ) methods is far from clear because the "constants" depend so strongly on both the specific problem and the most mundane of implementation details. However, by extending the error-estimates in <ref> [17] </ref> to describe the interactions present in O (N ) algorithms, we believe that superior performance can be achieved for realistic values of N .
References-found: 17

