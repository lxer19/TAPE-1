URL: http://www.cse.ogi.edu/CSLU/fsj/issues/issue5/sparse-ann.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/fsj/html/home.html
Root-URL: http://www.cse.ogi.edu
Email: (nikko@speech.kth.se)  
Title: The Free Speech  Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks  
Author: Nikko Strm, 
Address: Hearing, KTH, Stockholm, Sweden  Stockholm, Sweden  
Affiliation: Department of Speech, Music and  Centre for Speech Technology, KTH,  
Note: Journal, Issue 5(1997) Published 10/22/97 fi 1997 All rights reserved.  
Abstract: This paper presents new methods for training large neural networks for phoneme probability estimation. An architecture combining timedelay windows and recurrent connections is used to capture the important dynamic information of the speech signal. Because the number of connections in a fully connected recurrent network grows super-linear with the number of hidden units, schemes for sparse connection and connection pruning are explored. It is found that sparsely connected networks outperform their fully connected counterparts with an equal number of connections. The implementation of the combined architecture and training scheme is described in detail. The networks are evaluated in a hybrid HMM/ANN system for phoneme recognition on the TIMIT database, and for word recognition on the WAXHOLM database. The achieved phone error-rate, 27.8%, for the standard 39 phoneme set on the core testset of the TIMIT database is in the range of the lowest reported. All training and simulation software used is made freely available by the author, and detailed information about the software and the training process is given in an Appendix. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Basu A. & Svendsen T. </author> <year> (1993): </year> <title> A time-frequency segmental neural network for phoneme recognition, </title> <booktitle> Proc. ICASSP 93, pp I-509 - I-512. </booktitle>
Reference: <author> Baum E. B. & Wilczek F. </author> <year> (1988): </year> <title> Supervised lerning of probabilty distributions by neural networks, Neural Information Processing Systems, </title> <editor> Ed. D. Z Anderson, </editor> <publisher> American Institute of Physics. </publisher>
Reference: <author> Bertenstam, J. Blomberg, M., Carlson, R., Elenius, K, Granstrm, B., Gustafson, J., Hunnicutt, S., Hgberg, J., Lindell, R., Neovius, L., de Serpa-Leitao, A. and Strm, N. </author> <year> (1995a): </year> <title> The Waxholm Application Database, </title> <booktitle> Proc. EUROSPEECH '95 Madrid, </booktitle> <pages> pp. 833 836. </pages>
Reference-contexts: The system used in the evaluations was developed for speech recognition in the WAXHOLM human/machine dialog system (Blomberg et al ., 1993). The Swedish database collected in the development of that system <ref> (Bertenstam et al., 1995a,b) </ref> is used for evaluation on the word level. Many aspects of the word recognition used in the WAXHOLM system are not covered in this paper, but the recognition module of the system is accounted for in depth by Strm (1996). <p> More specifically, the domain is boat traffic timetables and information about hotels, camping grounds and restaurants. A description of the ASR module of the WAXHOLM system is given in Strm (1996). In this paper, the WAXHOLM database <ref> (Bertenstam et al ., 1995a,b) </ref>, collected in wizard-of-oz simulations of the system, is used in recognition experiments to evaluate the hybrid HMM/ANN system on the word-level as well as the phoneme-level. The utterances are continuously spoken and the bigram perplexity is 28 for the test data.
Reference: <author> Bertenstam, J. Blomberg, M., Carlson, R., Elenius, K, Granstrm, B., Gustafson, J., Hunnicutt, S., Hgberg, J., Lindell, R., Neovius, L., de Serpa-Leitao, A., Nord, L. and Strm, N. </author> <year> (1995b): </year> <title> Spoken dialogue data collection in the Waxholm project, </title> <booktitle> STL-QPSR 1/1995, </booktitle> <pages> pp. 50-73. </pages>
Reference: <author> Bishop C. M. </author> <year> (1995): </year> <title> Neural Networks for Pattern Recognition , Oxford University Press, </title> <publisher> Oxford. </publisher>
Reference-contexts: Because we wish to make replication of the results straightforward, the description is rather detailed. Most of the material is well-known background-knowledge, covered in textbooks on ANN computing <ref> (e.g., Bishop, 1995 or Ripley, 1996) </ref>.
Reference: <author> Blomberg M., Carlson R., Elenius K., Granstrm B., Gustafson J., Hunnicut S., Lindell R., & Neovius L. </author> <year> (1993): </year> <title> An Experimental Dialogue System: </title> <booktitle> Waxholm, Proc EUROSPEECH '93, </booktitle> <pages> pp. </pages> <month> 1867-1870. </month> <title> Bourlard & Wellekens (1988): Links between Markov Models and Multilayer Perceptrons, </title> <journal> IEEE Trans on PAMI, </journal> <volume> 12 (12), </volume> <pages> pp. 1167-1178. </pages>
Reference-contexts: Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 26 5 Recognition results The phoneme probability ANNs have been evaluated in two different experimental environments. The system used in the evaluations was developed for speech recognition in the WAXHOLM human/machine dialog system <ref> (Blomberg et al ., 1993) </ref>. The Swedish database collected in the development of that system (Bertenstam et al., 1995a,b) is used for evaluation on the word level. <p> The diagonal line is the optimal Bayesian classifier function. We see that the ANN is very close to a Bayessian classifier on the training data but deviates slightly from the ideal line for the test data. 5.2 Word recognition results on the WAXHOLM human/machine dialog task The WAXHOLM demonstrator <ref> (Blomberg et al., 1993) </ref> is a human/machine spoken dialogue system for tourist information about the Stockholm archipelago. More specifically, the domain is boat traffic timetables and information about hotels, camping grounds and restaurants. A description of the ASR module of the WAXHOLM system is given in Strm (1996).
Reference: <author> Bourlard H. & Morgan N. </author> <year> (1993): </year> <title> Continuous Speech Recognition by Connectionist Statistical Methods, </title> <journal> IEEE trans. on Neural Networks, </journal> <volume> 4 (6), </volume> <pages> pp. 893-909. </pages> <month> Nikko Strm, </month> <title> Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 35 Bridle J. </title> <editor> S. </editor> <year> (1989): </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition, in Neuro-computing: Algorithms, Architectures and Applications, </title> <editor> Eds: </editor> <booktitle> Fougelman-Soulie and Hrault, </booktitle> <pages> pp. 227-236, </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Bundine W. L. & Wiegend A. S. </author> <year> (1994): </year> <title> Computing Second Derivatives in Feed-Forward Networks: A Review, </title> <journal> IEEE Trans on Neural Networks, </journal> <volume> 5 (3), </volume> <pages> pp. 1-9. </pages>
Reference-contexts: The connection pruning is then governed by the salience, 0.5 2 E/ w 2 w 2 , of each weight. An overview of methods for computing second derivatives in feed-forward ANNs can be found in <ref> (Bundine and Wiegend, 1994) </ref>. There are three approximations active in the OBD method: the off-diagonal terms of the Hessian, the higher-than-quadratic terms and the gradients deviation from zero.
Reference: <author> Cohen M., Franco H., Morgan N., Rumelhart D. & Abrash V. </author> <year> (1992): </year> <title> Hybrid neural network/Hidden Markov Model continuous-speech recognition, </title> <booktitle> Proc ICSLP '92 , pp. </booktitle> <volume> 915 918. </volume>
Reference: <author> Digalakis V. V., Ostendorf M. & Rohlicek J. R. </author> <year> (1992): </year> <title> Fast algorithms for phone classification and recognition using segment-based models, </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol 40 , pp. </volume> <pages> 2885-2896. </pages>
Reference: <author> Duda R. O. & Hart P. E. </author> <year> (1973): </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference: <author> English, T. M. & Boggess, L. C. </author> <year> (1992): </year> <title> "Back-propagation training of a neural network for word spotting, </title> <booktitle> Proc. IEEE ICASSP 92, </booktitle> <volume> Vol 2 , pp. </volume> <pages> 357-360. </pages>
Reference: <author> Fahlman S. E. </author> <year> (1988): </year> <title> An empirical study of learning speed in backpropagation networks, </title> <institution> Technical Report CMU-CS-88-162 , Carnegie-Mellon University, Computer Science Dept., </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: There are many well-known methods that utilize curvature information for the optimization. General optimization methods, e.g., Newtons method or conjugate gradient methods (see for example Luenberger, 1984) can be applied, as well as more or less specialized methods for ANN-training like QuickProp <ref> (Fahlman 1988) </ref>, and application of Levenberg/Marquardts method, Levenberg (1944), Marquardt (1963). However, it is nontrivial combine these methods with stochastic approximation algorithms, where weights are updated before the whole training data is processed (an epoch). <p> The algorithm finds one particular local minimum of the objective function, and the particular minimum found depends heavily on the starting point in the search space, i.e., the initial connection weights. It is common practice to initialize the weights to small random numbers <ref> (e.g., Fahlman, 1988) </ref>. This implies that sigmoid and tanhyp units operate in the linear region of the nonlinearity.
Reference: <author> Fant G. </author> <year> (1969): </year> <title> Acoustic Theory of Speech Perception, </title> <publisher> Mouton, </publisher> <address> The Hague, The Netherlands. </address>
Reference-contexts: d i in (9), and the resulting d i is simply the difference between a i and the target (+1.0 for the correct class and -1.0 otherwise). 2.2 Recurrent connections and timedelay Dynamic features of speech such as formant movements, that are known to be of importance for phoneme classification <ref> (e.g., Fant 1969) </ref>, are not captured by the short time spectrum representation used as input to the network. Therefore, phonetic classification of short-time spectra can be greatly enhanced by considering also the context of neighboring spectra.
Reference: <author> Ghosh G. & Tumer K. </author> <year> (1994): </year> <title> Structural adaptation and generalization in supervised feed forward networks, </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1 (4), </volume> <pages> pp. 430-458. </pages>
Reference: <author> Gish, H. </author> <year> (1990): </year> <title> A Probabilistic Approach to the Understanding and Training of Neural Network Classifiers, </title> <booktitle> Proc IEEE ICASSP '90, </booktitle> <address> pp1361-1364. </address>
Reference: <author> Glass J., Chang J., & McCandless M. </author> <year> (1996): </year> <title> A Probabilistic Framework for Feature-Based Speech Recognition, </title> <booktitle> Proc ICSLP 96 pp. </booktitle> <pages> 2277-2280. </pages>
Reference: <author> Goldenthal W. </author> <year> (1994): </year> <title> Statistical trajectory models for phonetic recognition, </title> <type> Technical report MIT/LCS/TR-642, </type> <institution> MIT Lab. for Computer Science. </institution>
Reference: <author> Hampshire J. B. </author> <year> (1990): </year> <title> Equivalence Proofs for MultiLayer Perceptron Classifiers and the Bayesian Discriminant Function, </title> <booktitle> Proc. of the 1990 Connectionist Models Summer School , Eds: </booktitle> <editor> Touretsky, Sejnowski and Hinton, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA. </address>
Reference: <author> Hgberg J. & Sjlander K. </author> <year> (1996): </year> <title> Cross Phone State Clustering Using Lexical Stress and Context, </title> <booktitle> Proc ICSLP 96, </booktitle> <pages> pp. 474-477. </pages>
Reference-contexts: As can be seen in Table 4, the fully connected ANN performs worse than the sparsely connected ANN, in spite of the benefit of 12% more connections. Our results can be compared with a continuous density HMM system <ref> (Hgberg and Sjlander, 1996) </ref>, trained and evaluated on the same database as used in this study, yielding 25% phone error-rate, and later in (Sjlander and Hgberg, 1996) 22.6% phone error-rate, and only 14.7% word error-rate. All results are summarized in Table 4. <p> Our results can be compared with a continuous density HMM system (Hgberg and Sjlander, 1996), trained and evaluated on the same database as used in this study, yielding 25% phone error-rate, and later in <ref> (Sjlander and Hgberg, 1996) </ref> 22.6% phone error-rate, and only 14.7% word error-rate. All results are summarized in Table 4. In contrast to the TIMIT database, the phoneme recognition is slightly worse for the ANN-based system than the HMM. <p> The word-level results on the WAXHOLM database are not as good as the phoneme results. A continuous density HMM trained and evaluated on the same data as the Baseline, fully connected network, 125 hidden units. 98,175 connections. Sparsely connected network, 300 hidden units. 86,903 connections. Triphone CDHMM recognizer.. <ref> (Sjlander and Hgberg, 1996) </ref>. phone error rate 27.5% 25.2% 22.6% word error rate 24.1% 23.2% 14.7% Table 4. Recognition results on the WAXHOLM database. Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 34 ANN system performed significantly better.
Reference: <author> Hornik K., Stinchcombe M. & White H. </author> <year> (1989): </year> <title> Multilayer feed-forward networks are universal approximators, </title> <booktitle> Neural Networks, </booktitle> <pages> 2 , pp. 359-366. </pages>
Reference-contexts: The strength of ANN models is the weak constraints they put on the mapping; It has been shown that, given a sufficient number of hidden units and characterizing data, feed-forward ANNs with one layer of hidden units can approximate any bounded function on a compact set with arbitrary accuracy <ref> (e.g., Hornik, Stinchcombe and White 1989) </ref>. In practice however, the performance is limited due to: i) problems finding the set of connection weights that gives the optimal network and ii) the size of the available database. <p> Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 19 3.4 Network topology The topology of the network, i.e., the number of hidden units and the manner in which they are connected, determines the functional capacity of the classifier. It has been proved <ref> (e.g., Hornik, Stinchcombe and White, 1989) </ref> that ANNs with one layer of hidden units can approximate with arbitrary precision any smooth function on a compact domain. However, this is a theoretical result that requires that the function is completely known and that the number of hidden units is unbounded.
Reference: <author> Juang B.-H. & Katagiri S. </author> <year> (1992): </year> <title> Discriminative Learning for Minimum Error Classification, </title> <journal> IEEE trans. On Signal Processing, </journal> <volume> 40 (12), </volume> <pages> pp. 3043-3054. </pages>
Reference: <author> Kershaw D. J., Hochberg M. M. & Robinson A. J. </author> <title> (1996 ): Context-dependent classes in a hybrid recurrent network-HMM speech recognition system , In Advances in Neural Information Processing Systems 8 , eds: </title> <editor> Touretsky D. S., Mozer M. C, and Hasselmo M. E., </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lamel L. & Gauvain J. L. </author> <year> (1993): </year> <title> High performance speaker-independent phone recognition using CDHMM, </title> <booktitle> Proc. EUROSPEECH, </booktitle> <pages> pp. 121 124. </pages>
Reference: <author> Le Cun Y., Boser B., Denker J. S., Henderson J. S., Howard R. E., Hubbard W. </author> & <title> Jackel L. </title>
Reference: <author> D. </author> <year> (1990b): </year> <title> Handwritten Digit Recognition with a Backpropagation Network, In Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 36 Advances in Neural Information Processing Systems vol. </title> <editor> II , ed: Touretsky D. </editor> <booktitle> S., </booktitle> <pages> pp. 396 404, </pages> <address> San Mateo, </address> <publisher> California IEEE, Morgan Kaufmann. </publisher>
Reference: <author> Le Cun Y., Denker J. S. & Solla S. A. </author> <year> (1990a): </year> <title> Optimal brain damage, </title> <booktitle> In Advances in Neural Information Processing Systems vol. </booktitle> <editor> II , ed: Touretsky D. </editor> <booktitle> S., </booktitle> <pages> pp. 589-605, </pages> <address> San Mateo, </address> <publisher> California IEEE, Morgan Kaufmann. </publisher>
Reference-contexts: Several different criteria for selecting connections for deletion have been suggested in the literature. An overview of the methods can be found in (Thimm and Fiesler, 1995). The most well known are smallest variance (Sietsma and Dow, 1991) and optimal brain damage (OBD) <ref> (Le Cun, Denker, and Solla, 1990a) </ref>. The OBD method is based on a local approximation of the contribution of individual weights to the objective function.
Reference: <author> Lee, K. F. </author> <year> (1989): </year> <title> Automatic Speech Recognition; The Development of the SPHINX System , Kluwer Academic Publishers, Dordrecht Lee K-F & Hon H-W (1989): Speaker-independent Phone Recognition using Hidden Markov Models, </title> <journal> IEEE Trans. On Acoustics, Speech, and Signal Processing, </journal> <volume> 37 (11), </volume> <pages> pp. 1641-1648. </pages>
Reference-contexts: The 61 symbols of the database are sometimes considered a too narrow transcription for practical use, and are therefore collapsed into a smaller number of classes. In this study we perform evaluations using the 39 phoneme set defined in <ref> (Lee and Hon, 1989) </ref> that have evolved into an unofficial standard for phoneme recognition experiments. However, in conformity with the study of Robinson (1994), the full 61 symbol set is represented in the output layer of the ANN as well as in the phoneme bigram grammar of the dynamic decoding.
Reference: <author> Levenberg K. </author> <year> (1944): </year> <title> A method for the solution of certain problems in least squares, </title> <journal> Quart. Appl. Math., </journal> <pages> 2 , pp. 164-168. </pages>
Reference: <author> Levin, E. </author> <year> (1990): </year> <title> "Word recognition using hidden control neural architecture, </title> <booktitle> Proc IEEE ICASSP '90, </booktitle> <volume> Vol 1 , pp. </volume> <pages> 433-436. </pages>
Reference-contexts: Several different methods exist for combining the ANN classification of subunits into sequences that constitutes words. A few of the more well-known methods are: Hybrid HMM/ANN Architecture (Bourlard and Wellekens, 1988), Linked Predictive Neural Networks (Tebelskis and Waibel, 1990), Hidden Control Neural Architecture <ref> (Levin, 1990) </ref>, and Stochastic Observation HMM (Mitchel, Harper, and Jamieson, 1996). Of the mentioned methods, the hybrid HMM/ANN architecture is the most widespread today. ANN systems undisputedly solve some problems better than all other methods in automatic speech recognition.
Reference: <author> Li, K. P., Naylor, J. A. & Rossen, M. L. </author> <year> (1992): </year> <title> "A whole word recurrent neural network for keyword spotting, </title> <booktitle> Proc. IEEE ICASSP 92, </booktitle> <volume> Vol 2 , pp. </volume> <pages> 81-84. </pages>
Reference: <author> Luenberger G. L. </author> <year> (1984): </year> <title> Linear and Nonlinear Programming, </title> <publisher> Addison-Wesley Publishing Company, Inc. </publisher>
Reference-contexts: There are many well-known methods that utilize curvature information for the optimization. General optimization methods, e.g., Newtons method or conjugate gradient methods <ref> (see for example Luenberger, 1984) </ref> can be applied, as well as more or less specialized methods for ANN-training like QuickProp (Fahlman 1988), and application of Levenberg/Marquardts method, Levenberg (1944), Marquardt (1963).
Reference: <author> Mari J. F., Fohr D. & Junqua J.C. </author> <title> (1996) A second-order HMM for high performance word and phoneme-based continuous speech recognition, </title> <booktitle> Proc. ICASSP, </booktitle> <pages> pp. 435438. </pages>
Reference: <author> Marquardt D. </author> <year> (1963): </year> <title> An algorithm for least-squares estimation of nonlinear parameters, </title> <journal> SIAM Jl. Appl. Math., </journal> <pages> 11 , pp. 431-441. </pages>
Reference: <author> Mitchel C. D., Harper M. P. & Jamieson L. H. </author> <year> (1996): </year> <title> Stochastic Observation Hidden Markov Models, </title> <booktitle> Proc IEEE ICASSP '96, </booktitle> <pages> pp. </pages> <month> 617-620. </month> <title> Pearlmutter B A (1990): Dynamic Recurrent Neural Networks, </title> <type> Technical Report CMU-CS 88-191, </type> <institution> Carnegie-Mellon University, Computer Science Dept. Pittsburg, </institution> <address> PA. </address>
Reference-contexts: Several different methods exist for combining the ANN classification of subunits into sequences that constitutes words. A few of the more well-known methods are: Hybrid HMM/ANN Architecture (Bourlard and Wellekens, 1988), Linked Predictive Neural Networks (Tebelskis and Waibel, 1990), Hidden Control Neural Architecture (Levin, 1990), and Stochastic Observation HMM <ref> (Mitchel, Harper, and Jamieson, 1996) </ref>. Of the mentioned methods, the hybrid HMM/ANN architecture is the most widespread today. ANN systems undisputedly solve some problems better than all other methods in automatic speech recognition.
Reference: <author> Rabiner L. and Juang B-H (1993): </author> <title> Fundamentals of Speech Recognition, </title> <address> Englewood Cliffs NJ, </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Richard M. D. & Lippman R. P. </author> <year> (1991): </year> <title> Neural network classifiers estimate Bayesian a posteriori probabilities , Neural Computation, </title> <note> vol 3 , pp. 461-483. </note>
Reference: <author> Ripley B. D. </author> <year> (1996): </year> <title> Pattern Recognition and Neural Networks, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Robinson A.J. </author> <year> (1994): </year> <title> An application of Recurrent Nets to Phone Probability Estimation, </title> <booktitle> IEEE trans. on Neural Networks Vol 5 (2), </booktitle> <pages> pp. 298-305. </pages>
Reference-contexts: The lowest error-rate achieved on the core test set of the TIMIT database, 27.8%, compares favorably with systems using other methods. The only lower error-rate reported is from another ANN based system <ref> (Robinson, 1994) </ref>. The word-level results on the WAXHOLM database are not as good as the phoneme results. A continuous density HMM trained and evaluated on the same data as the Baseline, fully connected network, 125 hidden units. 98,175 connections. Sparsely connected network, 300 hidden units. 86,903 connections.
Reference: <author> Robinson T. & Fallside F. </author> <year> (1991): </year> <title> A Recurrent Error Propagation Network Speech Recognition System, </title> <booktitle> Computer Speech & Language 5:3, </booktitle> <pages> pp. 259-274. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986): </year> <title> Learning internal representations by error propagation, </title> <editor> in Rumelhart, D. E., G. E. Hinton, (eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Vol. 1 Foundations ., chapter 8. </booktitle> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, ISBN 0-262-18120-7. </address>
Reference-contexts: This will be covered more thoroughly in section 3. Except for some special units, activities in our framework are computed in the same way as the classic ANNs of <ref> (Rumelhart, Hinton and Williams, 1986) </ref>, but the sigmoid function is replaced by the computationally more convenient tanhyp function. <p> This approach differs from TDNN in that the activity of a unit at a particular time depends recursively on activities in its layer and lower layer at all previous times. Networks with recurrent connections are called recurrent neural networks (RNN) <ref> (Rumelhart, Hinton and Williams, 1986) </ref> or dynamic neural networks (Pearlmutter, 1990) and this is currently the most successful architecture for phoneme recognition (Robinson and Fallside, 1991; Robinson, 1994). TDNNs and RNNs have much in common; in particular, both use timedelayed connections to incorporate context into the classification.
Reference: <author> Schroeder M. R., Atal B. S. & Hall J. L. </author> <year> (1979): </year> <title> Objective Measure of Certain Speech Signal Degradations Based on Masking Properties of the Human Auditory Perception, in Frontiers of Speech Communication Research, </title> <editor> B. Lindblom and S. hman, eds., </editor> <publisher> Academic Press, </publisher> <pages> pp. 217-229. </pages>
Reference-contexts: The filter-bank consists of a number of overlapping triangular, equidistantly Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 17 spaced filters (see Figure 4 ) on the perceptually motivated Mel frequency scale <ref> (Schroeder, Atal and Hall, 1979) </ref>. We use 24 Mel spaced filters covering the frequency range 0-8000 Hz. The cosine transform is applied to the filter-bank vector, yielding the cepstrum coefficients.
Reference: <author> Sietsma J.& Dow R. J. F. </author> <year> (1991): </year> <title> Creating artificial neural networks that generalize, </title> <booktitle> Neural Networks, </booktitle> <pages> 4 (1) pp. 67-69. </pages> <month> Nikko Strm, </month> <title> Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 37 Sjlander & Hgberg (1996): Trying to improve phone and word recognition using finely tuned phone-like units, </title> <booktitle> Proc. Swedish Phonetics Conference 96 , PHONUM 4:1996 , pp. </booktitle> <pages> 125-128, </pages> <address> Ume universitets tryckeri, Ume, Sweden. </address>
Reference-contexts: Several different criteria for selecting connections for deletion have been suggested in the literature. An overview of the methods can be found in (Thimm and Fiesler, 1995). The most well known are smallest variance <ref> (Sietsma and Dow, 1991) </ref> and optimal brain damage (OBD) (Le Cun, Denker, and Solla, 1990a). The OBD method is based on a local approximation of the contribution of individual weights to the objective function.
Reference: <author> Steeneken H. J. M. & van Leeuwen D. A. </author> <year> (1995): </year> <title> Multilingual Assessment of Speaker Independent Large Vocabulary Speech-recognition Systems: </title> <booktitle> the SQALE-project, Proc. EUROSPEECH '95, </booktitle> <pages> pp. 1271-1274. </pages>
Reference-contexts: Preference for a well-established technology and the relatively few equally excellent results reported on the word-level are possible reasons for the mild interest in ANN solutions. However, the evaluation of the SQALE-project <ref> (Steeneken and van Leeuwen, 1995) </ref> is an example of a case where hybrid HMM/ANN technology significantly outperforms state-ofthe-art HMM systems provided by leading research sites for large vocabulary tasks.
Reference: <author> Solla S. A., Levin E. & Fleisher M. </author> <year> (1988): </year> <title> Accelerated Learning in Layered Neural Networks, </title> <journal> Complex Systems, </journal> <pages> 2 , pp. 625-640. </pages>
Reference-contexts: The target values for the output units are 1.0 for the unit corresponding to the correct class and -1.0 for all other units. The objective function for the backpropagation training is based on the cross-entropy distance in the RHW domain <ref> (Solla, Levin, and Fleisher, 1988) </ref>.
Reference: <author> Strm N. </author> <year> (1992): </year> <title> Development of a Recurrent TimeDelay Neural Net Speech Recognition System, </title> <address> STL-QPSR 2-3/1992, </address> <pages> pp. 1-44, </pages> <institution> KTH (Royal Institute of Technology), Dept. of Speech, Music and Hearing, Sweden. </institution>
Reference: <author> Strm N. </author> <year> (1996): </year> <title> Continuous speech recognition in the WAXHOLM dialogue system, </title> <booktitle> STL QPSR 4/1996, </booktitle> <pages> pp. </pages> , <institution> KTH (Royal Institute of Technology), Dept. of Speech, Music and Hearing, Sweden. </institution>
Reference-contexts: The transform used in our experiments computes a standard variation of the Mel cepstrum coefficients. Readers familiar with the HTK toolkit (Young et al., 1995) will notice that the features used here are almost identical to those of its feature extraction tool. The procedure is discussed more elaborately in <ref> (Strm, 1996) </ref>. Here we give only an outline of the procedure. The speech signal is divided into short overlapping frames as shown in Figure 4 . The framerate is 100 frames per second. The DC offset in each frame is removed and pre-emphasis is applied to the signal. <p> In the dynamic decoding step of the ASR, this frame-based output from the network is used to find the (in some sense) optimal sequence of phonemes or words. The dynamic decoding of the system used in this study is described in more detail in <ref> (Strm, 1996) </ref>. To summarize this study, the well-known hybrid HMM/ANN paradigm (Bourlard and Wellekens, 1990) is adopted, where the output activities are interpreted as the a posteriori phoneme probabilities, p (c i | o) (see section 2.6).
Reference: <author> Tebelskis, J. & Waibel, A. </author> <year> (1990): </year> <title> "Large vocabulary recognition using linked predictive neural networks, </title> <booktitle> Proc. IEEE ICASSP '90, </booktitle> <volume> Vol 1 , pp. </volume> <pages> 437-440. </pages>
Reference-contexts: Several different methods exist for combining the ANN classification of subunits into sequences that constitutes words. A few of the more well-known methods are: Hybrid HMM/ANN Architecture (Bourlard and Wellekens, 1988), Linked Predictive Neural Networks <ref> (Tebelskis and Waibel, 1990) </ref>, Hidden Control Neural Architecture (Levin, 1990), and Stochastic Observation HMM (Mitchel, Harper, and Jamieson, 1996). Of the mentioned methods, the hybrid HMM/ANN architecture is the most widespread today. ANN systems undisputedly solve some problems better than all other methods in automatic speech recognition.
Reference: <author> Thimm G. & Fiesler E. </author> <year> (1995): </year> <title> Evaluationg pruning methods, </title> <booktitle> In 1995 International Symposium on Artificial Neural Networks , Proc. </booktitle> <volume> ISANN 95, </volume> <pages> pp. </pages> <address> A2 20-25, </address> <institution> National Chiao Tung University, Hsinchu, Taiwan. </institution>
Reference-contexts: Several different criteria for selecting connections for deletion have been suggested in the literature. An overview of the methods can be found in <ref> (Thimm and Fiesler, 1995) </ref>. The most well known are smallest variance (Sietsma and Dow, 1991) and optimal brain damage (OBD) (Le Cun, Denker, and Solla, 1990a). The OBD method is based on a local approximation of the contribution of individual weights to the objective function.
Reference: <author> Waibel A., Hanazawa T., Hinton G., Shikano K. & Lang K. </author> <title> (1987) : Phoneme Recognition Using TimeDelay Neural Networks, </title> <type> ATR Technical Report TR-006, </type> <institution> ATR, </institution> <address> Japan. </address>
Reference-contexts: Recurrent connections are of course not the only path to good results, but other existing solutions have different problems. An ANN architecture without recurrent connections is used with good results by Bourlard and Morgan (1993). Instead they use time-delay windows <ref> (Waibel et al ., 1987) </ref> to capture the temporal cues of the speech signal. The lack of recurrent connections make the training algorithm more robust, but very large networks are used to achieve good results, and therefore the available computing resources limit the performance of the system.
Reference: <author> White H. </author> <year> (1989): </year> <title> Learning in Artificial Neural Networks: A Statistical Perspective, </title> <booktitle> Neural Computation 1 (4), </booktitle> <pages> pp. 425-464. </pages>
Reference-contexts: The strength of ANN models is the weak constraints they put on the mapping; It has been shown that, given a sufficient number of hidden units and characterizing data, feed-forward ANNs with one layer of hidden units can approximate any bounded function on a compact set with arbitrary accuracy <ref> (e.g., Hornik, Stinchcombe and White 1989) </ref>. In practice however, the performance is limited due to: i) problems finding the set of connection weights that gives the optimal network and ii) the size of the available database. <p> Nikko Strm, Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 19 3.4 Network topology The topology of the network, i.e., the number of hidden units and the manner in which they are connected, determines the functional capacity of the classifier. It has been proved <ref> (e.g., Hornik, Stinchcombe and White, 1989) </ref> that ANNs with one layer of hidden units can approximate with arbitrary precision any smooth function on a compact domain. However, this is a theoretical result that requires that the function is completely known and that the number of hidden units is unbounded.
Reference: <author> Young S., Jansen J., Odell J., Ollason D. & Woodland P. </author> <year> (1995): </year> <title> HTK Hidden Markov Toolkit, </title> <institution> Entropic Cambridge Research Laboratory. </institution>
Reference-contexts: Recognition results on the TIMIT and WAXHOLM databases are reported in section 5. The HMM paradigm has currently the advantage of a large mature body of easily available software <ref> (e.g., Young et al., 1995) </ref>. To promote further development in the hybrid HMM/ANN field, and to make reproduction of our results easier, the software toolkit used for training and running the neural networks of this study is made freely available. <p> Therefore, it is common practice to transform the input speech signal to the short-time frequency domain before feeding it to the phonetic classifier. The transform used in our experiments computes a standard variation of the Mel cepstrum coefficients. Readers familiar with the HTK toolkit <ref> (Young et al., 1995) </ref> will notice that the features used here are almost identical to those of its feature extraction tool. The procedure is discussed more elaborately in (Strm, 1996). Here we give only an outline of the procedure. <p> Equation (26) can be derived from linear regression and is equivalent to the default delta coefficients of the HTK toolkit <ref> (Young et al ., 1995) </ref>. Second order time derivatives, so called delta-delta parameters, are computed in the same manner by applying (26) again to the delta coefficients.
Reference: <author> Zue V., Seneff S. & Glass J. </author> <year> (1991): </year> <title> Speech Database Development: TIMIT and beyond, </title> <journal> Speech Communication, </journal> <volume> 9 (4), </volume> <pages> pp. 351-356. </pages> <month> Nikko Strm, </month> <title> Phoneme Probability Estimation with Dynamic Sparsely Connected Artificial Neural Networks 38 </title>
Reference-contexts: Many aspects of the word recognition used in the WAXHOLM system are not covered in this paper, but the recognition module of the system is accounted for in depth by Strm (1996). In addition to the word-level evaluation, phoneme recognition on the well-known American English TIMIT database <ref> (Zue, Seneff and Glass, 1991) </ref> has been performed to be able to calibrate the performance against other systems. The decoder from the word recognizer of the WAXHOLM system was used also for the phoneme recognition experiments by simply assigning a word to each phoneme.
References-found: 53

