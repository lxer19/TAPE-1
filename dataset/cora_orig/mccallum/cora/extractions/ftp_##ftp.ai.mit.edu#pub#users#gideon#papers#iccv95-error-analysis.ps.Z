URL: ftp://ftp.ai.mit.edu/pub/users/gideon/papers/iccv95-error-analysis.ps.Z
Refering-URL: http://www.ai.mit.edu/people/gideon/gideon.html
Root-URL: 
Title: Large Accurate Internal Camera Calibration using Rotation with Analysis of Sources of Error  
Author: G. P. Stein 
Address: Cambridge, MA, 02139  
Affiliation: AI Lab  
Abstract: This paper describes a simple and accurate method for internal camera calibration based on tracking image features through a sequence of images while the camera undergoes pure rotation. A special calibration object is not required and the method can therefore be used both for laboratory calibration and for self calibration in autonomous robots working in unstructured environments. Experimental results with real images show that focal length and aspect ratio can be found to within 0.15 percent, and lens distortion error can be reduced to a fraction of a pixel. The location of the principal point and the location of the center of radial distortion can each be found to within a few pixels. In this paper we perform a simple analysis to show to what extent do the various technical details affect the accuracy of the results. We show that having pure rotation is important if the features are derived from objects close to the camera. In the basic method accurate angle measurement is important. The need to accurately measure the angular displacements can be eliminated by rotating the camera through a complete circle while taking an overlapping sequence of images and using the constraint that the sum of the angles must equal 360 degrees. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-91-J-4038. Support for Gideon P. Stein is provided by a fellowship from the National Science Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Basu, A., </author> <title> "Active Calibration: </title> <booktitle> Alternative Strategy and Analysis" In Proceedings of IEEE Conference on Computer Vision and Pattern recognition, </booktitle> <pages> 495-500, </pages> <address> New York, NY, </address> <month> June </month> <year> (1993) </year>
Reference-contexts: This method is currently too inaccurate for finding internal camera parameters. Gennery [10] also uses unconstrained motion. No information is given as to the accuracy of the results. Methods that use camera rotation are presented in <ref> [1] </ref> [5] [11]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> This method is currently too inaccurate for finding internal camera parameters. Gennery [10] also uses unconstrained motion. No information is given as to the accuracy of the results. Methods that use camera rotation are presented in <ref> [1] </ref> [5] [11]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> Since we will only be using the center part of the image the signal to noise ratio of the feature detector will also be reduced. 6.5 Use of small angle approximations Both [5] and <ref> [1] </ref> use small angle approximations to reach closed form solutions for the camera parameters. These approximations, sin () ' and cos () ' 1, are only good for very small angles.
Reference: [2] <author> Beardley, P., et al, </author> <title> "Camera Calibration Using Multiple Images" In Proceedings of ECCV, </title> <address> 312-320, Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: These methods do not require knowing or finding the position of the object relative to the camera. In [4] the vanishing points of parallel lines drawn on the faces of a cube are used to locate the focal length and principal point. In <ref> [2] </ref> planar sets of parallel lines are rotated around an axis not perpendicular to the plane. The motion of the vanishing points due to the rotation is used to find the principal point.
Reference: [3] <author> Brown, </author> <note> D.C.,"Close -range Camera Calibration" Photogrammetric Engineering 37 855-866 (1971) </note>
Reference-contexts: In [2] planar sets of parallel lines are rotated around an axis not perpendicular to the plane. The motion of the vanishing points due to the rotation is used to find the principal point. Results using the plumb line method <ref> [3] </ref>, which uses the images of straight lines for calibrating lens distortion, a method for finding the aspect ratio using the image of a sphere [16], are given in section 7 for comparison to the rotation method.
Reference: [4] <author> Caprile, B. and Torre, V., </author> <title> "Using Vanishing Points for Camera Calibration" International Journal of Computer Vision4, </title> <month> 127-140 </month> <year> (1990) </year>
Reference-contexts: These methods do not require knowing or finding the position of the object relative to the camera. In <ref> [4] </ref> the vanishing points of parallel lines drawn on the faces of a cube are used to locate the focal length and principal point. In [2] planar sets of parallel lines are rotated around an axis not perpendicular to the plane.
Reference: [5] <author> Du, F. and Brady, M., </author> <title> "Self Calibration of the Intrinsic Parameters of Cameras for Active Vision Systems" In Proceedings of IEEE Conference on Computer Vision and Pattern recognition, </title> <address> 477-482, New York, NY, </address> <month> June </month> <year> (1993) </year>
Reference-contexts: This method is currently too inaccurate for finding internal camera parameters. Gennery [10] also uses unconstrained motion. No information is given as to the accuracy of the results. Methods that use camera rotation are presented in [1] <ref> [5] </ref> [11]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. No experimental results were given for the principal point. In <ref> [5] </ref> first the principal point is found and then using the principal point the focal length and scale factor are found. The principal point is found by tracing the motion of feature points as the camera rotates. <p> Simulation results show that given a feature detector with accuracy of 0:2 pixels the principal point can be found to within a few pixels the focal length can be found to within 1 percent. The accuracy of the experimental results are not shown. The work shown in <ref> [5] </ref> is the most closely related to this paper and a comparison is in order. The method presented in [5] uses three separate stages to recover the camera parameters. In the rotation method presented here, all the parameters are found in one step and is therefore more straightforward. <p> The accuracy of the experimental results are not shown. The work shown in <ref> [5] </ref> is the most closely related to this paper and a comparison is in order. The method presented in [5] uses three separate stages to recover the camera parameters. In the rotation method presented here, all the parameters are found in one step and is therefore more straightforward. The method for finding the focal length presented in [5] uses small angle approximations to predict the motion of points in the <p> The method presented in <ref> [5] </ref> uses three separate stages to recover the camera parameters. In the rotation method presented here, all the parameters are found in one step and is therefore more straightforward. The method for finding the focal length presented in [5] uses small angle approximations to predict the motion of points in the image while for signal to noise reasons, large rotations are required to make accurate measurements. In this paper the full 3D rotation equations are used which are accurate for large and small angles. <p> In this paper the full 3D rotation equations are used which are accurate for large and small angles. The method for finding the radial distortion in <ref> [5] </ref> requires being able to rotate the camera accurately in two orthogonal directions and that these be parallel to the XY axes of the image plane. This is a rather complex mechanical setup. <p> One could try to use points that are located only near the center of the image as in <ref> [5] </ref>. The effects of radial distortion will be much smaller but that will limit one to small angle rotations and loss in angle measurement accuracy. <p> Since we will only be using the center part of the image the signal to noise ratio of the feature detector will also be reduced. 6.5 Use of small angle approximations Both <ref> [5] </ref> and [1] use small angle approximations to reach closed form solutions for the camera parameters. These approximations, sin () ' and cos () ' 1, are only good for very small angles.
Reference: [6] <editor> Faugeras,O.D., et al., </editor> <booktitle> "Camera Self-Calibration : Theory and Experiments" In Proceedings of ECCV, </booktitle> <pages> 321-334, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May </month> <year> (1992) </year>
Reference-contexts: It would be a great advantage to be able to calibrate a camera using only feature coordinates in the image plane. This cannot be done with a single image. Instead it requires camera motion. The rotation method developed here fits into this category. Faugeras et al. <ref> [6] </ref> develop a method where a motion sequence of a camera moving in an unconstrained manner can be used to calculate the internal camera parameters. A camera model with no lens distortion is used.
Reference: [7] <author> Faugeras,O.D., </author> <title> "What can be seen in three dimensions with an uncalibrated stereo rig" In Proceedings of ECCV, </title> <type> 563-578, </type> <institution> Italy, </institution> <month> May </month> <year> (1992) </year>
Reference-contexts: Camera calibration is important if we wish to derive metric information from the images, although qualitative information can obtained from uncalibrated cameras and a stereo camera pair. Some knowledge can be obtained using uncalibrated cameras <ref> [7] </ref> [14], but the mainstream efforts in robot vision assume some means of calibration. In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the internal and external camera parameters.
Reference: [8] <author> Fryer, J.G. and Mason, </author> <title> S.O.,"Rapid Lens Calibration of a Video Camera"Photogrammetric Engineering and Remote Sensing 55 437-442, </title> <year> (1989) </year>
Reference: [9] <author> Ganapathy,S., </author> <title> "Decomposition of transformation matrices for robot vision" Pattern Recognition Letters2, </title> <month> 401-412 </month> <year> (1984) </year>
Reference-contexts: In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the internal and external camera parameters. The intermediate parameters can often be found quite easily [19] <ref> [9] </ref> and can be used in this form for some tasks. Finding the internal parameters from the intermediate parameters explicitly is a difficult task [19]. Work on motion vision and pose estimation often use the perspective projection model and assume that the internal camera parameters are known [13].
Reference: [10] <author> Gennery, </author> <title> D.B., </title> <booktitle> "Stereo-camera Calibration" in Proc. Image Understanding Workshop, </booktitle> <pages> 101-108, </pages> <month> Nov. </month> <year> (1979) </year>
Reference-contexts: A camera model with no lens distortion is used. Simulations show that assuming zero mean Gaussian noise of only 0.01 pixels this method produces errors of over 9% in the focal length. This method is currently too inaccurate for finding internal camera parameters. Gennery <ref> [10] </ref> also uses unconstrained motion. No information is given as to the accuracy of the results. Methods that use camera rotation are presented in [1] [5] [11]. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles.
Reference: [11] <author> Hartley, R.I., </author> <title> "Self-Calibration from Multiple Views with a Rotating Camera" in Proceedings of the European Conference on Computer Vision (1994) </title>
Reference-contexts: This method is currently too inaccurate for finding internal camera parameters. Gennery [10] also uses unconstrained motion. No information is given as to the accuracy of the results. Methods that use camera rotation are presented in [1] [5] <ref> [11] </ref>. Basu [1] tracks the motion of contour lines in the image as the camera pans or tilts through small angles. No lens distortion model is used. The experimental results show that the focal length and scale factor can be found within 3 percent error. <p> This is a rather complex mechanical setup. In the method presented here, one is only required to be able to rotate about the center of projection. Recent work by Hartley <ref> [11] </ref> has shown that given a perfect perspective projection model it is possible in theory to recover both the camera parameters and the angle of rotation. <p> For example, an increase in the estimate of the focal length can be compensated for by a decrease in the estimate of the angles. In practice we found that we could not achieve better than 4 percent accuracy in focal length estimates using the iterative method presented in <ref> [11] </ref>. 1.3 Brief overview of the rotation method The basic idea of the rotation method is very simple.
Reference: [12] <author> Lenz, R.K. and Tsai, R.Y., </author> <title> "Techniques for Calibration of the Scale Factor and Image Center for High accuracy 3-D Machine Vision Metrology" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 10,713-720 </month> <year> (1988) </year>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object <ref> [12] </ref> [23]. Outdoors, control points could be markings on the ground [18] or buildings whose positions can be verified from maps [19]. <p> If any of these values are unknown, as in our case or are known to vary as in <ref> [12] </ref> then S must be found by calibration.
Reference: [13] <author> Longuet-Higgins, H.C., </author> <title> "A computer algorithm for reconstructing a scene from two projections" Nature 293,133-135 (1981) </title>
Reference-contexts: Finding the internal parameters from the intermediate parameters explicitly is a difficult task [19]. Work on motion vision and pose estimation often use the perspective projection model and assume that the internal camera parameters are known <ref> [13] </ref>. They also assume that the parameters that can correct for lens distortion are known. In the case of motion vision the structure of the world and the position of the camera are unknown and will be determined using a calibrated camera.
Reference: [14] <author> Mohr, R. and Arbogast, E., </author> <title> "It can be done without camera calibration" Pattern Recognition Letters 12,39-43 (1991) </title>
Reference-contexts: Camera calibration is important if we wish to derive metric information from the images, although qualitative information can obtained from uncalibrated cameras and a stereo camera pair. Some knowledge can be obtained using uncalibrated cameras [7] <ref> [14] </ref>, but the mainstream efforts in robot vision assume some means of calibration. In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the internal and external camera parameters.
Reference: [15] <institution> More, J.J., et al., "User Guide for Minpack-1" Argonne National Laboratory, Argonne, Illinois (1980) </institution>
Reference-contexts: The features were defined as the corners of the black squares. A typical pair of images is shown in figure 4. 4.4 Nonlinear optimization code The camera parameters were found using a nonlinear optimization program based on the subroutine LMDIF from the software package MINPACK-1 <ref> [15] </ref>. This subroutine uses a modified Levenberg-Marquart algorithm adjust the parameters so as to minimize the sum square error P i . It calculates derivatives using forward differencing. Each of the camera parameters could either be fixed to a constant value or could be allowed to be optimized.
Reference: [16] <author> Penna, M.A. </author> <title> "Camera Calibration: A quick and Easy Way to Determine the Scale Factor" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 13,1240-1245 </month> <year> (1991) </year>
Reference-contexts: The motion of the vanishing points due to the rotation is used to find the principal point. Results using the plumb line method [3], which uses the images of straight lines for calibrating lens distortion, a method for finding the aspect ratio using the image of a sphere <ref> [16] </ref>, are given in section 7 for comparison to the rotation method.
Reference: [17] <editor> Press, W.H. et al., </editor> <publisher> "Numerical Recipes in C" Cambridge University Press, </publisher> <year> (1988) </year>
Reference-contexts: Since each of the angles, and hence the sum of the angles, is a monotonic function of our guess of f it is simple to find the correct f iteratively using simple numerical methods such as bisection or Brent's method <ref> [17] </ref>. camera parameters do not stay constant as we vary our guess of f . As an example Figure 3d shows the values of the radial distortion parameters K 1 and K 2 obtained using various values of focal length.
Reference: [18] <editor> Slama, C.C. ed, </editor> <title> Manual of Photogramme-try,4th edition, </title> <journal> American Society of Photogram-metry (1980). </journal>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [12] [23]. Outdoors, control points could be markings on the ground <ref> [18] </ref> or buildings whose positions can be verified from maps [19]. <p> There is a problem that arises due to the interaction between the external and the internal parameters. The Manual of Photogrammetry <ref> [18] </ref> claims that "the strong coupling that exists between interior elements of orientation [principal point and focal length] and exterior elements can be expected to result in unacceptably large variances for these particular projective parameters when recovered on a frame-by-frame basis". <p> The problem manifests itself when one tries to use subsets of the camera parameters for other tasks. Taking multiple views of the calibration object from different camera locations can help solve this problem <ref> [18] </ref>. 1.2.2 Methods that use geometric properties There are a variety of methods that use geometric objects whose images have some characteristic that is invariant to the actual position of the object in space and can be used to calibrate some of the internal camera parameters. <p> The standard model for lens distortion <ref> [18] </ref> is a mapping from the distorted image coordinates, (x d ; y d ), that are observable, to the undistorted image plane coordinates, (x u ; y u ), which are not physically undistorted image coordinates (x u ; y u ) and the frame buffer coordinates (x; y) measurable. <p> y d = (x 2 d c yr ) 2 (3) It has been shown in [20] that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is equivalent to adding a term for decenter ing distortion as given in <ref> [18] </ref>. Finally, ~ P is converted to frame buffer coordinates: x = S y = y d + c y where (c x ; c y ) is the principal point in frame buffer coordinates and S is the aspect ratio or scale factor of the image.
Reference: [19] <author> Strat,T.M., </author> <title> "Recovering the Camera Parameters from a Transformation Matrix" in Proc. </title> <booktitle> DARPA Image Understanding Workshop, </booktitle> <pages> 264-271, </pages> <month> Oct. </month> <year> (1984) </year>
Reference-contexts: In some cases we do not need to find the camera parameters explicitly. In other words, the transformation is defined in terms of intermediate parameters which are combinations of the internal and external camera parameters. The intermediate parameters can often be found quite easily <ref> [19] </ref> [9] and can be used in this form for some tasks. Finding the internal parameters from the intermediate parameters explicitly is a difficult task [19]. Work on motion vision and pose estimation often use the perspective projection model and assume that the internal camera parameters are known [13]. <p> The intermediate parameters can often be found quite easily <ref> [19] </ref> [9] and can be used in this form for some tasks. Finding the internal parameters from the intermediate parameters explicitly is a difficult task [19]. Work on motion vision and pose estimation often use the perspective projection model and assume that the internal camera parameters are known [13]. They also assume that the parameters that can correct for lens distortion are known. <p> These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [12] [23]. Outdoors, control points could be markings on the ground [18] or buildings whose positions can be verified from maps <ref> [19] </ref>.
Reference: [20] <author> Stein, </author> <title> G.P., "Internal Camera Calibration using Rotation and Geometric Shapes" AITR-1426, </title> <type> Master's Thesis, </type> <institution> Massachussets Institute of Technology, Artificial Intelligence Laboratory (1993). </institution>
Reference-contexts: A camera centered coordinate system is used, therefore the external calibration is not important. 1.2 Brief review of other methods and re lated work We provide here a brief review of the various approaches to camera calibration. More extensive reviews of calibration methods appear in [12][18] <ref> [20] </ref>. Since the cameras used in machine vision are not designed to highly accurate specifications nor to be highly stable they might have to be calibrated frequently. <p> More details on these experiments can be found in <ref> [20] </ref>. 1.2.3 Methods that do not require known cal ibration points Since the traditional methods of camera calibration use known world coordinates they are not suitable for self calibration of mobile robots. <p> coordinates are: d (K 1 r d + K 2 r d + ) (2) d (K 1 r d + K 2 r d + ) where: r d = x d + y d = (x 2 d c yr ) 2 (3) It has been shown in <ref> [20] </ref> that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is equivalent to adding a term for decenter ing distortion as given in [18]. <p> The XY stage was mounted on a precision rotary stage with a vernier scale that can measure angles to an accuracy of 1 0 . (1 0 = 1 60 ). In practice we found that the repeatability of angular measurements was between 1:22 0 <ref> [20] </ref>. 4.2 How to obtain pure rotation ? As shown in section 2.3 it is very important that we obtain what is nearly pure rotation (i.e. the axis of rotation passes very close to the center of projection). <p> From this we conclude that, with care, we could position the center of projection less than 0:5 mm from the axis of rotation. For more details see <ref> [20] </ref>. 4.3 Features and feature detector The rotation method does not require any special calibration object but it does require that features can be detected reliably in the image. We wished to have features that could be located with sub-pixel accuracy rotation. <p> The image is viewed on the TV monitor (b). with the assumption that we could always reduce the accuracy of our measurements at a later stage. The details of the features and feature detector are not critical and are presented fully in <ref> [20] </ref>. In order to simplify the feature extraction process we used a black and white checkerboard pattern printed on a single sheet of 8 fi 11 inch paper as our calibration object. The features were defined as the corners of the black squares.
Reference: [21] <author> Sutherland, </author> <title> I.E., </title> <booktitle> "Three-Dimensional Data Input by Tablet" in Proceedings of the IEEE, </booktitle> <month> 62,453-458 </month> <year> (1974) </year>
Reference: [22] <author> Thomas, </author> <title> G.B., "Calculus and Analytic Geometry" 4th Edition, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> (1969) </year>
Reference: [23] <author> Weng, J et al. </author> <title> "Camera Calibration with Distortion Models and Accuracy Evaluation" IEEE Trans. </title> <journal> Pattern Anal. Machine Intell. </journal> <month> 14,965-980 </month> <year> (1992) </year>
Reference-contexts: These are sometimes called control points. In laboratories, control points can be obtained using a calibration object [12] <ref> [23] </ref>. Outdoors, control points could be markings on the ground [18] or buildings whose positions can be verified from maps [19]. <p> The main problem is the error in the focal length. In <ref> [23] </ref> simulation results show that errors of about 0:5 percent can occur with even small amounts of additive Gaussian noise. We can expect larger errors when dealing with real cameras. <p> We can expect larger errors when dealing with real cameras. Despite this problem the calibration can still be used to accurately measure the position of an object in the workspace from it's image position in a stereo image pair <ref> [23] </ref>. The problem manifests itself when one tries to use subsets of the camera parameters for other tasks.
References-found: 23

