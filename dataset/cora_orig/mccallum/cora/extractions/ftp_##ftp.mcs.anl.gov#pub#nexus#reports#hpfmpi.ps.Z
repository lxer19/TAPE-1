URL: ftp://ftp.mcs.anl.gov/pub/nexus/reports/hpfmpi.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/
Root-URL: http://www.mcs.anl.gov
Email: ffoster,kohrg@mcs.anl.gov  rakesh@cat.syr.edu  choudhar@cat.syr.edu  
Title: Double Standards: Bringing Task Parallelism to HPF via the Message Passing Interface  
Author: Ian Fostery David R. Kohr, Jr. Rakesh Krishnaiyerz Alok Choudharyx 
Address: Argonne, IL 60439 U.S.A.  Syracuse, NY 13244 U.S.A.  Syracuse, NY 13244 U.S.A.  
Affiliation: yMathematics and Computer Science Division Argonne National Laboratory  zDepartment of Computer and Information Science Syracuse University  xDepartment of Electrical and Computer Engineering Syracuse University  
Abstract: High Performance Fortran (HPF) does not allow efficient expression of mixed task/data-parallel computations or the coupling of separately compiled data-parallel modules. In this paper, we show how a coordination library implementing the Message Passing Interface (MPI) can be used to represent these common parallel program structures. This library allows data-parallel tasks to exchange distributed data structures using calls to simple communication functions. We present microbenchmark results that characterize the performance of this library and that quantify the impact of optimizations that allow reuse of communication schedules in common situations. In addition, results from two-dimensional FFT, convolution, and multiblock programs demonstrate that the HPF/MPI library can provide performance superior to that of pure HPF. We conclude that this synergistic combination of two parallel programming standards represents a useful approach to task parallelism in a data-parallel framework, increasing the range of problems addressable in HPF without requiring complex compiler technology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 13, 18, 22] </ref>. High Performance Fortran [14] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> An HPF compiler normally generates a single-program, multiple-data (SPMD) parallel program by applying the owner computes rule to partition the operations performed by the program; the processor that "owns" a value is responsible for updating its value <ref> [1, 18, 22] </ref>. The compiler also introduces communication operations when local computation requires remote data.
Reference: [2] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Language-based approaches. Advocates of explicit, language-based approaches propose new language constructs that allow programmers to specify the creation and coordination of tasks explicitly. The basic concept is that of a coordination language <ref> [2, 6] </ref>, except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel tasks.
Reference: [3] <author> G. Cheng, G. Fox, and K. Mills. </author> <title> Integrating multiple programming paradigms on Connection Machine CM5 in a dataflow-based software environment. </title> <type> Technical report, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [8, 11, 16], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 5, 15, 20] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, while language-based approaches also introduce the need to standardize new language features. <p> The basic concept is that of a coordination language [2, 6], except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel tasks. Language-based approaches have been proposed that use a graphical notation <ref> [3] </ref>, channels [5], remote procedure calls [15], and a simple pipeline notation [20] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [4] <author> A. N. Choudhary, Narahari, D. M. Nicol, and R. Simha. </author> <title> Optimal processor assignment for pipeline computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 439-445, </pages> <year> 1994. </year>
Reference-contexts: However, signal-processing systems must often process quickly a stream of arrays of relatively small size. (The array size corresponds to the sensor resolution and might be 256fi256 or less.) In these situations, 4 program (image courtesy of T. Canfield) 5 an alternative pipelined algorithm is often more efficient <ref> [4, 11] </ref>. The alternative algorithm partitions the FFT computation among the processors such that P/2 processors perform the read and the first set of 1-D FFTs, while the other P/2 perform the second set of 1-D FFTs and the write. <p> As expected, the HPF/MPI program is faster than the HPF code for larger numbers of processors and smaller problems. Convolution. Convolution is a standard technique used to extract feature information from images <ref> [4, 19] </ref>. It involves two 2-D FFTs, an elementwise multiplication, and an inverse 2-D FFT (Figure 9) and is applied to two streams of input images to generate a single output stream.
Reference: [5] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 293-300. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year> <month> 17 </month>
Reference-contexts: Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. Examples of applications that are not easily expressed using HPF alone <ref> [5, 11] </ref> include multidisciplinary applications where different modules represent distinct scientific disciplines, programs that interact with user interface devices, applications involving irregularly structured data such as multiblock codes, and image-processing applications in which pipeline structures can be used to increase performance. <p> These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [8, 11, 16], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 5, 15, 20] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, while language-based approaches also introduce the need to standardize new language features. <p> This distribution allows the rowfft routine to proceed without communication. However, the transposition A=transpose (A) involves all-to-all communication. 2.2 Task Parallelism Certain important program structures and application classes are not directly expressible in HPF <ref> [5, 11] </ref>. For example, both real-time monitoring and computational steering require that programmers connect a data-parallel simulation code to another sequential or parallel program that handles I/O. <p> Language-based approaches have been proposed that use a graphical notation [3], channels <ref> [5] </ref>, remote procedure calls [15], and a simple pipeline notation [20] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [6] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1990. </year>
Reference-contexts: Language-based approaches. Advocates of explicit, language-based approaches propose new language constructs that allow programmers to specify the creation and coordination of tasks explicitly. The basic concept is that of a coordination language <ref> [2, 6] </ref>, except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel tasks.
Reference: [7] <author> K. S. Gatlin and S. B. Baden. </author> <title> Brick: A benchmark for irregular block structured applications. </title> <type> Technical report, </type> <institution> University of California at San Diego, Department of Computer Science and Engineering, </institution> <year> 1996. </year>
Reference-contexts: Multiblock. Multiblock codes decompose a complex geometry into multiple simpler blocks [21]. A solver is run within each block, and boundary data is exchanged between blocks periodically. For our experiments, we use a program that applies a simple Poisson solver within each block and that supports only simple geometries <ref> [7] </ref>. Figure 11 shows the three-block geometry used in our experiments, and an intermediate solution computed on this geometry.
Reference: [8] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <year> 1992. </year>
Reference-contexts: Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [8, 11, 16] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 5, 15, 20]. Both approaches have shown promise in certain application areas, but each also has disadvantages.
Reference: [9] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: MPI standardizes an interaction model that has been widely used and is well understood within the high-performance computing community. It defines functions for both point-to-point and collective communication among tasks executing in separate address spaces; its definition permits efficient implementations on both shared and distributed-memory computers <ref> [9] </ref>. Our HPF/MPI library allows these same functions to be used to communicate and synchronize among HPF tasks. This integration of two parallel programming standards allows us to incorporate useful new functionality into HPF programming environments without requiring complex new directives or compiler technology. <p> Previous MPI implementations have supported bindings only for the sequential languages C and Fortran 77 <ref> [9] </ref>. However, there is no reason why MPI functions may not also be used for communication among data-parallel tasks. Our HPF binding for MPI makes this possible.
Reference: [10] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Processing with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: The potentially complex communication and synchronization operations required to transfer data among process groups are encapsulated within the coordination library implementations. To illustrate and explore this approach, we have defined and implemented a library that allows the use of a subset of the Message Passing Interface (MPI) <ref> [10] </ref> to coordinate HPF tasks. MPI standardizes an interaction model that has been widely used and is well understood within the high-performance computing community.
Reference: [11] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(2) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. Examples of applications that are not easily expressed using HPF alone <ref> [5, 11] </ref> include multidisciplinary applications where different modules represent distinct scientific disciplines, programs that interact with user interface devices, applications involving irregularly structured data such as multiblock codes, and image-processing applications in which pipeline structures can be used to increase performance. <p> Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [8, 11, 16] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 5, 15, 20]. Both approaches have shown promise in certain application areas, but each also has disadvantages. <p> This distribution allows the rowfft routine to proceed without communication. However, the transposition A=transpose (A) involves all-to-all communication. 2.2 Task Parallelism Certain important program structures and application classes are not directly expressible in HPF <ref> [5, 11] </ref>. For example, both real-time monitoring and computational steering require that programmers connect a data-parallel simulation code to another sequential or parallel program that handles I/O. <p> However, signal-processing systems must often process quickly a stream of arrays of relatively small size. (The array size corresponds to the sensor resolution and might be 256fi256 or less.) In these situations, 4 program (image courtesy of T. Canfield) 5 an alternative pipelined algorithm is often more efficient <ref> [4, 11] </ref>. The alternative algorithm partitions the FFT computation among the processors such that P/2 processors perform the read and the first set of 1-D FFTs, while the other P/2 perform the second set of 1-D FFTs and the write. <p> Compiler-based approaches. Advocates of implicit, compiler-based approaches seek to develop more sophisticated compilers capable of extracting task-parallel algorithms from data-parallel specifications. Frequently, they will use new directives to trigger the application of specific transformations. This general approach has been used to exploit pipeline <ref> [11] </ref> and functional parallelism [16], for example. Implicit, compiler-based approaches maintain a deterministic, sequential reading for programs. However, these approaches also tend to increase the complexity of the mapping from user program to executable code. This increased complexity can be a disadvantage for both programmers and compiler writers.
Reference: [12] <author> W. Hillis and G. Steele, Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: data parallelism by discussing data parallelism and HPF and then reviewing approaches to the extension of the data-parallel model. 2.1 Data Parallelism and HPF Data-parallel languages allow programmers to exploit the concurrency that derives from the application of the same operation to all or most elements of large data structures <ref> [12] </ref>. Data-parallel languages have significant advantages relative to the lower-level mechanisms that might otherwise be used to develop parallel programs. Programs are deterministic and have a sequential reading. This simplifies development and allows reuse of existing program development methodologies|and, with some modification, tools.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 13, 18, 22] </ref>. High Performance Fortran [14] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops.
Reference: [14] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) provides a portable, high-level notation for expressing data-parallel algorithms <ref> [14] </ref>. An HPF computation has a single-threaded control structure, global name space, and loosely synchronous parallel execution model. Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. <p> In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs [1, 13, 18, 22]. High Performance Fortran <ref> [14] </ref> is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> HPF extrinsic call. The communication operation is invoked as an HPF extrinsic call to a procedure in the HPF/MPI library. The procedure is invoked in "local" mode, meaning that a separate thread of control executes on each processor on which the invoking task is running <ref> [14] </ref>. 3. Descriptor exchange. Sending processors exchange distribution information with receiving processors. In general, each sending processor need communicate only with a subset of the receiving processors. 4. Communications schedule.
Reference: [15] <author> P. Mehrotra and M. Haines. </author> <title> An overview of the Opus language and runtime system. </title> <type> ICASE Report 94-39, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, Va., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [8, 11, 16], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 5, 15, 20] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, while language-based approaches also introduce the need to standardize new language features. <p> Language-based approaches have been proposed that use a graphical notation [3], channels [5], remote procedure calls <ref> [15] </ref>, and a simple pipeline notation [20] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [16] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A framework for exploiting data and functional parallelism on distributed memory multicomputers. </title> <type> Technical Report CRHC-94-10, </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois, Urbana, Ill., </institution> <year> 1994. </year>
Reference-contexts: Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [8, 11, 16] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 5, 15, 20]. Both approaches have shown promise in certain application areas, but each also has disadvantages. <p> Compiler-based approaches. Advocates of implicit, compiler-based approaches seek to develop more sophisticated compilers capable of extracting task-parallel algorithms from data-parallel specifications. Frequently, they will use new directives to trigger the application of specific transformations. This general approach has been used to exploit pipeline [11] and functional parallelism <ref> [16] </ref>, for example. Implicit, compiler-based approaches maintain a deterministic, sequential reading for programs. However, these approaches also tend to increase the complexity of the mapping from user program to executable code. This increased complexity can be a disadvantage for both programmers and compiler writers.
Reference: [17] <author> Shankar Ramaswamy and Prithviraj Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 342-349, </pages> <address> McLean, Va., </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The communication schedules required in Step 4 are computed with algorithms based on the FALLS (FAmiLy of Line Segments) representation of Ramaswamy and Banerjee <ref> [17] </ref>. These algorithms incorporate efficient and general techniques for computing the minimal sequence of communication operations required to perform a redistribution. Note that while the implementation strategy of Figure 4 is efficient for typical multicomputers, other strategies are possible and may perform better in some situations.
Reference: [18] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 13, 18, 22] </ref>. High Performance Fortran [14] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> An HPF compiler normally generates a single-program, multiple-data (SPMD) parallel program by applying the owner computes rule to partition the operations performed by the program; the processor that "owns" a value is responsible for updating its value <ref> [1, 18, 22] </ref>. The compiler also introduces communication operations when local computation requires remote data.
Reference: [19] <author> A. Rosenfeld and A. Kak. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: As expected, the HPF/MPI program is faster than the HPF code for larger numbers of processors and smaller problems. Convolution. Convolution is a standard technique used to extract feature information from images <ref> [4, 19] </ref>. It involves two 2-D FFTs, an elementwise multiplication, and an inverse 2-D FFT (Figure 9) and is applied to two streams of input images to generate a single output stream.
Reference: [20] <author> B. Seevers, M. Quinn, and P. Hatcher. </author> <title> A parallel programming environment supporting data-parallel modules. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(5), </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [8, 11, 16], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 5, 15, 20] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, while language-based approaches also introduce the need to standardize new language features. <p> Language-based approaches have been proposed that use a graphical notation [3], channels [5], remote procedure calls [15], and a simple pipeline notation <ref> [20] </ref> to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [21] <author> V. N. Vatsa, M. D. Sanetrik, and E. B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based muliblock flow solver; AIAA-93-0677. </title> <booktitle> In Proc. 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: We do not consider this option here.) Figure 10 shows our results. Once again, we see that the HPF/MPI version is often significantly faster than the pure HPF version. Multiblock. Multiblock codes decompose a complex geometry into multiple simpler blocks <ref> [21] </ref>. A solver is run within each block, and boundary data is exchanged between blocks periodically. For our experiments, we use a program that applies a simple Poisson solver within each block and that supports only simple geometries [7].
Reference: [22] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD paral-lelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 18 </month>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 13, 18, 22] </ref>. High Performance Fortran [14] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> An HPF compiler normally generates a single-program, multiple-data (SPMD) parallel program by applying the owner computes rule to partition the operations performed by the program; the processor that "owns" a value is responsible for updating its value <ref> [1, 18, 22] </ref>. The compiler also introduces communication operations when local computation requires remote data.
References-found: 22

