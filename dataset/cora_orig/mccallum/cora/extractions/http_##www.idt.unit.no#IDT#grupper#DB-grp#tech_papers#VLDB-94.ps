URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/VLDB-94.ps
Refering-URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/tech_papers.html
Root-URL: 
Email: frhj,rogerg@idt.unit.no  
Title: Modelling and Querying Video Data  
Author: Rune Hjelsvold and Roger Midtstraum 
Affiliation: Department of Computer Systems and Telematics Norwegian Institute of Technology  
Abstract: As video data is penetrating many information systems the need for database support for video data evolves. In this paper we present a generic data model that captures the structure of a video document and that provides a means for indexing a video stream. We also discuss query language features that can take advantage of the proposed model. We have identified basic operators that should be implemented in the query language to support content based queries. The paper also analyses how these operators can be used to provide video data queries. The model has been used as a basis for a television news archive prototype and some experimental results are presented.
Abstract-found: 1
Intro-found: 1
Reference: [A + 93] <editor> Allen et al. VCTV: </editor> <title> A Video-On-Demand Market Test. </title> <journal> AT&T Technical Journal, </journal> <month> Jan-uary/February </month> <year> 1993. </year>
Reference-contexts: Most current digital video applications do not really take advantage of a digital video data stream, they are more like digitised versions of traditional video cassette recorders - VCRs. One example of this is the Video-On-Demand service - VOD, <ref> [A + 93] </ref> that gives the user on-line access to a digital library of movies.
Reference: [BGT92] <author> C. Breiteneder, S. Gibbs, and D. Tsichritzis. </author> <title> Modelling of Audio/Video Data. </title> <booktitle> In Proceedings of the 11th International Conference on the Entity-Relationship Approach, </booktitle> <address> Karl-sruhe, Germany, </address> <month> October 7-9 </month> <year> 1992. </year>
Reference-contexts: The focus for our work has been the support of video data and we have not paid special attention to problems arising when multiple media are integrated, e.g. audio/video syn-chronisation and multiple audio tracks. Such problems are covered in detail in, for instance, <ref> [BGT92] </ref>. Previous work on data models for video information can be found in [RD89], [DSP91], [KHT91], [Mer93], [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation".
Reference: [DSP91] <author> G. Davenport, T.A. Smith, and N. Pincever. </author> <title> Cinematic Primitives for Multimedia. </title> <journal> IEEE Computer Graphics & Applications, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in [RD89], <ref> [DSP91] </ref>, [KHT91], [Mer93], [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. Smith and other researchers at MIT have proposed an alternative approach, named "stratification", where they propose to segment the contextual information rather than segmenting contiguous frames. The stratification approach is discussed in <ref> [DSP91] </ref>, [SP91] and [Smi92]. A similar approach has been chosen for the video object database OVID [OT93]. Video objects in OVID correspond to sets of video frame sequences. Each video object has a set of attributes and a unique identifier. <p> The semantics of the subclasses are best explained bottom-up starting with the Shot which can be consid ered as the basic structural unit. As defined in <ref> [DSP91] </ref> a Shot consists of one or more frames generated and recorded contiguously, representing a continuous action in time and space. Shots which are related in time and space are assembled in a Scene and a number of scenes which together give a meaning are put together in a Sequence. <p> The user can activate a digital video player from the browser/query interface. 6 Conclusion and Further Work Previous works on video data models have either focussed on video document structures - e.g. <ref> [DSP91] </ref> or thematic indexes - e.g. [SP91] but not both. In this paper we propose a generic data model which does provide a framework for modelling both the structure and the contents of a video document.
Reference: [EN94] <author> R. Elmasri and S.B. Navathe. </author> <title> Fundamentals of Database Systems. </title> <publisher> The Ben-jamin/Cummings Publishing Company, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: I our generic model a segmentation approach is used to define the video document structure which can provide well defined levels of abstraction. The generic data model is given in Figure 1 using the enhanced-ER notation from <ref> [EN94] </ref>. The intention has been to make the following possible within the same framework: 1. Structuring of video material, 2. free annotations of video material and 3. sharing and reuse of video data, Video data is stored in a video database as congigu-ous groups of frames called StoredVideoSegments.
Reference: [Hje94] <author> R. Hjelsvold. </author> <title> Video Information Contents and Architecture. </title> <booktitle> In Proceedings of the 4th International Conference on Extending Database Technology, </booktitle> <address> Cambridge, UK, </address> <month> March 28-31 </month> <year> 1994. </year>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in [RD89], [DSP91], [KHT91], [Mer93], [OT93] and <ref> [Hje94] </ref>. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> Section 5 presents the results from a project where librarians from Norwegian Broadcasting Corporation (NRK) participated. Section 6 concludes the work and summarises areas for further work. 2 A Generic Video Data Model A database system managing video information should provide database support for a diverse range of applications <ref> [Hje94] </ref>. Ideally, the same database should be available for several different purposes such as video production, scientific analysis of video material for instance a study of the use of narrative techniques in early movies by Hitchcock, and video selection by end-users. <p> As discussed in [Smi92] and <ref> [Hje94] </ref> it should be possible to make detailed descriptions of the content of the video material which are not necessarily directly linked to structural components but more often to arbitrary frame sequences. <p> A set of general annotation types, however, (see the subclasses shown) should be provided by the generic model while allowing the applications to augment the descriptive power with domain specific annotation types. Different types of annotations are more thoroughly discussed in <ref> [Hje94] </ref> and [Mer93]. 2.3 Sharing and Reuse of Video Material As recognised in [MD89] the same basic video material may be used in several different video documents.
Reference: [KHT91] <author> W. Kameyama, T. Hanamura, and H. Tom-inaga. </author> <title> A Proposal of Multimedia Document Architecture and Video Document Architecture. </title> <booktitle> In Proceedings of ICC '91 The International Conference on Communication Conference Record, </booktitle> <address> Denver, USA, </address> <year> 1991. </year>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in [RD89], [DSP91], <ref> [KHT91] </ref>, [Mer93], [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> This is because a single frame spans a very short interval of time and because there are so many individual frames even in a quite short video document (the Eu-ropean video standard, PAL, results for instance in 25 frames per second). [RD89] and <ref> [KHT91] </ref> strongly emphasize the need for some sort of structuring method. From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items makes it easier for the user to make references to video information and easier to comprehend its contents.
Reference: [LG93] <author> T.D.C. Little and A. Ghafoor. </author> <title> Interval-Based Conceptual Models for Time-Dependent Multimedia Data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: Figure 2 shows a video stream where two frame sequences are non-overlapping (a) and where two frame sequences overlap (b). A frame sequence is essentially a temporal interval with a temporal relation between the frames. Temporal interval operators are therefore applicable to frame sequences. In <ref> [LG93] </ref> the following temporal interval operators are discussed: * A Equals B: Returns true if the two sequences A and B are identical. * A Before B: Returns true if A happens before B. (The complementary operator is Af ter.) * A M eets B: Returns true if B starts with
Reference: [LL94] <author> A. Laursen and B. Linder. </author> <title> Delivering Real-time Audio/Video with the Oracle Media Server. </title> <booktitle> In Proceedings of the EOUG Oracle User Forum 94, </booktitle> <address> Maastricth, Netherlands, </address> <month> April 17-20 </month> <year> 1994. </year>
Reference-contexts: Both users and vendors of DBMS's are are beginning to recognise the needs for video data support and the first commercial products are already available to the market - e.g. the Oracle Media Server <ref> [LL94] </ref>. Our research is aimed at developing functionality for video data support that can be included in DBMS's. This paper is concerned with two aspects of video data support: 1) video data modelling and 2) query language extensions.
Reference: [MD89] <author> W.E. Mackay and G. Davenport. </author> <title> Virtual Video Editing In Interactive Multimedia Applications. </title> <journal> Communications of the ACM, </journal> <volume> 32(7), </volume> <year> 1989. </year>
Reference-contexts: Different types of annotations are more thoroughly discussed in [Hje94] and [Mer93]. 2.3 Sharing and Reuse of Video Material As recognised in <ref> [MD89] </ref> the same basic video material may be used in several different video documents.
Reference: [Mer93] <author> P. Merok. </author> <title> Data Models for Digital Film and Video Archives. </title> <type> Master's thesis, </type> <institution> Norwegian Institute of Technology, </institution> <year> 1993. </year> <note> In Norwegian. </note>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in [RD89], [DSP91], [KHT91], <ref> [Mer93] </ref>, [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> From experiments with a television news archive <ref> [Mer93] </ref> we have learned that abstractions such as scenes and news items makes it easier for the user to make references to video information and easier to comprehend its contents. Other researchers have shown that video information browsing is difficult (see [Ste91]). <p> A set of general annotation types, however, (see the subclasses shown) should be provided by the generic model while allowing the applications to augment the descriptive power with domain specific annotation types. Different types of annotations are more thoroughly discussed in [Hje94] and <ref> [Mer93] </ref>. 2.3 Sharing and Reuse of Video Material As recognised in [MD89] the same basic video material may be used in several different video documents.
Reference: [Mon81] <author> J. Monaco. </author> <title> How to Read a Film. The Art, Technology, Language, History and Theory of Film and Media. </title> <publisher> Oxford University Press, </publisher> <year> 1981. </year>
Reference-contexts: Other researchers have shown that video information browsing is difficult (see [Ste91]). Our experiments show that structural abstractions give valuable support to video browsers. The structure part of our model is inspired from film theory <ref> [Mon81] </ref> and work based on segmentation of video material. It is built around the concept of a StructuralComponent which has an associated FrameSequence of video material. The concept of StructuralComponent is specialised into the CompoundUnit, Sequence, Scene and Shot subclasses and a hierarchical relationship is defined between the different subclasses.
Reference: [OT93] <author> E. Oomoto and K. Tanaka. OVID: </author> <title> Design and Implementation of a Video-Object Database System. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in [RD89], [DSP91], [KHT91], [Mer93], <ref> [OT93] </ref> and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> Smith and other researchers at MIT have proposed an alternative approach, named "stratification", where they propose to segment the contextual information rather than segmenting contiguous frames. The stratification approach is discussed in [DSP91], [SP91] and [Smi92]. A similar approach has been chosen for the video object database OVID <ref> [OT93] </ref>. Video objects in OVID correspond to sets of video frame sequences. Each video object has a set of attributes and a unique identifier. OVID's video data model does not explicitly support modelling of the video document structure. <p> The main weakness of the proposed model is its complexity. Thematic indexes and structural components may implicit relate to each other because different frame sequences may overlap and because frame sequences may be reused. Significant processing is required to explicitly identify these relations. OVID's video query language VideoSQL <ref> [OT93] </ref> allows the user to specify video object properties but VideoSQL does not address the temporal relations between video objects. In our work we have studied video query features that allow the user to define temporal relationships between frame sequences.
Reference: [RD89] <author> B. Rubin and G. Davenport. </author> <title> Structured Content Modeling for Cinematic Information. </title> <journal> SIGCIHI Bulletin, </journal> <volume> 21(2), </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: Such problems are covered in detail in, for instance, [BGT92]. Previous work on data models for video information can be found in <ref> [RD89] </ref>, [DSP91], [KHT91], [Mer93], [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith [SP91] Page 1 mainly because of its inflexibility. <p> This is because a single frame spans a very short interval of time and because there are so many individual frames even in a quite short video document (the Eu-ropean video standard, PAL, results for instance in 25 frames per second). <ref> [RD89] </ref> and [KHT91] strongly emphasize the need for some sort of structuring method. From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items makes it easier for the user to make references to video information and easier to comprehend its contents.
Reference: [Smi92] <author> T.G.A. Smith. </author> <title> If You Could See What I Mean... Descriptions of Video in an Anthropologist's Notebook. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference-contexts: Smith and other researchers at MIT have proposed an alternative approach, named "stratification", where they propose to segment the contextual information rather than segmenting contiguous frames. The stratification approach is discussed in [DSP91], [SP91] and <ref> [Smi92] </ref>. A similar approach has been chosen for the video object database OVID [OT93]. Video objects in OVID correspond to sets of video frame sequences. Each video object has a set of attributes and a unique identifier. <p> In Section 3.1 we show how the model can be tailored to the television news domain. 2.2 Thematic Indexing The structure of a video document captures some aspects of the video material but is not suited as a representation of every characteristic of the material. As discussed in <ref> [Smi92] </ref> and [Hje94] it should be possible to make detailed descriptions of the content of the video material which are not necessarily directly linked to structural components but more often to arbitrary frame sequences.
Reference: [SP91] <author> T.G.A. Smith and N.C. Pincever. </author> <title> Parsing Movies in Context. </title> <booktitle> In Proceedings of the 1991 Summer USENIX Conference, </booktitle> <address> Nashville, USA, </address> <year> 1991. </year>
Reference-contexts: Previous work on data models for video information can be found in [RD89], [DSP91], [KHT91], [Mer93], [OT93] and [Hje94]. An early proposal was to divide a video document into segments and describe every segment independently, so called "segmentation". This approach has been strongly criticised by Smith <ref> [SP91] </ref> Page 1 mainly because of its inflexibility. Smith and other researchers at MIT have proposed an alternative approach, named "stratification", where they propose to segment the contextual information rather than segmenting contiguous frames. The stratification approach is discussed in [DSP91], [SP91] and [Smi92]. <p> This approach has been strongly criticised by Smith <ref> [SP91] </ref> Page 1 mainly because of its inflexibility. Smith and other researchers at MIT have proposed an alternative approach, named "stratification", where they propose to segment the contextual information rather than segmenting contiguous frames. The stratification approach is discussed in [DSP91], [SP91] and [Smi92]. A similar approach has been chosen for the video object database OVID [OT93]. Video objects in OVID correspond to sets of video frame sequences. Each video object has a set of attributes and a unique identifier. <p> Some applications may expand the common data model with specific concepts while still using the core concepts while less demanding applications may use only a subset. To describe - e.g. index, contents which are not intimately related to structural components we have adopted ideas from the stratification approach in <ref> [SP91] </ref>. The stratification approach, whilst strong on free annotations, has ignored the need for structure as a tool to navigate and comprehend large volumes of video data. I our generic model a segmentation approach is used to define the video document structure which can provide well defined levels of abstraction. <p> The user can activate a digital video player from the browser/query interface. 6 Conclusion and Further Work Previous works on video data models have either focussed on video document structures - e.g. [DSP91] or thematic indexes - e.g. <ref> [SP91] </ref> but not both. In this paper we propose a generic data model which does provide a framework for modelling both the structure and the contents of a video document. The model is generic and it can be tailored to different application domains to adopt domain specific terminology or attributes.
Reference: [Ste91] <author> S.M. Stevens. </author> <title> Next Generation Network and Operating System Requirements for Continuous Time Media. </title> <booktitle> In Proceedings of the Second International Workshop for Network and Operating System Support for Digital Audio and Video, </booktitle> <address> Heidelberg, Germany, </address> <month> November </month> <year> 1991. </year> <pages> Page 9 </pages>
Reference-contexts: From experiments with a television news archive [Mer93] we have learned that abstractions such as scenes and news items makes it easier for the user to make references to video information and easier to comprehend its contents. Other researchers have shown that video information browsing is difficult (see <ref> [Ste91] </ref>). Our experiments show that structural abstractions give valuable support to video browsers. The structure part of our model is inspired from film theory [Mon81] and work based on segmentation of video material. It is built around the concept of a StructuralComponent which has an associated FrameSequence of video material.
References-found: 16

