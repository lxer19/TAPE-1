URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/biblio/mw95.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/notes-08.html
Root-URL: http://www.cs.dartmouth.edu
Title: Fast Instruction Cache Analysis via Static Cache Simulation  
Author: Frank Mueller and David B. Whalley 
Address: FL 32306-4019  
Affiliation: Dept. of Computer Science, Florida State University, Tallahassee,  
Abstract: This paper introduces a new method for instruction cache analysis that outperforms conventional trace-driven methods. The new method, static cache simulation, analyzes a program for a given cache configuration and determines prior to execution time if an instruction reference will always result in a cache hit or miss. At run time, counters are incremented to provide the execution frequency of portions of code. In addition, the cache behavior is simulated for references that could not be predicted statically. The dynamic simulation employs a novel view of the cache by updating local state information associated with code portions. The total number of cache hits and misses can be inferred from the frequency counters at program exit. Measurements taken from a variety of programs show that this new method speeds up cache analysis over conventional trace-driven methods by almost an order of a magnitude. Thus, cache analysis with static cache simulation makes it possible to analyze the instruction cache behavior of longer and more realistic program executions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: During the compilation the control flow of the functions of a program is partitioned into unique paths (UPs). Informally, a UP is a set of basic blocks <ref> [1] </ref> (vertices) connected by control-flow transitions (edges) that contain at least one unique transition, i.e. a transition that does not occur in any other UP. In this study, UPs were restricted to not cross loop boundaries, function boundaries, or calls. <p> The correctness of the algorithm for data-flow analysis is discussed in <ref> [1] </ref>. The calculation can be performed for an arbitrary control-flow graph, even if it is irreducible. In addition, the order of processing basic blocks is irrelevant for the correctness of the algorithm.
Reference: [2] <author> R. Arnold, F. Mueller, D. B. Whalley, and M. Har-mon. </author> <title> Bounding worst-case instruction cache performance. </title> <booktitle> In IEEE Symposium on Real-Time Systems, </booktitle> <pages> pages 172-181, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Finally, data cache behavior could be analyzed statically as well under certain restrictive conditions, such as absence of heap allocation and pointers. There are several other applications of static cache simulation. For example, the worst-case execution time of real-time programs can be predicted more precisely for architectures with caches <ref> [2] </ref>. Other applications include detailed profiling and tracking of execution time for a real-time debugger [13]. 9 Conclusion A new method to evaluate instruction cache performance was designed and implemented. The cache performance of programs for various cache configurations can be obtained without recompiling the analyzed program.
Reference: [3] <author> T. Ball and J. R. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The postprocessing pass, which generates the complete trace from the subset of addresses stored, was much slower (about 3,000 addresses/sec). No information was given on the overhead required to analyze the cache performance. Ball and Larus <ref> [3, 10] </ref> also reduced the overhead of the trace generation by storing a portion of the trace from which the complete trace can be generated. They optimized the placement of the instrumentation code to produce the reduced trace with respect to a weighting of the control-flow graph.
Reference: [4] <author> M. E. Benitez and J. W. Davidson. </author> <title> A portable global optimizer and linker. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 329-338, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Cache measurements were obtained for user programs, benchmarks, and UNIX utilities. The measurements were produced by modifying the back-end of the optimizing compiler VPO (Very Portable Optimizer) <ref> [4] </ref> and by performing static cache simulation. The simulation was performed for the Sun SPARC instruction set, a RISC architecture with a uniform instruction size of one word (four bytes). The parameters for cache simulation included direct-mapped caches with sizes of 64B to 8kB.
Reference: [5] <author> A. Borg, R. E. Kessler, and D. W. Wall. </author> <title> Generation and analysis of very long address traces. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 270-279, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A technique called inline tracing can be used to generate the trace of addresses with much less overhead than trapping or simulation. Measurement instructions are inserted in the program to record the addresses that are referenced during the execution. Borg, Kessler, and Wall <ref> [5] </ref> modified programs at link time to write addresses to a trace buffer, and these addresses were analyzed by a separate higher priority process.
Reference: [6] <author> S. J. Eggers, D. R. Keppel, E. J. Koldinge, and H. M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-47, </pages> <year> 1990. </year>
Reference-contexts: Overhead rates of 8x to 12x normal execution time were reported for the trace generation. Analysis of the trace was stated to require at least 10x the overhead of the generation of the trace (or about 100x slower than normal execution time). Eggers et. al. <ref> [6] </ref> also used the technique of inline tracing to generate a trace of addresses in a trace buffer, which was copied to disk by a separate process. They used several strategies for minimizing the overhead of generating the trace.
Reference: [7] <author> J. Hennessy and D. Patterson. </author> <booktitle> Computer Architecture: </booktitle>
Reference-contexts: This is certainly true for code which is physically locked into memory. It also holds for virtual memory mapping, if and only if the page size is an integer multiple of the instruction cache size, which is typical for many systems <ref> [7] </ref>. In this case, the relocation of a virtual page would not affect the mapping of program lines into cache lines.
References-found: 7

