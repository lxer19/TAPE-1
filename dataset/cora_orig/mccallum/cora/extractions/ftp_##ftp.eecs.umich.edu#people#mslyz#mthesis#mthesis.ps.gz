URL: ftp://ftp.eecs.umich.edu/people/mslyz/mthesis/mthesis.ps.gz
Refering-URL: http://www.eecs.umich.edu/~mslyz/
Root-URL: http://www.eecs.umich.edu
Title: THE COOPER UNION  IMAGE COMPRESSION USING A ZIV-LEMPEL TYPE CODER  
Author: ALBERT NERKEN By Marko Slyz 
Degree: Thesis Advisor: Dr. Fred L. Fontaine A thesis submitted in partial fulfillment of the requirements for the degree of Master of Engineering  
Note: THE COOPER UNION FOR THE ADVANCEMENT OF SCIENCE AND ART  
Date: May 28, 1991  
Affiliation: SCHOOL OF ENGINEERING  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Timothy C. Bell, John G. Cleary, Ian H. Witten, </author> <title> Text Compression, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1990 </year>
Reference-contexts: The closer the generated data is to the kind of data for which the algorithm was designed, the better that algorithm, since otherwise too many compression codes are being wasted on untypical input. Trying this with good text compressors produces something like the more obscure passages in Finnegans Wake <ref> [1] </ref>. This is far better than the decompressors typically used for images, which give random-looking noise [12]. 3 2. Image Compression Using Paths This thesis uses a one-dimensional data compressor for lossless image coding, although some of the methods and algorithms examined may also be useful for lossy coding. <p> This step is called parsing and turns out to be relatively unimportant. An independent and more significant issue is the choice of strings that go into the dictionary. Following Bell, Cleary and Witten <ref> [1] </ref> there are three types of coding schemes: A static scheme always use the same dictionary, known by both the coder and decoder. A semiadaptive scheme make two passes through the input. On the first pass it determines the dictionary, and on the second, it codes according to that dictionary. <p> A better way of dividing this would be q, uaauqua. This only needs 9 bits because it uses the dictionary more efficiently. Algorithms for generating optimum parsings are described in <ref> [1] </ref>. One problem with optimum parsing is that potentially all of the succeeding input might have to be examined to make any one parsing decision. An example, from [1], illustrates this. <p> This only needs 9 bits because it uses the dictionary more efficiently. Algorithms for generating optimum parsings are described in <ref> [1] </ref>. One problem with optimum parsing is that potentially all of the succeeding input might have to be examined to make any one parsing decision. An example, from [1], illustrates this. Say that the dictionary D2, given in Figure 3.2 is used, string code a 000 ab 10 bb 0100000 14 and that the input is (ba) k ba, i.e. baba....bab. <p> Another issue is that parsing is done with respect to some dictionary and adaptive coders are always changing their dictionaries. Since optimum parsers must look ahead they have to know the dictionary for all times in the future. Some adaptive coders, such as LZ77 [13] <ref> [1] </ref>, have simple dictionaries so this is not problem. But many coders' dictionaries depend on the parsing itself. Thus, the dictionary needed to determine the parsing cannot be known until after the parsing. <p> At first this would seem like a way to get around the dictionary needed for parsing but parsing cannot be done without dictionary situation for adaptive coders described above. In practice, however, the adaptive coders that meet these conditions are usually outperformed by those that do not <ref> [1] </ref>. <p> Once coding starts, both coders exchange each string that the parser returns for its corresponding code. As an example consider a dictionary with all the strings of length N as entries. The Shannon-Fano and Huffman algorithms <ref> [1] </ref> [9] can be thought of as ways of choosing codes based on the entries' probabilities. <p> Arithmetic compressors could also design codes and they do not send a code table of their own; they are not used in this way since any dictionary coder can be simulated by some arithmetic coder, and it would be easier to use an arithmetic coder by itself <ref> [1] </ref>. Also, simpler codes work well . Static and semiadaptive techniques do not provide as much compression as adaptive techniques because their dictionaries do not reflect the characteristics of the input as well. Their advantage is that having a constant dictionary allows decoding to begin anywhere in the compressed file [1]. <p> <ref> [1] </ref>. Also, simpler codes work well . Static and semiadaptive techniques do not provide as much compression as adaptive techniques because their dictionaries do not reflect the characteristics of the input as well. Their advantage is that having a constant dictionary allows decoding to begin anywhere in the compressed file [1]. This can be useful when that file is a large collection of small items that are accessed at random. <p> The middle column in Figure 3.4 is itself a compressed form of the LZW dictionary since the actual strings could have been stored instead of their (n,c) 24 representation. For example (2,u) = qu. This is called front compression <ref> [1] </ref> and is useful whenever some strings in a numbered list have prefixes in that list. <p> The first solution is preferable because it gives a little more compression. 3.3.2 The Lempel-Ziv-Fiala-Greene Compressor In 1989, Edward Fiala and Daniel Greene published some variants of the original Ziv-Lempel [4] algorithms. The most successful one, algorithm number C2, is referred to as LZFG <ref> [1] </ref>. LZFG is based on LZ77's idea of encoding pointers to the preceding text but, like LZ78, acknowledges the inefficiencies in being able to point to any previous string. <p> Character 8 is an A, which has not yet occurred, so a new leaf is entered for it. Since characters 9 to 12 are the same as 8 to 11, they can be coded using this new leaf. The coder outputs: <ref> [LeafCopy 0, 1] </ref>[Lit A] 35 [LeafCopy 2, 0] root C D pos 5 1 4 len 5 len pos 2 6 len pos 9 A (AA) 3 len A (AAA...) pos 5 1 len pos 8 D (D...) C (DDA...) The parsing of the last two characters, DD, ends on a
Reference: [2] <author> J.L. Bentley, D.D. Sleator, R.E. Tarjan, V.K. Wei, </author> <title> A locally adaptive data compression scheme, </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> Vol. 29, No. 4, </volume> <pages> pp. 320-330, </pages> <month> April, </month> <year> 1986 </year>
Reference-contexts: Since the next character on the leaf arc is another D, while the next character in the input is a C, the parser can only go one character down the arc. The parser now knows that the Literal is over and so can output the first two characters: <ref> [LeafCopy 0, 2] </ref>[Lit C D] Recall that Literals start out as a LeafCopy with length of 0. The coder must wait until after the Literal has ended before it codes it. <p> One must use more complicated data structures to have this flexibility <ref> [2] </ref>. Decoding takes less time than coding because no parsing has to be done. In order for the LRU lists to remain synchronized, everything has to occur in the same order in the decoder as it did in the coder.
Reference: [3] <author> Lawrence Chisvin and R. James Duckworth, </author> <title> Content-Addressable and Associative Memory, </title> <journal> IEEE Computer, </journal> <volume> Vol. 22, No. 7, </volume> <pages> pp. 51-64 </pages>
Reference-contexts: For implementaions in the C language, an extra benefit is that, if 0 can occur in the input, one would not be able to use the usual null terminated strings. All of these implementations of tries are simulating a content-addressable memory (CAM). <ref> [3] </ref> Figure 3.4 without its first column is a diagram of one (n,c) is the tag field and Code is the data field. Simply put, every cell in a CAM has a comparator. <p> The previous code was again a Literal of less than maximum length so <ref> [LeafCopy 1, 3] </ref> is output. A new node is entered after the last character that matched, and a new leaf is added at that node for the new phrase. Character 8 is an A, which has not yet occurred, so a new leaf is entered for it.
Reference: [4] <author> Edward R. Fiala and Daniel H. </author> <title> Greene Data Compression with Finite Windows, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, No. 4, </volume> <pages> pp. 490 - 505, </pages> <month> April, </month> <year> 1989 </year>
Reference-contexts: Another solution would be to make the compressor enter the new string after it outputs the next code. The first solution is preferable because it gives a little more compression. 3.3.2 The Lempel-Ziv-Fiala-Greene Compressor In 1989, Edward Fiala and Daniel Greene published some variants of the original Ziv-Lempel <ref> [4] </ref> algorithms. The most successful one, algorithm number C2, is referred to as LZFG [1]. LZFG is based on LZ77's idea of encoding pointers to the preceding text but, like LZ78, acknowledges the inefficiencies in being able to point to any previous string. <p> A bit can be saved when s = n by not coding the unary code's terminating zero. As an example a (0, 1, 5) code is given in Figure 3.8. A procedure that outputs x with a (start, step, stop) code is <ref> [4] </ref>: while (x &gt;= 2^start) - PutBits (1,1); x = x - 2^start; start = start + step; - if (start &lt; stop) PutBits (x, start + 1); /* 0 followed by a field start bits long */ else PutBits (x, start);/* start == stop, save a bit */ Startstep-stop codes <p> The problem with this implementation is that the root will ordinarily have many more children than any other node, and so its hash table will be very full. This is solved by using, as recommended by <ref> [4] </ref>, a permanent array of depth one nodes with all of the single characters. The LRU leaf list is implemented with a queue that has pointers to all of the leaves. The number of any particular leaf is its distance from the head of the queue.
Reference: [5] <editor> John J. Grefenstette, ed., </editor> <booktitle> Genetic Algorithms and Their Applications: Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <address> Hillsdale, N.J.: </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1987 </year>
Reference-contexts: But an exact closed-form solution or algorithm to obtain the solution is not known, the solution may not be unique, and a suboptimal solution may be sufficient. Therefore we employ a genetic algorithm <ref> [5] </ref> [6] [10]. Such algorithms are useful when one does not know how to obtain a solution but can distinguish between good solutions and bad ones.
Reference: [6] <author> John H. Holland, </author> <booktitle> Adaptation in Natural and Artificial Systems, </booktitle> <address> Ann Arbor: </address> <publisher> The University of Michigan Press, </publisher> <year> 1975 </year>
Reference-contexts: But an exact closed-form solution or algorithm to obtain the solution is not known, the solution may not be unique, and a suboptimal solution may be sufficient. Therefore we employ a genetic algorithm [5] <ref> [6] </ref> [10]. Such algorithms are useful when one does not know how to obtain a solution but can distinguish between good solutions and bad ones.
Reference: [7] <author> Taejeong Kim and David Neuhoff, </author> <title> Delta Codes for Line Drawings, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 34, No.3, </volume> <pages> pp. 400-416, </pages> <month> May </month> <year> 1988 </year>
Reference-contexts: For this to work, the contours defining each region must be coded efficiently or this extra overhead will offset any compression gains. Fortunately much work has been done in coding line drawings (chain codes as described in <ref> [7] </ref> for example) and the contours may not have to be particularly accurate. 53
Reference: [8] <author> William F. Schreiber, </author> <title> Fundamentals of Electronic Imaging Systems, </title> <publisher> Berlin: Springer-Verlag, </publisher> <year> 1986 </year>
Reference-contexts: Lossy coding is really data reduction but it is frequently valuable for data that was originally analog. This is partly because, as <ref> [8] </ref> mentions, going from analog to digital is itself a lossy process which requires an understanding of human perception. But we are far from understanding perception and therefore there is room for opinion in digitization. <p> Ideally image coding should be done with intrinsically two-dimensional compressors but, with a few exceptions which do not work well <ref> [8] </ref>, most true two-dimensional compressors are lossy. One dimensional compressors are currently much better understood and more effective. To make a one-dimensional compressor work with an image, the images pixels must be read off in some pattern, which will be referred to as a path. <p> This procedure was repeated twice more for a total of three pictures. The compression ratio improved as the pictures got less complicated. This was expected because the simpler pictures have less entropy. A good discussion of the relationship between entropy, picture complexity and sampling density is found in <ref> [8] </ref>. The relative compression results 51 are reported in Figure 5.5. It is found that the path in Figure 4.1 degrades compression more as the pictures become simpler.
Reference: [9] <author> Robert Sedgewick, </author> <title> Algorithms, 2nd ed., </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1988 </year>
Reference-contexts: A more likely reason for compression to fail is that the particular compressor cannot recognize or take advantage of the redundancy in a file. Consider, for example, a large file of pseudorandom numbers <ref> [9] </ref>. Such numbers are generated by a well-defined, deterministic algorithm, and the entire file is determined by the specification of the algorithm and a single number, called the seed; nevertheless, the numbers appear to be statistically random. <p> Once coding starts, both coders exchange each string that the parser returns for its corresponding code. As an example consider a dictionary with all the strings of length N as entries. The Shannon-Fano and Huffman algorithms [1] <ref> [9] </ref> can be thought of as ways of choosing codes based on the entries' probabilities. <p> That is, adaptive compressors exhibit an overhead associated with startup or latency. The property of random access also allows finding a string in the compressed data by searching for its code. This is like having the hashing already done in a crude version of the Rabin-Karp algorithm <ref> [9] </ref>. <p> This parsing is simple if the dictionary is held in a radix trie data structure. The word trie comes from retrieval but is pronounced try or try-ee to avoid confusion with tree <ref> [9] </ref>. A tree is a trie when the branch taken out of any node depends only on the next character in the search key. <p> Also many pointers will be seldom used; in the example it is unnecessary to be able to point to "he" since it will only occur as part of "the". The data structure used to store LZFG's dictionary is a simplified version of the PATRICIA trie <ref> [9] </ref>. These differ from ordinary tries primarily in that each arc can represent more than one character. Nodes are inserted only at the points where strings diverge. Figure 3.5a and b illustrate this. At each time instant, every location in an LZFG trie is designated with a unique code. <p> Nodes are somewhat more complicated to delete because they have children. The solution chosen here is to reattach the deleted node's newest subtree to the node's parent and to delete the other subtrees. An alternative which might give slightly better compression is to do a "lazy deletion" <ref> [9] </ref> by marking the node as deleted, taking away its number, and merely using it to split the arcs to the children. Such a node could then be permanently deleted when it has one or zero children, or reinstated if a parsing ever ends exactly at it.
Reference: [10] <author> Peter Wayner, </author> <title> Genetic Algorithms, </title> <journal> Byte, </journal> <volume> Vol. 16, No. 1, pp.361-368, </volume> <month> January </month> <year> 1991 </year>
Reference-contexts: But an exact closed-form solution or algorithm to obtain the solution is not known, the solution may not be unique, and a suboptimal solution may be sufficient. Therefore we employ a genetic algorithm [5] [6] <ref> [10] </ref>. Such algorithms are useful when one does not know how to obtain a solution but can distinguish between good solutions and bad ones.
Reference: [11] <author> Terry A. Welch, </author> <title> A Technique for High-Performance Data Compression, </title> <journal> IEEE Computer, </journal> <volume> Vol. 17, No. 6, </volume> <pages> pp. 8-19, </pages> <month> June </month> <year> 1984 </year>
Reference-contexts: It is possible to adaptively compress each of the small items individually. While they could then also be decoded individually, this is not worthwhile because adaptive compressors must see some input before they learn the characteristics of the text; before then they usually expand rather than compress their input <ref> [11] </ref>. That is, adaptive compressors exhibit an overhead associated with startup or latency. The property of random access also allows finding a string in the compressed data by searching for its code. This is like having the hashing already done in a crude version of the Rabin-Karp algorithm [9]. <p> This occurs because each code, whose size is more or less the same as any other, can represent widely differing amounts of input. A buffer is needed. 3.3.1 The Lempel-Ziv-Welch Adaptive Dictionary Compressor In 1984, Terry Welch published a modification of LZ78, which is abbreviated as LZW <ref> [11] </ref>. This is the most popular LZ algorithm because of its high speed and simplicity. Coding starts after the dictionary is initialized. The coder outputs the code of the longest dictionary entry that matches the characters at the current position in the input.
Reference: [12] <author> Paul A. Wintz, </author> <title> Transform Picture Coding, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 60, No. 7, </volume> <pages> pp. </pages> <address> 809- 820, </address> <month> July </month> <year> 1972 </year>
Reference-contexts: Trying this with good text compressors produces something like the more obscure passages in Finnegans Wake [1]. This is far better than the decompressors typically used for images, which give random-looking noise <ref> [12] </ref>. 3 2. Image Compression Using Paths This thesis uses a one-dimensional data compressor for lossless image coding, although some of the methods and algorithms examined may also be useful for lossy coding. <p> Another related problem is that the regions in a picture that are correlated are often fairly small compared to the size of the picture <ref> [12] </ref> so that many of these regions are entered and exited by a single scan pass.
Reference: [13] <author> Jacob Ziv and Abraham Lempel, </author> <title> A Universal Algorithm for Sequential Data Compression, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 23, No. </volume> <pages> 3 pp. 337-343, </pages> <month> May, </month> <year> 1977 </year> <month> 54 </month>
Reference-contexts: Another issue is that parsing is done with respect to some dictionary and adaptive coders are always changing their dictionaries. Since optimum parsers must look ahead they have to know the dictionary for all times in the future. Some adaptive coders, such as LZ77 <ref> [13] </ref> [1], have simple dictionaries so this is not problem. But many coders' dictionaries depend on the parsing itself. Thus, the dictionary needed to determine the parsing cannot be known until after the parsing.
Reference: [14] <author> Jacob Ziv and Abraham Lempel, </author> <title> Compression of Individual Sequences Via Variable-rate Coding, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 24, No. 5, </volume> <pages> pp. 530-536, </pages> <month> September, </month> <year> 1978 </year> <month> 55 </month>
References-found: 14

