URL: http://www.ics.uci.edu/~pazzani/Publications/Pazzani-isis96.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pazzani@ics.uci.edu  
Title: Constructive Induction of Cartesian Product Attributes  
Author: Michael J. Pazzani 
Keyword: Bayesian Classifier, Nearest Neighbor, Dependencies. Area of Interest: Concept Formation and Classification.  
Address: Irvine, CA 92717 USA fx: +1-714-824-4056  
Affiliation: Department of Information and Computer Science University of California, Irvine ph: +1-714-824-5888  
Abstract: Constructive induction is the process of changing the representation of examples by creating new attributes from existing attributes. In classification, the goal of constructive induction is to find a representation that facilitates learning a concept description by a particular learning system. Typically, the new attributes are Boolean or arithmetic combinations of existing attributes and the learning algorithms used are decision trees or rule learners. We describe the construction of new attributes that are the Cartesian product of existing attributes. We consider the effects of this operator on a Bayesian classifier an a nearest neighbor algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. & Bankert, R. </author> <year> (1995). </year> <title> A Comparative evaluation of sequential feature selection algorithms. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, </address> <month> Fl. </month> <title> 3 A 1 A 2 A A 1 A 2 A 33 attributes in a Bayesian classifier corresponds to introducing a hidden variable in a Bayesian network Almuallim, </title> <editor> H., and Dietterich, T. G. </editor> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 547-552. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <editor> In Cohen, W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann Cherkauer, </publisher> <editor> K & Shavlik (1993). </editor> <title> Protein Structure Prediction: Selecting Salient Features from large candidate pools. </title> <booktitle> Inter-nation Conference on Intelligent Systems in Molecular Biology. </booktitle> <publisher> AAAI Press. </publisher>
Reference: <author> Cooper, G. & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features Machine Learning, </title> <booktitle> 10, </booktitle> <pages> 57-78. </pages>
Reference-contexts: "wrapper model" (John, Kohavi & Pfleger, 1994) for the construction of Cartesian product attributes and describe the effect of constructive induction of Cartesian product attributes on three learning algorithms: a naive Bayesian classifier (Duda & Hart, 1973); ID3, a decision tree learner (Quinlan, 1986); and PEBLS, a nearest neighbor algorithm <ref> (Cost & Salzberg, 1993) </ref>. <p> Since the naive Bayesian classifier is the Bayes optimal classifier when attributes are conditionally independent given the class, this suggests that the reason the naive Bayesian classifier is not as accurate as the decision tree learner on these problems is the violation of the independence assumption. 3.3 PEBLS PEBLS <ref> (Cost & Salzberg, 1993) </ref> is a nearest neighbor algorithm that makes use of a modification of the value difference metric, MVDM, (Stanfill & Waltz, 1986) for computing the distance between two examples. This distance between two examples is the sum of the value differences of all attributes of the examples. <p> Since this is not true on exclusive-or, FSSJ does not perform as well on this problem nor three naturally occurring problems. Before leaving the topic of attribute selection, we should note that we also tried BSEJ on the standard nearest neighbor algorithm with the overlap metric. PEBLS <ref> (Cost & Salzberg, 1993) </ref> is usually more accurate that the standard nearest neighbor algorithm. BSEJ provided no additional benefits over attribute selection alone with BSE on standard nearest neighbor.
Reference: <author> Craven M.W., & Shavlik J.W. </author> <year> (1993). </year> <title> Learning to Represent Codons: A Challenge Problem for Constructive Induction, in Bajcsy R.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1319-1324. </address>
Reference: <author> Danyluk, A. & Provost, F. </author> <year> (1993). </year> <title> Small disjuncts in action: Learning to diagnose errors in the telephone network local loop. </title> <booktitle> Machine Learning Conference, </booktitle> <pages> pp 81-88. </pages>
Reference: <author> Dougherty, J., Kohavi, J., & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <booktitle> Machine Learning Conference, </booktitle> <pages> pp 194-202 Duda, </pages> <editor> R. & Hart, P. </editor> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> John, G. Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: In this paper, we first propose an algorithm based on the "wrapper model" <ref> (John, Kohavi & Pfleger, 1994) </ref> for the construction of Cartesian product attributes and describe the effect of constructive induction of Cartesian product attributes on three learning algorithms: a naive Bayesian classifier (Duda & Hart, 1973); ID3, a decision tree learner (Quinlan, 1986); and PEBLS, a nearest neighbor algorithm (Cost & Salzberg,
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature selection and extraction. In Young & Fu, </title> <editor> (eds.), </editor> <booktitle> Handbook of pattern recognition and image processing. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: This problem has also recently received attention in the context of Bayesian classifiers (Langley & Sage, 1994), Bayesian networks (Provan & Singh, 1995) and nearest neighbor algorithms (Aha & Bankert, 1995; Townsend-Webber & Kibler, 1994). The subset selection problem has also been studied in pattern recognition <ref> (e.g., Kittler, 1986) </ref>. It has proved important in applying machine learning to real world tasks (Salzberg, Chan-dar, Ford, Murthy & White, 1995; Kubat, Flotzinger and Pfurtscheller, 1993) in which the attributes were not selected by hand by experts. <p> They advocate the use of attribute selection to deal with dependencies in Bayesian classifier. Another alternative to eliminating attributes is Backwards Sequential Elimination (BSE) <ref> (Kittler, 1986) </ref> In the next experiment, we compare BSEJ to FSS and BSE to see if there is an advantage in using constructive induction to create attributes.
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> Wrappers for performance enhancement and oblivious decision graphs. </title> <type> Ph.D. dissertation. </type> <institution> Stanford University. </institution>
Reference-contexts: For example, the naive Bayesian classifier is limited in expressiveness in that it cannot learn nonlinearly separable functions (Langley & Sage, 1994; Rachlin, Kasif, Salzberg & Aha, 1994). In addition, it cannot learn with 100% accuracy some m-of -n concepts <ref> (Kohavi, 1995) </ref>. Second, the naive Bayesian classifier and PEBLS make independence assumptions that are violated by some data sets. When attributes are treated individually, joint probabilities are computed by taking the product of individual probabilities estimated from data. When two of those attributes is used in the example representation.
Reference: <author> Kononenko, I. </author> <year> (1990). </year> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga (Eds.), </editor> <booktitle> Current trends in knowledge acquisition. </booktitle> <address> Amster-dam: </address> <publisher> IOS Press. </publisher>
Reference-contexts: To determine the most likely class of a test example, the probability of each class is computed. A classifier created in this manner is sometimes called a simple (Langley, 1993) or naive <ref> (Kononenko, 1990) </ref> Bayesian classifier.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> Proceedings of the Sixth Euro-pean Working Session on Learning. </booktitle> <pages> (pp. 206-219). </pages> <address> Porto, Portugal: </address> <publisher> Pittman. </publisher>
Reference: <author> Kubat, M., Flotzinger., D., & Pfurtscheller, G. </author> <year> (1993). </year> <title> Discovering patterns in EEG signals: Comparative study of a few methods. </title> <booktitle> Proceedings of the 1993 European Conference on Machine Learning. </booktitle> <pages> (pp. 367-371). </pages> <address> Vienna: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recur-sive Bayesian classifiers. </title> <booktitle> Proceedings of the 1993 European Conference on Machine Learning. </booktitle> <pages> (pp. 153-164). </pages> <address> Vienna: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To determine the most likely class of a test example, the probability of each class is computed. A classifier created in this manner is sometimes called a simple <ref> (Langley, 1993) </ref> or naive (Kononenko, 1990) Bayesian classifier.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994). </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence. </booktitle> <address> Seattle, WA Matheus C.J., & Rendell L.A. </address> <year> (1989). </year> <title> Constructive Induction On Decision Trees, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <pages> 645-650. </pages>
Reference-contexts: This problem has also recently received attention in the context of Bayesian classifiers <ref> (Langley & Sage, 1994) </ref>, Bayesian networks (Provan & Singh, 1995) and nearest neighbor algorithms (Aha & Bankert, 1995; Townsend-Webber & Kibler, 1994). The subset selection problem has also been studied in pattern recognition (e.g., Kittler, 1986).
Reference: <author> Moore, A. W., and Lee, M. S. </author> <year> 1994. </year> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In Cohen, W. W., and Hirsh, H., eds., </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Murthy, S. & Salzberb, S. </author> <year> (1995). </year> <booktitle> Looka-head and Pathology in Decision Tree Induction Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <pages> 1025-1031. </pages>
Reference-contexts: Like Cartesian product attributes, look-ahead in decision trees has been shown to be beneficial on artificial problems, but it is not usually helpful and sometimes harmful on naturally occurring problems <ref> (Murthy & Salzberg, 1995) </ref>. Cartesian product attributes require that continuous attributes be discretized, losing the ordering information that can be exploited by the decision tree learner. Therefore, we would not recommend using Cartesian product attributes with decision tree learners.
Reference: <author> Pagallo G., & Haussler D. </author> <year> (1989). </year> <title> Two algorithms That Learn DNF by Discovering Relevant Features, </title> <booktitle> in Segre A.M.(ed.), Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, pp.119-123. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Provan, G. & Singh, M. </author> <year> (1995). </year> <title> Learning Bayesian networks using feature selection. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, Fl. </address>
Reference-contexts: This problem has also recently received attention in the context of Bayesian classifiers (Langley & Sage, 1994), Bayesian networks <ref> (Provan & Singh, 1995) </ref> and nearest neighbor algorithms (Aha & Bankert, 1995; Townsend-Webber & Kibler, 1994). The subset selection problem has also been studied in pattern recognition (e.g., Kittler, 1986).
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: we first propose an algorithm based on the "wrapper model" (John, Kohavi & Pfleger, 1994) for the construction of Cartesian product attributes and describe the effect of constructive induction of Cartesian product attributes on three learning algorithms: a naive Bayesian classifier (Duda & Hart, 1973); ID3, a decision tree learner <ref> (Quinlan, 1986) </ref>; and PEBLS, a nearest neighbor algorithm (Cost & Salzberg, 1993). <p> Although it would be possible to consider joining three (or more attributes) in one step, the computational complexity makes it impractical for most databases. 3.2 ID3 To conserve space, we will assume that the reader is familiar with decision tree learning algorithms such as ID3 <ref> (Quinlan, 1986) </ref>. In order to minimize differences between algorithms in our experiments, the decision tree learner uses the exact same example representation as the naive Bayesian classifier (i.e., partitioning numeric attributes into discrete categories).
Reference: <author> Rachlin, Kasif, Salzberg & Aha, </author> <year> (1994). </year> <title> Towards a better understanding of memory-based reasoning systems. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: PEBLS makes an independence assumption and its accuracy degrades on problems in which this assumption is violated <ref> (Rachlin, Kasif, Salzberg & Aha, 1994) </ref>. In Table 5, we report on experiments run in the same manner as the previous experiments. Equation 4 is designed to be an estimate of the relevance of an attribute toward making a classification. <p> For example, on the exclusive-or problem discussed earlier, as the number of examples increases, PEBLS without BSEJ will assign a distance of 0 between all examples, since for all attributes and values will be the probability of a class given an attribute value will be 0.5 <ref> (Rachlin, Kasif, Salzberg & Aha, 1994) </ref>. However, if a Cartesian product attribute is formed from the two relevant attributes, a test example will be considered most similar to those training examples that share the two relevant attributes.
Reference: <author> Ragavan, H. & Rendell, L. </author> <year> (1993). </year> <title> Looka-head feature construction for learning hard concepts. </title> <booktitle> Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann Salzberg, </publisher> <editor> S., Chandar, R., Ford, H., Murthy, S. & White, R. </editor> <year> (1995). </year> <title> Decision Trees for Automated Identification of Cosmic Ray Hits in Hubble Space Telescope Images. </title> <booktitle> Publications of the Astronomical Society of the Pacific. </booktitle>
Reference: <author> Sanger T.D., Sutton R.S., & Matheus C.J.(1992). </author> <title> Iterative Construction of Sparse Polynomial Approximations, </title> <editor> in Moody J.E., et al.(eds.), </editor> <booktitle> Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, pp.1064-1071. </address>
Reference: <author> Schlimmer, J. </author> <year> (1987). </year> <title> Incremental adjustment of representations for learning. </title> <booktitle> Machine Learning: Proceedings of the Fourth International Workshop. </booktitle> <editor> Morgan Kauf-mann Schaffer, C. </editor> <booktitle> (1994). A conservation law of generalization performance Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Spiegelhalter, D., Dawid, P., Lauritzen, S. and Cowell, R. </author> <year> (1993). </year> <title> Bayesian Analysis in Expert Systems. </title> <journal> Statistical Science, </journal> <volume> 8, </volume> <pages> 219-283. </pages>
Reference: <author> Stanfill, C. & Waltz, D. </author> <year> (1986). </year> <title> Towards memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference-contexts: that the reason the naive Bayesian classifier is not as accurate as the decision tree learner on these problems is the violation of the independence assumption. 3.3 PEBLS PEBLS (Cost & Salzberg, 1993) is a nearest neighbor algorithm that makes use of a modification of the value difference metric, MVDM, <ref> (Stanfill & Waltz, 1986) </ref> for computing the distance between two examples. This distance between two examples is the sum of the value differences of all attributes of the examples.
Reference: <author> Skalak, D. </author> <title> (1994) Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Townsend-Webber, T. & Kibler, D. </author> <year> (1994). </year> <title> Instance-based prediction of continuous values. </title> <booktitle> AAAI Workshop on Case-based Reasoning. </booktitle>
Reference: <author> Wan, S. & Wong S. </author> <year> (1989). </year> <title> A mesaure for concept dissimilarity and its application in Machine Learning. </title> <editor> In R. Janicki and W. Koczkodaj (eds.) </editor> <booktitle> Computing and Information 267-274. </booktitle>
References-found: 30

