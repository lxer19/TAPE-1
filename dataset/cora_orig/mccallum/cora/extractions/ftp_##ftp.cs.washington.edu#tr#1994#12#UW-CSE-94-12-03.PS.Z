URL: ftp://ftp.cs.washington.edu/tr/1994/12/UW-CSE-94-12-03.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/zahorjan/homepage/listof.htm
Root-URL: 
Title: Runtime Support for Dynamic Space-Based Applications on Distributed Memory Multiprocessors  
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy,  
Note: The research described in this dissertation was supported in part by the National Science Foundation (Grants CCR-8619663, CCR-9123308, and CCR-9200832), the Washington Technology Center, and Digital Equipment Corporation (Systems Re search Center and External Research Program).  
Address: Seattle, WA 98195  1994.  
Affiliation: Department of Computer Science and Engineering University of Washington  University of Washington,  
Abstract: Immaneni Ashok Technical Report # 94-12-03 December 1994 
Abstract-found: 1
Intro-found: 1
Reference: [Agrawal et al. 93] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> Compiler and Runtime Support for Structured and Block Structured Applications. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 578-587, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The PARTI primitives <ref> [Berryman et al 91, Agrawal et al. 93] </ref> provide low-level support for block structured and irregular mesh applications.
Reference: [Allen & Tildesley 87] <author> M. P. Allen and D. J. Tildesley. </author> <title> Computer Simulation of Liquids. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year>
Reference-contexts: Examples of simulations are study of metallic interfaces and film deposition (from vapor and by sputtering process). Here we describe a film deposition simulation that studies the formation of crystals of metals such as platinum and copper <ref> [Allen & Tildesley 87] </ref>. Atoms are dropped, one by one, onto the existing crystal. When a new atom collides with the stable crystal, the existing atoms are disturbed, settling down after a while. <p> atoms to update velocities .. - 49 5.3 Molecular Dynamics Molecular Dynamics simulation, herein after referred to as MD, is a very powerful and popular technique used to study a number of interesting problems related to film deposition, materials interfaces, formation and properties of minerals in extreme atmospheric conditions, etc. <ref> [Allen & Tildesley 87, Fincham 87, Pinches et al. 91] </ref>. Conceptually, MD involves numerical integration (over time) of the classical New-ton's equations of motion for a system of interacting particles. These particles move about inside a simulation box with periodic boundaries.
Reference: [Anderson et al. 92] <author> T. Anderson, B. Bershad, E. Lazowska, and H. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Volume 10(1), </volume> <pages> pages 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Fornili 90] and unstructured mesh computations [Weaver & Schnabel 92, Williams 91b]. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control [Tucker & Gupta 89], dynamic process allocation policies [McCann et al. 92], and adapting to processor reallocations <ref> [Anderson et al. 92] </ref>.
Reference: [Ashok & Zahorjan 94] <author> I. Ashok and J. Zahorjan. Adhara: </author> <title> Runtime Support for Dynamic Space-Based Applications on Distributed Memory Multiprocessors. </title> <booktitle> Proceedings of the Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: multiprocessors: * How to provide support for efficient programming? * How to provide support for efficient execution? The thesis makes the following contributions: Support for Efficient Programming * Design of a parallel programming model: We propose a new programming model, which we call Adhara, to conveniently express dynamic space-based computations <ref> [Ashok & Zahorjan 94] </ref>. The model requires the programmer to specify information using only concepts that are natural to the application.
Reference: [Baden 91] <author> S. Baden. </author> <title> Programming Abstractions for Dynamically Partitioning and Coordinating Localized Scientific Calculations Running on Multiprocessors. </title> <journal> SIAM Journal of Science and Statistical Computation, </journal> <volume> Volume 12, Number 1, </volume> <pages> pages 145-157, </pages> <month> January </month> <year> 1991. </year>
Reference: [Baden & Kohn 91] <author> S. Baden and S. Kohn. </author> <title> A Comparison of Load Balancing Strategies for Particle Methods Running on MIMD Multiprocessors. </title> <booktitle> Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1991. </year>
Reference: [Baden & Kohn 94] <author> S. Baden and S. Kohn. </author> <title> A Robust Parallel Programming Model for Dynamic Non-Uniform Scientific Computations. </title> <booktitle> Proceedings of the Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year> <month> 147 </month>
Reference-contexts: Specialized runtime systems such as the DIME [Williams 91a], Voxel Database [Williams 92] and DYNO [Weaver & Schnabel 92] perform automatic data partitioning based on the characteristics of the specific classes of applications being targeted. Programming environments such as the LPAR <ref> [Baden & Kohn 94] </ref> leave the task of data partitioning to the programmer (the programmer can choose from the generic schemes available in the application libraries).
Reference: [Belkhale & Banerjee 90] <author> K. P. Belkhale and P. Banerjee. </author> <title> Recursive Partitions on Multiprocessors. </title> <booktitle> Proceedings of the 4th Distributed Memory Computing Conference, </booktitle> <pages> pages 930-938, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Discretization There are two general schemes for partitioning the space into regions of equal load: one uses the list of objects sorted by their spatial coordinates <ref> [Belkhale & Banerjee 90] </ref> and the other discretizes the space into rectangular slots and uses the load estimate in each slot. We describe the sorting method for a simple case where the space is partitioned along just one dimension, say along the X-axis.
Reference: [Berger & Bokhari 87] <author> M. Berger and S. Bokhari. </author> <title> A Partitioning Strategy for Nonuniform Problems on Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume C-36, Number 5, </volume> <month> May </month> <year> 1987. </year>
Reference: [Berryman et al 91] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution Time Support for Adaptive Scientific Algorithms on Distributed Memory Machines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 3(3), </volume> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The PARTI primitives <ref> [Berryman et al 91, Agrawal et al. 93] </ref> provide low-level support for block structured and irregular mesh applications.
Reference: [Birdsall & Langdon 85] <author> C. K. Birdsall and A. B. Langdon. </author> <title> Plasma Physics via Computer Simulation. </title> <publisher> McGraw-Hill International, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: the objects in a particle data structure are non-uniformly distributed in the space and the distribution can change dynamically. 1.2 Examples of Dynamic Space-Based Applications 1.2.1 Electro-Magnetic Particle-In-Cell The Electro-Magnetic Particle-In-Cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. The goal of the simulation is to understand the behavior of the particles. <p> The grid points refer to the Electric Field, Magnetic Field or Current Density data, depending on the context.) 5.1 Electro-Magnetic Particle-In-Cell The electro-magnetic particle-in-cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. <p> In the solve phase, new values of electric and magnetic fields are computed at each grid point, using the old field values and the current density assigned in the scatter phase (Figure 5.1 (b)). A leap-frog time integration scheme is used for solving Maxwell's differential equations <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88] </ref>.
Reference: [Bokhari et al. 93] <author> S. Bokhari, T. Crockett, and D. Nicol. </author> <title> Parametric Binary Dissection. ICASE Techincal Report No. </title> <type> 93-39, </type> <institution> NASA Langley Research Center, </institution> <month> July </month> <year> 1993. </year>
Reference: [Bozkus et al. 94] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF Compiler for Distributed Memory MIMD Computers: Design, Implementation and Performance Results. </title> <booktitle> Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 351-360, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF <ref> [Bozkus et al. 94, Chapman et al. 93b] </ref> extend the Fortran language with annotations for controlling data partitioning. <p> Parallelizing compilers for languages such as Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF <ref> [Bozkus et al. 94] </ref> make use of the annotations provided by the programmer to partition the data.
Reference: [Bozkus et al. 94] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M. Wu. </author> <title> Compiling Fortran 90D/HPF for Distributed Memory MIMD Computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Volume 21, Number 1, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF <ref> [Bozkus et al. 94, Chapman et al. 93b] </ref> extend the Fortran language with annotations for controlling data partitioning. <p> Parallelizing compilers for languages such as Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF <ref> [Bozkus et al. 94] </ref> make use of the annotations provided by the programmer to partition the data.
Reference: [Bruge & Fornili 90] <author> F. Bruge and S. Fornili. </author> <title> A distributed Dynamic Load Balancer and its Implementation on Multi-transputer Systems for Molecular Dynamics Simulation. </title> <journal> Computer Physics Communications, </journal> <volume> Volume 60, </volume> <pages> pages 39-45, </pages> <year> 1990. </year> <month> 148 </month>
Reference-contexts: automatic data partitioning techniques for parallelizing compilers [Gupta & Banerjee 92]. 3.2.2 Dynamic Load Balancing Researchers have analyzed the performance of different load balancing techniques for specific classes of applications, such as particle-based simulations [Baden & Kohn 91, Campbell et al. 90, Hanxleden & Scott 91, Hinz 90], molecular dynamics <ref> [Bruge & Fornili 90] </ref> and unstructured mesh computations [Weaver & Schnabel 92, Williams 91b]. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control [Tucker & Gupta 89], dynamic process allocation policies [McCann et al. 92], and adapting to processor
Reference: [Campbell et al. 90] <author> P. Campbell, E. A. Carmona, and D. W. Walker. </author> <title> Hierarchical Domain Decomposition With Unitary Load Balancing for Electromagnetic Particle-In-Cell Codes. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 943-950, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The statement COMPUTATION-SPACE ( 2.0, 1.5, 1.0 ) creates a bounded three dimensional space of size 2:0 fi 1:5 fi 1:0 units. Each spatial object must lie within this space. To exploit spatial locality, Adhara partitions the data using domain decomposition <ref> [Campbell et al. 90, Walker 90] </ref>, where the computation space is partitioned into contiguous regions and each processor is assigned one region.
Reference: [Chapman et al. 93a] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima. </author> <title> Dynamic Data Distributions in Vienna Fortran. </title> <booktitle> Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 284-293, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Fortran-D [Hiranandani et al. 91], Vienna Fortran <ref> [Chapman et al. 93a] </ref> and HPF [Bozkus et al. 94, Chapman et al. 93b] extend the Fortran language with annotations for controlling data partitioning. <p> Programming environments such as the LPAR [Baden & Kohn 94] leave the task of data partitioning to the programmer (the programmer can choose from the generic schemes available in the application libraries). Parallelizing compilers for languages such as Fortran-D [Hiranandani et al. 91], Vienna Fortran <ref> [Chapman et al. 93a] </ref> and HPF [Bozkus et al. 94] make use of the annotations provided by the programmer to partition the data.
Reference: [Chapman et al. 93b] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> High Performance Fortran Without Templates: An Alternative Model for Distribution and Alignment. </title> <booktitle> Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF <ref> [Bozkus et al. 94, Chapman et al. 93b] </ref> extend the Fortran language with annotations for controlling data partitioning.
Reference: [Cybenko & Allen] <author> G. Cybenko and T. Allen. </author> <title> Multidimensional Binary Partitions: Distributed Data Structures for Spatial Partitioning. </title> <journal> International Journal of Control, </journal> <volume> Volume 54, Number 6, </volume> <pages> pages 1335-1352, </pages> <year> 1991. </year>
Reference: [Fallavollita et al. 92] <author> M. A. Fallavollita, J. D. McDonald, and D. Baganoff. </author> <title> Parallel Implementation of a Particle Simulation for Modeling Rarefied Gas Dynamic Flow. </title> <journal> Computing Systems in Engineering, </journal> <volume> volume 3, </volume> <pages> pages 283-289, </pages> <year> 1992. </year>
Reference-contexts: This application is used by aerospace researchers to study the forces exerted on space vehicles as they pass through the upper atmosphere at hypersonic speeds. In low density conditions, Monte Carlo methods that rely on the discrete particle nature are used <ref> [Fallavollita et al. 92, McDonald 89] </ref>. Computation is performed on random pairs of molecules. For the purposes of efficient collision pairing, the active space is represented as a three-dimensional space array of unit-sized cells. <p> Here we mention the work done on dy namic space-based applications: particle-in-cell (plasma physics) [Campbell et al. 90, Ferraro et al. 93, Liewer & Decyk 89, Walker 90], rarefied fluid flow (aeronautics) <ref> [Fallavollita et al. 92, McDonald 89, Singh et al 90] </ref>, and molecular dynamics (materials science and chemistry) [Bruge & Fornili 90, Fincham 87, Pinches et al. 91, Raine et al 89, Rapaport 91, Smith 91]. 3.2 Application Induced Load Balancing 3.2.1 Automatic Data Partitioning Much research has been done on parallel <p> P-&gt;velocity = function ( field_on_P, P-&gt;velocity ); P-&gt;coordinate = function ( P-&gt;velocity, timeStep ); - UPDATE-COORDINATES OF ChargedParticle; - 5.2 Rarefied Fluid Flow The rarefied fluid flow application, referred to as MP3D in the SPLASH benchmarks [Singh et al 90], simulates trajectories of the gaseous molecules using Monte Carlo method <ref> [Fallavollita et al. 92, McDonald 89] </ref>. The active space is a rectangular tunnel with openings at each end and reflecting walls on the remaining sides. The object being studied (e.g., a space vehicle) is represented as a set of additional boundaries in the active space.
Reference: [Ferraro et al. 93] <author> R. Ferraro, P. Liewer, and V. Decyk. </author> <title> Dynamic Load Balancing for a 2D Concurren Plasma PIC Code. </title> <journal> Journal of Computational Physics, </journal> <volume> Volume 109, </volume> <pages> pages 329-341, </pages> <year> 1993. </year>
Reference: [Fincham 87] <author> D. Fincham. </author> <title> Parallel Computers and Molecular Simulations. </title> <journal> Journal of Molecular Simulation, </journal> <volume> Volume 1, </volume> <year> 1987. </year>
Reference-contexts: atoms to update velocities .. - 49 5.3 Molecular Dynamics Molecular Dynamics simulation, herein after referred to as MD, is a very powerful and popular technique used to study a number of interesting problems related to film deposition, materials interfaces, formation and properties of minerals in extreme atmospheric conditions, etc. <ref> [Allen & Tildesley 87, Fincham 87, Pinches et al. 91] </ref>. Conceptually, MD involves numerical integration (over time) of the classical New-ton's equations of motion for a system of interacting particles. These particles move about inside a simulation box with periodic boundaries.
Reference: [Griswold et al. 90] <author> W. G. Griswold, G. A. Harrison, D. Notkin, and L. Snyder. </author> <title> Scalable Abstractions for Parallel Programming. </title> <booktitle> Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: Various techniques for extracting parallelism from the sequential source code and for minimizing execution overheads are discussed in the literature [Bozkus et al. 94, Gupta & Banerjee 93, Hiranandani et al. 94, Rogers & Pingali 94, Saltz et al. 91]. 3.1.2 Specialized Runtime Systems The Phase Abstractions model <ref> [Snyder 89, Griswold et al. 90] </ref> provides scalable abstractions for decomposing parallel computations into phases of different data access, computation and communication characteristics. The ZPL language [Lin & Snyder 93] is specialized for array based computations. <p> Phases The computation is divided into phases, where each phase computes on a particular data structure called its primary data structure. (It often corresponds to the com putation in a do-loop of a sequential algorithm.) This phase, in concept, is similar to the phase defined in the Phase Abstractions Model <ref> [Griswold et al. 90] </ref>. The compute load in a phase is assumed to be proportional to the sum of the computation (measured in terms of iterations) on the objects in the corresponding primary data structure. This information is used by Adhara to balance the load in each phase.
Reference: [Gupta & Banerjee 92] <author> M. Gupta and P. Benerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> 149 IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Volume 3, Number 2, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Parallelizing compilers for languages such as Fortran-D [Hiranandani et al. 91], Vienna Fortran [Chapman et al. 93a] and HPF [Bozkus et al. 94] make use of the annotations provided by the programmer to partition the data. Recently, researchers 16 started looking at automatic data partitioning techniques for parallelizing compilers <ref> [Gupta & Banerjee 92] </ref>. 3.2.2 Dynamic Load Balancing Researchers have analyzed the performance of different load balancing techniques for specific classes of applications, such as particle-based simulations [Baden & Kohn 91, Campbell et al. 90, Hanxleden & Scott 91, Hinz 90], molecular dynamics [Bruge & Fornili 90] and unstructured mesh computations
Reference: [Gupta & Banerjee 93] <author> M. Gupta and P. Benerjee. </author> <title> PARADIGM: A Compiler for Automatic Data Distribution on Multicomputers. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference: [Hanxleden & Scott 91] <author> R. Hanxleden and R. Scott. </author> <title> Load Balancing on Message Passing Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Volume 13, </volume> <pages> pages 312-324, </pages> <year> 1991. </year>
Reference: [Hinz 90] <author> D. Y. Hinz. </author> <title> A Run-Time Load Balancing Strategy for Highly Parallel Systems. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 951-961, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Non-Rectangular Regions The regions can be either rectangular or non-rectangular. The motivation of using non-rectangular regions (Figure 7.1) is to exploit the characteristics of the application for achieving a fine load balance with a small communication overhead <ref> [Hinz 90, Weaver & Schnabel 92, Williams 91b] </ref>. Non-rectangular regions are typically used for partitioning irregular meshes [Williams 91b]. Computing a good non-rectangular partition is expensive. A non-rectangular region also introduces significant overhead of maintaining the data partitions.
Reference: [Hiranandani et al. 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An Overview of the Fortran D Programming System. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Fortran-D <ref> [Hiranandani et al. 91] </ref>, Vienna Fortran [Chapman et al. 93a] and HPF [Bozkus et al. 94, Chapman et al. 93b] extend the Fortran language with annotations for controlling data partitioning. <p> Programming environments such as the LPAR [Baden & Kohn 94] leave the task of data partitioning to the programmer (the programmer can choose from the generic schemes available in the application libraries). Parallelizing compilers for languages such as Fortran-D <ref> [Hiranandani et al. 91] </ref>, Vienna Fortran [Chapman et al. 93a] and HPF [Bozkus et al. 94] make use of the annotations provided by the programmer to partition the data.
Reference: [Hiranandani et al. 94] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluating Compiler Optimizations for Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Volume 21, Number 1, </volume> <month> April </month> <year> 1994. </year>
Reference: [Hockney & Eastwood 88] <author> R. W. Hockney and J. W. Eastwood. </author> <title> Computer Simulation Using Particles. Adam Hilger, </title> <address> Bristol, England, </address> <year> 1988. </year>
Reference-contexts: the objects in a particle data structure are non-uniformly distributed in the space and the distribution can change dynamically. 1.2 Examples of Dynamic Space-Based Applications 1.2.1 Electro-Magnetic Particle-In-Cell The Electro-Magnetic Particle-In-Cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. The goal of the simulation is to understand the behavior of the particles. <p> The grid points refer to the Electric Field, Magnetic Field or Current Density data, depending on the context.) 5.1 Electro-Magnetic Particle-In-Cell The electro-magnetic particle-in-cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. <p> In the solve phase, new values of electric and magnetic fields are computed at each grid point, using the old field values and the current density assigned in the scatter phase (Figure 5.1 (b)). A leap-frog time integration scheme is used for solving Maxwell's differential equations <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88] </ref>.
Reference: [Kohn & Baden 93] <author> S. Kohn and S. Baden. </author> <title> An Implementation of the LPAR Parallel Programming Model for Scientific Computations. </title> <booktitle> Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference: [Lanoski et al. 93] <author> D. Lanoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Volume 4, Number 1, </volume> <month> January </month> <year> 1993. </year> <month> 150 </month>
Reference: [Leuze et al. 89] <author> M. Leuze, L. Dowdy, and K. H. Park. </author> <title> Multiprogramming a Distributed-Memory Multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 1, </volume> <pages> pages 19-33, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: On distributed memory multiprocessors, researchers have studied the issues in multiprogramming <ref> [Leuze et al. 89, Park & Dowdy 89] </ref>, process scheduling [Setia et al. 93] and dynamic reallocation policies [McCann & Zahorjan 93, McCann 94]. 3.4 Summary In the first part (chapters 1-3) of this report, we discussed the need for specialized programming support for dynamic space-based applications, the contributions of this
Reference: [Liewer & Decyk 89] <author> P. C. Liewer and V. K. Decyk. </author> <title> A General Concurrent Algorithm for Plasma Particle-In-Cell Simulation Codes. </title> <journal> Journal of Computational Physics, </journal> <volume> Volume 85, </volume> <year> 1989. </year>
Reference: [Lin & Snyder 90] <author> C. Lin and L. Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages II 163-180, </pages> <year> 1990. </year>
Reference-contexts: We focus on the non-shared memory (or message passing) machines in this work because it is harder to develop efficient parallel programs for message passing machines and since the techniques used for achieving efficiency on non-shared memory machines apply well for shared memory machines <ref> [Lin & Snyder 90, Ngo & Snyder 92] </ref>. 7 HIGH SPEED INTERCONNECTION NETWORK Processor Memory Unit Node 1 Node 2 Node P 1.3.2 Programming Distributed Memory Multiprocessors It is hard and time consuming to develop efficient parallel programs for distributed memory multiprocessors for two reasons: it is hard to write correct
Reference: [Lin & Snyder 93] <author> C. Lin and L. Snyder. ZPL: </author> <title> An Array Sublanguage. </title> <booktitle> Proceedings of the Languages and Compilers for Parallel Computing Conference, </booktitle> <pages> pages 96-114, </pages> <year> 1993. </year>
Reference-contexts: The ZPL language <ref> [Lin & Snyder 93] </ref> is specialized for array based computations.
Reference: [McCann et al. 92] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Multiprogrammed, Shared Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Volume 11(2), </volume> <pages> pages 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: & Scott 91, Hinz 90], molecular dynamics [Bruge & Fornili 90] and unstructured mesh computations [Weaver & Schnabel 92, Williams 91b]. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control [Tucker & Gupta 89], dynamic process allocation policies <ref> [McCann et al. 92] </ref>, and adapting to processor reallocations [Anderson et al. 92].
Reference: [McCann & Zahorjan 93] <author> C. McCann and J. Zahorjan. </author> <title> Processor Allocation Policies for Message-Passing Parallel Computers. </title> <booktitle> Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 19-32, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: On distributed memory multiprocessors, researchers have studied the issues in multiprogramming [Leuze et al. 89, Park & Dowdy 89], process scheduling [Setia et al. 93] and dynamic reallocation policies <ref> [McCann & Zahorjan 93, McCann 94] </ref>. 3.4 Summary In the first part (chapters 1-3) of this report, we discussed the need for specialized programming support for dynamic space-based applications, the contributions of this thesis, and an overview of relevant research work. <p> In this chapter, we look at the application's response to the changes in the processor allocation. Policies that can be used by an operating system to determine when and how to reallocate the processors are discussed in detail in a recent thesis <ref> [McCann 94, McCann & Zahorjan 93] </ref>, and we do not discuss them here. Figure 13.1 shows the basic approach taken by the operating system. When the allocation to a job is changed, the job's threads are relocated onto its new processor set as evenly as possible.
Reference: [McCann 94] <author> C. McCann. </author> <title> Processor Allocation Policies for Message-Passing Parallel Computers. </title> <type> PhD Thesis, </type> <institution> University of Washington, </institution> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: On distributed memory multiprocessors, researchers have studied the issues in multiprogramming [Leuze et al. 89, Park & Dowdy 89], process scheduling [Setia et al. 93] and dynamic reallocation policies <ref> [McCann & Zahorjan 93, McCann 94] </ref>. 3.4 Summary In the first part (chapters 1-3) of this report, we discussed the need for specialized programming support for dynamic space-based applications, the contributions of this thesis, and an overview of relevant research work. <p> In this chapter, we look at the application's response to the changes in the processor allocation. Policies that can be used by an operating system to determine when and how to reallocate the processors are discussed in detail in a recent thesis <ref> [McCann 94, McCann & Zahorjan 93] </ref>, and we do not discuss them here. Figure 13.1 shows the basic approach taken by the operating system. When the allocation to a job is changed, the job's threads are relocated onto its new processor set as evenly as possible.
Reference: [McDonald 89] <author> J. D. McDonald. </author> <title> A Computationally Efficient Particle Simulation Method Suited to Vector Computer Architectures. </title> <type> Ph.D. Thesis, </type> <institution> Department of Aeronautics and Astronautics, Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: This application is used by aerospace researchers to study the forces exerted on space vehicles as they pass through the upper atmosphere at hypersonic speeds. In low density conditions, Monte Carlo methods that rely on the discrete particle nature are used <ref> [Fallavollita et al. 92, McDonald 89] </ref>. Computation is performed on random pairs of molecules. For the purposes of efficient collision pairing, the active space is represented as a three-dimensional space array of unit-sized cells. <p> Here we mention the work done on dy namic space-based applications: particle-in-cell (plasma physics) [Campbell et al. 90, Ferraro et al. 93, Liewer & Decyk 89, Walker 90], rarefied fluid flow (aeronautics) <ref> [Fallavollita et al. 92, McDonald 89, Singh et al 90] </ref>, and molecular dynamics (materials science and chemistry) [Bruge & Fornili 90, Fincham 87, Pinches et al. 91, Raine et al 89, Rapaport 91, Smith 91]. 3.2 Application Induced Load Balancing 3.2.1 Automatic Data Partitioning Much research has been done on parallel <p> P-&gt;velocity = function ( field_on_P, P-&gt;velocity ); P-&gt;coordinate = function ( P-&gt;velocity, timeStep ); - UPDATE-COORDINATES OF ChargedParticle; - 5.2 Rarefied Fluid Flow The rarefied fluid flow application, referred to as MP3D in the SPLASH benchmarks [Singh et al 90], simulates trajectories of the gaseous molecules using Monte Carlo method <ref> [Fallavollita et al. 92, McDonald 89] </ref>. The active space is a rectangular tunnel with openings at each end and reflecting walls on the remaining sides. The object being studied (e.g., a space vehicle) is represented as a set of additional boundaries in the active space.
Reference: [Ngo & Snyder 92] <author> T. Ngo and L. Snyder. </author> <title> On the Influence of Programming Models on Shared Memory Computer Performance. </title> <booktitle> Proceedings of the Scalable High Performance Computing Conference, </booktitle> <year> 1992. </year> <month> 151 </month>
Reference-contexts: We focus on the non-shared memory (or message passing) machines in this work because it is harder to develop efficient parallel programs for message passing machines and since the techniques used for achieving efficiency on non-shared memory machines apply well for shared memory machines <ref> [Lin & Snyder 90, Ngo & Snyder 92] </ref>. 7 HIGH SPEED INTERCONNECTION NETWORK Processor Memory Unit Node 1 Node 2 Node P 1.3.2 Programming Distributed Memory Multiprocessors It is hard and time consuming to develop efficient parallel programs for distributed memory multiprocessors for two reasons: it is hard to write correct
Reference: [Nicol & Saltz 88] <author> D. M. Nicol and J. H. Saltz. </author> <title> Dynamic Remapping of Parallel Computations with Varying Resource Demands. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume 37, Number 9, </volume> <pages> pages 1073-1087, </pages> <year> 1988. </year>
Reference-contexts: The threshold value is chosen to minimize LILB overhead. This method assumes that the load imbalance increases linearly, an assumption which does not hold most of time in real applications. Moreover, global communication is required for computing the load imbalance at every time step. The monitoring method <ref> [Nicol & Saltz 88] </ref> monitors the load imbalance after each time step of the simulation, and initiates load balancing when it detects a minimal 71 value of the LILB overhead in the current interval. This method is robust, but requires global synchronization for monitoring at every time step.
Reference: [Nicol & Saltz 90] <author> D. M. Nicol and J. H. Saltz. </author> <title> An Analysis of Scatter Decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume 39, </volume> <pages> pages 1337-1345, </pages> <year> 1990. </year>
Reference-contexts: The method of decomposing the space into P contiguous regions of equal load (but not necessarily of equal volume) is called a block decomposition. If the load distribution changes dynamically, the space needs to be repartitioned. Dynamic load balancing can be avoided by a scatter decomposition scheme <ref> [Nicol & Saltz 90] </ref>. This scheme decomposes the space into sP sub-regions of equal volume and assigns s sub-regions to each processor in a regular manner (Figure 7.3).
Reference: [Nicol 91] <author> D. M. Nicol. </author> <title> Rectilinear Partitioning of Irregular Data Parallel Computations. </title> <type> ICASE Technical Report No. 91-55, </type> <institution> NASA Langley Research Center, </institution> <month> July </month> <year> 1991. </year>
Reference: [Park & Dowdy 89] <author> K. H. Park and L. W. Dowdy. </author> <title> Dynamic Partitioning of Multiprocessor System. </title> <journal> International Journal of Parallel Programming, </journal> <volume> Volume 18, Number 2, </volume> <pages> pages 91-120, </pages> <year> 1989. </year>
Reference-contexts: On distributed memory multiprocessors, researchers have studied the issues in multiprogramming <ref> [Leuze et al. 89, Park & Dowdy 89] </ref>, process scheduling [Setia et al. 93] and dynamic reallocation policies [McCann & Zahorjan 93, McCann 94]. 3.4 Summary In the first part (chapters 1-3) of this report, we discussed the need for specialized programming support for dynamic space-based applications, the contributions of this
Reference: [Pilkington & Baden 94] <author> J. Pilkington and S. Baden. </author> <title> Partitioning with Spacefilling Curves. </title> <type> Technical Report No. </type> <institution> CS94-349, Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <month> March </month> <year> 1994. </year>
Reference: [Pinches et al. 91] <author> M. R. S. Pinches, D. J. Tildesley, and W. Smith. </author> <title> Large Scale Molecular Dynamics on Parallel Computers Using the Link-Cell Algorithm. </title> <journal> Journal of Molecular Simulation, </journal> <volume> Volume 6, </volume> <year> 1991. </year>
Reference-contexts: atoms to update velocities .. - 49 5.3 Molecular Dynamics Molecular Dynamics simulation, herein after referred to as MD, is a very powerful and popular technique used to study a number of interesting problems related to film deposition, materials interfaces, formation and properties of minerals in extreme atmospheric conditions, etc. <ref> [Allen & Tildesley 87, Fincham 87, Pinches et al. 91] </ref>. Conceptually, MD involves numerical integration (over time) of the classical New-ton's equations of motion for a system of interacting particles. These particles move about inside a simulation box with periodic boundaries.
Reference: [Raine et al 89] <author> A. R. C. Raine, D. Fincham, and W. Smith. </author> <title> Systolic Loop Methods for Molecular Dynamics Simulation Using Multiple Transputers. </title> <journal> Computer Physics Communications, </journal> <volume> Volume 55, </volume> <year> 1989. </year>
Reference: [Rapaport 91] <author> D. C. Rapaport. </author> <title> Multi-million Particle Molecular Dynamics II. Design Considerations for Distributed Processing. </title> <journal> Computer Physics Communications, </journal> <volume> Volume 62, </volume> <pages> pages 217-228, </pages> <year> 1991. </year>
Reference: [Reed et al. 87] <author> D. Reed, L. Adams, and M. Patrick. </author> <title> Stencils and Problem Partitioning: Their Influence on the Performance of Multiple Processor Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume C-36, Number 7, </volume> <month> July </month> <year> 1987. </year> <month> 152 </month>
Reference: [Rogers 91] <author> A. M. Rogers. </author> <title> Compiling for Locality of Reference. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1991. </year>
Reference: [Rogers & Pingali 94] <author> A. M. Rogers and K. Pingali. </author> <title> Compiling for Distributed Memory Architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Volume 5, Number 3, </volume> <month> March </month> <year> 1994. </year>
Reference: [Rosing et al 91] <author> M. Rosing, R. B. Schnabel, and R. P. Weaver. </author> <title> The DINO Parallel Programming Language. </title> <journal> Journal of Parallel and Distrbitued Computing, </journal> <volume> Volume 13, Number 9, </volume> <pages> pages 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The ZPL language [Lin & Snyder 93] is specialized for array based computations. DINO <ref> [Rosing et al 91] </ref> is a language for expressing data-parallel numerical computations, and its extension DYNO [Weaver & Schnabel 92] is specialized for un structured applications. 15 The LPAR programming environment [Baden & Kohn 91, Baden & Kohn 94, Kohn & Baden 93] supports dynamic non-uniform scientific applications.
Reference: [Saltz et al. 91] <author> J. Saltz, H. Berryman and J. Wu. </author> <title> Multiprocessors and Runtime Compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 3(6), </volume> <pages> pages 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference: [Singh et al 90] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title>
Reference-contexts: In the push phase, using the field values at the grid points, the force on each particle is computed, and the position and velocity of all the particles are updated (Figure 1.2 (c)). 1.2.2 Rarefied Fluid Flow The rarefied fluid flow application, referred to as MP3D in the SPLASH benchmarks <ref> [Singh et al 90] </ref>, simulates trajectories of gaseous molecules in low density regions. This application is used by aerospace researchers to study the forces exerted on space vehicles as they pass through the upper atmosphere at hypersonic speeds. <p> Here we mention the work done on dy namic space-based applications: particle-in-cell (plasma physics) [Campbell et al. 90, Ferraro et al. 93, Liewer & Decyk 89, Walker 90], rarefied fluid flow (aeronautics) <ref> [Fallavollita et al. 92, McDonald 89, Singh et al 90] </ref>, and molecular dynamics (materials science and chemistry) [Bruge & Fornili 90, Fincham 87, Pinches et al. 91, Raine et al 89, Rapaport 91, Smith 91]. 3.2 Application Induced Load Balancing 3.2.1 Automatic Data Partitioning Much research has been done on parallel <p> += function_of ( ElectricField [I,J,K], MagneticField [I,J,K], distance between (I,J,K) and P ); 40 P-&gt;velocity = function ( field_on_P, P-&gt;velocity ); P-&gt;coordinate = function ( P-&gt;velocity, timeStep ); - UPDATE-COORDINATES OF ChargedParticle; - 5.2 Rarefied Fluid Flow The rarefied fluid flow application, referred to as MP3D in the SPLASH benchmarks <ref> [Singh et al 90] </ref>, simulates trajectories of the gaseous molecules using Monte Carlo method [Fallavollita et al. 92, McDonald 89]. The active space is a rectangular tunnel with openings at each end and reflecting walls on the remaining sides.
Reference: [Singh et al. 93] <author> J. P. Singh, T. Joe, J. L. Hennessy, and A. Gupta. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Each node consists of a general purpose processor and a memory unit. The nodes are connected via high-speed interconnection network. A processor can access the memory situated in another node (termed as the non-local memory) either directly through hardware (as in the KSR-1 and DASH <ref> [Singh et al. 93] </ref>) or indirectly through messages (as in the Intel Paragon, CM-5 and nCUBE). In either case, the time required to access non-local memory is at least an order of magnitude greater than the time required to access local memory.
Reference: [Smith 91] <author> W. Smith. </author> <title> Molecular Dynamics on Hypercube Parallel Computers. </title> <journal> Computer Physics Communications, </journal> <volume> Volume 62, </volume> <pages> pages 229-248, </pages> <year> 1991. </year>
Reference: [Snyder 89] <author> L. Snyder. </author> <title> The XYZ abstraction levels of Poker-like languages. </title> <booktitle> Proceedings of the Second Workshop on Parallel Compilers and Algorithms, </booktitle> <address> Urbana, Illinois, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Various techniques for extracting parallelism from the sequential source code and for minimizing execution overheads are discussed in the literature [Bozkus et al. 94, Gupta & Banerjee 93, Hiranandani et al. 94, Rogers & Pingali 94, Saltz et al. 91]. 3.1.2 Specialized Runtime Systems The Phase Abstractions model <ref> [Snyder 89, Griswold et al. 90] </ref> provides scalable abstractions for decomposing parallel computations into phases of different data access, computation and communication characteristics. The ZPL language [Lin & Snyder 93] is specialized for array based computations.
Reference: [Snyder 93] <author> L. Snyder. </author> <booktitle> Foundations of Practical Parallel Programming Languages. Proceedings of the Second International Conference of the Austrian Center for Parallel Computation, </booktitle> <year> 1993. </year> <month> 153 </month>
Reference: [Setia et al. 93] <author> S. Setia, M. S. Squillante, and S. Tripathi. </author> <title> Processor Scheduling on Multiprogrammed, </title> <booktitle> Distributed Memory Parallel Systems. Proceedings of ACM SIGMETRICS Conference, </booktitle> <pages> pages 158-170, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: On distributed memory multiprocessors, researchers have studied the issues in multiprogramming [Leuze et al. 89, Park & Dowdy 89], process scheduling <ref> [Setia et al. 93] </ref> and dynamic reallocation policies [McCann & Zahorjan 93, McCann 94]. 3.4 Summary In the first part (chapters 1-3) of this report, we discussed the need for specialized programming support for dynamic space-based applications, the contributions of this thesis, and an overview of relevant research work.
Reference: [Tucker & Gupta 89] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: & Kohn 91, Campbell et al. 90, Hanxleden & Scott 91, Hinz 90], molecular dynamics [Bruge & Fornili 90] and unstructured mesh computations [Weaver & Schnabel 92, Williams 91b]. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control <ref> [Tucker & Gupta 89] </ref>, dynamic process allocation policies [McCann et al. 92], and adapting to processor reallocations [Anderson et al. 92].
Reference: [Walker 90] <author> D. W. Walker. </author> <title> Characterizing the Parallel Performance of a large-scale Particle-In-Cell Plasma Simulation Code. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 2, </volume> <pages> pages 257-288, </pages> <year> 1990. </year>
Reference-contexts: the objects in a particle data structure are non-uniformly distributed in the space and the distribution can change dynamically. 1.2 Examples of Dynamic Space-Based Applications 1.2.1 Electro-Magnetic Particle-In-Cell The Electro-Magnetic Particle-In-Cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. The goal of the simulation is to understand the behavior of the particles. <p> The statement COMPUTATION-SPACE ( 2.0, 1.5, 1.0 ) creates a bounded three dimensional space of size 2:0 fi 1:5 fi 1:0 units. Each spatial object must lie within this space. To exploit spatial locality, Adhara partitions the data using domain decomposition <ref> [Campbell et al. 90, Walker 90] </ref>, where the computation space is partitioned into contiguous regions and each processor is assigned one region. <p> The grid points refer to the Electric Field, Magnetic Field or Current Density data, depending on the context.) 5.1 Electro-Magnetic Particle-In-Cell The electro-magnetic particle-in-cell (EMPIC) application simulates movement of charged particles that interact by exerting electric and magnetic field forces on each other <ref> [Birdsall & Langdon 85, Hockney & Eastwood 88, Walker 90] </ref>. The force experienced by a particle depends on the current position and velocity of all the particles, and this changes continuously with time. <p> All these methods attempt to minimize the sum of load balancing and load imbalance overheads per time step of the simulation, which we call the LILB overhead. Note that the load balancing cost is amortized over the interval in which load balancing is not done. The event driven method <ref> [Walker 90] </ref> initiates load balancing when the load imbalance crosses a threshold. The threshold value is chosen to minimize LILB overhead. This method assumes that the load imbalance increases linearly, an assumption which does not hold most of time in real applications.
Reference: [Weaver & Schnabel 92] <author> R. Weaver and R. Schnabel. </author> <title> Automatic Mapping and Load Balancing of Pointer-Based Dynamic Data Structures on Distributed Memory Machines. </title> <booktitle> Proceedings of the Scalable High Performance Computing Conference, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: The ZPL language [Lin & Snyder 93] is specialized for array based computations. DINO [Rosing et al 91] is a language for expressing data-parallel numerical computations, and its extension DYNO <ref> [Weaver & Schnabel 92] </ref> is specialized for un structured applications. 15 The LPAR programming environment [Baden & Kohn 91, Baden & Kohn 94, Kohn & Baden 93] supports dynamic non-uniform scientific applications. <p> Specialized runtime systems such as the DIME [Williams 91a], Voxel Database [Williams 92] and DYNO <ref> [Weaver & Schnabel 92] </ref> perform automatic data partitioning based on the characteristics of the specific classes of applications being targeted. <p> & Banerjee 92]. 3.2.2 Dynamic Load Balancing Researchers have analyzed the performance of different load balancing techniques for specific classes of applications, such as particle-based simulations [Baden & Kohn 91, Campbell et al. 90, Hanxleden & Scott 91, Hinz 90], molecular dynamics [Bruge & Fornili 90] and unstructured mesh computations <ref> [Weaver & Schnabel 92, Williams 91b] </ref>. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control [Tucker & Gupta 89], dynamic process allocation policies [McCann et al. 92], and adapting to processor reallocations [Anderson et al. 92]. <p> Non-Rectangular Regions The regions can be either rectangular or non-rectangular. The motivation of using non-rectangular regions (Figure 7.1) is to exploit the characteristics of the application for achieving a fine load balance with a small communication overhead <ref> [Hinz 90, Weaver & Schnabel 92, Williams 91b] </ref>. Non-rectangular regions are typically used for partitioning irregular meshes [Williams 91b]. Computing a good non-rectangular partition is expensive. A non-rectangular region also introduces significant overhead of maintaining the data partitions.
Reference: [Williams 91a] <author> R. Williams. DIME: </author> <title> A users manual. Caltech Concurrent Computation Project report C3P 861, </title> <month> February </month> <year> 1991. </year>
Reference-contexts: The PARTI primitives [Berryman et al 91, Agrawal et al. 93] provide low-level support for block structured and irregular mesh applications. The DIME system <ref> [Williams 91a] </ref> and the Voxel Database system [Williams 92] provide high-level support for irregular mesh applications. 3.1.3 Parallel Scientific Applications Several researchers studied issues in parallelizing specific scientific applications in different fields of science and engineering. <p> Specialized runtime systems such as the DIME <ref> [Williams 91a] </ref>, Voxel Database [Williams 92] and DYNO [Weaver & Schnabel 92] perform automatic data partitioning based on the characteristics of the specific classes of applications being targeted.
Reference: [Williams 91b] <author> R. Williams. </author> <title> Performance of Dynamic Load Balancing Algorithms for Unstructured Mesh Calculations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 3(5), </volume> <pages> pages 457-481, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: & Banerjee 92]. 3.2.2 Dynamic Load Balancing Researchers have analyzed the performance of different load balancing techniques for specific classes of applications, such as particle-based simulations [Baden & Kohn 91, Campbell et al. 90, Hanxleden & Scott 91, Hinz 90], molecular dynamics [Bruge & Fornili 90] and unstructured mesh computations <ref> [Weaver & Schnabel 92, Williams 91b] </ref>. 3.3 Operating System Induced Load Balancing Researchers have studied the issues in scheduling on multiprogrammed shared-memory machines, such as process control [Tucker & Gupta 89], dynamic process allocation policies [McCann et al. 92], and adapting to processor reallocations [Anderson et al. 92]. <p> Non-Rectangular Regions The regions can be either rectangular or non-rectangular. The motivation of using non-rectangular regions (Figure 7.1) is to exploit the characteristics of the application for achieving a fine load balance with a small communication overhead <ref> [Hinz 90, Weaver & Schnabel 92, Williams 91b] </ref>. Non-rectangular regions are typically used for partitioning irregular meshes [Williams 91b]. Computing a good non-rectangular partition is expensive. A non-rectangular region also introduces significant overhead of maintaining the data partitions. <p> The motivation of using non-rectangular regions (Figure 7.1) is to exploit the characteristics of the application for achieving a fine load balance with a small communication overhead [Hinz 90, Weaver & Schnabel 92, Williams 91b]. Non-rectangular regions are typically used for partitioning irregular meshes <ref> [Williams 91b] </ref>. Computing a good non-rectangular partition is expensive. A non-rectangular region also introduces significant overhead of maintaining the data partitions. This approach might be practical for the problems where the load distribution is static and the partitioning is performed just once at the beginning of the execution.
Reference: [Williams 92] <author> R. Williams. </author> <title> Voxel Database: A Paradigm for Parallelism with Spatial Structure. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Volume 4(8), </volume> <pages> pages 619-636, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: The PARTI primitives [Berryman et al 91, Agrawal et al. 93] provide low-level support for block structured and irregular mesh applications. The DIME system [Williams 91a] and the Voxel Database system <ref> [Williams 92] </ref> provide high-level support for irregular mesh applications. 3.1.3 Parallel Scientific Applications Several researchers studied issues in parallelizing specific scientific applications in different fields of science and engineering. <p> Specialized runtime systems such as the DIME [Williams 91a], Voxel Database <ref> [Williams 92] </ref> and DYNO [Weaver & Schnabel 92] perform automatic data partitioning based on the characteristics of the specific classes of applications being targeted.
References-found: 66

