URL: http://www.cs.purdue.edu/homes/spaf/tech-reps/9430.ps
Refering-URL: http://www.cs.purdue.edu/homes/spaf/students.html
Root-URL: http://www.cs.purdue.edu
Email: krsul@cs.purdue.edu  
Title: Authorship Analysis: Identifying The Author of a Program 1  
Author: Ivan Krsul 
Address: West Lafayette, IN 47907-1398  
Affiliation: The COAST Project Department of Computer Sciences Purdue University  
Abstract: May 3, 1994 Technical Report CSD-TR-94-030 1 This paper was originally written as a Master's thesis at Purdue University. 
Abstract-found: 1
Intro-found: 1
Reference: [All86] <author> L. Allison. </author> <title> A practical introduction to denotational semantics. </title> <publisher> Cambridge University Press, </publisher> <address> first edition, </address> <year> 1986. </year>
Reference-contexts: Formal methods for defining the semantics, or connotative meaning, of programs and programming languages, such as axiomatic semantics 1 and denotational semantics <ref> [All86, Set89, Hoa69] </ref>, could be used to search for identifying patterns in program logic or semantic structure. Also, consider a programmer who codes with a top-down approach.
Reference: [And91] <author> G. R. Andrews. </author> <title> Concurrent Programming. </title> <publisher> The Ben-jamin/Cummings Publishing Co., </publisher> <address> first edition, </address> <year> 1991. </year>
Reference-contexts: A predicate is a propositional formula in which relational and quantified expressions can be used in place of prepositional variables. To interpret 7 a predicate, we first interpret each relational and quantified expression| yielding true and false for each|and then interpret the resulting prepositional formula <ref> [And91, page 20] </ref>. A complex system may be divided into simpler pieces called modules.
Reference: [BB89] <author> A. Benander and B. Benander. </author> <title> An empirical study of COBOL programs via a style analyzer: The benefits of good programming style. </title> <journal> The Journal of Systems and Software, </journal> <volume> 10(2) </volume> <pages> 271-279, </pages> <year> 1989. </year>
Reference-contexts: Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90b, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. 3.4 Software Metrics Categories All these sources give us ample material to select the metrics we will use.
Reference: [BM85] <author> R. Berry and B. Meekings. </author> <title> A style analysis of C programs. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 80-88, </pages> <year> 1985. </year>
Reference-contexts: Metric STY6c: Ratio of lines of block style comments to lines of code. * Metric STY7: Ratio of white lines to lines of code [RN93, pages 70-71]. 3.5.2 Programming Style Metrics * Metric PRO1: Mean program line length (characters per line) <ref> [BM85] </ref>. * Metric PRO2: A vector of metrics that will consider name lengths. Metric PRO2a: Mean local variable name length. Metric PRO2b: Mean global variable name length. Metric PRO2c: Mean function name length. <p> However, there are some techniques that are widely used and we will concentrate on these. 35 * Metric PSM4: The assert macro is used. * Metric PSM5: Lines of code per function <ref> [KP78, BM85] </ref>. * Metric PSM6: Variable count to lines of code ratio. This metric could identify those programmers who tend to avoid reusing variables, creating new variables for each loop control variable, etc. <p> Rather, each instance of the if, for, while, do, case statements and the ? operator increases our decision count by one. * Metric PSM9: Is the goto keyword used? Surprisingly, software design ers and programmers still rely on these <ref> [BM85] </ref>. * Metric PSM10: Simple software complexity metrics offer little information that might be application independent [OC89]. <p> Because the tools available calculate only software complexity metrics, we developed a series of programs designed specifically for our purpose and ran them through a series of controlled experiments. 3.6.1 C Source Code Analyzer Although languages such as Awk can be used for extracting all the necessary metrics <ref> [BM85] </ref>, we avoided the Awk and Perl [WS90] programming languages for calculating most of the metrics dealing with programming structure because existing tools in compiler construction make it easier to write C code, 39 and our programs benefit from the reuse of code that was already tested.
Reference: [BS84] <author> H. Berghel and D. Sallach. </author> <title> Measurements of program similarity in identical task environments. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(8) </volume> <pages> 65-76, </pages> <year> 1984. </year>
Reference-contexts: They briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes. Two other studies by Berghel and Sallach <ref> [BS84] </ref> and Evangelist [Eva84] support this theory. Cook and Oman describe the use of "markers" to describe the occurrences of certain peculiar characteristics, much like the markers used to resolve 9 authorship disputes of written works. The markers used in their work are based purely on typographic characteristics.
Reference: [Coo87] <author> Doug Cooper. Condensed Pascal. W. W. Norton and Company, </author> <year> 1987. </year>
Reference-contexts: Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90b, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. 3.4 Software Metrics Categories All these sources give us ample material to select the metrics we will use. <p> Program 3 has low values for this metric (two of these are the lowest values for the programmer) and this is to be expected. The routines needed to do the job in this application, create stack, pop, push and delete stack, are naturally short as can be seen in <ref> [Coo87, pages 374-375] </ref> and [NS86, page 100]. Program 2 has the highest values for this metric, which is logical because this program deals with I/O and user interaction. Metrics PSM1 and PSM2 also show large variations.
Reference: [Dau90] <author> K. Dauber. </author> <title> The Idea of Authorship in America. </title> <institution> The University of Wisconsin Press, </institution> <year> 1990. </year>
Reference-contexts: Programmers that are involved in high security projects or programmers that have been known to break the law are attractive candidates for classification. 1.1 Statement of the Problem. Authorship analysis in literature has been widely debated for hundreds of years, and a large body of knowledge has been developed <ref> [Dau90] </ref>. Authorship analysis on computer software, however, is different and more difficult than in literature. Several reasons make this problem difficult. Authorship analysis in computer software does not have the same stylistic characteristics as authorship analysis in literature.
Reference: [Den87] <author> D. Denning. </author> <title> An intrusion detection system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(2) </volume> <pages> 222-232, </pages> <year> 1987. </year>
Reference-contexts: Real-time intrusion detection systems could be enhanced to include authorship information. Dorothy Denning writes in <ref> [Den87] </ref> about a proposed real-time intrusion detection system: The model is based on the hypothesis that exploitation of a system's vulnerabilities involves abnormal use of the system; therefore, security violations could be detected from abnor mal patterns of system usage.
Reference: [Dij68] <author> E. Dijkstra. </author> <title> Goto statement considered harmful. </title> <journal> Communications of the ACM, </journal> <volume> 11(3) </volume> <pages> 147-148, </pages> <year> 1968. </year>
Reference-contexts: Because the use of goto statements has been virtually banned from the academic community <ref> [Dij68, SCS86, RN93, Set89] </ref>, and because faculty members oppose their use in programming courses, we have chosen to eliminate metric PSM9 from further analysis. The probability of finding these statements in students code at Purdue University is negligible.
Reference: [Dis37] <editor> B. Disraeli. Venetia. </editor> <address> New York and London, 1837. </address> <month> 65 </month>
Reference-contexts: Hundreds of books and essays have been written on this topic, some as early as 1837 <ref> [Dis37] </ref>. Especially interesting was W. Elliott's attempt to 8 resolve the authorship of Shakespeare's work with a computer by examining literary minutiae from word frequency to punctuation and proclivity to use clauses and compounds.
Reference: [EV91] <author> W. Elliot and R. Valenza. </author> <title> Was the Earl of Oxford the true Shake--speare? Notes and Queries, </title> <booktitle> 38 </booktitle> <pages> 501-506, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: For three years, the Shakespeare Clinic of Claremont Colleges used computers to see which of fifty-eight claimed authors of Shakespeare's works matched Shakespeare's writing style <ref> [EV91] </ref>. Among the techniques used was a modal test that divided a text into blocks, fifty-two keywords in each block, and measured and ranked eigenvalues, or modes. Rather than representing keyword occurrences, modes measure patterns of deviation from a writer's rates of word frequency. <p> Rather than representing keyword occurrences, modes measure patterns of deviation from a writer's rates of word frequency. Other more conventional tests examined were hyphenated compound words; relative clauses per thousand; grade-level of writing; and percentage of open and feminine ended lines <ref> [EV91] </ref>. Although much controversy surrounds the specific results obtained by Elliott's computer analysis, it is clear from the results that Shakespeare fits a narrow and distinctive profile. As W. Elliot and R. Valenza write in [EV91]: Our conclusion from the clinic was that Shakespeare fit within a fairly narrow, distinctive profile <p> relative clauses per thousand; grade-level of writing; and percentage of open and feminine ended lines <ref> [EV91] </ref>. Although much controversy surrounds the specific results obtained by Elliott's computer analysis, it is clear from the results that Shakespeare fits a narrow and distinctive profile. As W. Elliot and R. Valenza write in [EV91]: Our conclusion from the clinic was that Shakespeare fit within a fairly narrow, distinctive profile under our best tests. If his poems were written by a committee, it was a remarkably consistent committee.
Reference: [Eva84] <author> M. </author> <title> Evangelist. Program complexity and programming style. </title> <booktitle> In Proceedings of the International Conference of Data Engineering, </booktitle> <pages> pages 534-541. </pages> <publisher> IEEE, </publisher> <year> 1984. </year>
Reference-contexts: They briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes. Two other studies by Berghel and Sallach [BS84] and Evangelist <ref> [Eva84] </ref> support this theory. Cook and Oman describe the use of "markers" to describe the occurrences of certain peculiar characteristics, much like the markers used to resolve 9 authorship disputes of written works. The markers used in their work are based purely on typographic characteristics.
Reference: [GJM91] <author> C. Ghezzi, M. Jazayeri, and D. Mandrioli. </author> <title> Fundamentals of Software Engineering. </title> <publisher> Prentice Hall, </publisher> <address> first edition, </address> <year> 1991. </year>
Reference-contexts: Education is only one of many factors that have an effect on the evolution of programming styles. Not only do software engineering models impose particular naming conventions, parameter passing methods and commenting styles; they also impose a planning and development strategy. The waterfall model <ref> [GJM91] </ref>, for example, encourages the design of precise specifications, utilization of program modules and extensive module testing. These have a marked impact on programming style.
Reference: [Gri81] <author> S. Grier. </author> <title> A tool that detects plagiarism in Pascal programs. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 13(1) </volume> <pages> 15-20, </pages> <year> 1981. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in figure 2.4.
Reference: [GS92] <author> S. Garfinkel and E. Spafford. </author> <title> Practical Unix Security. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Software evolves over time. As time passes, programmers vary their programming habits and their choice of programming languages. The 1 Trojan horses are defined in <ref> [GS92] </ref> as programs that appear to have one function but actually perform another function. 2 Viruses are defined in [GS92] as programs that modify other programs in a computer, inserting copies of themselves. 3 Logic bombs are defined in [GS92] as hidden features in programs that go off after certain conditions <p> Software evolves over time. As time passes, programmers vary their programming habits and their choice of programming languages. The 1 Trojan horses are defined in <ref> [GS92] </ref> as programs that appear to have one function but actually perform another function. 2 Viruses are defined in [GS92] as programs that modify other programs in a computer, inserting copies of themselves. 3 Logic bombs are defined in [GS92] as hidden features in programs that go off after certain conditions are met. 1 development of new software engineering methods, the introduction of formal methods for program verification, and the <p> The 1 Trojan horses are defined in <ref> [GS92] </ref> as programs that appear to have one function but actually perform another function. 2 Viruses are defined in [GS92] as programs that modify other programs in a computer, inserting copies of themselves. 3 Logic bombs are defined in [GS92] as hidden features in programs that go off after certain conditions are met. 1 development of new software engineering methods, the introduction of formal methods for program verification, and the development of user-friendly, graphic oriented code processing systems and debuggers all contribute to making programming a highly dynamic field. 3. <p> Some programmers tend to ignore the error return values of system calls that are considered reliable <ref> [GS92, page 164] </ref>. Thus, a metric can be obtained out of the percentage of reliable system calls whose error codes are ignored by the programmer. Also, some programmers tend to overlook the error codes returned by system calls that should never have them ignored (like "malloc").
Reference: [Han91] <author> D. Hanson. </author> <title> Code generation interface for ANSI C. </title> <journal> Software - Practice and Experience, </journal> <volume> 38 </volume> <pages> 963-988, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: At the heart of the software tools is a software analyzer built for the lcc C compiler front-end developed by David R. Hanson of the University of Princeton <ref> [Han91] </ref>. An early version of the software analyzer was written by Goran Larsson.
Reference: [HH92] <author> W. Hope and K. Holston. </author> <title> The Shakespeare Controversy. </title> <publisher> McFarland & Company, </publisher> <year> 1992. </year>
Reference-contexts: Such great figures as Mark Twain, Irving Wall and Sigmund Freud debated at length this particular issue <ref> [HH92] </ref>. Mark Twain, for example, made the following observations about Shakespeare and his writings: * Shakespeare's parents could not read, write or sign names. * He made a will, signed on each of the three pages.
Reference: [Hoa69] <author> C. A. R. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 576-580, </pages> <year> 1969. </year>
Reference-contexts: Formal methods for defining the semantics, or connotative meaning, of programs and programming languages, such as axiomatic semantics 1 and denotational semantics <ref> [All86, Set89, Hoa69] </ref>, could be used to search for identifying patterns in program logic or semantic structure. Also, consider a programmer who codes with a top-down approach.
Reference: [HU79] <author> John Hopcroft and Jeffrey Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> first edition, </address> <year> 1979. </year>
Reference-contexts: Consider the case of the function in figure 2.8 where the program has that type of bug. The result of the program is undefined when both input strings are empty. However, we must note that it is an undecidable problem to determine the correctness of an arbitrary function <ref> [HU79] </ref>. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in [KP78] that "A comment is of zero (or negative) value if it is wrong". Ranade and Nash [RN93, page 89] devote a rule to the truth of every comment.
Reference: [Jan88] <author> H. Jankowitz. </author> <title> Detecting plagiarism in student Pascal programs. </title> <journal> Computer Journal, </journal> <volume> 31(1) </volume> <pages> 1-8, </pages> <year> 1988. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in figure 2.4.
Reference: [JW88] <author> R. Johnson and D. Wichern. </author> <title> Applied Multivariate Statistical Analysis. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: Unfortunately, it is difficult to find ranges for each of the metrics that could be used for any group of programmers without loss of accuracy. The second statistical analysis method we can use, and the one chosen for our analysis, is discriminant analysis. This method, described in <ref> [SAS, JW88] </ref> is a multivariate technique concerned with separating observations and with allocating new observations into previously defined groups. 50 4.4 Preliminary Analysis and Elimination of Metrics Not all metrics calculated proved to be useful in further analysis.
Reference: [KP78] <author> B. Kernighan and P. Plauger. </author> <title> The Elements of Programming Style. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: Only programer two has chosen temporary variable names that reflect the use that will be given to the variable. 4. One of the programs has a significant bug: The return of the function is undefined when the two input strings are empty (i.e. it fails to "do nothing" gracefully <ref> [KP78] </ref>). 20 5. The program for programmer one has an incorrect comment. 6. Only one of the programmers has consistently indented his programs using three spaces. The rest used only two spaces. 7. The placement of curly braces (f) is distinct for all programmers. <p> a wide variety of sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base for extracting metrics dealing with programming style. * Conte, Dunsmore and Shen [SCS86] give a comprehensive list of soft ware complexity metrics. * Kernighan and Plauger <ref> [KP78] </ref> give over seventy programming rules that should be part of "good" programming practice. 28 * D. <p> However, because of the large number of rules and metrics available, we have decided to divide our metrics into three categories. Programming style, as shown by example by Kernighan and Plauger <ref> [KP78] </ref>, Oman and Cook [OC90a] and Ranade and Nash [RN93], is a broad 29 term that encompasses a much greater universe than the one we have chosen in this paper. <p> This vector will consist of: Metric PRO3a: Some names use the underscore character. Metric PRO3b: Use of temporary variables 2 that are named XXX1, XXX2, etc. <ref> [KP78] </ref>, or "tmp," "temp," "tmpXXX" or "tem pXXX" [RN93]. Metric PRO3c: Percentage of variable names that start with an uppercase letter. 2 It can be argued that all local variables are temporary and no global variable is temporary. <p> All three can be used for the same purposes [KR85]. * Metric PRO8: Does the programmer use comments that are nearly an echo of the code <ref> [KP78, page 143] </ref> [RN93, page 82]? * Metric PRO9: Type of function parameter declaration. <p> However, there are some techniques that are widely used and we will concentrate on these. 35 * Metric PSM4: The assert macro is used. * Metric PSM5: Lines of code per function <ref> [KP78, BM85] </ref>. * Metric PSM6: Variable count to lines of code ratio. This metric could identify those programmers who tend to avoid reusing variables, creating new variables for each loop control variable, etc. <p> (), socket (), etc. * Metric PSM12: Does the programmer rely on the internal representation of data objects? This metric would check for programmers relying on the size and byte order of integers, the size of floats, etc. * Metric PSM13: Do functions do "nothing" successfully? Kernighan and Plauger in <ref> [KP78, pages 111-114] </ref> and Jay Ranade and Alan Nash in [RN93, page 32] emphasize the need to make sure that there are no unexpected side effects in functions when these must "do nothing." In this context, functions that "do nothing" successfully are functions that correctly test for boundary conditions on their <p> The result of the program is undefined when both input strings are empty. However, we must note that it is an undecidable problem to determine the correctness of an arbitrary function [HU79]. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in <ref> [KP78] </ref> that "A comment is of zero (or negative) value if it is wrong". Ranade and Nash [RN93, page 89] devote a rule to the truth of every comment. Even if the comments were initially accurate, it is possible that during the maintenance cycle of a program they became inaccurate.
Reference: [KR85] <author> B. Kernighan and D. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Pren-tice Hall, </publisher> <year> 1985. </year> <month> 66 </month>
Reference-contexts: Here we would specifically search for the "#ifdef" keyword at the beginning of code lines. * Metric PRO7: Preference of either "while," "for" or "do" loops. All three can be used for the same purposes <ref> [KR85] </ref>. * Metric PRO8: Does the programmer use comments that are nearly an echo of the code [KP78, page 143] [RN93, page 82]? * Metric PRO9: Type of function parameter declaration.
Reference: [LC90] <author> A. Lake and C. Cook. </author> <title> STYLE: An automated program style ana-lyzer for Pascal. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 22(3) </volume> <pages> 29-33, </pages> <year> 1990. </year>
Reference-contexts: Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90b, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. 3.4 Software Metrics Categories All these sources give us ample material to select the metrics we will use.
Reference: [Led87] <author> Henry Ledgard. </author> <title> C With Excellence: Programming Proverbs. </title> <publisher> Hay-den Books, </publisher> <year> 1987. </year>
Reference-contexts: a chapter to programming style for im proving the readability of programs. * Jay Ranade and Alan Nash [RN93] give more than three hundred pages of style rules specifically for the "C" programming language. * Henry Ledgard gives a comprehensive list of "C" programming proverbs that contribute to programming excellence <ref> [Led87] </ref>. Many other sources have influenced our choice of metrics [LC90, BB89, OC90b, Coo87] but do not contain a specific set of rules, metrics or proverbs. 3.4 Software Metrics Categories All these sources give us ample material to select the metrics we will use.
Reference: [MB93] <author> R. Madison and M. Beaven. </author> <title> FORTRAN For Scientists and Engineers: Laboratory Manual. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Emacs, for example, encourages consistent indentation and curly bracket placement. Furthermore, many programmers learn their first programming language in university courses that impose a rigid and specific set of style rules regarding indentations, placement of comments and the alike <ref> [MB93] </ref>. 30 3.4.2 Programming Style Metrics Also useful are the metrics that deal with characteristics that are difficult to change automatically by pretty printers and code formatters, and are also related to the layout of the code.
Reference: [Mor91] <author> D. Moreaux. </author> <title> A formalism for the detection and prevention of illicit program derivations. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Idaho, </institution> <year> 1991. </year>
Reference-contexts: In commercial development projects, this is seldomly the case. 2.4 What Authorship Analysis is not 2.4.1 Plagiarism Detection It is important to realize that authorship analysis is markedly different from Plagiarism Detection. In <ref> [Mor91] </ref>, D. Moreaux defined Software Plagiarism as a general form of software theft, which can in turn be defined as the complete, partial or modified replication of software, with or without the permission of the original author.
Reference: [MW92] <editor> Merriam-Webster. </editor> <booktitle> Webster's 7th collegiate dictionary, </booktitle> <year> 1992. </year>
Reference-contexts: Chapter 3 introduces the methodology and experimental setup. Chapter 4 details the experiments performed and outlines the findings. Finally, Chapter 5 presents the conclusions and details the work that must be done in the future. 6 Chapter 2 Authorship Analysis 2.1 Definitions An Author is defined by Webster <ref> [MW92] </ref> as "one that writes or composes a literary work," or as "one who originates or creates." In the context of software development we adapt the definition of author to be: "one that originates or creates a piece of software." Authorship is then defined as, "the state of being an author."
Reference: [Nei63] <author> C. Neider. </author> <title> The Complete Essays of Mark Twain. </title> <publisher> Doubleday, </publisher> <year> 1963. </year>
Reference-contexts: not a poet's. * His will does not mention a single book, nor a poem nor any scrap of manuscript of any kind. * There is no other known specimen of his penmanship that can be proved his, except one poem that he wrote to be engraved on his tomb <ref> [Nei63] </ref>. Hundreds of books and essays have been written on this topic, some as early as 1837 [Dis37]. Especially interesting was W.
Reference: [NS86] <author> T. Naps and B. Singh. </author> <title> Introduction to Data Structures with Pascal. </title> <publisher> West Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: The routines needed to do the job in this application, create stack, pop, push and delete stack, are naturally short as can be seen in [Coo87, pages 374-375] and <ref> [NS86, page 100] </ref>. Program 2 has the highest values for this metric, which is logical because this program deals with I/O and user interaction. Metrics PSM1 and PSM2 also show large variations. However, for one programmer it gives useful information: programmer 3 never uses "void" function definitions.
Reference: [OC89] <author> P. Oman and C. Cook. </author> <title> Programming style authorship analysis. </title> <booktitle> In Seventeenth Anual ACM Computer Science Conference Proceedings, </booktitle> <pages> pages 320-326. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: they were written by the Earl of Oxford, he must, after taking the name of Shakespeare, have undergone several stylistic changes of a type and magnitude unknown in Shakespeare's accepted works. 2.2.2 Authorship Analysis With Programming Style The issue of identifying program authorship was explored by Cook and Oman in <ref> [OC89] </ref> as a means for determining instances of software theft and plagiarism. They briefly explored the use of software complexity metrics to define a relationship between programs and programmers, concluding that these are inadequate measures of stylistic factors and domain attributes. <p> However, if all of the o's in the sample have their centers filled in, that feature may identify the author." 15 This has a high degree of correlation with the identification of program authors using style analysis <ref> [OC89] </ref>. However, Software Forensics is really a superset of authorship analysis using style analysis because some of the measurements suggested by Spafford and Weeber [WS93] include, but are not limited to, some of the measurements made by Cook and Oman [OC89]. <p> correlation with the identification of program authors using style analysis <ref> [OC89] </ref>. However, Software Forensics is really a superset of authorship analysis using style analysis because some of the measurements suggested by Spafford and Weeber [WS93] include, but are not limited to, some of the measurements made by Cook and Oman [OC89]. <p> What we are trying to measure, for establishing the authorship of a program, are precisely some of these features. Hence, the term software metric, 27 or simply metric, is more appropriate to describe these special characteristics than the term "marker" used by Cook and Oman in <ref> [OC89] </ref> or the term "identifying feature" used by Spafford and Weeber [WS93]. 3.3 Sources for the Collection of Metrics We can collect metrics for authorship detection from a wide variety of sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base <p> if, for, while, do, case statements and the ? operator increases our decision count by one. * Metric PSM9: Is the goto keyword used? Surprisingly, software design ers and programmers still rely on these [BM85]. * Metric PSM10: Simple software complexity metrics offer little information that might be application independent <ref> [OC89] </ref>. The metrics that we could consider are: cyclomatic complexity number, program volume, complexity of data structures used, mean live variables per statement, and mean variable span [SCS86]. * Metric PSM11: Error detection after system calls that rarely fail. <p> Finally, we considered the following Programming Structure Metrics defined in Section 3.5.3: PSM1, PSM2, PSM3, PSM4, PSM5, PSM6, PSM7, PSM8, PSM9 and PSM14. We chose to exclude all metrics dealing with software complexity (metric PSM10) because there is evidence that these are ill suited for our purpose <ref> [OC89] </ref>. Metrics PSM11a and PSM11b were excluded because we cannot guarantee that all programs tested use such system calls and because it is impossible to detect if the error result of most system calls is being ignored without tracing the execution of the program. <p> Cluster analysis, as used by Oman and Cook in <ref> [OC89] </ref> can only be used if we discretize the values for our metrics. Unfortunately, it is difficult to find ranges for each of the metrics that could be used for any group of programmers without loss of accuracy.
Reference: [OC90a] <author> P. Oman and C. Cook. </author> <title> A taxonomy for programming style. </title> <booktitle> In Eighteenth Anual ACM Computer Science Conference Proceedings, </booktitle> <pages> pages 244-247. </pages> <publisher> ACM, </publisher> <year> 1990. </year>
Reference-contexts: However, because of the large number of rules and metrics available, we have decided to divide our metrics into three categories. Programming style, as shown by example by Kernighan and Plauger [KP78], Oman and Cook <ref> [OC90a] </ref> and Ranade and Nash [RN93], is a broad 29 term that encompasses a much greater universe than the one we have chosen in this paper.
Reference: [OC90b] <author> P. Oman and C. Cook. </author> <title> Typographic style is more than cosmetic. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 506-520, </pages> <year> 1990. </year>
Reference-contexts: Many other sources have influenced our choice of metrics <ref> [LC90, BB89, OC90b, Coo87] </ref> but do not contain a specific set of rules, metrics or proverbs. 3.4 Software Metrics Categories All these sources give us ample material to select the metrics we will use.
Reference: [OC91] <author> P. Oman and C. Cook. </author> <title> A programming style taxonomy. </title> <journal> Journal of Systems Software, </journal> <volume> 15(4) </volume> <pages> 287-301, </pages> <year> 1991. </year>
Reference-contexts: describe these special characteristics than the term "marker" used by Cook and Oman in [OC89] or the term "identifying feature" used by Spafford and Weeber [WS93]. 3.3 Sources for the Collection of Metrics We can collect metrics for authorship detection from a wide variety of sources: * Oman and Cook <ref> [OC91] </ref> collected a list of 236 style rules that can be used as a base for extracting metrics dealing with programming style. * Conte, Dunsmore and Shen [SCS86] give a comprehensive list of soft ware complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part
Reference: [Ott77] <author> K. Ottenstein. </author> <title> An algorithmic approach to the detection and prevention of plagiarism. </title> <journal> ACM SIGCSE Bulletin, </journal> <volume> 8(4) </volume> <pages> 30-41, </pages> <year> 1977. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in figure 2.4.
Reference: [RN93] <author> J. Ranade and A. Nash. </author> <title> The Elements of C Programming Style. </title> <publisher> McGraw-Hill Inc., </publisher> <year> 1993. </year> <month> 67 </month>
Reference-contexts: Van Tassel [Tas78] devotes a chapter to programming style for im proving the readability of programs. * Jay Ranade and Alan Nash <ref> [RN93] </ref> give more than three hundred pages of style rules specifically for the "C" programming language. * Henry Ledgard gives a comprehensive list of "C" programming proverbs that contribute to programming excellence [Led87]. <p> However, because of the large number of rules and metrics available, we have decided to divide our metrics into three categories. Programming style, as shown by example by Kernighan and Plauger [KP78], Oman and Cook [OC90a] and Ranade and Nash <ref> [RN93] </ref>, is a broad 29 term that encompasses a much greater universe than the one we have chosen in this paper. Programming style generally considers all our three categories. 3.4.1 Programming Layout Metrics We would like to examine those metrics that deal specifically with the layout of the program. <p> We don not examine include files or type declarations because there is no way of differentiating between those declarations that are imported from external modules, and those that are native to the programmer. 3.5.1 Programming Layout Metrics * Metric STY1: A vector of metrics indicating indentation style <ref> [RN93, pages 68-69] </ref>. For example, consider the styles shown in figure 3.4. Our metrics should distinguish among them. This vector of metrics will be composed of: 31 Metric STY1a: Indentation of C statements within surrounding blocks. Metric STY1b: Percentage of open curly brackets (f) that are alone in a line. <p> The vector will be composed of: Metric STY6a: Use of borders to highlight comments. Metric STY6b: Percentage of lines of code with inline comments. Metric STY6c: Ratio of lines of block style comments to lines of code. * Metric STY7: Ratio of white lines to lines of code <ref> [RN93, pages 70-71] </ref>. 3.5.2 Programming Style Metrics * Metric PRO1: Mean program line length (characters per line) [BM85]. * Metric PRO2: A vector of metrics that will consider name lengths. Metric PRO2a: Mean local variable name length. Metric PRO2b: Mean global variable name length. Metric PRO2c: Mean function name length. <p> This vector will consist of: Metric PRO3a: Some names use the underscore character. Metric PRO3b: Use of temporary variables 2 that are named XXX1, XXX2, etc. [KP78], or "tmp," "temp," "tmpXXX" or "tem pXXX" <ref> [RN93] </ref>. Metric PRO3c: Percentage of variable names that start with an uppercase letter. 2 It can be argued that all local variables are temporary and no global variable is temporary. <p> All three can be used for the same purposes [KR85]. * Metric PRO8: Does the programmer use comments that are nearly an echo of the code [KP78, page 143] <ref> [RN93, page 82] </ref>? * Metric PRO9: Type of function parameter declaration. <p> We would specifically be looking at identifiers or constants containing the words "debug" or "dbg" <ref> [RN93, pages38-53] </ref>. Figure 3.6 shows three common debugging styles in C. 3 Debugging is difficult. Many non standard techniques have been developed [RN93], and we cannot hope to identify all forms of debugging symbols. <p> We would specifically be looking at identifiers or constants containing the words "debug" or "dbg" [RN93, pages38-53]. Figure 3.6 shows three common debugging styles in C. 3 Debugging is difficult. Many non standard techniques have been developed <ref> [RN93] </ref>, and we cannot hope to identify all forms of debugging symbols. <p> rely on the internal representation of data objects? This metric would check for programmers relying on the size and byte order of integers, the size of floats, etc. * Metric PSM13: Do functions do "nothing" successfully? Kernighan and Plauger in [KP78, pages 111-114] and Jay Ranade and Alan Nash in <ref> [RN93, page 32] </ref> emphasize the need to make sure that there are no unexpected side effects in functions when these must "do nothing." In this context, functions that "do nothing" successfully are functions that correctly test for boundary conditions on their input parameters. <p> However, we must note that it is an undecidable problem to determine the correctness of an arbitrary function [HU79]. * Metric PSM14: Do comments and code agree? Kernighan and Plauger write in [KP78] that "A comment is of zero (or negative) value if it is wrong". Ranade and Nash <ref> [RN93, page 89] </ref> devote a rule to the truth of every comment. Even if the comments were initially accurate, it is possible that during the maintenance cycle of a program they became inaccurate. <p> Because the use of goto statements has been virtually banned from the academic community <ref> [Dij68, SCS86, RN93, Set89] </ref>, and because faculty members oppose their use in programming courses, we have chosen to eliminate metric PSM9 from further analysis. The probability of finding these statements in students code at Purdue University is negligible.
Reference: [RR83] <author> A. Ralston and E. Reilly. </author> <booktitle> Encyclopedia of Computer Science and Engineering. </booktitle> <publisher> Van Nostrand Reinhold Co., </publisher> <address> second edition, </address> <year> 1983. </year>
Reference-contexts: A program is a specification of a computation [Set89] or alternatively, a sequence of instructions that permit a computer to perform a particular task [Spe83]. A programming language is a notation for writing programs <ref> [RR83, Set89] </ref>. A programmer is a person who designs, writes and tests computer programs [Spe83]. In the fullest meaning of the term, a programmer will participate in the definition and specification of the problem itself, as well as the algorithms to be used in its solution. <p> He will then design the more detailed structure of the implementation, select a suitable programming language and related data structures and write and debug the necessary programs <ref> [RR83] </ref>. Programming style is a distinctive or characteristic manner present in those programs written by a programmer. A predicate is a propositional formula in which relational and quantified expressions can be used in place of prepositional variables.
Reference: [SAS] <institution> The SAS Institute. </institution> <note> SAS/STAT User's Guide. Volume 1, ANOVA-FREQ, fourth edition. </note>
Reference-contexts: Unfortunately, it is difficult to find ranges for each of the metrics that could be used for any group of programmers without loss of accuracy. The second statistical analysis method we can use, and the one chosen for our analysis, is discriminant analysis. This method, described in <ref> [SAS, JW88] </ref> is a multivariate technique concerned with separating observations and with allocating new observations into previously defined groups. 50 4.4 Preliminary Analysis and Elimination of Metrics Not all metrics calculated proved to be useful in further analysis. <p> Surprisingly, most of the metrics that showed large variations among programmers were eliminated as well. The performance of our statistical analysis with the remaining metrics was discouraging, with only twenty percent of the programs being classified correctly. The step discrimination tool provided by the SAS program <ref> [SAS] </ref> should theoretically be capable of eliminating bad metrics from the statistical base. Unfortunately, this tool was not helpful because it failed to eliminate any of the metrics from our set.
Reference: [SCS86] <author> H. Dunsmore S. Conte and V. Shen. </author> <title> Software Engineering Metrics and Models. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Many of the metrics we could use for identifying authorship in one of these programming languages will be of no use in the others. 3.2 Definition of Software Metrics The term "Software Metric" was defined by Conte, Dunsmore and Shen in <ref> [SCS86] </ref> as: Software metrics are used to characterize the essential features for software quantitatively, so that classification, comparison, and mathematical analysis can be applied. What we are trying to measure, for establishing the authorship of a program, are precisely some of these features. <p> Sources for the Collection of Metrics We can collect metrics for authorship detection from a wide variety of sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base for extracting metrics dealing with programming style. * Conte, Dunsmore and Shen <ref> [SCS86] </ref> give a comprehensive list of soft ware complexity metrics. * Kernighan and Plauger [KP78] give over seventy programming rules that should be part of "good" programming practice. 28 * D. <p> To simplify the computation of this metric, we have chosen to modify the definition of decision count as given in <ref> [SCS86] </ref>. We do not count each logical operator inside a test as a separate decision. <p> The metrics that we could consider are: cyclomatic complexity number, program volume, complexity of data structures used, mean live variables per statement, and mean variable span <ref> [SCS86] </ref>. * Metric PSM11: Error detection after system calls that rarely fail. Some programmers tend to ignore the error return values of system calls that are considered reliable [GS92, page 164]. <p> Because the use of goto statements has been virtually banned from the academic community <ref> [Dij68, SCS86, RN93, Set89] </ref>, and because faculty members oppose their use in programming courses, we have chosen to eliminate metric PSM9 from further analysis. The probability of finding these statements in students code at Purdue University is negligible.
Reference: [Set89] <author> R. Sethi. </author> <title> Programming Languages Concepts and Constructs. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Furthermore, some of these authors can take an existing work and add things to it, evolving the original creation. A program is a specification of a computation <ref> [Set89] </ref> or alternatively, a sequence of instructions that permit a computer to perform a particular task [Spe83]. A programming language is a notation for writing programs [RR83, Set89]. A programmer is a person who designs, writes and tests computer programs [Spe83]. <p> A program is a specification of a computation [Set89] or alternatively, a sequence of instructions that permit a computer to perform a particular task [Spe83]. A programming language is a notation for writing programs <ref> [RR83, Set89] </ref>. A programmer is a person who designs, writes and tests computer programs [Spe83]. In the fullest meaning of the term, a programmer will participate in the definition and specification of the problem itself, as well as the algorithms to be used in its solution. <p> Formal methods for defining the semantics, or connotative meaning, of programs and programming languages, such as axiomatic semantics 1 and denotational semantics <ref> [All86, Set89, Hoa69] </ref>, could be used to search for identifying patterns in program logic or semantic structure. Also, consider a programmer who codes with a top-down approach. <p> Because the use of goto statements has been virtually banned from the academic community <ref> [Dij68, SCS86, RN93, Set89] </ref>, and because faculty members oppose their use in programming courses, we have chosen to eliminate metric PSM9 from further analysis. The probability of finding these statements in students code at Purdue University is negligible.
Reference: [Spa89] <author> E. Spafford. </author> <title> The internet worm program. </title> <type> Technical Report CSD-TR-823, </type> <institution> Department of Computer Science. Purdue University, </institution> <year> 1989. </year>
Reference-contexts: Cliff Stoll's German hacker [Sto90] never feared prosecution precisely because of our inability to identify him even after tracking him down (the hacker had to be caught red-handed to be prosecuted). The author of the Internet Worm that attacked several hundred machines in the evening of November 2, 1988 <ref> [Spa89] </ref> might also have been identified quickly since he was a student and many samples of his programming style were readily available. Authors of computer viruses might also be identified if a piece of the source code is available. 5 1.4 Summary.
Reference: [Spe83] <author> D. Spencer. </author> <title> The Illustrated Computer Dictionary. </title> <publisher> Merrill Publishing Co., </publisher> <address> first edition, </address> <year> 1983. </year>
Reference-contexts: Furthermore, some of these authors can take an existing work and add things to it, evolving the original creation. A program is a specification of a computation [Set89] or alternatively, a sequence of instructions that permit a computer to perform a particular task <ref> [Spe83] </ref>. A programming language is a notation for writing programs [RR83, Set89]. A programmer is a person who designs, writes and tests computer programs [Spe83]. <p> A program is a specification of a computation [Set89] or alternatively, a sequence of instructions that permit a computer to perform a particular task <ref> [Spe83] </ref>. A programming language is a notation for writing programs [RR83, Set89]. A programmer is a person who designs, writes and tests computer programs [Spe83]. In the fullest meaning of the term, a programmer will participate in the definition and specification of the problem itself, as well as the algorithms to be used in its solution.
Reference: [Sto90] <author> C. Stoll. </author> <title> The Cuckoo's Egg. Pocket Books, </title> <booktitle> first edition, </booktitle> <year> 1990. </year>
Reference-contexts: Ultimately, we would like to find a signature for each individual programmer so that at any given point in time we could identify the ownership of any program. The implications of being able to find such a signature are enormous. Cliff Stoll's German hacker <ref> [Sto90] </ref> never feared prosecution precisely because of our inability to identify him even after tracking him down (the hacker had to be caught red-handed to be prosecuted).
Reference: [Tas78] <author> Dennie Van Tassel. </author> <title> Program Style, Design, Efficiency, Debugging, and Testing. </title> <publisher> Prentice Hall, </publisher> <year> 1978. </year>
Reference-contexts: Van Tassel <ref> [Tas78] </ref> devotes a chapter to programming style for im proving the readability of programs. * Jay Ranade and Alan Nash [RN93] give more than three hundred pages of style rules specifically for the "C" programming language. * Henry Ledgard gives a comprehensive list of "C" programming proverbs that contribute to programming
Reference: [Wha86] <author> G. Whale. Plague: </author> <title> Detection of plagiarism using program structure. </title> <booktitle> In Proceedings of the Ninth Australian Computer Science Conference, </booktitle> <pages> pages 231-241, </pages> <year> 1986. </year>
Reference-contexts: While plagiarism detection needs to detect the similarity between these two programs, authorship analysis does not. For the purpose of authorship identification, these two programs have distinct authors. Many people have devoted time and resources to the development of plagiarism detection <ref> [Ott77, Gri81, Jan88, Wha86] </ref>, and a comprehensive analysis of their work is beyond the scope of this paper. We can, however give a simple example that will illustrate how measurements traditionally used for plagiarism detection are ill suited to authorship analysis. Consider the two functions shown in figure 2.4.
Reference: [WS90] <author> Larry Wall and Randal Schwartz. </author> <title> Programming Perl. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <address> first edition, </address> <year> 1990. </year>
Reference-contexts: only software complexity metrics, we developed a series of programs designed specifically for our purpose and ran them through a series of controlled experiments. 3.6.1 C Source Code Analyzer Although languages such as Awk can be used for extracting all the necessary metrics [BM85], we avoided the Awk and Perl <ref> [WS90] </ref> programming languages for calculating most of the metrics dealing with programming structure because existing tools in compiler construction make it easier to write C code, 39 and our programs benefit from the reuse of code that was already tested.
Reference: [WS93] <author> Stephen A. Weeber and Eugene H. Spafford. </author> <title> Software forensics: Can we track code to its authors? Computers & Security, </title> <booktitle> 12(6) </booktitle> <pages> 585-595, </pages> <month> December </month> <year> 1993. </year> <month> 68 </month>
Reference-contexts: They theorize that this technique, called Software Forensics, could be used to examine and analyze software in any form; be it source code for any language or executable images <ref> [WS93] </ref>. The authorship of a program might well be proven beyond any reasonable doubt by the results of such analysis if there exists a large enough statistical 13 base to support our comparisons. If not, they might provide hints about where more serious investigation should be performed. <p> Spafford and Weeber write in <ref> [WS93] </ref> the following of software forensics: "...would be similar to the use of handwriting analysis by law enforcement officials to identify the authors of documents involved in crimes, or to provide confirmation of the role of a suspect. Handwriting analysis involves identifying features of the writing in question. <p> However, Software Forensics is really a superset of authorship analysis using style analysis because some of the measurements suggested by Spafford and Weeber <ref> [WS93] </ref> include, but are not limited to, some of the measurements made by Cook and Oman [OC89]. <p> While this might be possible, it should be left to the researchers in psychology departments. We also believe that it is possible to determine something about the background of a programmer by looking at his code <ref> [WS93] </ref>. Faculty members and teaching assistants at Purdue University agree that undergraduate students and electrical engineers are notorious for abusing 3 hashing, Lisp programmers 3 Abuse in this context refers to using a data structure that is inappropriate or unreasonably expensive. <p> Sorting with linked lists or hashing techniques where keys have a high degree of collisions are examples of such abuses. 19 are notorious for abusing linked-list data structures, and native Fortran pro-grammers prefer using short lines. While all this information might be useful in forensic analysis <ref> [WS93] </ref>, it is beyond the scope of our study. 2.5 A Simple Example As a simple example of the differences in programming style among programmers, consider the programs shown in figures 2.5, 2.6, 2.7 and 2.8. These are variations on a program written by graduate students at Purdue University. <p> Hence, the term software metric, 27 or simply metric, is more appropriate to describe these special characteristics than the term "marker" used by Cook and Oman in [OC89] or the term "identifying feature" used by Spafford and Weeber <ref> [WS93] </ref>. 3.3 Sources for the Collection of Metrics We can collect metrics for authorship detection from a wide variety of sources: * Oman and Cook [OC91] collected a list of 236 style rules that can be used as a base for extracting metrics dealing with programming style. * Conte, Dunsmore and
References-found: 47

