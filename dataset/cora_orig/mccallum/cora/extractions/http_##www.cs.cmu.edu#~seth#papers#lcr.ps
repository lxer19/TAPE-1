URL: http://www.cs.cmu.edu/~seth/papers/lcr.ps
Refering-URL: http://www.cs.cmu.edu/~seth/papers.html
Root-URL: 
Abstract: ENABLING PRIMITIVES FOR COMPILING PARALLEL LANGUAGES Seth Copen Goldstein, Klaus Erik Schauser*, David Culler Computer Science Division, University of California at Berkeley fsethg,cullerg@cs.berkeley.edu *Department of Computer Science, University of California at Santa Barbara schauser@cs.ucsb.edu ABSTRACT This paper presents three novel language implementation primitiveslazy threads, stacklets, and synchronizersandshows how they combine to provide a parallel call at nearly the efficiency of a sequential call. The central idea is to transform parallel calls into parallel-ready sequential calls. Excess parallelism degrades into sequential calls with the attendant efficient stack management and direct transfer of control and data, unless a call truly needs to execute in parallel, in which case it gets its own thread of control. We show how these techniques can be applied to distribute work efficiently on multiprocessors. 1 INTRODUCTION Many modern parallel languages provide methods for dynamically creating multiple independent threads of control, such as forks, parallel calls, futures [15], object methods, and non-strict evaluation of argument expressions [17, 12]. Generally, these threads describe the logical parallelism in the program. The programming language implementation maps this dynamic collection of threads onto the fixed set of physical processors executing the program, either by providing its own language-specific scheduling mechanisms or by using a general threads package. These languages stand in contrast to languages with a single logical thread of control, such as Fortran90, or a fixed set of threads, such as Split-C. There are many reasons to have the logical parallelism of the program exceed the physical parallelism of the machine, including ease of expressing parallelism and better utilization in the presence of synchronization delays [16, 25], load imbalance, and long communication latency. Moreover, the semantics of the language or the synchronization primitives may allow dependencies to be expressed in such a way that progress can be made only by interleaving multiple threads, effectively running them in parallel even on a single processor. 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. W. Appel. </author> <title> Compiling with continuations. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Instead of passing the child two return addresses, the parent calls the child with a single address from which it can derive both addresses. At the implementation level, this use of multiple return addresses can be thought of as an extended version of continuation passing <ref> [1] </ref>, where the child is passed two different continuations, one for normal return and one for suspension. The compiler ensures that the suspension entry point precedes the normal return entry point by a fixed number of instructions. <p> As shown in Figure 8, a thread seed is a code fragment with three entry points: one for child return, one for child suspension, and one for an external work-stealing request. At the implementation level, a thread seed can be thought of as an extended version of continuation passing <ref> [1] </ref>, where the child is passed three different continuations, one for each of the three cases. When the compiler determines that there is work that could be run in parallel, it creates a thread seed which represents the work.
Reference: [2] <author> Arvind and D. E. Culler. </author> <title> Dataflow architectures. </title> <booktitle> In Annual Reviews in Computer Science, </booktitle> <volume> volume 1, </volume> <pages> pages 225-253. </pages> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1986. </year>
Reference-contexts: existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations [7, 19, 16, 25, 23, 20, 10], and direct hardware support for fine-grained parallel execution <ref> [13, 2] </ref>. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism.
Reference: [3] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, P. Lisiecki, K. H. Randall, A. Shaw, and Y. Zhou. </author> <title> Cilk 1.1 reference manual. </title> <institution> MIT Lab for Comp. Sci., 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk <ref> [3] </ref>, Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [4] <author> M.C. Carlisle, A. Rogers, J.H. Reppy, and L.J. Hendren. </author> <title> Early experiences with Olden (parallel programming). </title> <booktitle> In Languages and Compilers for Parallel Computing. 6th International Workshop Proceedings, </booktitle> <pages> pages 1-20. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden <ref> [4] </ref>, and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole. <p> Unlike the techniques we use, it restricts the behavior of the program in an attempt to reduce the cost of futures. We use stacklets for efficient stack-based frame allocation in parallel programs. Previous work in [10] developed similar ideas for handling continuations efficiently. Olden <ref> [4] </ref> uses a spaghetti stack. In both systems, the allocation of a new stack frame always requires memory references and a garbage collector. The way thread seeds encode future work builds on the use of multiple offsets from a single return address to handle special cases. <p> This technique was used in SOAR [22]. It was also applied to Self, which uses parent controlled return continuations to handle debugging [11]. We extend these two ideas to form synchronizers. Building on LTC, Olden <ref> [20, 4] </ref> applies similar techniques for the automatic paral-lelization of programs using dynamic data structures. Of the systems mentioned so far, Olden's integration is closest to ours. Finally, user-level thread packages are still not as lightweight as many of the systems mentioned above.
Reference: [5] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: compositional parallel programming. </title> <booktitle> In Languages and Compilers for Parallel Computing. 5th International Workshop Proceedings, </booktitle> <pages> pages 124-44. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ <ref> [5] </ref>, Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [6] <author> E. C. Cooper and R. P. Draves. C-Threads. </author> <type> Technical Report CMU-CS-88-154, </type> <institution> Carnegie-Mellon University, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages <ref> [8, 21, 6] </ref>, compiler techniques and clever run-time representations [7, 19, 16, 25, 23, 20, 10], and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g.
Reference: [7] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> TAM a compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 347-370, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: A lazy thread fork is performed exactly like a sequential call; control 1 This is similar to what is provided in many kernel threads packages. Our threads, however, are stronger than those in TAM <ref> [7] </ref> and in some user-level threads packages, e.g. Chorus [21], which require that the maximum stack size be specified upon thread creation so that memory can be preallocated. 4 Chapter 1 and data are transferred to the child and the call is made on the parent stack. <p> By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 <ref> [7, 19] </ref>, CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole. <p> Our work grew out of previous efforts to implement the non-strict functional language Id90 for commodity parallel machines. Our earlier work developed a Threaded Abstract Machine (TAM) which serves as an intermediate compilation target <ref> [7] </ref>. The two key differences between this work and TAM are that under TAM calls are always Enabling Primitives for Compiling Parallel Languages 5 parallel, and due to TAM's scheduling hierarchy, calling another function does not immediately transfer control. <p> The programs are described in <ref> [7] </ref>. the seed version. The reason is that our stacklet management code does not use register windows, which introduce a high overhead on the Sparc. For a fair comparison we wrote an assembly version of Fib that also does not use register windows.
Reference: [8] <author> J.E. Faust and H.M. Levy. </author> <title> The performance of an object-oriented threads package. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 278-88, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages <ref> [8, 21, 6] </ref>, compiler techniques and clever run-time representations [7, 19, 16, 25, 23, 20, 10], and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g.
Reference: [9] <author> S. C. Goldstein, K. E. Schauser, and D. E. Culler. </author> <title> Lazy Threads, Stacklets, and Synchronizers: Enabling primitives for compiling parallel languages. </title> <type> Technical report, </type> <institution> University of California at Berkeley, </institution> <year> 1995. </year>
Reference-contexts: The amount of code duplicated is small since we need to copy only the code fragments that deal with returned results. This allows us to combine the return with synchronization at no additional run-time cost. For full details see <ref> [9] </ref>. that we have two threads, T 1 and T 2 , which return to the code fragments A and B, respectively, in the parent. <p> At this time our Id90 compiler uses a primitive version of explicit seed creation. In addition to the primitives described so far, the compiler uses strands, a mechanism to support fine-grained parallelism within a thread <ref> [9] </ref>. We see a performance improvement ranging from 1.1 times faster for coarse-grained programs like blocked matrix multiply (MMT) to 2.7 times faster for finer-grained programs. We expect an additional benefit of up to 30% when the compiler generates code using synchronizers.
Reference: [10] <author> R. Hieb, R. Kent Dybvig, and C. Bruggeman. </author> <title> Representing control in the presence of first-class continuations. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 66-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> Another proposed technique for improving LTC is leapfrogging [25]. Unlike the techniques we use, it restricts the behavior of the program in an attempt to reduce the cost of futures. We use stacklets for efficient stack-based frame allocation in parallel programs. Previous work in <ref> [10] </ref> developed similar ideas for handling continuations efficiently. Olden [4] uses a spaghetti stack. In both systems, the allocation of a new stack frame always requires memory references and a garbage collector.
Reference: [11] <author> U. H olzle, C. Chambers, and D. Ungar. </author> <title> Debugging optimized code with dynamic deoptimization. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 32-43, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The way thread seeds encode future work builds on the use of multiple offsets from a single return address to handle special cases. This technique was used in SOAR [22]. It was also applied to Self, which uses parent controlled return continuations to handle debugging <ref> [11] </ref>. We extend these two ideas to form synchronizers. Building on LTC, Olden [20, 4] applies similar techniques for the automatic paral-lelization of programs using dynamic data structures. Of the systems mentioned so far, Olden's integration is closest to ours.
Reference: [12] <author> P. Hudak, S. Peyton Jones, P. Walder, B. Boutel, J. Fairbairn, J. Fasel, M.M. Guzman, K. Hammond, J. Hughes, T. Johnsson, D. Kieburtz, R. Nikhil, W. Partain, and J. Peterson. </author> <title> Report on the programming language Haskell: a non-strict, purely functional language (version 1.2). </title> <journal> SIGPLAN Notices, vol.27 (no.5): Ri-Rx, </journal> <volume> Rl-R163, </volume> <month> May </month> <year> 1992. </year> <note> 16 Chapter 1 </note>
Reference: [13] <author> H. F. Jordan. </author> <title> Performance measurement on HEP a pipelined MIMD computer. </title> <booktitle> In Proc. of the 10th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Stockholm, Sweden, </address> <month> June </month> <year> 1983. </year>
Reference-contexts: existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations [7, 19, 16, 25, 23, 20, 10], and direct hardware support for fine-grained parallel execution <ref> [13, 2] </ref>. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism.
Reference: [14] <author> L.V. Kale and S. Krishnan. CHARM++: </author> <title> a portable concurrent object oriented system based on C++. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 91-108, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm <ref> [14] </ref>, Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [15] <author> D.A. Kranz, R.H. Halstead Jr., and E. Mohr. Mul-T: </author> <title> a high-performance parallel Lisp. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 81-90, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T <ref> [15] </ref>, Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [16] <author> E. Mohr, D.A. Kranz, and R.H. Halstead Jr. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol.2 (no.3): </volume> <pages> 264-80, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> In the framework of previous work it allows excess parallelism to degrade efficiently into a sequential call. Many other researchers have proposed schemes which deal lazily with excess parallelism. Our approach builds on lazy task creation (LTC) which maintains a data structure to record previously encountered parallel calls <ref> [16] </ref>. When a processor runs out of work, dynamic load balancing can be effected by stealing previously created lazy tasks from other processors. These ideas were studied for Mul-T running on shared-memory machines. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. Next, we look at the efficiency of work-stealing combined with seeds on a parallel machine by examining the performance of the synthetic benchmark proposed in <ref> [16] </ref> and also used in [25]. Grain is a doubly recursive program that computes a sum, but each leaf executes a loop of g instructions, allowing us to control the granularity of the leaf nodes. We compare its efficiency to the sequential C code compiled by gcc. <p> Most of the inefficiency comes from the need to poll the CM-5 network. The speed-up curve in Figure 9 shows that even for very fine-grained programs, the thread seeds successfully exploit the entire machine. 14 Chapter 1 tation as a function of granularity. We use the synthetic benchmark Grain <ref> [16, 25] </ref>. 7 SUMMARY We have shown that by integrating a set of innovative techniques for call frame management, call/return linkage, and thread generation we can provide a fast parallel call which obtains nearly the full efficiency of a sequential call when the child thread executes locally and runs to completion
Reference: [17] <author> R. S. Nikhil. </author> <title> Id (version 88.0) reference manual. </title> <type> Technical Report CSG Memo 284, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1988. </year>
Reference: [18] <author> Rishiyur S. Nikhil. Cid: </author> <title> A parallel, shared memory C for distributed-memory machines. </title> <booktitle> In Languages and Compilers for Parallel Computing. 7th International Workshop Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid <ref> [18] </ref>. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [19] <author> R.S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Languages and Compilers for Parallel Computing. 6th International Workshop Proceedings, </booktitle> <pages> pages 390-405. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 <ref> [7, 19] </ref>, CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. The common goal is to reduce the overhead associated with managing the logical parallelism. While much of this work overlaps ours, none has combined the techniques described in this paper into an integrated whole.
Reference: [20] <author> A. Rogers, J. Reppy, and L. Hendren. </author> <title> Supporting SPMD execution for dynamic data structures. </title> <booktitle> In Languages and Compilers for Parallel Computing. 5th International Workshop Proceedings, </booktitle> <pages> pages 192-207. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> This technique was used in SOAR [22]. It was also applied to Self, which uses parent controlled return continuations to handle debugging [11]. We extend these two ideas to form synchronizers. Building on LTC, Olden <ref> [20, 4] </ref> applies similar techniques for the automatic paral-lelization of programs using dynamic data structures. Of the systems mentioned so far, Olden's integration is closest to ours. Finally, user-level thread packages are still not as lightweight as many of the systems mentioned above.
Reference: [21] <author> M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemont, F. Herrman, C. Kaiser, S. Langlois, P. Leonard, and W. Neuhauser. </author> <title> Overview of the CHORUS distributed operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 39-69. </pages> <publisher> USENIX Assoc, </publisher> <year> 1992. </year>
Reference-contexts: A lazy thread fork is performed exactly like a sequential call; control 1 This is similar to what is provided in many kernel threads packages. Our threads, however, are stronger than those in TAM [7] and in some user-level threads packages, e.g. Chorus <ref> [21] </ref>, which require that the maximum stack size be specified upon thread creation so that memory can be preallocated. 4 Chapter 1 and data are transferred to the child and the call is made on the parent stack. <p> By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages <ref> [8, 21, 6] </ref>, compiler techniques and clever run-time representations [7, 19, 16, 25, 23, 20, 10], and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g.
Reference: [22] <author> David M. Ungar. </author> <title> The design and evaluation of a high performance Smalltalk system. </title> <publisher> ACM distinguished dissertations. MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: In both systems, the allocation of a new stack frame always requires memory references and a garbage collector. The way thread seeds encode future work builds on the use of multiple offsets from a single return address to handle special cases. This technique was used in SOAR <ref> [22] </ref>. It was also applied to Self, which uses parent controlled return continuations to handle debugging [11]. We extend these two ideas to form synchronizers. Building on LTC, Olden [20, 4] applies similar techniques for the automatic paral-lelization of programs using dynamic data structures.
Reference: [23] <author> M.T. Vandevoorde and E.S. Roberts. WorkCrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, vol.17 (no.4): </journal> <pages> 347-66, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18].
Reference: [24] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In the case of a remote fork, the stub handler uses indirect active messages <ref> [24] </ref> to return data and control to the parent's message handler, which in turn has responsibility for integrating the data into the parent frame and indicating to the parent that its child has returned. 2.2 Compilation To reduce the cost of frame allocation even further we construct a call graph which
Reference: [25] <author> D.B. Wagner and B.G. Calder. </author> <title> Leapfrogging: a portable technique for implementing efficient futures. </title> <journal> In SIGPLAN Notices, </journal> <pages> pages 208-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: By manipulating the existing indirect return jump, conditional tests for synchronization and special cases can be avoided. 1.2 Related Work Attempts to accommodate logical parallelism have include thread packages [8, 21, 6], compiler techniques and clever run-time representations <ref> [7, 19, 16, 25, 23, 20, 10] </ref>, and direct hardware support for fine-grained parallel execution [13, 2]. These approaches have been used to implement many parallel languages, e.g. Mul-T [15], Id90 [7, 19], CC++ [5], Charm [14], Cilk [3], Olden [4], and Cid [18]. <p> LTC also depends on a garbage collector, which hides many of the costs of stack management. Finally, while earlier systems based on LTC relied on shared-memory hardware capabilities, our implementation works on both distributed-and shared-memory systems. Another proposed technique for improving LTC is leapfrogging <ref> [25] </ref>. Unlike the techniques we use, it restricts the behavior of the program in an attempt to reduce the cost of futures. We use stacklets for efficient stack-based frame allocation in parallel programs. Previous work in [10] developed similar ideas for handling continuations efficiently. Olden [4] uses a spaghetti stack. <p> We expect an additional benefit of up to 30% when the compiler generates code using synchronizers. Next, we look at the efficiency of work-stealing combined with seeds on a parallel machine by examining the performance of the synthetic benchmark proposed in [16] and also used in <ref> [25] </ref>. Grain is a doubly recursive program that computes a sum, but each leaf executes a loop of g instructions, allowing us to control the granularity of the leaf nodes. We compare its efficiency to the sequential C code compiled by gcc. <p> Most of the inefficiency comes from the need to poll the CM-5 network. The speed-up curve in Figure 9 shows that even for very fine-grained programs, the thread seeds successfully exploit the entire machine. 14 Chapter 1 tation as a function of granularity. We use the synthetic benchmark Grain <ref> [16, 25] </ref>. 7 SUMMARY We have shown that by integrating a set of innovative techniques for call frame management, call/return linkage, and thread generation we can provide a fast parallel call which obtains nearly the full efficiency of a sequential call when the child thread executes locally and runs to completion
Reference: [26] <author> Akinori Yonezawa. </author> <title> ABCL- an object-oriented concurrent system . MIT Press series in computer systems. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
References-found: 26

