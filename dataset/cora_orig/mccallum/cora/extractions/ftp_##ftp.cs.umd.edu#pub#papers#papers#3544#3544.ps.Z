URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3544/3544.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: keleher@cs.umd.edu  
Title: Sparks: Coherence as an Abstract Type  
Author: Pete Keleher 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: We are currently designing Sparks, a protocol construction library that we hope will allow us to improve the performance of DSM systems to within a few percent of tightly-coupled multiprocessors. Sparks' abstractions will allow us to cleanly and systematically explore the design space of high-level synchronization operations, rather than proposing and implementing new operations in an ad hoc fashion. Sparks' basic abstraction is the coherence history, an object that summarizes past coherence actions to shared segments. Our emphasis here is more on creating and investigating the abstractions that make a broad variety of optimizations possible, rather than on the individual optimizations themselves. However, we will thoroughly quantify the performance gains allowed by the synchronization types created via the Sparks library. Our overall goal is to improve DSM performance. We will gauge our success by targeting applications from benchmark suites such as SPLASH-2, as well as representative applications from computational chemistry, biology, and satellite image analysis. Sparks' history abstraction will be used to make several important contributions towards our performance goal: (1) efficient techniques to implement high-level synchronization, (2) efficient automatic prefetching using prefetch playbacks, and (3) external interfaces to run-time libraries and automatically parallelized code sections. By improving DSM efficiency, we hope to make the shared memory paradigm more appealing, and therefore useful, to the research community. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: TreadMarks is proof that user-level DSMs are capable of efficiently running applications that were written for tightly-coupled multiprocessors, without requiring a different programming model or user annotations. 3 Sparks: Abstract Type Support for Coherence DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 3, 9, 6] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them. <p> In a page-based DSM like CVM, a segment consists of a set of pages. However, segments could also be composed of arbitrarily-shaped objects in distributed object systems such as Midway <ref> [1] </ref>, CRL [6], or Emerald [2]. The thread extent names the set of threads whose write notices may be contained in the history. Usually this includes all threads in a system. For example, the thread extent of H 3 is P 1 , P 2 , and P 3 .
Reference: [2] <author> A. Black, N. Hutchinson, E. Jul, H. Levy, and L. Carter. </author> <title> Distribution and abstract types in Emerald. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):65-74, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: In a page-based DSM like CVM, a segment consists of a set of pages. However, segments could also be composed of arbitrarily-shaped objects in distributed object systems such as Midway [1], CRL [6], or Emerald <ref> [2] </ref>. The thread extent names the set of threads whose write notices may be contained in the history. Usually this includes all threads in a system. For example, the thread extent of H 3 is P 1 , P 2 , and P 3 .
Reference: [3] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: TreadMarks is proof that user-level DSMs are capable of efficiently running applications that were written for tightly-coupled multiprocessors, without requiring a different programming model or user annotations. 3 Sparks: Abstract Type Support for Coherence DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 3, 9, 6] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them.
Reference: [4] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22 </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Second, our technique will be used for prefetching, not to maintain coherence. We will not violate correctness if subsequent iterations access different data. 3.7 Compiler/Runtime Library Interfaces We will use Sparks to generate interfaces to code created by the SUIF [16] parallelizing compiler, and to the CHAOS <ref> [4] </ref> runtime library. Our collaboration with Dr. Tseng's compiler group [16] will use communication analysis to determine when data will be needed by other processors. <p> By combining this information with standard dataflow and dependence analysis, the compiler can initiate asynchronous data updates and overlap communication with computation. 9 Similar work is being pursued in collaboration with Dr. Saltz's CHAOS <ref> [4] </ref> group. The general approach is to create mechanisms that let CHAOS assume sole responsibility for consistency in a confined region of shared space. The same mechanism is later used to turn the default consistency management back on.
Reference: [5] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Unfortunately, the latencies for global operations in either environment are several orders of magnitude more expensive than on tightly-coupled multiprocessors. As a result, early DSMs performed well for only a restricted class of applications. Previous work <ref> [5, 7] </ref> addressed part of the problem by proposing weak memory consistency models. These memory models allow processors' views of shared memory to temporarily diverge, bringing them back into agreement only at subsequent synchronization. <p> As part of my dissertation research, I defined lazy release consistency (LRC) [7], a close relation to the eager release consistency (ERC) <ref> [5] </ref> memory model. DSMs that implement ERC delay propagating modifications of shared data until they execute a release (see Figure 2), and then the modifications are performed globally. Under LRC protocols, processors further delay performing modifications remotely until subsequent acquires by other processors.
Reference: [6] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> To appear in The Proceedings of the 15th ACM Symposium on Operating Systems Principles. </booktitle>
Reference-contexts: TreadMarks is proof that user-level DSMs are capable of efficiently running applications that were written for tightly-coupled multiprocessors, without requiring a different programming model or user annotations. 3 Sparks: Abstract Type Support for Coherence DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 3, 9, 6] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them. <p> In a page-based DSM like CVM, a segment consists of a set of pages. However, segments could also be composed of arbitrarily-shaped objects in distributed object systems such as Midway [1], CRL <ref> [6] </ref>, or Emerald [2]. The thread extent names the set of threads whose write notices may be contained in the history. Usually this includes all threads in a system. For example, the thread extent of H 3 is P 1 , P 2 , and P 3 .
Reference: [7] <author> P. Keleher. </author> <title> Distributed Shared Memory Using Lazy Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: Unfortunately, the latencies for global operations in either environment are several orders of magnitude more expensive than on tightly-coupled multiprocessors. As a result, early DSMs performed well for only a restricted class of applications. Previous work <ref> [5, 7] </ref> addressed part of the problem by proposing weak memory consistency models. These memory models allow processors' views of shared memory to temporarily diverge, bringing them back into agreement only at subsequent synchronization. <p> While early systems strictly emulated the sequentially consistent [13] programming model of tightly-coupled multiprocessors, most recent systems support relaxed consistency models that produce identical results for most applications, but allow the use of many performance optimizations. As part of my dissertation research, I defined lazy release consistency (LRC) <ref> [7] </ref>, a close relation to the eager release consistency (ERC) [5] memory model. DSMs that implement ERC delay propagating modifications of shared data until they execute a release (see Figure 2), and then the modifications are performed globally.
Reference: [8] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: By applying one node's history at another node, the second node's view of shared state is brought up to date with respect to events seen by the first. More formally, a history is a partially ordered set of intervals <ref> [8] </ref>, where an interval describes an interval of time for a single processor. Intervals contain write notices, which are generally just indications that a given page has been modified. Applying such a notice invalidates the associated page.
Reference: [9] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: TreadMarks is proof that user-level DSMs are capable of efficiently running applications that were written for tightly-coupled multiprocessors, without requiring a different programming model or user annotations. 3 Sparks: Abstract Type Support for Coherence DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 3, 9, 6] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them.
Reference: [10] <author> Pete Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <type> Technical Report CS-TR-3543, </type> <institution> University of Maryland, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: The access misses of Jacobi, a coarse-grained application, can be completely covered with simple analysis. Data access latencies directly account for 17% of the runtime for Water on top of CVM on an eight-node SP-2, and indirectly account for more through synchronization delays and load imbalance <ref> [10] </ref>. Since our experience indicates that access miss latency is at least as important as synchronization latency, we expect prefetch mechanisms to provide significant performance benefits. The above routines begin record and end record can be used to cleanly record data creation.
Reference: [11] <author> Povl T. Koch, Robert J. Fowler, and Eric Jul. </author> <title> Message-driven relaxed consistency in a software distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 75-86, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In general, applying unnecessary coherence operations can waste bandwidth, create extra CPU overhead, and cause unnecessary page faults. The Sparks class library can be used to build high level synchronization objects that accurately reflect the synchronization objects' coherence semantics. Our approach is related to the causality annotations of CarlOS <ref> [11] </ref>, but Sparks will provide a much richer set of mechanisms and finer control over the scope of consistency actions. Sparks will replace the top layer of CVM.
Reference: [12] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [13] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The DSM layer traps page faults and satisfies them by fetching data across the network. While early systems strictly emulated the sequentially consistent <ref> [13] </ref> programming model of tightly-coupled multiprocessors, most recent systems support relaxed consistency models that produce identical results for most applications, but allow the use of many performance optimizations.
Reference: [14] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the 1995 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: During the next iteration, newly created data is pushed to the pid returned by produce barrier () while waiting for the barrier to complete. Recording and playing back data transfers was first used by the Mukherjee <ref> [14] </ref> in the context of a sequentially consistent DSM. Our work differs in two ways. First, our recording mechanisms will be part of the synchronization type definitions. The playbacks will be initiated by automatic heuristics, making them more reliable and easier to apply.
Reference: [15] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [16] <author> S. Tjiang, M. E. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrating scalar optimization and parallelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, California, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Second, our technique will be used for prefetching, not to maintain coherence. We will not violate correctness if subsequent iterations access different data. 3.7 Compiler/Runtime Library Interfaces We will use Sparks to generate interfaces to code created by the SUIF <ref> [16] </ref> parallelizing compiler, and to the CHAOS [4] runtime library. Our collaboration with Dr. Tseng's compiler group [16] will use communication analysis to determine when data will be needed by other processors. <p> We will not violate correctness if subsequent iterations access different data. 3.7 Compiler/Runtime Library Interfaces We will use Sparks to generate interfaces to code created by the SUIF <ref> [16] </ref> parallelizing compiler, and to the CHAOS [4] runtime library. Our collaboration with Dr. Tseng's compiler group [16] will use communication analysis to determine when data will be needed by other processors. By combining this information with standard dataflow and dependence analysis, the compiler can initiate asynchronous data updates and overlap communication with computation. 9 Similar work is being pursued in collaboration with Dr.
Reference: [17] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-37, </pages> <month> June </month> <year> 1995. </year> <month> 11 </month>
Reference-contexts: Page-based DSMs have no explicit association between data and the synchronization used to guard it. However, a given program usually obeys a fairly simple mapping between the two. Our trace-driven simulation shows that 81% of all access misses on shared data in Water <ref> [17] </ref>, a relatively complicated molecular simulation, can be avoided by replaying data transfers. The access misses of Jacobi, a coarse-grained application, can be completely covered with simple analysis.
References-found: 17

