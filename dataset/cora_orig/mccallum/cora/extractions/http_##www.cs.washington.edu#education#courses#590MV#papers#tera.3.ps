URL: http://www.cs.washington.edu/education/courses/590MV/papers/tera.3.ps
Refering-URL: http://www.cs.washington.edu/education/courses/590MV/
Root-URL: 
Title: Scheduling on the Tera MTA  
Author: Gail Alverson Simon Kahan Richard Korry Cathy McCann Burton Smith 
Address: Seattle, Washington USA  
Affiliation: Tera Computer Company  
Abstract: This paper describes the scheduling issues specific to the Tera MTA high performance shared memory multithreaded multiprocessor and presents solutions to classic scheduling problems. The Tera MTA exploits parallelism at all levels, from fine-grained instruction-level parallelism within a single processor to parallel programming across processors, to multiprogramming among several applications simultaneously. Consequently, scheduling of resources occurs at many levels, and managing these resources poses unique and challenging scheduling concerns. This paper outlines the scheduling algorithms of the user level runtime and operating system and describes the issues relevant to each. Many of the issues encountered and solutions proposed are novel, given the multithreaded, multiprogrammed nature of our architecture. In particular, we present an algorithm for swapping a set of tasks to and from memory that achieves minimal overhead, largely independent of the order in which tasks are swapped. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In 1990 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Section 6 describes the issues and policies involved in memory scheduling, and Section 7 discusses the processor scheduler. Conclusions are provided in section 8. 2 Tera MTA System Architecture The Tera MTA architecture <ref> [1] </ref> implements a physically shared-memory multiprocessor with multi-stream processors and interleaved memory units interconnected by a packet-switched interconnection network. The network provides a bisection bandwidth that supports a request and a response from each processor to a random memory location in each clock tick.
Reference: [2] <author> I. Ashok and J. Zahorjan. </author> <title> Scheduling a mixed interactive and batch workload on a parallel,shared memory supercomputer. </title> <booktitle> In Supercomputing 1992, </booktitle> <month> Nov </month> <year> 1992. </year>
Reference-contexts: Because large memory and highly parallel tasks have very different characteristics from small memory interactive tasks, we propose mechanisms for scheduling memory and processors that differentiate these two classes of workload. Studies of interactive and batch workloads by Ashok and Zahorjan <ref> [2] </ref> support this differentiation. The memory and processing resources are thus divided between large, batch oriented tasks and short, interactive tasks. Memory and Processor Schedulers. Two memory schedulers are employed, the MB-scheduler for large (big) memory tasks and the ML-scheduler for small memory (little) tasks. <p> Similarly, a more flexible partition could be implemented, where the amount of memory reserved for the small memory tasks varies with the changing load exerted on the system by the batch and interactive tasks. However, research by Ashok and Zahorjan <ref> [2] </ref> suggest that the benefits of such dynamic partitions are not easily attained. 13 7 Processor Scheduler Early research on processor scheduling for parallel machines highlighted the need for co-scheduling [16]; that is, ensuring that all parallel activities of a job execute at once.
Reference: [3] <author> M. J. Bach. </author> <title> The Design of the Unix Operating Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1986. </year>
Reference-contexts: This allows scheduling of single-team tasks on a per processor basis by independent instances of the PL-scheduler. The PL-scheduler uses a priority mechanism for job differentiation of memory resident tasks that follows the Unix standard <ref> [3] </ref> rather than the ranking mechanism used in scheduling memory. A newly created task is given a high priority. When a task's processor quantum expires, its priority is decremented. A task's priority rises when it unblocks (e.g. I/O occurs) or it has not received service for a long time (anti-starvation).
Reference: [4] <author> D. Burger, R. Hyder, B. Miller, and D. Wood. </author> <title> Paging tradeoffs in distributed-shared-memory multiprocessors. </title> <booktitle> In Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Thus, the swapping overhead for paging systems is similar. Furthermore, demand paging in distributed memory systems can severely degrade performance <ref> [4] </ref>. 6.2 User Demand Typically, a task's importance is represented by the priority level it is assigned. Tasks at higher priority levels receive preferential consideration in the allocation of resources.
Reference: [5] <author> D.L. Eager, E.D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> May </month> <year> 1986. </year>
Reference-contexts: In the absence of memory swapping, load imbalances could occur. Based on previous research <ref> [5] </ref> [6], PL-schedulers migrate work in two situations. When a new task is created (normally as part of the Unix fork () system call), the PL-scheduler selects another PL-scheduler at random and checks the length of that scheduler's Ready queue.
Reference: [6] <author> D.L. Eager, E.D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender-initiated adaptive load sharing. Performance Evaluation, </title> <month> March </month> <year> 1986. </year>
Reference-contexts: In the absence of memory swapping, load imbalances could occur. Based on previous research [5] <ref> [6] </ref>, PL-schedulers migrate work in two situations. When a new task is created (normally as part of the Unix fork () system call), the PL-scheduler selects another PL-scheduler at random and checks the length of that scheduler's Ready queue.
Reference: [7] <author> R. Essick. </author> <title> An event-based fair share scheduler. </title> <booktitle> In Winter 90 USENIX Conference, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: All schedulers are event driven and similar in design to the Unix Esched scheduler [19] and its improved variants [20] <ref> [7] </ref>. These schedulers loop forever processing events as they arrive on their event queue. The scheduler receives an event and creates a kernel privileged stream, known as a kernel daemon, to examine the event and take the appropriate action.
Reference: [8] <author> A. Gottleib, B. Lubachevsky, and L. Rudolph. </author> <title> Basic techniques for the efficient coordi-nation of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2), </volume> <month> April </month> <year> 1983. </year>
Reference-contexts: The runtime systems representing each of these tasks compete with one another for resources. 4.2 Scheduling Work Work appears in two types of queues in the runtime. New futures are spawned into the ready pool of the task. The ready pool is a parallel FIFO based on fetch&add <ref> [8] </ref>. Futures that had blocked and are now ready to run again are placed in the unblocked pool of the team on which they blocked. Distributed unblocked pools serve as a convenient means to direct special work, such as a command to exit, to a particular team.
Reference: [9] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Each multi-threaded processor can support up to 16 jobs simultaneously that compete for the processor's instruction streams. In addition, memory must be scheduled because of the potentially broad spectrum of job memory requirements. Most parallel systems are composed of two levels of scheduling <ref> [16, 24, 23, 9] </ref>. A kernel or low level scheduler divides the system resources among competing jobs, and a user level scheduler divides the parallel work among the job's available processing resources. Like other systems, the Tera MTA supports this division.
Reference: [10] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Alhorithms. </title> <publisher> Computer Science Press, </publisher> <year> 1984. </year>
Reference-contexts: Each task is represented by the number of protection domains it requires. The goal is to fit the tasks into the minimum number of bins. Since the problem is NP-complete <ref> [10] </ref>, a standard heuristic solution uses a decreasing first fit algorithm (see Standard Algorithm in Figure 4a). Sort the tasks by decreasing number of teams. Let x i be the number of teams in task i .
Reference: [11] <author> S. Leutenegger and M. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: The former strategy is known as space sharing, in which the processors are partitioned among the jobs in the system [21]. The latter strategy is generally realized by round-robin scheduling policies that rotate possession of the processors among the jobs in the system in a coordinated manner <ref> [11] </ref>. Studies comparing the performance of space-sharing and time-sharing systems [25, 17] conclude that space sharing policies make more efficient use of processors that might otherwise be idle when running a single job.
Reference: [12] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor allocation policy for mul-tiprogrammed, shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> May </month> <year> 1993. </year>
Reference-contexts: Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
Reference: [13] <author> C. McCann and J. Zahorjan. </author> <title> Processor allocation policies for message-passing parallel computers. </title> <booktitle> In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
Reference: [14] <author> V. Naik, S. Setia, and M. Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Supercomputing 1993, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
Reference: [15] <author> V. Naik, S. Setia, and M. Squillante. </author> <title> Scheduling of large scientific applications on distributed memory multiprocessor systems. </title> <booktitle> In 6th SIAM Conference on Parallel Processing for Scientific Computation, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
Reference: [16] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In 3rd International Conference on Distributed Computing, </booktitle> <month> Oct </month> <year> 1982. </year>
Reference-contexts: Each multi-threaded processor can support up to 16 jobs simultaneously that compete for the processor's instruction streams. In addition, memory must be scheduled because of the potentially broad spectrum of job memory requirements. Most parallel systems are composed of two levels of scheduling <ref> [16, 24, 23, 9] </ref>. A kernel or low level scheduler divides the system resources among competing jobs, and a user level scheduler divides the parallel work among the job's available processing resources. Like other systems, the Tera MTA supports this division. <p> A big job processor scheduler (called the PB-scheduler) schedules multi-team tasks, while a small job scheduler (PL-scheduler) schedules single-team tasks. As multi-team tasks execute on multiple processors, a single PB-scheduler using global knowledge of processor utilization is required per machine. The PB-scheduler co-schedules <ref> [16] </ref> multi-team tasks on the set of protection domains it controls on all processors. Since single-team tasks execute on a single processor, each processor runs its own copy of the PL-scheduler. <p> However, research by Ashok and Zahorjan [2] suggest that the benefits of such dynamic partitions are not easily attained. 13 7 Processor Scheduler Early research on processor scheduling for parallel machines highlighted the need for co-scheduling <ref> [16] </ref>; that is, ensuring that all parallel activities of a job execute at once. This avoids inefficiencies due to synchronization losses when some activities are blocked waiting for progress from another parallel activity that is not scheduled.
Reference: [17] <author> S. Setia, M. Squillante, and S. Tripathi. </author> <title> Processor scheduling on multiprogrammed, </title> <booktitle> distributed memory parallel systems. In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The latter strategy is generally realized by round-robin scheduling policies that rotate possession of the processors among the jobs in the system in a coordinated manner [11]. Studies comparing the performance of space-sharing and time-sharing systems <ref> [25, 17] </ref> conclude that space sharing policies make more efficient use of processors that might otherwise be idle when running a single job. However, in dividing the processors among multiple jobs, space sharing policies often allocate fewer processors to jobs than the job's parallelism may allow.
Reference: [18] <author> K. Sevcik. </author> <title> Characterization of parallelism in applications and their use in scheduling. </title> <booktitle> In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
Reference: [19] <author> J. H. Straathof, A. A. Thareja, and A. K. Agrawala. </author> <title> Unix scheduling for large systems. </title> <booktitle> In Denver USENIX Conference, </booktitle> <month> January </month> <year> 1986. </year>
Reference-contexts: Each memory scheduler selects a PL-scheduler for a newly im-swapped single-team task. 3 Using the memory scheduler as a dispatcher provides a mechanism for load balancing and control over which processors receive new work. All schedulers are event driven and similar in design to the Unix Esched scheduler <ref> [19] </ref> and its improved variants [20] [7]. These schedulers loop forever processing events as they arrive on their event queue. The scheduler receives an event and creates a kernel privileged stream, known as a kernel daemon, to examine the event and take the appropriate action.
Reference: [20] <author> J. H. Straathof, A. A. Thareja, and A. K. Agrawala. </author> <title> Methodology and results of performance measurements for a new unix scheduler. </title> <booktitle> In Washington USENIX Conference, </booktitle> <month> January </month> <year> 1987. </year>
Reference-contexts: All schedulers are event driven and similar in design to the Unix Esched scheduler [19] and its improved variants <ref> [20] </ref> [7]. These schedulers loop forever processing events as they arrive on their event queue. The scheduler receives an event and creates a kernel privileged stream, known as a kernel daemon, to examine the event and take the appropriate action.
Reference: [21] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In 12th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: On traditional multiprocessors, scheduling requires that either a processor be allocated to a single job or that context switching among multiple jobs assigned to a processor be globally coordinated. The former strategy is known as space sharing, in which the processors are partitioned among the jobs in the system <ref> [21] </ref>. The latter strategy is generally realized by round-robin scheduling policies that rotate possession of the processors among the jobs in the system in a coordinated manner [11]. <p> However, in dividing the processors among multiple jobs, space sharing policies often allocate fewer processors to jobs than the job's parallelism may allow. Tucker and Gupta <ref> [21] </ref> describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load [25, 12, 13, 15, 14, 15, 18].
Reference: [22] <author> D. Wagner and B. Calder. </author> <title> Leapfrogging: A portable technique for implementing efficient futures. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Some programs, especially those using divide-and-conquer algorithms, may do better with an ordering different from FIFO. Tera provides the user with a language extension, touch (- future), to assist with the effective execution of those programs. Wagner and Calder <ref> [22] </ref>, outline a more general version of touch, called leapfrogging. When touch is applied to a future variable, the vp stops executing its current work and executes instead the work associated with the future variable, unless it has already started.
Reference: [23] <author> J. Zahorjan, E. Lazowska, and D. Eager. </author> <title> Spinning versus blocking in parallel systems with uncertainty. </title> <booktitle> In International Sympossium on Performance of Distributed and Parallel Systems, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: Each multi-threaded processor can support up to 16 jobs simultaneously that compete for the processor's instruction streams. In addition, memory must be scheduled because of the potentially broad spectrum of job memory requirements. Most parallel systems are composed of two levels of scheduling <ref> [16, 24, 23, 9] </ref>. A kernel or low level scheduler divides the system resources among competing jobs, and a user level scheduler divides the parallel work among the job's available processing resources. Like other systems, the Tera MTA supports this division.
Reference: [24] <author> J. Zahorjan, E. Lazowska, and D. Eager. </author> <title> The effects of scheduling discipline on spin overhead in shared memory parallel systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1991. </year>
Reference-contexts: Each multi-threaded processor can support up to 16 jobs simultaneously that compete for the processor's instruction streams. In addition, memory must be scheduled because of the potentially broad spectrum of job memory requirements. Most parallel systems are composed of two levels of scheduling <ref> [16, 24, 23, 9] </ref>. A kernel or low level scheduler divides the system resources among competing jobs, and a user level scheduler divides the parallel work among the job's available processing resources. Like other systems, the Tera MTA supports this division.
Reference: [25] <author> J. Zahorjan and C. McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: The latter strategy is generally realized by round-robin scheduling policies that rotate possession of the processors among the jobs in the system in a coordinated manner [11]. Studies comparing the performance of space-sharing and time-sharing systems <ref> [25, 17] </ref> conclude that space sharing policies make more efficient use of processors that might otherwise be idle when running a single job. However, in dividing the processors among multiple jobs, space sharing policies often allocate fewer processors to jobs than the job's parallelism may allow. <p> Tucker and Gupta [21] describe "process-control", an approach that adapts a job's parallelism to the processors available to it. Also, efficient space sharing policies must be able to adapt the partition sizes and configurations based on the changing system load <ref> [25, 12, 13, 15, 14, 15, 18] </ref>. Tera's multi-threaded architecture makes it possible to combine space-sharing and timesharing and realize the benefits of both. Multiple tasks execute simultaneously on a single processor. Thus, there is no need for global coordination of context switching.
References-found: 25

