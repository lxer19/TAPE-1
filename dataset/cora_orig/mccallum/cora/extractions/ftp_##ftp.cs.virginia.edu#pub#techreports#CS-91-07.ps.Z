URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-91-07.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Introduction to Parallel Object-Oriented Programming with Mentat  
Author: Andrew S. Grimshaw 
Note: This work was partially supported by NASA grant NAG-1-1181.  
Abstract: Computer Science Report No. TR-91-07 April 4, 1991 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Intel Corporation, </author> <title> ``iPSC/2 USER'S GUIDE'', Intel Scientific Computers, </title> <address> Beaverton, OR, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: 1. Introduction The last several years have witnessed the development of very high performance MIMD architectures. Examples include the Intel iPSC/2 <ref> [1] </ref>, the BBN Butterfly [2], and recently the Intel i860 cube [3]. <p> worker; loc_matrix *res [MAX_PIECES], *pm, *answer; // Determine the # pieces using a heuristic for (i=0;i&lt;pieces;i++) - rows=dim/pieces; if (dim%pieces&gt;i) rows++; pm = new loc_matrix (mat1-&gt;get_r_ptr (no_rows), dim, rows); res [i] = worker.mpy (pm [i], mat2); delete pm; no_rows += rows; -; switch (pieces) - case 2:rtf (worker.combine_results (2,res [0],res <ref> [1] </ref>); break; case 4:rtf (worker.combine_results (4,res [0],res [1],res [2],res [3]); break; ... <p> Instead the run-time system is layered on top of an existing host operating system, using the host operating system's process, memory, C library, and interprocess communication (IPC) services. To date, Mentat has been hosted on 4.3 BSD Unix [32], NX/2 <ref> [1] </ref> (the operating system on the Intel iPSC/2), and will soon be complete on Mach [33] and Mach-1000 [2] (BBN's dialect of Mach). The logical structure of a Mentat system is that of a collection of hosts communicating through an interconnection network (see Figure 8).
Reference: [2] <institution> BBN Advanced Computers Inc., ``Mach-1000 Reference Manual'', </institution> <address> Cambridge, Mass., </address> <year> 1988. </year>
Reference-contexts: 1. Introduction The last several years have witnessed the development of very high performance MIMD architectures. Examples include the Intel iPSC/2 [1], the BBN Butterfly <ref> [2] </ref>, and recently the Intel i860 cube [3]. The peak performance of the most recent entry, a 128 i860 node hypercube, is in excess of 7.5 giga-flops, rivaling the performance of high-end supercomputers such as the Cray Y-MP (2.7 giga-flops) and the NEC SX-2 (1.3 giga-flops) [4]. <p> To date, Mentat has been hosted on 4.3 BSD Unix [32], NX/2 [1] (the operating system on the Intel iPSC/2), and will soon be complete on Mach [33] and Mach-1000 <ref> [2] </ref> (BBN's dialect of Mach). The logical structure of a Mentat system is that of a collection of hosts communicating through an interconnection network (see Figure 8).
Reference: [3] <author> S. Squires, </author> <title> Keynote address, </title> <booktitle> Fifth Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: 1. Introduction The last several years have witnessed the development of very high performance MIMD architectures. Examples include the Intel iPSC/2 [1], the BBN Butterfly [2], and recently the Intel i860 cube <ref> [3] </ref>. The peak performance of the most recent entry, a 128 i860 node hypercube, is in excess of 7.5 giga-flops, rivaling the performance of high-end supercomputers such as the Cray Y-MP (2.7 giga-flops) and the NEC SX-2 (1.3 giga-flops) [4]. <p> In the near future a 2048 i860-node computer using a mesh interconnection network will be built. When complete it will have a peak performance of over 122 giga-flops <ref> [3] </ref>! Concurrent with the development of the new high performance parallel supercomputers has been the rapid development in distributed systems technology, both hardware and software. In terms of hardware the now mature Ethernet LAN technology combined with low cost workstations has led to the proliferation of networks of workstations. <p> # pieces using a heuristic for (i=0;i&lt;pieces;i++) - rows=dim/pieces; if (dim%pieces&gt;i) rows++; pm = new loc_matrix (mat1-&gt;get_r_ptr (no_rows), dim, rows); res [i] = worker.mpy (pm [i], mat2); delete pm; no_rows += rows; -; switch (pieces) - case 2:rtf (worker.combine_results (2,res [0],res [1]); break; case 4:rtf (worker.combine_results (4,res [0],res [1],res [2],res <ref> [3] </ref>); break; ... Suppose that the matrix multiply operation is invoked as in the code fragment below. loc_matrix* a,b,c; // Assume a,b are set up with values matrix_operator A; c = A.mpy (a,b); c-&gt;print (); The A.mpy (a,b) results in a multiply operation being invoked.
Reference: [4] <author> J. J. </author> <title> Hack,"Peak vs. Sustained Performance in Highly Concurrent Vector Machines", </title> <booktitle> IEEE Computer, </booktitle> <month> September, </month> <year> 1986. </year>
Reference-contexts: The peak performance of the most recent entry, a 128 i860 node hypercube, is in excess of 7.5 giga-flops, rivaling the performance of high-end supercomputers such as the Cray Y-MP (2.7 giga-flops) and the NEC SX-2 (1.3 giga-flops) <ref> [4] </ref>. In the near future a 2048 i860-node computer using a mesh interconnection network will be built.
Reference: [5] <author> S. J. Mullender, </author> <title> Distributed Systems, </title> <publisher> ACM Press, Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference: [6] <author> E. Levy, and A. Silbershatz, </author> <title> "Distributed File Systems: Concepts and Examples", </title> <institution> Computer Science TR-89-04, University of Texas at Austin, </institution> <month> March </month> <year> 1989. </year>
Reference: [7] <author> K. Kennedy, </author> <title> "Optimizations of Vector Operations in an Extended Fortran Compiler", </title> <institution> IBM Research Report, RC-7784, </institution> <year> 1979. </year>
Reference: [8] <author> D. Kuck, R. Kuhn, B. Leasure, D. Padua, and M. Wolfe, </author> <title> ``Dependence Graphs and Compiler Optimizations,'' </title> <booktitle> ACM Proc. 8th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 207-218, </pages> <month> Jan., </month> <year> 1981. </year>
Reference: [9] <author> W. B. Ackerman, </author> <title> ``Data Flow Languages,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 15, no. 2, </volume> <pages> pp. 15-25, </pages> <month> February, </month> <year> 1982. </year>
Reference: [10] <author> J. R. </author> <title> McGraw,``The VAL Language: Description and Analysis,'' </title> <journal> ACM Transactions on Programming Languages and Systems, pp. </journal> <volume> 44-82, vol. 4, no. 1, </volume> <month> January, </month> <year> 1982. </year>
Reference-contexts: The priorities can be used to force an order of evaluation on the guards. Using the example above, if we wanted to prioritize deposits over withdrawals we could change the code as shown below. select - [0] (amount&lt;balance (account)): accept withdrawal (int account,int amount); break; <ref> [10] </ref> :accept deposit (int account,int amount); break; -; Select/accept statements can also be used to test, without blocking, whether there are pending member function invocations. This is accomplished by using test instead of accept. Tests are non-blocking.
Reference: [11] <author> G. R. Andrews, and F. B. Schneider, </author> <title> ``Concepts and Notions for Concurrent Programming,'' </title> <journal> ACM Computing Surveys, pp. </journal> <volume> 3-44, vol. 15, no. 1, </volume> <month> March, </month> <year> 1983. </year>
Reference: [12] <author> H. E. Bal, J. G. Steiner, and A. S. </author> <title> Tannenbaum,``Programming Languages for Distributed Computing Systems,'' </title> <journal> ACM Computing Surveys, pp. </journal> <volume> 261-322, vol. 21, no. 3, </volume> <month> September, </month> <year> 1989. </year>
Reference-contexts: The above properties have made distributed object-oriented systems based on RPC very popular. Examples include Argus [22], Clouds [23], Eden [24], ARTS [25], and many others <ref> [12] </ref>. The primary difference between Mentat and other distributed object-oriented systems is that Mentat is designed for parallelism and thus treats member function invocation differently. The Mentat programming language is an extension of C++. We chose C++ from among the object-oriented languages for several reasons. <p> Mentat supports both more parallelism and potentially less message traffic than the traditional approach by exploiting intra-object and inter-object parallelism opportunities and by sending intermediate results only where needed. For a good survey of programming languages for distributed systems see <ref> [12] </ref>. 5.2. Promises Promises [40] offers an extension to traditional RPC that allows multiple invocations of a remote procedure to be pipelined, not delaying execution of the caller until the result is actually needed. As shown in Example 3 Mentat supports pipelining as a subset of its capabilities.
Reference: [13] <author> C. M. Pancake, and D. Bergmark, </author> <title> "Do Parallel Languages Respond to the Needs of Scientific Programmers?", </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 13-23, </pages> <month> December, </month> <year> 1990. </year>
Reference: [14] <author> A. S. Grimshaw, and J. W. S. Liu, </author> <title> ``Mentat: An Object-Oriented Data-Flow System,'' </title> <booktitle> Proceedings of the 1987 Object-Oriented Programming Systems, Languages and Applications Conference, ACM, </booktitle> <pages> pp. 35-47, </pages> <month> October, </month> <year> 1987. </year>
Reference: [15] <author> A. S. Grimshaw and E. </author> <title> Loyot,``The Mentat Programming Language: Users Manual and Tutorial,'' </title> <institution> Computer Science TR-90-08, University of Virginia, </institution> <month> April, </month> <year> 1990. </year>
Reference: [16] <author> A. S. </author> <title> Grimshaw,``The Mentat Run-Time System: Support for Medium Grain Parallel Computation,'' </title> <booktitle> Proc. of the 5th Distributed Memory Computing Conference, </booktitle> <pages> pp. 1064-1073, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: Because Mentat uses a data-driven computation model, it is particularly well-suited for message passing distributed systems There are two primary components of Mentat: the Mentat Programming Language (MPL) [14,15] and the Mentat run-time system <ref> [16] </ref>. The MPL is an object-oriented programming language based on C++ [17] that masks the complexity of the parallel environment from the programmer. The granule of computation is the Mentat class instance, which consists of contained objects (local and member variables), their procedures, and a thread of control. <p> Similarly, instances of a Mentat class cannot access each other's private data. 3. The Run-Time System The Mentat run-time system provides run-time support to Mentat applications <ref> [16] </ref>. The run-time system can be divided into two distinct sets of services. First are the library routines and data structures that support data dependence detection, guard evaluation, select/accept management, and communications services.
Reference: [17] <author> B. Stroustrup, </author> <title> The C++ Programming Language, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Because Mentat uses a data-driven computation model, it is particularly well-suited for message passing distributed systems There are two primary components of Mentat: the Mentat Programming Language (MPL) [14,15] and the Mentat run-time system [16]. The MPL is an object-oriented programming language based on C++ <ref> [17] </ref> that masks the complexity of the parallel environment from the programmer. The granule of computation is the Mentat class instance, which consists of contained objects (local and member variables), their procedures, and a thread of control. <p> Throughout this section we develop several examples that illustrate MPL concepts, and show how the parallelism is realized. 2.1. A Brief Introduction to C++ C++ is an object-oriented extension of C developed by Bjarne Stroustrup <ref> [17] </ref>. C++ supports classes with private data, public data, member functions that operate on instances of classes, multiple inheritance, polymorphism, and function and operator overloading. Classes in C++ are defined in a manner similar to structs in C. <p> To illustrate member function invocation, suppose that x is an instance of int_stack. Member functions are invoked using either the dot notation, x.push (5);, or if x is a pointer, the arrow notation, x-&gt;push (5);. For a more complete description of C++ see <ref> [17] </ref>. 2.2. Goals of the Language Extensions The MPL was designed to meet four goals. First and foremost, the MPL would be object-oriented [18-21]. We would extend the usual notions of data and method encapsulation to include parallelism encapsulation.
Reference: [18] <author> J. D. McGregor, and T. Torson, </author> <title> guest eds., </title> <journal> Special Issue on Object-Oriented Design, CACM, </journal> <month> September, </month> <year> 1990. </year>
Reference: [19] <author> B. </author> <title> Stroustrup,"What is Object-Oriented Programming?," </title> <journal> IEEE Software, </journal> <pages> pp. 10-20, </pages> <month> May, </month> <year> 1988. </year>
Reference: [20] <author> P. </author> <booktitle> Wegner,"Dimensions of Object-Based Language Design," Proceedings of the 1987 Object-Oriented Programming Systems, Languages and Applications Conference, ACM, </booktitle> <pages> pp. 168-182, </pages> <month> October, </month> <year> 1987. </year>
Reference-contexts: This is important because the objective in parallel processing is performance. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 For a discussion of the merits of the object-oriented paradigm see [18-19]. See <ref> [20] </ref> for a discussion of the difference between object oriented and object-based languages. 6 Second, the language does not already have language constructs for concurrency and parallelism as, for example, ADA does. This is important to avoid having two different parallelism constructs in our language.
Reference: [21] <author> A. Goldberg, and D. Robson, </author> <title> Smalltalk-80: The Language and its Implementation, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference: [22] <author> B. Liskov, and R. Scheifler, </author> <title> ``Guardians and Actions: Linguistic Support for Robust, Distributed Programs,'' </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 5, no. 3, </volume> <pages> pp. 381-414, </pages> <year> 1983. </year>
Reference-contexts: A user of an object invokes the object member functions in the same way independently of whether the object invocation is local or remote, thus obtaining location transparency. The above properties have made distributed object-oriented systems based on RPC very popular. Examples include Argus <ref> [22] </ref>, Clouds [23], Eden [24], ARTS [25], and many others [12]. The primary difference between Mentat and other distributed object-oriented systems is that Mentat is designed for parallelism and thus treats member function invocation differently. The Mentat programming language is an extension of C++.
Reference: [23] <author> R. H. LeBlanc, and C. T. Wilkes, </author> <title> ``Systems Programming with Objects and Actions,'' </title> <booktitle> Proceedings 5th Distributed Computer Systems, IEEE, </booktitle> <pages> pp. 132-139, </pages> <year> 1985. </year>
Reference-contexts: A user of an object invokes the object member functions in the same way independently of whether the object invocation is local or remote, thus obtaining location transparency. The above properties have made distributed object-oriented systems based on RPC very popular. Examples include Argus [22], Clouds <ref> [23] </ref>, Eden [24], ARTS [25], and many others [12]. The primary difference between Mentat and other distributed object-oriented systems is that Mentat is designed for parallelism and thus treats member function invocation differently. The Mentat programming language is an extension of C++.
Reference: [24] <author> A. Black, </author> <title> ``Supporting Distributed Applications: Experience with Eden,'' </title> <booktitle> Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 181-193, </pages> <month> December, </month> <year> 1985. </year> <month> 36 </month>
Reference-contexts: A user of an object invokes the object member functions in the same way independently of whether the object invocation is local or remote, thus obtaining location transparency. The above properties have made distributed object-oriented systems based on RPC very popular. Examples include Argus [22], Clouds [23], Eden <ref> [24] </ref>, ARTS [25], and many others [12]. The primary difference between Mentat and other distributed object-oriented systems is that Mentat is designed for parallelism and thus treats member function invocation differently. The Mentat programming language is an extension of C++.
Reference: [25] <author> C. Mercer, and H. </author> <title> Tokuda,``The ARTS Object Model,'' </title> <booktitle> Proceedings of the 11th Real-Time Systems Symposium, </booktitle> <address> Orlando, FL, Dec.4-7, </address> <year> 1990. </year>
Reference-contexts: The above properties have made distributed object-oriented systems based on RPC very popular. Examples include Argus [22], Clouds [23], Eden [24], ARTS <ref> [25] </ref>, and many others [12]. The primary difference between Mentat and other distributed object-oriented systems is that Mentat is designed for parallelism and thus treats member function invocation differently. The Mentat programming language is an extension of C++. We chose C++ from among the object-oriented languages for several reasons. <p> Much of the difference can be accounted for by the difference in objectives between Mentat and the others. Mentat strives for parallelism as opposed to distribution [24,38], fault-tolerance [22,23,36], or real-time operation <ref> [25] </ref>. Most object-oriented distributed systems provide a traditional, call, block, 31 continue, RPC [39] semantics. Mentat supports both more parallelism and potentially less message traffic than the traditional approach by exploiting intra-object and inter-object parallelism opportunities and by sending intermediate results only where needed.
Reference: [26] <author> J.W.S. Liu and A. S. Grimshaw, </author> <title> ``An object-oriented macro data flow architecture,'' </title> <booktitle> Proceedings of the 1986 National Communications Forum, </booktitle> <month> September, </month> <year> 1986. </year>
Reference: [27] <author> J.W.S. Liu and A. S. Grimshaw, </author> <title> ``A Distributed System Architecture Based on Macro Data Flow Model,'' </title> <booktitle> Proceedings Workshop on Future Directions in Architecture and Software, </booktitle> <address> South Carolina, </address> <month> May 7-9, </month> <year> 1986. </year>
Reference: [28] <author> C.A.R. Hoare,``Monitors: </author> <title> An Operating System Structuring Concept,'' </title> <journal> Communications of the ACM pp.549-557, </journal> <volume> vol. 17, no. 10, </volume> <month> October, </month> <year> 1974 </year>
Reference-contexts: First, because there is no shared memory in our model, shared state can only be 9 realized using a Mentat object with which other objects can communicate. Second, because Mentat objects service a single member function at a time, they provide a monitor-like <ref> [28] </ref> synchronization, providing synchronized access to their state. Syntactically, the specification of a Mentat class is simple. We add the keyword MENTAT to the class definition. The class may be further specified to be PERSISTENT or REGULAR.
Reference: [29] <author> A. S. Grimshaw, and V. E. Vivas, </author> <title> ``FALCON: A Distributed Scheduler for MIMD Architectures'', submitted to Symposium on Experiences with Distributed and Multiprocessor Systems, </title> <address> Atlanta, GA, </address> <month> March, </month> <year> 1991. </year>
Reference-contexts: There are four flavors of create (). When create () is used as in Figure 2 (a) the system will choose on which processor to instantiate the object <ref> [29] </ref>. The programmer may optionally provide location hints. The hints are COLOCATE, DISJOINT, and HIGH_COMPUTATION_RATIO. These hints allow the programmer to specify where he wants the new object to be instantiated. <p> high latency/low bandwidth/high contention (Ethernet using IP datagrams), to low latency/low bandwidth/low contention (the iPSC/2), to a very low latency/very high bandwidth/low contention network (the hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Token matching is the gathering together the arguments of an invocation of a REGULAR object member function onto one host. 4 The scheduler <ref> [29] </ref> uses communication cost information in making scheduling decisions. 5 Arbitrary in that insufficient memory at the receiver is the only limitation. A process is as defined by the host operating system. 26 BBN Butterfly using shared memory). <p> These include the instantiation manager and the token matching unit (TMU). The instantiation manager is responsible for high level Mentat object scheduling (deciding on which host to locate an object), and for instantiating new instances. The high level scheduling algorithm is distributed, adaptive, and stable, and is discussed in <ref> [29] </ref>. One final note on the run-time system. Because we use a layered approach and mask differences in the underlying operating system and inter-process communication, applications are completely source code portable between supported architectures.
Reference: [30] <institution> Reference Manual for the Ada Programming Language, United States Department of Defense, Ada Joint Program Office, </institution> <month> July </month> <year> 1982. </year>
Reference-contexts: Select/Accept The select/accept construct in the MPL is similar in many ways to the ADA <ref> [30] </ref> select/accept. MPL select/accept statements are used to selectively choose which member functions are candidates for execution, i.e., those member functions for which the object will accept an invocation.
Reference: [31] <author> J. A. Feldman, </author> <title> ``High Level Programming for Distributed Computing,''Communications of the ACM, </title> <journal> pp. </journal> <volume> 353-368, vol. 22, no. 6, </volume> <month> January, </month> <year> 1979. </year>
Reference-contexts: The ability to selectively receive invocations was inspired by PLITS <ref> [31] </ref>. Guards are evaluated in the order of their priority, from high priority to low priority. Priorities are in the range (-MAXINT) to MAXINT. The default priority is zero. Guards within a given priority level are evaluated in a non-deterministic order until a guard evaluates to true.
Reference: [32] <author> W. Joy, E. Cooper, R. Fabry, S. Leffer, K. McKusick, and D.Mosher, </author> <title> ``4.2BSD System Manual,'' </title> <institution> Computer Systems Research Group, Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> July </month> <year> 1983. </year>
Reference-contexts: Instead the run-time system is layered on top of an existing host operating system, using the host operating system's process, memory, C library, and interprocess communication (IPC) services. To date, Mentat has been hosted on 4.3 BSD Unix <ref> [32] </ref>, NX/2 [1] (the operating system on the Intel iPSC/2), and will soon be complete on Mach [33] and Mach-1000 [2] (BBN's dialect of Mach). The logical structure of a Mentat system is that of a collection of hosts communicating through an interconnection network (see Figure 8).
Reference: [33] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian, and M. Young, </author> <title> ``Mach: A New Kernel Foundation for Unix Development'', </title> <booktitle> Summer Usenix Conference Proceedings, </booktitle> <pages> pp. 93-112, </pages> <year> 1986. </year>
Reference-contexts: To date, Mentat has been hosted on 4.3 BSD Unix [32], NX/2 [1] (the operating system on the Intel iPSC/2), and will soon be complete on Mach <ref> [33] </ref> and Mach-1000 [2] (BBN's dialect of Mach). The logical structure of a Mentat system is that of a collection of hosts communicating through an interconnection network (see Figure 8).
Reference: [34] <author> A. S. Grimshaw, D. Mack, and T. Strayer,"MMPS: </author> <title> Portable Message Passing Support for Parallel Computing," </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 784-789, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: MMPS (Modular Message Passing System <ref> [34] </ref>) provides an extensible point-to-point message service that reliably delivers messages of arbitrary size from one process to another 5 MMPS provides a uniform message passing interface that we have ported to physical interconnection networks that span the performance spectrum, from high latency/low bandwidth/high contention (Ethernet using IP datagrams), to low <p> In Tables T-1 and T-2 below the time spent in each of these activities is shown for a Sun 3/60 and a node on the Intel iPSC/2 respectively. The message transport cost includes the host operating system scheduling and task switch overhead. For information on the communication costs see <ref> [34] </ref>. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Function Time iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Transport Message (one way) 5.7mS iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Null RPC 14.0mS iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Mentat overhead 2.6mS iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c Table T-1.
Reference: [35] <author> M. B. Jones, and R. Rashid, </author> <title> "Mach and Matchmaker: Kernel and Language Support for Object-Based Distributed Systems,'' </title> <institution> Carnegie Mellon University, </institution> <year> 1986. </year>
Reference: [36] <author> D. L. Detlefs, M. P. Herlihy, and J. M. Wing, </author> <title> "Inheritance of Synchronization and Recovery Properties in Avalon/C++," </title> <booktitle> IEEE Computer, </booktitle> <month> December, </month> <year> 1988. </year>
Reference: [37] <author> A. S. Tanenbaum, and R. van Renesse, </author> <title> ``Distributed Operating Systems," </title> <journal> ACM Computing Surveys, pp. </journal> <volume> 419-470, vol. 17, no. 4, </volume> <month> December, </month> <year> 1985. </year>
Reference: [38] <author> R. van Renessee, H. van Staveren, and A. Tannenbaum, </author> <title> "Performance of the World's Fastest Distributed Operating System," </title> <journal> ACM OSR, </journal> <volume> vol. 22, no. </volume> <month> 4 (Oct.), </month> <year> 1984. </year>
Reference: [39] <author> B. J. </author> <title> Nelson,``Remote Procedure Call,'' </title> <type> Xerox Corporation Technical Report CSL-81-9, </type> <month> May, </month> <year> 1981. </year>
Reference-contexts: Much of the difference can be accounted for by the difference in objectives between Mentat and the others. Mentat strives for parallelism as opposed to distribution [24,38], fault-tolerance [22,23,36], or real-time operation [25]. Most object-oriented distributed systems provide a traditional, call, block, 31 continue, RPC <ref> [39] </ref> semantics. Mentat supports both more parallelism and potentially less message traffic than the traditional approach by exploiting intra-object and inter-object parallelism opportunities and by sending intermediate results only where needed. For a good survey of programming languages for distributed systems see [12]. 5.2.
Reference: [40] <author> B. Liskov, and L. Shrira,"Promises: </author> <title> Linguistic Support for Efficient Asynchronous Procedure Calls in Distributed Systems," </title> <booktitle> Proceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 260-267, </pages> <month> June, </month> <year> 1988. </year>
Reference-contexts: Mentat supports both more parallelism and potentially less message traffic than the traditional approach by exploiting intra-object and inter-object parallelism opportunities and by sending intermediate results only where needed. For a good survey of programming languages for distributed systems see [12]. 5.2. Promises Promises <ref> [40] </ref> offers an extension to traditional RPC that allows multiple invocations of a remote procedure to be pipelined, not delaying execution of the caller until the result is actually needed. As shown in Example 3 Mentat supports pipelining as a subset of its capabilities.
Reference: [41] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference: [42] <author> C. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference: [43] <author> D. Kuck, D. Lawrie, R. Cytron, A. Sameh, and D. Gajski, </author> <title> ``THE ARCHITECTURE AND PROGRAMMING OF THE CEDAR SYSTEM,'' Cedar Document no. </title> <type> 21, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> August, </month> <year> 1983. </year>
Reference: [44] <author> S. Ahuja, N. Carriero, and D. Gelernter, "Linda and Friends," </author> <booktitle> IEEE Computer, </booktitle> <pages> pp. 26-34, </pages> <month> August, </month> <year> 1986. </year> <month> 37 </month>
Reference-contexts: We feel that hybrid approaches, such as the approach used by Mentat, offer a good compromise between the programming simplicity of implicit schemes and the control and power of explicit schemes. 33 5.4. Generative Communication - Linda Generative communication and Linda <ref> [44] </ref> have been proposed as a means of writing parallel and distributed software. Mentat differs from Linda two respects. First, the underlying model of computation is different. Linda uses the generative communication model in which processes communicate with one another by reading and writing to the shared tuple space (TS).
References-found: 44

