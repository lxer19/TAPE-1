URL: ftp://theory.lcs.mit.edu/pub/people/danar/leave-one-out-long.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~danar/papers.html
Root-URL: 
Email: mkearns@research.att.com  danar@theory.lcs.mit.edu  
Title: Algorithmic Stability and Sanity-Check Bounds for Leave-One-Out Cross-Validation  
Author: Michael Kearns Dana Ron 
Address: 180 Park Avenue Florham Park, NJ  545 Technology Square Cambridge, MA  
Affiliation: AT&T Labs Research  Laboratory for Computer Science, MIT  
Abstract: In this paper we prove sanity-check bounds for the error of the leave-one-out cross-validation estimate of the generalization error: that is, bounds showing that the worst-case error of this estimate is not much worse than that of the training error estimate. The name sanity-check refers to the fact that although we often expect the leave-one-out estimate to perform considerably better than the training error estimate, we are here only seeking assurance that its performance will not be considerably worse. Perhaps surprisingly, such assurance has been given only for limited cases in the prior literature on cross-validation. Any nontrivial bound on the error of leave-one-out must rely on some notion of algorithmic stability. Previous bounds relied on the rather strong notion of hypothesis stability, whose application was primarily limited to nearest-neighbor and other local algorithms. Here we introduce the new and weaker notion of error stability, and apply it to obtain sanity-check bounds for leave-one-out for other classes of learning algorithms, including training error minimization procedures and Bayesian algorithms. We also provide lower bounds demonstrating the necessity of some form of error stability for proving bounds on the error of the leave-one-out estimate, and the fact that for training error minimization algorithms, in the worst case such bounds must still depend on the Vapnik-Chervonenkis dimension of the hypothesis class. 
Abstract-found: 1
Intro-found: 1
Reference: [DGL96] <author> L. Devroye, L. Gyoyfi, and G. Lugosi. </author> <title> A Probabilistic Theory of Pattern Recognition. </title> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference-contexts: There are surprisingly few previous results providing bounds on the accuracy of the various estimates [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] (see the recent book of Devroye, Gyorfi and Lugosi <ref> [DGL96] </ref> for an excellent introduction and survey of the topic). <p> The algorithms they consider are primarily variants of nearest-neighbor and other local procedures, and as such do not draw their hypotheses from a fixed class of bounded VC dimension, which is the situation we are primarily interested in here. Devroye, Gyorfi and Lugosi <ref> [DGL96, Chap. 24] </ref> obtain a bound on the error of the leave-one-out estimate for another particular class of algorithms, namely that of histogram rules. Vapnik [Vap82, Chap. 8] studies the leave-one-out estimate, (which he refers to as the moving-control estimate), for a special case of linear regression. <p> This unlimited complexity often makes it difficult to quantify the performance of the learning algorithm except in terms of the asymptotic generalization error (see Devroye, Gyorfi and Lugosi <ref> [DGL96] </ref> for a detailed survey of results for nearest-neighbor algorithms). For this and other reasons, practitioners often prefer to commit to a hypothesis class H of fixed VC dimension d, and use heuristics to find a good function in H . <p> Unfortunately, it is well-known <ref> [DGL96, Chap. 24] </ref> (and demonstrated in Section 5) that, at least in the unrealizable setting, a 1=ffi dependence is in general unavoidable for the leave-one-out estimate. <p> This again is the price of generality for particular algorithms, such as k-nearest neighbor rules, it is possible to show only logarithmic dependence on 1=ffi [RW78] (stated in <ref> [DGL96, Thm. 24.2] </ref>). <p> In any case, in Section 5 we show that some additional assumptions (beyond error stability) are required to obtain nontrivial bounds for the error of leave-one-out. Before stating the main theorem of this section, we give the following simple but important lemma. This result is well-known <ref> [DGL96, Chap. 24] </ref>, but we include its proof for the sake of completeness. <p> Setting d = 1 in Theorem 4.1 shows that the dependence on ffi given there is tight (up to logarithmic factors). This theorem has appeared elsewhere <ref> [DGL96, Chap. 24] </ref>, but we include it here for completeness.
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Two very fortunate properties of the combination of linear functions and squared error make the sanity-check bound given in Theorem 4.7 of particular interest: 18 * There exist polynomial-time algorithms for performing minimization of squared training er-ror <ref> [DH73] </ref> by linear functions.
Reference: [DW79a] <author> L. P. Devroye and T. J. Wagner. </author> <title> Distribution-free inequalities for the deleted and holdout error estimates. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-25(2):202-207, </volume> <year> 1979. </year>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> On the other hand, among the strongest bounds (in the sense of the quality of the estimate) are those given for the leave-one-out estimate by the work of Rogers and Wagner [RW78], Devroye and Wagner <ref> [DW79a, DW79b] </ref>, and Vapnik [Vap82]. The (classification error) leave-one-out estimate is computed by running the learning algorithm m times, each time removing one of the m training examples, and testing the resulting hypothesis on the training example that was deleted; the fraction of failed tests is the leave-one-out estimate. <p> Rogers and Wagner [RW78] and Devroye and Wagner <ref> [DW79a, DW79b] </ref> proved that for several specific algorithms, but again for any target function and input distribution, the leave-one-out estimate can be as close as O (1= p m) to the true error.
Reference: [DW79b] <author> L. P. Devroye and T. J. Wagner. </author> <title> Distribution-free performance bounds for potential function rules. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-25(5):601-604, </volume> <year> 1979. </year>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> On the other hand, among the strongest bounds (in the sense of the quality of the estimate) are those given for the leave-one-out estimate by the work of Rogers and Wagner [RW78], Devroye and Wagner <ref> [DW79a, DW79b] </ref>, and Vapnik [Vap82]. The (classification error) leave-one-out estimate is computed by running the learning algorithm m times, each time removing one of the m training examples, and testing the resulting hypothesis on the training example that was deleted; the fraction of failed tests is the leave-one-out estimate. <p> Rogers and Wagner [RW78] and Devroye and Wagner <ref> [DW79a, DW79b] </ref> proved that for several specific algorithms, but again for any target function and input distribution, the leave-one-out estimate can be as close as O (1= p m) to the true error. <p> A moment's reflection should make it intuitively clear that, in contrast to the training error, even a sanity-check bound for leave-one-out cannot come without restrictions on the algorithm under consideration: some form of algorithmic stability is required <ref> [DW79b, Hol96b, Koh95] </ref>. If the removal of even a single example from the training sample may cause the learning algorithm to jump to a different hypothesis with, say, much larger error than the full-sample hypothesis, it seems hard to expect the leave-one-out estimate to be accurate. <p> The precise nature of the required form of stability is less obvious. Devroye and Wagner <ref> [DW79b] </ref> first identified a rather strong notion of algorithmic stability that we shall refer to as hypothesis stability, and showed that bounds on hypothesis stability directly lead to bounds on the error of the leave-one-out estimate. <p> Perhaps the strongest notion of stability that an interesting learning algorithm might be expected to obey is that of hypothesis stability: namely, that small changes in the sample can only cause the algorithm to move to nearby hypotheses. The notion of hypothesis stability is due to Devroye and Wagner <ref> [DW79b] </ref>, and is formalized in a way that suits our purposes in the following definition 3 . <p> Thus, we ask that with high probability, the hypotheses output by A on S m and S m1 be similar. We shall shortly argue that hypothesis stability is in fact too demanding a notion in many realistic situations. But first, we state the elegant theorem of Devroye and Wagner <ref> [DW79b] </ref> that relates the error of the leave-one-out estimate for an algorithm to the hypothesis stability. THEOREM 3.1 Let A be any symmetric algorithm that has hypothesis stability (fi 1 ; fi 2 ). <p> THEOREM 3.1 Let A be any symmetric algorithm that has hypothesis stability (fi 1 ; fi 2 ). Then for any ffi &gt; 0, with probability at least 1 ffi over S m , j^* A s ffi 3 Devroye and Wagner <ref> [DW79b] </ref> formalized hypothesis stability in terms of the expected difference between the hy potheses; here we translate to the high probability form for consistency. 5 Thus, if we are fortunate enough to have an algorithm with strong hypothesis stability (that is, small fi 1 and fi 2 ), the leave-one-out estimate <p> What kind of hypothesis stability should we expect for natural algorithms? Devroye, Rogers and Wagner <ref> [RW78, DW79b] </ref> gave rather strong hypothesis stability results for certain nonparametric local learning algorithms (such as nearest-neighbor rules), and thus were able to show that the error of the leave-one-out estimate for such algorithms decreases like 1=m ff (for values of ff ranging from 1=4 to 1=2, depending on the details
Reference: [GG84] <author> S. Geman and D. Geman. </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741, </pages> <year> 1984. </year>
Reference-contexts: Such algorithms are frequently studied in the simulated annealing and statistical physics literature on learning <ref> [SST92, GG84] </ref>.
Reference: [Hau92] <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: More generally, many of the results given in this paper can be generalized to other loss functions via the proper generalizations of uniform convergence <ref> [Hau92] </ref>. 4.4 Other Algorithms We now comment briefly on the application of Theorem 4.1 to algorithms other than error minimization and Bayesian procedures.
Reference: [HKST96] <author> D. Haussler, M. Kearns, H.S. Seung, and N. Tishby. </author> <title> Rigorous learning curve bounds from statistical mechanics. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 195-236, </pages> <year> 1996. </year>
Reference-contexts: (S m )) = 1=2 by 1=2, and for half of the sample it underestimates the error by 1=2. (Theorem 5.4) 6 Extensions and Open Problems It is worth mentioning explicitly that in the many situations when uniform convergence bounds better than V C (d; m; ffi) can be obtained <ref> [SST92, HKST96] </ref> our resulting bounds for leave-one-out will be correspondingly better as well. There are a number of interesting open problems, both theoretical and experimental.
Reference: [Hol96a] <author> S. B. Holden. </author> <title> Cross-validation and the PAC learning model. </title> <note> Research Note RN/96/64, </note> <institution> Dept. of CS, Univ. College, </institution> <address> London, </address> <year> 1996. </year>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> are special to the realizable case.) Thus by the triangle inequality, with probability at least 1 ffi 0 , dist (A (S m ); A (S m1 )) = O m : (15) The theorem follows from Theorem 3.1, where ffi 0 is set to d=m. (Theorem 3.2) 4 Holden <ref> [Hol96a] </ref> has recently obtained sanity-check bounds, again for the realizable setting, for other cross-validation estimates. 7 We should note immediately that the bound of Theorem 3.2 has a dependence on q 1=ffi, as opposed to the log (1=ffi) dependence for the training error given by Theorem 2.1.
Reference: [Hol96b] <author> S. B. Holden. </author> <title> PAC-like upper bounds for the sample complexity of leave-one-out cross validation. </title> <booktitle> In Proceedings of the Ninth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 41-50, </pages> <year> 1996. </year> <month> 24 </month>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> A moment's reflection should make it intuitively clear that, in contrast to the training error, even a sanity-check bound for leave-one-out cannot come without restrictions on the algorithm under consideration: some form of algorithmic stability is required <ref> [DW79b, Hol96b, Koh95] </ref>. If the removal of even a single example from the training sample may cause the learning algorithm to jump to a different hypothesis with, say, much larger error than the full-sample hypothesis, it seems hard to expect the leave-one-out estimate to be accurate. <p> For algorithms drawing hypotheses from a class of fixed VC dimension, the first sanity-check bounds for the leave-one-out estimate were provided by Holden <ref> [Hol96b] </ref> for two specific algorithms in the realizable case (that is, when the target function is actually contained in the class of hypothesis functions). <p> In Section 2, we begin by stating some needed preliminaries. In Section 3, we review the Devroye and Wagner notion of hypothesis stability, and generalize the results of Holden <ref> [Hol96b] </ref> by showing that in the realizable case this notion can be used to obtain sanity-check bounds for any consistent learning algorithm; but we also discuss the limitations of hypothesis stability in the unrealizable case. <p> In the realizable * opt = 0 case, there is still hope for applying hypothesis stability. Indeed, Holden <ref> [Hol96b] </ref> was the first to apply uniform convergence results to obtain sanity-check bounds for leave-one-out via hypothesis stability, for two particular (consistent) algorithms in the realizable setting 4 . Here we generalize Holden's results by giving a sanity-check bound on the leave-one-out error for any consistent algorithm.
Reference: [Kea96] <author> M. Kearns. </author> <title> A bound on the error of cross validation, with consequences for the training--test split. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 183-189, </pages> <year> 1996. </year> <note> To Appear in Neural Computation. </note>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic).
Reference: [KMN + 95] <author> M. J. Kearns, Y. Mansour, A. Ng, , and D. Ron. </author> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proceedings of the Eighth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 21-30, </pages> <year> 1995. </year> <note> To Appear in Machine Learning, COLT95 Special Issue. </note>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic).
Reference: [Koh95] <author> Ron Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: A moment's reflection should make it intuitively clear that, in contrast to the training error, even a sanity-check bound for leave-one-out cannot come without restrictions on the algorithm under consideration: some form of algorithmic stability is required <ref> [DW79b, Hol96b, Koh95] </ref>. If the removal of even a single example from the training sample may cause the learning algorithm to jump to a different hypothesis with, say, much larger error than the full-sample hypothesis, it seems hard to expect the leave-one-out estimate to be accurate.
Reference: [KSS94] <author> M. Kearns, R. Schapire, and L. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17 </volume> <pages> 115-141, </pages> <year> 1994. </year>
Reference-contexts: However, in the more realistic unrealizable (or agnostic <ref> [KSS94] </ref>) case, the notion of hypothesis stability may simply be too strong to be obeyed by many natural learning algorithms.
Reference: [Mil90] <author> A.J. Miller. </author> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: These algorithms do not necessarily obey the constraint jjwjj B, but we suspect this is not an obstacle to the validity of Theorem 4.7 in most practical settings. * There is an efficient procedure for computing the leave-one-out estimate for training error minimization of the squared error over linear functions <ref> [Mil90] </ref>. Thus, it is not necessary to run the error minimization procedure m times; there is a closed-form solution for the leave-one-out estimate that can be computed directly from the data much more quickly.
Reference: [RW78] <author> W. H. Rogers and T. J. Wagner. </author> <title> A finite sample distribution-free performance bound for local discrimination rules. </title> <journal> The Annals of Statistics, </journal> <volume> 6(3) </volume> <pages> 506-514, </pages> <year> 1978. </year>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> On the other hand, among the strongest bounds (in the sense of the quality of the estimate) are those given for the leave-one-out estimate by the work of Rogers and Wagner <ref> [RW78] </ref>, Devroye and Wagner [DW79a, DW79b], and Vapnik [Vap82]. <p> The (classification error) leave-one-out estimate is computed by running the learning algorithm m times, each time removing one of the m training examples, and testing the resulting hypothesis on the training example that was deleted; the fraction of failed tests is the leave-one-out estimate. Rogers and Wagner <ref> [RW78] </ref> and Devroye and Wagner [DW79a, DW79b] proved that for several specific algorithms, but again for any target function and input distribution, the leave-one-out estimate can be as close as O (1= p m) to the true error. <p> What kind of hypothesis stability should we expect for natural algorithms? Devroye, Rogers and Wagner <ref> [RW78, DW79b] </ref> gave rather strong hypothesis stability results for certain nonparametric local learning algorithms (such as nearest-neighbor rules), and thus were able to show that the error of the leave-one-out estimate for such algorithms decreases like 1=m ff (for values of ff ranging from 1=4 to 1=2, depending on the details <p> This again is the price of generality for particular algorithms, such as k-nearest neighbor rules, it is possible to show only logarithmic dependence on 1=ffi <ref> [RW78] </ref> (stated in [DGL96, Thm. 24.2]).
Reference: [SST92] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review, </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: Such algorithms are frequently studied in the simulated annealing and statistical physics literature on learning <ref> [SST92, GG84] </ref>. <p> (S m )) = 1=2 by 1=2, and for half of the sample it underestimates the error by 1=2. (Theorem 5.4) 6 Extensions and Open Problems It is worth mentioning explicitly that in the many situations when uniform convergence bounds better than V C (d; m; ffi) can be obtained <ref> [SST92, HKST96] </ref> our resulting bounds for leave-one-out will be correspondingly better as well. There are a number of interesting open problems, both theoretical and experimental.
Reference: [Vap82] <author> V.N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year> <month> 25 </month>
Reference-contexts: For each of these estimates, the hope is that for a fairly wide class of learning algorithms, the estimate will usually produce a value ^* that is close to the true (generalization) error *. There are surprisingly few previous results providing bounds on the accuracy of the various estimates <ref> [RW78, DW79a, DW79b, Vap82, Hol96b, Hol96a, KMN + 95, Kea96] </ref> (see the recent book of Devroye, Gyorfi and Lugosi [DGL96] for an excellent introduction and survey of the topic). <p> Perhaps the most general results are those given for the (classification) training error estimate by Vapnik <ref> [Vap82] </ref>, who proved that for any target function and input distribution, and for any learning algorithm that chooses its hypotheses from a class of VC dimension d, the training error estimate is at most ~ O ( q away from the true error, where m is the size of the training <p> On the other hand, among the strongest bounds (in the sense of the quality of the estimate) are those given for the leave-one-out estimate by the work of Rogers and Wagner [RW78], Devroye and Wagner [DW79a, DW79b], and Vapnik <ref> [Vap82] </ref>. The (classification error) leave-one-out estimate is computed by running the learning algorithm m times, each time removing one of the m training examples, and testing the resulting hypothesis on the training example that was deleted; the fraction of failed tests is the leave-one-out estimate. <p> Devroye, Gyorfi and Lugosi [DGL96, Chap. 24] obtain a bound on the error of the leave-one-out estimate for another particular class of algorithms, namely that of histogram rules. Vapnik <ref> [Vap82, Chap. 8] </ref> studies the leave-one-out estimate, (which he refers to as the moving-control estimate), for a special case of linear regression. He proves bounds of order 1= p m on the error of the estimate under certain assumptions on the distribution over the examples, and their labels. <p> We are thus interested in providing bounds on the error j^* A CV (S m ) *(A (S m ))j of the leave-one-out estimate. The following uniform convergence bound, due to Vapnik <ref> [Vap82] </ref>, will be central to this paper. 4 THEOREM 2.1 Let H be a hypothesis class with VC dimension d &lt; m. <p> Then for every ffi &gt; 0, with probability at least 1 ffi, j^* CV (S m ) *(A (S m ))j = O (d=m)(log (d=m)=ffi : (70) Note that while the bound given in Theorem 4.7 is weaker than that proved by Vapnik <ref> [Vap82, Chap. 8] </ref> (for squared error minimization over the class of linear functions), it is much more general. Namely, we make no assumptions on the distribution according to which the examples are generated and the function labeling them.
References-found: 17

