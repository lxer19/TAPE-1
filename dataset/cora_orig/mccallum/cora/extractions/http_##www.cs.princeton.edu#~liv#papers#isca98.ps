URL: http://www.cs.princeton.edu/~liv/papers/isca98.ps
Refering-URL: http://www.cs.princeton.edu/~liv/papers/papers.html
Root-URL: http://www.cs.princeton.edu
Title: Design Choices in the SHRIMP System: An Empirical Study  
Author: Matthias A. Blumrich Richard D. Alpert Yuqun Chen Douglas W. Clark Stefanos N. Damianakis Cezary Dubnicki Edward W. Felten Liviu Iftode Kai Li Margaret Martonosi and Robert A. Shillner 
Abstract: The SHRIMP cluster-computing system has progressed to a point of relative maturity; a variety of applications are running on a 16-node system. We have enough experience to understand what we did right and wrong in designing and building the system. In this paper we discuss some of the lessons we learned about computer architecture, and about the challenges involved in building a significant working system in an academic research environment. We evaluate significant design choices by modifying the network interface firmware and the system software in order to empirically compare our design to other approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chaiken, K.L. Johnson, D. Kranz, J. Kubiatowicz, B. Lim, K. Machenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: A key contribution of this paper is an empirical design retrospective based on a working 16-node SHRIMP system. In that sense, this paper can be categorized along with previous design evaluations of research machines such as the DASH multiprocessor [33], the Illinois Cedar machine [32], the MIT Alewife multiprocessor <ref> [1] </ref>, and the J-machine multicomputer [38]. SHRIMP has leveraged commodity components to a much greater degree than J-machine, Cedar, Alewife or even DASH, thus this paper focuses primarily on evaluating its custom hardware support for communication.
Reference: [2] <author> Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Kai Li. </author> <title> Design and Implementation of NX Message Passing Using SHRIMP Virtual Memory-Mapped Communication. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: packet is stored in the OPT, while deliberate update allows the bit to be dynamically set as part of an explicit transfer initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library <ref> [2] </ref>, a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27]. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications.
Reference: [3] <author> Richard Alpert and James Philbin. cBSP: </author> <title> Zero-Cost Synchronization in a Modified BSP Model. </title> <type> Technical Report 97-054, </type> <institution> NEC Research Institute, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: OPT, while deliberate update allows the bit to be dynamically set as part of an explicit transfer initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library <ref> [3] </ref>, a Unix stream sockets compatible library [17], a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27]. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications.
Reference: [4] <author> J. Asplin and S. Mehus. </author> <title> On the Design and Performance of the PARFUM Parallel Fault Tolerant Volume Renderer. </title> <type> Technical Report 97-28, </type> <institution> Univerity of Tromso, Norway, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: Thus there are many node-to-node block transfers but no disk I/O in the experiments. Render-sockets This is a Parallel Fault Tolerant Volume Renderer <ref> [4] </ref> which does dynamic load-balancing and runs in a distributed environment. Render-sockets is based on a traditional ray-casting algorithm for rendering volumetric data sets.
Reference: [5] <author> A. Basu, Buch V, Vogels W, and von Eicken T. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 4053, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: This model has been revived by Thekkath et al. [42] using fast traps. Druschel et al. [19] proposed the concept of application device channels which provide protected user-level access to a network interface. U-Net <ref> [5] </ref> uses a similar abstraction to support high-level protocols such as TCP/IP. The automatic update mechanism in SHRIMP is derived from the Pipelined RAM network interface [36], but is able to perform virtual memory-mapped communication and map DRAM memory instead of dedicated memory on the network interface board.
Reference: [6] <institution> BCPR Services Inc. </institution> <note> EISA Specification, Version 3.12, </note> <year> 1992. </year>
Reference-contexts: The transceiver boards are necessary because the PCs and the backplane are on separate power supplies, requiring differential signaling between them. The SHRIMP network interface (Figure 1) consists of two boards because it connects to both the Xpress memory bus [28] and the EISA I/O bus <ref> [6] </ref>. The memory-bus board simply snoops all main-memory writes, passing address and data pairs to the EISA-bus board. The EISA-bus board contains the bulk of the hardware, and connects to the routing backplane. Figure 2 shows the principal datapaths of the network interface.
Reference: [7] <author> Angelos Bilas and Edward W. Felten. </author> <title> Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <month> February </month> <year> 1997. </year>
Reference-contexts: part of an explicit transfer initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library <ref> [7] </ref>, a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27]. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications. In this paper we selected applications based on four different APIs: VMMC, NX, Stream sockets, and SVM. <p> initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library <ref> [7] </ref>, a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27]. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications. In this paper we selected applications based on four different APIs: VMMC, NX, Stream sockets, and SVM.
Reference: [8] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24]. Page-based automatic-update approaches were also used in Memnet [18], Merlin [37], SESAME [45], Plus <ref> [8] </ref> and Galactica Net [30]. These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support.
Reference: [9] <author> Matthias A. Blumrich. </author> <title> Network Interface for Protected, </title> <type> User-Level Communication. PhD thesis, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> June </month> <year> 1996. </year> <note> Available as Technical Report TR-522-96. </note>
Reference-contexts: First, it has extremely low latency. The end-to-end latency is just 3.71 s for a single-word transfer between two user-level processes <ref> [9] </ref>. Second, it can eliminate the need to gather and scatter data. In particular, large data structures that are written sparsely can be exported in their entirety, and mapped remotely for automatic update.
Reference: [10] <author> Matthias A. Blumrich, Cezary Dubnick, Edward W. Felten, and Kai Li. </author> <title> Protected, User-Level DMA for the SHRIMP Network Interface. </title> <booktitle> In IEEE 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 154165, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: emphasis on avoiding receiver side interrupts? In addition to answering these questions, we discuss other lessons learned, including some things that consumed much of our design time, yet turned out not to matter. 2 The SHRIMP System The architecture of the SHRIMP system has been described in several previous publications <ref> [10, 11, 12, 23] </ref> notably [9]and will only be described in as much detail as necessary here. <p> Deliberate Update An application or user-level library initiates a deliberate update transfer by using the network interface's user-level DMA mechanism <ref> [10] </ref>. By executing a two-instruction load/store sequence to special I/O-mapped addresses, the application tells the SHRIMP DMA engine the source, destination, and size of the desired transfer. Protection is guaranteed by a combination of page-mapping tricks and simple error checking in the network interface hardware. <p> This occurred because our applications have relatively low communication requirements. 4.5.3 Deliberate Update Queueing The SHRIMP deliberate update mechanism operates by performing user-level DMA <ref> [10] </ref> transfers from main memory to the network interface. Transfers of up to a page (4K bytes) are specified with two user-level memory references to proxy memory, which is mapped to the network interface. Protection of local and remote memory is provided through proxy memory mappings.
Reference: [11] <author> Matthias A. Blumrich, Cezary Dubnicki, Edward W. Felten, Kai Li, and Malena Mesarina. </author> <title> Virtual-Memory-Mapped Network Interfaces. </title> <journal> IEEE MICRO, </journal> <volume> 15(1):2128, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: emphasis on avoiding receiver side interrupts? In addition to answering these questions, we discuss other lessons learned, including some things that consumed much of our design time, yet turned out not to matter. 2 The SHRIMP System The architecture of the SHRIMP system has been described in several previous publications <ref> [10, 11, 12, 23] </ref> notably [9]and will only be described in as much detail as necessary here.
Reference: [12] <author> Matthias A. Blumrich, Kai Li, Richard D. Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan S. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the Shrimp Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The SHRIMP project studies how to design network interfaces to satisfy the design challenges. Our approach is to use a virtual memory-mapped communication model <ref> [12, 21] </ref>, and implement it with some hardware support at the network interface level to minimize software overhead. Several other projects and commercial products have used similar memory-mapped communication models, including HP's Hamlyn project [15], Digital's MemoryChannel [24] and Dolphin's network interface. <p> emphasis on avoiding receiver side interrupts? In addition to answering these questions, we discuss other lessons learned, including some things that consumed much of our design time, yet turned out not to matter. 2 The SHRIMP System The architecture of the SHRIMP system has been described in several previous publications <ref> [10, 11, 12, 23] </ref> notably [9]and will only be described in as much detail as necessary here.
Reference: [13] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabig-per-Second Local Area Network. </title> <journal> IEEE MICRO, </journal> <volume> 15(1):2936, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: The first reason is performance. Our communication has better latency than several commercial network interfaces such as Myrinet <ref> [13] </ref>, even though our nodes are old 60 MHz, EISA-bus based Pentium PCs and our network interface was designed in 1993. <p> In terms of networking fabric, the Intel Paragon backplane used in SHRIMP is admittedly not commodity hardware, but to first-order it resembles (both in design and performance) current commodity networks such as Tandem's ServerNet [41] and Myrinet <ref> [13] </ref>. At the network interface, SHRIMP uses its automatic and deliberate update mechanisms to support particular parallel programming models and constructs. This work relates to several prior efforts.
Reference: [14] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H. T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P.S. Tseng, J. Sutton, J. Urbanski, and J. Webb. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <pages> pages 330339, </pages> <year> 1988. </year>
Reference-contexts: The sender-based communication in Hamlyn also supports user-level message passing, but places more burden on application programs by requiring them to construct their own message headers [15]. Some previous machines have worked to streamline the hardware-software interface by mapping network interface FIFOs into processor registers <ref> [14, 25, 38] </ref>. Such approaches go against SHRIMP's goal of using commodity CPUs. A slightly less integrated approachmapping FIFOs to memory rather than registerswas employed in the CM-5 [43]. CM-5 implementation restrictions limited the degree of multiprogramming, however, and applications were still required to construct their own message headers.
Reference: [15] <author> Greg Buzzard, David Jacobson, Milon Mackey, Scott Marovich, and John Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 245260, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Our approach is to use a virtual memory-mapped communication model [12, 21], and implement it with some hardware support at the network interface level to minimize software overhead. Several other projects and commercial products have used similar memory-mapped communication models, including HP's Hamlyn project <ref> [15] </ref>, Digital's MemoryChannel [24] and Dolphin's network interface. <p> As with active messages [22], SHRIMP's mechanisms provide low-level support for fast communication and for effective overlap of communication with computation. The sender-based communication in Hamlyn also supports user-level message passing, but places more burden on application programs by requiring them to construct their own message headers <ref> [15] </ref>. Some previous machines have worked to streamline the hardware-software interface by mapping network interface FIFOs into processor registers [14, 25, 38]. Such approaches go against SHRIMP's goal of using commodity CPUs. A slightly less integrated approachmapping FIFOs to memory rather than registerswas employed in the CM-5 [43].
Reference: [16] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 152164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Finally, at the applications level, our software evaluations draw on prior work on several programming models. The shared virtual memory used here relates to a significant body of prior SVM research <ref> [16, 31, 34, 47] </ref>. We also leverage off of the NX model for message passing programs [39]. 6 Conclusions We constructed a 16-node prototype SHRIMP system and experimented with applications using various high-level APIs.
Reference: [17] <author> Stefanos Damianakis, Cezary Dubnicki, and Edward W. Felten. </author> <title> Stream Sockets on SHRIMP. </title> <booktitle> In Proc. of 1st Intl. Workshop on Communication and Architectural Support for Network-Based Parallel Computing (Proceedings available as Lecture Notes in Computer Science 1199), </booktitle> <month> February </month> <year> 1997. </year>
Reference-contexts: to be dynamically set as part of an explicit transfer initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library <ref> [17] </ref>, a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27]. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications.
Reference: [18] <author> G. S. Delp, D. J. Farber, R. G. Minnich, J. M. Smith, and M. C. Tam. </author> <title> Memory as a Network Abstraction. </title> <journal> IEEE Network, </journal> <volume> 5(4):3441, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24]. Page-based automatic-update approaches were also used in Memnet <ref> [18] </ref>, Merlin [37], SESAME [45], Plus [8] and Galactica Net [30]. These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support.
Reference: [19] <author> P. Druschel, B. S. Davie, and L. L. Peterson. </author> <title> Experiences with a High-Speed Network Adapter: A Software Perspective. </title> <booktitle> In Proceedings of SIGCOMM '94, </booktitle> <pages> pages 213, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: This work relates to several prior efforts. Spector [40] proposed a remote memory reference model to perform communication over a local area network and the implementation is programmed in a processor's microcode. This model has been revived by Thekkath et al. [42] using fast traps. Druschel et al. <ref> [19] </ref> proposed the concept of application device channels which provide protected user-level access to a network interface. U-Net [5] uses a similar abstraction to support high-level protocols such as TCP/IP.
Reference: [20] <author> Cezary Dubnicki, Angelos Bilas, Kai Li, and James Philbin. </author> <title> Design and Implementation of Virtual Memory-Mapped Communication on Myrinet. </title> <booktitle> In Proceedings of the IEEE 11th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: SHRIMP has a deliberate update latency of 6 s, while the best latency achieved with 166 MHz, PCI-bus based Pentium PCs and Myrinet network interfaces running our optimized firmware for the system: shared virtual memory, native VMMC, and NX message-passing library same API <ref> [20] </ref> is slightly under 10 s. Except for the automatic update mechanism, both systems implement the same VMMC API. The latency of SHRIMP is substantially better than that of the Myrinet system, even though the nodes in the SHRIMP system are much slower than those in the Myrinet system.
Reference: [21] <author> Cezary Dubnicki, Liviu Iftode, Edward W. Felten, and Kai Li. </author> <title> Software Support for Virtual Memory-Mapped Communication. </title> <booktitle> In Proceedings of the IEEE 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: The SHRIMP project studies how to design network interfaces to satisfy the design challenges. Our approach is to use a virtual memory-mapped communication model <ref> [12, 21] </ref>, and implement it with some hardware support at the network interface level to minimize software overhead. Several other projects and commercial products have used similar memory-mapped communication models, including HP's Hamlyn project [15], Digital's MemoryChannel [24] and Dolphin's network interface. <p> bit for an automatic update packet is stored in the OPT, while deliberate update allows the bit to be dynamically set as part of an explicit transfer initiation. 3 Applications and Experiments We have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library <ref> [21] </ref>, an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) [26, 27].
Reference: [22] <author> T. Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support. As with active messages <ref> [22] </ref>, SHRIMP's mechanisms provide low-level support for fast communication and for effective overlap of communication with computation. The sender-based communication in Hamlyn also supports user-level message passing, but places more burden on application programs by requiring them to construct their own message headers [15].
Reference: [23] <author> Edward W. Felten, Richard Alpert, Angelos Bilas, Matthias A. Blumrich, Douglas W. Clark, Stefanos Damianakis, Cezary Dubnicki, Liviu Iftode, and Kai Li. </author> <title> Early Experience with Message-Passing on the Shrimp Multicomputer. </title> <booktitle> In Proceedings of the 23nd Annual Symposium on Computer Architecture, </booktitle> <pages> pages 296307, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: emphasis on avoiding receiver side interrupts? In addition to answering these questions, we discuss other lessons learned, including some things that consumed much of our design time, yet turned out not to matter. 2 The SHRIMP System The architecture of the SHRIMP system has been described in several previous publications <ref> [10, 11, 12, 23] </ref> notably [9]and will only be described in as much detail as necessary here.
Reference: [24] <author> Richard B. Gillett. </author> <title> Memory Channel Network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1):1218, </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: Our approach is to use a virtual memory-mapped communication model [12, 21], and implement it with some hardware support at the network interface level to minimize software overhead. Several other projects and commercial products have used similar memory-mapped communication models, including HP's Hamlyn project [15], Digital's MemoryChannel <ref> [24] </ref> and Dolphin's network interface. <p> SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes <ref> [24] </ref>. Page-based automatic-update approaches were also used in Memnet [18], Merlin [37], SESAME [45], Plus [8] and Galactica Net [30]. These prior systems did not, however, provide for both automatic and deliberate update.
Reference: [25] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly--Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectur al Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The sender-based communication in Hamlyn also supports user-level message passing, but places more burden on application programs by requiring them to construct their own message headers [15]. Some previous machines have worked to streamline the hardware-software interface by mapping network interface FIFOs into processor registers <ref> [14, 25, 38] </ref>. Such approaches go against SHRIMP's goal of using commodity CPUs. A slightly less integrated approachmapping FIFOs to memory rather than registerswas employed in the CM-5 [43]. CM-5 implementation restrictions limited the degree of multiprogramming, however, and applications were still required to construct their own message headers.
Reference: [26] <author> Liviu Iftode, Cezary Dubnicki, Edward Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In Proceedings of IEEE 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) <ref> [26, 27] </ref>. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications. In this paper we selected applications based on four different APIs: VMMC, NX, Stream sockets, and SVM. <p> We call this approach HLRC-AU. The third approach implements the Automatic Update Release Consistency (AURC) protocol <ref> [26] </ref>. This implementation eliminates diffs entirely and uses automatic update mappings to propagate updates eagerly to home pages. The left-hand side of Figure 4 compares the three SVM implementations using three different applications on the 16-node SHRIMP system.
Reference: [27] <author> Liviu Iftode, Jaswinder Pal Singh, and Kai Li. </author> <title> Scope Consistency: A Bridge between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: have implemented several high-level communication APIs and systems on the SHRIMP multicomputer, including the native VMMC library [21], an NX message-passing library [2], a BSP message-passing library [3], a Unix stream sockets compatible library [17], a Sun-RPC compatible library [7], a specialized RPC library [7], and Shared Virtual Memory (SVM) <ref> [26, 27] </ref>. Each API implementation takes advantage of the low-overhead, user-level communication mechanisms on the system and supports a few applications. In this paper we selected applications based on four different APIs: VMMC, NX, Stream sockets, and SVM.
Reference: [28] <author> Intel Corporation. </author> <title> Asynchronous Modular Architecture Memory Bus Interface Specification, </title> <journal> Rev 1.4. Document number 130-1354-44. </journal>
Reference-contexts: The transceiver boards are necessary because the PCs and the backplane are on separate power supplies, requiring differential signaling between them. The SHRIMP network interface (Figure 1) consists of two boards because it connects to both the Xpress memory bus <ref> [28] </ref> and the EISA I/O bus [6]. The memory-bus board simply snoops all main-memory writes, passing address and data pairs to the EISA-bus board. The EISA-bus board contains the bulk of the hardware, and connects to the routing backplane. Figure 2 shows the principal datapaths of the network interface.
Reference: [29] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Specific details of the architecture and implementation will be described more thoroughly throughout this paper. 2.1 Architecture The SHRIMP system consists of sixteen PC nodes connected by an Intel routing backplane, which is the same as that used for the Paragon multicomputer <ref> [29] </ref>. The backplane is organized as a two-dimensional mesh, and supports oblivious, wormhole routing with a maximum link bandwidth of 200 Mbytes/second [44]. The right-hand photograph in Figure 1 shows the basic interconnection between the nodes and the backplane.
Reference: [30] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware Assist for Distributed Shared Memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24]. Page-based automatic-update approaches were also used in Memnet [18], Merlin [37], SESAME [45], Plus [8] and Galactica Net <ref> [30] </ref>. These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support.
Reference: [31] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 1321, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Finally, at the applications level, our software evaluations draw on prior work on several programming models. The shared virtual memory used here relates to a significant body of prior SVM research <ref> [16, 31, 34, 47] </ref>. We also leverage off of the NX model for message passing programs [39]. 6 Conclusions We constructed a 16-node prototype SHRIMP system and experimented with applications using various high-level APIs.
Reference: [32] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, C.-Q Zhu, A. Veidenbaum, J. Konicek, P. Yew, K. Gallivan, W. Jalby, H. Wijshoff, R. Bramley, U.M. Yang, P. Emrath, D. Padua, R. Eigenmann, J. Hoefinger, G. Jaxon, Z. Li, T. Murphy, J. Andrewes, and S. Turner. </author> <title> The Cedar System and an Initial Performance Study. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 213223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A key contribution of this paper is an empirical design retrospective based on a working 16-node SHRIMP system. In that sense, this paper can be categorized along with previous design evaluations of research machines such as the DASH multiprocessor [33], the Illinois Cedar machine <ref> [32] </ref>, the MIT Alewife multiprocessor [1], and the J-machine multicomputer [38]. SHRIMP has leveraged commodity components to a much greater degree than J-machine, Cedar, Alewife or even DASH, thus this paper focuses primarily on evaluating its custom hardware support for communication.
Reference: [33] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The Stanford DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):4161, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: Here we discuss a selection of closely-related papers. A key contribution of this paper is an empirical design retrospective based on a working 16-node SHRIMP system. In that sense, this paper can be categorized along with previous design evaluations of research machines such as the DASH multiprocessor <ref> [33] </ref>, the Illinois Cedar machine [32], the MIT Alewife multiprocessor [1], and the J-machine multicomputer [38]. SHRIMP has leveraged commodity components to a much greater degree than J-machine, Cedar, Alewife or even DASH, thus this paper focuses primarily on evaluating its custom hardware support for communication.
Reference: [34] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, volume II Software, </booktitle> <pages> pages 94101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Finally, at the applications level, our software evaluations draw on prior work on several programming models. The shared virtual memory used here relates to a significant body of prior SVM research <ref> [16, 31, 34, 47] </ref>. We also leverage off of the NX model for message passing programs [39]. 6 Conclusions We constructed a 16-node prototype SHRIMP system and experimented with applications using various high-level APIs.
Reference: [35] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229239, </pages> <month> August </month> <year> 1986. </year> <note> A revised version appeared in ACM Transactions on Computer Systems, 7(4):321359, </note> <month> November </month> <year> 1989. </year>
Reference-contexts: In particular, large data structures that are written sparsely can be exported in their entirety, and mapped remotely for automatic update. We built three implementations to evaluate the impact of using automatic update support to improve the performance of shared virtual memory <ref> [35] </ref>. The first implements the HLRC protocol [47] which uses only deliberate update communication. The second is similar to the first except that it uses automatic update to propagate the diffs transparently as they are produced, instead of buffering them and sending them explicitly using deliberate update messages.
Reference: [36] <author> Richard Lipton and Jonathan Sandberg. </author> <title> PRAM: A Scalable Shared Memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Druschel et al. [19] proposed the concept of application device channels which provide protected user-level access to a network interface. U-Net [5] uses a similar abstraction to support high-level protocols such as TCP/IP. The automatic update mechanism in SHRIMP is derived from the Pipelined RAM network interface <ref> [36] </ref>, but is able to perform virtual memory-mapped communication and map DRAM memory instead of dedicated memory on the network interface board. SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24].
Reference: [37] <author> Creve Maples. </author> <title> A High-Performance, Memory-Based Interconnection System For Multicomputer Environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24]. Page-based automatic-update approaches were also used in Memnet [18], Merlin <ref> [37] </ref>, SESAME [45], Plus [8] and Galactica Net [30]. These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support.
Reference: [38] <author> Michael D. Noakes, Deborah A. Wallach, and William J. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 224235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In that sense, this paper can be categorized along with previous design evaluations of research machines such as the DASH multiprocessor [33], the Illinois Cedar machine [32], the MIT Alewife multiprocessor [1], and the J-machine multicomputer <ref> [38] </ref>. SHRIMP has leveraged commodity components to a much greater degree than J-machine, Cedar, Alewife or even DASH, thus this paper focuses primarily on evaluating its custom hardware support for communication. <p> The sender-based communication in Hamlyn also supports user-level message passing, but places more burden on application programs by requiring them to construct their own message headers [15]. Some previous machines have worked to streamline the hardware-software interface by mapping network interface FIFOs into processor registers <ref> [14, 25, 38] </ref>. Such approaches go against SHRIMP's goal of using commodity CPUs. A slightly less integrated approachmapping FIFOs to memory rather than registerswas employed in the CM-5 [43]. CM-5 implementation restrictions limited the degree of multiprogramming, however, and applications were still required to construct their own message headers.
Reference: [39] <author> Paul Pierce. </author> <title> The NX Message Passing Interface. </title> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Finally, at the applications level, our software evaluations draw on prior work on several programming models. The shared virtual memory used here relates to a significant body of prior SVM research [16, 31, 34, 47]. We also leverage off of the NX model for message passing programs <ref> [39] </ref>. 6 Conclusions We constructed a 16-node prototype SHRIMP system and experimented with applications using various high-level APIs. We found that the SHRIMP multicomputer performs quite well for applications that do not perform very well with traditional network interfaces.
Reference: [40] <author> Alfred Z. Spector. </author> <title> Performing Remote Operations Efficiently on a Local Computer Network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4):260273, </volume> <month> April </month> <year> 1982. </year>
Reference-contexts: At the network interface, SHRIMP uses its automatic and deliberate update mechanisms to support particular parallel programming models and constructs. This work relates to several prior efforts. Spector <ref> [40] </ref> proposed a remote memory reference model to perform communication over a local area network and the implementation is programmed in a processor's microcode. This model has been revived by Thekkath et al. [42] using fast traps.
Reference: [41] <institution> ServerNet Interconnect Technology. </institution> <note> http://www.tandem.fi/product/snet1.htm, 1997. </note>
Reference-contexts: In terms of networking fabric, the Intel Paragon backplane used in SHRIMP is admittedly not commodity hardware, but to first-order it resembles (both in design and performance) current commodity networks such as Tandem's ServerNet <ref> [41] </ref> and Myrinet [13]. At the network interface, SHRIMP uses its automatic and deliberate update mechanisms to support particular parallel programming models and constructs. This work relates to several prior efforts.
Reference: [42] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating Data and Control Transfer in Distributed Operating Systems. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2 11, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: This work relates to several prior efforts. Spector [40] proposed a remote memory reference model to perform communication over a local area network and the implementation is programmed in a processor's microcode. This model has been revived by Thekkath et al. <ref> [42] </ref> using fast traps. Druschel et al. [19] proposed the concept of application device channels which provide protected user-level access to a network interface. U-Net [5] uses a similar abstraction to support high-level protocols such as TCP/IP.
Reference: [43] <institution> Thinking Machines Corporation. </institution> <type> Connection Machine CM-5 Technical Summary, </type> <month> November </month> <year> 1992. </year>
Reference-contexts: Some previous machines have worked to streamline the hardware-software interface by mapping network interface FIFOs into processor registers [14, 25, 38]. Such approaches go against SHRIMP's goal of using commodity CPUs. A slightly less integrated approachmapping FIFOs to memory rather than registerswas employed in the CM-5 <ref> [43] </ref>. CM-5 implementation restrictions limited the degree of multiprogramming, however, and applications were still required to construct their own message headers. Finally, at the applications level, our software evaluations draw on prior work on several programming models.
Reference: [44] <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The backplane is organized as a two-dimensional mesh, and supports oblivious, wormhole routing with a maximum link bandwidth of 200 Mbytes/second <ref> [44] </ref>. The right-hand photograph in Figure 1 shows the basic interconnection between the nodes and the backplane. The backplane is actually relatively small but, for convenience, we power it with the standard Paragon cabinet which is capable of housing a complete 64-node system.
Reference: [45] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager Sharing for Efficient Massive Parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: SHRIMP's automatic update is also similar to MemoryChannel (developed independently and concurrently at Digital), in which memory updates are automatically reflected to other nodes [24]. Page-based automatic-update approaches were also used in Memnet [18], Merlin [37], SESAME <ref> [45] </ref>, Plus [8] and Galactica Net [30]. These prior systems did not, however, provide for both automatic and deliberate update. This paper also quantifies the relationship between particular low-level hardware primitives and the performance of the higher-level software they support.
Reference: [46] <author> Steven C. Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 2437, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Characteristics of the applications used in our experiments. (Ocean-NX does not run on a uniprocessor; two-node running time is given) Barnes-SVM This application is from the SPLASH-2 benchmark suite <ref> [46] </ref>. It uses the Barnes-Hut hierarchical N-body method to simulate the interactions among a system of particles over time. The computational domain is represented as an octree of space cells.
Reference: [47] <author> Yuanyuan Zhou, Liviu Iftode, and Kai Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 7588, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: In particular, large data structures that are written sparsely can be exported in their entirety, and mapped remotely for automatic update. We built three implementations to evaluate the impact of using automatic update support to improve the performance of shared virtual memory [35]. The first implements the HLRC protocol <ref> [47] </ref> which uses only deliberate update communication. The second is similar to the first except that it uses automatic update to propagate the diffs transparently as they are produced, instead of buffering them and sending them explicitly using deliberate update messages. We call this approach HLRC-AU. <p> Finally, at the applications level, our software evaluations draw on prior work on several programming models. The shared virtual memory used here relates to a significant body of prior SVM research <ref> [16, 31, 34, 47] </ref>. We also leverage off of the NX model for message passing programs [39]. 6 Conclusions We constructed a 16-node prototype SHRIMP system and experimented with applications using various high-level APIs.
References-found: 47

