URL: ftp://ftp.cs.toronto.edu/pub/parallel/Gamsa_etal_ICPP94.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: ben@cs.toronto.edu  Email: okrieg@eecg.toronto.edu Email: stumm@eecg.toronto.edu  
Title: Optimizing IPC Performance for Shared-Memory Multiprocessors  
Author: Benjamin Gamsa Orran Krieger and Michael Stumm 
Address: Toronto, Canada M5S 1A4  Toronto, Canada M5S 1A4  
Affiliation: Department of Computer Science University of Toronto  Department of Electrical and Computer Engineering University of Toronto  
Note: Proc. Intl. Conf. on Parallel Processing, 1994.  
Abstract: We assert that in order to perform well, a shared-memory multiprocessor inter-process communication (IPC) facility must avoid a) accessing any shared data, and b) acquiring any locks. In addition, such a multiprocessor IPC facility must preserve the locality and concurrency of the applications themselves so that the high performance of the IPC facility can be fully exploited. In this paper we describe the design and implementation of a new shared-memory multiprocessor IPC facility that in the common case internally requires no accesses to shared data and no locking. In addition, the model of IPC we support and our implementation ensure that local resources are made available to the server to allow it to exploit any locality and concurrency available in the service. To the best of our knowledge, this is the first IPC subsystem with these attributes. The performance data we present demonstrates that the end-to-end performance of our multiprocessor IPC facility is competitive with the fastest uniprocessor IPC times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian N. Bershad. </author> <title> The increasing irrelevance of IPC performance for microkernel-based operating systems. </title> <booktitle> In Proceedings of the Usenix Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 205-212, </pages> <address> Seattle, 1992. </address> <publisher> Usenix. </publisher>
Reference-contexts: To date, the majority of the research on performance conscious IPC has been done on uniprocessor systems. Excellent results have been reported for these systems, to the point where Bershad argues that the IPC overhead has become largely irrelevant <ref> [1] </ref>. <p> For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [12]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 <ref> [1, 8] </ref>; and QNX requires 76 secs on a 33MHz 486 [11].
Reference: [2] <author> Brian N. Bershad, Thomas E. Anderson,Edward D. Lazowska, and Henry M. Levy. </author> <title> Lightweight remote procedure call. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 102-13, </pages> <year> 1989. </year>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [2, 5, 7] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call handling code on each call, affecting an upcall directly into the service routine. <p> A possible compromise solution would collect servers that trust each other into groups and only share stacks between servers in the same Page 2 group. Probably the most important prior work in performance conscious multiprocessor IPCs is Bershad's Light-Weight RPC (LRPC) facility <ref> [2] </ref>. On the surface LRPC appears similar to our own. For example, it uses the same PPC model as its basic abstraction, and it is designed to minimize the use of shared data and locks.
Reference: [3] <author> Brian N. Bershad, Richard P. Draves, and Alessandro Forsin. </author> <title> Mi-crobenchmarks to evaluate system performance. </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems(WWOS-3), </booktitle> <year> 1992. </year>
Reference-contexts: Hence, the NUMAness is not addressed here. To measure the cost of individual PPC operations, we used a microsecond timer (with 10 cycle access overhead), thus eliminating some of the problems associated with microbenchmark-ing <ref> [3] </ref>. Figure 2 shows the performance of the PPC operations under a variety of conditions. A round trip user-to-user null call (with up to 8 arguments) requires approximately 34.1 sec if the cache is warm.
Reference: [4] <author> D. L. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <year> 1990. </year>
Reference-contexts: These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory [6]; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques <ref> [4, 6] </ref>; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance.
Reference: [5] <author> Jeffrey S. Chase, Henry M. Levy, Michael J. Feeley, and Edward D. Lazowska. </author> <title> Sharing and protection in a single address space operating system. </title> <type> Technical Report TR-93-04-02, </type> <institution> Department of Computer Science and Engineering, Unversity of Washington, </institution> <address> Seattle, WA 98195, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [2, 5, 7] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call handling code on each call, affecting an upcall directly into the service routine.
Reference: [6] <author> David R. Cheriton. </author> <title> An experiment using registers for fast message-based interprocess communication. </title> <journal> Operating System Review, </journal> (4):12-20, 1984. 
Reference-contexts: These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory <ref> [6] </ref>; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques [4, 6]; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance. <p> These implementations apply a common set of techniques to achieve good performance: i) registers are used to directly pass data across address spaces, circumventing the need to use slow memory [6]; ii) the generalities of the scheduling subsystem are avoided with hand-off scheduling techniques <ref> [4, 6] </ref>; iii) code and data is organized to minimize the number of cache misses and TLB faults; and iv) architectural and machine-specific features are exploited or avoided depending on whether they help or hinder performance.
Reference: [7] <author> Partha Dasgupta, Richard J. Leblanc, Jr., and William F. Appelbe. </author> <title> The Clouds distributed operating systems: Functional description, implementation details and related work. </title> <booktitle> In The 8th International Conference on Distributed Computer Systems, </booktitle> <pages> pages 2-9, </pages> <address> S. Jose CA (USA), </address> <month> June </month> <year> 1988. </year> <note> (IEEE). </note>
Reference-contexts: The PPC model is thus a key component to enabling locality and concurrency within servers. Although PPCs are most naturally implemented by having the invoking process cross directly into the server's address space <ref> [2, 5, 7] </ref>, our implementation uses separate worker processes in the server to service client calls. Worker processes are created dynamically as needed and (re)initialized to the server's call handling code on each call, affecting an upcall directly into the service routine.
Reference: [8] <author> Richard P. Draves, Brian N. Bershad, Richard F. Rashid, and Randall W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 122-36. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [12]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 <ref> [1, 8] </ref>; and QNX requires 76 secs on a 33MHz 486 [11].
Reference: [9] <author> B. Gamsa, O. Krieger, and M. Stumm. </author> <title> Optimizing IPC performance for shared-memory multiprocessors. </title> <type> Technical Report 294, </type> <institution> CSRI, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: The performance results presented in Figure 2 are for a quiet system with only a single client making calls. Results from measurements with multiple clients making requests to a common server are presented in <ref> [9] </ref>, and show a near-linear increase in throughput as the number of clients is increased. 4 Concluding Remarks We have described the design and implementation of a new shared-memory multiprocessor client-server interprocess communication system, and discussed the most important tradeoffs in its design.
Reference: [10] <author> G. Hamilton and P. Kougiouris. </author> <title> The Spring nucleus: A micro-kernel for objects. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference. Usenix, </booktitle> <year> 1993. </year>
Reference-contexts: The call descriptors serve two purposes: they store return information during a call, and they point to physical memory used for the stack of a worker process during a call. These pools are accessed exclusively by the local processor. 2 Similar factors were coincidentally noted in Spring <ref> [10] </ref>. 3 If a server supports multiple services, there is one pool per service.
Reference: [11] <author> Dan Hildebrand. </author> <title> Architectural overview of QNX. </title> <booktitle> In Proceedings of the Usenix Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 113-126, </pages> <address> Seattle, 1992. </address> <publisher> Usenix. </publisher>
Reference-contexts: 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC [12]; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 [1, 8]; and QNX requires 76 secs on a 33MHz 486 <ref> [11] </ref>.
Reference: [12] <author> J. Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-187, </pages> <year> 1993. </year>
Reference-contexts: Excellent results have been reported for these systems, to the point where Bershad argues that the IPC overhead has become largely irrelevant [1]. For example, Liedtke reports requiring 60 secs on a 20 MHz 386 system and 10 secs on a 50MHz 486 system for a null, round-trip RPC <ref> [12] </ref>; a recent version of Mach requires 57 secs on a 25 MHz MIPS R3000 and 95 secs on a 16 MHz MIPS R2000 [1, 8]; and QNX requires 76 secs on a 33MHz 486 [11].
Reference: [13] <author> Michael Stumm, Ron Unrau, and Orran Krieger. </author> <title> Designing a Scalable Operating System for Shared Memory Multiprocessors. </title> <booktitle> In USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <pages> pages 285-303, </pages> <address> Seattle, Wa., </address> <month> April </month> <year> 1992. </year>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system <ref> [13, 15] </ref> running on the Hector shared memory multiprocessor [16]. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. <p> We believe that the overhead of our facility is close to the minimum achievable on our platform. We have incorporated this facility into the Hurricane operating system <ref> [13, 15] </ref>, and adapted most of the servers to use it.
Reference: [14] <author> Charles P. Thacker, Lawrence C. Stewart, and Edwin H. Satterth-waite, Jr. Firefly: </author> <title> A Multiprocessor Workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: It is also interesting to observe how the recent changes in technology lead to design tradeoffs far different from what they used to be. The Firefly multiprocessor <ref> [14] </ref> on which Bershad's IPC work was developed has a smaller ratio of processor to memory speed, has caches that are no faster than main memory (but are used to reduce bus traffic), and uses an updating cache consistency protocol.
Reference: [15] <author> R. Unrau, M. Stumm, O. Krieger, and B. Gamsa. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <type> Technical Report CSRI-268, </type> <institution> Computer Systems Research Institute, University of Toronto, Toronto, Canada, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system <ref> [13, 15] </ref> running on the Hector shared memory multiprocessor [16]. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. <p> We believe that the overhead of our facility is close to the minimum achievable on our platform. We have incorporated this facility into the Hurricane operating system <ref> [13, 15] </ref>, and adapted most of the servers to use it.
Reference: [16] <author> Zvonko G. Vranesic, Michael Stumm, Ron White, and David Lewis. </author> <title> The Hector Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year> <pages> Page 4 </pages>
Reference-contexts: This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 3 Performance The platform used for our implementation is the Hurricane operating system [13, 15] running on the Hector shared memory multiprocessor <ref> [16] </ref>. These experiments were performed on a fully configured but otherwise idle 16 processor system. This prototype system uses Motorola 88100/88200 processors running at 16.67 MHz, with 16KB data and instruction caches and a 16 byte line size. Each processor has a local portion of the globally accessible memory. <p> Although many of the design decisions were influenced by our particular hardware base, a M88000-based, non-cache-coherent multiprocessor <ref> [16] </ref>, we argue that similar design decisions would apply to most current multiprocessors.
References-found: 16

