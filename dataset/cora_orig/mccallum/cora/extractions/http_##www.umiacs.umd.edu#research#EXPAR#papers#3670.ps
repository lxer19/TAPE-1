URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3670.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3670.html
Root-URL: 
Email: dbaderg@umiacs.umd.edu  
Title: A New Deterministic Parallel Sorting Algorithm With an Experimental Evaluation  
Author: David R. Helman Joseph JaJa David A. Bader fhelman, joseph, 
Keyword: Parallel Algorithms, Generalized Sorting, Integer Sorting, Sorting by Regular Sam pling, Parallel Performance.  
Note: Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183. The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged.  
Date: August 15, 1996  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Department of Electrical Engineering, University of Maryland,  
Abstract: We introduce a new deterministic parallel sorting algorithm based on the regular sampling approach. The algorithm uses only two rounds of regular all-to-all personalized communication in a scheme that yields very good load balancing with virtually no overhead. Moreover, unlike previous variations, our algorithm efficiently handles the presence of duplicate values without the overhead of tagging each element with a unique identifier. This algorithm was implemented in Split-C and run on a variety of platforms, including the Thinking Machines CM-5, the IBM SP-2-WN, and the Cray Research T3D. We ran our code using widely different benchmarks to examine the dependence of our algorithm on the input distribution. Our experimental results illustrate the efficiency and scalability of our algorithm across different platforms. In fact, the performance compares closely to that of our random sample sort algorithm, which seems to outperform all similar algorithms known to the authors on these platforms. Together, their performance is nearly invariant over the set of input distributions, unlike previous efficient algorithms. However, unlike our randomized sorting algorithm, the performance and memory requirements of our regular sorting algorithm can be deterministically guaranteed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP Model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [11, 19, 1] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps. <p> Randomized Duplicates [RD], an input of duplicates in which each processor fills an array T with some constant number range (range is 32 for our work) of random values between 0 and (range 1) whose sum is S. The first T <ref> [1] </ref> p values of the input are then set to a random value between 0 and (range 1), the next T [2] p values of the input are then set to another random value between 0 and (range 1), and so forth. 9 See [14] for a detailed justification of these
Reference: [2] <author> R.H. Arpaci, D.E. Culler, A. Krishnamurthy, S.G. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <publisher> In ACM Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The first T [1] p values of the input are then set to a random value between 0 and (range 1), the next T <ref> [2] </ref> p values of the input are then set to another random value between 0 and (range 1), and so forth. 9 See [14] for a detailed justification of these benchmarks. 4.2 Experimental Results For each experiment, the input is evenly distributed amongst the processors. <p> We acknowledge the use of the UMIACS 16-node IBM SP-2-TN2, which was provided by an IBM Shared University Research award and an NSF Academic Research Infrastructure Grant No. CDA9401151. Arvind Krishnamurthy provided additional help with his port of Split-C to the Cray Research T3D <ref> [2] </ref>. The Jet Propulsion Lab/Caltech 256-node Cray T3D Supercomputer used in this investigation was provided by funding from the NASA Offices of Mission to Planet Earth, Aeronautics, and Space Science.
Reference: [3] <author> D.A. Bader, D.R. Helman, and J. JaJa. </author> <title> Practical Parallel Algorithms for Personalized Communication and Integer Sorting. </title> <institution> CS-TR-3548 and UMIACS-TR-95-101 Technical Report, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> November </month> <year> 1995. </year> <note> To appear in ACM Journal of Experimental Algorithmics. </note>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of 2 which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm.
Reference: [4] <author> D.A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <institution> Technical Report CS-TR-3384 and UMIACS-TR-94-133, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1994. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of 2 which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm.
Reference: [5] <author> D.A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <booktitle> In Fifth ACM SIGPLAN Symposium of Principles and Practice of Parallel Programming, </booktitle> <pages> pages 123-133, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of 2 which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm.
Reference: [6] <author> D.A. Bader and J. JaJa. </author> <title> Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection. </title> <institution> Technical Report CS-TR-3494 and UMIACS-TR-95-74, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> July </month> <year> 1995. </year> <booktitle> Presented at the 10th International Parallel Processing Symposium, </booktitle> <pages> pages 292-301, </pages> <address> Honolulu, HI, </address> <month> April 15-19, </month> <year> 1996. </year>
Reference-contexts: The remote read and write typically have both blocking and non-blocking versions. Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in <ref> [6] </ref>, are similar to those of the MPI [17], the IBM POWERparallel [7], and the Cray MPP systems [9] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of 2 which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm. <p> Steps (2), (5), and (7) call the communica tion primitives transpose, bcast, and transpose, respectively. The analysis of these primitives in <ref> [6] </ref> shows that these three steps require T comm (n; p) p 2 (p 1) , T comm (n; p) (t +2 (p1)), and T comm (n; p) p 2 + n , respectively.
Reference: [7] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6 </volume> <pages> 154-164, </pages> <year> 1995. </year>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI [17], the IBM POWERparallel <ref> [7] </ref>, and the Cray MPP systems [9] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Second, different implementations of the communication primitives were allowed for each machine. Wherever possible, we tried to use the vendor supplied implementations. In fact, IBM does provide all of our communication primitives as part of its machine specific Collective Communication Library (CCL) <ref> [7] </ref> and MPI. As one might expect, they were faster than the high level Split-C implementation.
Reference: [8] <author> W.W. Carlson and J.M. Draper. </author> <title> AC for the T3D. </title> <type> Technical Report SRC-TR-95-141, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: We also acknowledge William Carlson and Jesse Draper from the Center for Computing Science (formerly Supercomputing Research Center) for writing the parallel compiler AC (version 2.6) <ref> [8] </ref> on which the T3D port of Split-C has been based. 18 We also thank the Numerical Aerodynamic Simulation Systems Division of NASA's Ames Research Center for use of their 160-node IBM SP-2-WN.
Reference: [9] <institution> Cray Research, Inc. </institution> <note> SHMEM Technical Note for C, October 1994. Revision 2.3. 20 </note>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI [17], the IBM POWERparallel [7], and the Cray MPP systems <ref> [9] </ref> and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [10] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Together, their performance is nearly indifferent to the set of input distributions, unlike previous efficient algorithms. However, unlike our randomized sorting algorithm, the performance and memory requirements of our regular sorting algorithm can be guaranteed with probability one. The high-level language used in our studies is Split-C <ref> [10] </ref>, an extension of C for distributed memory machines. The algorithm makes use of MPI-like communication primitives but does not make any assumptions as to how these primitives are actually implemented. The basic data transport is a read or write operation. <p> to model the cost of the irregular communication used by the most efficient algorithms. 7 Hence, it is very important to perform an empirical evaluation of an algorithm using a wide variety of benchmarks, as we will do next. 4 Performance Evaluation Our sample sort algorithm was implemented using Split-C <ref> [10] </ref> and run on a variety of machines and processors, including the Cray Research T3D, the IBM SP-2-WN, and the Thinking Machines CM-5.
Reference: [11] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [11, 19, 1] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps.
Reference: [12] <author> A.V. Gerbessiotis. </author> <title> Data for Regular Sorting. </title> <type> Personal Communication, </type> <month> July </month> <year> 1996. </year>
Reference-contexts: All but one of these studies used machines which are no longer available. The single exception is the recent work of Gerbessiotis and Siniolakis <ref> [13, 12] </ref>. Table X compares the performance of their deterministic algorithm with that of our regular sampling algorithm on an IBM SP-2.
Reference: [13] <author> A.V. Gerbessiotis and C.J. Siniolakis. </author> <title> Deterministic Sorting and Randomized Median Finding on the BSP Model. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 223-232, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: All but one of these studies used machines which are no longer available. The single exception is the recent work of Gerbessiotis and Siniolakis <ref> [13, 12] </ref>. Table X compares the performance of their deterministic algorithm with that of our regular sampling algorithm on an IBM SP-2.
Reference: [14] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> A Randomized Parallel Sorting Algorithm With an Experimental Study. </title> <institution> Technical Report CS-TR-3669 and UMIACS-TR-96-53, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: The first T [1] p values of the input are then set to a random value between 0 and (range 1), the next T [2] p values of the input are then set to another random value between 0 and (range 1), and so forth. 9 See <ref> [14] </ref> for a detailed justification of these benchmarks. 4.2 Experimental Results For each experiment, the input is evenly distributed amongst the processors. <p> to sort the [U] integer benchmark on the IBM SP-2, comparing our results (HJB) with those reported by Gerbessiotis and Siniolakis (GS) . 4.4 Comparison With Our Sample Sort Algorithm Table XI compares the performance of our sorting by regular sampling algorithm with that of our random sample sort algorithm <ref> [14] </ref> on both the T3D and the SP-2-WN using the [WR] benchmark.
Reference: [15] <author> D.R. Helman, D.A. Bader, and J. JaJa. </author> <title> Parallel Algorithms for Personalized Communication and Sorting With an Experimental Study. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 211-220, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: 1 Introduction We present a novel variation on the approach of sorting by regular sampling which leads to a new deterministic sorting algorithm that achieves optimal computational speedup with very little communication <ref> [15] </ref>. Our algorithm exchanges the single step of irregular communication used by previous implementations for two steps of regular communication. In return, our algorithm reduces the problem of poor load balancing because it is able to sustain a high sampling rate at substantially less cost.
Reference: [16] <author> X. Li, P. Lu, J. Schaeffer, J. Shillington, P.S. Wong, and H. Shi. </author> <title> On the Versatility of Parallel Sorting by Regular Sampling. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1079-1103, </pages> <year> 1993. </year>
Reference-contexts: One way to choose the splitters is by regularly sampling the sorted input elements at each processor hence the name Sorting by Regular Sampling. A previous version of regular sample sort <ref> [18, 16] </ref>, known as Parallel Sorting by Regular Sampling (PSRS) , first sorts the n p elements at each processor and then selects every p 2 element as a sample. <p> This could be reduced by choosing more samples, but this would also increase the overhead. And no matter how many samples are chosen, previous studies have shown that the load balance would still deteriorate linearly with the number of duplicates <ref> [16] </ref>. One could, of course, tag each item with a unique value, but this would also double the cost of both memory access and interprocessor communication.
Reference: [17] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: And our algorithm achieves predictable, regular communication requirements which are essentially invariant with respect to the input distribution. Utilizing regular communication has become more important with the advent of message passing standards, such as MPI <ref> [17] </ref>, which seek to guarantee the availability of very efficient (often machine specific) implementations of certain basic collective communication routines. Our algorithm was implemented in a high-level language and run on a variety of platforms, including the Thinking Machines CM-5, the IBM SP-2, and the Cray Research T3D. <p> Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI <ref> [17] </ref>, the IBM POWERparallel [7], and the Cray MPP systems [9] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Moreover, such an irregular communication scheme cannot take advantage of the regular communication primitives proposed under the MPI standard <ref> [17] </ref>. In our algorithm, which is parameterized by a sampling ratio s p 2 , we guarantee that, at the completion of sorting, each processor will have at most p + n elements, while incurring no overhead in gathering the set of samples used to identify the splitters.
Reference: [18] <author> H. Shi and J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 361-372, </pages> <year> 1992. </year>
Reference-contexts: One way to choose the splitters is by regularly sampling the sorted input elements at each processor hence the name Sorting by Regular Sampling. A previous version of regular sample sort <ref> [18, 16] </ref>, known as Parallel Sorting by Regular Sampling (PSRS) , first sorts the n p elements at each processor and then selects every p 2 element as a sample.
Reference: [19] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [11, 19, 1] </ref>) that have recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps.
References-found: 19

