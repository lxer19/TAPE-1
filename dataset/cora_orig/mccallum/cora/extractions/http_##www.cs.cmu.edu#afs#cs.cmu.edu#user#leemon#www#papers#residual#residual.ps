URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/residual/residual.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Email: baird@cs.usafa.af.mil  
Title: Residual Algorithms: Reinforcement Learning with Function Approximation  
Author: Leemon Baird 
Web: http://kirk.usafa.af.mil/~baird  
Address: CO 80840-6234  
Affiliation: Department of Computer Science U.S. Air Force Academy,  
Abstract: A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basis-function system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Q-learning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L. C. </author> <year> (1995). </year> <note> Advantage Learning. To be published as a U.S. </note> <institution> Air Force technical report by the Department of Computer Science, U.S. Air Force Academy. </institution>
Reference-contexts: For this particular type of MDP, if each V (x) is a separate entry in a lookup table, then update (2) is also equivalent to three other reinforcement learning algorithms: TD (0) (Sutton 88), Q-learning (Watkins 89), and advantage learning <ref> (Baird 95) </ref>.
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Such problems are often solved using algorithms based upon dynamic programming <ref> (Bertsekas 87) </ref>, which involves storing information associated with each state, then updating the information in one state based upon the information in subsequent states.
Reference: <author> Bradtke, S. </author> <title> J (1993). Reinforcement learning applied to linear quadratic regulation. </title> <booktitle> Proceedings of the Fifth Conference on Neural Information Processing Systems (pp. </booktitle> <pages> 295-302). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Algorithms have been proved to converge for quadratic function-approximation systems <ref> (Bradtke 93) </ref>, but it would be useful to find an algorithm that converges for any function-approximation system. To find an algorithm that is more stable than the direct algorithm, it is useful to specify the exact goal for the learning system.
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature. </journal> <volume> 323 , 9 October, </volume> <pages> 533-536. </pages>
Reference-contexts: If V (x) was represented by a function-approximation system other than a lookup table, update (2) could be implemented directly by combining it with the backpropagation algorithm <ref> (Rumelhart, Hinton, Williams 86) </ref>.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 , 9-44. </pages>
Reference-contexts: For this particular type of MDP, if each V (x) is a separate entry in a lookup table, then update (2) is also equivalent to three other reinforcement learning algorithms: TD (0) <ref> (Sutton 88) </ref>, Q-learning (Watkins 89), and advantage learning (Baird 95).
Reference: <author> Tesauro, G. </author> <year> (1990). </year> <title> Neurogammon: A neural-network backgammon program. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks 3 (pp. </booktitle> <pages> 33-40). </pages> <address> San Diego, CA. </address>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 257-277. </pages>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Doctoral thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: For this particular type of MDP, if each V (x) is a separate entry in a lookup table, then update (2) is also equivalent to three other reinforcement learning algorithms: TD (0) (Sutton 88), Q-learning <ref> (Watkins 89) </ref>, and advantage learning (Baird 95).
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference-contexts: The various states can be visited in any order during learning, and some can be visited more often than others, yet the algorithm will still converge if the learning rates decay appropriately <ref> (Watkins, Dayan 92) </ref>. If V (x) was represented by a function-approximation system other than a lookup table, update (2) could be implemented directly by combining it with the backpropagation algorithm (Rumelhart, Hinton, Williams 86).
Reference: <author> Werbos, P. J. </author> <year> (1990). </year> <title> Consistency of HDP Applied to a Simple Reinforcement Learning Problem. Neural Networks 3 , 179-189. </title>

References-found: 10

