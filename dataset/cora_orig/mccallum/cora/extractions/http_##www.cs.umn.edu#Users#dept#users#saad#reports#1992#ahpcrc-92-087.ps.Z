URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/ahpcrc-92-087.ps.Z
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/
Root-URL: http://www.cs.umn.edu
Title: HIGHLY PARALLEL PRECONDITIONERS FOR GENERAL SPARSE MATRICES  
Author: Youcef Saad 
Keyword: Large linear systems; Krylov subspace methods; iterative methods; precondi tioned conjugate gradient; Multi-coloring; incomplete LU preconditioning.  
Address: 4-192 EE/CSci Building, 200 Union Street S.E., Minneapolis, MN 55455  
Affiliation: University of Minnesota, Computer Science Department,  
Note: This research was supported in part by the Army Research Office under contract DAAL03-89-C-0038 and in part by NIST under grant number 60NANB2D1272  
Abstract: The degree of parallelism in the preconditioned Krylov subspace method using standard preconditioners is limited and can lead to poor performance on massively parallel computers. In this paper we examine this problem and consider a number of alternatives based both on multi-coloring ideas and polynomial preconditioning. The emphasis is on methods that deal specifically with general unstructured sparse matrices such as those arising from finite element methods on unstructured grids. It is argued that multi-coloring can be combined with multiple-step relaxation preconditioners to achieve a good level of parallelism while keeping the rates of convergence to good levels. We also exploit the idea of multi-coloring and independent set orderings to introduce a multi-elimination incomplete LU factorization named ILUM, which is related to multifrontal elimination. The main goal of the paper is to discuss some of the prevailing ideas and to compare them on a few test problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adams. </author> <title> M-step preconditioned conjugate gradient methods. </title> <journal> SIAM J. Sci. Statist. Comp, </journal> <volume> 6 </volume> <pages> 452-463, </pages> <year> 1985. </year>
Reference-contexts: Form the approximate solution: Compute x m = x 0 + Z m y m where y m = argmin y kfie 1 H m yk 2 and e 1 = <ref> [1; 0; : : : ; 0] </ref> T . 4. Restart: If satisfied stop, else set x 0 x m and goto 2. <p> Adams <ref> [4, 1] </ref> studied such preconditioners in the context of the conjugate gradient method and showed in particular that for the SSOR splitting, the preconditioning matrix is positive definite under certain conditions.
Reference: [2] <author> L. Adams and H. </author> <title> Jordan. </title> <journal> Is SOR color-blind? SIAM J. Sci. Statist. Comp, </journal> <volume> 6 </volume> <pages> 490-506, </pages> <year> 1985. </year>
Reference-contexts: Wu [54] presents the greedy algorithm for multi-coloring vertices and uses it for SOR type iterations. Finally, the effect of multi-coloring has been extensively studied by Adams <ref> [4, 2] </ref> and Poole and Ortega [34]. 5.2 Multiple step SOR / SSOR preconditioners Just as for the red black ordering, we can use ILU0 or SOR, SSOR preconditioning on the reordered system.
Reference: [3] <author> L. Adams and J. Ortega. </author> <title> A multi-color SOR Method for Parallel Computers. </title> <booktitle> In Proceedings 1982 Int. Conf. Par. Proc., </booktitle> <pages> pages 53-56, </pages> <year> 1982. </year>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example <ref> [4, 3, 34, 14, 33] </ref>. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques [10, 49].
Reference: [4] <author> L. M. Adams. </author> <title> Iterative algorithms for large sparse linear systems on parallel computers. </title> <type> PhD thesis, </type> <institution> Applied Mathematics, University of Virginia, </institution> <address> Charlottsville, VA, 22904, </address> <year> 1982. </year> <note> Also NASA Contractor Report 166027. </note>
Reference-contexts: Adams <ref> [4, 1] </ref> studied such preconditioners in the context of the conjugate gradient method and showed in particular that for the SSOR splitting, the preconditioning matrix is positive definite under certain conditions. <p> More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example <ref> [4, 3, 34, 14, 33] </ref>. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques [10, 49]. <p> Wu [54] presents the greedy algorithm for multi-coloring vertices and uses it for SOR type iterations. Finally, the effect of multi-coloring has been extensively studied by Adams <ref> [4, 2] </ref> and Poole and Ortega [34]. 5.2 Multiple step SOR / SSOR preconditioners Just as for the red black ordering, we can use ILU0 or SOR, SSOR preconditioning on the reordered system.
Reference: [5] <author> E. C. Anderson. </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations. </title> <type> Technical Report 805, </type> <institution> CSRD, University of Illinois, Urbana, IL, </institution> <year> 1988. </year> <type> MS Thesis. 32 </type>
Reference-contexts: These first and last few steps may take a heavy toll on achievable speed-ups on massively parallel computers. - x i1;j x ij The simple scheme described above can be generalized to irregular grids. The technique, referred to as level scheduling is described for example in <ref> [6, 5, 43, 52] </ref> but we omit the details. p t ILUT time GMRES time tot. time Iter.
Reference: [6] <author> E. C. Anderson and Y. Saad. </author> <title> Solving sparse triangular systems on parallel computers. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1 </volume> <pages> 73-96, </pages> <year> 1989. </year>
Reference-contexts: These first and last few steps may take a heavy toll on achievable speed-ups on massively parallel computers. - x i1;j x ij The simple scheme described above can be generalized to irregular grids. The technique, referred to as level scheduling is described for example in <ref> [6, 5, 43, 52] </ref> but we omit the details. p t ILUT time GMRES time tot. time Iter. <p> In addition, the matrix vector multiplications that arise in the algorithm as well as in the level-scheduled forward and backward solutions are optimized by using what is referred to as the "jagged diagonal" format <ref> [38, 6] </ref>. The numbers in the `memory' columns reported for Problems 2 and 3, represent the memory locations needed to store the ILUT factorization only. No attempt has been made to optimize the preprocessing phase and as a result the ILUT computation is likely to run close to scalar speed. <p> Similarly to Table 2.1, level scheduling is used to optimize the forward and backward solves and all the matrix vector multiplications are performed using the "jagged diagonal" format <ref> [38, 6] </ref>. Again, the preprocessing needed to compute the incomplete factorization itself is not optimized. p t ILUT time GMRES time tot. time Iter.
Reference: [7] <author> S. F. Ashby. </author> <title> Polynomial Preconditioning for Conjugate Gradient Methods. </title> <type> PhD thesis, </type> <institution> Computer Science Dept. , University of Illinois, Urbana, IL, </institution> <year> 1987. </year> <note> Available as Technical Report 1355. </note>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique.
Reference: [8] <author> S. F. Ashby, T. A. Manteuffel, and P. E. </author> <title> Saylor. Adaptive polynomial preconditioning for Hermitian indefinite linear systems. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 583-609, </pages> <year> 1989. </year>
Reference: [9] <author> O. Axelsson. </author> <title> A generalized conjugate gradient, least squares method. </title> <journal> Num. Math., </journal> <volume> 51 </volume> <pages> 209-227, </pages> <year> 1987. </year>
Reference: [10] <author> M. Benantar and J. E. Flaherty. </author> <title> A Six color procedure for the parallel solution of Elliptic systems using the finite Quadtree structure. </title> <editor> In J. Dongarra, P. Messina, D. C. Sorenson, and R. G. Voigt, editors, </editor> <booktitle> Proceedings of the fourth SIAM conference on parallel processing for scientific computing, </booktitle> <pages> pages 230-236, </pages> <year> 1990. </year>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33]. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques <ref> [10, 49] </ref>. Multi-coloring is especially useful in element-by-element techniques when forming the residual, i.e., in multiplying an unassembled matrix by a vector. The contributions of the elements of the same color can all be evaluated and applied simultaneously to the resulting vector [23, 16, 45].
Reference: [11] <author> P. Concus, G. H. Golub, and D. P. O'Leary. </author> <title> A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations. </title> <editor> In James R. Bunch and Donald J. Rose, editors, </editor> <booktitle> Sparse Matrix Computations, </booktitle> <pages> pages 309-332, </pages> <address> New York, 1976. </address> <publisher> Academic Press. </publisher>
Reference-contexts: In the symmetric positive definite case, where H is reduced to an interval, it is not too difficult to obtain an approximate interval H containing the spectrum by exploiting the relationship between the Lanczos algorithm and the conjugate gradient method; see e.g., <ref> [11, 22] </ref>. In the non-Hermitian case a number of techniques can be used similarly. We next describe an implementation based on a combination with GMRES [42]. Eigenvalue estimates can be computed from the Hessenberg matrices generated from GMRES.
Reference: [12] <author> I. S. Duff and G. A. Meurant. </author> <title> The effect of reordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: The drawback of this approach is that if an ILU type preconditioned Krylov subspace method is used then the number of iterations may increase substantially, often defeating the benefits gained from the higher degree of parallelism. Although it was observed <ref> [12] </ref> that there are orderings that have precisely the opposite effect these are not too well understood, and it is unlikely that any specific rules can be derived for the general case where the matrix arises from the discretization of coupled Partial Differential Equations on an unstructured grid as is typically
Reference: [13] <author> H. C. Elman. </author> <title> Iterative Methods for Large Sparse Nonsymmetric Systems of Linear Equations. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Dept., </institution> <address> New Haven, CT., </address> <year> 1982. </year>
Reference: [14] <author> H. C. Elman and E. Agron. </author> <title> Ordering techniques for the precondiotioning conjugate gradient method on parallel computers. </title> <type> Technical Report UMIACS-TR-88-53, UMIACS, </type> <institution> University of Maryland, College Park, MD, </institution> <year> 1988. </year>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example <ref> [4, 3, 34, 14, 33] </ref>. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques [10, 49].
Reference: [15] <author> H. C. Elman and G. H. Golub. </author> <title> Iterative methods for cyclically reduced non-self-adjoint linear systems. </title> <type> Technical Report CS-TR-2145, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, MD, </institution> <year> 1988. </year>
Reference-contexts: The preprocessing to compute the reduced system is highly parallel and inexpensive. In addition the reduced system is usually well-conditioned and has some interesting properties when the original system is highly nonsymmetric <ref> [15] </ref>. We should point out that it is not necessary to form the reduced system. This latter strategy is more often employed when D 1 is not diagonal, such in Domain Decomposition methods, but it can also have some uses in other situations.
Reference: [16] <author> R. M. Ferencz. </author> <title> Element-by-element preconditioning techniques for large scale vectorized finite element analysis in nonlinear solid and structural mechanics. </title> <type> PhD thesis, </type> <institution> Applied Mathematics, Stanford, </institution> <address> CA, </address> <year> 1989. </year>
Reference-contexts: Multi-coloring is especially useful in element-by-element techniques when forming the residual, i.e., in multiplying an unassembled matrix by a vector. The contributions of the elements of the same color can all be evaluated and applied simultaneously to the resulting vector <ref> [23, 16, 45] </ref>. We start by describing a simple technique for multi-coloring a graph.
Reference: [17] <author> B. Fischer and R. Freund. </author> <title> Chebyshev polynomials are not always optimal. J. Approximation Theory, </title> <address> 65:-, </address> <year> 1991. </year>
Reference-contexts: The simplest idea is to use an ellipse E [28, 29] that encloses an approximate convex hull of the spectrum. Then the shifted and scaled Chebyshev polynomials are nearly optimal, and even optimal in some special instances <ref> [17] </ref>. A second alternative is to use a polygon H that contains (A) [46, 36]. The motivation here is that polygons may better represent the shape of an arbitrary spectrum.
Reference: [18] <author> R. Freund, M. H. Gutknecht, and N. M. Nachtigal. </author> <title> An implementation of the Look-Ahead Lanczos algorithm for non-Hermitian matrices, Part I. </title> <type> Technical Report 90-11, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1990. </year>
Reference: [19] <author> R. Freund and N. M. Nachtigal. </author> <title> An implementation of the look-ahead lanczos algorithm for non-Hermitian matrices, Part II. </title> <type> Technical Report 90-11, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1990. </year> <month> 33 </month>
Reference: [20] <author> R. S. Varga G. H. Golub. </author> <title> Chebyshev semi iterative methods successive overrelaxation iterative methods and second order Richardson iterative methods. </title> <journal> Numer. Math., </journal> <volume> 3 </volume> <pages> 147-168, </pages> <year> 1961. </year>
Reference-contexts: The general technique of multi-coloring has been used in particular for understanding the theory of relaxation techniques [55, 48] as well as for deriving efficient alternative formulations of some relaxation algorithms <ref> [48, 20] </ref>. More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33].
Reference: [21] <author> Alan George and Joseph W-H Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31 </volume> <pages> 1-19, </pages> <year> 1989. </year>
Reference-contexts: In addition, iterative methods are currently gaining ground because they are much easier than direct methods to implement efficiently on high-performance computers. Typical sparse direct solvers have taken decades to reach the stage of their current level of efficiency <ref> [21] </ref>. Given the complexity of these computer codes, it will take a substantial time before we reach a comparable level of efficiency on new architectures.
Reference: [22] <author> A. L. Hageman and D. M. Young. </author> <title> Applied Iterative Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: In the symmetric positive definite case, where H is reduced to an interval, it is not too difficult to obtain an approximate interval H containing the spectrum by exploiting the relationship between the Lanczos algorithm and the conjugate gradient method; see e.g., <ref> [11, 22] </ref>. In the non-Hermitian case a number of techniques can be used similarly. We next describe an implementation based on a combination with GMRES [42]. Eigenvalue estimates can be computed from the Hessenberg matrices generated from GMRES.
Reference: [23] <author> T. J. R. Hughes, R. M. Ferencz, and J. O. Hallquist. </author> <title> Large-scale vectorized implicit calculations in solid mechanics on a cray x-mp/48 utilizing ebe preconditioning conjugate gradients. </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> 61 </volume> <pages> 215-248, </pages> <year> 1987. </year>
Reference-contexts: Multi-coloring is especially useful in element-by-element techniques when forming the residual, i.e., in multiplying an unassembled matrix by a vector. The contributions of the elements of the same color can all be evaluated and applied simultaneously to the resulting vector <ref> [23, 16, 45] </ref>. We start by describing a simple technique for multi-coloring a graph.
Reference: [24] <author> K. C. Jea and D. M. Young. </author> <title> Generalized conjugate gradient acceleration of nonsymmetriz-able iterative methods. </title> <journal> Linear Algebra Appl., </journal> <volume> 34 </volume> <pages> 159-194, </pages> <year> 1980. </year>
Reference: [25] <author> O. G. Johnson, C. A. Micchelli, and G. Paul. </author> <title> Polynomial preconditionings for conjugate gradient calculations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 362-376, </pages> <year> 1983. </year>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique. <p> The motivation here is that polygons may better represent the shape of an arbitrary spectrum. The polynomial is not explicitly known but it may be computed iteratively by a Remez-type algorithm. 10 An alternative considered by Johnson et al. in <ref> [25] </ref> for the Hermitian case and generalized to non-Hermitian matrices in [36] is to use a least squares polynomial instead of the infinity norm polynomial.
Reference: [26] <author> T. L. Jordan. </author> <title> Conjugate gradient preconditioners for vector and parallel processors. </title> <editor> In G. N. Birkhoff and A. Schoenstadt, editors, </editor> <title> Elliptic problem solvers II, Proceedings of the elliptic problem solvers conference, </title> <address> Monterey CA. </address> , <month> Jan 10-12 </month> <year> 1983, </year> <pages> pages 127-139. </pages> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique.
Reference: [27] <author> T. I. Karush, N. K. Madsen, and G. H. Rodrigue. </author> <title> Matrix multiplication by diagonals on vector/parallel processors. </title> <type> Technical Report UCUD, </type> <institution> Lawrence Livermore National Lab., Livermore, </institution> <address> CA, </address> <year> 1975. </year>
Reference-contexts: The rest causes no major difficulties. Thus, matrix-by-vector product operations are relatively easy to implement efficiently on most computers. In the simplest case where the matrix is regularly structured, i.e., when it consists of a few diagonals this operation can be performed by multiplying diagonals with the vector <ref> [27] </ref>. The matrix can be stored in a rectangular array together with the offsets of these diagonals from the main diagonal. A number of generalizations of this formats for general sparse matrices have been proposed, the first of which is the ELLPACK-ITPACK format [32, 56].
Reference: [28] <author> T. A. Manteuffel. </author> <title> The Tchebychev iteration for nonsymmetric linear systems. </title> <journal> Numer. Math., </journal> <volume> 28 </volume> <pages> 307-327, </pages> <year> 1977. </year>
Reference-contexts: The simplest idea is to use an ellipse E <ref> [28, 29] </ref> that encloses an approximate convex hull of the spectrum. Then the shifted and scaled Chebyshev polynomials are nearly optimal, and even optimal in some special instances [17]. A second alternative is to use a polygon H that contains (A) [46, 36].
Reference: [29] <author> T. A. Manteuffel. </author> <title> Adaptive procedure for estimation of parameter for the nonsymmetric Tchebychev iteration. </title> <journal> Numer. Math., </journal> <volume> 28 </volume> <pages> 187-208, </pages> <year> 1978. </year>
Reference-contexts: The simplest idea is to use an ellipse E <ref> [28, 29] </ref> that encloses an approximate convex hull of the spectrum. Then the shifted and scaled Chebyshev polynomials are nearly optimal, and even optimal in some special instances [17]. A second alternative is to use a polygon H that contains (A) [46, 36].
Reference: [30] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp., </journal> <volume> 31(137) </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: Incomplete LU factorization without fill-in (ILU (0)). 2. Increased fill-in Incomplete LU (ILU) factorizations. Two distinct approaches have been used in the past. The first, level-of-fill approach, is based on the pattern only <ref> [30, 51] </ref>. The second is based on numerical values and threshold strategies [40, 57]. 3. Relaxation type preconditioners (SOR, SSOR). We now briefly discuss these three approaches in turn. ILU (0). The Incomplete LU factorization without fill-in [30] is one of the simplest and most popular techniques in sequential machines. <p> The first, level-of-fill approach, is based on the pattern only [30, 51]. The second is based on numerical values and threshold strategies [40, 57]. 3. Relaxation type preconditioners (SOR, SSOR). We now briefly discuss these three approaches in turn. ILU (0). The Incomplete LU factorization without fill-in <ref> [30] </ref> is one of the simplest and most popular techniques in sequential machines.
Reference: [31] <author> T. C. Oppe, W. Joubert, and D. R. Kincaid. </author> <title> Nspcg user's guide. a package for solving large linear systems by various iterative methods. </title> <type> Technical report, </type> <institution> The University of Texas at Austin, </institution> <year> 1988. </year>
Reference: [32] <author> T. C. Oppe and D. R. Kincaid. </author> <title> The performance of ITPACK on vector computers for solving large sparse linear systems arising in sample oil reservoir simulation problems. </title> <journal> Communications in applied numerical methods, </journal> <volume> 2 </volume> <pages> 1-7, </pages> <year> 1986. </year>
Reference-contexts: The matrix can be stored in a rectangular array together with the offsets of these diagonals from the main diagonal. A number of generalizations of this formats for general sparse matrices have been proposed, the first of which is the ELLPACK-ITPACK format <ref> [32, 56] </ref>. Assuming that the maximum number of nonzero elements per row jmax is small we can store the entries of the matrix in a real array C (1 : n; 1 : jmax), the i-th row of which contains the nonzero elements of the i-th row of A.
Reference: [33] <author> J. M. Ortega. </author> <title> Introduction to Parallel and Vector Solution of Linear Systems. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year> <month> 34 </month>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example <ref> [4, 3, 34, 14, 33] </ref>. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques [10, 49].
Reference: [34] <author> E. L Poole and J. M. Ortega. </author> <title> Mullticolor ICCG methods for vector computers. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 24 </volume> <pages> 1394-1418, </pages> <year> 1987. </year>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example <ref> [4, 3, 34, 14, 33] </ref>. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques [10, 49]. <p> Wu [54] presents the greedy algorithm for multi-coloring vertices and uses it for SOR type iterations. Finally, the effect of multi-coloring has been extensively studied by Adams [4, 2] and Poole and Ortega <ref> [34] </ref>. 5.2 Multiple step SOR / SSOR preconditioners Just as for the red black ordering, we can use ILU0 or SOR, SSOR preconditioning on the reordered system.
Reference: [35] <author> Y. Saad. </author> <title> Practical use of polynomial preconditionings for the conjugate gradient method. </title> <journal> SIAM J. Stat. Sci. Comput., </journal> <volume> 6 </volume> <pages> 865-881, </pages> <year> 1985. </year>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique.
Reference: [36] <author> Y. Saad. </author> <title> Least squares polynomials in the complex plane and their use for solving sparse nonsymmetric linear systems. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 24 </volume> <pages> 155-169, </pages> <year> 1987. </year>
Reference-contexts: Then the shifted and scaled Chebyshev polynomials are nearly optimal, and even optimal in some special instances [17]. A second alternative is to use a polygon H that contains (A) <ref> [46, 36] </ref>. The motivation here is that polygons may better represent the shape of an arbitrary spectrum. <p> The polynomial is not explicitly known but it may be computed iteratively by a Remez-type algorithm. 10 An alternative considered by Johnson et al. in [25] for the Hermitian case and generalized to non-Hermitian matrices in <ref> [36] </ref> is to use a least squares polynomial instead of the infinity norm polynomial. <p> We would then need to solve Find s 2 k that minimizes: k1 s ()k w ; (9) where w is some weight function on the boundary of H and k:k w is the L 2 -norm associated with the corresponding L 2 inner product. In <ref> [36] </ref> we used an L 2 -norm associated with Chebyshev weights on the edges of a polygon H containing the spectrum and expressed the best polynomial as a linear combination of Chebyshev polynomials associated with the ellipse of smallest area containing H. <p> c i and half-length d i , then the weight on each edge is defined by w i () = With these weights, or any other Jacobi weights on the edges, there is a finite procedure to compute the best polynomial that does not require numerical integration; for details see <ref> [36] </ref>. An advantage of the least squares approach over the Chebyshev approach is its better robustness properties. In addition, there are common simple regions in the complex plane which ellipse cannot represent well. Thus, it is also more general.
Reference: [37] <author> Y. Saad. </author> <title> Krylov subspace methods on supercomputers. </title> <journal> SIAM J. Scient. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1200-1232, </pages> <year> 1989. </year>
Reference-contexts: In addition, our code sets m 2 = m 1 . Since the matrix-by-vector operation is the only sparse operation in PGMR, it is important to optimize it in order to achieve good overall performance. We used the jagged diagonal storage scheme <ref> [38, 37] </ref> for this purpose. In the tables, m is the dimension of the Krylov subspaces, i.e., m = m 1 = m 2 .
Reference: [38] <author> Y. Saad. SPARSKIT: </author> <title> A basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1990. </year>
Reference-contexts: In addition, the matrix vector multiplications that arise in the algorithm as well as in the level-scheduled forward and backward solutions are optimized by using what is referred to as the "jagged diagonal" format <ref> [38, 6] </ref>. The numbers in the `memory' columns reported for Problems 2 and 3, represent the memory locations needed to store the ILUT factorization only. No attempt has been made to optimize the preprocessing phase and as a result the ILUT computation is likely to run close to scalar speed. <p> In addition, our code sets m 2 = m 1 . Since the matrix-by-vector operation is the only sparse operation in PGMR, it is important to optimize it in order to achieve good overall performance. We used the jagged diagonal storage scheme <ref> [38, 37] </ref> for this purpose. In the tables, m is the dimension of the Krylov subspaces, i.e., m = m 1 = m 2 . <p> Similarly to Table 2.1, level scheduling is used to optimize the forward and backward solves and all the matrix vector multiplications are performed using the "jagged diagonal" format <ref> [38, 6] </ref>. Again, the preprocessing needed to compute the incomplete factorization itself is not optimized. p t ILUT time GMRES time tot. time Iter.
Reference: [39] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <type> Technical Report 91-279, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, Minnesota, </institution> <year> 1991. </year>
Reference-contexts: For later reference we now describe a variant of the GMRES algorithm based on this approach which was developed in <ref> [39] </ref>. The last step in the GMRES algorithm for solving (2) forms the solution as a linear combination of the preconditioned vectors z i = M 1 v i ; i = 1; : : : ; m, where v i are the Arnoldi vectors and M is the preconditioning. <p> Restart: If satisfied stop, else set x 0 x m and goto 2. For additional details, see reference <ref> [39] </ref>. 2.1 Standard preconditioners Apart from the trivial preconditioners such as Jacobi preconditioning (i.e., diagonal scaling), the most popular sequential preconditioners for general sparse linear systems are variants of the following three approaches. 4 1. Incomplete LU factorization without fill-in (ILU (0)). 2. Increased fill-in Incomplete LU (ILU) factorizations.
Reference: [40] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. </title> <type> Technical report 92-38, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, </institution> <year> 1992. </year>
Reference-contexts: Incomplete LU factorization without fill-in (ILU (0)). 2. Increased fill-in Incomplete LU (ILU) factorizations. Two distinct approaches have been used in the past. The first, level-of-fill approach, is based on the pattern only [30, 51]. The second is based on numerical values and threshold strategies <ref> [40, 57] </ref>. 3. Relaxation type preconditioners (SOR, SSOR). We now briefly discuss these three approaches in turn. ILU (0). The Incomplete LU factorization without fill-in [30] is one of the simplest and most popular techniques in sequential machines. <p> For example: ILU (0) is obtained by performing the standard Gaussian Elimination and replacing by zero any fill-in element during the process. Clearly, the zeros that are introduced need not be stored. ILUT (p; t ). This incomplete factorization technique is based on a two-parameter strategy for dropping elements <ref> [40] </ref>. The algorithm is based on the usual (i; k; j) (row-oriented) version of Gaussian elimination, i.,e., the i rows of L and U are determined simultaneously at step i. <p> The iteration part of an optimized ILU (0)-GMRES (10) should therefore take about the same time as that with ILUT (0,0.0001) - GMRES (10) shown on the second part of the table. We also point out that the meaning of p is slightly different for that in <ref> [40] </ref>. We allow p fill-ins in addition to the number of nonzero elements in the lower part of A. Similarly with the upper part. In all the tests below the dimension of the Krylov subspace in GMRES is taken to be equal to 10. <p> Dashed line: natural ordering, Solid line: Red-Black ordering. Based on this, one can raise the interesting question as to whether or not the number of iterations can be reduced back to a competitive level by using a more accurate ILU factorization on the red-black system, e.g., ILUT <ref> [40] </ref>. Some recent experiments reveal that the answer is yes. In fact the situation is often reversed in that for the same level of fill-in p, the red-black ordering will outperform the natural ordering preconditioner for p large enough, in terms of number of iterations.
Reference: [41] <author> Y. Saad and M. H. Schultz. </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Mathematics of Computation, </journal> <volume> 44(170) </volume> <pages> 417-424, </pages> <year> 1985. </year>
Reference: [42] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: A small list of references in this area is [8, 9, 13, 19, 18, 24, 22, 28, 29, 31, 50, 36, 41, 42]. GMRES is a technique introduced in <ref> [42] </ref> for solving general large sparse nonsymmetric linear systems of equations by minimizing the 2-norm of the residual vector b Ax over x in the Krylov subspace K m = Spanfr 0 ; Ar 0 ; : : : ; A m1 r 0 g; where r 0 is the initial <p> In the non-Hermitian case a number of techniques can be used similarly. We next describe an implementation based on a combination with GMRES <ref> [42] </ref>. Eigenvalue estimates can be computed from the Hessenberg matrices generated from GMRES. Thus, the idea is to use a certain number, say m 1 , of steps of GMRES to get eigenvalue estimates and to improve the current solution.
Reference: [43] <author> J. H. Saltz. </author> <title> Automated problem scheduling and reduction of synchronization delay effects. </title> <type> Technical Report 87-22, </type> <institution> ICASE, Hampton, VA, </institution> <year> 1987. </year>
Reference-contexts: These first and last few steps may take a heavy toll on achievable speed-ups on massively parallel computers. - x i1;j x ij The simple scheme described above can be generalized to irregular grids. The technique, referred to as level scheduling is described for example in <ref> [6, 5, 43, 52] </ref> but we omit the details. p t ILUT time GMRES time tot. time Iter.
Reference: [44] <author> M. K. Seager. </author> <title> Parallelizing conjugate gradient for the CRAY X-MP. </title> <type> Technical report, </type> <institution> Lawrence Livermore National Lab, Livermore, </institution> <address> CA, </address> <year> 1984. </year>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique.
Reference: [45] <author> F. Shakib. </author> <title> Finite element analysis of the compressible Euler and Navier Stokes Equations. </title> <type> PhD thesis, </type> <institution> Aeronautics Dept., Stanford, </institution> <address> CA, </address> <year> 1989. </year>
Reference-contexts: Multi-coloring is especially useful in element-by-element techniques when forming the residual, i.e., in multiplying an unassembled matrix by a vector. The contributions of the elements of the same color can all be evaluated and applied simultaneously to the resulting vector <ref> [23, 16, 45] </ref>. We start by describing a simple technique for multi-coloring a graph.
Reference: [46] <author> D. C. Smolarski and P. E. </author> <title> Saylor. An optimum iterative method for solving any linear system with a square matrix. </title> <journal> BIT, </journal> <volume> 28 </volume> <pages> 163-178, </pages> <year> 1988. </year>
Reference-contexts: Then the shifted and scaled Chebyshev polynomials are nearly optimal, and even optimal in some special instances [17]. A second alternative is to use a polygon H that contains (A) <ref> [46, 36] </ref>. The motivation here is that polygons may better represent the shape of an arbitrary spectrum.
Reference: [47] <author> H. A. van der Vorst. </author> <title> High performance preconditioning. </title> <journal> SIAM j. Scient. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1174-1185, </pages> <year> 1989. </year>
Reference-contexts: For 3-D problems the parallelism is of the order of the maximum size of the sets of domain points x i;j;k , where i + j + k = lev, a constant level lev. See <ref> [47] </ref> for details on vector implementations. However, as can be easily seen there is little parallelism or vectorization at the beginning and at the end of the sweep.
Reference: [48] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: In fact in the context of iterative methods keeping preprocessing costs to a minimum is essential and in this framework, a sub-optimal multi-coloring is generally sufficient. The general technique of multi-coloring has been used in particular for understanding the theory of relaxation techniques <ref> [55, 48] </ref> as well as for deriving efficient alternative formulations of some relaxation algorithms [48, 20]. More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33]. <p> The general technique of multi-coloring has been used in particular for understanding the theory of relaxation techniques [55, 48] as well as for deriving efficient alternative formulations of some relaxation algorithms <ref> [48, 20] </ref>. More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33].
Reference: [49] <author> V. Venkatakrishnan, H. D. Simon, and T. J. Barth. </author> <title> A MIMD Implementation of a Parallel Euler Solver for Unstructured Grids. </title> <type> Technical Report RNR-91-024, </type> <institution> NASA Ames research center, Moffett Field, </institution> <address> CA, </address> <year> 1991. </year> <month> 35 </month>
Reference-contexts: More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33]. Multi-Coloring is also commonly employed in a slightly different form coloring elements (or edges) as opposed to nodes infinite elements (or finite volume) 20 techniques <ref> [10, 49] </ref>. Multi-coloring is especially useful in element-by-element techniques when forming the residual, i.e., in multiplying an unassembled matrix by a vector. The contributions of the elements of the same color can all be evaluated and applied simultaneously to the resulting vector [23, 16, 45].
Reference: [50] <author> P. K. W. Vinsome. Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations. </title> <booktitle> In Proceedings of the Fourth Symposium on Resevoir Simulation, </booktitle> <pages> pages 149-159. </pages> <booktitle> Society of Petroleum Engineers of AIME, </booktitle> <year> 1976. </year>
Reference: [51] <author> J. W. Watts-III. </author> <title> A conjugate gradient truncated direct method for the iterative solution of the reservoir simulation pressure equation. </title> <journal> Society of Petroleum Engineer Journal, </journal> <volume> 21 </volume> <pages> 345-353, </pages> <year> 1981. </year>
Reference-contexts: Incomplete LU factorization without fill-in (ILU (0)). 2. Increased fill-in Incomplete LU (ILU) factorizations. Two distinct approaches have been used in the past. The first, level-of-fill approach, is based on the pattern only <ref> [30, 51] </ref>. The second is based on numerical values and threshold strategies [40, 57]. 3. Relaxation type preconditioners (SOR, SSOR). We now briefly discuss these three approaches in turn. ILU (0). The Incomplete LU factorization without fill-in [30] is one of the simplest and most popular techniques in sequential machines.
Reference: [52] <author> O. Wing and J. W. Huang. </author> <title> A computation model of parallel solution of linear equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:632-638, </volume> <year> 1980. </year>
Reference-contexts: These first and last few steps may take a heavy toll on achievable speed-ups on massively parallel computers. - x i1;j x ij The simple scheme described above can be generalized to irregular grids. The technique, referred to as level scheduling is described for example in <ref> [6, 5, 43, 52] </ref> but we omit the details. p t ILUT time GMRES time tot. time Iter.
Reference: [53] <author> Y. S. Wong. </author> <title> Solving large elliptic difference equations on CYBER 205. </title> <journal> Parallel Comput., </journal> <volume> 6 </volume> <pages> 195-207, </pages> <year> 1988. </year>
Reference-contexts: 0.245E+01 75822 20 Table 2.3 Performance of Comparison of GMRES (10)-ILUT (p,t ), for prob lem 3, using various values of p and t , with level-scheduling. 3 Polynomial preconditioning When vector computers first became available, polynomial preconditioners were among the first alternatives proposed to the standard ILU preconditioning techniques <ref> [7, 25, 35, 26, 44, 53] </ref>. These methods consist of choosing a polynomial s and replacing the original linear system by s (A)Ax = s (A)b or A (s (A)y) = b; x = s (A)y ; (7) which is then solved by a conjugate gradient type technique.
Reference: [54] <author> C. H. Wu. </author> <title> A multicolour SOR method for the finite-element method. </title> <journal> J. of Comput. and App. Math., </journal> <volume> 30 </volume> <pages> 283-294, </pages> <year> 1990. </year>
Reference-contexts: A second strategy, is to use a divide-and-conquer, or `domain-decomposition' approach: recursively color the subdomains then the nodes in the interfaces. The idea of the greedy multi-coloring algorithm is known in Finite Element techniques (to color elements), see e.g., Berger-Brouays-Syre (1982), and Benantar and Flaherty. Wu <ref> [54] </ref> presents the greedy algorithm for multi-coloring vertices and uses it for SOR type iterations.
Reference: [55] <author> D. M. Young. </author> <title> Iterative solution of large linear systems. </title> <publisher> Academic Press, </publisher> <address> New-York, </address> <year> 1971. </year>
Reference-contexts: Matrices that can be permuted into the above form are said to have property A <ref> [55] </ref>. Once the system has been reduced to the convenient form (11) a number of different techniques can be used for solving such systems. <p> In fact in the context of iterative methods keeping preprocessing costs to a minimum is essential and in this framework, a sub-optimal multi-coloring is generally sufficient. The general technique of multi-coloring has been used in particular for understanding the theory of relaxation techniques <ref> [55, 48] </ref> as well as for deriving efficient alternative formulations of some relaxation algorithms [48, 20]. More recently, it was emerged as a useful tool for introducing parallelism in iterative methods, see for example [4, 3, 34, 14, 33].
Reference: [56] <author> D. M. Young, T. C. Oppe, D. R. Kincaid, and L. J. Hayes. </author> <title> On the use of vector computers for solving large sparse linear systems. </title> <type> Technical Report CNA-199, </type> <institution> Center for Numerical Analysis, University of Texas at Austin, Austin, Texas, </institution> <year> 1985. </year>
Reference-contexts: The matrix can be stored in a rectangular array together with the offsets of these diagonals from the main diagonal. A number of generalizations of this formats for general sparse matrices have been proposed, the first of which is the ELLPACK-ITPACK format <ref> [32, 56] </ref>. Assuming that the maximum number of nonzero elements per row jmax is small we can store the entries of the matrix in a real array C (1 : n; 1 : jmax), the i-th row of which contains the nonzero elements of the i-th row of A.
Reference: [57] <author> Z. Zlatev. </author> <title> Use of iterative refinement in the solution of sparse linear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19 </volume> <pages> 381-399, </pages> <year> 1982. </year> <month> 36 </month>
Reference-contexts: Incomplete LU factorization without fill-in (ILU (0)). 2. Increased fill-in Incomplete LU (ILU) factorizations. Two distinct approaches have been used in the past. The first, level-of-fill approach, is based on the pattern only [30, 51]. The second is based on numerical values and threshold strategies <ref> [40, 57] </ref>. 3. Relaxation type preconditioners (SOR, SSOR). We now briefly discuss these three approaches in turn. ILU (0). The Incomplete LU factorization without fill-in [30] is one of the simplest and most popular techniques in sequential machines.
References-found: 57

