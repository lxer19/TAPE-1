URL: http://www.research.att.com/~singer/papers/ecuts_colt.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Email: fpereira,singerg@research.att.com  
Title: An Efficient Extension to Mixture Techniques for Prediction and Decision Trees  
Author: Fernando Pereira and Yoram Singer 
Address: 600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: AT&T Labs  
Abstract: We present a method for maintaining mixtures of prunings of a prediction or decision tree that extends the "node-based" prunings of [Bun90, WST95, HS97] to the larger class of edge-based prunings. The method includes an efficient online weight allocation algorithm that can be used for prediction, compression and classification. Although the set of edge-based prunings of a given tree is much larger than that of node-based prunings, our algorithm has similar space and time complexity to that of previous mixture algorithms for trees. Using the general on-line framework of Freund and Schapire [FS97], we prove that our algorithm maintains correctly the mixture weights for edge-based prunings with any bounded loss function. We also give a similar algorithm for the logarithmic loss function with a corresponding weight allocation algorithm. Finally, we describe experiments comparing node-based and edge-based mixture models for estimating the probability of the next word in English text, which show the ad vantages of edge-based models.
Abstract-found: 1
Intro-found: 1
Reference: [Bun90] <author> W.L. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: The following lemma gives an efficient method of computing g and may thus be used to compute (3) in particular. The lemma generalizes Helmbold and Schapire's Lemma 1 of [HS97], which in turn generalizes the more specialized results of Bun-tine <ref> [Bun90, Lemma 6.5.1] </ref> and Willems, Shtarkov and Tjalkens [WST95, Appendices III and IV]. Lemma 1 Let g, g be as above. Then, for any node u of ~ T : 1. If u is a leaf then g (u) = g (u?); 2.
Reference: [CBFH + 97] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helm-bold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <title> How to use expert advice. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 44(3) </volume> <pages> 427-485, </pages> <year> 1997. </year> <title> Model Perplexity Backoff Trigram 95.2 Node-Based Mixture 100.5 Edge-Based Mixture 98.9 Node-Based Mixture (w/ adaptation) 97.9 Edge-Based Mixture (w/ adaptation) 96.0 Table 1: Perplexity results on test data from the NAB corpus. </title>
Reference-contexts: We derive a weight update rule for bounded loss predictors using the framework introduced by Freund and Schapire [FS97] which generalizes former on-line weight allocation algorithms <ref> [DMW88, Vov90, CBFH + 97, LW94] </ref> and can be applied to a wide variety of learning problems. This derivation does not depend on the precise form of the loss function, requiring only that the appropriate loss value be provided to the learning algorithm after each prediction.
Reference: [DMW88] <author> A. DeSantis, G. Markowsky, </author> <title> and M.N. Wegman. Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <year> 1988. </year>
Reference-contexts: We derive a weight update rule for bounded loss predictors using the framework introduced by Freund and Schapire [FS97] which generalizes former on-line weight allocation algorithms <ref> [DMW88, Vov90, CBFH + 97, LW94] </ref> and can be applied to a wide variety of learning problems. This derivation does not depend on the precise form of the loss function, requiring only that the appropriate loss value be provided to the learning algorithm after each prediction. <p> all prunings for the logarithmic loss is simply the ratio w t+1 () t = w t (x t+1 ) + w (x t+1 ) The time and the space complexity of the algorithm remain the same and using Lemma 1 combined with the proof technique of DeSantis et. al. <ref> [DMW88] </ref> we get the following bound on the predictions of the mixture: Corollary 3 Let T be a template tree, let x 1 ; : : : ; x T be any sequence of instances, and let the logarithmic losses l t P associated with each pruning P of T be
Reference: [FS97] <author> Y. Freund and R.E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <year> 1997. </year>
Reference-contexts: We derive a weight update rule for bounded loss predictors using the framework introduced by Freund and Schapire <ref> [FS97] </ref> which generalizes former on-line weight allocation algorithms [DMW88, Vov90, CBFH + 97, LW94] and can be applied to a wide variety of learning problems. <p> The algorithm is based on Freund and Schapire's Hedge <ref> [FS97] </ref>, which maintains a non-negative weight vector over predictors. In our case, the predictors are prunings. We denote the unnormalized weight of a pruning at time t by w t P , and its initial weight by w 1 P . Initial weights can be viewed as priors over prunings.
Reference: [Goo53] <author> I.J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3) </volume> <pages> 237-264, </pages> <year> 1953. </year>
Reference-contexts: The first estimates the probability of a symbol at a node as n n+r where r is the number of different symbols observed at the node. The second method, based on an approximation of the Good-Turing estimation scheme <ref> [Goo53] </ref>, estimates the probability of a symbol as (nt 1 )n n , where t 1 is the number of different words that have been observed only once at the node. This scheme requires a "fall-back" estimate when t 1 = n, as described in more detail in [WB91].
Reference: [HS97] <author> D.P. Helmbold and R.E. Schapire. </author> <title> Predicting nearly as well as the best pruning of a decision tree. </title> <journal> Machine Learning, </journal> <volume> 27(1) </volume> <pages> 51-68, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction Recent work in information theory [WST95] and machine learning <ref> [HS97] </ref> shows that it is possible to maintain efficiently the mixture weights for certain sets of prunings of a decision (or prediction) tree. The solution employs a recursive algorithm that updates the weights of each possible pruning, and also computes the weighted prediction or decision of the entire mixture. <p> If e is an edge (u is a node), we write e &lt; x (u &lt; x) to indicate that e (u) is on the maximal path associated with x. The above definitions for trees generalize those used in [WST95] and in <ref> [HS97] </ref>, since those papers consider only complete K-ary trees. In Figure 1 we show a template tree over = f0; 1g and its extended version. Extended template trees are used in the analysis, but the actual algorithm does not need to represent explicitly additional terminal edges of the extended tree. <p> The size of pruning P, written jPj, is sum of the number of internal edges of P and the number of terminal edges of P that are not terminal edges of ~ T . For further discussion of the relationship between edge-based prunings and the node-based prunings used in <ref> [WST95, HS97] </ref> refer to Appendix A. The main goal of this paper is to show that the mixture of all possible edge-based prunings can be maintained with practically the same space and time complexity as for the node-based algorithms. <p> Such an approach is clearly infeasible given the huge number of possible prunings, especially when is large. However, we can adopt the techniques of Willems, Shtarkov, and Tjalkens [WST95] and Helmbold and Schapire <ref> [HS97] </ref> to our case. Those techniques require jx t j time to update the weights, where jx t j is the number of edges in the path associated to x t in T . <p> The following lemma gives an efficient method of computing g and may thus be used to compute (3) in particular. The lemma generalizes Helmbold and Schapire's Lemma 1 of <ref> [HS97] </ref>, which in turn generalizes the more specialized results of Bun-tine [Bun90, Lemma 6.5.1] and Willems, Shtarkov and Tjalkens [WST95, Appendices III and IV]. Lemma 1 Let g, g be as above. Then, for any node u of ~ T : 1.
Reference: [Kat87] <author> S.M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics Specch and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference-contexts: To conclude, we describe statistical language-modeling experiments in which edge-based mixture models are used to estimate the probability of the next word in English news text, showing that edge-based models perform better than node-based models, and that both kinds of mixture models are competitive with the backoff models <ref> [Kat87] </ref> that have been a standard tool in statistical language modeling. 2 Preliminaries The tasks we examine here are online classification and prediction. At each time step t = 1; : : : ; T the learning algorithm receives an instance x t and outputs a prediction ^y t . <p> Our experiments show, however, that an online algorithm has a similar performance as the most widely-used batch language-modeling method <ref> [Kat87] </ref> even in the case of very large vocabulary. The experiments also suggest that edge-based pruning has considerable advantages over node-based pruning for the very large alphabets demanded by the application. Our experiment compare edge-based and node-based mixture algorithms with the baseline backoff method of Katz [Kat87] on training and test <p> widely-used batch language-modeling method <ref> [Kat87] </ref> even in the case of very large vocabulary. The experiments also suggest that edge-based pruning has considerable advantages over node-based pruning for the very large alphabets demanded by the application. Our experiment compare edge-based and node-based mixture algorithms with the baseline backoff method of Katz [Kat87] on training and test material derived from the NIST-supplied North-American Business News (NAB) corpus of English news text. The text is tokenized into words. <p> Finally, we compared the performance of both mixture models with a standard backoff model <ref> [Kat87] </ref>, which must must be built in batch mode. For a fair comparison, we took an unseen test set of 13 million words, and measured the loss of a fixed mixture model, built from the training set, on that test set.
Reference: [KT81] <author> R.E. Krichevsky and V.K. Trofimov. </author> <title> The performance of universal coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27 </volume> <pages> 199-207, </pages> <year> 1981. </year>
Reference-contexts: The modified Laplace rule, also known as the Krichevsky and Trofimov estimator <ref> [KT81] </ref>, often used in character-based text compression, turned to be inadequate for large alphabets.
Reference: [LW94] <author> N. Littlestone and M.K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: We derive a weight update rule for bounded loss predictors using the framework introduced by Freund and Schapire [FS97] which generalizes former on-line weight allocation algorithms <ref> [DMW88, Vov90, CBFH + 97, LW94] </ref> and can be applied to a wide variety of learning problems. This derivation does not depend on the precise form of the loss function, requiring only that the appropriate loss value be provided to the learning algorithm after each prediction.
Reference: [Ris86] <author> J. Rissanen. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 32(4) </volume> <pages> 526-532, </pages> <year> 1986. </year>
Reference: [RL81] <author> J. Rissanen and G.G. Langdon. </author> <title> Universal modeling and coding. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-27(1):12-23, </volume> <month> January </month> <year> 1981. </year>
Reference: [RST96] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of Amnesia: Learning probabilistic automata with variable memory length. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 117-149, </pages> <year> 1996. </year>
Reference: [Vov90] <author> V.G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: We derive a weight update rule for bounded loss predictors using the framework introduced by Freund and Schapire [FS97] which generalizes former on-line weight allocation algorithms <ref> [DMW88, Vov90, CBFH + 97, LW94] </ref> and can be applied to a wide variety of learning problems. This derivation does not depend on the precise form of the loss function, requiring only that the appropriate loss value be provided to the learning algorithm after each prediction.
Reference: [WB91] <author> I.H. Witten and T.C. Bell. </author> <title> The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Infor mation Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <year> 1991. </year>
Reference-contexts: Instead, we considered two estimation techniques that were empirically shown to perform well on natural data sets <ref> [WB91] </ref>. The first estimates the probability of a symbol at a node as n n+r where r is the number of different symbols observed at the node. <p> This scheme requires a "fall-back" estimate when t 1 = n, as described in more detail in <ref> [WB91] </ref>. We stress that the goal of the experiments was to compare the two different mixture techniques and not possible online estimation techniques for the node pre dictors.
Reference: [WLZ92] <author> M. Weinberger, A. Lempel, and J. Ziv. </author> <title> Universal coding of finite-memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(3) </volume> <pages> 1002-1014, </pages> <year> 1992. </year>
Reference: [WMF94] <author> M. Weinberger, N. Merhav, and M. Feder. </author> <title> Optimal sequential probability assignment for individual sequence. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40(2) </volume> <pages> 384-396, </pages> <year> 1994. </year>
Reference: [WRF95] <author> M. Weinberger, J. Rissanen, and M. Feder. </author> <title> A universal finite memory source. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 643-652, </pages> <year> 1995. </year>
Reference: [WST95] <author> F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. </author> <title> The context tree weighting method: Basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Recent work in information theory <ref> [WST95] </ref> and machine learning [HS97] shows that it is possible to maintain efficiently the mixture weights for certain sets of prunings of a decision (or prediction) tree. <p> If e is an edge (u is a node), we write e &lt; x (u &lt; x) to indicate that e (u) is on the maximal path associated with x. The above definitions for trees generalize those used in <ref> [WST95] </ref> and in [HS97], since those papers consider only complete K-ary trees. In Figure 1 we show a template tree over = f0; 1g and its extended version. <p> The size of pruning P, written jPj, is sum of the number of internal edges of P and the number of terminal edges of P that are not terminal edges of ~ T . For further discussion of the relationship between edge-based prunings and the node-based prunings used in <ref> [WST95, HS97] </ref> refer to Appendix A. The main goal of this paper is to show that the mixture of all possible edge-based prunings can be maintained with practically the same space and time complexity as for the node-based algorithms. <p> In a nave implementation, the weight vector and its contribution to the normalization would be computed for each pruning separately. Such an approach is clearly infeasible given the huge number of possible prunings, especially when is large. However, we can adopt the techniques of Willems, Shtarkov, and Tjalkens <ref> [WST95] </ref> and Helmbold and Schapire [HS97] to our case. Those techniques require jx t j time to update the weights, where jx t j is the number of edges in the path associated to x t in T . <p> The following lemma gives an efficient method of computing g and may thus be used to compute (3) in particular. The lemma generalizes Helmbold and Schapire's Lemma 1 of [HS97], which in turn generalizes the more specialized results of Bun-tine [Bun90, Lemma 6.5.1] and Willems, Shtarkov and Tjalkens <ref> [WST95, Appendices III and IV] </ref>. Lemma 1 Let g, g be as above. Then, for any node u of ~ T : 1. If u is a leaf then g (u) = g (u?); 2.
References-found: 18

