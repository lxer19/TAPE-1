URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P162.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: AN INTERIOR POINT ALGORITHM FOR LINEARLY CONSTRAINED OPTIMIZATION  
Author: STEPHEN J. WRIGHT 
Keyword: Key words. potential reduction algorithm, gradient porojection algorithm, linearly constrained optimization  
Date: 1992 012  
Note: SIAM J. OPTIMIZATION. c 1992 Society for Industrial and Applied Mathematics Vol. 1, No. 4, pp. 000-000, Month  AMS(MOS) subject classifications. 65K10, 90C30  
Abstract: We describe an algorithm for optimization of a smooth function subject to general linear constraints. An algorithm of the gradient projection class is used, with the important feature that the "projection" at each iteration is performed using a primal-dual interior point method for convex quadratic programming. Convergence properties can be maintained even if the projection is done inexactly in a well-defined way. Higher-order derivative information on the manifold defined by the apparently active constraints can be used to increase the rate of local convergence. 1. Introduction. We address the problem 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. V. Burke and J. J. Mor e, </author> <title> On the identification of active constraints, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 25 (1988), </volume> <pages> pp. 1197-1211. </pages>
Reference-contexts: Lemma 4.3. Suppose that (A) holds and that (B) holds at z fl = x k . When x k is critical, then ffi k (ff) = 0 for all ff 2 <ref> [0; 1] </ref>, and x k (ff; ffi k (ff)) = x k for all ff 2 [0; 1]. Proof. Clearly the result is true for ff = 0. For the remainder of the proof, we assume that ff 2 (0; 1]. <p> Lemma 4.3. Suppose that (A) holds and that (B) holds at z fl = x k . When x k is critical, then ffi k (ff) = 0 for all ff 2 <ref> [0; 1] </ref>, and x k (ff; ffi k (ff)) = x k for all ff 2 [0; 1]. Proof. Clearly the result is true for ff = 0. For the remainder of the proof, we assume that ff 2 (0; 1]. All vectors in the subspace T k are orthogonal to N (x k ; X). <p> k (ff) 2 and hence ffi k (ff)[1 jff o=22 ffi k (ff)] 0:(22) INTERIOR POINT ALGORITHM FOR LINEAR CONSTRAINTS 9 From (10) and the fact that jC 1 &lt; 1, 1 jff o=22 ffi k (ff) 1 jC 1 ff o2 &gt; 1 ff o2 0; since ff 2 <ref> [0; 1] </ref> and o &gt; 2. Since ffi k (ff) 0, the inequality (22) can hold only if ffi k (ff) = 0. Thus, the first statement is proved. Proof of the second statement follows immediately. Lemma 4.4. <p> Thus, the first statement is proved. Proof of the second statement follows immediately. Lemma 4.4. Suppose I k is defined by (3), where * k is any positive number. Suppose that (A) holds and that (B) holds for all z fl = x k (ff; 0) for ff 2 <ref> [0; 1] </ref>. <p> For the main result of this section, we need to be more specific about the choice of * k . We now assume that * k = min (*; ^c k ^*(x k ));(38) where there is a constant ^ B such that ^c k 2 <ref> [1; ^ B] </ref>; ^*(x) is a continuous function of x that is zero only when x is critical. Theorem 4.5. Suppose that * k satisfies condition (38), that (A) holds, and that (B) holds for x k (ff; 0), for all ff 2 [0; 1] and all k sufficiently large. <p> Theorem 4.5. Suppose that * k satisfies condition (38), that (A) holds, and that (B) holds for x k (ff; 0), for all ff 2 <ref> [0; 1] </ref> and all k sufficiently large. Then every accumulation point x k generated by the algorithm is critical. Proof. The proof is quite similar to the proof of Proposition 2 of Gafni and Bertsekas [5]. <p> (39) INTERIOR POINT ALGORITHM FOR LINEAR CONSTRAINTS 13 lim 1 kx k (ff k ; ffi k (ff k )) (x k + ff k ~ d k )k 2 = 0:(40) Taking a subsequence if necessary, assume that lim ff k = ff fl for some ff fl 2 <ref> [0; 1] </ref>. Two cases arise. First assume that ff fl &gt; 0. Then from (39), d k k2K ! 0 and so ! 0 and d k+ k2K ! rf (x k ). <p> (x fl ; X) relative to the affine hull of N (x fl ; X). (E) * k is defined as * k = min (*; ^c k *(x k )); where * &gt; 0 is a positive constant, *(x) = kx P (x rf (x))k; and ^c k 2 <ref> [1; ^ B] </ref> for some ^ B &lt; 1. (^c k is a "random" quantity and need not be a function of x k .) If Assumption (B) also holds at x fl , then Assumption (D) implies that there are unique scalars y fl i &gt; 0 such that rf <p> From this definition of * k , the following active set identification result can be proved: Lemma 5.1. Suppose that assumptions (A), (D), and (E) hold and that (B) holds for x k (ff; 0), for ff 2 <ref> [0; 1] </ref> and all k sufficiently large. Assuming that x fl is a limit point of the sequence fx k g, we have lim k!1 x k = x fl and I k = A for all k sufficiently large. Proof. <p> Proof. The result follows from Lemma B.1 of Gafni and Bertsekas [5]; trivial modifications are required because of our relaxed definition of * k . The Assumption (B) in [5] corresponds to our Assumption (C) (see Theorem 2.8 in Burke and More <ref> [1] </ref>). We next show that the steplengths do not vanish as k ! 1. Lemma 5.2. Under the assumptions of Lemma 5.1, there is ^ff &gt; 0 such that ff k ^ff for all k sufficiently large. Proof. <p> Under the assumptions of Lemma 5.1, we have for all k sufficiently large that x k (ff; 0) = x fl + Zv k (ff) 2 x fl + T k ; for all ff 2 <ref> [ff k ; 1] </ref>, where v k (ff) 2 R nr . Also, (x k + ff ( ~ d k + d k+ )) x k (ff; 0) = Ay k (ff) 2 N (x fl ; X);(52) for all ff 2 [ff k ; 1], where y k (ff) <p> T k ; for all ff 2 <ref> [ff k ; 1] </ref>, where v k (ff) 2 R nr . Also, (x k + ff ( ~ d k + d k+ )) x k (ff; 0) = Ay k (ff) 2 N (x fl ; X);(52) for all ff 2 [ff k ; 1], where y k (ff) 2 R r has y k i (ff) &gt; C 2 ff for i = 1; ; r and some constant C 2 &gt; 0. Proof. We prove only the last statement concerning the lower bound on y k (ff). <p> Hence 0 kx k (ff k ; 0)x k k kx k (ff k ; ffi k (ff k ))x k (ff k ; 0)k+kx k+1 x k k ffi k (ff k )+kx k+1 x k k ! 0: Now by Lemma 4.2, and since ff 2 <ref> [ff k ; 1] </ref>, 1 kx k (ff; 0) x k k ff k Since ff k ^ff, it follows from this inequality that x k (ff; 0) x k ! 0.
Reference: [2] <author> J. C. Dunn, </author> <title> On the convergence of projected gradient processes to singular critical points, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 55 (1987), </volume> <pages> pp. </pages> <month> 203-216. </month> <title> [3] , Gradient projection methods for systems optimization problems, Control and Dynamic Systems, </title> <booktitle> 29 (1988), </booktitle> <pages> pp. </pages> <month> 135-195. </month> <title> [4] , A projected Newton method for minimization problems with nonlinear inequality constraints, </title> <journal> Numerische Mathematik, </journal> <volume> 53 (1988), </volume> <pages> pp. 377-409. </pages>
Reference-contexts: The next result follows easily from Lemma 5.2, Lemma B.2 of [5], and the analysis of Dunn <ref> [2] </ref> INTERIOR POINT ALGORITHM FOR LINEAR CONSTRAINTS 17 Lemma 5.3.
Reference: [5] <author> E. M. Gafni and D. P. Bertsekas, </author> <title> Two-metric projection methods for constrained optimization, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 (1984), </volume> <pages> pp. 936-964. </pages>
Reference-contexts: WRIGHT interior-point methods with the desirable properties of gradient projection algorithms | most notably, rapid identification of the final active set. In addition, we allow second-derivative information to be used in the definition of g (as is also done by Dunn [4, 3] and Gafni and Bertsekas <ref> [5] </ref>) to speed up the asymptotic convergence rate after the correct active set has been identified. The "inexactness" in the projection is quantified by a duality gap, which is updated at each iteration of the projection subproblem. <p> The calculation of g k is somewhat different in Gafni and Bertsekas <ref> [5] </ref>. They define an "almost tangent cone" at x k by C k = fz j a T and then define d k as the projection of rf (x k ) onto this cone. <p> INTERIOR POINT ALGORITHM FOR LINEAR CONSTRAINTS 5 The steplength rule (11) reduces to the one proposed by Gafni and Bertsekas <ref> [5] </ref> (and also used by Dunn [3]) when ffi k (ff) j 0. <p> Proof. rf (x k ) T [x k x k (ff; ffi k (ff))] and for ff 2 (0; * k =kg k k), it can be proved by using a similar argument to that in <ref> [5, Proposition 1 (b)] </ref> that rf (x k ) T [x k x k (ff; 0)] ff kx k (ff; 0) (x k + ff ~ d k )k 2 : By the smoothness assumptions on f , there is a constant B such that krf (x)k B for all x <p> ff (oe) = min (1; * k =(2kg k k); ff 1 ; ff 2 ): The second part of the result (i.e., that f (x k ) &gt; f (x k (ff; ffi k (ff))) for sufficiently small ff) is obtained by modifying the argument of Gafni and Bertsekas <ref> [5, Proposition 1 (b)] </ref>. <p> Then every accumulation point x k generated by the algorithm is critical. Proof. The proof is quite similar to the proof of Proposition 2 of Gafni and Bertsekas <ref> [5] </ref>. Some modifications are necessary because of the inexactness in x k (ff) and because of the need for the quantity oe in Lemma 4.4. We include most of the details here, and refer the reader to [5] for the remainder. <p> is quite similar to the proof of Proposition 2 of Gafni and Bertsekas <ref> [5] </ref>. Some modifications are necessary because of the inexactness in x k (ff) and because of the need for the quantity oe in Lemma 4.4. We include most of the details here, and refer the reader to [5] for the remainder. Suppose for contradiction that there is a noncritical point x fl and a subsequence K such that lim k2K x k = x fl . <p> Assuming that x fl is a limit point of the sequence fx k g, we have lim k!1 x k = x fl and I k = A for all k sufficiently large. Proof. The result follows from Lemma B.1 of Gafni and Bertsekas <ref> [5] </ref>; trivial modifications are required because of our relaxed definition of * k . The Assumption (B) in [5] corresponds to our Assumption (C) (see Theorem 2.8 in Burke and More [1]). We next show that the steplengths do not vanish as k ! 1. Lemma 5.2. <p> Proof. The result follows from Lemma B.1 of Gafni and Bertsekas <ref> [5] </ref>; trivial modifications are required because of our relaxed definition of * k . The Assumption (B) in [5] corresponds to our Assumption (C) (see Theorem 2.8 in Burke and More [1]). We next show that the steplengths do not vanish as k ! 1. Lemma 5.2. Under the assumptions of Lemma 5.1, there is ^ff &gt; 0 such that ff k ^ff for all k sufficiently large. <p> k ) T [x k x k (ff; ffi k (ff))] oe ffd kT D k d k + ff If we use L as an upper bound on r 2 f (x) for x in some neighborhood of x fl , it follows exactly as in Gafni and Bertsekas <ref> [5] </ref> that f (x k ) f (x k (ff; ffi k (ff))) ff L)kx k (ff; ffi k (ff)) (x k + ff ~ d k )k 2 : If we choose ~ff = sup min ff (oe); L 2 oe oe it follows from the line search mechanism (11) <p> The next result follows easily from Lemma 5.2, Lemma B.2 of <ref> [5] </ref>, and the analysis of Dunn [2] INTERIOR POINT ALGORITHM FOR LINEAR CONSTRAINTS 17 Lemma 5.3.
Reference: [6] <author> C.-G. Han, P. Pardalos, and Y. Ye, </author> <title> Computational aspects of an interior point algorithm for quadratic programming problems with box constraints, in Large-Scale Numerical Optimization, </title> <editor> T. F. Coleman and Y. Li, eds., </editor> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1990, </year> <pages> pp. 92-112. </pages>
Reference-contexts: Projection onto the polyhedral set X can be achieved by solving a convex quadratic program or, equivalently, a linear complementarity problem (LCP). In this section, we formulate the problem and outline a primal-dual potential reduction algorithm for solving it. The discussion will be brief, since other papers such as <ref> [6, 7, 10, 11] </ref> can be consulted for details about motivation, analysis, and implementation issues for this class of interior-point algorithms. <p> In Han, Pardalos, and Ye <ref> [6] </ref>, the choice ae j j m 2 is made for convex quadratic programs. <p> In practical implementations, the line search parameter j is also chosen differently. In Han, Pardalos, and Ye <ref> [6] </ref>, the following choice appeared to give good experimental results: j = 0:99 min min j i i=1;;m; y i &lt;0 y i ! An issue of particular concern in this context is the choice of a feasible initial point at which to start the interior-point iteration.
Reference: [7] <author> M. Kojima, S. Mizuno, and A. Yoshise, </author> <title> An o( p nl) iteration potential reduction algorithm for linear complementarity problems, </title> <type> Tech. Rep. </type> <institution> Research Report B-217, Department of Information Sciences, Tokyo Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: Projection onto the polyhedral set X can be achieved by solving a convex quadratic program or, equivalently, a linear complementarity problem (LCP). In this section, we formulate the problem and outline a primal-dual potential reduction algorithm for solving it. The discussion will be brief, since other papers such as <ref> [6, 7, 10, 11] </ref> can be consulted for details about motivation, analysis, and implementation issues for this class of interior-point algorithms. <p> The progress of the interior-point algorithm can be gauged by using the potential function defined by (; y) = ae P log ( T y) i=1 where ae P m + p m. In Kojima, Mizuno, and Yoshise <ref> [7] </ref>, the step from iterate j to iterate j + 1 is obtained by solving the linear system = I A y ;(16) together with i y i + y i i = ae j j j fl j 6 STEPHEN J. <p> A steplength j is chosen such that j fi fi i j fi fi fi fi fi fi i fi fi o; i = 1; ; m;(18) for some o 2 (0; 1). Trivial modifications of the results of Kojima, Mizuno, and Yoshise <ref> [7] </ref> indicate that for the choices ae j j ae P = m + p m and o = 0:4, we have that ( j + j ; y j + j y) ( j ; y j ) 0:2:(19) When some iterate (z j ; j ; y j )
Reference: [8] <author> O. L. Mangasarian and T.-H. Shiau, </author> <title> Error bounds for monotone linear complementarity problems, </title> <journal> Mathematical Programming, </journal> <volume> 36 (1986), </volume> <pages> pp. 81-89. </pages>
Reference-contexts: Our result in Lemma 4.1 is similar to, but more specific than, the bound that would be obtained by applying the analysis of Mangasarian and Shiau <ref> [8] </ref> to (13). We state without proof the following well-known result, which actually applies for any closed convex X ae R n . Lemma 4.2.
Reference: [9] <author> S. J. Wright, </author> <title> Interior point methods for optimal control of discrete-time systems, </title> <type> Tech. Rep. </type> <institution> MCS-P229-0491, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In our case, the situation is better: good starting points will usually be available from previous iterates and from approximate projections for larger values of ff. A priori information has been observed to significantly decrease the number of interior-point iterations (see, for example <ref> [9] </ref>).
Reference: [10] <author> Y. Ye, K. O. Kortanek, J. A. Kaliski, and S. Huang, </author> <title> Near-boundary behavior of primal-dual potential reduction algorithms for linear programming, </title> <type> Tech. Rep. Working Paper Number 90-9, </type> <institution> College of Business Administration, The University of Iowa, </institution> <year> 1990. </year> <note> 24 STEPHEN J. WRIGHT </note>
Reference-contexts: Projection onto the polyhedral set X can be achieved by solving a convex quadratic program or, equivalently, a linear complementarity problem (LCP). In this section, we formulate the problem and outline a primal-dual potential reduction algorithm for solving it. The discussion will be brief, since other papers such as <ref> [6, 7, 10, 11] </ref> can be consulted for details about motivation, analysis, and implementation issues for this class of interior-point algorithms. <p> programs, Zhang, Dennis, and Tapia [11] observe that it is even desirable to let ae j grow unboundedly large as the solution is approached. (The steps produced by (16),(17) are then very close to being Newton steps for the nonlinear equations formed by the equalities in (14).) Ye et al. <ref> [10] </ref> have shown such "large" choices of ae j are not incompatible with obtaining reductions in the potential function. In practical implementations, the line search parameter j is also chosen differently.
Reference: [11] <author> Y. Zhang, J. E. Dennis, and R. A. Tapia, </author> <title> On the superlinear and quadratic convergence of primal-dual interior point linear programming algorithms, </title> <type> Tech. Rep. </type> <institution> TR90-6, Department of Mathematical Sciences, Rice University, </institution> <year> 1990. </year>
Reference-contexts: Projection onto the polyhedral set X can be achieved by solving a convex quadratic program or, equivalently, a linear complementarity problem (LCP). In this section, we formulate the problem and outline a primal-dual potential reduction algorithm for solving it. The discussion will be brief, since other papers such as <ref> [6, 7, 10, 11] </ref> can be consulted for details about motivation, analysis, and implementation issues for this class of interior-point algorithms. <p> In Han, Pardalos, and Ye [6], the choice ae j j m 2 is made for convex quadratic programs. In the context of linear programs, Zhang, Dennis, and Tapia <ref> [11] </ref> observe that it is even desirable to let ae j grow unboundedly large as the solution is approached. (The steps produced by (16),(17) are then very close to being Newton steps for the nonlinear equations formed by the equalities in (14).) Ye et al. [10] have shown such "large" choices <p> m+1 ; y; y m+1 ) generated by the projection algorithm, each time it is called, satisfies i y i l=1 l y l =(m + 1) : Although this assumption conflicts to some extent with the desire for fast asymptotic convergence of the interior-point method, Zhang, Dennis, and Tapia <ref> [11, Theorem 3.1] </ref> observed that, at least in the case of linear programming that they consider, it appears to hold in practice. 8 STEPHEN J. WRIGHT 4. Global Convergence. In this section we prove that all accumulation points of the algorithm of section 2 are critical.
References-found: 9

