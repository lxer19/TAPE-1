URL: http://www.cs.huji.ac.il/labs/learning/Papers/qbc.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Title: Selective sampling using the Query by Committee algorithm  
Author: YOAV FREUND H. SEBASTIAN SEUNG seung@bell-labs.com ELI SHAMIR AND NAFTALI TISHBY Editor: David Haussler 
Keyword: selective sampling, query learning, Bayesian Learning, experimental design  
Address: Murray Hill, NJ 07974  Murray Hill, NJ 07974  Jerusalem, ISRAEL  
Affiliation: AT&T Labs,  Bell Laboratories, Lucent Technologies,  Institute of Computer Science, Hebrew University,  
Note: 1-38 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: yoav@research.att.com  fshamir,tishbyg@cs.huji.ac.il  
Date: Received May 1, 1991  
Abstract: We analyze the "query by committee" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference: <author> A. C. Atkinson and A. N. Donev. </author> <title> Optimum Experimental Designs. </title> <publisher> Oxford science publications, </publisher> <year> 1992. </year>
Reference: <author> Ian Barland. </author> <title> Some ideas on learning with directional feedback. </title> <type> Master's thesis, </type> <institution> University of California at Santa Cruz, </institution> <month> June </month> <year> 1992. </year>
Reference: <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: This added capability enables the learner to maintain its sensitivity to the input distribution, while reducing the number of labels that it needs to know. Baum <ref> (Baum,1991) </ref>, proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence (Baum & Lang,1992) that it works for larger networks.
Reference: <author> E. B. Baum and K. Lang. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference in Neural Networks, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference-contexts: Baum (Baum,1991), proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence <ref> (Baum & Lang,1992) </ref> that it works for larger networks. However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem (Baum & Lang,1992). <p> algorithm is proved to work for networks with at most four hidden units, and there is experimental evidence <ref> (Baum & Lang,1992) </ref> that it works for larger networks. However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem (Baum & Lang,1992). The problem was that many of the images generated by the algorithm as queries did not contain any recognizable character, they were artificial combinations of character images that had no natural meaning.
Reference: <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year> <note> 38 FREUND, </note> <author> SEUNG, SHAMIR AND TISHBY T. Bonnesen and W. Fenchel. </author> <title> Theory of Convex Bodies. </title> <publisher> BCS Associates, </publisher> <address> Moscow, Idaho, USA, </address> <year> 1987. </year>
Reference-contexts: As the distribution of the instances is within D from the uniform distribution, the probability of this set is at least D cos 1 (ff). On the other hand, as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> (Blumer et al.,1989) </ref>. Theorem 2.1 in (Blumer et al.,1989) guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these two arguments, <p> On the other hand, as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> (Blumer et al.,1989) </ref>. Theorem 2.1 in (Blumer et al.,1989) guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these two arguments, we get the statement of the
Reference: <author> David Cohn, Les Atlas, and Richard Ladner. </author> <title> Training connectionist networks with queries and selective sampling. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ido Dagan and Sean P. Engelson. </author> <title> Committee-based sampling for training probabilistic classfiers. </title> <editor> In Priedits and Russel, editors, </editor> <booktitle> The XII International Conference on Machine Learning, </booktitle> <pages> pages 150-157. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: <author> Bonnie Eisenberg and Ronald L. Rivest. </author> <title> On the sample complexity of pac-learning using random and chosen examples. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 154-162, </pages> <year> 1990. </year>
Reference: <author> V. V. Fedorov. </author> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The problem of selecting the optimal examples for learning is closely related to the problem of experimental design in statistics (see e.g. <ref> (Fedorov,1972, Atkinson,1992) </ref>). Experimental design is the analysis of methods for selecting sets of experiments, which correspond to membership queries in the context of learning theory.
Reference: <author> David Haussler, Michael Kearns, and Robert E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 83-113, </pages> <year> 1994. </year>
Reference: <author> W. Kinzel and P. Rujan. </author> <title> Improving a network generalization ability by selecting examples. </title> <journal> Europhys. Lett., </journal> <volume> 13 </volume> <pages> 473-477, </pages> <year> 1990. </year>
Reference-contexts: In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the entropy of the posterior distribution that is induced by the answer to the query. Similar suggestions have been made in the perceptron learning literature <ref> (Kinzel & Rujan,1990) </ref>. A different experimental design criterion is the accuracy with which the outcome of future experiments, chosen from some constrained domain, can be predicted using the hypothesis. This criterion is very similar to criteria used in learning theory. Both criteria are important for us in this paper.
Reference: <author> D. V. Lindley. </author> <title> On a measure of the information provided by an experiment. </title> <journal> Ann. Math. Statist., </journal> <volume> 27 </volume> <pages> 986-1005, </pages> <year> 1956. </year>
Reference-contexts: One natural criterion is the accuracy with which the parameters that define the hypothesis can be estimated <ref> (Lindley,1956) </ref>. In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the entropy of the posterior distribution that is induced by the answer to the query. Similar suggestions have been made in the perceptron learning literature (Kinzel & Rujan,1990).
Reference: <author> C. McDiarmid. </author> <title> On the method of bounded differences. In Survey of Combinatorics, </title> <booktitle> 10th British Combinatorial Conference, </booktitle> <year> 1989. </year>
Reference-contexts: As the instantaneous information gain is bounded between 0 and 1, we get that g Y i 1 g. We can thus use Hoeffding's bound on the tails of bounded step sub-martingales <ref> (McDiarmid,1989) </ref> 5 from which we know that for any * &gt; 0 Pr ( i=1 g ) g+* ( 1 g * n Setting * = g and taking logs we get Pr ( i=1 Y i gn) 1 (1+)g n Choosing = 1=2 we get the bound. <p> Note that the number of calls to Sample is (d=*) ((Blumer et al.,1989)), even if all of the instances are used as queries to Label. 5. The bound as it appears in <ref> (McDiarmid,1989) </ref> is given for martingales. However, it is easily checked that it is also true for super-martingales. Reversing the sign of the Y i we get an equivalent theorem for sub-martingales. 6.
Reference: <author> T.M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2), </volume> <year> 1982. </year>
Reference-contexts: We use ~ X 1:::m to denote the sequence of the first m elements in ~ X. We use the terminology of <ref> (Mitchell,1982) </ref>, and define the version space generated by the sequence of labeled examples h ~ X 1:::m ; c ( ~ X 1:::m )i to be the set of concepts c 0 2 C that are consistent with c on ~ X, i.e. that c 0 (x i ) = c
Reference: <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Fix a sequence of examples ~ X, recall that ~ X M denotes the first m examples. Then Pr c2P I (h ~ X M ; c ( ~ X M )i) (d + 1)(log d d : (7) 14 FREUND, SEUNG, SHAMIR AND TISHBY Proof: From Sauer's Lemma <ref> (Sauer,1972) </ref> we know that the number of different labelings created by m examples is at most P d m (em=d) d .
Reference: <author> H.S Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In (Cohn, Atlas & Ladner,1990) there are several suggestions for query filters together with some empirical tests of their performance on simple problems. In <ref> (Seung, Opper & Sompolinsky,1992) </ref> the authors suggested a filter called "query by committee," (QBC) and analytically calculated its performance for some perceptron-type learning problems. For these problems, they found that the prediction error decreases exponentially fast in the number of queries. <p> In the next section we present such a method. 4. The Query by Committee learning algorithm In <ref> (Seung, Opper & Sompolinsky,1992) </ref> the authors devise an algorithm for learning with queries which they called "Query by Committee" and we shall refer to as the QBC algorithm. <p> If the two predictions differ, it calls Label with input x, and adds the labeled example to the set of labeled examples that define the version space. 8 FREUND, SEUNG, SHAMIR AND TISHBY It then proceeds to the next iteration. In <ref> (Seung, Opper & Sompolinsky,1992) </ref> the authors treat the query by committee algorithm as an on-line learning algorithm, and analyze the rate at which the error of the two Gibbs learners reduces as a function of the number of queries made. <p> Summary We have proved that the Query by Committee algorithm is an efficient query algorithm for the perceptron concept class with distributions that are close to uniform. This gives a rigorous proof to the results given in <ref> (Seung, Opper & Sompolinsky,1992) </ref> which were obtained using the replica method of statistical mechanics. It also generalizes their results by relaxing the requirements on the distribution of the examples and on the prior distribution. In addition, we show that exact knowledge of the prior distribution is not required.
Reference: <author> Peter Smith. </author> <title> Convexity Methods in Variational Calculus. </title> <publisher> Research studies press, John Wiley & sons, </publisher> <year> 1985. </year>
Reference: <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year> <note> Received Date Accepted Date Final Manuscript Date </note>
Reference-contexts: In this paradigm the learner is passive and has no control over the information that it receives. In contrast, in the query paradigm, the learner is given the power to ask questions. What does the learner gain from this additional power? Study of the use of queries in learning <ref> (Valiant,1984, Angluin,1988) </ref>, has mostly concentrated on algorithms for exact identification of the target concept. This type of analysis concentrates on the worst case behavior of the algorithm, and no probabilistic assumptions are made.
References-found: 19

