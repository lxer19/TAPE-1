URL: http://netlib.bell-labs.com/cm/cs/doc/92/2-28.ps.gz
Refering-URL: http://www.ics.uci.edu/~eppstein/gina/typography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Polygonal Approximations that Minimize the Number of Inflections  
Author: John D. Hobby 
Abstract: Consider the problem of processing shape information derived from a noisy source such as a digital scanner. The object is to construct a polygon or a closed curve that matches the input polygon to within a fixed error tolerance and maximizes some intuitive notion of "smoothness and simplicity". Part of this goal should be to minimize the number of inflections. The algorithm presented here finds an inflection-minimizing polygonal approximation and produces a data structure that characterizes a set of closed curves that fall within the error tolerance and minimize the number of inflections. The algorithm runs in linear time, is reasonably fast in practice, and can be implemented in low-precision integer arithmetic. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Albano. </author> <title> Representation of digitized contours in terms of conic arcs and straight-line segments. Com-put. Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 3(1) </volume> <pages> 23-33, </pages> <month> Mar. </month> <year> 1974. </year> <note> 10 J. </note> <editor> D. </editor> <publisher> Hobby </publisher>
Reference-contexts: An earlier paper by the present author [13] does claim to minimize inflections, but it only works for perfect, noise-free images. When spline approximations are needed, they can be generated by extending polygonal approximation algorithms as Albano <ref> [1] </ref> and Gangnet [11] have done, or by postprocessing a polygonal approximation as Liao [18] and Rosin and West [27] do. The algorithm presented below is well-suited to such post-processing. We begin in Section 2 with a summary of the applications that have motivated this work.
Reference: [2] <author> N. Ansari and E. J. Delp. </author> <title> On detecting dominant points. </title> <journal> Pattern Recog., </journal> <volume> 24(4) </volume> <pages> 441-451, </pages> <year> 1991. </year>
Reference-contexts: Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. Algorithms by Davis [6], Hemminger and Pomalaza-Raez [12], and Ansari and Delp <ref> [2] </ref> provide little control over the approximation error. None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. <p> See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp <ref> [2] </ref> are all significantly worse than linear. Another important way a polygonal approximation algorithm can be optimal is to minimize the number of inflections subject to a bound on the error. In order to approach the intuitive notion of "the smoothest allow 1 2 J. D.
Reference: [3] <author> H. Aoyama and M. Kawagoe. </author> <title> A piecewise linear approximation method preserving visual feature points of original figures. CVGIP Gr. </title> <booktitle> Models Image Proc., </booktitle> <volume> 53(5) </volume> <pages> 435-446, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] [9] [17] <ref> [3] </ref> [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in <p> Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] [9] [17] <ref> [3] </ref> [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in milliseconds per input vertex. The N and T columns give the number of input vertices and the error tolerance in pixels or as a multiple of the average input segment length . <p> Roberge [26] used a CDC Cyber 6000; Dunham [8] used a T.I. Professional Computer; Fahn et. al. [9] used a VAX 11/780; Leung and Yang [17] used a Sun 4/280; and Aoyama and Kawagoe <ref> [3] </ref> used an Apollo DN3000. (a) (b) with tolerances (a) 1 pixel; (b) 2 pixels. Dots mark output vertices and thin lines show the input path. (a) (b) (a) 1 pixel; (b) 2 pixels. Dots mark output vertices and thin lines show the input path.
Reference: [4] <author> F. Badi'i and B. Peikari. </author> <title> Functional approximation of planar curves via adaptive segmentation. </title> <journal> Int. J. Syst. Sci., </journal> <volume> 13(6) </volume> <pages> 667-674, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari <ref> [4] </ref>. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. <p> Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari <ref> [4] </ref>. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] <ref> [4] </ref> [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0
Reference: [5] <author> J. P. Bixler, L. T. Watson, and J. P. Sanford. </author> <title> Spline-based recognition of straight lines and curves in engineering line drawings. Image Vis. </title> <journal> Comput., </journal> <volume> 6(4) </volume> <pages> 262-269, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Another computer vision problem is digitizing engineering diagrams to be stored in CAD/CAM systems. (Some existing nuclear power plants are described only in hundreds of thousands of hard-copy drawings.) Smoothing polygons is an important step in reading such diagrams; see Bixler et. al. for more details <ref> [5] </ref>. 3 Data Structures for Allowable Curves In order for a polygon Q to match the input polygon P to within an error bound of *, we require Q to pass P 's vertices in order, each within 1-norm distance *.
Reference: [6] <author> L. S. Davis. </author> <title> Shape matching using relaxation techniques. </title> <journal> IEEE Trans. Patt. Anal. and Machine Intel., </journal> <volume> PAMI-1(1):60-72, </volume> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. Algorithms by Davis <ref> [6] </ref>, Hemminger and Pomalaza-Raez [12], and Ansari and Delp [2] provide little control over the approximation error. None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. <p> Much work in computer vision involves extracting outlines from a digitized image and smoothing them while retaining important features. In the shape matching problem, a robot identifies parts coming down an assembly line by scanning images for shapes that match a set of templates. Davis <ref> [6] </ref> begins the shape matching process by finding a polygonal approximation to the shape outlines that preserves the important features while eliminating noise. <p> Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] <ref> [6] </ref> [30] [4] [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750
Reference: [7] <author> G. Dettori. </author> <title> An on-line algorithm for polygonal approximation of digitized plane curves. </title> <booktitle> In Proceedings of the 6th International Joint Conference on Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 739-741, </pages> <year> 1982. </year>
Reference-contexts: Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori <ref> [7] </ref>, and Ansari and Delp [2] are all significantly worse than linear. Another important way a polygonal approximation algorithm can be optimal is to minimize the number of inflections subject to a bound on the error. In order to approach the intuitive notion of "the smoothest allow 1 2 J.
Reference: [8] <author> J. G. Dunham. </author> <title> Optimum uniform piecewise linear approximation of planar curves. </title> <journal> IEEE Trans. Patt. Anal. and Machine Intel., </journal> <volume> PAMI-8(1):67-75, </volume> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham <ref> [8] </ref>, and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp [2] are all significantly worse than linear. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] <ref> [8] </ref> [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published <p> Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] <ref> [8] </ref> [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in milliseconds per input vertex. <p> The N and T columns give the number of input vertices and the error tolerance in pixels or as a multiple of the average input segment length . Roberge [26] used a CDC Cyber 6000; Dunham <ref> [8] </ref> used a T.I. Professional Computer; Fahn et. al. [9] used a VAX 11/780; Leung and Yang [17] used a Sun 4/280; and Aoyama and Kawagoe [3] used an Apollo DN3000. (a) (b) with tolerances (a) 1 pixel; (b) 2 pixels.
Reference: [9] <author> C.-S. Fahn, J.-F. Wang, and J.-Y. Lee. </author> <title> An adaptive reduction procedure for the piecewise linear approximation of digitized curves. </title> <journal> IEEE Trans. Patt. Anal. and Machine Intel., </journal> <volume> 11(9) </volume> <pages> 967-973, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: All except for Pavlidis [23] produce a subset of the input vertices. The output for [23] is a sequence of disjoint line segments. Most of the sources agree that Roberge [26] gives the fastest algorithm, although Fahn et. al. <ref> [9] </ref> claim their's runs a little faster. The comparative timings were done with Roberge's "extension factor" parameter equal to 1; Roberge suggests larger values for which his timings are 1.5 to 1.6 times slower. Thus Wall and Danielsson [34] is a reasonable choice for speed comparisons. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] <ref> [9] </ref> [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation <p> Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] <ref> [9] </ref> [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in milliseconds per input vertex. <p> The N and T columns give the number of input vertices and the error tolerance in pixels or as a multiple of the average input segment length . Roberge [26] used a CDC Cyber 6000; Dunham [8] used a T.I. Professional Computer; Fahn et. al. <ref> [9] </ref> used a VAX 11/780; Leung and Yang [17] used a Sun 4/280; and Aoyama and Kawagoe [3] used an Apollo DN3000. (a) (b) with tolerances (a) 1 pixel; (b) 2 pixels.
Reference: [10] <author> H. Freeman and J. M. Glass. </author> <title> On quantization of line-drawing data. </title> <journal> IEEE Trans. Syst. Sci. and Cybernetics, </journal> <volume> SSC-5(1):70-79, </volume> <month> Jan. </month> <year> 1969. </year>
Reference-contexts: The algorithm has been implemented and is being tested on real-world applications. The need for simple approximations that eliminate noise has motivated a lot of work on polygonal and spline approximation beginning with Freeman and Glass <ref> [10] </ref>, Montanari [19], and Ramer [24]. None of these early papers give linear-time algorithms, and Mon-tanari's algorithm takes time (n 3 ) on an n-vertex polygon. Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output.
Reference: [11] <author> M. Gangnet. </author> <title> Approximation of digitized contours. </title> <editor> In R. A. Earnshaw, editor, </editor> <booktitle> Theoretical Foundations of Computer Graphics and CAD, </booktitle> <pages> pages 833-855. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: An earlier paper by the present author [13] does claim to minimize inflections, but it only works for perfect, noise-free images. When spline approximations are needed, they can be generated by extending polygonal approximation algorithms as Albano [1] and Gangnet <ref> [11] </ref> have done, or by postprocessing a polygonal approximation as Liao [18] and Rosin and West [27] do. The algorithm presented below is well-suited to such post-processing. We begin in Section 2 with a summary of the applications that have motivated this work.
Reference: [12] <author> T. L. Hemminger and C. A. Pomalaza-Raez. </author> <title> Polygonal represenctaion: A maximum likelihood approach. Comput. Vision Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 52(2) </volume> <pages> 239-247, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. Algorithms by Davis [6], Hemminger and Pomalaza-Raez <ref> [12] </ref>, and Ansari and Delp [2] provide little control over the approximation error. None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time.
Reference: [13] <author> J. D. </author> <title> Hobby. Smoothing digitized contours. </title> <editor> In R. A. Earnshaw, editor, </editor> <booktitle> Theoretical Foundations of Computer Graphics and CAD, </booktitle> <pages> pages 777-793. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Hence, the minimum perimeter method does not yield approximation algorithms; it just computes the minimum number of inflections. An earlier paper by the present author <ref> [13] </ref> does claim to minimize inflections, but it only works for perfect, noise-free images.
Reference: [14] <author> H. Imai and M. Iri. </author> <title> Computational-geometric methods for polygonal approximation of a curve. Comput. Vision Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 36(1) </volume> <pages> 31-41, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri <ref> [14] </ref>. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp [2] are all significantly worse than linear.
Reference: [15] <author> Y. Kurozumi and W. A. Davis. </author> <title> Polygonal approximation by the minimax method. Comput. Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 19(3) </volume> <pages> 248-264, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis <ref> [15] </ref>, Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp [2] are all significantly worse than linear.
Reference: [16] <author> D. Lee, G. W. Wasilkowski, and R. Mehrotra. </author> <title> A new zero-crossing-based discontinuity detector. </title> <journal> IEEE Trans. Image Proc., </journal> <note> to appear. </note>
Reference-contexts: He proposes some automatable techniques, but information loss during preprocessing led him to do some of the feature extraction by hand. A promising approach to feature extraction combines our algorithm with an edge detection algorithm such as that of Lee et. al. <ref> [16] </ref>. The edge detection algorithm generates polygonal boundaries, which are then smoothed via our algorithm as shown in Figure 2.
Reference: [17] <author> M. K. Leung and Y.-H. Yang. </author> <title> Dynamic strip algorithm in curve fitting. Comput. Vision Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 51(2) </volume> <pages> 145-165, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez [30], Leung and Yang <ref> [17] </ref>, and Badi'i and Peikari [4]. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] [9] <ref> [17] </ref> [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times <p> Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] [26] [8] [9] <ref> [17] </ref> [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in milliseconds per input vertex. <p> Roberge [26] used a CDC Cyber 6000; Dunham [8] used a T.I. Professional Computer; Fahn et. al. [9] used a VAX 11/780; Leung and Yang <ref> [17] </ref> used a Sun 4/280; and Aoyama and Kawagoe [3] used an Apollo DN3000. (a) (b) with tolerances (a) 1 pixel; (b) 2 pixels. Dots mark output vertices and thin lines show the input path. (a) (b) (a) 1 pixel; (b) 2 pixels.
Reference: [18] <author> Y.-Z. Liao. </author> <title> A two-stage method of fitting conic arcs and straight line segments to digitized contours. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on Pattern Recognition and Image Processing, </booktitle> <pages> pages 224-229, </pages> <month> Aug. </month> <year> 1981. </year>
Reference-contexts: When spline approximations are needed, they can be generated by extending polygonal approximation algorithms as Albano [1] and Gangnet [11] have done, or by postprocessing a polygonal approximation as Liao <ref> [18] </ref> and Rosin and West [27] do. The algorithm presented below is well-suited to such post-processing. We begin in Section 2 with a summary of the applications that have motivated this work.
Reference: [19] <author> U. Montanari. </author> <title> A note on minimal length polygonal approximation to a digitized contour. </title> <journal> Commun. ACM, </journal> <volume> 13(1) </volume> <pages> 41-47, </pages> <month> Jan. </month> <year> 1970. </year>
Reference-contexts: The algorithm has been implemented and is being tested on real-world applications. The need for simple approximations that eliminate noise has motivated a lot of work on polygonal and spline approximation beginning with Freeman and Glass [10], Montanari <ref> [19] </ref>, and Ramer [24]. None of these early papers give linear-time algorithms, and Mon-tanari's algorithm takes time (n 3 ) on an n-vertex polygon. Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output. <p> In order to approach the intuitive notion of "the smoothest allow 1 2 J. D. Hobby able approximation" it is surely necessary to minimize inflections. The only relevant algorithms are based on Montanari's idea of minimizing the perimeter subject to the error bounds <ref> [19] </ref>. (See also Sklansky [28, 29, 31].) All of these algorithms have the basic flaw that every output vertex is at the maximum allowable distance from the input. Hence, the minimum perimeter method does not yield approximation algorithms; it just computes the minimum number of inflections.
Reference: [20] <author> T. Pavlidis. </author> <title> Polygonal approximations by Newton's method. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-26(8):801-807, </volume> <month> Aug. </month> <year> 1977. </year>
Reference-contexts: None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. See Pavlidis <ref> [20] </ref>, Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp [2] are all significantly worse than linear.
Reference: [21] <author> T. Pavlidis. </author> <title> Structural Pattern Recognition, </title> <publisher> pages 11-45,147-184. Springer Series in Electrophysics. Springer Verlag, </publisher> <year> 1977. </year>
Reference-contexts: None of these early papers give linear-time algorithms, and Mon-tanari's algorithm takes time (n 3 ) on an n-vertex polygon. Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output. Tomek <ref> [33, 21] </ref> and Reumann fl AT&T Bell Laboratories, Murray Hill, NJ 07974 (a) (b) be obtained from a digital image. and Witkam [25] developed linear-time algorithms that are fast in practice. <p> Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis <ref> [23, 21, 22] </ref>, Dettori [7], and Ansari and Delp [2] are all significantly worse than linear. Another important way a polygonal approximation algorithm can be optimal is to minimize the number of inflections subject to a bound on the error.
Reference: [22] <author> T. Pavlidis. </author> <title> Algorithms for Graphics and Image Processing, section 12.5. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1982. </year>
Reference-contexts: Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis <ref> [23, 21, 22] </ref>, Dettori [7], and Ansari and Delp [2] are all significantly worse than linear. Another important way a polygonal approximation algorithm can be optimal is to minimize the number of inflections subject to a bound on the error. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] <ref> [22] </ref> [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3
Reference: [23] <author> T. Pavlidis and S. L. Horowitz. </author> <title> Segmentation of plane curves. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-23(8):860-870, </volume> <month> Aug. </month> <year> 1974. </year>
Reference-contexts: Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams [36], Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis <ref> [23, 21, 22] </ref>, Dettori [7], and Ansari and Delp [2] are all significantly worse than linear. Another important way a polygonal approximation algorithm can be optimal is to minimize the number of inflections subject to a bound on the error. <p> By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. All except for Pavlidis <ref> [23] </ref> produce a subset of the input vertices. The output for [23] is a sequence of disjoint line segments. Most of the sources agree that Roberge [26] gives the fastest algorithm, although Fahn et. al. [9] claim their's runs a little faster. <p> By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. All except for Pavlidis <ref> [23] </ref> produce a subset of the input vertices. The output for [23] is a sequence of disjoint line segments. Most of the sources agree that Roberge [26] gives the fastest algorithm, although Fahn et. al. [9] claim their's runs a little faster. <p> Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] <ref> [23] </ref> [35] [6] [30] [4] [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2
Reference: [24] <author> U. Ramer. </author> <title> An iterative procedure for the polygonal approximation of plane curves. Comput. Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 1(3) </volume> <pages> 244-256, </pages> <month> Nov. </month> <year> 1972. </year>
Reference-contexts: The algorithm has been implemented and is being tested on real-world applications. The need for simple approximations that eliminate noise has motivated a lot of work on polygonal and spline approximation beginning with Freeman and Glass [10], Montanari [19], and Ramer <ref> [24] </ref>. None of these early papers give linear-time algorithms, and Mon-tanari's algorithm takes time (n 3 ) on an n-vertex polygon. Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output. <p> Thus Wall and Danielsson [34] is a reasonable choice for speed comparisons. An optimized C implementation was readily available, and this algorithm is popular in certain application areas. The implementation used one pass of Ramer's algorithm <ref> [24] </ref> to improve the output. Figure 10 shows the results of improved Wall and Danielsson, and Figure 11 shows how the results of Algorithm 4.1 compare. Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. <p> Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T <ref> [24] </ref> [23] [35] [6] [30] [4] [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3
Reference: [25] <author> K. Reumann and A. P. M. Witkam. </author> <title> Optimizing curve segmentation in computer graphics. </title> <editor> In A. Gunther, B. Levrat, and H. Lipps, editors, </editor> <booktitle> International Computing Symposium, </booktitle> <pages> pages 467-472. </pages> <publisher> American Elsevier, </publisher> <year> 1974. </year>
Reference-contexts: Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output. Tomek [33, 21] and Reumann fl AT&T Bell Laboratories, Murray Hill, NJ 07974 (a) (b) be obtained from a digital image. and Witkam <ref> [25] </ref> developed linear-time algorithms that are fast in practice. They sequentially choose a subset of the input vertices based on a bound on the maximum pointwise error for the resulting polygonal approximation. Roberge [26] gives an improvement that reduces the number of vertices chosen.
Reference: [26] <author> J. Roberge. </author> <title> A data reduction algorithm for planar curves. Comput. Vision Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 29(2) </volume> <pages> 168-195, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: They sequentially choose a subset of the input vertices based on a bound on the maximum pointwise error for the resulting polygonal approximation. Roberge <ref> [26] </ref> gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari [4]. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> All except for Pavlidis [23] produce a subset of the input vertices. The output for [23] is a sequence of disjoint line segments. Most of the sources agree that Roberge <ref> [26] </ref> gives the fastest algorithm, although Fahn et. al. [9] claim their's runs a little faster. The comparative timings were done with Roberge's "extension factor" parameter equal to 1; Roberge suggests larger values for which his timings are 1.5 to 1.6 times slower. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] <ref> [26] </ref> [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] [34] <ref> [26] </ref> [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table 1: Published computation times in milliseconds <p> The N and T columns give the number of input vertices and the error tolerance in pixels or as a multiple of the average input segment length . Roberge <ref> [26] </ref> used a CDC Cyber 6000; Dunham [8] used a T.I. Professional Computer; Fahn et. al. [9] used a VAX 11/780; Leung and Yang [17] used a Sun 4/280; and Aoyama and Kawagoe [3] used an Apollo DN3000. (a) (b) with tolerances (a) 1 pixel; (b) 2 pixels.
Reference: [27] <author> P. L. Rosin and G. A. W. West. </author> <title> Segmentation of edges into lines and arcs. Image Vis. </title> <journal> Comput., </journal> <volume> 7(2) </volume> <pages> 109-114, </pages> <year> 1989. </year>
Reference-contexts: Depending on how the conversion is done, shape boundaries extracted from black-and-white images look like the polygons in Figure 1a or Figure 1b. It is also possible to extract similar information from gray-level images as explained by Rosin and West <ref> [27] </ref>. The "jaggies" in Figure 1 represent noise that complicates the polygons and tends to interfere with character recognition, shape matching, or whatever the shape information is to be used for. This paper presents an algorithm that gives superior results and is fast enough to be very practical. <p> When spline approximations are needed, they can be generated by extending polygonal approximation algorithms as Albano [1] and Gangnet [11] have done, or by postprocessing a polygonal approximation as Liao [18] and Rosin and West <ref> [27] </ref> do. The algorithm presented below is well-suited to such post-processing. We begin in Section 2 with a summary of the applications that have motivated this work.
Reference: [28] <author> J. Sklansky. </author> <title> Recognition of convex blobs. </title> <journal> Pattern Recog., </journal> <volume> 2(1) </volume> <pages> 3-10, </pages> <month> Jan. </month> <year> 1970. </year>
Reference-contexts: In order to approach the intuitive notion of "the smoothest allow 1 2 J. D. Hobby able approximation" it is surely necessary to minimize inflections. The only relevant algorithms are based on Montanari's idea of minimizing the perimeter subject to the error bounds [19]. (See also Sklansky <ref> [28, 29, 31] </ref>.) All of these algorithms have the basic flaw that every output vertex is at the maximum allowable distance from the input. Hence, the minimum perimeter method does not yield approximation algorithms; it just computes the minimum number of inflections.
Reference: [29] <author> J. Sklansky, R. L. Chazin, and B. J. Hansen. </author> <title> Minimum perimeter polygons of digitized silhouetts. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21(3):260-268, </volume> <month> Mar. </month> <year> 1972. </year>
Reference-contexts: In order to approach the intuitive notion of "the smoothest allow 1 2 J. D. Hobby able approximation" it is surely necessary to minimize inflections. The only relevant algorithms are based on Montanari's idea of minimizing the perimeter subject to the error bounds [19]. (See also Sklansky <ref> [28, 29, 31] </ref>.) All of these algorithms have the basic flaw that every output vertex is at the maximum allowable distance from the input. Hence, the minimum perimeter method does not yield approximation algorithms; it just computes the minimum number of inflections.
Reference: [30] <author> J. Sklansky and V. Gonzalez. </author> <title> Fast polygonal approximation of digitized contours. </title> <journal> Pattern Recog., </journal> <volume> 12(5) </volume> <pages> 327-331, </pages> <year> 1980. </year>
Reference-contexts: Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez <ref> [30] </ref>, Leung and Yang [17], and Badi'i and Peikari [4]. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] <ref> [30] </ref> [4] [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big
Reference: [31] <author> J. Sklansky and D. F. Kibler. </author> <title> A theory of nonuniformity in digitized binary pictures. </title> <journal> IEEE Trans. Syst., Man, and Cybernetics, </journal> <volume> SMC-6(9):637-647, </volume> <month> Sept. </month> <year> 1976. </year>
Reference-contexts: In order to approach the intuitive notion of "the smoothest allow 1 2 J. D. Hobby able approximation" it is surely necessary to minimize inflections. The only relevant algorithms are based on Montanari's idea of minimizing the perimeter subject to the error bounds [19]. (See also Sklansky <ref> [28, 29, 31] </ref>.) All of these algorithms have the basic flaw that every output vertex is at the maximum allowable distance from the input. Hence, the minimum perimeter method does not yield approximation algorithms; it just computes the minimum number of inflections.
Reference: [32] <author> M. K. </author> <title> Sparrow. Topological Coding of Single Fingerprints. </title> <type> PhD thesis, </type> <institution> University of Kent at Canterbury, </institution> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Figures 10 and 11 in Section 6 give examples of how the polygons produced by our algorithm compare to those from faster algorithms: they appear much smoother and they follow the input polygons more closely. Another potential application domain is automated fingerprint processing. Sparrow explains <ref> [32] </ref> that it is very difficult to extract a storable feature set that allows isolated fingerprints to be recognized in the presence of common distortions (such as smudging, blurring, and stretching).
Reference: [33] <author> I. Tomek. </author> <title> Two algorithms for piecewise-linear continuous approximation of functions of one variable. </title> <journal> IEEE Trans. Comput., </journal> <volume> 23(4) </volume> <pages> 445-448, </pages> <month> Apr. </month> <year> 1974. </year>
Reference-contexts: None of these early papers give linear-time algorithms, and Mon-tanari's algorithm takes time (n 3 ) on an n-vertex polygon. Subsequent work has focused on reducing running time, improving quality of approximation, and minimizing complexity of output. Tomek <ref> [33, 21] </ref> and Reumann fl AT&T Bell Laboratories, Murray Hill, NJ 07974 (a) (b) be obtained from a digital image. and Witkam [25] developed linear-time algorithms that are fast in practice.
Reference: [34] <author> K. Wall and P.-E. Danielsson. </author> <title> A fast sequential method for polygonal approximation of digitized curves. Comput. Vision Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 28(2) </volume> <pages> 220-227, </pages> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. Other linear-time algorithms include Wall and Dan-ielsson <ref> [34] </ref> which measures error by keeping track of the area between the input polygon and the approximation. Algorithms by Davis [6], Hemminger and Pomalaza-Raez [12], and Ansari and Delp [2] provide little control over the approximation error. <p> The algorithm presented here is an ideal way to do this. Henry Baird of AT&T Bell Laboratories plans to incorporate our implementation into his OCR system. He is currently using the Wall and Danielsson algorithm <ref> [34] </ref>; he plans to continue to use that algorithm for easy cases, and to switch to our more elaborate algorithm when the system requires more accuracy. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> The comparative timings were done with Roberge's "extension factor" parameter equal to 1; Roberge suggests larger values for which his timings are 1.5 to 1.6 times slower. Thus Wall and Danielsson <ref> [34] </ref> is a reasonable choice for speed comparisons. An optimized C implementation was readily available, and this algorithm is popular in certain application areas. The implementation used one pass of Ramer's algorithm [24] to improve the output. <p> Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] [35] [6] [30] [4] [22] <ref> [34] </ref> [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo 1750 big 9.0 7.3 Table
Reference: [35] <author> C. M. Williams. </author> <title> An efficient algorithm for the piecewise-linear approximation of planar curves. Com-put. Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 8(2) </volume> <pages> 286-293, </pages> <month> Oct. </month> <year> 1978. </year>
Reference-contexts: Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams <ref> [35, 36] </ref>, Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari [4]. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. <p> No published algorithm is quite so simple-minded, but some of them do use simple rules to select a subset of the input vertices. Table 1 gives comparative timings for a number of algorithms of this type <ref> [35, 30, 4, 34, 26, 17] </ref> as well as a number of other algorithms. By listing comparative timing data from available literature, the table gives a rough idea of the relative speed of a wide range of competing algorithms. <p> Relative timings for Algorithm 4.1 and improved Wall and Danielsson appear in Table 2. Algorithm 4.1 is 9.5 to 14.5 times slower, but is still quite fast on an absolute scale. Polygonal Approximations that Minimize Inflections 9 Time (ms./vertex) for each algorithm Source N T [24] [23] <ref> [35] </ref> [6] [30] [4] [22] [34] [26] [8] [9] [17] [3] [26] 1000 1.6 .31 .10 Cyber 1000 16 .28 .09 [8] 402 1 75 600 75 66 82 118 88 66 241 [9] 292 1 5.4 5.0 12.6 4.2 [17] 522 1 .06 .12 [3] 1337 big 8.3 20.2 Apollo
Reference: [36] <author> C. M. Williams. </author> <title> Bounded straight-line approximation of digitized planar curves and lines. Comput. Gr. </title> <booktitle> and Image Proc., </booktitle> <volume> 16(4) </volume> <pages> 370-381, </pages> <month> Aug. </month> <year> 1981. </year>
Reference-contexts: Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams <ref> [35, 36] </ref>, Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari [4]. All but [4] run in linear time; [36] is unique in that the output vertices are not restricted to be a subset of the input. <p> Roberge [26] gives an improvement that reduces the number of vertices chosen. A slightly better formulation of the greedy approach is the cone intersection method of Williams [35, 36], Sklansky and Gonzalez [30], Leung and Yang [17], and Badi'i and Peikari [4]. All but [4] run in linear time; <ref> [36] </ref> is unique in that the output vertices are not restricted to be a subset of the input. Other linear-time algorithms include Wall and Dan-ielsson [34] which measures error by keeping track of the area between the input polygon and the approximation. <p> None of the linear-time algorithms achieves an optimal trade-off between the approximation error and the number of output vertices. Existing methods for doing this involve at least (n 2 ) time. See Pavlidis [20], Williams <ref> [36] </ref>, Kurozumi and Davis [15], Dunham [8], and Imai and Iri [14]. Some speed can be gained looking for local rather than global optima, but algorithms by Pavlidis [23, 21, 22], Dettori [7], and Ansari and Delp [2] are all significantly worse than linear.
References-found: 36

