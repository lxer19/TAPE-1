URL: ftp://ftp.idsia.ch/pub/juergen/fki-198-94.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/caruana/pub/transferbib.html
Root-URL: 
Email: schmidhu@informatik.tu-muenchen.de  
Title: ON LEARNING HOW TO LEARN LEARNING STRATEGIES  
Author: Jurgen Schmidhuber 
Web: http://papa.informatik.tu-muenchen.de/mitarbeiter/schmidhu.html  
Address: 80290 Munchen, Germany  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  
Date: (revised)  Revised January 31, 1995  
Pubnum: Technical Report FKI-198-94  
Abstract: This paper introduces the "incremental self-improvement paradigm". Unlike previous methods, incremental self-improvement encourages a reinforcement learning system to improve the way it learns, and to improve the way it improves the way it learns ..., without significant theoretical limitations | the system is able to "shift its inductive bias" in a universal way. Its major features are: (1) There is no explicit difference between "learning", "meta-learning", and other kinds of information processing. Using a Turing machine equivalent programming language, the system itself occasionally executes self-delimiting, initially highly random "self-modification programs" which modify the context-dependent probabilities of future action sequences (including future self-modification programs). (2) The system keeps only those probability modifications computed by "useful" self-modification programs: those which bring about more payoff (reward, reinforcement) per time than all previous self-modification programs. (3) The computation of payoff per time takes into account all the computation time required for learning | the entire system life is considered: boundaries between learning trials are ignored (if there are any). A particular implementation based on the novel paradigm is presented. It is designed to exploit what conventional digital machines are good at: fast storage addressing, arithmetic operations etc. Experiments illustrate the system's mode of operation. Keywords: Self-improvement, self-reference, introspection, machine-learning, reinforcement learning. Note: This is the revised and extended version of an earlier report from November 24, 1994. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adleman, L. </author> <year> (1979). </year> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Laboratory for Computer Science, MIT. </institution>
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist approaches for control. </title> <type> Technical Report COINS Technical Report 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003. </address>
Reference: <author> Chaitin, G. </author> <year> (1969). </year> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159. </pages>
Reference: <author> Dayan, P. and Sejnowski, T. </author> <year> (1994). </year> <title> TD(): Convergence with probability 1. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 295-301. </pages>
Reference: <author> Dickmanns, D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1986). </year> <title> Der genetische Algorithmus: Eine Imple-mentierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference: <author> Dietterich, T. G. </author> <year> (1989). </year> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, </address> <pages> pages 124-128. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hoffmeister, F. and Back, T. </author> <year> (1991). </year> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <editor> In Manner, R. and Schwefel, H. P., editors, </editor> <booktitle> Proc. of 1st International Conference on Parallel Problem Solving from Nature, </booktitle> <address> Berlin. </address> <publisher> Springer. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor. </address>
Reference: <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11. </pages>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 313-324. </pages> <publisher> Addison Wesley Publishing Company. </publisher>
Reference-contexts: all other previous approaches I am aware of, were either quite limited (many essential aspects of system behavior being unmodifiable), and/or lacked a convincing global credit assignment strategy (as embodied by the top-level strategy of the incremental self-improvement paradigm). 5 Today, this approach would be classified as "Genetic Programming", e.g. <ref> (Koza, 1992) </ref>. 17 of higher level GAs whose domains were to construct construction strategies (Schmidhuber, 1987). Meta--evolution recursively creates a growing hierarchy of pools of programs | higher-level pools containing program modifying programs being applied to lower-level programs and being rewarded based on lower-level performance. Collapsing meta-levels.
Reference: <author> Lenat, D. </author> <year> (1983). </year> <title> Theory formation by heuristic search. </title> <journal> Machine Learning, </journal> <volume> 21. </volume>
Reference: <author> Levin, L. A. </author> <year> (1974). </year> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference: <author> Paul, W. and Solomonoff, R. J. </author> <year> (1991). </year> <title> Autonomous theory building systems. </title> <type> Manuscript, </type> <note> revised 1994. 19 Rechenberg, </note> <author> I. </author> <year> (1971). </year> <title> Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biol--ogischen Evolution. Dissertation. </title> <note> Published 1973 by Fromman-Holzboog. </note>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference-contexts: So are self-modifications speeding up the search for self-modifications speeding up payoff intake. This encourages "learning how to learn", and "learning how to learn how to learn"..., and represents an essential difference to previous approaches to continual learning, see <ref> (Ring, 1994) </ref>. 4. Directed mutations as opposed to random mutations. Unlike evolutionary and genetic algorithms (Rechenberg, 1971; Schwefel, 1974; Holland, 1975; Hoffmeister and Back, 1991; Koza, 1992), self-modification programs may lead to very specific, directed sequences of strategy mutations, as opposed to undirected, totally random mutations.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1987). </year> <title> Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... </title> <type> hook. </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: aspects of system behavior being unmodifiable), and/or lacked a convincing global credit assignment strategy (as embodied by the top-level strategy of the incremental self-improvement paradigm). 5 Today, this approach would be classified as "Genetic Programming", e.g. (Koza, 1992). 17 of higher level GAs whose domains were to construct construction strategies <ref> (Schmidhuber, 1987) </ref>. Meta--evolution recursively creates a growing hierarchy of pools of programs | higher-level pools containing program modifying programs being applied to lower-level programs and being rewarded based on lower-level performance. Collapsing meta-levels. The explicit creation of "meta-levels" and "meta-meta-levels" seemed unnatural, however. <p> Collapsing meta-levels. The explicit creation of "meta-levels" and "meta-meta-levels" seemed unnatural, however. For this reason, alternative systems based on "self-referential" languages were explored, the goal being to collapse all meta-levels into one <ref> (Schmidhuber, 1987) </ref>. At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. Later work presented a neural network with the potential to run its own weight change algorithm (Schmidhuber 1992, 1993a, 1993b). With this system, top-level credit assignment is performed by gradient descent.
Reference: <author> Schmidhuber, J. H. </author> <year> (1991). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Universality implies that the system is in principle capable of creating programs for storing representations of environmental events. Unlike with most previous reinforcement-learning algorithms, see e.g. (Barto, 1989; Watkins, 1989; Dayan and Sejnowski, 1994; Williams, 1992; Sutton, 1991), there is no need for a Markovian interface <ref> (Schmidhuber, 1991) </ref> between the environment and the learning system. Also, there is no need for a "discount factor" discounting the system's expectation of future payoff in case of potentially infinite life spans. 9. <p> From the system's point of view, its interface to the environment is non-Markovian <ref> (Schmidhuber, 1991) </ref>: the current input does not provide all information about the agent's current position. (3) To make use of the few inputs it gets, the system first has to discover that certain 15 input cells may be relevant for solving its task. (4) The total environment (including the work area)
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Steps towards "self-referential" learning. </title> <type> Technical Report CU-CS-627-92, </type> <institution> Dept. of Comp. Sci., University of Colorado at Boulder. </institution>
Reference-contexts: At that time, however, no convincing global credit assignment strategy was provided. Self-referential neural nets. Later work presented a neural network with the potential to run its own weight change algorithm <ref> (Schmidhuber 1992, 1993a, 1993b) </ref>. With this system, top-level credit assignment is performed by gradient descent. This is unsatisfactory, however, due to problems with local minima, and because repeatable training sequences are required. In general, this makes it impossible to take the entire learning history into account.
Reference: <author> Schmidhuber, J. H. </author> <year> (1993a). </year> <title> A neural network that embeds its own meta-levels. </title> <booktitle> In Proc. of the International Conference on Neural Networks '93, </booktitle> <address> San Francisco. </address> <publisher> IEEE. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1993b). </year> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1994). </year> <title> Discovering problem solutions with low Kolmogorov complexity and high generalization capability. </title> <type> Technical Report FKI-194-94, </type> <institution> Fakultat fur Informatik, Technische Uni-versitat Munchen. </institution>
Reference-contexts: 1 Introduction A recent debate in the machine-learning community highlighted a fact that appears discouraging at first glance: in general, generalization cannot be expected, inductive inference is impossible, and nothing can be learned. See, e.g., (Dietterich, 1989; Schaffer, 1993; Wolpert, 1993; Schmidhuber, 1994). Paraphrasing from a previous argument <ref> (Schmidhuber, 1994) </ref>: let the task be to learn some relation between finite bitstrings and finite bitstrings. Somehow, a training set is chosen. In almost all cases, the shortest algorithm computing a (non-overlapping) test set essentially has the same size as the whole test set. <p> Atypical real world / Previous learning algorithms. Apparently, however, generalization and inductive inference do make sense in the real world! One reason for this may be that the real world is run by a short algorithm. See <ref> (Schmidhuber, 1994) </ref>. Anyway, problems that humans consider to be typical are atypical when compared to the general set of all well-defined problems. <p> The language is assembler-like and has primitive instructions designed to exploit what conventional digital machines are good at: fast storage addressing, jumping, basic arithmetic operations, etc. The language is universal (i.e., Tur-ing machine equivalent). It is related to one previously published <ref> (Schmidhuber, 1994) </ref>, but there are significant differences and extensions. <p> Inserting prior bias / Efficiency considerations. Primitive instructions need not be low-level instructions like those in Table 1. They may correspond to complex submodules reflecting the user's prior knowledge. Informally, there is one general constraint to obey <ref> (Schmidhuber, 1994) </ref>: whatever is computable on the used hardware, should be computable just as efficiently (up to a small constant factor) by a program written in the programming language. For instance, on a typical serial digital machine we would like to have instructions exploiting fast storage addressing mechanisms. <p> Otherwise use the instruction executed during the most recent visit of the program cell. This leads to an explicit bias towards low algorithmic probability (Solomonoff, 1964), and has been done previously in <ref> (Schmidhuber, 1994) </ref>. Occasionally, this will lead to non-halting programs. For such cases, upper runtime bounds need to be introduced. In the spirit of the incremental self-improvement paradigm, such time bounds should be computed by the system itself (using appropriate special primitives). <p> Only one of them, namely V 0 , is correctly re-initialized with its own address after each payoff event. (4) There is no explicit a priori bias towards short programs, such as the one in <ref> (Schmidhuber, 1994) </ref> for a related task. (5) Finally, recall that the work area is never re-initialized after system birth. Hence, as mentioned above, it may be viewed as part of the environment of the program area. The environment is changing quite unpredictably, due to actions executed by the system itself. <p> For instance, to improve future performance, Solomonoff (1964, 1990) describes more traditional (as opposed to self-improving) methods for assigning probabilities to successful "subprograms". Alternatively, one of the actually implemented systems in <ref> (Schmidhuber, 1994) </ref> simply keeps successful code in its program area. This system was a conceptual starting point for the one in the current paper. With first attempts (in September 1994), the probability distributions underlying the Turing machine equivalent language required for universal search were modified heuristically.
Reference: <author> Schwefel, H. P. </author> <year> (1974). </year> <title> Numerische Optimierung von Computer-Modellen. Dissertation. </title> <note> Published 1977 by Birkhauser, Basel. </note>
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, XXVII:379-423. </journal>
Reference-contexts: Due to MinP being positive, there is always a non-vanishing halting probability. System life. At time step 0, storage is initialized with zeros. The probability distributions of all program cells are initialized with maximum entropy distributions <ref> (Shannon, 1948) </ref>. That is, all P ij values are initialized to the same value, so that there is no bias for a particular value in any cell. After initialization, runs are repeated over and over again until time T .
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference-contexts: Otherwise use the instruction executed during the most recent visit of the program cell. This leads to an explicit bias towards low algorithmic probability <ref> (Solomonoff, 1964) </ref>, and has been done previously in (Schmidhuber, 1994). Occasionally, this will lead to non-halting programs. For such cases, upper runtime bounds need to be introduced. In the spirit of the incremental self-improvement paradigm, such time bounds should be computed by the system itself (using appropriate special primitives).
Reference: <author> Solomonoff, R. </author> <year> (1990). </year> <title> A system for incremental learning based on algorithmic probability. </title> <editor> In Pednault, E. P. D., editor, </editor> <booktitle> The Theory and Application of Minimal-Length Encoding (Preprint of Symposium papers of AAAI 1990 Spring Symposium). </booktitle>
Reference-contexts: One nice thing about open-ended incremental self-improvement is that there is no significant theoretical limit to what the system may learn. This is, of course, due to the universal nature of the underlying programming language. Informally, a "revolution" corresponds to a self-improvement with high "conceptual jump size" <ref> (an expression coined by Solomonoff, 1990) </ref>, while "evolution" corresponds to a sequence of self improvements with low conceptual jump sizes. 3. Inserting prior bias.
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Integrated modeling and control based on reinforcement learning and dynamic programming. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 471-478. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The first task requires to compute regular integer strings. The second task is a maze task from <ref> (Sutton, 1991) </ref>. With both tasks, the system uses low-level problem-specific primitives in addition to the general primitives from Table 1. The primitives reflect the system's initial (weak) bias. Of course, different problem-specific primitives lead to different initial bias and performance. <p> Such stack entries may be interpreted as results of "adjusting the prior on the space of solution candidates" or "fine-tuning search space structure" or "learning to create directed mutations" or "learning how to learn". 5.2 A Navigation Task Task <ref> (following Sutton, 1991) </ref>. The external environment consists of a two-dimensional grid with 9 by 6 fields. F i;j denotes the field in the i-th row and the j-th column.
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <booktitle> In Machine Learning, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference-contexts: Otherwise, things like "learning by analogy", "learning by chunking", "incremental learning", "continual learning", "learning from invariances", "learning by knowledge transfer" etc. would not be possible, and experience with previous problems could not sensibly adjust the prior distribution of solution candidates in the search space for a new problem <ref> (shift of inductive bias, e.g. Utgoff, 1986) </ref>. In fact, all previous learning systems are implicitly or explicitly designed to exploit task-specific regularities of some kind or another.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College. </institution>
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On overfitting avoidance as bias. </title> <type> Technical Report SFI TR 93-03-016, </type> <institution> Santa Fe Institute, </institution> <address> NM 87501. </address> <month> 20 </month>
References-found: 32

