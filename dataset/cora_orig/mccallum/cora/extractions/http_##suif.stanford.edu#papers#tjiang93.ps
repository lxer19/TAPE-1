URL: http://suif.stanford.edu/papers/tjiang93.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: AUTOMATIC GENERATION OF DATA-FLOW ANALYZERS: A TOOL FOR BUILDING OPTIMIZERS  
Author: Steven Weng-Kiang Tjiang 
Degree: A DISSERTATION SUBMIITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY By  
Date: July 1993  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> A. V. Aho, M. Ganapathi, and S. W. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM Trans. on Programming Lang. and Systems 11, </journal> <month> 4 (October </month> <year> 1989), </year> <pages> 491-516. </pages>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig <ref> [1] </ref>, Graham-Glanville code generator generators [38, 45, 44], attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers.
Reference: 2. <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: An optimizer performs program transformations such as constant propagation and folding, forward and backward code motion, removing useless computations, and register allocation <ref> [2] </ref>. These transformations are applied generally to computations involving scalarsincluding addressing computations for array accesses. phases. The front-end of the compiler translates a program into an intermediate language (IL). The optimizer reads each procedure of the IL program. Each phase of the optimizer then analyzes or transforms the procedure. <p> The structure of these optimization is usually: a data-ow analysis step that collects information, followed by an IR graph traversal that checks at each node whether the collected information enables the optimization. For example, Aho, Sethi and Ullman <ref> [2] </ref>, and Chow [20] use such an organization to describe their optimizations. Sharlit extends Kildalls model with path simplification and with techniques for combining simpler data-ow analyzers into more complex ones. With the two extensions, compiler writers can make optimizers simpler, modular, and extensible. <p> Although there are elimination methods that work on all graphs, they are even more complex to implement. Existing compilers deal with irreducibilities either by node-splitting which enlarges the graph, or by isolating the irreducibility in a small subgraph to which iteration is applied <ref> [2] </ref>. <p> These two phases are custom built by the compiler writer. A second drawback is that compiler writers must understand the DFA at two levels before they can build prototype DFAs. The two levels appear in most published descriptions of DFAs <ref> [2, 20, 42] </ref>. Given that any DFA can be understood and solved knowing only the data-ow effect of each instruction, the two-level structure is an implementation detail and constitutes unnecessary concepts. A third drawback is that the two-level structure encourages non-modularity. <p> Evaluating the ow functions yields the solution to the DFA problem. TFPA uses the loop structure of a reducible graph to solve SSPP. It is similar to elimination algorithms such as Graham and Wegman [39], and Ullman [86]. Like their algorithm, TFPA finds the natural loops <ref> [2] </ref> of a program, then removes the back-edges of the loops from inner-most loop out by replacing edges. The key steps in the algorithm are: 1. Control-ow analysis (CFA): Determine the loop structure of the ow graph. <p> IR_ldc: loads a constant into the variable dst. IR_kill: modifies the variable dst. IR_initial: is the instruction type used to mark the source node of the graph. IR_others: doesnt affect the contents of any of the variables. We assume that value numbering <ref> [2, 20] </ref> has been performedpossibly in a previous phaseon the instructions. For each IR instruction, value numbering has set the field expr_no to identify the symbolic expression that is being computed by the instruction. <p> The optimizers must perform local analysis, the act of computing the data-ow effect of each basic block from its instructions. Sharlit does not explicitly represent basic blocks but achieves a similar reduction by using path simplification. or EBB <ref> [2] </ref>. An extended basic block is a tree of ow-graph nodes in which the root is the only node that may have more than one predecessor; all internal nodes of the EBB must have one predecessor. <p> These numbers are used to detect back-edges and to compute the reverse post-ordering. Every loop has a back-edge, an edge (u, h) in the ow graph in which the target h dominates the source u <ref> [2] </ref>. For any node u, its dominator must lie on the path from the root of the DFST to the node u. Therefore, a back-edge from u must connect u with one of the nodes on that path. <p> To eliminate this partial redundancy, insert tx+y just before the loop, and replace x+y inside the loop with t. The effect is to move the loop-invariant expression out of the loop. 6.2.1 Value Numbering The first step of code motion is value numbering <ref> [2] </ref>, a process that identifies movable expressions. Value numbering gives to each syntactically equivalent expressionmodulo commutativity and different node register numbersa unique expression number, which is used to index bit vectors of expressions. The bit vectors are the ow values of the EPR solvers. <p> In Chapter 7SUIF 111 addition, assigning a pseudo register to a machine register involves only substituting the pseudo register number with the assigned hard (physical) register number. Global data-ow analysis, in SUIF, concentrates on pseudo registers. Because pseudo registers are not aliased, gen/kill <ref> [2] </ref> information for them becomes very simple. When the analysis encounters an assignment of a pseudo register, only the data-ow information about that register has to be changed. The assignment cannot affect information collected about other registers. <p> Notice that we can represent the functions within an inlined function by replicating subtrees. function function f h g r y scope1 a g k inline_g a z Chapter 7SUIF 113 resource IDs of Coutant [23]. Alias analysis <ref> [2] </ref> tracks each variables reference set, the set of addresses it can point to. A variable is aliased if its address is in a reference set of a pointer used as a memory address of a load or store.
Reference: 3. <author> F. E. Allen. </author> <title> A basis for program optimization. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1971, </year> <pages> pp. 385-390. </pages>
Reference-contexts: However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods <ref> [3, 39, 76, 82, 86] </ref> use the structure of the CFG to solve DFA. They have two chief advantages. First, they can be faster than iteration.
Reference: 4. <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <type> Tech. Rept. Technical Report RC 13115, </type> <institution> IBM, </institution> <month> Sept, </month> <year> 1987. </year>
Reference-contexts: While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 5. <author> F. E. Allen, J. L. Carter, J. Fabri, J. Ferrante, W. H. Harrison, P. G. Loewner, and L. H. Trevillyan. </author> <title> The experimental compiling system. </title> <journal> IBM Journal of Research and Development 24, </journal> <month> 6 (November </month> <year> 1980). </year>
Reference-contexts: Loops can be permuted by updating the fields of the FOR constructs; loops can be distributed by surrounding new loop bodies with FOR constructs. After analysis and optimizations on high-level constructs, the expander phase lowers the high-level constructs to low-level SUIF. Following the approach of Harrison <ref> [5, 41] </ref> and of Auslander [11], our expander macro replaces the high-level constructs with lower level code using a code template. Each construct can be lowered independently. The expander makes no attempt to generate tight, fast code. Instead, later optimizations tune and customize the result to fit the surrounding code.
Reference: 6. <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of Operator Strength. </title> <editor> In S. S. Muchnick and N. D. Jones, Ed., </editor> <title> Program Flow Analysis: Theory and Applications, </title> <booktitle> Prentice-Hall 1981, </booktitle> <pages> pp. 79-101. </pages>
Reference-contexts: Strength reduction, in general, can handle more complex expressions. It can deal with affine expressions where the coefficients are variables that are loop constants. It can replace other expensive operationsexponentiations with multiplications, for example. For a comprehensive discussion of strength reduction, see Allen and Cocke <ref> [6] </ref>. The SUIF optimizer uses a restricted form of strength reduction that operates only on affine expressions whose coefficients are constants.
Reference: 7. <author> R. Allen and S. C. Johnson. </author> <title> Compiling C for vectorization, parallelization, and inline expansion. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988. </year>
Reference: 8. <author> B. Alpern, M. N. Wegman, and F. K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In 15th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1988. </year>
Reference-contexts: Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al <ref> [8] </ref>, Ballance et al [12], Cytron et al [25, 26], Ferrante et al [29], and Wegman and Zadeck [89]. <p> While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 9. <author> Z. Ammarguellat and W. L. Harrison III. </author> <title> Automatic recognition of induction variables and recurrence relations by abstract interpretation. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1990. </year>
Reference: 10. <author> P. Anklam, D. Cutler, R. Heinen Jr., and M. D. Maclaren. </author> <title> Engineering a compiler: VAX-11 Code Generation and Optimization. </title> <institution> Digital Equipment Corporation, </institution> <year> 1982. </year> <note> Bibliography 124 </note>
Reference: 11. <author> M. Auslander and M. Hopkins. </author> <title> An overview of the PL.8 compiler. </title> <booktitle> In SIGPLAN Conference on Compiler Construction, </booktitle> <year> 1982. </year>
Reference-contexts: After analysis and optimizations on high-level constructs, the expander phase lowers the high-level constructs to low-level SUIF. Following the approach of Harrison [5, 41] and of Auslander <ref> [11] </ref>, our expander macro replaces the high-level constructs with lower level code using a code template. Each construct can be lowered independently. The expander makes no attempt to generate tight, fast code. Instead, later optimizations tune and customize the result to fit the surrounding code.
Reference: 12. <author> R. A. Ballance, A. B. Maccabe, K. J. Ottenstein. </author> <title> The program dependence web: a representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1990, </year> <pages> pp. 257-271. </pages>
Reference-contexts: Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al [8], Ballance et al <ref> [12] </ref>, Cytron et al [25, 26], Ferrante et al [29], and Wegman and Zadeck [89]. <p> While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 13. <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: Parallelizing transformations, however, prefer the loop structure to have a unique induction variable, and an explicit representation of loop bounds. Some dependence tests (for example, extended GCD <ref> [13] </ref>) require that each index expression in a multidimensional array reference be identifiable. Linearizing these array references by combining the index expressions into one lengthy computation would limit our choice of dependence tests. SUIF has a core of small set of simple instructions with register operands.
Reference: 14. <author> M. E. Benitez and J. W. Davidson. </author> <title> A portable global optimizer and linker. </title> <booktitle> In SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988. </year>
Reference: 15. <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: effective performance evaluation of supercomputers. </title> <type> Tech. Rept. Technical Report (UIUCSRD) 827, </type> <institution> University of Illinois, Urbana-Champaign. Center for Supercomputing Research and Development, </institution> <year> 1989. </year>
Reference: 16. <author> B. M. Brosgol, J. M. Newcomer, D. A. Lamb, D. R. Levine, Mary S. Van Deusen, and William A. Wulf. TCOL_Ada: </author> <title> Revised Report on An Intermediate Representation for the Preliminary Ada Language. </title> <type> Tech. Rept. Technical Report CMU-CS-80-105, </type> <institution> Carnegie Mellon University, </institution> <month> February, </month> <year> 1980. </year>
Reference: 17. <author> M. Burke and B. G. Ryder. </author> <title> Incremental Iterative Data Flow Analysis Algorithms. </title> <type> Tech. Rept. Technical Report LCSR-TR-96, </type> <institution> Rutgers University, Laboratory for Computer Science Research, </institution> <month> August, </month> <year> 1987. </year>
Reference: 18. <author> M. D. Carroll and B. G. Ryder. </author> <title> Incremental data ow analysis via dominator and attribute updates. </title> <booktitle> In 15th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1988. </year>
Reference-contexts: This synergism between attribute grammars and data-ow analysis may allow us to apply efficient incremental update algorithms used in attribute evaluation [72] to data-ow analysis. This idea was first put forth by Carroll and Ryder <ref> [18, 74] </ref>. While attribute evaluation has been applied to solving DFA problems [72], that work has largely not taken advantage of the techniques developed in traditional data-ow analysis.
Reference: 19. <author> J. Choi, R. Cytron, and J. Ferrante. </author> <title> Automatic construction of sparse data ow evaluation graph. </title> <booktitle> In 18th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1991. </year>
Reference-contexts: The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al <ref> [19] </ref>, Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. <p> f*= f ' 3 f 3 f 4 f 3 ( ) *= Chapter 2Data-flow AnalysisA Basis for Optimizers 19 2.2.3 Sparse Data-ow Evaluation Graphs: Skipping Regions Sparse DFA problemsthose in which many of the ow functions are identity functions can be solved efficiently with sparse data-ow evaluation graphs (SDFEG) <ref> [19] </ref>, a generalization of the static single assignment representation (SSA) [25]. Sparse DFA problems occur, for example, when we wish to consider only a subset of a procedures variables: for example, when trying to find the reaching definitions for a single variable as in Section 2.1.1. Partitionable DFA problems [19] can <p> (SDFEG) <ref> [19] </ref>, a generalization of the static single assignment representation (SSA) [25]. Sparse DFA problems occur, for example, when we wish to consider only a subset of a procedures variables: for example, when trying to find the reaching definitions for a single variable as in Section 2.1.1. Partitionable DFA problems [19] can be partitioned into a set of sparse DFA problems: for example, SSA can be separated into a set of sparse DFA problems, one for each variable. <p> An incomplete set may mean that a path may failed to be sim plified because it contains a combination of ow functions which cannot be com bined by any rule. We demonstrate incomplete rule sets by showing how to compute SDFEG as described in Chapter 2 and in Choi <ref> [19] </ref>. Extensibility: Sharlit generates analyzers that are extensible. We demonstrate Sharlit with three examples, all solving the available expression problem (AVP)[2]. <p> Sharlit identifies such paths with the path simplification rules in Figure 4-17. Figure 4-18 compares the SDFEG computed using Sharlit and those computed by Chois <ref> [19] </ref>. Sharlits SDFEG differs in that we use nodes to represent some of the edges in Chois SDFEGWe indicate those nodes with dashed circles. <p> Therefore only one path function is required for each basic block. We show three control ow graphs: on the left, the original ow graph; in the middle, the graph after path simplification with eliminated nodes unshaded; on the right, the SDFEG obtained by applying Chois algorithm <ref> [19] </ref>. In all graphs the abbreviations Ip stands for the path function Id_path defined in Figure 4-17. All nodes but d 1 and d 2 are identity ow function. In the middle graph, the path simplifier has simplified the paths that are identity into Id_paths.
Reference: 20. <author> F. C. Chow. </author> <title> A portable Machine-Independent Global Optimizer --- Design and Measurement. </title> <type> Ph.D. </type> <institution> Th., Stanford University, </institution> <year> 1983. </year>
Reference-contexts: The structure of these optimization is usually: a data-ow analysis step that collects information, followed by an IR graph traversal that checks at each node whether the collected information enables the optimization. For example, Aho, Sethi and Ullman [2], and Chow <ref> [20] </ref> use such an organization to describe their optimizations. Sharlit extends Kildalls model with path simplification and with techniques for combining simpler data-ow analyzers into more complex ones. With the two extensions, compiler writers can make optimizers simpler, modular, and extensible. <p> Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. Several authorsChow <ref> [20] </ref>, Joshi and Dhamdhere [50, 51], and Morel and Renvoise [65]have used Kildalls model; they used data-ow analysis as the centerpiece of their optimizations. This dissertation presents a tool that supports Kildalls model directly. <p> These two phases are custom built by the compiler writer. A second drawback is that compiler writers must understand the DFA at two levels before they can build prototype DFAs. The two levels appear in most published descriptions of DFAs <ref> [2, 20, 42] </ref>. Given that any DFA can be understood and solved knowing only the data-ow effect of each instruction, the two-level structure is an implementation detail and constitutes unnecessary concepts. A third drawback is that the two-level structure encourages non-modularity. <p> The first depth-first traversal computes the reverse post-ordering of the graph, the back-edges, and the loop headers. A second traversal uses the loop headers and back-edge information to compute the lists of nodes described above. This is the same amount of work as CFA in existing optimizers <ref> [20, 24] </ref>. For more detail on this algorithm, see Section 1.1.2.1. 3.2.2 Simplify After control-ow analysis, the first step of path simplification computes path expressions from the innermost loops outward (See Procedure 3-2). Within each loop L, simplification visits nodes in topological order. <p> An expression is available at a node if every path from the source to has evaluated the expression and the value of the expression remains the same if reevaluated at . Determining available expressions is important in global common subexpression elimination and code motion <ref> [20, 50, 51, 65] </ref>. We built each example from the previous by adding new ow functions and new path simplification rules. The first example is a prototype solver that uses iteration to solve the AVP. <p> IR_ldc: loads a constant into the variable dst. IR_kill: modifies the variable dst. IR_initial: is the instruction type used to mark the source node of the graph. IR_others: doesnt affect the contents of any of the variables. We assume that value numbering <ref> [2, 20] </ref> has been performedpossibly in a previous phaseon the instructions. For each IR instruction, value numbering has set the field expr_no to identify the symbolic expression that is being computed by the instruction. <p> The phases implement the following analysis and transformations: Code Motion: The SUIF optimizer moves code by eliminating partial redundancies, a technique first proposed by Morel and Renvoise [65], and later refined by Joshi and Dhamdhere [50, 51], and by Chow <ref> [20] </ref>. Approaches based on eliminating partial redundancy offer an important advantage: they perform global common-subexpres-sion elimination, loop-invariant code motion, and strength reduction simultaneously. Code motion consists of phases that solve for available, partially available, and anticipated expressions [20]. <p> [65], and later refined by Joshi and Dhamdhere [50, 51], and by Chow <ref> [20] </ref>. Approaches based on eliminating partial redundancy offer an important advantage: they perform global common-subexpres-sion elimination, loop-invariant code motion, and strength reduction simultaneously. Code motion consists of phases that solve for available, partially available, and anticipated expressions [20]. This information serves as input to a placement phase, a data-ow analysis problem that solves for which expressions to delete, which expressions to insert, and where to insert them. We have implemented all the analy ses with Sharlit. <p> We have implemented all the analy ses with Sharlit. Register Allocation: We use the priority-based coloring approach first implemented by Chow <ref> [20] </ref>. This optimization consists of two data-ow analysis steps that com pute live ranges, followed by a coloring routine that builds the interference graph and colors it. The coloring routine does not fit the data-ow model, thus we did not write it with Sharlit. <p> We explain how we solve a DFA problem to determine what computations to move and where to compute them. The next section discusses the details of implementation of this DFA problem with Sharlit. Well omit the proofs because other authors have covered them <ref> [20, 65, 55] </ref>. Morel and Renvoise, the originators of PRE, defined a DFA problem called anticipated expressions that leads us to the following formulation of code motion. Suppose we have a node v which computes the expression e. <p> Load and store instructions are the only way to move values between registers and memory. Load constant instructions or ldcs (lines 16--18 in Figure 7-1) are the only way to put constants into registers. The SUIF type system is similar to the Ucode type system <ref> [20, 67] </ref>. SUIF types represent data types and sizes that are naturally representable on the target machine. In Figure 7-1, the types (s.32) and (a.32) stand for a 32-bit signed integer and a 32-bit address respectively. Even though SUIF types are machine dependent, past experiences [20, 67] have shown that we <p> to the Ucode type system <ref> [20, 67] </ref>. SUIF types represent data types and sizes that are naturally representable on the target machine. In Figure 7-1, the types (s.32) and (a.32) stand for a 32-bit signed integer and a 32-bit address respectively. Even though SUIF types are machine dependent, past experiences [20, 67] have shown that we can easily control this dependency by parameterizing the compiler. 7.1.1 Registers Normally, registers and low-level instructions like SUIFs are good for scalar optimizations, but undesirable for high-level transformations. The latter transformations prefer expression trees to be represented explicitly.
Reference: 21. <author> F. C. Chow. </author> <title> Register Allocation by Priority-based Coloring. </title> <booktitle> In SIGPLAN Conference on Compiler Construction, </booktitle> <year> 1984. </year>
Reference: 22. <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <month> January, </month> <year> 1977, </year> <pages> pp. 238-252. Bibliography 125 </pages>
Reference-contexts: Thus, data-ow analysis can only estimate the run-time behaviorin general, DFA can determine whether the assertions are true or false or undetermined. A general technique to do this estimation is abstract interpretation <ref> [22] </ref>, which maps a user program into equations over some abstract domain. Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman <p> A general technique to do this estimation is abstract interpretation <ref> [22] </ref>, which maps a user program into equations over some abstract domain. Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83].
Reference: 23. <author> D. S. Coutant. </author> <title> Retargetable high-level alias analysis. </title> <booktitle> In 13th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1986. </year>
Reference-contexts: Notice that we can represent the functions within an inlined function by replicating subtrees. function function f h g r y scope1 a g k inline_g a z Chapter 7SUIF 113 resource IDs of Coutant <ref> [23] </ref>. Alias analysis [2] tracks each variables reference set, the set of addresses it can point to. A variable is aliased if its address is in a reference set of a pointer used as a memory address of a load or store.
Reference: 24. <author> D. S. Coutant, C. L. Hammond, and J. W. Kelly. </author> <title> Compilers for the new generation of Hewlett-Packard computers. </title> <journal> Hewlett-Packard Journal 37, </journal> <month> 1 (January </month> <year> 1986). </year>
Reference-contexts: The first depth-first traversal computes the reverse post-ordering of the graph, the back-edges, and the loop headers. A second traversal uses the loop headers and back-edge information to compute the lists of nodes described above. This is the same amount of work as CFA in existing optimizers <ref> [20, 24] </ref>. For more detail on this algorithm, see Section 1.1.2.1. 3.2.2 Simplify After control-ow analysis, the first step of path simplification computes path expressions from the innermost loops outward (See Procedure 3-2). Within each loop L, simplification visits nodes in topological order.
Reference: 25. <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <journal> ACM Trans. on Programming Lang. and Systems 13, </journal> <month> 4 (October </month> <year> 1991), </year> <pages> 451-490. </pages>
Reference-contexts: ( ) *= Chapter 2Data-flow AnalysisA Basis for Optimizers 19 2.2.3 Sparse Data-ow Evaluation Graphs: Skipping Regions Sparse DFA problemsthose in which many of the ow functions are identity functions can be solved efficiently with sparse data-ow evaluation graphs (SDFEG) [19], a generalization of the static single assignment representation (SSA) <ref> [25] </ref>. Sparse DFA problems occur, for example, when we wish to consider only a subset of a procedures variables: for example, when trying to find the reaching definitions for a single variable as in Section 2.1.1. <p> Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al [8], Ballance et al [12], Cytron et al <ref> [25, 26] </ref>, Ferrante et al [29], and Wegman and Zadeck [89]. Solving a DFA problem using a SDFEG is efficient because a SDFEG contains a subset of nodes in the original ow graph, those whose ow functions are not identity, and those at which ow valuesgenerated at distinct nodesmeet.
Reference: 26. <author> R. Cytron, A. Lowry, and K. Zadeck. </author> <title> Code motion of control structures in high-level languages. </title> <booktitle> In ACM Symposium on Principles of Compiler Construction, </booktitle> <year> 1986. </year>
Reference-contexts: Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al [8], Ballance et al [12], Cytron et al <ref> [25, 26] </ref>, Ferrante et al [29], and Wegman and Zadeck [89]. Solving a DFA problem using a SDFEG is efficient because a SDFEG contains a subset of nodes in the original ow graph, those whose ow functions are not identity, and those at which ow valuesgenerated at distinct nodesmeet. <p> While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 27. <author> J. W. Davidson and C. W. Fraser. </author> <title> Code selection through object code optimization. </title> <journal> ACM Trans. on Programming Lang. and Systems 6, </journal> <month> 4 (October </month> <year> 1984), </year> <pages> 505-526. </pages>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation <ref> [27] </ref>, twig [1], Graham-Glanville code generator generators [38, 45, 44], attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers.
Reference: 28. <author> C. D. Farnum. </author> <title> Pattern-Based Languages for Prototyping of Compiler Optimizers. </title> <type> Ph.D. </type> <institution> Th., University of California, Berkeley, </institution> <year> 1990. </year>
Reference: 29. <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Trans. on Programming Lang. and Systems 9, </journal> <month> 3 (July </month> <year> 1987), </year> <pages> 319-349. </pages>
Reference-contexts: Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al [8], Ballance et al [12], Cytron et al [25, 26], Ferrante et al <ref> [29] </ref>, and Wegman and Zadeck [89]. Solving a DFA problem using a SDFEG is efficient because a SDFEG contains a subset of nodes in the original ow graph, those whose ow functions are not identity, and those at which ow valuesgenerated at distinct nodesmeet. <p> While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 30. <author> A. Fong, J. Kam, and J. Ullman. </author> <title> Application of lattice algebra to loop optimization. </title> <booktitle> In 2nd ACM Symposium on Principles of Programming Languages, </booktitle> <month> January, </month> <year> 1975, </year> <pages> pp. 1-9. </pages>
Reference: 31. <author> C. W. Fraser and A. L. Wendt. </author> <title> Integrating code generation and optimization. </title> <booktitle> In SIG-PLAN Conference on Compiler Construction, </booktitle> <year> 1986. </year>
Reference: 32. <author> M. Ganapathi. </author> <title> Retargetable Code Generation and Optimization using Attribute Grammars. </title> <type> Ph.D. </type> <institution> Th., University of Wisconsin-Madison, </institution> <year> 1980. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators [38, 45, 44], attributed parsing <ref> [32, 33] </ref>. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. With the tools, writing these other compiler parts has become much easier.
Reference: 33. <author> M. Ganapathi and C. N. Fischer. </author> <title> Description-driven code generation using attributed grammars. </title> <booktitle> In 9th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1982. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators [38, 45, 44], attributed parsing <ref> [32, 33] </ref>. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. With the tools, writing these other compiler parts has become much easier.
Reference: 34. <author> M. Ganapathi and C. N. Fischer. </author> <title> Attributed linear intermediate representations for retargetable code generators. </title> <journal> Software --- Practice and Experience 14, </journal> <month> 4 (April </month> <year> 1984), </year> <pages> 347-364. </pages>
Reference: 35. <author> M. Ganapathi, C. N. Fischer, and J. L. Hennessy. </author> <title> Retargetable compiler code generation. </title> <journal> ACM Computing Surveys 14, </journal> <volume> 4 (1982), </volume> <pages> 573-592. </pages>
Reference-contexts: Besides being easy to build trees, bounding the lifetimes of node registers is useful to those parts of the compiler that dont use trees. For example, node registers provide clues to interpretive code generators <ref> [35] </ref>. These generators associate information, machine resources, and machine instructions with each encountered node register as it scans through a SUIF program. When the last and only use of a node register is encountered, they can forget the associations.
Reference: 36. <author> D. Gay. </author> <title> Private Communication. </title> <journal> ATT Software. </journal>
Reference-contexts: Because SUIF has only one representation, we are free in ordering the phases with respect to each other. We have found this structure to be invaluable for experimentation, permitting several implementation projects to coexist. SUIF compiler. The first phase f2c, a FORTRAN-to-C converter <ref> [36] </ref> translates the program to an equivalent C program. Then the program FE, a front-end based on the portable C compiler, translates the C program to a high-SUIF program. During parallelization, the program representations have the features of high-SUIF. The high-SUIF program is passed through a sequence of phases.
Reference: 37. <author> P. Gerring, P. Nye, A. Rodriquez, and A. Samuel. </author> <title> A Universal P-Code for the S-1 Project. </title> <type> Tech. Rept. CSL Technical Note 159, </type> <institution> Stanford University, </institution> <month> Aug, </month> <year> 1979. </year> <note> Bibliography 126 </note>
Reference-contexts: Many compilers strictly separate the two categories of transformations by using two representations: a low-level one such as Ucode <ref> [37] </ref> and a high-level one such as abstract syntax trees. When there are two representations, many of the functions provided by scalar phases must be re-implemented for each representation, and high-level analyses can no longer directly communicate with low-level transformations.
Reference: 38. <author> R. S. </author> <title> Glanville. A Machine-Independent Algorithm for Code Generation and Its Use in Retargetable Compilers. </title> <type> Ph.D. </type> <institution> Th., University of California, Berkeley, </institution> <year> 1978. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators <ref> [38, 45, 44] </ref>, attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. With the tools, writing these other compiler parts has become much easier.
Reference: 39. <author> S. Graham and M. Wegman. </author> <title> Fast and usually linear algorithm for global ow analysis. </title> <journal> J. ACM 23, </journal> <month> 1 (January </month> <year> 1976), </year> <pages> 172-102. </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman <ref> [39] </ref>, Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. <p> properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman <ref> [39] </ref>, Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. <p> However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods <ref> [3, 39, 76, 82, 86] </ref> use the structure of the CFG to solve DFA. They have two chief advantages. First, they can be faster than iteration. <p> Evaluating the ow functions yields the solution to the DFA problem. TFPA uses the loop structure of a reducible graph to solve SSPP. It is similar to elimination algorithms such as Graham and Wegman <ref> [39] </ref>, and Ullman [86]. Like their algorithm, TFPA finds the natural loops [2] of a program, then removes the back-edges of the loops from inner-most loop out by replacing edges. The key steps in the algorithm are: 1. Control-ow analysis (CFA): Determine the loop structure of the ow graph. <p> A ag argument dir tells FG::headers and FG::body which set to return. The default CFA algorithm provided by FG is based on Tarjans algorithm for testing reducibility of a ow graph [81] and on a closely related algorithm by Graham and Wegman <ref> [39] </ref>. In addition to finding loop headers, Tarjans algorithm computes an ordering of the nodes with which to apply Ullmans T 1 and T 2 transformations [86] on a graph. The appli Function Description FG::next_nodes (u) Returns a vector of nodes that follow the node u. <p> Procedure 5-1 shows the main analysis function FG::cfa, which is called from FG::analyze. Within FG::cfa are three steps. This following will outline the steps in the context of reducible ow graphs. Readers interested in other aspect of control-ow analysis should refer to the literature <ref> [39, 59, 78, 80, 81] </ref>. To see how the path simplifier operates on irreducibilities see Section 3.2.2.
Reference: 40. <author> D. R. Grundman. </author> <title> Graph Transformations and Program Flow Analysis. </title> <type> Ph.D. </type> <institution> Th., University of California, Berkeley, </institution> <year> 1990. </year>
Reference: 41. <author> W. Harrison. </author> <title> A new strategy for code generation --- the general purpose optimizing compiler. </title> <booktitle> In ACM Fourth Symposium on Principles of Programming Languages, </booktitle> <year> 1977, </year> <pages> pp. 29-37. </pages>
Reference-contexts: Loops can be permuted by updating the fields of the FOR constructs; loops can be distributed by surrounding new loop bodies with FOR constructs. After analysis and optimizations on high-level constructs, the expander phase lowers the high-level constructs to low-level SUIF. Following the approach of Harrison <ref> [5, 41] </ref> and of Auslander [11], our expander macro replaces the high-level constructs with lower level code using a code template. Each construct can be lowered independently. The expander makes no attempt to generate tight, fast code. Instead, later optimizations tune and customize the result to fit the surrounding code.
Reference: 42. <author> M. S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> Elsevier North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht <ref> [42, 43] </ref>, Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. <p> These two phases are custom built by the compiler writer. A second drawback is that compiler writers must understand the DFA at two levels before they can build prototype DFAs. The two levels appear in most published descriptions of DFAs <ref> [2, 20, 42] </ref>. Given that any DFA can be understood and solved knowing only the data-ow effect of each instruction, the two-level structure is an implementation detail and constitutes unnecessary concepts. A third drawback is that the two-level structure encourages non-modularity.
Reference: 43. <author> M. S. Hecht and J. D. Ullman. </author> <title> A simple algorithm for global data-ow analysis problems. </title> <journal> SIAM J. Comput. </journal> <volume> 4, </volume> <month> 4 (December </month> <year> 1975), </year> <pages> 519-532. </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht <ref> [42, 43] </ref>, Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. <p> Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman <ref> [43] </ref>, Horwitz et al [46], Tarjan [82], and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. <p> Although iteration requires no CFG analysis, examining the graph to create an order in which the equations are evaluated will make iteration converge faster. Hecht and Ullman <ref> [43] </ref> have shown convergence is asymptotically fastestrequiring the fewest equation evaluationswhen the equation order corresponds to a topological sort of the graph ignoring the back-edges of loops. <p> If the ow functions are simple as in most bit-vector DFA problems, then equation evaluation is fast when compared with control-ow analysis. In that case, Hecht and Ullman has shown that elimination has roughly the same performance as iteration <ref> [43] </ref>. On the other hand, if the ow functions are more complex as in DFA problems involving symbolic analysis then the advantage swings in favor of elimination methods (assuming that finding the data-ow effect of a loop can be done quickly). Second and more importantly, elimination methods discover program structure. <p> Ideally these libraries would be independent of the internal program representation. 8.1.2 Efficient Iteration The iteration algorithm used in Sharlit evaluates every node even when the ow values that enter a node have not changed. This is the technique used by Hecht and Ullman <ref> [43] </ref>. A more efficient algorithm, proposed by Horwitz, Demers and Teitelbaum [46], would use the header foresthence ow graph regionsand change status of nodes to reduce the number of node evaluations.
Reference: 44. <author> R. R. Henry. </author> <title> Graham-Glanville code generators. </title> <type> Ph.D. </type> <institution> Th., University of California, Berkeley, </institution> <year> 1984. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators <ref> [38, 45, 44] </ref>, attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. With the tools, writing these other compiler parts has become much easier.
Reference: 45. <author> R. R. Henry. </author> <title> The CODEGEN user's manual. </title> <type> Tech. Rept. Technical Report 87-08-04, </type> <institution> University of Washington, </institution> <year> 1987. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators <ref> [38, 45, 44] </ref>, attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. With the tools, writing these other compiler parts has become much easier.
Reference: 46. <author> S. Horwitz, A. Demers, and T. Teitelbaum. </author> <title> An efficient general iterative algorithm for dataow analysis. </title> <journal> Acta Inf. </journal> <volume> 24 (1987), </volume> <pages> 679-694. </pages>
Reference-contexts: Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al <ref> [46] </ref>, Tarjan [82], and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. <p> In practice, we can decrease the number of equation evaluations by using the techniques of Horwitz, Demers, and Teitelbaum <ref> [46] </ref>. However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods [3, 39, 76, 82, 86] use the structure of the CFG to solve DFA. <p> This is the technique used by Hecht and Ullman [43]. A more efficient algorithm, proposed by Horwitz, Demers and Teitelbaum <ref> [46] </ref>, would use the header foresthence ow graph regionsand change status of nodes to reduce the number of node evaluations. While the worst case running-time for both algorithms are identical, it is expected that Horwitzs algorithm will have better average running-time.
Reference: 47. <author> S. C. Johnson. </author> <title> A Portable compiler: theory and practice. </title> <booktitle> In Fifth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1978, </year> <pages> pp. 97-104. </pages>
Reference: 48. <author> S. C. Johnson. </author> <title> YACC--yet another compiler compiler. </title> <type> Tech. </type> <institution> Rept. CSTR-32, Bell Laboratories, </institution> <year> 1975. </year>
Reference-contexts: This situation stands in stark contrast with that of other parts of the compiler where years of experience Chapter 1Introduction 3 have culminated in models and structuresembodied in specialized toolswith which to construct these other parts. For example, Lexical analysis generators: LEX. Parser generators: YACC <ref> [48] </ref>. Front-end analyses: Attributed grammars [72]. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators [38, 45, 44], attributed parsing [32, 33]. <p> It is the simplest kind of specification possible, and contains the minimal ingredients necessary for a working DFA solver. The braces in the specification delimit C++ code, which Sharlit inserts into a C++ code framework to build a solver, just as YACC <ref> [48] </ref> inserts user-supplied C code into a C code framework to build a parser. Within the C++ parts, variables that begin with an underscore class IR_instruction - ... public: int kind; IR_variable dst,src1,src2; Expression expr_no; ... Chapter 4Writing Data-flow Analyzers with Sharlit 44 are special.
Reference: 49. <author> M. S. Johnson and T. C. Miller. </author> <title> Effectiveness of a machine-Level, global optimizer. </title> <booktitle> In SIGPLAN Conference on Compiler Construction, </booktitle> <year> 1986. </year>
Reference: 50. <author> S. M. Joshi and D. M. Dhamdhere. </author> <title> A composite hoisting-strength reduction transformation for global program optimization: part I. </title> <journal> Intern. Journal of Computer Math. </journal> <volume> 11 (1982), </volume> <pages> 21-41. </pages>
Reference-contexts: By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. Several authorsChow [20], Joshi and Dhamdhere <ref> [50, 51] </ref>, and Morel and Renvoise [65]have used Kildalls model; they used data-ow analysis as the centerpiece of their optimizations. This dissertation presents a tool that supports Kildalls model directly. <p> An expression is available at a node if every path from the source to has evaluated the expression and the value of the expression remains the same if reevaluated at . Determining available expressions is important in global common subexpression elimination and code motion <ref> [20, 50, 51, 65] </ref>. We built each example from the previous by adding new ow functions and new path simplification rules. The first example is a prototype solver that uses iteration to solve the AVP. <p> Then a sequence of phases is applied to the CFG. The phases implement the following analysis and transformations: Code Motion: The SUIF optimizer moves code by eliminating partial redundancies, a technique first proposed by Morel and Renvoise [65], and later refined by Joshi and Dhamdhere <ref> [50, 51] </ref>, and by Chow [20]. Approaches based on eliminating partial redundancy offer an important advantage: they perform global common-subexpres-sion elimination, loop-invariant code motion, and strength reduction simultaneously. Code motion consists of phases that solve for available, partially available, and anticipated expressions [20]. <p> Such address computations are usually affine expressions. The reducer is closely related to the EPR code motion algorithm discussed above. Joshi and Dhamdhere <ref> [50, 51] </ref> first pointed out that EPR can be used to perform strength reduction, if candidatesexpressions eligible for strength reductionare defined and increments of induction variables are interpreted as having no effect on candidates. This is illustrated in Figure 6-4 in which a loop has a candidate, 4*i.
Reference: 51. <author> S. M. Joshi and D. M. Dhamdhere. </author> <title> A composite hoisting-strength reduction transformation for global program optimization: part II. </title> <journal> Intern. Journal of Computer Math. </journal> <volume> 11 (1982), </volume> <pages> 111-126. </pages>
Reference-contexts: By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. Several authorsChow [20], Joshi and Dhamdhere <ref> [50, 51] </ref>, and Morel and Renvoise [65]have used Kildalls model; they used data-ow analysis as the centerpiece of their optimizations. This dissertation presents a tool that supports Kildalls model directly. <p> An expression is available at a node if every path from the source to has evaluated the expression and the value of the expression remains the same if reevaluated at . Determining available expressions is important in global common subexpression elimination and code motion <ref> [20, 50, 51, 65] </ref>. We built each example from the previous by adding new ow functions and new path simplification rules. The first example is a prototype solver that uses iteration to solve the AVP. <p> Then a sequence of phases is applied to the CFG. The phases implement the following analysis and transformations: Code Motion: The SUIF optimizer moves code by eliminating partial redundancies, a technique first proposed by Morel and Renvoise [65], and later refined by Joshi and Dhamdhere <ref> [50, 51] </ref>, and by Chow [20]. Approaches based on eliminating partial redundancy offer an important advantage: they perform global common-subexpres-sion elimination, loop-invariant code motion, and strength reduction simultaneously. Code motion consists of phases that solve for available, partially available, and anticipated expressions [20]. <p> Such address computations are usually affine expressions. The reducer is closely related to the EPR code motion algorithm discussed above. Joshi and Dhamdhere <ref> [50, 51] </ref> first pointed out that EPR can be used to perform strength reduction, if candidatesexpressions eligible for strength reductionare defined and increments of induction variables are interpreted as having no effect on candidates. This is illustrated in Figure 6-4 in which a loop has a candidate, 4*i.
Reference: 52. <author> J. B. Kam and J. D. Ullman. </author> <title> Global data ow analysis and iterative algorithms. </title> <journal> J. ACM 23, </journal> <month> 1 (January </month> <year> 1976), </year> <pages> 158-171. Bibliography 127 </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman <ref> [52, 53] </ref>, Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86].
Reference: 53. <author> J. B. Kam and J. D. Ullman. </author> <title> Monotone data ow analysis frameworks. </title> <journal> Acta Inf. </journal> <volume> 7 (1977), </volume> <pages> 305-317. </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman <ref> [52, 53] </ref>, Marlowe and Ryder [62], Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86].
Reference: 54. <author> G. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In ACM Symposium on Principle of Programming Languages, </booktitle> <year> 1973, </year> <pages> pp. 194-206. </pages>
Reference-contexts: This is an organization first proposed by Kildall <ref> [54] </ref>. This organization is general in that it can implement most classical optimizations. The structure of these optimization is usually: a data-ow analysis step that collects information, followed by an IR graph traversal that checks at each node whether the collected information enables the optimization. <p> Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. Kildall <ref> [54] </ref> was the first to recognize that program analysis can serve as a basis for implementing optimizations. By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. <p> However, some DFA problems have ow functions that resemble an interpretation of a programs instructions. One such problem is Kildals constant propagation problem <ref> [54] </ref>. This problem tracks the values of the variables and performs computations with the tracked values. In this DFA a ow value maps each program variable to a value. <p> Reaching Definitions: The optimizer applies the standard bit-vector based reaching definitions problem and annotates each use of a variable with the set of definitions that reach it. Some SUIF parallelization phases use this information. Constant propagation and folding: This phase uses Kildalls technique <ref> [54] </ref> which determines, for every instruction in the ow graph, a set of variables with constant values. The phase replaces each instruction that computes a constant value with an instruction that loads the constant. <p> The second is how we can integrate scalar optimizationsthat require a low-level program representationwith other code transformations that require a high-level program representation. The optimizer architecture is based on Kildalls original proposition of using data-ow analysis as a basis for describing optimizations <ref> [54] </ref>. To simplify the construction of data-ow analyzers and to encourage the use of Kildalls model, a tool called Sharlit has been provided.
Reference: 55. <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy Code Motion. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992, </year> <pages> pp. 224-234. </pages>
Reference-contexts: We explain how we solve a DFA problem to determine what computations to move and where to compute them. The next section discusses the details of implementation of this DFA problem with Sharlit. Well omit the proofs because other authors have covered them <ref> [20, 65, 55] </ref>. Morel and Renvoise, the originators of PRE, defined a DFA problem called anticipated expressions that leads us to the following formulation of code motion. Suppose we have a node v which computes the expression e.
Reference: 56. <author> W. R. Lalonde and J. </author> <title> des Rivieres. A exible compiler structure that allows dynamic phase ordering. </title> <booktitle> In SIGPLAN Conference on Compiler Construction, </booktitle> <year> 1982. </year>
Reference: 57. <author> M. S. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <type> Ph.D. </type> <institution> Th., Carnegie-Mellon University, </institution> <month> May </month> <year> 1987. </year> <note> Also available as CMU-CS-87-187. </note>
Reference: 58. <author> D. A. Lamb. </author> <title> Sharing intermediate representations: The interface description language. </title> <type> Ph.D. </type> <institution> Th., Carnegie-Mellon University, </institution> <month> May </month> <year> 1983. </year> <note> Also available as CMU-CS-83-129. </note>
Reference: 59. <author> T. Lengauer and R. E. Tarjan. </author> <title> A fast algorithm for finding dominators in a ow-graph. </title> <journal> ACM Trans. on Programming Lang. and Systems 1, </journal> <month> 1 (July </month> <year> 1979), </year> <pages> 121-141. </pages>
Reference-contexts: Procedure 5-1 shows the main analysis function FG::cfa, which is called from FG::analyze. Within FG::cfa are three steps. This following will outline the steps in the context of reducible ow graphs. Readers interested in other aspect of control-ow analysis should refer to the literature <ref> [39, 59, 78, 80, 81] </ref>. To see how the path simplifier operates on irreducibilities see Section 3.2.2.
Reference: 60. <author> M. E. Lesk. </author> <title> LEX--a lexical analyzer generator. </title> <type> Tech. </type> <institution> Rept. CSTR-39, Bell Laboratories, </institution> <year> 1975. </year>
Reference: 61. <author> S. MacLane and G. Birkhoff. </author> <title> Algebra. </title> <publisher> MacMillan Company, </publisher> <year> 1967. </year>
Reference-contexts: ,( ) S, , I u 1 ( ) x ^,( ) for all variables x -= x c,( ) I i c ^ x u i I u i ( ) O u i ( ) Chapter 2Data-flow AnalysisA Basis for Optimizers 16 properties: Flow values form a semi-lattice <ref> [61] </ref> with finite descending-chain condition (FDCC): The meet operator of the DFA must satisfy the relationship where is the partial order of the semi-lattice. The finite descending-chain condition states that every non-increasing sequence of ow values must be finite.
Reference: 62. <author> T. J. Marlowe and B. G. Ryder. </author> <title> Properties of Data Flow Frameworks: A Unified Model. </title> <type> Tech. Rept. Technical Report LCSR-TR-103, </type> <institution> Laboratory for Computer Science Research, Rutgers University, </institution> <month> April, </month> <year> 1988. </year>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder <ref> [62] </ref>, Ryder [76], and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. <p> The finite descending-chain condition states that every non-increasing sequence of ow values must be finite. Intuitively the partial order of the lattice compares the relative information content of two ow values. If then has more information than <ref> [62] </ref>. Monotonicity: The ow functions satisfy the relationship where and are ow values. Monotonicity ensures that during iteration and are assigned progressively smaller ow values.
Reference: 63. <author> H. Z. Marshall. </author> <title> The linear graph package, a compiler building environment. </title> <booktitle> In SIG-PLAN Conference on Compiler Construction, </booktitle> <year> 1982. </year>
Reference: 64. <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence testing. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June, </month> <year> 1991. </year>
Reference-contexts: A subsequent pass of dead-store elimination removes the increments to K0 and K1, made redundant by the substitutions. On the face of it, IVR could slow down programs because it replaces simple references with expressions that may include multiplications. But IVR improves the effectiveness of dependency analysis <ref> [64] </ref>, enhancing the effectiveness of parallelization and other optimizations that relies on dependence information. Moreover, IVR usually runs before parallel-ization. After parallelization, we run scalar optimizations to remove introduced multiplications. Our implementation of IVR consists of two solvers. The first solver has only a path simplification step. <p> The scalar optimizations have already been described in Sections 6.1. 7.3.1 High-level Transformations and Analysis Array and loop-level analyses and transformations require data-dependence information. By arranging dependence tests in increasing strengths and costs, and using memoization, the SUIF compiler can generate exact dependence information efficiently <ref> [64] </ref>. This dependence information can be used in low-level passes such as a superscalar/VLIW instruction scheduler, and in high-level passes that privatize scalars, distribute and transform loops. Loop optimizations in the SUIF compiler are based on unified unimodular transforma-tionsinterchange, reversal, skewingtogether with a new transformation theory [93, 94]. <p> We can compile and validate nine of the Perfect club benchmarks with full scalar optimizations. The high-level phases are functioning. Data-dependence tests will run on all the Perfect club <ref> [64] </ref>. Linear loop transformations can auto Chapter 7SUIF 118 matically block matrix algorithms such as QR decomposition and LU decomposition with out pivoting, whose performance we have measured on an SGI multiprocessor [94]. Chapter 8Conclusion 119 Conclusion This dissertation has made two contributions to the building optimizing compilers. <p> Using the ability to share phases inherent in SUIF, we have, in a reasonable time frame obtained interesting results in individual topics such as data dependence analysis <ref> [64] </ref>, par-allelization [68] and loop transformations [92, 93]. There are still many subjects to be Chapter 8Conclusion 121 explored with the SUIF platform.
Reference: 65. <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Comm. ACM 22, </journal> <month> 2 (February </month> <year> 1979), </year> <pages> 96-103. </pages>
Reference-contexts: An expression is available at a node if every path from the source to has evaluated the expression and the value of the expression remains the same if reevaluated at . Determining available expressions is important in global common subexpression elimination and code motion <ref> [20, 50, 51, 65] </ref>. We built each example from the previous by adding new ow functions and new path simplification rules. The first example is a prototype solver that uses iteration to solve the AVP. <p> Then a sequence of phases is applied to the CFG. The phases implement the following analysis and transformations: Code Motion: The SUIF optimizer moves code by eliminating partial redundancies, a technique first proposed by Morel and Renvoise <ref> [65] </ref>, and later refined by Joshi and Dhamdhere [50, 51], and by Chow [20]. Approaches based on eliminating partial redundancy offer an important advantage: they perform global common-subexpres-sion elimination, loop-invariant code motion, and strength reduction simultaneously. <p> We explain how we solve a DFA problem to determine what computations to move and where to compute them. The next section discusses the details of implementation of this DFA problem with Sharlit. Well omit the proofs because other authors have covered them <ref> [20, 65, 55] </ref>. Morel and Renvoise, the originators of PRE, defined a DFA problem called anticipated expressions that leads us to the following formulation of code motion. Suppose we have a node v which computes the expression e.
Reference: 66. <author> K. J. Ottenstein. </author> <title> Data-Flow Graphs as An Intermediate Program Form. </title> <type> Ph.D. </type> <institution> Th., Purdue University, </institution> <year> 1978. </year>
Reference: 67. <author> D. L. Perkins and R. L. </author> <title> Sites. Machine-independent pascal code optimization. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <year> 1979, </year> <pages> pp. 201-207. Bibliography 128 </pages>
Reference-contexts: Load and store instructions are the only way to move values between registers and memory. Load constant instructions or ldcs (lines 16--18 in Figure 7-1) are the only way to put constants into registers. The SUIF type system is similar to the Ucode type system <ref> [20, 67] </ref>. SUIF types represent data types and sizes that are naturally representable on the target machine. In Figure 7-1, the types (s.32) and (a.32) stand for a 32-bit signed integer and a 32-bit address respectively. Even though SUIF types are machine dependent, past experiences [20, 67] have shown that we <p> to the Ucode type system <ref> [20, 67] </ref>. SUIF types represent data types and sizes that are naturally representable on the target machine. In Figure 7-1, the types (s.32) and (a.32) stand for a 32-bit signed integer and a 32-bit address respectively. Even though SUIF types are machine dependent, past experiences [20, 67] have shown that we can easily control this dependency by parameterizing the compiler. 7.1.1 Registers Normally, registers and low-level instructions like SUIFs are good for scalar optimizations, but undesirable for high-level transformations. The latter transformations prefer expression trees to be represented explicitly.
Reference: 68. <author> K. L. Pieper. </author> <title> Parallelizing Compilers: Implementation and Effectiveness. </title> <type> Ph.D. </type> <institution> Th., Stanford University, </institution> <year> 1993. </year> <note> In preparation. </note>
Reference-contexts: Using the ability to share phases inherent in SUIF, we have, in a reasonable time frame obtained interesting results in individual topics such as data dependence analysis [64], par-allelization <ref> [68] </ref> and loop transformations [92, 93]. There are still many subjects to be Chapter 8Conclusion 121 explored with the SUIF platform.
Reference: 69. <author> J. R. Reif. </author> <title> Code motion. </title> <journal> SIAM J. Comput. </journal> <volume> 9, </volume> <month> 2 (May </month> <year> 1980). </year>
Reference: 70. <author> J. H. Reif and H. R. Lewis. </author> <title> Symoblic evaluation and the global value graph. </title> <booktitle> In 4th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January, </month> <year> 1977, </year> <pages> pp. 104-118. </pages>
Reference: 71. <author> J. H. Reif and H. R. Lewis. </author> <title> Efficient Symbolic Analysis of Programs. </title> <type> Tech. Rept. Technical Report TR-37-82, </type> <institution> Harvard University, Aiken Computation Laboratory, </institution> <year> 1982. </year>
Reference: 72. <author> T. W. Reps. </author> <title> Generating Language-Based Environments. </title> <publisher> The M.I.T. Press, </publisher> <year> 1984. </year>
Reference-contexts: For example, Lexical analysis generators: LEX. Parser generators: YACC [48]. Front-end analyses: Attributed grammars <ref> [72] </ref>. Instruction selection: Abstract Interpretation [27], twig [1], Graham-Glanville code generator generators [38, 45, 44], attributed parsing [32, 33]. These tools provide a layer of abstractions to hide implementation details from the compiler writer, and to provide powerful algorithms that can be tailored to the needs of compiler writers. <p> This leads us to the natural interpretation of viewing the data-ow analysis as attribute evaluation in an attributed parse tree. This synergism between attribute grammars and data-ow analysis may allow us to apply efficient incremental update algorithms used in attribute evaluation <ref> [72] </ref> to data-ow analysis. This idea was first put forth by Carroll and Ryder [18, 74]. While attribute evaluation has been applied to solving DFA problems [72], that work has largely not taken advantage of the techniques developed in traditional data-ow analysis. <p> This synergism between attribute grammars and data-ow analysis may allow us to apply efficient incremental update algorithms used in attribute evaluation <ref> [72] </ref> to data-ow analysis. This idea was first put forth by Carroll and Ryder [18, 74]. While attribute evaluation has been applied to solving DFA problems [72], that work has largely not taken advantage of the techniques developed in traditional data-ow analysis.
Reference: 73. <author> B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Global value number graphs and redundant computations. </title> <booktitle> In 15th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1988. </year>
Reference-contexts: While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 74. <author> B. G. Ryder. </author> <title> Incremental data ow analysis. </title> <booktitle> In 10th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January, </month> <year> 1982, </year> <pages> pp. 167-176. </pages>
Reference-contexts: This synergism between attribute grammars and data-ow analysis may allow us to apply efficient incremental update algorithms used in attribute evaluation [72] to data-ow analysis. This idea was first put forth by Carroll and Ryder <ref> [18, 74] </ref>. While attribute evaluation has been applied to solving DFA problems [72], that work has largely not taken advantage of the techniques developed in traditional data-ow analysis.
Reference: 75. <author> B. G. Ryder, T. J. Marlowe, and M. C. Paull. </author> <title> Incremental Iteration: When Will It Work?. </title> <type> Tech. Rept. Technical Report LCSR-TR-89, </type> <institution> Rutgers University, Laboratory for Computer Science Research, </institution> <month> March, </month> <year> 1987. </year>
Reference: 76. <author> B. G. Ryder and M. C. Paull. </author> <title> Elimination algorithms for data ow analysis. </title> <journal> Com-put. Surv. </journal> <volume> 18, </volume> <month> 3 (September </month> <year> 1986), </year> <pages> 277-316. </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder <ref> [76] </ref>, and Tarjan [83]. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. <p> However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods <ref> [3, 39, 76, 82, 86] </ref> use the structure of the CFG to solve DFA. They have two chief advantages. First, they can be faster than iteration.
Reference: 77. <author> J. T. Schwartz and M. Sharir. </author> <title> A Design for Optimizations of the Bitvectoring Class. </title> <type> Tech. </type> <institution> Rept. NSO-17, Courant Institute of Mathematical Sciences, </institution> <year> 1979. </year>
Reference: 78. <author> M. Sharir. </author> <title> Structural analysis: a new approach to ow analysis in optimizing compilers. </title> <booktitle> Computer Languages 5 (1980), </booktitle> <pages> 141-153. </pages>
Reference-contexts: Procedure 5-1 shows the main analysis function FG::cfa, which is called from FG::analyze. Within FG::cfa are three steps. This following will outline the steps in the context of reducible ow graphs. Readers interested in other aspect of control-ow analysis should refer to the literature <ref> [39, 59, 78, 80, 81] </ref>. To see how the path simplifier operates on irreducibilities see Section 3.2.2.
Reference: 79. <author> B. Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference: 80. <author> R. E. Tarjan. </author> <title> Finding dominators in directed graphs. </title> <journal> SIAM J. Comput. </journal> <volume> 3, </volume> <month> 1 (March </month> <year> 1974). </year>
Reference-contexts: Procedure 5-1 shows the main analysis function FG::cfa, which is called from FG::analyze. Within FG::cfa are three steps. This following will outline the steps in the context of reducible ow graphs. Readers interested in other aspect of control-ow analysis should refer to the literature <ref> [39, 59, 78, 80, 81] </ref>. To see how the path simplifier operates on irreducibilities see Section 3.2.2.
Reference: 81. <author> R. E. Tarjan. </author> <title> Testing ow graph reducibility. </title> <institution> J. of Comput. and Syst. Sci. </institution> <month> (9 </month> <year> 1974), </year> <pages> 355-365. </pages>
Reference-contexts: For example, applying CFA to Figure 3-1 gives the lists of loops as , , , and . Control-ow analysis also computes a reverse post-ordering of the nodes, an ordering that will be used in iteration and propagation. Our control-ow analysis uses an algorithm of Tarjans <ref> [81] </ref> which can compute the above information in time. This algorithm traverses the graph twice. The first depth-first traversal computes the reverse post-ordering of the graph, the back-edges, and the loop headers. A second traversal uses the loop headers and back-edge information to compute the lists of nodes described above. <p> A ag argument dir tells FG::headers and FG::body which set to return. The default CFA algorithm provided by FG is based on Tarjans algorithm for testing reducibility of a ow graph <ref> [81] </ref> and on a closely related algorithm by Graham and Wegman [39]. In addition to finding loop headers, Tarjans algorithm computes an ordering of the nodes with which to apply Ullmans T 1 and T 2 transformations [86] on a graph. <p> Procedure 5-1 shows the main analysis function FG::cfa, which is called from FG::analyze. Within FG::cfa are three steps. This following will outline the steps in the context of reducible ow graphs. Readers interested in other aspect of control-ow analysis should refer to the literature <ref> [39, 59, 78, 80, 81] </ref>. To see how the path simplifier operates on irreducibilities see Section 3.2.2.
Reference: 82. <author> R. E. Tarjan. </author> <title> Fast algorithm for solving path problems. </title> <journal> J. ACM 28, </journal> <month> 3 (July </month> <year> 1981), </year> <pages> 594-614. Bibliography 129 </pages>
Reference-contexts: These abstractions are general in that they can implement a wide range of program analyses, from those based on traditional bit-vector to those based on sophisticated symbolic analysis. The key to achieving this versatility is an algorithm called path simplification. Path simplification, a version of Tarjans fast path algorithm <ref> [82] </ref>, computes a path expression for each node u in the ow graph. This expression represents all paths from the source to u. A path expression is a regular expressions built from node labels and the operators: , +, and *. <p> Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan <ref> [82] </ref>, and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. <p> However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods <ref> [3, 39, 76, 82, 86] </ref> use the structure of the CFG to solve DFA. They have two chief advantages. First, they can be faster than iteration. <p> Sharlits analyzers can solve a wide range of program-analysis problems, from the traditional bit-vector based DFAs to ones using sophisticated symbolic analysis. The analyzers use one general algorithm path simplificationto solve these DFAs. This algorithm is based on Tarjans fast path algorithm <ref> [82] </ref>. Path simplification extends these previous algorithms by integrating iteration, elimination and sparse data-ow evaluation techniques into one algorithm. Compiler writers can pose their DFA problems directly on a CFG in which each node is an instruction. Such graphs have many ow equations and ow variables. <p> We use this algorithm because it can be customized to solve different DFA problems, and because it is efficient. In this section we give an outline of TFPA. Tarjans TFPA algorithm solves the single-source path problem (SSPP) <ref> [82] </ref>. In an accompanying paper [83], Tarjan recognized that the SSPP is a generalization of many graph problems, one of which is data-ow analysis. SSPP finds a path expression representing all paths from the source to every node in a graph. <p> In the case of node F, its path expressions is computed because it is necessary in computing the path expression of the node S. Tarjan <ref> [82] </ref> calls this on-demand computation path compression.
Reference: 83. <author> R. E. Tarjan. </author> <title> A unified approach to path problems. </title> <journal> J. ACM 28, </journal> <month> 3 (July </month> <year> 1981), </year> <pages> 577-593. </pages>
Reference-contexts: Solving these equations yields the desired data-ow information. DFA has been extensively studied in the literature. The properties of DFA problems have been formalized by many authors: Cousot [22], Graham and Wegman [39], Hecht [42, 43], Kam and Ullman [52, 53], Marlowe and Ryder [62], Ryder [76], and Tarjan <ref> [83] </ref>. Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman [86]. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. <p> We use this algorithm because it can be customized to solve different DFA problems, and because it is efficient. In this section we give an outline of TFPA. Tarjans TFPA algorithm solves the single-source path problem (SSPP) [82]. In an accompanying paper <ref> [83] </ref>, Tarjan recognized that the SSPP is a generalization of many graph problems, one of which is data-ow analysis. SSPP finds a path expression representing all paths from the source to every node in a graph.
Reference: 84. <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992, </year> <pages> pp. 82-93. </pages>
Reference: 85. <author> S. W. K. Tjiang, M. E. Wolf, M. S. Lam, K. L. Pieper, and J. L. Hennessy. </author> <title> Integrating scalar optimization and parallelization. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug., </month> <year> 1991, </year> <pages> pp. 137-151. </pages>
Reference: 86. <author> J. D. Ullman. </author> <title> Fast algorithms for the elimination of common subexpressions. </title> <journal> Acta Inf. </journal> <volume> 2, </volume> <month> 3 (July </month> <year> 1973), </year> <pages> 191-213. </pages>
Reference-contexts: Efficient algorithms to solve DFA problems have been developed by Choi et al [19], Graham and Wegman [39], Hecht and Ullman [43], Horwitz et al [46], Tarjan [82], and Ullman <ref> [86] </ref>. Kildall [54] was the first to recognize that program analysis can serve as a basis for implementing optimizations. By choosing the appropriate abstract domain, a data-ow analyzer can compute not only simple information such as reaching definitions and liveness, but detect recurrences and perform code motion. <p> However, their techniques require a control-ow analysis, more complicated than a simple depth-first walk of the graph and similar to that used in elimination algorithms. 2.2.2 Elimination: Exploiting Flow Graph Structure Elimination methods <ref> [3, 39, 76, 82, 86] </ref> use the structure of the CFG to solve DFA. They have two chief advantages. First, they can be faster than iteration. <p> Evaluating the ow functions yields the solution to the DFA problem. TFPA uses the loop structure of a reducible graph to solve SSPP. It is similar to elimination algorithms such as Graham and Wegman [39], and Ullman <ref> [86] </ref>. Like their algorithm, TFPA finds the natural loops [2] of a program, then removes the back-edges of the loops from inner-most loop out by replacing edges. The key steps in the algorithm are: 1. Control-ow analysis (CFA): Determine the loop structure of the ow graph. <p> This rule, together will other rules, is used to remove back edges (See Figure 4-8). 3. join Replace the path with a path that represents the data-ow effect of the path (Figure 4-9). These rules resemble those of T 1 -T 2 graph reductions of Ullman <ref> [86] </ref>. And Sharlit uses them in a similar fashion. <p> In addition to finding loop headers, Tarjans algorithm computes an ordering of the nodes with which to apply Ullmans T 1 and T 2 transformations <ref> [86] </ref> on a graph. The appli Function Description FG::next_nodes (u) Returns a vector of nodes that follow the node u. FG::prev_nodes (u) Returns a vector of nodes that precede the node u. FG::analyze () Performs or updates control-ow analysis.
Reference: 87. <author> G. A. Venkatesch. </author> <title> A framework for construction and evaluation of high-level specifications for program analysis techniques. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1989, </year> <pages> pp. 1-12. </pages>
Reference: 88. <author> H. S. Warren, Jr., M. A. Auslander, G. J. Chaitin, A. C. Chibib, M. E. Hopkins, and A. L. Mackay. </author> <title> Final Code Generation in the PL.8 Compiler. </title> <type> Tech. Rept. Technical Report RC 11974, </type> <institution> IBM Thomas J. Watson Research Center, </institution> <month> June, </month> <year> 1986. </year>
Reference: 89. <author> M. Wegman and K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In 12th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1985. </year>
Reference-contexts: Sparse techniques such as SSA, its extensions, and their applicationshave been studied extensively recently by many authors: Alpern et al [8], Ballance et al [12], Cytron et al [25, 26], Ferrante et al [29], and Wegman and Zadeck <ref> [89] </ref>. Solving a DFA problem using a SDFEG is efficient because a SDFEG contains a subset of nodes in the original ow graph, those whose ow functions are not identity, and those at which ow valuesgenerated at distinct nodesmeet. <p> While this focus on CFG suits many optimizations, a uniform representation for all dependencies can benefit many optimizations. For example, several powerful techniques <ref> [4, 8, 12, 26, 29, 73, 89] </ref> are formulated on graphs in which control-dependence and data-ow are uniformly represented.
Reference: 90. <author> E. B. White. </author> <title> Charlotte's Web. </title> <publisher> Harper and Row, </publisher> <year> 1952. </year>
Reference: 91. <author> D. Whitfield and M. L. Soffa. </author> <title> Automatic Generation of Global Optimizers. </title> <booktitle> In SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1991, </year> <pages> pp. 120-129. </pages>
Reference: 92. <author> M. E. Wolf. </author> <title> Improving Parallelism and Data Locality in Nested Loops. </title> <type> Ph.D. </type> <institution> Th., Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Using the ability to share phases inherent in SUIF, we have, in a reasonable time frame obtained interesting results in individual topics such as data dependence analysis [64], par-allelization [68] and loop transformations <ref> [92, 93] </ref>. There are still many subjects to be Chapter 8Conclusion 121 explored with the SUIF platform.
Reference: 93. <author> M. E. Wolf and M. S. Lam. </author> <title> An algorithmic approach to compound loop transformations. </title> <booktitle> In Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug., </month> <year> 1990. </year>
Reference-contexts: This dependence information can be used in low-level passes such as a superscalar/VLIW instruction scheduler, and in high-level passes that privatize scalars, distribute and transform loops. Loop optimizations in the SUIF compiler are based on unified unimodular transforma-tionsinterchange, reversal, skewingtogether with a new transformation theory <ref> [93, 94] </ref>. The theory shows how these transformations, together with tiling (or blocking), can expose parallelism and improve data locality, without introducing excessive communication or over-committing caches. This theory simplifies implementation, finding directly the optimal loop transformations and computing directly the loop bounds without searching exhaustively through the transformation space. <p> Using the ability to share phases inherent in SUIF, we have, in a reasonable time frame obtained interesting results in individual topics such as data dependence analysis [64], par-allelization [68] and loop transformations <ref> [92, 93] </ref>. There are still many subjects to be Chapter 8Conclusion 121 explored with the SUIF platform.
Reference: 94. <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year> <note> Bibliography 130 </note>
Reference-contexts: This dependence information can be used in low-level passes such as a superscalar/VLIW instruction scheduler, and in high-level passes that privatize scalars, distribute and transform loops. Loop optimizations in the SUIF compiler are based on unified unimodular transforma-tionsinterchange, reversal, skewingtogether with a new transformation theory <ref> [93, 94] </ref>. The theory shows how these transformations, together with tiling (or blocking), can expose parallelism and improve data locality, without introducing excessive communication or over-committing caches. This theory simplifies implementation, finding directly the optimal loop transformations and computing directly the loop bounds without searching exhaustively through the transformation space. <p> The high-level phases are functioning. Data-dependence tests will run on all the Perfect club [64]. Linear loop transformations can auto Chapter 7SUIF 118 matically block matrix algorithms such as QR decomposition and LU decomposition with out pivoting, whose performance we have measured on an SGI multiprocessor <ref> [94] </ref>. Chapter 8Conclusion 119 Conclusion This dissertation has made two contributions to the building optimizing compilers. The first is a software architecture with which to write scalar optimizers that are modular and extensible.
References-found: 94

