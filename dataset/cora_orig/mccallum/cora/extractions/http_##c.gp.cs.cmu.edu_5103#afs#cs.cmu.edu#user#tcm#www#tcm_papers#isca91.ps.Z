URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/tcm/www/tcm_papers/isca91.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: http://www.cs.cmu.edu
Title: Comparative Evaluation of Latency Reducing and Tolerating Techniques  
Author: Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: Techniques that can cope with the large latency of memory accesses are essential for achieving high processor utilization in large-scale shared-memory multiprocessors. In this paper, we consider four architectural techniques that address the latency problem: (i) hardware coherent caches, (ii) relaxed memory consistency, (iii) software-controlled prefetching, and (iv) multiple-context support. While some studies of benefits of the individual techniques have been done, no study evaluates all of the techniques within a consistent framework. This paper attempts to remedy this by providing a comprehensive evaluation of the benefits of the four techniques, both individually and in combinations, using a consistent set of architectural assumptions. The results in this paper have been obtained using detailed simulations of a large-scale shared-memory multiprocessor. Our results show that caches and relaxed consistency uniformly improve performance. The improvements due to prefetching and multiple contexts are sizeable, but are much more application-dependent. Combinations of the various techniques generally attain better performance than each one on its own. Overall, we show that using suitable combinations of the techniques, performance can be improved by 4 to 7 times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models <ref> [1, 5, 8] </ref> hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. <p> The main advantage of the relaxed models is the potential for increased performance. The main disadvantage is increased hardware complexity and a more complex programming model. Other relaxed models that have been discussed in the literature are processor consistency [8, 10], weak consistency [5], and DRF0 <ref> [1] </ref>. These models fall between sequential and release consistency mod els in terms of flexibility and are not considered further in this study.
Reference: [2] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. MIT VLSI Memo 89-566, </title> <institution> Lab. for Comput. Sci., </institution> <note> Submitted for publication, </note> <month> September </month> <year> 1989. </year>
Reference-contexts: We presented a preliminary investigation of the benefits of multiple-context processors in cache-coherent multiprocessors in a previous study [29]. More recently, there have also been two analytical evaluations of multiple contexts <ref> [2, 24] </ref>. In this study we present a more detailed evaluation of the performance of multiple-context processors, and we also consider the combined effect with other latency hiding techniques. We use processors with two and four contexts.
Reference: [3] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These large memory latencies can quickly offset any performance gains expected from the use of parallelism. Techniques that can help to reduce or hide these latencies are essential for achieving high processor utilization. To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches <ref> [3, 4, 18, 30] </ref> allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. <p> Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts <ref> [3, 12, 13, 26, 29] </ref> allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. <p> Unfortunately, as a result of the combination of distributed memory, caches, and general interconnection networks used by large-scale multiprocessors <ref> [3, 18, 22] </ref>, multiple requests issued by a processor may execute out of order. This may result in incorrect program behavior if the program depends on accesses to complete in order. Consequently, restrictions have to be placed on the types of buffering and pipelining allowed. <p> Processors with multiple hardware contexts <ref> [3, 12, 13, 26, 29] </ref> do not have this disadvantage. They make use of increased concurrency to hide latency. Each processor has several processes assigned to it, which are kept Page 6 as hardware contexts.
Reference: [4] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: These large memory latencies can quickly offset any performance gains expected from the use of parallelism. Techniques that can help to reduce or hide these latencies are essential for achieving high processor utilization. To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches <ref> [3, 4, 18, 30] </ref> allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. <p> Their use in multiprocessors, however, is complicated by the fact that the caches need to be kept coherent. While the coherence problem is easily solved for small bus-based multiprocessors through the use of snoopy cache-coherence protocols <ref> [4] </ref>, the problem is much more complicated for large-scale multiprocessors that use general interconnection networks. <p> If multiple prefetches are issued back-to-back to fetch the data structure, the latency of all but the first prefetched reference can be hidden due to the pipelining of the memory accesses. Prefetching offers another benefit in multiprocessors that use an ownership-based cache coherence protocol <ref> [4] </ref>. If a line is to be modified, prefetching it directly with ownership can significantly reduce the write latencies and the ensuing network traffic for ob taining ownership.
Reference: [5] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models <ref> [1, 5, 8] </ref> hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. <p> The main advantage of the relaxed models is the potential for increased performance. The main disadvantage is increased hardware complexity and a more complex programming model. Other relaxed models that have been discussed in the literature are processor consistency [8, 10], weak consistency <ref> [5] </ref>, and DRF0 [1]. These models fall between sequential and release consistency mod els in terms of flexibility and are not considered further in this study.
Reference: [6] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluating the performance of four snooping cache coherency protocols. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 2-15, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Hardware-controlled prefetching includes schemes such as long cache lines and instruction look-ahead [16]. The effectiveness of long cache lines is limited by the reduced spatial locality in multiprocessor applications <ref> [6, 28] </ref>, while instruction look-ahead is limited by branches and the finite look-ahead buffer size. With software-controlled prefetching, explicit prefetch instructions are issued.
Reference: [7] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Int. Conf. Arch. Support Prog. </booktitle> <address> Lang. </address> <institution> Oper. Syst., </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. Although one can find papers that focus on the performance of the individual techniques <ref> [7, 11, 29] </ref>, it is not possible to use these papers to perform a comparative evaluation, since the benchmark programs differ, or the architectural assumptions differ, or both. We believe that a consistent comparative evaluation is essential to understanding the tradeoffs in the use of the different techniques. <p> A remote node is any node, other than the local or home node. The latency 1 The architectural parameters and benchmark data sets in this paper differ from those in previous papers <ref> [7, 21] </ref>, and therefore results cannot be directly compared. Table 1: Latency for various memory system operations in processor clock cycles (1 pclock = 30 ns). <p> These models fall between sequential and release consistency mod els in terms of flexibility and are not considered further in this study. For a detailed performance evaluation of relaxed memory consistency models, we refer the reader to a previous study <ref> [7] </ref>. 4.1 Implementation of Consistency Schemes Sequential consistency is satisfied in our implementation by ensuring that the memory accesses from each process complete in the order that they appear in the program. This is achieved by delaying the issue of an access until the previous access completes. <p> To fully realize the benefits of RC, we allow reads to bypass the write buffer and provide lockup-free caches that allow reads to be serviced while there are write misses outstanding <ref> [7] </ref>. This ensures that reads are not stalled due to previous writes. The lockup-free cache also allows multiple write accesses to be pipelined.
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models <ref> [1, 5, 8] </ref> hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. <p> Unfortunately, SC imposes severe restrictions on the outstanding accesses that a process may have, and thus limits the buffering and pipelining allowed. One of the most relaxed models is the release consistency (RC) model <ref> [8] </ref>. Release consistency requires that synchronization accesses in the program be identified and classified as either acquires (e.g., locks) or releases (e.g., unlocks). <p> The main advantage of the relaxed models is the potential for increased performance. The main disadvantage is increased hardware complexity and a more complex programming model. Other relaxed models that have been discussed in the literature are processor consistency <ref> [8, 10] </ref>, weak consistency [5], and DRF0 [1]. These models fall between sequential and release consistency mod els in terms of flexibility and are not considered further in this study.
Reference: [9] <author> S. R. Goldschmidt and H. Davis. </author> <title> Tango introduction and tutorial. </title> <type> Technical Report CSL-TR-90-410, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: However, some of our applications do not scale well to that many processes given the small data sets that we can simulate. The architecture simulator is tightly coupled to the Tango reference generator <ref> [9] </ref> to assure a correct interleaving of accesses. For example, a process doing a read operation is blocked until that read completes, where the latency of the read is determined by the architecture simulator.
Reference: [10] <author> J. R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report no. 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The main advantage of the relaxed models is the potential for increased performance. The main disadvantage is increased hardware complexity and a more complex programming model. Other relaxed models that have been discussed in the literature are processor consistency <ref> [8, 10] </ref>, weak consistency [5], and DRF0 [1]. These models fall between sequential and release consistency mod els in terms of flexibility and are not considered further in this study.
Reference: [11] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Int. Conf. Supercomputing, </booktitle> <pages> pages 354-368, </pages> <year> 1990. </year>
Reference-contexts: Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques <ref> [11, 16, 21, 23] </ref> hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts [3, 12, 13, 26, 29] allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. <p> Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. Although one can find papers that focus on the performance of the individual techniques <ref> [7, 11, 29] </ref>, it is not possible to use these papers to perform a comparative evaluation, since the benchmark programs differ, or the architectural assumptions differ, or both. We believe that a consistent comparative evaluation is essential to understanding the tradeoffs in the use of the different techniques.
Reference: [12] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 443-451, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts <ref> [3, 12, 13, 26, 29] </ref> allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. <p> Processors with multiple hardware contexts <ref> [3, 12, 13, 26, 29] </ref> do not have this disadvantage. They make use of increased concurrency to hide latency. Each processor has several processes assigned to it, which are kept Page 6 as hardware contexts.
Reference: [13] <author> R. A. </author> <title> Iannucci. Toward a dataflow/von Neumann hybrid architecture. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 131-140, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts <ref> [3, 12, 13, 26, 29] </ref> allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. <p> Processors with multiple hardware contexts <ref> [3, 12, 13, 26, 29] </ref> do not have this disadvantage. They make use of increased concurrency to hide latency. Each processor has several processes assigned to it, which are kept Page 6 as hardware contexts.
Reference: [14] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 81-85, </pages> <year> 1981. </year>
Reference-contexts: The interface consists of a read buffer and a write buffer. The write buffer is 16 entries deep. We allow reads to bypass writes in the write buffer if permitted by the memory consistency model. Both the primary and secondary caches are lockup-free <ref> [14] </ref>, direct-mapped, and use 16 byte lines.
Reference: [15] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-28(9):241-248, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Consequently, restrictions have to be placed on the types of buffering and pipelining allowed. These restrictions are determined by the memory consistency model supported by the multiprocessor. Several memory consistency models have been proposed. The strictest model is that of sequential consistency (SC) <ref> [15] </ref>. It requires the execution of a parallel program to appear as some interleaving of the execution of the parallel processes on a sequential machine. Unfortunately, SC imposes severe restrictions on the outstanding accesses that a process may have, and thus limits the buffering and pipelining allowed.
Reference: [16] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques <ref> [11, 16, 21, 23] </ref> hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts [3, 12, 13, 26, 29] allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. <p> In contrast, non-binding prefetching also brings the data close to the processor, but the data remains visible to the cache coherence protocol and is thus kept consistent until the processor actually reads the value. Hardware-controlled prefetching includes schemes such as long cache lines and instruction look-ahead <ref> [16] </ref>. The effectiveness of long cache lines is limited by the reduced spatial locality in multiprocessor applications [6, 28], while instruction look-ahead is limited by branches and the finite look-ahead buffer size. With software-controlled prefetching, explicit prefetch instructions are issued.
Reference: [17] <author> R. L. Lee, P.-C. Yew, and D. H. Lawrie. </author> <title> Data prefetching in shared memory multiprocessors. </title> <booktitle> In Proc. Int. Conf. Paral. Proc., </booktitle> <pages> pages 28-31, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: This places restrictions on when a binding prefetch can be issued, since the value will become stale if another processor modifies the same location during the interval between prefetch and reference. Binding prefetching studies done by Lee <ref> [17] </ref> reported significant performance loss due to such limitations. In contrast, non-binding prefetching also brings the data close to the processor, but the data remains visible to the cache coherence protocol and is thus kept consistent until the processor actually reads the value.
Reference: [18] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hen-nessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Large-scale shared-memory multiprocessors are expected to have remote memory reference latencies of several tens to hundreds of processor cycles <ref> [18, 22, 25, 30] </ref>. The large latencies arise partly due to the increased physical dimensions of the parallel machine and partly due to the ever increasing clock rates at which the individual processors operate. These large memory latencies can quickly offset any performance gains expected from the use of parallelism. <p> These large memory latencies can quickly offset any performance gains expected from the use of parallelism. Techniques that can help to reduce or hide these latencies are essential for achieving high processor utilization. To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches <ref> [3, 4, 18, 30] </ref> allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. <p> The results presented in this paper are obtained from detailed architectural simulations performed for three parallel applications. The architecture used is based on the Stanford DASH multiprocessor <ref> [18] </ref>, a large-scale shared-memory multiprocessor that provides coherent caches, a relaxed memory consistency model, and support for software-controlled prefetching. The applications we study are a particle-based simulator used in aeronautics (MP3D) [20], an LU-decomposition program (LU), and a digital logic simulation program (PTHOR) [27]. <p> Network Directory Memory & Controller Processor Cache Memory Architecture Processor Cache Primary Secondary Cache Write Buffer STORESLOADS Processor Environment architectural assumptions, the benchmark applications, and the simulation environment used to get the performance results. 2.1 Architectural Assumptions For this study, we have chosen an architecture that resembles the DASH multiprocessor <ref> [18] </ref>, a large-scale cache-coherent machine currently being built at Stanford. Figure 1 shows the high-level organization of the simulated architecture. The architecture consists of several processing nodes connected through a low-latency scalable interconnection network. Physical memory is distributed among the nodes. <p> As a result, some existing large-scale multiprocessors do not provide caches (e.g., BBN Butterfly [25]), others provide caches that must be kept coherent by software (e.g., IBM RP3 [22]), and still others provide full hardware support for coherent caches (e.g., Stanford DASH <ref> [18] </ref>). In this section we evaluate the performance benefits when both private and shared read-write data are cacheable as allowed by hardware coherent caches versus the case when only private data are cacheable. An alternative to hardware coherence is software cache coherence. <p> Unfortunately, as a result of the combination of distributed memory, caches, and general interconnection networks used by large-scale multiprocessors <ref> [3, 18, 22] </ref>, multiple requests issued by a processor may execute out of order. This may result in incorrect program behavior if the program depends on accesses to complete in order. Consequently, restrictions have to be placed on the types of buffering and pipelining allowed. <p> Another important advantage of prefetching, in contrast to other latency hiding techniques such as multiple contexts, is that it can be implemented using existing commercial processors, as has been done in DASH <ref> [18] </ref>. 6 Multiple-Context Processors Although prefetching is useful for many applications, it requires explicit programmer or compiler intervention. Processors with multiple hardware contexts [3, 12, 13, 26, 29] do not have this disadvantage. They make use of increased concurrency to hide latency.
Reference: [19] <editor> E. Lusk, R. Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: This information will be useful in later sections for understanding the performance results. The selected applications are representative of algorithms used in an engineering computing environment. All of the applications are written in C. The Argonne National Laboratory macro package <ref> [19] </ref> is used to provide synchronization and sharing primitives. Some general statistics for the benchmarks are shown in Table 2. MP3D [20] is a 3-dimensional particle simulator. It is used to study the pressure and temperature profiles created as an object flies at high speed through the upper atmosphere.
Reference: [20] <author> J. D. McDonald and D. Baganoff. </author> <title> Vectorization of a particle simulation method for hypersonic rarified flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The architecture used is based on the Stanford DASH multiprocessor [18], a large-scale shared-memory multiprocessor that provides coherent caches, a relaxed memory consistency model, and support for software-controlled prefetching. The applications we study are a particle-based simulator used in aeronautics (MP3D) <ref> [20] </ref>, an LU-decomposition program (LU), and a digital logic simulation program (PTHOR) [27]. The applications are typical of those that may be found in an engineering environment. Our results show that the provision of coherent caches leads to significant performance benefits. <p> The selected applications are representative of algorithms used in an engineering computing environment. All of the applications are written in C. The Argonne National Laboratory macro package [19] is used to provide synchronization and sharing primitives. Some general statistics for the benchmarks are shown in Table 2. MP3D <ref> [20] </ref> is a 3-dimensional particle simulator. It is used to study the pressure and temperature profiles created as an object flies at high speed through the upper atmosphere.
Reference: [21] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> J. Paral. Distr. Computing, </journal> <note> to appear in June 1991. </note>
Reference-contexts: Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques <ref> [11, 16, 21, 23] </ref> hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts [3, 12, 13, 26, 29] allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. <p> A remote node is any node, other than the local or home node. The latency 1 The architectural parameters and benchmark data sets in this paper differ from those in previous papers <ref> [7, 21] </ref>, and therefore results cannot be directly compared. Table 1: Latency for various memory system operations in processor clock cycles (1 pclock = 30 ns). <p> The disadvantages of software control include the extra instruction overhead to generate the prefetches as well as the need for sophisticated software intervention. In this study, we consider non-binding software-controlled prefetching <ref> [21] </ref>. The benefits due to prefetching come from several sources. The most obvious benefit occurs when a prefetch is issued early enough in the code, so that the line is already in the cache by the time it is referenced. <p> The reason for having a separate prefetch buffer is to avoid delaying prefetch requests unnecessarily behind writes in the write buffer <ref> [21] </ref>. We model a prefetch buffer that is 16 entries deep. Once the prefetch reaches the head of the prefetch buffer, the secondary cache is checked to see whether the line is already present. If so, the prefetch is discarded.
Reference: [22] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, V. A. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proc. Int. Conf. Paral. Proc., </booktitle> <pages> pages 764-771, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Large-scale shared-memory multiprocessors are expected to have remote memory reference latencies of several tens to hundreds of processor cycles <ref> [18, 22, 25, 30] </ref>. The large latencies arise partly due to the increased physical dimensions of the parallel machine and partly due to the ever increasing clock rates at which the individual processors operate. These large memory latencies can quickly offset any performance gains expected from the use of parallelism. <p> As a result, some existing large-scale multiprocessors do not provide caches (e.g., BBN Butterfly [25]), others provide caches that must be kept coherent by software (e.g., IBM RP3 <ref> [22] </ref>), and still others provide full hardware support for coherent caches (e.g., Stanford DASH [18]). In this section we evaluate the performance benefits when both private and shared read-write data are cacheable as allowed by hardware coherent caches versus the case when only private data are cacheable. <p> Unfortunately, as a result of the combination of distributed memory, caches, and general interconnection networks used by large-scale multiprocessors <ref> [3, 18, 22] </ref>, multiple requests issued by a processor may execute out of order. This may result in incorrect program behavior if the program depends on accesses to complete in order. Consequently, restrictions have to be placed on the types of buffering and pipelining allowed.
Reference: [23] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Coherent caches [3, 4, 18, 30] allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques <ref> [11, 16, 21, 23] </ref> hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts [3, 12, 13, 26, 29] allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered.
Reference: [24] <author> R. H. Saavedra-Barrera, D. E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> In ACM Symp. Paral. Alg. Arch., </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: We presented a preliminary investigation of the benefits of multiple-context processors in cache-coherent multiprocessors in a previous study [29]. More recently, there have also been two analytical evaluations of multiple contexts <ref> [2, 24] </ref>. In this study we present a more detailed evaluation of the performance of multiple-context processors, and we also consider the combined effect with other latency hiding techniques. We use processors with two and four contexts.
Reference: [25] <author> G. E. Schmidt. </author> <title> The Butterfly parallel processor. </title> <booktitle> In Proc. Int. Conf. Supercomputing, </booktitle> <pages> pages 362-365, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Large-scale shared-memory multiprocessors are expected to have remote memory reference latencies of several tens to hundreds of processor cycles <ref> [18, 22, 25, 30] </ref>. The large latencies arise partly due to the increased physical dimensions of the parallel machine and partly due to the ever increasing clock rates at which the individual processors operate. These large memory latencies can quickly offset any performance gains expected from the use of parallelism. <p> While the coherence problem is easily solved for small bus-based multiprocessors through the use of snoopy cache-coherence protocols [4], the problem is much more complicated for large-scale multiprocessors that use general interconnection networks. As a result, some existing large-scale multiprocessors do not provide caches (e.g., BBN Butterfly <ref> [25] </ref>), others provide caches that must be kept coherent by software (e.g., IBM RP3 [22]), and still others provide full hardware support for coherent caches (e.g., Stanford DASH [18]).
Reference: [26] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts <ref> [3, 12, 13, 26, 29] </ref> allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. <p> Processors with multiple hardware contexts <ref> [3, 12, 13, 26, 29] </ref> do not have this disadvantage. They make use of increased concurrency to hide latency. Each processor has several processes assigned to it, which are kept Page 6 as hardware contexts. <p> Given processor caches, the interval between long-latency operations (i.e., cache misses) becomes fairly large, allowing just a handful of hardware contexts to hide most of the latency [29]. This is in contrast to the early multiple-context processors such as the HEP <ref> [26] </ref>, where context switches occurred on every cycle. The performance gain to be expected from multiple context processors depends on several factors. First, there is the number of contexts. With more contexts available, we are less likely to be out of ready-to-run contexts.
Reference: [27] <author> L. Soule and A. Gupta. </author> <title> Parallel distributed-time logic simulation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 6(6) </volume> <pages> 32-48, </pages> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: The applications we study are a particle-based simulator used in aeronautics (MP3D) [20], an LU-decomposition program (LU), and a digital logic simulation program (PTHOR) <ref> [27] </ref>. The applications are typical of those that may be found in an engineering environment. Our results show that the provision of coherent caches leads to significant performance benefits. For this reason, all remaining experiments in the paper were done assuming that coherent caches are provided. <p> Once a processor completes a column, it releases any processors waiting for that column. For our experiments we performed LU-decomposition on a 200x200 matrix. PTHOR <ref> [27] </ref> is a parallel logic simulator based on the Chandy-Misra distributed-time simulation algorithm. The primary data Page 2 Table 2: General statistics for the benchmarks.
Reference: [28] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Measurement, analysis, and improvement of the cache behavior of shared data in cache coherent multiprocessors. </title> <type> Technical Report CSL-TR-90-412, </type> <institution> Stanford University, </institution> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Hardware-controlled prefetching includes schemes such as long cache lines and instruction look-ahead [16]. The effectiveness of long cache lines is limited by the reduced spatial locality in multiprocessor applications <ref> [6, 28] </ref>, while instruction look-ahead is limited by branches and the finite look-ahead buffer size. With software-controlled prefetching, explicit prefetch instructions are issued.
Reference: [29] <author> W.-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references. Prefetching techniques [11, 16, 21, 23] hide the latency by bringing data close to the processor before it is actually needed. Multiple contexts <ref> [3, 12, 13, 26, 29] </ref> allow a processor to hide latency by switching from one context to another when a high-latency operation is encountered. Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. <p> Our primary objective in this paper is to characterize the benefits and costs of these four latency hiding techniques in a systematic and consistent manner. Although one can find papers that focus on the performance of the individual techniques <ref> [7, 11, 29] </ref>, it is not possible to use these papers to perform a comparative evaluation, since the benchmark programs differ, or the architectural assumptions differ, or both. We believe that a consistent comparative evaluation is essential to understanding the tradeoffs in the use of the different techniques. <p> Processors with multiple hardware contexts <ref> [3, 12, 13, 26, 29] </ref> do not have this disadvantage. They make use of increased concurrency to hide latency. Each processor has several processes assigned to it, which are kept Page 6 as hardware contexts. <p> In this manner the memory latency of one context can be hidden with computation of another context. Given processor caches, the interval between long-latency operations (i.e., cache misses) becomes fairly large, allowing just a handful of hardware contexts to hide most of the latency <ref> [29] </ref>. This is in contrast to the early multiple-context processors such as the HEP [26], where context switches occurred on every cycle. The performance gain to be expected from multiple context processors depends on several factors. First, there is the number of contexts. <p> Also, as is the case with release consistency and prefetching, the memory system is more heavily loaded by multiple contexts, and thus latencies may increase. We presented a preliminary investigation of the benefits of multiple-context processors in cache-coherent multiprocessors in a previous study <ref> [29] </ref>. More recently, there have also been two analytical evaluations of multiple contexts [2, 24]. In this study we present a more detailed evaluation of the performance of multiple-context processors, and we also consider the combined effect with other latency hiding techniques. We use processors with two and four contexts.
Reference: [30] <author> A. W. Wilson, Jr. </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessors. </title> <booktitle> In Proc. Int. Symp. Comput. Arch., </booktitle> <pages> pages 244-252, </pages> <month> June </month> <year> 1987. </year> <pages> Page 10 </pages>
Reference-contexts: 1 Introduction Large-scale shared-memory multiprocessors are expected to have remote memory reference latencies of several tens to hundreds of processor cycles <ref> [18, 22, 25, 30] </ref>. The large latencies arise partly due to the increased physical dimensions of the parallel machine and partly due to the ever increasing clock rates at which the individual processors operate. These large memory latencies can quickly offset any performance gains expected from the use of parallelism. <p> These large memory latencies can quickly offset any performance gains expected from the use of parallelism. Techniques that can help to reduce or hide these latencies are essential for achieving high processor utilization. To cope with the large latencies, several different architectural techniques have been proposed. Coherent caches <ref> [3, 4, 18, 30] </ref> allow shared read-write data to be cached and significantly reduce the memory latency seen by the processors. Relaxed memory consistency models [1, 5, 8] hide latency by allowing buffering and pipelining of memory references.
References-found: 30

