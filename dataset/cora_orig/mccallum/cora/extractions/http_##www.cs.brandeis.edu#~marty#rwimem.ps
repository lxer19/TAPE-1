URL: http://www.cs.brandeis.edu/~marty/rwimem.ps
Refering-URL: http://www.cs.brandeis.edu/~marty/
Root-URL: http://www.cs.brandeis.edu
Email: (marty@cs.brandeis.edu)  
Title: On the Channel Capacity of Read/Write Isolated Memory  
Author: Martin Cohn 
Date: July 12, 1994  
Address: Waltham MA 02254  
Affiliation: Computer Science Department Brandeis University  
Abstract: We apply graph theory to find upper and lower bounds on the channel capacity of a serial, binary, rewritable medium in which consecutive locations may not store 1's, and consecutive locations may not be altered during a single rewriting pass. If the true capacity is close to the upper bound, then a trivial code is nearly optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. V. Freiman and A.D. Wyner, </author> <title> "Optimum Block Codes for Noiseless Input-Restricted Channels" Information and Control vol. </title> <month> 7 </month> <year> (1964) </year> <month> 398-415 </month>
Reference-contexts: A read/write isolated memory (RWIM) is a binary, linearly ordered, rewritable storage medium obeying both restrictions. 1.1 Origin of the Problem The first restriction alone, no consecutive 1's, is typical of magnetic recording and has recurred in optical recording. The problem was first studied by Freiman and Wyner <ref> [1] </ref>, and a subcase by Kautz [2]; they showed that the capacity was 0:694 . . . = log 2 bits per symbol, where is the larger characteristic root of the Fibonacci recurrence. <p> Awareness in this sense corresponds to the special case of state-dependent encoding or decoding <ref> [1] </ref> in which the state of the system is identified with just the previous configuration. The case of greatest practical interest is encoder aware, decoder unaware, because the write mechanism can usually preview the memory before or during a write pass, while the read mechanism usually can not.
Reference: [2] <author> W.H. Kautz, </author> <title> "Fibonacci Codes for Synchronization Control" IEEE Transactions on Information Theory, </title> <address> IT-11 (April l965) 284-292 </address>
Reference-contexts: The problem was first studied by Freiman and Wyner [1], and a subcase by Kautz <ref> [2] </ref>; they showed that the capacity was 0:694 . . . = log 2 bits per symbol, where is the larger characteristic root of the Fibonacci recurrence.
Reference: [3] <author> J.P. </author> <title> Robinson "An Asymmetric Error-Correcting Ternary Code" IEEE Transaction on Information Theory, </title> <type> IT-24, </type> <month> 2 </month> <year> (1978) </year> <month> 258-261 </month>
Reference-contexts: The second restriction, no consecutive changes during rewriting, has arisen more recently in the context of bar codes <ref> [3] </ref> and rewritable optical discs [4], where again the capacity turns out to be log 2 . In this paper we consider the conjunction of these two restrictions.
Reference: [4] <author> G. Cohen and G. Zemor, </author> <note> "Write-Isolated Memories" Discrete Mathematics 114 (1993) pp. 105-113 </note>
Reference-contexts: The second restriction, no consecutive changes during rewriting, has arisen more recently in the context of bar codes [3] and rewritable optical discs <ref> [4] </ref>, where again the capacity turns out to be log 2 . In this paper we consider the conjunction of these two restrictions. <p> In this way the rewritable memory can be viewed over its lifetime as a noiseless communication channel, and a channel capacity, measured in bits per character, can be defined <ref> [4, 5] </ref>. Let k be the size of the memory in binary symbols, let r be the lifetime of the memory in rewrite cycles, and let N (k; r) be the number of distinct sequences of r characters. <p> The simplicity of this coding rule would be remarkable in view of the apparent difficulty of devising an efficient practical code for write-inhibited memory obeying just the single restriction "no consecutive changes." <ref> [4] </ref> If we consider the case of realistic serial memories, like optical disc, there are two reasons why capacity is likely a loose upper bound on achievable code rates: First, as mentioned in the Introduction, the definition of "capacity" contemplates encoding and decoding arbitrarily many write/read generations as a single message,
Reference: [5] <author> J. K. Wolf, A. D. Wyner, J. Ziv, J. Korner, </author> <title> "Coding for write-once memory", </title> <journal> AT & T Bell Laboratories Technical Journal vol. </journal> <volume> 63, no. </volume> <month> 6 </month> <year> (1984) </year> <month> 1089-1112. </month>
Reference-contexts: In this way the rewritable memory can be viewed over its lifetime as a noiseless communication channel, and a channel capacity, measured in bits per character, can be defined <ref> [4, 5] </ref>. Let k be the size of the memory in binary symbols, let r be the lifetime of the memory in rewrite cycles, and let N (k; r) be the number of distinct sequences of r characters. <p> The purpose of this paper is to derive expressions for the capacities C k and thus to bound C. 2 1.3 Coding The constructive coding problem for rewritable-memory has four variants <ref> [5] </ref>, according to whether the encoder and decoder, respectively, are aware or not aware of the previous memory configuration. Awareness in this sense corresponds to the special case of state-dependent encoding or decoding [1] in which the state of the system is identified with just the previous configuration.
Reference: [6] <author> C.E. </author> <title> Shannon The Mathematical Theory of Communication U. </title> <publisher> of Illinois Press, </publisher> <month> Ur-bana </month> <year> (1949) </year>
Reference-contexts: Then the capacity of the memory, in bits per rewrite, is defined to be <ref> [6] </ref> C k = lim 1 log 2 N (k; r) and the capacity of the read/write-isolated medium, in bits per symbol per rewrite, is defined to be C = lim 1 C k By observing that N (k; r) is the number of distinct paths through a channel graph that <p> r) and the capacity of the read/write-isolated medium, in bits per symbol per rewrite, is defined to be C = lim 1 C k By observing that N (k; r) is the number of distinct paths through a channel graph that describes permissible transitions among characters, Shannon showed in Ref. <ref> [6] </ref> that C k = log 2 k , where k is the largest (real) eigenvalue of the channel graph or of its associated adjacency matrix, and proved that capacity is an upper bound on the rate achievable by any coding scheme.
Reference: [7] <author> D. Cvetkovic, M. Doob, and H. Sachs, </author> <title> Spectra of Graphs Academic Press, </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> New York (1980) 8 </address>
Reference-contexts: On the other hand, when awareness entails a knowledge of only the single previous generation, a lower bound on C k may not be valid for any of the cases. 3.1 A Lower Bound A lower bound on C can be found via the following theorem: <ref> [7] </ref> Theorem 2 (Collatz, Sinogowitz) Let d be the mean value of the valencies and the greatest eigenvalue of a graph G. Then d, where equality holds if and only if G is regular. <p> But subtracting f k from s k would not change the capac ity. Theorem 2 is actually a special case (corresponding to the trivial partition) of the interlacing theorem <ref> [7] </ref>: Theorem 4 (Sims) Let A be a real symmetric matrix. Given a partition into blocks of sizes n i &gt; 0, consider the corresponding blocking A = (A ij ) so that A ij is an n i fi n j block. <p> We use a bound on the index of the union of two graphs with identical vertex sets, that is, the largest eigenvalue of the sum of their adjacency matrices <ref> [7, Theorem 2.1, p. 51] </ref> Theorem 6 (Courant-Weyl) If B 0 and B 00 are real symmetric matrices of order n, and if B = B 0 + B 00 , then k (B) k (B 0 ) + k (B 00 ).
References-found: 7

