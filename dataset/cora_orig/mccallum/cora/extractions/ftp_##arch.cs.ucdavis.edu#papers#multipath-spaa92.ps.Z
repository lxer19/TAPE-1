URL: ftp://arch.cs.ucdavis.edu/papers/multipath-spaa92.ps.Z
Refering-URL: http://arch.cs.ucdavis.edu/~chong/pubs.html
Root-URL: http://www.cs.ucdavis.edu
Title: Design and Performance of Multipath MIN Architectures  
Author: Frederic T. Chong and Thomas F. Knight, Jr. 
Keyword: (n percentage of network failure.  
Note: Acknowledgments: This research is supported by an Office of Naval Research Graduate Fellowship and the Defense Advanced Research Projects Agency under contract N00014-87-K-0825. To appear in Symposium on Parallel Algorithms and Architectures, San Diego, California, June 1992. ACM.  
Address: Cambridge, Massachusetts 02139  
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: In this paper, we discuss the use of multipath multistage interconnection networks (MINs) in the design of a fault-tolerant parallel computer. Multipath networks have multiple paths between any input and any output. In particular, we examine networks with either the property of expansion or maximal-fanout. We present an 4 ) lower time bound for a worst-case permutation on deterministic maximal-fanout networks. We further show how a randomized approach to maximal-fanout avoids the regularity from Unlike most previous work, we examine systems which can tolerate node failure and isolation. We describe mechanisms for fault identification and system reconfiguration. In reconfiguring a faulty system, a naive approach is to preserve processing power by maximizing the number of processing nodes left in operation. However, our results show that the synchronization requirements of applications make it critical to eliminate nodes with poor network connections. We find that a conservative fault-propagation algorithm for reconfiguration, adapted from work by Leighton and Maggs [LM92], performs well for all of our multipath networks. We also address some practical issues of network construction and present performance simulations based upon the MIT Transit architecture [DeH90]. Simulation results for 1024 node systems demonstrate that multipath networks, reconfigured with our fault-propagation algorithm, perform well not only in theory, but also in practice. In fact, our systems suffer only a small decrease in performance from network faults; the degradation is linear in the which this worst case arises.
Abstract-found: 1
Intro-found: 1
Reference: [ALM90] <author> S. Arora, T. Leighton, and B. Maggs. </author> <title> On-line algorithms for path selection in a non-blocking network. </title> <booktitle> In Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Bassalygo and Pinsker [BP74] used such interwiring to construct the first non-blocking networks of size O (N log N ) and depth O (log N ). On-line algorithms for non-blocking routing are described in <ref> [ALM90] </ref>. Other work on interwiring multipath networks can be found in [CS82] [RK84]. 2.1 Randomly-Wired Multibutterfly The first network we examine is the randomly-wired multibutter-fly, which randomly interwires within equivalence classes. Upfal first introduced the term multibutterfly in [Upf89]. With high A randomly-wired four-stage multibutterfly connecting 16 endpoints.
Reference: [BP74] <author> L. A. Bassalygo and M. S. Pinsker. </author> <title> Complexity of optimum nonblocking switching networks without re-connections. </title> <journal> Problems of Information Transmission, </journal> <volume> 9 </volume> <pages> 64-66, </pages> <year> 1974. </year>
Reference-contexts: The examples given in this section use a dilation-2 router, one which has two outputs in each logical direction. Interwiring refers to the practice of connecting these redundant outputs to different routers within the appropriate class. Bassalygo and Pinsker <ref> [BP74] </ref> used such interwiring to construct the first non-blocking networks of size O (N log N ) and depth O (log N ). On-line algorithms for non-blocking routing are described in [ALM90].
Reference: [CED92] <author> Frederic Chong, Eran Egozy, and Andre DeHon. </author> <title> Fault tolerance and performance of multipath multistage interconnection networks. </title> <editor> In Thomas F. Knight Jr. and John Savage, editors, </editor> <booktitle> Advanced Research in VLSI and Parallel Systems 1992, </booktitle> <pages> pages 227-242. </pages> <publisher> MIT Press, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: We use isomorphic wirings for these remaining stages of all our multipath networks. The deterministic network, presented earlier in <ref> [CED92] </ref> and shown in Figure 2, uses a regular butterfly topology to achieve maximal-fanout. Instead of using every output as a separate logical direction, the dilation of 2 in our network allows us to use pairs of outputs for each logical direction. <p> Network traffic occurs for two reasons. First, when data reads or writes are neither cached nor in local memory a message must be sent to a processing node whose local memory contains the data. Second, coherency messages are necessary to keep the data in caches consistent. Previous work <ref> [CED92] </ref> examined several shared-memory applications by looking at message traffic taken from [CFKA90]. It was found that traffic consisting uniformly of 24-byte messages was representative of effects observed from those shared-memory applications. Those applications used barrier-synchronization, manipulated 16-byte cache lines and maintained cache coherency. <p> However, performance results presented in this paper were also verified to be qualitatively unchanged under network loading half of that used here. 6 System Partitioning In this section, we describe two methods of system partitioning and compare their performance. Previous work [LLM90] <ref> [CED92] </ref> has concentrated on the fault performance of multipath networks in which every endpoint could communicate with every other endpoint. In a more realistic view, we now examine the performance of systems which can tolerate network failures which isolate endpoints.
Reference: [CFKA90] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-based cache-coherence in large-scale multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, coherency messages are necessary to keep the data in caches consistent. Previous work [CED92] examined several shared-memory applications by looking at message traffic taken from <ref> [CFKA90] </ref>. It was found that traffic consisting uniformly of 24-byte messages was representative of effects observed from those shared-memory applications. Those applications used barrier-synchronization, manipulated 16-byte cache lines and maintained cache coherency. Barrier-synchronization refers to points in an application where all processors must arrive at known points of execution.
Reference: [Com90] <author> IEEE Standards Committee. </author> <title> IEEE Standard Test Access Port and Boundary-Scan Architecture. </title> <publisher> IEEE, </publisher> <address> 345 East 47th Street, New York, NY 10017-2394, </address> <month> July </month> <year> 1990. </year> <note> IEEE Std 1149.1-1990. </note>
Reference-contexts: If no unique choice arises | all ports unused, but all unblocked or all blocked | then the router randomly decides between ports. Each chip also incorporates a serial Test Access Port (TAP) which accesses IEEE boundary scan facilities <ref> [Com90] </ref>. These Equivalence Classes Fanout Classes This figure shows how to achieve maximal-fanout while avoiding regularity. The routers shown are radix-2 and dilation-2. For each stage, we divide each routing equivalence class into an exponentially larger number of fanout classes until fanout classes will no longer fit.
Reference: [CS82] <author> L. Ciminiera and A. Serra. </author> <title> A fault-tolerant connecting network for multiprocessor systems. </title> <booktitle> In Proceedings of the 1982 International Conference on Parallel Processing, </booktitle> <pages> pages 113-122. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1982. </year>
Reference-contexts: Bassalygo and Pinsker [BP74] used such interwiring to construct the first non-blocking networks of size O (N log N ) and depth O (log N ). On-line algorithms for non-blocking routing are described in [ALM90]. Other work on interwiring multipath networks can be found in <ref> [CS82] </ref> [RK84]. 2.1 Randomly-Wired Multibutterfly The first network we examine is the randomly-wired multibutter-fly, which randomly interwires within equivalence classes. Upfal first introduced the term multibutterfly in [Upf89]. With high A randomly-wired four-stage multibutterfly connecting 16 endpoints. Each component in the first three stages is a radix-2, dilation-2 router.
Reference: [D + 92] <author> William J. Dally et al. </author> <title> The message-driven processor: A multicomputer processing node with efficient mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: If the message rate were any higher, the processors would just be stalled more often, and the network loading would not really change. Note that our analysis assumes low latency message handling, a concept demonstrated in the J-Machine <ref> [D + 92] </ref>. If, as with many commercial and research machines, there exists a high latency for message handling, the latency induces a feedback effect which prevents full utilization of the network [JA92].
Reference: [DeH90] <author> Andre DeHon. MBTA: </author> <title> Modular bootstrapping transit architecture. </title> <type> Transit Note 17, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Finally, it re-enables any ports from other chips which lead to still-functioning ports on the suspect chip. Similarly, interchip wiring may be tested for continuity, isolation, and electrical per formance. In order to decrease wire-lengths and machine size, the Transit architecture <ref> [DeH90] </ref> uses the three-dimensional stack packaging scheme shown in Figure 5. Vertical connections are made via button board connectors. Routing components are arranged in columns directly above one another. This suggests an economical method of wiring the diagnostic network. Columns of chips can have their boundary scan pins vertically connected.
Reference: [DeH91a] <author> Andre DeHon. </author> <title> Practical schemes for fat-tree network construction. </title> <editor> In Carlo H. Sequin, editor, </editor> <booktitle> Advanced Research in VLSI: International Conference 1991. </booktitle> <publisher> MIT Press, </publisher> <month> March </month> <year> 1991. </year>
Reference-contexts: In particular, we describe the simulated performance and design principles for the wiring of low-latency fault-tolerant multistage interconnection networks (MINs). Such multistage networks can provide nearly non-blocking interconnect with latency limited primarily by wire propagation delay. Very large networks, constructed in the fat-tree topology [Lei85] <ref> [DeH91a] </ref>, use these MINs as basic building blocks, achieving high interconnection bandwidth. Such networks form the wiring topology of the Thinking Machines CM5 processor array [Thi91]. The key factor in providing fault tolerance in MINs is the addition of multiple independent routing paths within the network.
Reference: [DeH91b] <author> Andre DeHon. </author> <title> RN2 proposal. </title> <type> Transit Note 44, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The system is based upon an architecture under construction by the MIT Transit project. The networks simulated use a circuit-switched routing component based upon the RN1 [MDK91], a custom VLSI chip, and the RN2 <ref> [DeH91b] </ref>, a chip under design. Each chip can act either as a single 8-input, radix-4, dilation-2 router, or as two independent 4-input, radix-4, dilation-1 routers.
Reference: [DeH92] <author> Andre DeHon. </author> <title> Using IEEE-1149 TAP in a fault-tolerant architecture. </title> <type> Transit Note 60, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: The disjoint nature of fanout classes ensures that fanout-trees will have physi cally distinct components. ports, in turn, are connected together in a diagnostic network controlled by a Boundary Scan Controller (BSC) which can provide in-operation diagnostics and chip reconfiguration <ref> [DeH92] </ref>. Each attempt to send a message through the network automatically receives status information from each router involved. If routing errors occur, an accumulation of this status information can be used to identify suspect chips.
Reference: [DKM91] <author> Andre DeHon, Thomas F. Knight Jr., and Henry Min-sky. </author> <title> Fault-tolerant design for multistage routing networks. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing. Information Processing Society of Japan, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: This concentration is most severe in the first and last stages, where each node only has a small number of connections to the network. We now describe methods of coping with this limited network I/O. Many of these ideas were initially explored in <ref> [DKM91] </ref>. 4.1 The First Stage The wiring of the nodes to the inputs of the first-stage routers involves a tradeoff between smoothness of degradation and mean time between machine reconfigurations. Each node has 2 inputs into distinct routers of the first stage of our networks.
Reference: [JA92] <author> Kirk Johnson and Anant Agarwal. </author> <title> The impact of communication locality on large-scale multiprocessor performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Queens-land, Australia, </address> <month> May </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: Note that our analysis assumes low latency message handling, a concept demonstrated in the J-Machine [D + 92]. If, as with many commercial and research machines, there exists a high latency for message handling, the latency induces a feedback effect which prevents full utilization of the network <ref> [JA92] </ref>. Although cache miss and message locality numbers are open to debate, we observe that the technological trends of multiple-issue processors and wider cache lines will only increase demands on the network.
Reference: [Lei85] <author> Charles E. Leiserson. Fat-trees: </author> <title> Universal networks for hardware efficient supercomputing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):892-901, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: In particular, we describe the simulated performance and design principles for the wiring of low-latency fault-tolerant multistage interconnection networks (MINs). Such multistage networks can provide nearly non-blocking interconnect with latency limited primarily by wire propagation delay. Very large networks, constructed in the fat-tree topology <ref> [Lei85] </ref> [DeH91a], use these MINs as basic building blocks, achieving high interconnection bandwidth. Such networks form the wiring topology of the Thinking Machines CM5 processor array [Thi91]. The key factor in providing fault tolerance in MINs is the addition of multiple independent routing paths within the network.
Reference: [Len88] <author> Daniel E. Lenoski. </author> <title> A highly integrated, fault-tolerant minicomputer: The nonstop CLX. </title> <booktitle> In Digest of Papers-Compcon Spring 88: Intellectual Leverage, </booktitle> <pages> pages 515-519, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: On the other hand, we can trade some time overhead for hardware overhead by using redundant processors. We can use redundant processing nodes to shadow the computation of the processors doing the actual work <ref> [Len88] </ref> [Web90]. Actually, we need not have entirely redundant processors; we can just have redundant processes spread across all the nodes. Each node can have some "real" work and some redundant work, as long as all of the redundant processes are for work being done on other nodes.
Reference: [LLM90] <author> Tom Leighton, Derek Lisinski, and Bruce Maggs. </author> <title> Empirical evaluation of randomly-wired multistage networks. </title> <booktitle> In International Conference on Computer Design: VLSI in Computers and Processors. IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: However, performance results presented in this paper were also verified to be qualitatively unchanged under network loading half of that used here. 6 System Partitioning In this section, we describe two methods of system partitioning and compare their performance. Previous work <ref> [LLM90] </ref> [CED92] has concentrated on the fault performance of multipath networks in which every endpoint could communicate with every other endpoint. In a more realistic view, we now examine the performance of systems which can tolerate network failures which isolate endpoints.
Reference: [LM89] <author> Tom Leighton and Bruce Maggs. </author> <title> Expanders might be practical: fast algorithms for routing around faults on multibutterflies. </title> <booktitle> In IEEE 30th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: Multistage networks with the property of expansion have been shown, in theory, to possess substantial fault tolerance and performance [Upf89] [LM92]. Our random inter-wirings, shown in Figure 1, are based upon the wiring presented by Leighton and Maggs <ref> [LM89] </ref>. 2.2 Deterministic Maximal-Fanout The other two networks we shall examine have the property of maximal-fanout rather than that of expansion. Maximal-fanout guarantees that the paths between any two endpoints will use as many distinct routers as possible.
Reference: [LM92] <author> Tom Leighton and Bruce Maggs. </author> <title> Fast algorithms for routing around faults in multibutterflies and randomly-wired splitter networks. </title> <journal> IEEE Transactions on Computing, </journal> <volume> 41(5) </volume> <pages> 1-10, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Multistage networks with the property of expansion have been shown, in theory, to possess substantial fault tolerance and performance [Upf89] <ref> [LM92] </ref>. Our random inter-wirings, shown in Figure 1, are based upon the wiring presented by Leighton and Maggs [LM89]. 2.2 Deterministic Maximal-Fanout The other two networks we shall examine have the property of maximal-fanout rather than that of expansion. <p> To aid the routing of messages, each component will have a pin dedicated to calculating flow control information according to the following blocking criterion taken from Leighton and Maggs in <ref> [LM92] </ref>. A router is blocked if it does not have at least one unused, operational output port in each logical direction which leads to a router which is not blocked. <p> Although fewer nodes are available to perform the task, we will no longer be limited by the speed of some of the weakest excluded nodes. Leighton and Maggs proved in <ref> [LM92] </ref> the following theorem: Theorem 2: No matter how an adversary chooses k faults in an N -input, N -output network with expansion, there exist at least N O (k) inputs and N O (k) outputs between which any permutation can be routed in O (log N ) router cycles. <p> Unlike most previous work, we have examined systems which can tolerate node failure and isolation. We have demonstrated that a simplified version of an algorithm from Leighton and Maggs <ref> [LM92] </ref> effectively determines which nodes to use in the presence of network failure. This fault-propagation algorithm is effective not only on multibutterflies, but also on other multipath networks such as maximal-fanout networks. Of particular importance is the quality of network connections available to each processing node.
Reference: [MDK91] <author> Henry Minsky, Andre DeHon, and Thomas F. Knight Jr. RN1: </author> <title> Low-latency, dilated, crossbar router. In Hot Chips Symposium III, </title> <year> 1991. </year>
Reference-contexts: The system is based upon an architecture under construction by the MIT Transit project. The networks simulated use a circuit-switched routing component based upon the RN1 <ref> [MDK91] </ref>, a custom VLSI chip, and the RN2 [DeH91b], a chip under design. Each chip can act either as a single 8-input, radix-4, dilation-2 router, or as two independent 4-input, radix-4, dilation-1 routers.
Reference: [RK84] <editor> S.M. Reddy and V.P. Kumar. </editor> <title> On fault-tolerant multistage interconnection networks. </title> <booktitle> In Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 155-165. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1984. </year>
Reference-contexts: Bassalygo and Pinsker [BP74] used such interwiring to construct the first non-blocking networks of size O (N log N ) and depth O (log N ). On-line algorithms for non-blocking routing are described in [ALM90]. Other work on interwiring multipath networks can be found in [CS82] <ref> [RK84] </ref>. 2.1 Randomly-Wired Multibutterfly The first network we examine is the randomly-wired multibutter-fly, which randomly interwires within equivalence classes. Upfal first introduced the term multibutterfly in [Upf89]. With high A randomly-wired four-stage multibutterfly connecting 16 endpoints. Each component in the first three stages is a radix-2, dilation-2 router.
Reference: [SW89] <author> A. Prasad Sistla and Jennifer L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the Eighth Annual Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223-238, </pages> <address> Edmonton, Alberta, Canada, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Several approaches are available: checkpointing, redundant processors, and evacuating state. Checkpointing computation is a well-known method of tolerating processor loss. Secondary storage devices would be attached to some of the network's endpoints. There are various types of checkpointing [SY85] <ref> [SW89] </ref> involving various levels of optimism.
Reference: [SY85] <author> Robert E. Strom and Shaula Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Several approaches are available: checkpointing, redundant processors, and evacuating state. Checkpointing computation is a well-known method of tolerating processor loss. Secondary storage devices would be attached to some of the network's endpoints. There are various types of checkpointing <ref> [SY85] </ref> [SW89] involving various levels of optimism.
Reference: [Thi91] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM5 Technical Summary, </type> <month> October </month> <year> 1991. </year>
Reference-contexts: Very large networks, constructed in the fat-tree topology [Lei85] [DeH91a], use these MINs as basic building blocks, achieving high interconnection bandwidth. Such networks form the wiring topology of the Thinking Machines CM5 processor array <ref> [Thi91] </ref>. The key factor in providing fault tolerance in MINs is the addition of multiple independent routing paths within the network. With independent routes, failures of components, wiring, or power in one path may leave a second path undamaged.
Reference: [Upf89] <author> E. Upfal. </author> <title> An O(log N ) deterministic packet routing scheme. </title> <booktitle> In 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 241-250. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: On-line algorithms for non-blocking routing are described in [ALM90]. Other work on interwiring multipath networks can be found in [CS82] [RK84]. 2.1 Randomly-Wired Multibutterfly The first network we examine is the randomly-wired multibutter-fly, which randomly interwires within equivalence classes. Upfal first introduced the term multibutterfly in <ref> [Upf89] </ref>. With high A randomly-wired four-stage multibutterfly connecting 16 endpoints. Each component in the first three stages is a radix-2, dilation-2 router. To prevent any unique critical paths between endpoints, the last stage is composed of radix-2, dilation-1 routers (see Section 4.2). <p> Multistage networks with the property of expansion have been shown, in theory, to possess substantial fault tolerance and performance <ref> [Upf89] </ref> [LM92]. Our random inter-wirings, shown in Figure 1, are based upon the wiring presented by Leighton and Maggs [LM89]. 2.2 Deterministic Maximal-Fanout The other two networks we shall examine have the property of maximal-fanout rather than that of expansion.
Reference: [VR88] <author> A. Varma and B.D. Rathi. </author> <title> A fault-tolerant routing scheme for unique-path multistage interconnnection networks. </title> <type> Technical Report IBM RC 13441, </type> <institution> IBM Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Such a scheme allows any node to communicate with another as long as they are connected in the transitive closure of node-to-node connections. The multi-hop protocol is an end-to-end variation of a similar scheme which was proposed in <ref> [VR88] </ref>. 1024-node (5-stage) systems based on randomly-wired networks. Recall that in order for the system to remain in operation, all live nodes must be able to communicate with each other.
Reference: [Web90] <author> Steve Webber. </author> <title> The Stratus architecture. </title> <type> Stratus Technical Report TR-1, </type> <institution> Status Computer, Inc., </institution> <address> 55 Fair-banks Blvd., Marlboro, Massachusetts 01752, </address> <year> 1990. </year>
Reference-contexts: On the other hand, we can trade some time overhead for hardware overhead by using redundant processors. We can use redundant processing nodes to shadow the computation of the processors doing the actual work [Len88] <ref> [Web90] </ref>. Actually, we need not have entirely redundant processors; we can just have redundant processes spread across all the nodes. Each node can have some "real" work and some redundant work, as long as all of the redundant processes are for work being done on other nodes.
References-found: 26

