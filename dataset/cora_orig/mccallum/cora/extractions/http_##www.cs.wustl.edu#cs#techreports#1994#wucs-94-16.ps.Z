URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-16.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: Trading Packet Headers for Packet Processing  
Author: George Varghese 
Date: June 29, 1994  
Abstract: In high speed networks, packet processing is relatively expensive while bandwidth is cheap. This begs the question: what fields can be added to packets to make packet processing easier? By exploring this question, we devise a number of novel mechanisms to speed up packet processing. With the advent of new standards for the Data Link, Network, and Transport layers, we believe there is an opportunity to apply these techniques to improve the performance of real protocols. First, we suggest adding a data manipulation header to an easily accessible portion of each packet. This header contains pointers to fields (in various layers) required for data manipulation. This information allows implementations to efficiently combine data manipulation steps (e.g., encryption and copying) in a structured fashion. Second, we suggest adding index fields to protocol identifiers at all layers (e.g.,, connection identifiers, network addresses, DSAPs) to reduce lookup costs and generic protocol processing. Several new ideas to utilize these index fields (threaded indexing, index passing, and source hashing) are proposed. Virtual Circuit Identifiers (VCIs) have been long used to simplify lookup and packet processing in virtual circuit. In source hashing and threaded indexing, the added indices essentially serve as VCIs, but for flows in a datagram network. In source hashing, for example, the "VCI" is a consistent random label chosen by the source. Our new methods provide the benefits of normal VCIs without requiring a round trip delay for set up. Our methods can lower worst case datagram lookup times from O(log(n)) to O(1), which may be important for Gigabit routers. 
Abstract-found: 1
Intro-found: 1
Reference: [AP93] <author> M. Abbott and L. Peterson. </author> <title> Increasing Network Throughput by Integrating Protocol Layers. </title> <journal> ACM Transactions on Networking, </journal> <volume> 1(5), </volume> <month> October </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Current implementations are handcrafted and combine processing at all layers; hence they have access to the information required for data manipulation. However, the diversity of protocol stacks and the difficulty of maintenance makes it desirable to provide more structured access to this information. Recently, Abbott and Peterson <ref> [AP93] </ref> have suggested a general technique to integrate protocol processing while preserving modularity. They also suggest adding a pointer to the start of the application data (in a lower layer header) in order to avoid the need for sequential parsing. <p> Summary of Previous Work and Paper Organization In summary, valuable work has been done on data manipulation and Integrated Layer Processing, but most existing implementations are handcrafted. It is desirable to have a structured and general <ref> [AP93] </ref> approach to this problem. Similarly, many existing techniques for reducing lookup times and packet processing depend on traffic assumptions, and do not guarantee worst-case improvements. Techniques like VCI assignment (in Virtual Circuit networks) require reliable set-up protocols that add complexity and latency to data transfer. <p> On the other hand, adding a DMH corresponds to partially "compiling" the protocol suite. Our Data Manipulation Header can be considered as a generalization of the technique of <ref> [AP93] </ref> in which a single pointer (to the start of the application data) is added to a lower layer header. Our scheme is more general because it allows a set of regions to be demarcated. <p> We believe our structured technique of using accumulated offsets to calculate regions is also useful. On the other hand, we have not addressed a number of other issues relating to modularity that are well treated in <ref> [AP93] </ref>. In [AP93], modularity is preserved by "automatically synthesizing 7 the integrated implementation from independently expressed protocols." This is an important idea, and is equally applicable to protocols that use a Data Manipulation Header. 4 Lookup techniques In the next three sections we concentrate on techniques to reduce lookup costs. 4.1 <p> We believe our structured technique of using accumulated offsets to calculate regions is also useful. On the other hand, we have not addressed a number of other issues relating to modularity that are well treated in <ref> [AP93] </ref>. In [AP93], modularity is preserved by "automatically synthesizing 7 the integrated implementation from independently expressed protocols." This is an important idea, and is equally applicable to protocols that use a Data Manipulation Header. 4 Lookup techniques In the next three sections we concentrate on techniques to reduce lookup costs. 4.1 Source Hashing
Reference: [BN84] <author> A. Birrell and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1), </volume> <month> February </month> <year> 1984. </year>
Reference-contexts: However, it is not useful for RPC Transport protocols (Section 4.1) which send data packets without waiting for a handshake. Thus source hashing is most useful for timer-based transport protocols [Tan81] that avoid the extra round-trip latency. Such protocols are crucial for lightweight RPC implementations <ref> [BN84] </ref>. In Birrell and Nelson's RPC [BN84], each RPC call packet sent by the source carries a unique identifier (a unique network wide identifier of the process that initiates the call concatenated with a unique sequence number). <p> Thus source hashing is most useful for timer-based transport protocols [Tan81] that avoid the extra round-trip latency. Such protocols are crucial for lightweight RPC implementations <ref> [BN84] </ref>. In Birrell and Nelson's RPC [BN84], each RPC call packet sent by the source carries a unique identifier (a unique network wide identifier of the process that initiates the call concatenated with a unique sequence number). <p> Unfortunately, we require a distributed algorithm to assign unique indexes each time the topology changes. 5 4 There are some design details that need elaboration; for example, we need to deal with source and destination crashes as described in <ref> [BN84] </ref>. 5 Source hashing offers one approach. Each time a new end node comes up at a router, the router could pick a random index for the endnode and broadcast this proposed index to other routers. On collisions, it could retry this procedure.
Reference: [BP93] <author> D. Banks and M. Prudence. </author> <title> A high-performance network architecture for a pa-risc workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <month> February </month> <year> 1993. </year>
Reference-contexts: It is desirable to avoid data copying by having packets be copied directly from the network to their final destination in application space <ref> [KLS86, CJRS89, BP93] </ref>. It seems desirable to have two regions for data copying. A memory destination region which is a field that can be used to find the location in memory for the data bytes, and another region to demarcate the data bytes themselves.
Reference: [CJRS89] <author> D.D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: They can be added to existing protocols without major changes. We emphasize that this paper is not about new protocols but about new techniques that can be profitably added to protocols, both new and old. Clark et al <ref> [CJRS89] </ref> argue that clever implementation of existing protocol suites (e.g., TCP/IP) can provide high throughput at the transport layer, using reasonably cheap CPUs and large packet sizes. <p> So why not stay with existing protocols (and their headers) but focus on clever implementation? There are three reasons to continue to look for new techniques. First, the measurements of <ref> [CJRS89] </ref> were at the transport level and did not consider application throughput. Second, as we scale to Gigabit/sec and Terabit/sec, it appears that their techniques, while valuable, will not be enough. <p> Clark and Tennehouse [CT90] call this technique (i.e., combining data manipulations) Integrated Layer Processing. For example, speedups have been reported by doing the checksum and copy operations for TCP at the same time <ref> [CJRS89] </ref>. Another idea, used to reduce the number of copies made of the data, is to add buffer names [KLS86],to packets; the names provided a pointer to the packet's destination in memory. <p> It is hard to characterize the cost of lookups because they are implementation dependent. For instance, hashing by exclusive-OR folding of address octets seems effective and easy to implement [Jai92]. In <ref> [CJRS89] </ref>, the cost of hashing (in the best case) is taken to be 25 instructions. However, we believe the lookup techniques described in this paper are useful because: 1) Our techniques do not depend on traffic patterns. 2) The lookup costs accumulated over several layers do add up. <p> An existing technique to reduce the cost of state lookups is caching [Par93, Jai92]. A generic technique used to reduce generic protocol processing is Header Prediction <ref> [CJRS89] </ref>. These existing techniques depend on traffic patterns ([Jai86]) and do not guarantee performance improvement. By contrast, our schemes use extra packet headers to provide performance improvements that are independent of traffic patterns. <p> It is desirable to avoid data copying by having packets be copied directly from the network to their final destination in application space <ref> [KLS86, CJRS89, BP93] </ref>. It seems desirable to have two regions for data copying. A memory destination region which is a field that can be used to find the location in memory for the data bytes, and another region to demarcate the data bytes themselves. <p> Then NCW will provide poor performance to both streams but OCW will guarantee that one of the two streams will get good performance. NCW has the interesting property that it will never behave worse than a cache of size 1 (as is used in say header prediction <ref> [CJRS89] </ref>) but it is highly probable (assuming only that the number of simultaneous packet streams is much smaller than the lookup table) than the performance will be much better. <p> Sources can use pseudorandom numbers, preferably with seeds that depend on a unique 2 In header prediction <ref> [CJRS89, Par93] </ref> the list is of size 1 10 source address. Source hashing does not require truly random random numbers that meet all statistical tests. To save computing time, a small queue of random numbers can be computed when a source first comes up.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: The major disadvantage with this approach is the extra latency required to set up VCIs. There are several hardware techniques for lookups including tree based searches (trie and binary search schemes <ref> [CLR90] </ref>) and Content Addressable Memories (CAMs). However, special hardware is expensive and inflexible; tree based schemes that require a logarithmic number of probes to memory do not scale well because memory is a bottleneck. <p> Simple probabilistic analysis suffices to understand the behavior of source hashing, similar to the analyses done for hashing with chaining <ref> [CLR90] </ref>. The probability of a collision depends on the size of the hash table and the number of concurrent packet streams. We omit the analysis for lack of space. The typical analyses for hashing assume hashed values are uniformly distributed. <p> We can use source hashing to improve the lookup performance of such a transport as follows. First, we modify the lookup table in Figure 2 such that the lookup entries point to a list of pointer blocks much as in hashing with chaining <ref> [CLR90] </ref>. Each pointer block contains a pointer to a state table entry; state table entries contain a Process ID, the last sequence number for that process and any other state.
Reference: [CT90] <author> D.D. Clark and D.L. Tennehouse. </author> <title> Architecural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of the ACM SIGCOMM '90 Symposium, </booktitle> <pages> pages 200-208, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: This trend could exacerbate lookup times required to retrieve context at 2 various layers. Current industry bridges and routers seem to be pushing the limits of technology to provide reasonable cost lookups of packets at link rates. * Networks are becoming more diverse: To quote from <ref> [CT90] </ref>: "protocols of tomorrow must operate over the range of coming network technology" and "a single end system will be expected to support applications that orchestrate a wide range of media . . . and access patterns." For example, there are already several standards for high speed Data Links (e.g., FDDI, <p> We would also like structured approaches to improve protocol performance that have the benefits of techniques like Integrated Layer Processing without their disadvantages. 2 Previous Work and Paper Organization In <ref> [CT90] </ref>, a distinction is made between two kinds of protocol processing: data manipulation (functions that read or modify data) and control (functions that regulate the transfer of data). The main data manipulation functions are copying data, checksumming, encryption and presentation formatting. <p> A sequence of data manipulations that involve x separate passes through the data bytes is often 3 limited to a throughput of B=x where B is the speed of the endsystem bus. To quote from <ref> [CT90] </ref>: ... data manipulations ... are usually thought of as distinct operations, since they occur in different layers. But this is not necessarily the most efficient design approach. <p> With current RISC chips, which pay a very high cost to read or write memory, it is more efficient to read the data once and perform as many manipula tions as possible while holding the data in cache or registers. Clark and Tennehouse <ref> [CT90] </ref> call this technique (i.e., combining data manipulations) Integrated Layer Processing. For example, speedups have been reported by doing the checksum and copy operations for TCP at the same time [CJRS89]. <p> Another idea, used to reduce the number of copies made of the data, is to add buffer names [KLS86],to packets; the names provided a pointer to the packet's destination in memory. Clark and Tennehouse <ref> [CT90] </ref> generalize this idea to Application Level Data Units (ADUs) and suggest that the sender add information about the eventual location of each ADU. <p> A memory destination region which is a field that can be used to find the location in memory for the data bytes, and another region to demarcate the data bytes themselves. It may also be desirable to have regions that demarcate regions that require presentation conversion <ref> [CT90] </ref> and specify the type of conversion required. However, it is infeasible to describe the conversions required for the general case (for example in an RPC application where each parameter has a separate type) using a single type field. <p> It also allows various portions of the data to be dispatched to separate processors in a multiprocessor system without requiring a sequential parsing of layer headers. <ref> [CT90] </ref> points out that the traditional approach corresponds to "interpreting" the protocol suite. On the other hand, adding a DMH corresponds to partially "compiling" the protocol suite.
Reference: [DKS89] <author> Alan Demers, Srinivasan Keshav, and Scott Shenker. </author> <title> Analysis and simulation of a fair queueing algorithm. </title> <booktitle> Proceedings of the Sigcomm '89 Symposium on Communications Archtectures and Protocols, </booktitle> <volume> 19(4) </volume> <pages> 1-12, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In some cases, in a datagram network the source may wish to provide finer discrimination for flows than the implicit flows defined by source-destination addresses. One important problem in datagram networks is to provide fair treatment <ref> [DKS89] </ref> to flows so that badly-behaved flows cannot penalize other flows: at the minimum we wish for firewalls between flows. <p> Nagle proposed identifying flows using source-destination addresses and then using separate output queues for each flow; the queues are then serviced in round-robin fashion. Nagle's scheme was later refined and generalized under the category of fair queueing <ref> [DKS89] </ref> and virtual clock [Zha91] schemes. With fair queueing, for example, it is possible for each flow in Figure 3 to receive 1=5 of the bandwidth of link L. Source hashing offers a simple alternative to Nagle's scheme of discriminating flows by source-destination addresses.
Reference: [DNA82] <author> Digital Equipment Corporation. </author> <title> DECnet Digital Network Architecture (Phase IV) General Description, 1982. Order No. </title> <publisher> AA-N149A-TC. </publisher>
Reference-contexts: Endnode caches are used by many routing layer protocols (e.g., IP, OSI [Per92]) in order to fill in information required to send a packet. Some routing protocols (e.g., DECNET Phase IV <ref> [DNA82] </ref>) even require updating this cache when packets are received. [Par93] describes a simple idea in which the transport protocol (which keeps state anyway per connection) keeps an index into the routing cache.
Reference: [Jai86] <author> Raj Jain. </author> <title> Packet trains: Measurements and a new model for computer traffic. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 1162-1167, </pages> <month> May </month> <year> 1986. </year>
Reference: [Jai92] <author> Raj Jain. </author> <title> A comparison of hashing schemes for address lookups in computer networks. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-40(10):1570-1573, </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: It is hard to characterize the cost of lookups because they are implementation dependent. For instance, hashing by exclusive-OR folding of address octets seems effective and easy to implement <ref> [Jai92] </ref>. In [CJRS89], the cost of hashing (in the best case) is taken to be 25 instructions. However, we believe the lookup techniques described in this paper are useful because: 1) Our techniques do not depend on traffic patterns. 2) The lookup costs accumulated over several layers do add up. <p> An existing technique to reduce the cost of state lookups is caching <ref> [Par93, Jai92] </ref>. A generic technique used to reduce generic protocol processing is Header Prediction [CJRS89]. These existing techniques depend on traffic patterns ([Jai86]) and do not guarantee performance improvement. By contrast, our schemes use extra packet headers to provide performance improvements that are independent of traffic patterns.
Reference: [KLS86] <author> N.P. Kronenberg, H. Levy, and W.D. Strecker. VAXClusters: </author> <title> a closely coupled distributed system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2), </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: It is desirable to avoid data copying by having packets be copied directly from the network to their final destination in application space <ref> [KLS86, CJRS89, BP93] </ref>. It seems desirable to have two regions for data copying. A memory destination region which is a field that can be used to find the location in memory for the data bytes, and another region to demarcate the data bytes themselves.
Reference: [Lam84] <author> Butler Lampson. </author> <title> Hints for computer system design. </title> <journal> IEEE Software, </journal> <volume> 1 </volume> <pages> 11-29, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: To implement this in a structured way, when transport calls routing it includes the index (say i) in the call as well as the destination address (say D). This index is used as a hint <ref> [Lam84] </ref> by routing. Routing uses i to lookup its cache. If the entry contains information about D that information is used; otherwise a normal search of the cache is done to locate the true index.
Reference: [Nag84] <author> John Nagle. </author> <title> Congestion control in TCP/IP internetworks. </title> <journal> Computer Communication Review, </journal> <volume> 14(4), </volume> <month> October </month> <year> 1984. </year>
Reference-contexts: This is sometimes called the parking lot problem because of its similarity to a crowded parking lot with one exit. Most datagram networks do not discriminate flows and so this potential unfairness exists. Nagle <ref> [Nag84] </ref> proposed an approximate solution to this problem for datagram networks by having routers discriminate flows and then providing round-robin service to flows for every bottlenecked resource.
Reference: [Par93] <author> Craig Partridge. </author> <title> Gigabit Networking. </title> <publisher> Addison-Wesley, </publisher> <address> Reading,MA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Networks are getting faster because of advances in transmission and switching. Optical fibers, for instance, allow raw bandwidths of up to a Terabit/sec while Gigabit switching systems are fl Washington University in St. Louis. being prototyped <ref> [Par93] </ref>. CPUs are certainly getting faster, but the rate at which CPUs are getting faster does not match the bandwidth increase rate. CPU processing is also limited by other factors such as caches, memories, busses, and disks. Thus packet processing appears to be the bottleneck while network bandwidth is inexpensive. <p> Third, significantly reduced packet processing overhead is always useful: it allows the CPU to do other things and reduces the packet size required for Gigabit throughput. With the need for efficient packet processing in Gigabit networks, there is also an opportunity. Protocol standards are in transition <ref> [Par93] </ref> to accommodate high speeds and new services. Data Link standards are being affected by emerging ATM specifications; in the near future, wavelength division multiplexing may prove successful. <p> must operate over the range of coming network technology" and "a single end system will be expected to support applications that orchestrate a wide range of media . . . and access patterns." For example, there are already several standards for high speed Data Links (e.g., FDDI, HIPPI, Fiber Channel <ref> [Par93] </ref>) besides the ATM standards. * Network Software will continue to be hard to implement and maintain: Since crucial portions of networking software often reside in the operating system kernel, designing and maintaining this software is difficult. <p> Without the techniques in this paper, lookups have a worst case cost of O (log n), where n is the number of state table entries. To 4 quote <ref> [Par93] </ref>, lookups "represent a very large faction of the cost of protocol processing, and finding ways to minimize lookup costs are important." Similarly, generic techniques to reduce packet processing are also important. It is hard to characterize the cost of lookups because they are implementation dependent. <p> An existing technique to reduce the cost of state lookups is caching <ref> [Par93, Jai92] </ref>. A generic technique used to reduce generic protocol processing is Header Prediction [CJRS89]. These existing techniques depend on traffic patterns ([Jai86]) and do not guarantee performance improvement. By contrast, our schemes use extra packet headers to provide performance improvements that are independent of traffic patterns. <p> Sources can use pseudorandom numbers, preferably with seeds that depend on a unique 2 In header prediction <ref> [CJRS89, Par93] </ref> the list is of size 1 10 source address. Source hashing does not require truly random random numbers that meet all statistical tests. To save computing time, a small queue of random numbers can be computed when a source first comes up. <p> We will report the results of some simple simulation experiments in the final paper. Assigning Flow IDs in Datagram Networks In a virtual circuit network, a virtual circuit identifier (VCI) can be used <ref> [Par93] </ref> to distinguish flows at the cost of a set up delay. VCIs are typically small integers that can be easily looked using a simple lookup table. <p> Because of these advantages, many similar proposals have been made <ref> [Par93] </ref> to assign flow IDs to flows in a datagram network by enhancing an existing internet protocol like IP. A strawman paper for assigning flow IDs presented in [Par93] is as follows. <p> Because of these advantages, many similar proposals have been made <ref> [Par93] </ref> to assign flow IDs to flows in a datagram network by enhancing an existing internet protocol like IP. A strawman paper for assigning flow IDs presented in [Par93] is as follows. The source picks a unique (easily done by concatenating the source unique ID with an ID that is unique per flow at the source) flow ID for the flow and sends regular IP datagrams. <p> The strawman proposal for assigning flow IDs has many problems; for instance, it does not address issues like heterogeneous receivers in a multicast flow, advance booking of flows etc. <ref> [Par93] </ref>. Thus we believe the application of source hashing to this problem is an interesting research area. Note also that the simple strawman proposal is a way to reduce packet processing for ordinary IP datagrams, where the IP header itself is the flow spec. <p> Endnode caches are used by many routing layer protocols (e.g., IP, OSI [Per92]) in order to fill in information required to send a packet. Some routing protocols (e.g., DECNET Phase IV [DNA82]) even require updating this cache when packets are received. <ref> [Par93] </ref> describes a simple idea in which the transport protocol (which keeps state anyway per connection) keeps an index into the routing cache.
Reference: [Per92] <author> Radia Perlman. </author> <title> Interconnections. </title> <publisher> Addison-Wesley, </publisher> <address> Reading,MA, </address> <year> 1992. </year>
Reference-contexts: R chooses its threaded index as the index (i.e., j) of its minimum cost neighbor (i.e., B). We give a quick example of how to modify an existing routing protocol to calculate threaded indices. Many routing protocols (e.g., RIP) are based on the Bellman-Ford algorithm <ref> [Tan81, Per92] </ref>. The modifications required for Bellman-Ford are illustrated in Figure 6. Normally each router sends its shortest cost to each destination D to all its neighbors. A router R calculates its shortest path to D by taking the minimum of the cost to D through each neighbor. <p> The idea is to modify the the header information passed between layers to reduce lookup costs. As an example, consider what is required to reduce the cost of looking up the endnode cache in Figure 5. Endnode caches are used by many routing layer protocols (e.g., IP, OSI <ref> [Per92] </ref>) in order to fill in information required to send a packet.
Reference: [Shr94] <author> M. Shreedhar and G. Varghese. </author> <title> Efficient Fair Queuing using Deficit Round Robin Computer Science Technical Report WUCS-94-17, </title> <address> Washington University, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Note that sources can obtain an arbitrarily fine level of discrimination for flows by assigning different random indexes for each flow. Simple round-robin servicing (as suggested by Nagle) does not provide perfectly fair service because the packet sizes used by the different flows could be different. In <ref> [Shr94] </ref> we show how to overcome this deficiency by a technique called Deficit Round Robin. Details can be found in [Shr94]. Load Balancing 12 Another use for consistent random labels is in load balancing as shown in Figure 4. <p> Simple round-robin servicing (as suggested by Nagle) does not provide perfectly fair service because the packet sizes used by the different flows could be different. In <ref> [Shr94] </ref> we show how to overcome this deficiency by a technique called Deficit Round Robin. Details can be found in [Shr94]. Load Balancing 12 Another use for consistent random labels is in load balancing as shown in Figure 4. In this scenario, two routers connect two high speed speed LANs (e.g., 100 mbit/sec) using lower speed (say 1 Mbit/sec T1 lines) trunk lines.
Reference: [SRC84] <author> J.H. Saltzer, D.P. Reed, and D.D. Clark. </author> <title> End-to-end arguments in system design. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4), </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: This simple structured technique of accumulating offsets deals with variable size headers and allows layer implementations to be changed easily. We turn to specific data manipulations. Checksumming of transport layer data is used as an end-to-end check <ref> [SRC84] </ref> against undetected corruption. Checksumming only requires the checksum region to be demarcated. A reasonable solution for end-to-end data integrity and privacy is encryption at the transport layer.
Reference: [Tan81] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1981. </year>
Reference-contexts: A generic technique used to reduce generic protocol processing is Header Prediction [CJRS89]. These existing techniques depend on traffic patterns ([Jai86]) and do not guarantee performance improvement. By contrast, our schemes use extra packet headers to provide performance improvements that are independent of traffic patterns. Another common technique <ref> [Tan81] </ref> used in virtual circuits is to first set up a virtual circuit identifier (VCI) for a packet flow; all packets with the same VCI can be processed similarly. The major disadvantage with this approach is the extra latency required to set up VCIs. <p> Once again this is a generalization of header prediction to allow several simultaneous flows to be processed efficiently. Timer-Based Transport Protocols Source hashing is also applicable to looking up state in transport protocols. Transport protocols that use a handshake to set up connection <ref> [Tan81] </ref> can use Round-Trip Index Passing. The general idea is that both endpoints pass indices to each other. Each endpoint stores the index of the other endpoint and places this index in any subsequent packets sent to the endpoint. <p> This delay is no problem for Transport protocols that use 3-way handshakes to set up a connection. However, it is not useful for RPC Transport protocols (Section 4.1) which send data packets without waiting for a handshake. Thus source hashing is most useful for timer-based transport protocols <ref> [Tan81] </ref> that avoid the extra round-trip latency. Such protocols are crucial for lightweight RPC implementations [BN84]. <p> On collisions, it could retry this procedure. However, the details are complex. 15 However, if we look at Virtual Circuit Identifiers (VCI's), we realize <ref> [Tan81] </ref> that a VCI is not a unique network wide index; instead a VCI is only unique per network link. <p> R chooses its threaded index as the index (i.e., j) of its minimum cost neighbor (i.e., B). We give a quick example of how to modify an existing routing protocol to calculate threaded indices. Many routing protocols (e.g., RIP) are based on the Bellman-Ford algorithm <ref> [Tan81, Per92] </ref>. The modifications required for Bellman-Ford are illustrated in Figure 6. Normally each router sends its shortest cost to each destination D to all its neighbors. A router R calculates its shortest path to D by taking the minimum of the cost to D through each neighbor.
Reference: [Zha91] <author> Lixia Zhang. </author> <title> Virtual clock: A new traffic control algorithm for packet switched networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 101-125, </pages> <month> May </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The applications will also use variations of the basic structure of Figure 2. Consistent Random Labels and Fair Queueing In the rest of the paper, we use the term flow <ref> [Zha91] </ref> to denote a stream of packets that traverse the same route from a source to a destination and have similar processing requirements. <p> Nagle proposed identifying flows using source-destination addresses and then using separate output queues for each flow; the queues are then serviced in round-robin fashion. Nagle's scheme was later refined and generalized under the category of fair queueing [DKS89] and virtual clock <ref> [Zha91] </ref> schemes. With fair queueing, for example, it is possible for each flow in Figure 3 to receive 1=5 of the bandwidth of link L. Source hashing offers a simple alternative to Nagle's scheme of discriminating flows by source-destination addresses.
References-found: 19

