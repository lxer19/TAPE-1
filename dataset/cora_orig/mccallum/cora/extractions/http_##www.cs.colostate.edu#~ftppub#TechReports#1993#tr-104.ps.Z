URL: http://www.cs.colostate.edu/~ftppub/TechReports/1993/tr-104.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Title: A Comparison of Explicit and Implicit Programming Styles for Distributed Memory Multiprocessors  
Affiliation: Department of Computer Science  Colorado State University  
Abstract: Matthew Haines and Wim Bohm Technical Report CS-93-104 March 30, 1993 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Shared memory for distributed memory multiprocessors. </title> <institution> Technical Report Rice COMP TR89-91, Rice University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Strict functional programming languages can also be restrictive in terms of expressibility, sometimes requiring complex and convoluted code to perform simple tasks. Another area of research that offers a language-independent shared memory paradigm is Distributed Shared Memory <ref> [1, 9, 12] </ref>. However, the inability to couple parallel tasks tightly with the distribution of data, controlled implicitly by the operating system, can result in misalignment, causing excessive message passing.
Reference: [2] <author> Wim Bohm, J.C. Browne, David Forslund, Andre Goforth, Ken Kennedy, and James McGraw. </author> <title> Politically incorrect languages for supercomputers a panel discussion. </title> <booktitle> In Proceedings of Supercomputing 92, </booktitle> <pages> pages 704-706. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: These measures are clearly subjective, but given the state-of-the-art of parallel software engineering, the best we can provide. We measure the execution time and storage use of our programs. In the panel discussion at Supercomputing 92 <ref> [2] </ref>, one of the authors claimed that it is considerably easier to write parallel programs for distributed memory multiprocessors in an implicit style rather than in an explicit style, and that the implicit style does not need to suffer from an overwhelming loss of efficiency. This paper quantifies these claims.
Reference: [3] <author> David Cann. </author> <title> Retire Fortran? A debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Sisal with VISA provides implicit management of both tasks and data, and offers reasonable performance while alleviating the programmer from the implementation details of an architecture. 13 The result is efficient machine-independent code that is portable among a wide range of archi-tectures <ref> [3] </ref>. Furthermore, since the current Sisal compiler is unaware of distributed memory and costs associated with accessing remote data, we expect a performance gain when such information is exploited by the compiler [14].
Reference: [4] <author> Matthew Haines and Wim Bohm. </author> <title> Thread management in a distributed memory implmentation of sisal. </title> <booktitle> In Proceedings of the Dataflow Workshop, International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <note> To Appear. </note>
Reference-contexts: The backend optimizes the intermediate representation and generates native C code. The runtime system provides the Sisal compiler with two main abstractions: task management and memory management. We are working on a runtime system that provides support for both abstractions in a distributed memory environment, and in <ref> [4] </ref> we introduced the design and initial performance of the distributed task management abstraction. In Section 2 provides an overview of VISA, and the design and implementation of the supporting system.
Reference: [5] <author> Matthew Haines and Wim Bohm. </author> <title> Task management, virtual shared memory, and multithreading in a distributed memory implementation of sisal. </title> <booktitle> In Proceedings of Parallel Architectures and Languages Europe, </booktitle> <month> June </month> <year> 1993. </year> <note> To Appear. </note>
Reference-contexts: We have implemented this fixed addressing scheme and found that although the actual translation process is faster, the fixed control parameters often cause mis-alignment with the parallel loops which access the data structures, resulting in an excessive number of remote references and severely degraded overall performance of the application <ref> [5] </ref>.
Reference: [6] <author> Matthew Haines and Wim Bohm. </author> <title> The VISA user's guide. </title> <type> Technical Report CS-93-102, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: An alternative approach is to employ a software system to provide implicit management for tasks and/or data. This paper introduces the design of a runtime system, called VISA <ref> [6] </ref>, for implicit memory management on a distributed memory multiprocessor. The compiler or programmer is provided with a shared memory abstraction, and a set of primitives for allocating and accessing shared data structures within a virtual address space.
Reference: [7] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Implicit task management using Sisal and explicit memory management using message passing primitives. This represents a machine-dependent Sisal compiler that has been given the ability to generate explicit distributed memory code, much like the distributed memory Fortran compilers <ref> [7, 15] </ref>. However, such a modification to the compiler has not been undertaken, and thus we cannot expand on this style in our analysis. 4. Implicit task management using Sisal and implicit memory management provided by the VISA runtime system. <p> only the two necessary arrays, but allocates an additional two elements per processor to hold the pre-fetched remote values from neighboring nodes. 5 Related Research The most common alternatives to programming distributed memory multiprocessors using an explicit parallel language with message passing are distributed memory language compilers, such as FortranD <ref> [7] </ref>, Kali [8], and Superb [15]. These systems offer the advantage of implicit management for both tasks and memory, and allow the programmer to use a familiar programming paradigm: sequential shared memory.
Reference: [8] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-Space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: two necessary arrays, but allocates an additional two elements per processor to hold the pre-fetched remote values from neighboring nodes. 5 Related Research The most common alternatives to programming distributed memory multiprocessors using an explicit parallel language with message passing are distributed memory language compilers, such as FortranD [7], Kali <ref> [8] </ref>, and Superb [15]. These systems offer the advantage of implicit management for both tasks and memory, and allow the programmer to use a familiar programming paradigm: sequential shared memory.
Reference: [9] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Strict functional programming languages can also be restrictive in terms of expressibility, sometimes requiring complex and convoluted code to perform simple tasks. Another area of research that offers a language-independent shared memory paradigm is Distributed Shared Memory <ref> [1, 9, 12] </ref>. However, the inability to couple parallel tasks tightly with the distribution of data, controlled implicitly by the operating system, can result in misalignment, causing excessive message passing.
Reference: [10] <author> J. R. McGraw, S. K. Skedzielewski, S. J. Allan, R. R. Oldehoeft, J. Glauert, C. Kirkham, W. Noyce, and R. Thomas. </author> <title> SISAL: Streams and iteration in a single assignment language: Reference manual version 1.2. Manual M-146, </title> <type> Rev. 1, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> March </month> <year> 1985. </year>
Reference-contexts: This paper quantifies these claims. Sisal is a functional language that supports data types and operations for scientific computation <ref> [10] </ref>. The Sisal compiler consists of three parts: a frontend, a backend, and a runtime system. The frontend translates the source program into intermediate dependence graph form. The backend optimizes the intermediate representation and generates native C code.
Reference: [11] <author> Cherri M. Pancake and Donna Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 13-24, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: amount of programmer interaction varies widely from system to system, which can make porting an application from one DMMP compiler to another a non-trivial task. * Programmers have long been aware that the language design has a significant impact on how easily an algorithm can be transformed into working code <ref> [11] </ref>. Even the so-called "general purpose" languages are recognized as being suited for certain problem solving approaches. The transformation process is more tedious and error prone when the conceptual models supported by the language relate only peripherally to the problem-solving model of the programmer.
Reference: [12] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Unifying synchronization and data transfer in maintaining coherence of distributed shared memory. </title> <type> Technical Report GIT-CS-88/23, </type> <institution> Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Strict functional programming languages can also be restrictive in terms of expressibility, sometimes requiring complex and convoluted code to perform simple tasks. Another area of research that offers a language-independent shared memory paradigm is Distributed Shared Memory <ref> [1, 9, 12] </ref>. However, the inability to couple parallel tasks tightly with the distribution of data, controlled implicitly by the operating system, can result in misalignment, causing excessive message passing.
Reference: [13] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An emperical study of Fortran programs for paralleliz-ing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Also, symbolic subscript terms with unknown values, coupled subscripts, and nonzero or nonunity coefficients of loop indices often make dependence analysis impossible for even the most sophisticated parallelizing compilers <ref> [13] </ref>. * Due to the complexity of these compilers and the difficulties in porting them to new machines, their availability is limited to only of few of the currently available distributed memory multiprocessor systems.
Reference: [14] <author> R. Wolski and J. Feo. </author> <title> An extended data flow model for program partitioning on NUMA architectures. </title> <booktitle> In Proceedings of the Second Sisal User Conference, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Furthermore, since the current Sisal compiler is unaware of distributed memory and costs associated with accessing remote data, we expect a performance gain when such information is exploited by the compiler <ref> [14] </ref>. Explicit parallel C with VISA offers the ability to increase the performance of an application, but at the cost of increased size, programming effort, and machine-dependence.
Reference: [15] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1986. </year> <month> 15 </month>
Reference-contexts: Implicit task management using Sisal and explicit memory management using message passing primitives. This represents a machine-dependent Sisal compiler that has been given the ability to generate explicit distributed memory code, much like the distributed memory Fortran compilers <ref> [7, 15] </ref>. However, such a modification to the compiler has not been undertaken, and thus we cannot expand on this style in our analysis. 4. Implicit task management using Sisal and implicit memory management provided by the VISA runtime system. <p> but allocates an additional two elements per processor to hold the pre-fetched remote values from neighboring nodes. 5 Related Research The most common alternatives to programming distributed memory multiprocessors using an explicit parallel language with message passing are distributed memory language compilers, such as FortranD [7], Kali [8], and Superb <ref> [15] </ref>. These systems offer the advantage of implicit management for both tasks and memory, and allow the programmer to use a familiar programming paradigm: sequential shared memory.
References-found: 15

