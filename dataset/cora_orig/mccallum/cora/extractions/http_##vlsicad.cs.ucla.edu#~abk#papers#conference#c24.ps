URL: http://vlsicad.cs.ucla.edu/~abk/papers/conference/c24.ps
Refering-URL: http://vlsicad.cs.ucla.edu/~abk/publications.html
Root-URL: http://www.cs.ucla.edu
Title: Temp. Initial Temperature Factor .002 .005 .007 .009 .0125 .015 steps using linear temperature schedules
Author: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (.) . . (.) . . (.) . . (.) . . (.) . (.) 
Keyword: Initial Standard Initial Standard Temp. Average Deviation Temp. Average Deviation  
Note: Final  0.0  4,096  5 Conclusion References  
Abstract: Table 2 explores a wide range of linear schedules ending at zero. It indicates again that the best schedules have temperatures in the range between 0.002 and 0.015 for most steps. Raising or lowering the initial temperature outside this range significantly worsens the final solution quality. Tables 3 and 4 contain results for the same set of initial and final temperatures, using a reduced schedule length of 4,096 steps. These two tables reinforce the qualitative results of Tables 1 and 2, and show that our main observations regarding the utility of non-decreasing schedules are not affected by schedule length. Table 3: BSF results for training runs of 4,096 steps using linear temperature schedules (aver Table 4: BSF results of training runs of length Practical implementations of the simulated annealing algorithm for global optimization reflect a "best-so-far" (BSF) criterion. However, SA temperature schedules that "cool" monotonically to zero have been justified by theoretical analysis of a "where-you-are" (WYA) implementation. We have presented experimental evidence that challenges the appropriateness of cooling schedules for BSF annealing. We have estimated numerically the optimal schedules for some very small combinatorial problems, e.g., the six-city TSP instance described above. These examples all have BSF-optimal schedules that are non-cooling. Our simulations to train a medium-sized perceptron neural network indicate that the best linear cooling schedules are not significantly better or worse than constant or "warming" schedules. In fact, it seems that the most important requirement for devising a good temperature schedule is to find the appropriate temperature range. Hence, methods for adaptively determining this optimal range during the actual SA run will be most effective in improving the performance of SA and other stochastic hill-climbing variants. Certainly, such adaptive procedures should not rule out the use of non-cooling strategies. [1] E. H. L. Aarts and J. Korst, Simulated Annealing and Boltzmann Machines: a Stochastic Approach to Combinatorial Optimization and Neural Computing, Wiley, 1989. [2] E. Barnard, "Optimization for Training Neural Nets", IEEE Trans. on Neural Networks 3(2) (1992), pp. 232-240. [3] G. L. Bilbro, W. E. Snyder, S. J. Garnier and J. W. Gault, "Mean Field Annealing: A Formalism for Constructing GNC-Like Algorithms", IEEE Trans. on Neural Networks 3(1) (1992), pp. 131-138. [4] K. D. Boese and A. B. Kahng, "Best-So-Far vs. Where-You-Are: New Directions in Simulated Annealing for CAD", technical report UCLA CSD TR-920050, 1992. [5] V. Cerny, "Thermodynamical Approach to the Traveling Salesman Problem: an Efficient Simulation Algorithm", J. Optimization Theory and Applications 45(1) (1985), pp. 41-51. [6] J. E. Dennis, Jr. and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and Nonlinear Equations, Englewood Cliffs, NJ: Prentice-Hall, 1983. [7] B. Hajek, "Cooling Schedules for Optimal Annealing", Mathematics of Operations Research 13 (1988), pp. 311-329. [8] R. Hecht-Neilsen, Neurocomputing, Reading, MA: Addison-Wesley, 1990. [9] G. E. Hinton, "Deterministic Boltzmann Learning Performs Steepest Descent in Weight Space", technical report CRG-TR-89-1, Univ. of Toronto, 1989. [10] A. B. Kahng, "Exploiting Fractalness in Error Surfaces: New Methods for Neural Network Learning", Proc. IEEE Intl. Symp. on Circuits and Systems, San Diego, 1992, pp. 41-44. [11] A. B. Kahng,"Random Structure of Error Surfaces: New Stochastic Learning Methods", invited paper, Proc. SPIE Conf. on Neural Networks and Optimization, Orlando, April 1992. [12] S. Kirkpatrick, Jr. C. D. Gelatt, and M. Vecchi, "Optimization by Simulated Annealing", Science 220(4598) (1983), pp. 671-680. [13] P. J. M. Laarhoven and E. H. L. Aarts, Simulated Annealing: Theory and Applications, Boston: D. Reidel, 1987. [14] E. L. Lawler, J. K. Lenstra, A. Rinnooy-Kan and D. Shmoys, The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization, Chichester: Wiley, 1985. [15] B. W. Lee and B. J. Sheu, "Electronic Neural Circuits with Simulated Annealing", Proc. IEEE/INNS Intl. Joint Conf. on Neural Networks, Washington, 1989, pp. II-615 II-618. [16] M. A. Lehr and B. Widrow, "Adaptive Multisource Decision-Making: Detecting Land Mines with Neural Networks Using Separated Aperture Sensor Data Collected at Fort Belvoir", report BRDE-ISL/TR-1/1, April 1992, Stanford University Department of Electrical Engineering. [17] M. Lundy and A. Mees, "Convergence of an Annealing Algorithm", Math. Programming 34 (1986), pp. 111-124. [18] C. Peterson and J. R. Anderson, "A Mean Field Theory Learning Algorithm for Neural Networks", Complex Systems 1 (1987), pp. 995-1019. [19] M. J. D. Powell, "Restart Procedures for the Conjugate Gradient Method", Mathematical Programming 12 (1977), pp. 241-254. [20] L. S. Riggs and C. A. Amazeen, "Research Efforts with the Waveguide Beyond Cutoff or Separated Aperture Dielectric Anomaly Detection Scheme", report, U. S. Army Belvoir RD&E Center, December 1989. [21] D. E. Rumelhart, J. L. McClelland et al., Parallel Distributed Processing: Explorations in the Microstructure of Cognition , Cambridge, MIT Press, 1986. [22] G. Sorkin, "Efficient Simulated Annealing on Fractal Energy Landscapes", Algorithmica 6 (1991), pp. 367-418. [23] P. Strenski and S. Kirkpatrick, "Analysis of Finite Length Annealing Schedules", Algorithmica 6 (1991), pp. 346-366. [24] S. A. Stricker, M. R. Azimi-Sadjadi, and D. E. Poole, "Application of the Karhunen-Loeve Transform for Target Detection and Classification Using Neural Networks", draft, 1992. [25] A. Torn and A. Zilinskas, Global Optimization , Lecture Notes in Computer Science 350, G. Goos and J. Hartmanis, eds., Springer-Verlag, 1987. [26] D. E. Van den Bout and T. K. Miller, "Graph Partitioning Using Annealed Networks", IEEE Trans. on Neural Networks 1(2) (1990), pp. 192-203. [27] D. F. Wong, H. W. Leong and C. L. Liu, Simulated Annealing for VLSI Design, Boston: Kluwer Academic, 1988. ages of 50 runs).
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. H. L. Aarts and J. Korst, </author> <title> Simulated Annealing and Boltzmann Machines: a Stochastic Approach to Combinatorial Optimization and Neural Computing, </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference: [2] <author> E. Barnard, </author> <title> "Optimization for Training Neural Nets", </title> <booktitle> IEEE Trans. on Neural Networks 3(2) (1992), </booktitle> <pages> pp. 232-240. </pages>
Reference: [3] <author> G. L. Bilbro, W. E. Snyder, S. J. Garnier and J. W. Gault, </author> <title> "Mean Field Annealing: A Formalism for Constructing GNC-Like Algorithms", </title> <booktitle> IEEE Trans. on Neural Networks 3(1) (1992), </booktitle> <pages> pp. 131-138. </pages>
Reference: [4] <author> K. D. Boese and A. B. Kahng, </author> <title> "Best-So-Far vs. Where-You-Are: New Directions in Simulated Annealing for CAD", </title> <type> technical report UCLA CSD TR-920050, </type> <year> 1992. </year>
Reference: [5] <author> V. Cerny, </author> <title> "Thermodynamical Approach to the Traveling Salesman Problem: an Efficient Simulation Algorithm", </title> <editor> J. </editor> <booktitle> Optimization Theory and Applications 45(1) (1985), </booktitle> <pages> pp. 41-51. </pages>
Reference: [6] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference: [7] <author> B. Hajek, </author> <title> "Cooling Schedules for Optimal Annealing", </title> <booktitle> Mathematics of Operations Research 13 (1988), </booktitle> <pages> pp. 311-329. </pages>
Reference: [8] <author> R. Hecht-Neilsen, </author> <title> Neurocomputing, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [9] <author> G. E. Hinton, </author> <title> "Deterministic Boltzmann Learning Performs Steepest Descent in Weight Space", </title> <type> technical report CRG-TR-89-1, </type> <institution> Univ. of Toronto, </institution> <year> 1989. </year>
Reference: [10] <author> A. B. Kahng, </author> <title> "Exploiting Fractalness in Error Surfaces: New Methods for Neural Network Learning", </title> <booktitle> Proc. IEEE Intl. Symp. on Circuits and Systems, </booktitle> <address> San Diego, </address> <year> 1992, </year> <pages> pp. 41-44. </pages>
Reference: [11] <author> A. B. </author> <title> Kahng,"Random Structure of Error Surfaces: New Stochastic Learning Methods", invited paper, </title> <booktitle> Proc. SPIE Conf. on Neural Networks and Optimization, </booktitle> <address> Orlando, </address> <month> April </month> <year> 1992. </year>
Reference: [12] <author> S. Kirkpatrick, Jr. C. D. Gelatt, and M. Vecchi, </author> <title> "Optimization by Simulated Annealing", </title> <booktitle> Science 220(4598) (1983), </booktitle> <pages> pp. 671-680. </pages>
Reference: [13] <author> P. J. M. Laarhoven and E. H. L. Aarts, </author> <title> Simulated Annealing: Theory and Applications, </title> <address> Boston: D. </address> <publisher> Reidel, </publisher> <year> 1987. </year>
Reference: [14] <author> E. L. Lawler, J. K. Lenstra, A. Rinnooy-Kan and D. Shmoys, </author> <title> The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization, </title> <address> Chichester: </address> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference: [15] <author> B. W. Lee and B. J. Sheu, </author> <title> "Electronic Neural Circuits with Simulated Annealing", </title> <booktitle> Proc. IEEE/INNS Intl. Joint Conf. on Neural Networks, </booktitle> <address> Washington, </address> <year> 1989, </year> <pages> pp. </pages> <address> II-615 II-618. </address>
Reference: [16] <author> M. A. Lehr and B. Widrow, </author> <title> "Adaptive Multisource Decision-Making: Detecting Land Mines with Neural Networks Using Separated Aperture Sensor Data Collected at Fort Belvoir", </title> <type> report BRDE-ISL/TR-1/1, </type> <month> April </month> <year> 1992, </year> <institution> Stanford University Department of Electrical Engineering. </institution>
Reference: [17] <author> M. Lundy and A. Mees, </author> <title> "Convergence of an Annealing Algorithm", Math. </title> <booktitle> Programming 34 (1986), </booktitle> <pages> pp. 111-124. </pages>
Reference: [18] <author> C. Peterson and J. R. Anderson, </author> <title> "A Mean Field Theory Learning Algorithm for Neural Networks", </title> <booktitle> Complex Systems 1 (1987), </booktitle> <pages> pp. 995-1019. </pages>
Reference: [19] <author> M. J. D. Powell, </author> <title> "Restart Procedures for the Conjugate Gradient Method", </title> <booktitle> Mathematical Programming 12 (1977), </booktitle> <pages> pp. 241-254. </pages>
Reference: [20] <author> L. S. Riggs and C. A. Amazeen, </author> <title> "Research Efforts with the Waveguide Beyond Cutoff or Separated Aperture Dielectric Anomaly Detection Scheme", </title> <type> report, </type> <institution> U. S. Army Belvoir RD&E Center, </institution> <month> December </month> <year> 1989. </year>
Reference: [21] <editor> D. E. Rumelhart, J. L. McClelland et al., </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition , Cambridge, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference: [22] <author> G. Sorkin, </author> <title> "Efficient Simulated Annealing on Fractal Energy Landscapes", </title> <booktitle> Algorithmica 6 (1991), </booktitle> <pages> pp. 367-418. </pages>
Reference: [23] <author> P. Strenski and S. Kirkpatrick, </author> <title> "Analysis of Finite Length Annealing Schedules", </title> <booktitle> Algorithmica 6 (1991), </booktitle> <pages> pp. 346-366. </pages>
Reference: [24] <author> S. A. Stricker, M. R. Azimi-Sadjadi, and D. E. Poole, </author> <title> "Application of the Karhunen-Loeve Transform for Target Detection and Classification Using Neural Networks", </title> <type> draft, </type> <year> 1992. </year>
Reference: [25] <author> A. Torn and A. Zilinskas, </author> <booktitle> Global Optimization , Lecture Notes in Computer Science 350, </booktitle> <editor> G. Goos and J. Hartmanis, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference: [26] <author> D. E. Van den Bout and T. K. Miller, </author> <title> "Graph Partitioning Using Annealed Networks", </title> <booktitle> IEEE Trans. on Neural Networks 1(2) (1990), </booktitle> <pages> pp. 192-203. </pages>


References-found: 26

