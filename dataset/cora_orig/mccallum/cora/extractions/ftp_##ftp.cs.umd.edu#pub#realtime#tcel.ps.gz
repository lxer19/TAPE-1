URL: ftp://ftp.cs.umd.edu/pub/realtime/tcel.ps.gz
Refering-URL: http://www.cs.umd.edu/projects/TimeWare/TimeWare-index-no-abs.html
Root-URL: 
Title: Compiling Real-Time Programs with Timing Constraint Refinement and Structural Code Motion  
Author: Richard Gerber and Seongsoo Hong 
Keyword: Real-time, programming languages, compiler optimization, code scheduling, static single assignment, gated single assignment, timing analysis, trace scheduling, code motion.  
Note: IEEE Transactions on Software Engineering,  This research is supported in part by ONR grant N00014-94-10228, NSF grant CCR-9209333, and an NSF Young Investigator Award CCR-9357850. A preliminary abstract of this material appeared in the Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Design and Implementation (June 1993).  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Email: rich@cs.umd.edu sshong@cs.umd.edu  
Phone: (301) 405-2710  
Date: 21(5), May 1995  
Abstract: We present a programming language called TCEL (Time-Constrained Event Language), whose semantics is based on time-constrained relationships between observable events. Such a semantics infers only those timing constraints necessary to achieve real-time correctness, without over-constraining the system. Moreover, an optimizing compiler can exploit this looser semantics to help tune the code, so that its worst-case execution time is consistent with its real-time requirements. In this paper we describe such a transformation system, which works in two phases. First the TCEL source code is translated into an intermediate representation. Then an instruction-scheduling algorithm rearranges selected unobservable operations and synthesizes tasks guaranteed to respect the original event-based constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: output (FLAP Cntrl, wflap); The net runtime effect would simply be a refinement of the potential behaviors; i.e., the time-event relationships exhibited by the altered program would be a subset of those in the original version. 4 Basic Notations The output of TCEL compiler's machine-independent pass is a flow graph <ref> [1] </ref>, which contains the original timing information. For example, Figure 3 (B) shows the flow graph for our flight control program in Figure 3 (A), where for the sake of brevity we have left the code in its original C form. <p> A node d is called a dominator of node n, if every path from entry (B) to n goes through d <ref> [1] </ref>. Similarly, a node p is a postdominator of node n, if every path from n to exit (B) goes through d [1]. Data dependence. Let Def (n) and Use (n) be sets of variables defined and used by node n in B, respectively. <p> A node d is called a dominator of node n, if every path from entry (B) to n goes through d <ref> [1] </ref>. Similarly, a node p is a postdominator of node n, if every path from n to exit (B) goes through d [1]. Data dependence. Let Def (n) and Use (n) be sets of variables defined and used by node n in B, respectively.
Reference: [2] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: (S4) = 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques to handle these cases in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 6, 8, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [8] or Percolation Scheduling <ref> [2, 6] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [3] <author> J.-D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: A Appendix: The Static Single Assignment Form The static single assignment (SSA) form of a program can be considered not only as a sparse representation of flow data dependences, but also as a notation where the spurious data dependences such as output and anti-dependences are eliminated <ref> [3, 4] </ref>. A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it [4].
Reference: [4] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13 </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: And the situation gets much more complicated when the program possesses a branching structure, i.e., when actual execution paths are determined at runtime. Thus we take a greedy approximation approach, which works in several phases. First the TCEL source is translated into a gated single assignment (GSA) representation <ref> [4] </ref>, whose naming conventions help isolate the "worst-case" execution paths. Next the code is decomposed into several blocks, and equations are generated to constrain their start and finish-times. Finally, a variant of Trace Scheduling [8] is used to relocate the unobservable code, and hopefully attain feasibility. <p> Thus we take the following alternative approach, in which feasible tasks are synthesized in a two-step process section decomposition (Section 6) and code scheduling (Section 7). Section Decomposition. First the code is translated into its gated single assignment (GSA) form <ref> [4, 13] </ref>. <p> A Appendix: The Static Single Assignment Form The static single assignment (SSA) form of a program can be considered not only as a sparse representation of flow data dependences, but also as a notation where the spurious data dependences such as output and anti-dependences are eliminated <ref> [3, 4] </ref>. A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it [4]. <p> A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it <ref> [4] </ref>.
Reference: [5] <author> B. Dasarathy. </author> <title> Timing constraints of real-time systems: Constructs for expressing them, method for validating them. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(1) </volume> <pages> 80-86, </pages> <month> Jan-uary </month> <year> 1985. </year> <month> 32 </month>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [16], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL <ref> [5] </ref>, Timed IO Automata [23], ACSR [18], and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [6] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In In--ternational Conference on Supercomputing, </booktitle> <pages> pages 154-163. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: (S4) = 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques to handle these cases in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 6, 8, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [8] or Percolation Scheduling <ref> [2, 6] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [7] <author> J. Ferrante and K. J. Ottenstein. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9 </volume> <pages> 319-345, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Our definition of control dependence is simpler than that found in <ref> [7] </ref>, since it covers a restricted language possessing only structured program constructs. Dependence Closure. The dependence closure for node n in the block B, denoted by "DC (n; B)," contains n and all nodes m that reach n via zero or more control or data dependence edges.
Reference: [8] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30 </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: First the TCEL source is translated into a gated single assignment (GSA) representation [4], whose naming conventions help isolate the "worst-case" execution paths. Next the code is decomposed into several blocks, and equations are generated to constrain their start and finish-times. Finally, a variant of Trace Scheduling <ref> [8] </ref> is used to relocate the unobservable code, and hopefully attain feasibility. The remainder of this paper is organized as follows. In the following section we survey related work in programming languages, semantics and optimization methods for real-time systems. <p> Then, the surrounding node is handled. If this level is found inconsistent, the inner nodes are "opened up" once again, and more aggressive optimization is carried out. The actual transformations are similar to those used in Trace Scheduling <ref> [8] </ref>. As the name implies, the Trace Scheduling algorithm works on specific traces: it selects a path (or trace) from a given code block, and then selects instructions on that path to move. <p> (S4) = 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques to handle these cases in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 6, 8, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling <ref> [8] </ref> or Percolation Scheduling [2, 6]. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [9] <author> M. R. Garey and D. S. Johnson. </author> <title> Computer and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: Clause (3) means that the new code is feasible. This problem is NP-hard, due to the existence of immovable operations and data dependences. Theorem 5.1 Feasible code synthesis is NP-hard. Proof: The proof follows by a straightforward transformation from "Partition [SP12]" <ref> [9] </ref> to feasible code synthesis. Consider an instance (A; s) of Partition, where A = fa 1 ; a 2 ; : : : ; a n g is a set of elements, and where s : A 7! NN is the cost of each element.
Reference: [10] <author> R. Gerber and S. Hong. </author> <title> Semantics-based compiler transformations for enhanced schedulabil-ity. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 232-242. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [10, 11, 14, 24, 22] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> Unlike the semantics for TCEL, however, the execution times of non-time-critical statements are not explicitly decoupled from timing constraints imposed on the events. Thus, the applicability of some transformations may be unnecessarily restricted. Finally, in <ref> [10] </ref> we describe how we use TCEL's event-based semantics to specialize tasks for multi-threaded applications. Specifically, a program-slicing tool splits a task into a deadline-sensitive sub-thread, and a thread which can be postponed past its deadline. <p> With our scheduling engine as its foundation, a graphical interface would allow a programmer to selectively apply the transformations and also remain informed of the results. Pushing Forward. We have recently turned our attention to a more aggressive goal inter-task transformations to achieve schedulability. In <ref> [10] </ref> we explore a technique that helps auto-tune an unschedulable task set into a schedulable one, by isolating the time-critical threads, and then ensuring that they can be run under a fixed-priority dispatcher.
Reference: [11] <author> P. Gopinath and R. Gupta. </author> <title> Applying compiler techniques to scheduling in real-time systems. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 247-256. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [10, 11, 14, 24, 22] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> There have been other compiler-based approaches to real-time programming [10, 11, 14, 24, 22]. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. In <ref> [11] </ref> a compiler classifies an application program on the basis of its predictability and monotonicity, and creates partitions which have a higher degree of adaptability. The objective is to produce a transformed program possessing a smaller variance in its execution time.
Reference: [12] <author> M. G. Harmon, T. P. Baker, and D. B. Whalley. </author> <title> A retargetable technique for predicting execution time. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 68-77. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [12, 20, 28] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [13] <author> P. Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Thus we take the following alternative approach, in which feasible tasks are synthesized in a two-step process section decomposition (Section 6) and code scheduling (Section 7). Section Decomposition. First the code is translated into its gated single assignment (GSA) form <ref> [4, 13] </ref>. <p> Such a coupling will be avoidable, if we parameterize the -function with the associated predicate. Gated single assignment (GSA) form solves this problem <ref> [13] </ref>. GSA form is an extension of SSA form with new functions that encode control over value assignment. <p> defined in a loop body, a definition v 0 = (v init ; v iter ) is inserted at the loop header, where v init is the initial value of v reaching the header from outside and v iter is the iterative value reaching along the back-edge of the loop. <ref> [13] </ref> In our examples we denote v init and v 0 by v 0 and v 1 , respectively. v = 10; v = v1; g while (v &gt; 0) =) do f v 2 = v 1 1; g while ( v 2 &gt; 0)
Reference: [14] <author> S. Hong and R. Gerber. </author> <title> Scheduling with compiler transformations: the TCEL approach. </title> <booktitle> In Proceedings of IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 80-84. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1993. </year> <journal> IEEE RTTC Real-Time Newsletter, </journal> 9(1/2):80-84. 
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [10, 11, 14, 24, 22] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs.
Reference: [15] <author> Y. Ishikawa, H. Tokuda, and C. W. Mercer. </author> <title> Object-oriented real-time language design: Constructs for timing constraints. </title> <booktitle> In Proceedings of OOPSLA-90, </booktitle> <pages> pages 289-298, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the temporal requirements side of the design equation; examples are <ref> [15, 19, 21, 24, 27] </ref>. These languages provide programmers with a convenient means of expressing timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime.
Reference: [16] <author> F. Jahanian and Al Mok. </author> <title> Safety analysis of timing properties in real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(9) </volume> <pages> 890-904, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: That is, when reasoning about a real-time concurrent system it is often useful to consider only "events of interest," and to abstract away local-state information. Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic <ref> [16] </ref>, events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [5], Timed IO Automata [23], ACSR [18], and in almost every formal approach to real-time.
Reference: [17] <author> K. B. Kenny and K.-J. Lin. </author> <title> Building flexible real-time systems using the Flex language. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Similarly, let S4.start and S4.finish represent the start and finish times of section S4. Using these variables we can represent the section decomposition of a TCEL construct in a manner similar to that found in the Flex language <ref> [17] </ref>. Recall the flight controller program from Figure 3. Figure 5 illustrates its constituent sections. The constraint-expression for S6 corresponds to the program's outer, periodic loop. As the program is in GSA form, fl-functions appear at confluence points where different values of the same variable in the original program merge.
Reference: [18] <author> I. Lee, P. Bremond-Gregoire, and R. Gerber. </author> <title> A Process Algebraic Approach to the Specification and Analysis of Resource-Bound Real-Time Systems. </title> <journal> IEEE Proceedings, </journal> <volume> 82(1), </volume> <month> January </month> <year> 1994. </year> <month> 33 </month>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [16], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [5], Timed IO Automata [23], ACSR <ref> [18] </ref>, and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [19] <author> I. Lee and V. Gehlot. </author> <title> Language constructs for real-time programming. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 57-66. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the temporal requirements side of the design equation; examples are <ref> [15, 19, 21, 24, 27] </ref>. These languages provide programmers with a convenient means of expressing timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [21, 19, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in [21, 19, 27]), or they are postulated in a separate interface, and then passed to the scheduler as directives. A common language-based approach (first presented in <ref> [19] </ref>) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in [21], is to set up linear constraint expressions on the start times and deadlines of code blocks. <p> Both constructs are syntactic descendents of the temporal scope, first introduced in <ref> [19] </ref>. However, as we have stated, our semantics is quite different, in that it relies on constrained relationships between observable events. We first elaborate the "do" construct which establishes several types of relative timing constraints.
Reference: [20] <author> S. Lim, Y. Bae, C. Jang, B. Rhee, S. Min, C. Park, H. Shin, K. Park, and C. Kim. </author> <title> An accurate worst case timing analysis for risc processors. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 97-108. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1994. </year>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [12, 20, 28] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [21] <author> K. J. Lin and S. Natarajan. </author> <title> Expressing and maintaining timing constraints in FLEX. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1988. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the temporal requirements side of the design equation; examples are <ref> [15, 19, 21, 24, 27] </ref>. These languages provide programmers with a convenient means of expressing timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [21, 19, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> A common language-based approach (first presented in [19]) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in <ref> [21] </ref>, is to set up linear constraint expressions on the start times and deadlines of code blocks. We have borrowed from both approaches: in the TCEL source we use the higher-level constructs, while in our intermediate code we make use of the constraint representation.
Reference: [22] <author> T. Marlowe and S. Masticola. </author> <title> Safe optimization for hard real-time programming. </title> <booktitle> In Second International Conference on Systems Integration, </booktitle> <pages> pages 438-446, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [10, 11, 14, 24, 22] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> The objective is to produce a transformed program possessing a smaller variance in its execution time. In [24] a partial evaluator is applied to a source program, which produces residual code that is both more optimized and more deterministic. In <ref> [22] </ref> time-critical statements (or events) are assumed in the underlying programming language, and used for developing the notion of safe real-time code transformations. Based on this notion of 3 safety, a large number of conventional code transformations are examined, and then classified for application in real-time programming.
Reference: [23] <author> M. Merritt, F. Modungo, and M. Tuttle. </author> <title> Time-Constrained Automata. </title> <booktitle> In CONCUR '91, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [16], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [5], Timed IO Automata <ref> [23] </ref>, ACSR [18], and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [24] <author> V. Nirkhe. </author> <title> Application of Partial Evaluation to Hard Real-Time Programming. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland at College Park, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the temporal requirements side of the design equation; examples are <ref> [15, 19, 21, 24, 27] </ref>. These languages provide programmers with a convenient means of expressing timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [10, 11, 14, 24, 22] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> In [11] a compiler classifies an application program on the basis of its predictability and monotonicity, and creates partitions which have a higher degree of adaptability. The objective is to produce a transformed program possessing a smaller variance in its execution time. In <ref> [24] </ref> a partial evaluator is applied to a source program, which produces residual code that is both more optimized and more deterministic. In [22] time-critical statements (or events) are assumed in the underlying programming language, and used for developing the notion of safe real-time code transformations.
Reference: [25] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Of course these and similar methods will degrade the code scheduler's performance, either by increasing the amount of code, or by decreasing its efficacy. The good news, however, is that dependence analyzers are improving at a rapid rate, and our algorithm will improve along with them <ref> [25] </ref>. Limits of Timing Analysis. Another limiting factor is the difficulty of achieving accurate, static timing analysis in the face of more complicated architectures. Quite simply, it has become incredibly difficult to use vendor-supplied benchmarks, and model the interplay between pipelines, hierarchical caches, shared memories, register windows, etc.
Reference: [26] <author> M. Smith, M. Horowitz, and M. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: (S4) = 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques to handle these cases in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 6, 8, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [27] <author> V. Wolfe, S. Davidson, and I. Lee. RTC: </author> <title> Language support for real-time concurrency. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 43-52. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1991. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the temporal requirements side of the design equation; examples are <ref> [15, 19, 21, 24, 27] </ref>. These languages provide programmers with a convenient means of expressing timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [21, 19, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives.
Reference: [28] <author> N. Zhang, A. Burns, and M. Nicholson. </author> <title> Pipelined processors and worst case execution times. </title> <journal> The Journal of Real-Time Systems, </journal> <volume> 5(4), </volume> <month> October </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [12, 20, 28] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
References-found: 28

