URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-10.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-10.html
Root-URL: http://www.cs.brown.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bellman, Richard, </author> <title> Adaptive Control Processes, </title> <publisher> (Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1961). </year>
Reference-contexts: We mention two standard methods for solving Markov decision processes. Bellman's value iteration method <ref> [1] </ref> iterates by computing the optimal expected cumulative cost function accounting for n steps of lookahead using the optimal expected cumulative cost function accounting for n 1 steps of lookahead.
Reference: [2] <author> Caines, Peter E. and Wang, S., COCOLOG: </author> <title> A Conditional Observer and Controller Logic for Finite Machines, </title> <booktitle> Proceedings of the 29th IEEE Conference on Decision and Control, Hawaii, </booktitle> <year> 1990. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [13] and hierarchies of state-space operators [22] [12]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] <ref> [2] </ref>. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces [11].
Reference: [3] <author> Chvatal, Vasek, </author> <title> Linear Programming, </title> <editor> (W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980). </year>
Reference-contexts: This iterative method is based on a reduction to the methods of Kush-ner and Chen [15] that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle [4]. The details of the material presented in this section depend on some understanding of linear programming <ref> [3] </ref> and methods for decomposing and solving large systems [16]. We sketch the iterative method in the following, and refer the readers to the following two sections for more details. 1.
Reference: [4] <author> Dantzig, George and Wolfe, Philip, </author> <title> Decomposition Principle for Dynamic Programs, </title> <journal> Operations Research, </journal> <month> 8(1) </month> <year> (1960) </year> <month> 101-111. </month>
Reference-contexts: In this paper, we focus on a particular iterative method that resolves these issues. This iterative method is based on a reduction to the methods of Kush-ner and Chen [15] that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle <ref> [4] </ref>. The details of the material presented in this section depend on some understanding of linear programming [3] and methods for decomposing and solving large systems [16]. We sketch the iterative method in the following, and refer the readers to the following two sections for more details. 1. <p> The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [7] [8] and decomposing large systems generally <ref> [4] </ref> [16] and Markov decision processes specifically [15].
Reference: [5] <author> Dean, Thomas, Kaelbling, Leslie, Kirman, Jak, and Nicholson, Ann, </author> <title> Planning With Deadlines in Stochastic Domains, </title> <booktitle> Proceedings AAAI-93, </booktitle> <address> Wash-ington, D.C., </address> <publisher> AAAI, </publisher> <year> 1993, </year> <pages> 574-579. 43 </pages>
Reference-contexts: The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [7] [8] and decomposing large systems generally [4] [16] and Markov decision processes specifically [15]. The approach described in <ref> [5] </ref> represents a special case of the framework presented here, in which the partition consists of singleton sets for all of the states in the envelope and a set for all the states in the complement of the envelope. 11 Conclusion The benefit of decomposition techniques is that we are able
Reference: [6] <author> Dean, Thomas and Kanazawa, Keiji, </author> <title> A Model for Reasoning About Per--sistence and Causation, </title> <booktitle> Computational Intelligence, </booktitle> <month> 5(3) </month> <year> (1989) </year> <month> 142-150. </month>
Reference-contexts: A factored state-space representation uses state variables to represent different aspects of the overall state of the system. 2 Compact encodings for stochastic processes can be achieved for many applications using factored state-space representations, where the size of the model is usually logarithmic in the size of the state space <ref> [6] </ref>. Similarly, policies for large factored state spaces can often be efficiently encoded using decision trees that branch on state variables.
Reference: [7] <author> D'Epenoux, F., </author> <title> Sur un probleme de production et de stockage dans l'aleatoire, </title> <booktitle> Management Science, </booktitle> <month> 10 </month> <year> (1963) </year> <month> 98-108. </month>
Reference-contexts: current solution is less than or equal to the value of an optimal solution plus ffi fi where ffi fi = fi 0 z fl fi . 6.2 The DW Decomposition and Markov Decision Pro cesses It is well known that we can solve Markov decision processes as linear programs <ref> [7] </ref> [8] [14] [21] using standard techniques like the simplex method. Kushner and Chen [15] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> Assuming that the target can be reached with probability 1 from any other state, we can reformulate M = ( X ; A ; p; c) as the following linear program <ref> [7] </ref> [14]: min z = i;a Cost (i; a)E i;a P a E i;a = ~ i + j;a P ji (a)E j;a ; 8i 2 X 9 &gt; ; where E i;a denote the average number of time that action a is taken in state i, and ~ i is <p> Similarly, we have the following linear program for the criterion of expected discounted cumulative cost <ref> [7] </ref> [14]. min z = i;a Cost (i; a)E i;a P a E i;a = ~ i + fl j;a P ji (a)E j;a ; 8i 2 X 9 &gt; ; where E i;a denote the discounted average number of time that action a is taken in state i, and ~ <p> The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs <ref> [7] </ref> [8] and decomposing large systems generally [4] [16] and Markov decision processes specifically [15].
Reference: [8] <author> Derman, Cyrus, </author> <title> Finite State Markovian Decision Processes, </title> <publisher> (Cambridge University Press, </publisher> <address> New York, </address> <year> 1970). </year>
Reference-contexts: The abstract policy has the interpretation of providing a global perspective and indicating for each region the best local policy to use. Generally, it is best to set fl very close to one or use an alternative performance criterion such as average expected cost per step <ref> [8] </ref>. The following is an algorithm to construct a global policy using the abstract decision process (P; F ; ; p 0 ; c 0 ). 12 1. Set and compute R!S for R;S 2 P . 2. <p> solution is less than or equal to the value of an optimal solution plus ffi fi where ffi fi = fi 0 z fl fi . 6.2 The DW Decomposition and Markov Decision Pro cesses It is well known that we can solve Markov decision processes as linear programs [7] <ref> [8] </ref> [14] [21] using standard techniques like the simplex method. Kushner and Chen [15] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> like the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic con vergence rate respectively [21], and * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space <ref> [8] </ref> [21]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where (i) both c and k are constants and (ii) n is the size of the state space. k is 3 and 2 respectively when applying the policy iteration <p> The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [7] <ref> [8] </ref> and decomposing large systems generally [4] [16] and Markov decision processes specifically [15].
Reference: [9] <author> Fikes, Richard and Nilsson, Nils J., </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving, </title> <journal> Artificial Intelligence, </journal> <month> 2 </month> <year> (1971) </year> <month> 189-208. </month>
Reference-contexts: The size of the union of these abstract subspaces is no more 2 In artificial intelligence, such factored representations are often called intensional representations. Propositions representing fluents in STRIPS operators <ref> [9] </ref> correspond to state variables in a factored state-space representation. 4 unstructured state space, the middle view represents an abstraction obtained by projection, and the bottom view represents a decomposition obtained by partitioning the states into aggregate states. space is represented as the union of two-dimensional abstract subspaces shaded dark gray.
Reference: [10] <author> Howard, Ronald A., </author> <title> Dynamic Programming and Markov Processes, </title> <publisher> (MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960). </year>
Reference-contexts: Value iteration is guaranteed to converge in the limit to the optimal expected cumulative cost function accounting for an infinite lookahead. Howard's policy iteration <ref> [10] </ref> iterates by first computing the expected cumulative cost function for the current policy and then improving the policy by using this cost function. Policy iteration is guaranteed to converge to the optimal policy in O (N 3 ) iterations.
Reference: [11] <author> Kaelbling, Leslie Pack, </author> <title> Hierarchical Learning in Stochastic Domains: A Preliminary Report, </title> <booktitle> Proceedings Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] [2]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces <ref> [11] </ref>. The hierarchical policy construction method described in Section 4 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [11] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces. <p> In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces <ref> [11] </ref>. The hierarchical policy construction method described in Section 4 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [11] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces.
Reference: [12] <author> Knoblock, Craig A., </author> <title> Search Reduction in Hierarchical Problem Solving, </title> <booktitle> Proceedings AAAI-91, </booktitle> <address> Anaheim, California, </address> <publisher> AAAI, </publisher> <year> 1991, </year> <pages> 686-691. </pages>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [13] and hierarchies of state-space operators [22] <ref> [12] </ref>. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] [2]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces [11].
Reference: [13] <author> Korf, Richard, Macro-Operators: </author> <title> A Weak Method for Learning, </title> <journal> Artificial Intelligence, </journal> <month> 26 </month> <year> (1985) </year> <month> 35-77. </month>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators <ref> [13] </ref> and hierarchies of state-space operators [22] [12]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] [2].
Reference: [14] <author> Kushner, H. J. and Kleinman, A. J., </author> <title> Mathematical Programming and the control of Markov Chains, </title> <journal> International Journal on Control, </journal> <month> 13(5) </month> <year> (1971) </year> <month> 801-820. </month>
Reference-contexts: is less than or equal to the value of an optimal solution plus ffi fi where ffi fi = fi 0 z fl fi . 6.2 The DW Decomposition and Markov Decision Pro cesses It is well known that we can solve Markov decision processes as linear programs [7] [8] <ref> [14] </ref> [21] using standard techniques like the simplex method. Kushner and Chen [15] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> Assuming that the target can be reached with probability 1 from any other state, we can reformulate M = ( X ; A ; p; c) as the following linear program [7] <ref> [14] </ref>: min z = i;a Cost (i; a)E i;a P a E i;a = ~ i + j;a P ji (a)E j;a ; 8i 2 X 9 &gt; ; where E i;a denote the average number of time that action a is taken in state i, and ~ i is the <p> Similarly, we have the following linear program for the criterion of expected discounted cumulative cost [7] <ref> [14] </ref>. min z = i;a Cost (i; a)E i;a P a E i;a = ~ i + fl j;a P ji (a)E j;a ; 8i 2 X 9 &gt; ; where E i;a denote the discounted average number of time that action a is taken in state i, and ~ i
Reference: [15] <author> Kushner, Harold J. and Chen, Ching-Hui, </author> <title> Decomposition of systems governed by Markov chains, </title> <journal> IEEE Transactions on Automatic Control, </journal> <month> AC-19(5) </month> <year> (1974) </year> <month> 501-507. </month>
Reference-contexts: In this paper, we focus on a particular iterative method that resolves these issues. This iterative method is based on a reduction to the methods of Kush-ner and Chen <ref> [15] </ref> that demonstrate how to solve Markov decision processes as linear programs using the Dantzig-Wolfe decomposition principle [4]. The details of the material presented in this section depend on some understanding of linear programming [3] and methods for decomposing and solving large systems [16]. <p> This resulting partition Q induces a star topology that is critical in applying the techniques in <ref> [15] </ref>. U is the coupling region in the new partition Q; removing the states in U separates the state space into isolated regions, each of which corresponds to a K i , 1 i h. <p> Proposition 1 The iterative method described above improves the solution quality on each iteration, and converges to an optimal solution in a finite number of iterations. Proof. The iterative method is equivalent to (i) adopting the partition Q, and then (ii) applying the methods of Kushner and Chen <ref> [15] </ref> that solve Markov decision processes as large linear programs using the Dantzig-Wolfe decomposition principle, which have the properties described above. A more detailed description of the algorithm and its correspondence to the methods of Kushner and Chen [15] is provided in the next two sections. 2 In the following, we <p> partition Q, and then (ii) applying the methods of Kushner and Chen <ref> [15] </ref> that solve Markov decision processes as large linear programs using the Dantzig-Wolfe decomposition principle, which have the properties described above. A more detailed description of the algorithm and its correspondence to the methods of Kushner and Chen [15] is provided in the next two sections. 2 In the following, we briefly discuss the convergence rate and the time and space complexity of this iterative method. * Empirical experience suggests that the Dantzig-Wolfe decomposition prin ciple, which the analysis of the iterative method is based on, allows convergence to <p> This relationship then provides interesting insight into the iterative approximation framework. In the remainder of this paper, we abbreviate the Dantzig-Wolfe decomposition principle as the DW decomposition, and refer to the work of Kunshner and Chen <ref> [15] </ref> as the DW approach for Markov decision processes. 6.1 Linear Programming and the Dantzig-Wolfe De composition Principle To cope with linear programs of very large sizes, decomposition principles [16] that divide a large linear program into many correlated linear programs of smaller sizes have been well studied, among which the <p> Kushner and Chen <ref> [15] </ref> investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> Given a solution to (5) or (6), we can determine a nondeterministic policy where E i;a = P a E i;a is the probability of mapping state i to action a. 6.2.2 The DW Approach in Solving Markov Decision Processes Kushner and Chen <ref> [15] </ref> investigate the use of the DW decomposition in solving Markov decision processes formulated as the linear programs in (5) and (6). <p> We briefly summarize the use of DW approach in solving Markov decision processes as linear programs in the following and refer the readers to <ref> [15] </ref> for more details. 1. Choose a partition Q = fU; K 1 ; : : : ; K h g such that X divides into iso lated pieces K l 's, 1 l h when we remove U from X . 2. <p> Therefore the corresponding iterative method also converges to an optimal solution of the corresponding Markov decision process in a finite number iterations. 23 7.2 Overview of the Iterative Method By appropriately interpreting the DW approach in <ref> [15] </ref> under the iterative approximation framework, it yields the following iterative method. 1. <p> The solution quality is improved iteratively, and converge to optimum. In the following, we (i) provide the entire picture of the iterative method, and (ii) indicate its equivalence to the DW approach that uses the DW decomposition to solve Markov decision processes as linear programs <ref> [15] </ref>. In the remainder of this paper, we focus on the criterion of expected cumulative cost to reach target states. <p> The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [7] [8] and decomposing large systems generally [4] [16] and Markov decision processes specifically <ref> [15] </ref>.
Reference: [16] <author> Lasdon, Leon S., </author> <title> Optimization Theory for Large Systems, </title> <publisher> (Macmillan Company, </publisher> <year> 1970). </year>
Reference-contexts: The details of the material presented in this section depend on some understanding of linear programming [3] and methods for decomposing and solving large systems <ref> [16] </ref>. We sketch the iterative method in the following, and refer the readers to the following two sections for more details. 1. <p> this paper, we abbreviate the Dantzig-Wolfe decomposition principle as the DW decomposition, and refer to the work of Kunshner and Chen [15] as the DW approach for Markov decision processes. 6.1 Linear Programming and the Dantzig-Wolfe De composition Principle To cope with linear programs of very large sizes, decomposition principles <ref> [16] </ref> that divide a large linear program into many correlated linear programs of smaller sizes have been well studied, among which the DW decomposition may be the most well known. <p> In the following, we describe the general form of the DW decomposition without assuming additional structure in linear programs. 16 We refer the readers to <ref> [16] </ref> [19] for more details about linear programming and the DW decomposition. <p> X to AX = b can be properly represented as X = 1ir X j X (j) ; where * X i 's, 1 i r, and X j 's, 1 j t are N fi 1 column vectors representing the finite number of extreme points and extreme rays 3 <ref> [16] </ref> [19] respectively of the set fxjA 2 X = b 2 g, and * i 's and j 's must satisfy the following constraints A 1 ( 1ir i X i + 1jt 1ir i = 1 j 0; 8j = 1 : : : t: &gt; &gt; &gt; = <p> However, it is infeasible to exhaustively enumerate all these extreme points and extreme rays in general. To cope with this difficulty, a subproblem is devised according to the current simplex multipliers 4 <ref> [16] </ref> [19] associated with the m + 1 linear constraints in the master problem (3). <p> The analysis in this paper of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [7] [8] and decomposing large systems generally [4] <ref> [16] </ref> and Markov decision processes specifically [15].
Reference: [17] <author> Lin, Shieu-Hong and Dean, Thomas, </author> <title> Exploiting Locality in Temporal Reasoning, </title> <editor> Sandewall, E. and Backstrom, C., (Eds.), </editor> <booktitle> Current Trends in AI Planning, </booktitle> <address> Amsterdam, </address> <publisher> IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: The problem of automatically constructing such a partition is not addressed in this paper, but see <ref> [17] </ref> for some relevant techniques. Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces.
Reference: [18] <author> Moore, Andrew W. and Atkeson, Christopher G., </author> <title> The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State Spaces, </title> <note> To appear in Machine Learning, </note> <year> 1995. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] [2]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces <ref> [18] </ref> and stochastic models and discrete state spaces [11]. The hierarchical policy construction method described in Section 4 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [11] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces.
Reference: [19] <author> M.S. Bazaraa, H. D. Sherali, J. J. Jarvis, </author> <title> Linear Programming and Network Flows, </title> <publisher> (John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990). </year> <month> 44 </month>
Reference-contexts: and space complexity of this iterative method. * Empirical experience suggests that the Dantzig-Wolfe decomposition prin ciple, which the analysis of the iterative method is based on, allows convergence to within 1-5% of the optimum fairly quickly, although the tail convergence rate can be very slow (see page 325 in <ref> [19] </ref>). <p> In the following, we describe the general form of the DW decomposition without assuming additional structure in linear programs. 16 We refer the readers to [16] <ref> [19] </ref> for more details about linear programming and the DW decomposition. <p> to AX = b can be properly represented as X = 1ir X j X (j) ; where * X i 's, 1 i r, and X j 's, 1 j t are N fi 1 column vectors representing the finite number of extreme points and extreme rays 3 [16] <ref> [19] </ref> respectively of the set fxjA 2 X = b 2 g, and * i 's and j 's must satisfy the following constraints A 1 ( 1ir i X i + 1jt 1ir i = 1 j 0; 8j = 1 : : : t: &gt; &gt; &gt; = : <p> However, it is infeasible to exhaustively enumerate all these extreme points and extreme rays in general. To cope with this difficulty, a subproblem is devised according to the current simplex multipliers 4 [16] <ref> [19] </ref> associated with the m + 1 linear constraints in the master problem (3). <p> In particular, if ffi fi is zero or negative, we have reached an optimal solution of the master problem. Both the master problem and the subproblems center around the use of simplex method in solving the linear program in (3). We refer the readers to <ref> [19] </ref> for more details about (i) cycling prevention and (ii) the initialization of the simplex method using big-M method. We briefly summarize two important properties about the DW decomposition that are useful in the remainder of this paper. <p> Remark Such an initialization corresponds to the derivation of an initial so lution for the master problem using big-M method. 6 M is exactly the value used in big-M method to initiate the simplex method. We refer the readers to <ref> [19] </ref> about the choice of an appropriate M . 32 * The m + 1 dummy policies correspond to m + 1 artificial variables added into the master problem. <p> Every artificial variable is associated the cost M , which is a large positive value. M is exactly the value used in big-M method to initiate the simplex method. We refer the readers to <ref> [19] </ref> for the details of the big-M method and the issue of choosing an appropriate M . * When applying the big-M method, some of the artificial variables may leave and enter the basis more than one time.
Reference: [20] <author> Papadimitriou, Christos H. and Tsitsiklis, John N., </author> <title> The Complexity of Markov Chain Decision Processes, </title> <institution> Mathematics of Operations Research, </institution> <month> 12(3) </month> <year> (1987) </year> <month> 441-450. </month>
Reference-contexts: Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces. There exist methods for computing policies that are polynomial in the size of the state and action spaces <ref> [20] </ref> [21], but these methods are impractical for large state spaces (e.g., &gt; 10 6 states, given 20 state variables).
Reference: [21] <author> Puterman, Martin L., </author> <title> Markov Decision Processes, </title> <publisher> (John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994). </year>
Reference-contexts: Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces. There exist methods for computing policies that are polynomial in the size of the state and action spaces [20] <ref> [21] </ref>, but these methods are impractical for large state spaces (e.g., &gt; 10 6 states, given 20 state variables). <p> Howard's policy iteration [10] iterates by first computing the expected cumulative cost function for the current policy and then improving the policy by using this cost function. Policy iteration is guaranteed to converge to the optimal policy in O (N 3 ) iterations. Puterman <ref> [21] </ref> provides an up-to-date overview of algorithms for solving Markov decision processes. In iterative methods, it is often useful to be able to compute a bound on the difference between the value of the current solution and the optimum. Let fl denote an optimal policy for M. <p> Hierarchical policy construction does not guarantee to produce an optimal policy; however, it does so relatively efficiently and has an intuitive interpretation that makes it suitable for robot navigation domains. For a simple partition of the state space with no aggregation within regions, standard algorithms <ref> [21] </ref> on the base-level state space would be dominated by a factor quadratic in the size of the state space (j X j), while hierarchical policy construction would be dominated by the number of regions in the partition (jP j) times the maximum number of neighbors for any region (max R2P <p> less than or equal to the value of an optimal solution plus ffi fi where ffi fi = fi 0 z fl fi . 6.2 The DW Decomposition and Markov Decision Pro cesses It is well known that we can solve Markov decision processes as linear programs [7] [8] [14] <ref> [21] </ref> using standard techniques like the simplex method. Kushner and Chen [15] investigate the use of the DW decomposition in solving Markov decision processes as linear programs. <p> different approaches, and (ii) indicate when we should apply the decomposition tech niques. 9.1 Modeling the Computation Complexity of the Stan dard Techniques Note that * Standard techniques like the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic con vergence rate respectively <ref> [21] </ref>, and * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space [8] [21]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where <p> the policy iteration method and the value iteration method converge to optimal policies in linear and quadratic con vergence rate respectively <ref> [21] </ref>, and * Each iteration of the policy iteration method and the value iteration method takes time cubic and quadratic respectively in the size of the state space [8] [21]. * We model the computational complexity of directly applying the standard techniques to the state space as cn k , where (i) both c and k are constants and (ii) n is the size of the state space. k is 3 and 2 respectively when applying the policy iteration method
Reference: [22] <author> Sacerdoti, Earl, </author> <title> Planning in a Hierarchy of Abstraction Spaces, </title> <journal> Artificial Intelligence, </journal> <month> 7 </month> <year> (1974) </year> <month> 231-272. </month>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [13] and hierarchies of state-space operators <ref> [22] </ref> [12]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [23] [2]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces [11].
Reference: [23] <author> Zhong, H. and Wonham, W. M., </author> <title> On the Consistency of Hierarchical Supervision in Discrete-Event Systems, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35(10) (1990) 1125-1134. </volume> <pages> 45 </pages>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [13] and hierarchies of state-space operators [22] [12]. Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines <ref> [23] </ref> [2]. In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [18] and stochastic models and discrete state spaces [11].
References-found: 23

