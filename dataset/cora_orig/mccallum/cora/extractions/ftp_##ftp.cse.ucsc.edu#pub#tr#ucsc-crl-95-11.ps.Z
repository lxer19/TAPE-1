URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-95-11.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~karplus/research.html
Root-URL: http://www.cse.ucsc.edu
Title: Regularizers for Estimating Distributions of Amino Acids from Small Samples  
Author: Kevin Karplus ucsc-crl-- 
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: 30 March 1995  
Abstract-found: 0
Intro-found: 0
Reference: [ACL89] <author> Stephen F. Altschul, Raymond J. Carroll, and David J. Lipman. </author> <title> Weights for data related by a tree. </title> <journal> JMB, </journal> <volume> 207 </volume> <pages> 647-653, </pages> <year> 1989. </year> <note> 22 References </note>
Reference-contexts: Each column t of a multiple alignment will give us a count of amino acids, F t (i). If we use sequence weights (such as those suggested in <ref> [ACL89] </ref> or [HH94]), then F t (i) is the sum of the sequence weights for sequences having amino acid i in column t. These "counts" need not be integers. <p> Other weighting schemes have been used by other researchers (for example, tree distances [THG94] or weighting for pairs of alignments <ref> [ACL89] </ref>), and the position-specific one used here was chosen rather arbitrarily for its ease of computation. The experiments that used the same set for training and testing used the entire blocks database, but separate train-test sets were created as disjoint random subsets.
Reference: [Alt91] <author> Stephen F. Altschul. </author> <title> Amino acid substitution matrices from an information theoretic perspective. </title> <journal> JMB, </journal> <volume> 219 </volume> <pages> 555-565, </pages> <year> 1991. </year>
Reference-contexts: For example, in sequence-sequence alignment, the traditional scoring matrices assign a positive score for each amino acid that would be a good match to the one in the reference sequence, and a negative score to each that would be a poor match. As Altschul pointed out <ref> [Alt91] </ref>, any alignment scoring system is really making an assertion about the probability of the test sequences given the reference sequence. The score for an alignment is the sum of the scores for individual matched positions, plus the costs for insertions and deletions. <p> Then the score for matching test amino acid i to reference amino acid j is log b ( ^ P j (i)= ^ P 0 (i)) for some arbitrary logarithmic base b. <ref> [Alt91] </ref> Any method for estimating the probabilities ^ P j (i) and ^ P 0 (i) defines a match scoring system for sequence-sequence alignment. Rather than looking at the final scoring system, this paper will concentrate on the methods that can be used for estimating the probabilities themselves. <p> The scores are best interpreted as the logarithm of the ratio of the probability of the amino acid in the context to the background probability <ref> [Alt91] </ref>: M i;j = log ^ P s (i)=P 0 (i) ; where s is a sample containing exactly one amino acid: j. The averaging of the score matrices is intended to create a new score.
Reference: [BCHM94] <author> P. Baldi, Y. Chauvin, T. Hunkapillar, and M. McClure. </author> <title> Hidden Markov models of biological primary sequence information. </title> <journal> PNAS, </journal> <volume> 91 </volume> <pages> 1059-1063, </pages> <year> 1994. </year>
Reference-contexts: Rather than looking at the final scoring system, this paper will concentrate on the methods that can be used for estimating the probabilities themselves. In more sophisticated models than single sequence alignments, such as multiple alignments, profiles [GME87], and hidden Markov models <ref> [KBM + 94, BCHM94] </ref>, we may have more than one reference sequence in our training set. Each position of such a model will define a context for which we to want to estimate the probabilities of the twenty amino acids.
Reference: [BHK + 93] <author> M. P. Brown, R. Hughey, A. Krogh, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Using Dirichlet mixture priors to derive hidden Markov models for protein families. </title> <editor> In L. Hunter, D. Searls, and J. Shavlik, editors, </editor> <booktitle> ISMB-93, </booktitle> <pages> pages 47-55, </pages> <address> Menlo Park, CA, July 1993. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Many of the regularizers in Section 3 have been validated in this way <ref> [HH92, BHK + 93, TAK94] </ref>. This sort of anecdotal evidence is very valuable for establishing that techniques are useful in real biological problems, but is very difficult to quantify. <p> Note, it is not claimed that these feature alphabets are the best one can do, but they are better than any of the hand-created feature sets tried, and as good as the ones found by my earlier search programs. 3.8 Dirichlet mixtures The Dirichlet mixture method <ref> [BHK + 93] </ref> has similarities to the pseudocount methods, but is somewhat more complex. They have been used quite successfully by several researchers [BHK + 93, TAK94, HH95]. <p> They have been used quite successfully by several researchers <ref> [BHK + 93, TAK94, HH95] </ref>. In Section 6, we'll see that Dirichlet mixtures are superior to all the other regularizers examined, and that there is not much room for improvement to better regularizers. <p> The 9-component mixture was provided by Kimmen Sjolander, and was optimized for a different function on the blocks database with all sequence weights equal. The optimization was to provide the best Bayesian prior for the set of observed count vectors <ref> [BHK + 93] </ref>. Sjolander's 9-component mixture is the best we have for jsj = 5, but it does fairly poorly for jsj = 0; 1; 2.
Reference: [Cla94] <author> Jean-Michael Claverie. </author> <title> Some useful statistical properties of position-weight matrices. </title> <journal> Computers and Chemistry, </journal> <volume> 18(3) </volume> <pages> 287-294, </pages> <year> 1994. </year>
Reference-contexts: However, the construction of their matrices is rather ad hoc, and probably not optimized for the task. A similar method was proposed by Claverie <ref> [Cla94] </ref>. His method is equivalent to setting z (i) = 0 and scaling the s (i) by max ( p jsj; jsj=20), instead of jsj.
Reference: [DSO78] <author> M. O. Dayhoff, R. M. Schwartz, and B. C. Orcutt. </author> <title> A model of evolutionary change in proteins. In Atlas of Protein Sequence and Structure, </title> <booktitle> chapter 22, </booktitle> <pages> pages 345-358. </pages> <publisher> National Biomedical Research Foundation, </publisher> <address> Washington, D. C., </address> <year> 1978. </year>
Reference-contexts: There are several standard scoring matrices in use, most notably the Dayhoff matrices <ref> [DSO78] </ref> and the BLOSUM matrices [HH92], which were originally created for aligning one sequence with another (jsj = 1).
Reference: [GME87] <author> Michael Gribskov, Andrew D. McLachlan, and David Eisenberg. </author> <title> Profile analysis: Detection of distantly related proteins. </title> <journal> PNAS, </journal> <volume> 84 </volume> <pages> 4355-4358, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Rather than looking at the final scoring system, this paper will concentrate on the methods that can be used for estimating the probabilities themselves. In more sophisticated models than single sequence alignments, such as multiple alignments, profiles <ref> [GME87] </ref>, and hidden Markov models [KBM + 94, BCHM94], we may have more than one reference sequence in our training set. Each position of such a model will define a context for which we to want to estimate the probabilities of the twenty amino acids. <p> Using symmetric reasoning for pseudocounts that are higher than expected from the background probabilities, we also expect that M, Q, and S are less conserved than other amino acids. 3.3 Gribskov average-score method The Gribskov profile <ref> [GME87] </ref> or average-score method [TAK94] computes the weighted average of scores from a score matrix M . There are several standard scoring matrices in use, most notably the Dayhoff matrices [DSO78] and the BLOSUM matrices [HH92], which were originally created for aligning one sequence with another (jsj = 1).
Reference: [GR65] <author> I. S. Gradshteyn and I. M. Ryzhik. </author> <title> Table of Integrals, Series, and Products. </title> <publisher> Academic Press, </publisher> <address> fourth edition, </address> <year> 1965. </year>
Reference: [HH91] <author> Steven Henikoff and Jorja G. Henikoff. </author> <title> Automated assembly of protein blocks for database searching. </title> <journal> NAR, </journal> 19(23) 6565-6572, 1991. 
Reference-contexts: Throughout this paper, the trusted alignments used are the BLOCKS database <ref> [HH91] </ref> with the sequence weighting scheme mentioned in Section 5. 2.1 Encoding cost The encoding cost (sometimes called conditional entropy ) is a good measure of the residual variation among sequences of the multiple alignment. <p> can do for samples of size k: H min;k = T s;jsj=k X T s (i) log 2 jT s j 1 X X T s (i) log 2 jT s j Table 2.1 shows this lower bound on average encoding cost of the columns of the Blocks multiple alignment <ref> [HH91] </ref> (see Section 5 for details on how the database is used in this paper), given that we have sampled jsj amino acids from each column. <p> The first method gives us an estimate of how well we can do with the best tuned regularizer, while the second method gives us an estimate of how well the regularizer generalizes to other similar problems. The multiple alignments chosen are the BLOCKS database <ref> [HH91] </ref>. The sequences are weighted using a slight variant of the Henikoffs' position-specific weighting scheme [HH94], as implemented by Kimmen Sjolander.
Reference: [HH92] <author> Steven Henikoff and Jorja G. Henikoff. </author> <title> Amino acid substitution matrices from protein blocks. </title> <journal> PNAS, </journal> <volume> 89 </volume> <pages> 10915-10919, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: For example, the small differences between the PAM scoring matrices and the BLOSUM scoring matrices have been shown to make a significant difference in the quality of search results <ref> [HH92] </ref>. The differences between regularizers is often fairly small. In this paper we attempt to quantify these small differences for several different methods for estimating the distributions. <p> Many of the regularizers in Section 3 have been validated in this way <ref> [HH92, BHK + 93, TAK94] </ref>. This sort of anecdotal evidence is very valuable for establishing that techniques are useful in real biological problems, but is very difficult to quantify. <p> There are several standard scoring matrices in use, most notably the Dayhoff matrices [DSO78] and the BLOSUM matrices <ref> [HH92] </ref>, which were originally created for aligning one sequence with another (jsj = 1).
Reference: [HH94] <author> Steven Henikoff and Jorja G. Henikoff. </author> <title> Position-based sequence weights. </title> <journal> JMB, </journal> <volume> 243(4) </volume> <pages> 574-578, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Each column t of a multiple alignment will give us a count of amino acids, F t (i). If we use sequence weights (such as those suggested in [ACL89] or <ref> [HH94] </ref>), then F t (i) is the sum of the sequence weights for sequences having amino acid i in column t. These "counts" need not be integers. <p> The multiple alignments chosen are the BLOCKS database [HH91]. The sequences are weighted using a slight variant of the Henikoffs' position-specific weighting scheme <ref> [HH94] </ref>, as implemented by Kimmen Sjolander.
Reference: [HH95] <author> Steven Henikoff and Jorja G. Henikoff. </author> <type> Personal communication, </type> <month> January </month> <year> 1995. </year>
Reference-contexts: They have been used quite successfully by several researchers <ref> [BHK + 93, TAK94, HH95] </ref>. In Section 6, we'll see that Dirichlet mixtures are superior to all the other regularizers examined, and that there is not much room for improvement to better regularizers. <p> In fact they are so close to the theoretical optimum for regularizers, that there doesn't seem to be much point in looking for better regularizers. The evaluations of regularizers for searches in biological contexts have also found Dirichlet mixtures to be superior <ref> [TAK94, HH95] </ref>, validating the more information-theoretic approach taken here. Although most applications (such as training hidden Markov models or building profiles from multiple alignments) do not require frequent evaluation of regularizers, there are some applications References 21 (such as Gibbs sampling) that require recomputing the regularizers inside an inner loop.
Reference: [Hig92] <author> Desmond G. Higgins. </author> <title> Sequence ordinations: a multivariate analysis approach to analysing large sequence data sets. </title> <journal> CABIOS, </journal> <volume> 8(1) </volume> <pages> 15-22, </pages> <year> 1992. </year>
Reference-contexts: Table 3.2 shows these principal eigenvectors for several substitution matrices. These principal eigenvectors are very similar to the background distribution, as one would expect. Eigenvectors have been used to examine distance matrices <ref> [TJ93, Hig92] </ref>, but I have not yet found a reference to the use of eigenvectors with substitution matrices. 10 3. Estimation methods Note that each diagonal element of an optimal substitution matrix, divided by the total weight of the column, reflects how conserved that amino acid is.
Reference: [JTT92] <author> David T. Jones, William R. Taylor, and Janet M. Thornton. </author> <title> The rapid generation of mutation data matrices from protein sequences. </title> <journal> CABIOS, </journal> <volume> 8(3) </volume> <pages> 275-282, </pages> <year> 1992. </year>
Reference-contexts: The value P (i; j)=P 0 (j) is known as the relatedness odds ratio and has been widely used, for example <ref> [JTT92] </ref>. 3.
Reference: [KBM + 94] <author> A. Krogh, M. Brown, I. S. Mian, K. Sjolander, and D. Haussler. </author> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> JMB, </journal> <volume> 235 </volume> <pages> 1501-1531, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Rather than looking at the final scoring system, this paper will concentrate on the methods that can be used for estimating the probabilities themselves. In more sophisticated models than single sequence alignments, such as multiple alignments, profiles [GME87], and hidden Markov models <ref> [KBM + 94, BCHM94] </ref>, we may have more than one reference sequence in our training set. Each position of such a model will define a context for which we to want to estimate the probabilities of the twenty amino acids.
Reference: [LAB + 93] <author> C. E. Lawrence, S. Altschul, M. Boguski, J. Liu, A. Neuwald, and J. Wootton. </author> <title> Detecting subtle sequence signals: A Gibbs sampling strategy for multiple alignment. </title> <journal> Science, </journal> <volume> 262 </volume> <pages> 208-214, </pages> <year> 1993. </year>
Reference-contexts: For jsj = 0, we can get ^ P 0 (i) = P 0 (i), by setting z (i) = aP 0 (i), for any positive constant a. This setting of the pseudocounts has been referred to as background pseudocounts <ref> [LAB + 93] </ref> or the Bayesian prediction method [TAK94] (for the Bayesian interpretation of pseudocounts, see Appendix B). For the Blocks database and jsj &gt; 0, the optimal value of a is near 1.0.
Reference: [Mil93] <author> Aleksandar Milosavljevic. </author> <title> Discovering sequence similarity by the algorithmic significance method. </title> <booktitle> In ISMB-93, </booktitle> <pages> pages 284-291, </pages> <address> Menlo Park, </address> <year> 1993. </year>
Reference-contexts: Since entropy is additive, the encoding cost for independent columns can be added to get the encoding cost for entire sequences, and strict significance tests can be applied by looking at the difference in encoding cost between a hypothesized model and a null model <ref> [Mil93] </ref>. Each column t of a multiple alignment will give us a count of amino acids, F t (i).
Reference: [SS90] <author> Randall F. Smith and Temple F. Smith. </author> <title> Automatic generation of primary sequence patterns form sets of related protein sequences. </title> <journal> PNAS, </journal> <volume> 87 </volume> <pages> 118-122, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Feature alphabets have been used for creating patterns for searching protein databases <ref> [SS90] </ref>, and feature sets have been quite popular for describing sets of amino acids (for example, [Tay86]). The feature alphabets are used in quite a different way by Smith and Smith [SS90] than the ones presented here, since their scores were computed based only on the set of amino acids previously <p> Feature alphabets have been used for creating patterns for searching protein databases <ref> [SS90] </ref>, and feature sets have been quite popular for describing sets of amino acids (for example, [Tay86]). The feature alphabets are used in quite a different way by Smith and Smith [SS90] than the ones presented here, since their scores were computed based only on the set of amino acids previously seen in a context, not on the frequencies. Their method could be roughly approximated by the feature alphabet in Table 3.3. <p> Estimation methods features zero offsets A, C, D, E, F, G, H, I, V, K, L, M, N, P, Q, R, S, T, W, Y 0.168058 DEKRHNQST, ILVFWYCM, AG, P 2.062930 Table 3.3: Feature alphabet set based on the amino acid class hierarchy <ref> [SS90] </ref>.
Reference: [SS91] <author> C. Sander and R. Schneider. </author> <title> Database of homology-derived protein structures and the structural meaning of sequence alignment. </title> <journal> Proteins, </journal> <volume> 9(1) </volume> <pages> 56-68, </pages> <year> 1991. </year>
Reference-contexts: The appropriate regularizers for more variable columns may look somewhat different, though one would expect the pseudocount and substitution matrix methods to degrade more than the Dirichlet mixtures, which naturally handle high variability. I plan to build regularizers for the HSSP structural alignments <ref> [SS91] </ref> to check that Dirichlet mixtures are the most effective in that application as well. To get significantly better performance than a Dirichlet mixture regularizer, we have to step away from using a pure regularizer that only knows about the sample of amino acids seen in the context.
Reference: [TAK94] <author> Roman L. Tatusov, Stephen F. Altschul, and Eugen V. Koonin. </author> <title> Detection of conserved segments in proteins: Iterative scanning of sequence databases with alignment blocks. </title> <journal> PNAS, </journal> <volume> 91 </volume> <pages> 12091-12095, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Many of the regularizers in Section 3 have been validated in this way <ref> [HH92, BHK + 93, TAK94] </ref>. This sort of anecdotal evidence is very valuable for establishing that techniques are useful in real biological problems, but is very difficult to quantify. <p> For jsj = 0, we can get ^ P 0 (i) = P 0 (i), by setting z (i) = aP 0 (i), for any positive constant a. This setting of the pseudocounts has been referred to as background pseudocounts [LAB + 93] or the Bayesian prediction method <ref> [TAK94] </ref> (for the Bayesian interpretation of pseudocounts, see Appendix B). For the Blocks database and jsj &gt; 0, the optimal value of a is near 1.0. <p> Using symmetric reasoning for pseudocounts that are higher than expected from the background probabilities, we also expect that M, Q, and S are less conserved than other amino acids. 3.3 Gribskov average-score method The Gribskov profile [GME87] or average-score method <ref> [TAK94] </ref> computes the weighted average of scores from a score matrix M . There are several standard scoring matrices in use, most notably the Dayhoff matrices [DSO78] and the BLOSUM matrices [HH92], which were originally created for aligning one sequence with another (jsj = 1). <p> One easy way to accomplish this is to add the counts scaled by their sum: X s (i) jsjs (i) + z (i) + X M i;j s (j) : The method here is almost equivalent to the data-dependent pseudocount method <ref> [TAK94] </ref>. The data-dependent method sets X s (i) s (i) + j BP 0 (i)e A i;j s (j) ; 3. Estimation methods 11 for arbitrary parameters B and and a substitution matrix A. <p> They have been used quite successfully by several researchers <ref> [BHK + 93, TAK94, HH95] </ref>. In Section 6, we'll see that Dirichlet mixtures are superior to all the other regularizers examined, and that there is not much room for improvement to better regularizers. <p> In fact they are so close to the theoretical optimum for regularizers, that there doesn't seem to be much point in looking for better regularizers. The evaluations of regularizers for searches in biological contexts have also found Dirichlet mixtures to be superior <ref> [TAK94, HH95] </ref>, validating the more information-theoretic approach taken here. Although most applications (such as training hidden Markov models or building profiles from multiple alignments) do not require frequent evaluation of regularizers, there are some applications References 21 (such as Gibbs sampling) that require recomputing the regularizers inside an inner loop.
Reference: [Tay86] <author> William Ramsay Taylor. </author> <title> The classification of amino acid conservation. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 119 </volume> <pages> 205-218, </pages> <year> 1986. </year>
Reference-contexts: Feature alphabets have been used for creating patterns for searching protein databases [SS90], and feature sets have been quite popular for describing sets of amino acids (for example, <ref> [Tay86] </ref>). The feature alphabets are used in quite a different way by Smith and Smith [SS90] than the ones presented here, since their scores were computed based only on the set of amino acids previously seen in a context, not on the frequencies. <p> Their method could be roughly approximated by the feature alphabet in Table 3.3. A few feature alphabet sets were created by hand from the Taylor features <ref> [Tay86] </ref>, with from four to fifty-eight feature alphabets. The largest one consisted of one alphabet for the individual amino acids and 57 binary alphabets corresponding to the "most relevant" feature sets given in [Tay86, 12 3. <p> 2. features zero offsets A, C, D, E, F, G, H, I, V, K, L, M, N, P, Q, R, S, T, W, Y 0.102741 FILMPV, ACGHNQSTWY, DEKR 2.348270 DE, HKR, NQSTWY, ACFGILMPV 3.031990 AGS, C, DNPTV, EFHIKLMQRWY 3.624300 Table 3.4: Four-alphabet feature alphabet set based on Taylor's feature set <ref> [Tay86] </ref>. <p> Q, R, S, T, W, Y 0.172476 AGS, C, P, DE, ILV, KMNQRT, FHWY 1.87884 ACFGILMTVWY, DEHKNPQRS 2.59881 ACDEGIKLMNPQRSTV, FHWY 2.65647 CFILMPV, ADEGHKNQRSTWY 2.91754 ACDEFGILMNPQSTVWY, HKR 5.04085 CEFHIKLMQRVWY, ADGNPST 5.64207 FHILMVWY, ACDEGKNPQRST 6.23212 ACDEGIKLNPQRSTV, FHMWY 6.19452 ACDFGIKLMNSTV, EHPQRWY 7.54467 Table 3.5: Ten-alphabet feature alphabet set based on Taylor's feature set <ref> [Tay86] </ref>. Zero offsets were chosen for best performance on jsj = 1; 2. by grouping non-overlapping features into the same feature alphabet, and omitting the less useful feature alphabets. Two of these are shown in Tables 3.4 and 3.5. <p> Not changing the existing regularizer while doing the merging also makes it possible to apply the partitioning algorithm to other regularizers, since all that is needed are the X s (i) values for the 20 samples consisting of a single amino acid each. 1 Figure 5 of <ref> [Tay86] </ref> lists 61 sets, but several of them are complements of others in the list and complementary sets generate the same feature alphabet, giving only 57 distinct feature alphabets. 3. <p> The first three alphabet sets are presented in Tables 3.3 through 3.5. The "Taylor" column contains an 11-alphabet set consisting of the fundamental sets in <ref> [Tay86] </ref>, and the "Taylor-58" contains an alphabet for each feature set in [Tay86, Figure 5]. a different feature-based approach could work better. 6.7 Dirichlet mixtures Dirichlet mixtures are clearly the luxury choice among regularizers. <p> The first three alphabet sets are presented in Tables 3.3 through 3.5. The "Taylor" column contains an 11-alphabet set consisting of the fundamental sets in [Tay86], and the "Taylor-58" contains an alphabet for each feature set in <ref> [Tay86, Figure 5] </ref>. a different feature-based approach could work better. 6.7 Dirichlet mixtures Dirichlet mixtures are clearly the luxury choice among regularizers. The need for computing Gamma functions in order to evaluate the regularizer makes them much more expensive to use than any of the other regularizers reviewed here.
Reference: [THG94] <author> Julie D. Thompson, Desmond G. Higgins, and Toby J. Gibson. </author> <title> Improved sensitivity of profile searches through the use of sequence weights and gap excision. </title> <journal> CABIOS, </journal> <volume> 10(1) </volume> <pages> 19-29, </pages> <year> 1994. </year>
Reference-contexts: Other weighting schemes have been used by other researchers (for example, tree distances <ref> [THG94] </ref> or weighting for pairs of alignments [ACL89]), and the position-specific one used here was chosen rather arbitrarily for its ease of computation. The experiments that used the same set for training and testing used the entire blocks database, but separate train-test sets were created as disjoint random subsets.
Reference: [TJ93] <author> William R. Taylor and David T. Jones. </author> <title> Deriving an amino acid distance matrix. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 164 </volume> <pages> 65-83, </pages> <year> 1993. </year> <title> A. Partial derivatives for Dirichlet mixtures 23 </title>
Reference-contexts: Table 3.2 shows these principal eigenvectors for several substitution matrices. These principal eigenvectors are very similar to the background distribution, as one would expect. Eigenvectors have been used to examine distance matrices <ref> [TJ93, Hig92] </ref>, but I have not yet found a reference to the use of eigenvectors with substitution matrices. 10 3. Estimation methods Note that each diagonal element of an optimal substitution matrix, divided by the total weight of the column, reflects how conserved that amino acid is.
References-found: 23

