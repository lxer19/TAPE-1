URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/ML94.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: singh@psyche.mit.edu  tommi@psyche.mit.edu  jordan@psyche.mit.edu  
Title: Learning Without State-Estimation in Partially Observable Markovian Decision Processes  
Author: Satinder P. Singh Tommi Jaakkola Michael I. Jordan 
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences (E10) Massachusetts Institute of Technology  
Abstract: Reinforcement learning (RL) algorithms provide a sound theoretical basis for building learning control architectures for embedded agents. Unfortunately all of the theory and much of the practice (see Barto et al., 1983, for an exception) of RL is limited to Marko-vian decision processes (MDPs). Many real-world decision tasks, however, are inherently non-Markovian, i.e., the state of the environment is only incompletely known to the learning agent. In this paper we consider only partially observable MDPs (POMDPs), a useful class of non-Markovian decision processes. Most previous approaches to such problems have combined computationally expensive state-estimation techniques with learning control. This paper investigates learning in POMDPs without resorting to any form of state estimation. We present results about what TD(0) and Q-learning will do when applied to POMDPs. It is shown that the conventional discounted RL framework is inadequate to deal with POMDPs. Finally we develop a new framework for learning without state-estimation in POMDPs by including stochastic policies in the search space, and by defining the value or utility of a dis tribution over states.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G. & Anandan, P. </author> <year> (1985). </year> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15, </volume> <pages> 360-375. </pages>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <title> (to appear). Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence. </journal> <note> also, </note> <institution> University of Massachusetts, Amherst, </institution> <type> CMPSCI Technical Report 93-02. </type>
Reference: <author> Barto, A. G. & Duff, M. </author> <year> (1994). </year> <title> Monte carlo matrix inversion and reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: in defining optimal policies with the discounted payoff criterion. 7 Conclusion The motivation for this study came from the following simple observation: the first-principles definition of the value of a state under a fixed policy does not involve the Markov assumption and can be computed statistically via Monte Carlo evaluation <ref> (Barto and Duff, 1994) </ref>. This means that for any average payoff POMDP, given enough computational resources it is possible to determine the best policy from any finite set of policies with an arbitrarily high degree of confidence. Unfortunately hidden state introduced two complications.
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE SMC, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference-contexts: One can always apply RL algorithms developed specifically for Markovian processes to non-Markovian decision processes (N-MDPs) simply by treating the agent's sensor readings as state descriptions. There is some empirical evidence that such a technique can work well on particular non-Markovian problems <ref> (e.g., Barto et al., 1983) </ref>. However, as yet there is no theory of RL for N-MDPs, and no characterization of the class of N-MDPs on which conventional RL algorithms will perform reasonably well.
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. </author> <year> (1990). </year> <title> Sequential decision problems and neural networks. </title>
Reference-contexts: The agents are usually equipped with sensors that provide information about the state of the environment. Reinforcement learning (RL) techniques provide a sound theoretical basis for building learning control architectures for embedded agents <ref> (Barto et al., to appear, 1990) </ref>. Unfortunately all of the elegant theory of RL is limited to Markovian decision processes (MDPs) (Sut-ton, 1988; Watkins and Dayan, 1992; Dayan, 1992, Jaakkola et al., 1994, Tsitsiklis, to appear).
Reference: <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Other choices may exist for P X , but they are unlikely to be reasonable for all POMDPs. Average Payoff POMDPs: A second policy evaluation criterion that has been studied in the MDP literature <ref> (e.g., Bertsekas, 1987) </ref> and more recently in the RL literature (Schwartz, 1993; Singh, 1994) is the av erage payoff per time step criterion. <p> The average payoff per time step for a policy in is unaffected by the agent's inability to sense the state of the environment. Let the average payoff for policy be denoted fl . The relative value function in average payoff MDPs is defined as follows <ref> (see Bertsekas, 1987) </ref>: V (s) = a2A + s 0 2S # From here onwards we will use a subscript of fl to distinguish the value function for a discounted decision problem.
Reference: <author> Chrisman, L. </author> <title> Planning for closed-loop execution using partially observable markovian decision processes. </title> <note> Submitted to AAAI, </note> <year> 1992. </year>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In AAAI-92. </booktitle>
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8 (3/4), </type> <pages> 341-362. </pages>
Reference: <author> Jaakkola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1994). </year> <title> Stochastic convergence of iterative DP algorithms. In Advances in Neural Information Processing Systems 6. </title> <note> also submitted to Neural Computation. </note>
Reference-contexts: In this paper we developed a framework for assigning values to observations in POMDPs that does not involve the Markov assumption. In a subsequent paper, we present a new Monte Carlo algorithm for solving average payoff POMDPs that can do an efficient search of the infinite stochastic policy space <ref> (Jaakkola, Singh, and Jordan, 1994) </ref> as defined in this paper.
Reference: <author> Jaakkola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1994). </year> <title> Monte Carlo reinforcement learning in non-markovian decision problems. </title> <note> Submitted to COLT94. </note>
Reference-contexts: In this paper we developed a framework for assigning values to observations in POMDPs that does not involve the Markov assumption. In a subsequent paper, we present a new Monte Carlo algorithm for solving average payoff POMDPs that can do an efficient search of the infinite stochastic policy space <ref> (Jaakkola, Singh, and Jordan, 1994) </ref> as defined in this paper.
Reference: <author> Lin, L. J. & Mitchell, T. M. </author> <year> (1992). </year> <title> Reinforcement learning with hidden states. </title> <booktitle> In In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats. </booktitle>
Reference-contexts: Such approaches build an internal representation of the state of the environment by combining sensor readings with past internal representations (Whitehead and Lin, 1993). Several different forms of internal representations have been used: tapped-delay line representations for higher order Markov problems <ref> (e.g., Lin and Mitchell, 1992) </ref>, recurrent neural network based representations (Lin and Mitchell, 1992), and probability distributions over an underlying state space based on the theory of partially observable MDPs (Sondik, 1978; Chrisman, 1992a, 1992b; McCallum, 1993). <p> Several different forms of internal representations have been used: tapped-delay line representations for higher order Markov problems (e.g., Lin and Mitchell, 1992), recurrent neural network based representations <ref> (Lin and Mitchell, 1992) </ref>, and probability distributions over an underlying state space based on the theory of partially observable MDPs (Sondik, 1978; Chrisman, 1992a, 1992b; McCallum, 1993).
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <editor> In Utgoff, P. (Ed.), </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 190-196. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Narendra, K. S. & Thathachar, M. A. L. </author> <year> (1974). </year> <title> Learning automata|A survey. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 4, </volume> <pages> 323-334. </pages>
Reference: <author> Ross, S. </author> <year> (1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Such a policy is called an optimal policy. It is known that for every finite MDP there exists a stationary deterministic policy, fl : S ! A that is optimal <ref> (see Ross, 1983) </ref>. Therefore, in MDPs the agent can restrict its search to the finite set of stationary deterministic policies. In N-MDPs the control agent has sensors that return some estimate of the state of the environment.
Reference: <author> Schwartz, A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth Machine Learning Conference. </booktitle>
Reference: <author> Singh, S. P. </author> <year> (1994). </year> <title> Reinforcement learning algorithms for average-payoff markovian decision processes. </title> <booktitle> In Proceedings of the Twelth National Conference on Artificial Intelligence, </booktitle> <address> Seattle,WA. </address>
Reference-contexts: In this paper we developed a framework for assigning values to observations in POMDPs that does not involve the Markov assumption. In a subsequent paper, we present a new Monte Carlo algorithm for solving average payoff POMDPs that can do an efficient search of the infinite stochastic policy space <ref> (Jaakkola, Singh, and Jordan, 1994) </ref> as defined in this paper.
Reference: <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable markov processes over the infinite horizon: discounted case. </title> <journal> Operations Research, </journal> <volume> 26, </volume> <pages> 282-304. </pages>
Reference-contexts: A further drawback is that state estimation is computationally expensive and can require a large amount of data. Even if the true environment has a finite number of states, using state-estimation can result in a continuous space of estimated states making the search problem difficult <ref> (e.g., Sondik, 1978) </ref>. Also the computations performed by the learning control component are wasted until the state-estimation component becomes accurate enough to be useful. <p> When the underlying, non-observable MDP is in state s, the sensor reading, or observation, is X with fixed probability P (Xjs). Note that P (Xjs) is independent of the agent's policy. Such an N-MDP is called a partially observable MDP, or POMDP <ref> (e.g., Sondik, 1978) </ref>. In this paper we will consider only POMDPs. Henceforth we will use the word state to refer to an element of the set S, and the word observation to refer to an element of the set X .
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: We show why it is difficult to extend the conventional discounted RL framework to environments with hidden state. We present results about what TD (0) <ref> (Sutton, 1988) </ref> and Q-learning (Watkins, 1989) will do when applied to a class of N-MDPs. <p> No algorithm for POMDPs can retain that advantage because the control policy followed 2 Unlike TD (0), the more general family of TD ( &gt; 0) <ref> (Sutton, 1988) </ref> algorithms average multi-step predictions and will therefore learn a value function that assigns a higher value to observation 2 than observation 3 in the POMDP defined in Figure 2.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, it should be pointed out that the definition of an optimal policy suggested in this paper is somewhat arbitrary because the only reason to restrict the search space to stationary policies is computational economics. Note that RL researchers <ref> (Sutton, 1990) </ref> and learning automata researchers (e.g., Narendra and Thathachar,1974; Barto and Anandan, 1985) have used stochastic policies in the past, but as intermediate policies to ensure sufficient exploration, and always with the view that the ultimate goal is to learn the best deterministic policy.
Reference: <author> Sutton, R. S. </author> <year> (1994). </year> <type> personal communication. </type>
Reference: <author> Tsitsiklis, J. </author> <title> (to appear). Asynchronous stochastic approximation and Q-learning. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cam-bridge, England. </address>
Reference-contexts: We show why it is difficult to extend the conventional discounted RL framework to environments with hidden state. We present results about what TD (0) (Sutton, 1988) and Q-learning <ref> (Watkins, 1989) </ref> will do when applied to a class of N-MDPs. <p> true value function of observation 2, however, will be higher than the true value function of observation 3 because observation 2 reliably leads to a payoff of two after a delay of one time step, while observation 3 does not. 2 5 OPTIMAL CONTROL 5.1 WHAT DOES Q-LEARNING LEARN? Q-learning <ref> (Watkins, 1989) </ref> is a RL algorithm for finding optimal policies in MDPs. One of the big advantages of Q-learning is that it separates exploration from control.
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. D. </author> <year> (1992). </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX. </address> <publisher> M. </publisher>
Reference: <author> Whitehead, S. D. & Lin, L. J. </author> <year> (1993). </year> <title> Reinforcement learning in non-markov environments. </title> <note> working paper. </note>
Reference-contexts: Such approaches build an internal representation of the state of the environment by combining sensor readings with past internal representations <ref> (Whitehead and Lin, 1993) </ref>.
References-found: 28

