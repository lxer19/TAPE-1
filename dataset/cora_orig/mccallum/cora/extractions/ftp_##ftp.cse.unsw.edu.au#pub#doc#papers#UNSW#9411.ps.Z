URL: ftp://ftp.cse.unsw.edu.au/pub/doc/papers/UNSW/9411.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/research/tr.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: kevine@vast.unsw.edu.au  
Title: Issues in Implementing Virtual Memory  
Author: Kevin Elphinstone Stephen Russell Gernot Heiser 
Date: 29 SEPTEMBER 1994  
Address: NSW 2052 Australia  
Affiliation: School of Computer Science and Engineering University of  
Pubnum: UNSW-CSE-TR-9411  
Abstract-found: 0
Intro-found: 1
Reference: [ABC + 83] <author> M. Atkinson, P. Bailey, K. Chisholm, P. Cockshott, and R. Morrison. </author> <title> An approach to persistent programming. </title> <journal> The Computer Journal, </journal> <volume> 26, </volume> <year> 1983. </year>
Reference-contexts: These factors reduce the impact of memory-mapped files on PT size and fragmentation. 2 Page table entries may also contain modify, reference, and valid bits together with cacheability information and operating system specific bits. 4 3.3.2 Persistent Systems Persistent systems <ref> [ABC + 83] </ref> allow arbitrary data structures, including pointers, to be stored in a name-space that effectively replaces a file system.
Reference: [CBJ92] <author> J. Bradley Chen, Anita Borg, and Norman P. Jouppi. </author> <title> A simulation based study of TLB performance. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The likely improvements 10 in performance that can be achieved by replacement policy are minor <ref> [UNS + 94, CBJ92] </ref>. Hardware designers typically choose random replacement, as the cost involved in implementing other policies in hardware outweigh the performance benefits to be gained. Performance improvements through software involvement in replacement policy are doubtful. The costs of software misses are a tradeoff between speed and memory consumption. <p> Increasing the number of TLB entries is an obvious solution, as illustrated by Chen <ref> [CBJ92] </ref>. However, large fully associative structures are difficult to build. Reducing the associativityto increase the number of entries is a valid technique [UNS + 94, TH94]. However, it is not clear whether the number of entries could be increased sufficiently to cover a significant proportion of larger memory sizes. <p> Much larger page sizes appear unusable for general use. The use of multiple page sizes to combat both TLB coverage limitations and internal fragmentation seems to be the best approach. Investigations have shown <ref> [TKHP92, CBJ92, TH94] </ref> a dramatic decrease in TLB miss overheads with only a moderate increase in working set size.
Reference: [CLBHL92] <author> Jeffrey S. Chase, Henry M. Levy, Miche Baker-Harvey, and Ed-ward D. Lazowska. </author> <title> How to use a 64-bit virtual address space. </title> <type> Technical Report 92-03-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, </address> <year> 1992. </year>
Reference-contexts: As has been argued earlier, current translation schemes cannot support such sparse small-grain address spaces efficiently. 3.3.3 Single Address Space OS A particular class of persistent systems are single address space operating systems <ref> [HERV94, CLBHL92, MWO + 93] </ref>. These use permanent address allocation and thus suffer from the virtual address space fragmentation described above. One of the major properties of these systems is their separation of translation and protection.
Reference: [CM88] <author> Albert Chang and Mark F. Mergen. </author> <title> 801 storage: Architecture and programming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1), </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: Two basic techniques have been used: inverted page tables and directly hashed page tables. 4.1.1 Inverted Page Tables Inverted page tables (IPT) are characteristic of large address space architectures such as the System/38 [IBM78], the 801 <ref> [CM88] </ref> and the HP Precision Architecture [Lee89]. These architectures used short form addresses together with address space registers to generate long form addresses of 40 to 64 bits in length. Translation of these long form addresses was the motivation for using hashing techniques.
Reference: [Den70] <author> Peter J. Denning. </author> <title> Virtual memory. </title> <journal> Computing Surveys, </journal> <volume> 2(3), </volume> <month> Septem-ber </month> <year> 1970. </year>
Reference-contexts: 1 Introduction Virtual memory (VM) has been in use for over thirty years <ref> [Den70] </ref> and paged VM is now used in almost all modern computer systems.
Reference: [Elp93] <author> Kevin Elphinstone. </author> <title> Address space management issues in the Mungi operating system. </title> <type> Technical Report 9312, </type> <institution> School of Computer Science and Engineering, University of New South Wales, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: It is common in these systems to not re-use addresses to ensure uniqueness over time. This results in a sparse trail of currently allocated objects in a large virtual address space, whereas a reuse policy results in a much smaller virtual address range which is densely packed <ref> [Elp93, HERV94] </ref>. In either case, persistent systems lead to a large number of potentially quite small objects within a virtual address space.
Reference: [Hei93] <author> Joe Heinrich. </author> <title> MIPS R4000 user's manual. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year> <month> 13 </month>
Reference-contexts: The simple structure of VM management described above is no longer efficient, or even adequate. The following is a description of recent advances in system architecture and how they effect traditional VM systems. 3.1 64 bit Processors The advent of 64 bit microprocessors <ref> [Hei93, Sit92] </ref> has an immediate effect on the VM system. The need to translate a larger number of virtual address bits enlarges the silicon required for the TLB and affects the data structure used to store mappings between virtual and physical memory. <p> In order to reduce the effect of these overheads, DEC suggest for the Alpha [Sit92] a three-level tree which is mapped in the kernel virtual address space. Nodes in this tree are large virtual structures but are only partially allocated. The MIPS R4000 <ref> [Hei93] </ref> uses what is in effect a two-level tree which is also mapped into the kernel virtual address space as a large virtual array. 1 Each region 1 The array is extremely large ( 2 52 entries) but extremely virtual. 2 of allocated virtual address space is assigned a number of
Reference: [HERV94] <author> Gernot Heiser, Kevin Elphinstone, Stephen Russell, and Jerry Vochteloo. Mungi: </author> <title> A distributed single address-space operating system. </title> <booktitle> In 17th Annual Computer Science Conference. Australian Computer Science Communications, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: At first glance it would appear that implementation methods for VM are well established and little room is left for further investigation. However, many recent changes in operating system (OS) design and hardware architectures are forcing a reexamination of VM techniques. The Mungi <ref> [HERV94] </ref> project is currently constructing an OS based on a shared single 64-bit address space. As part of our research, we have identified several important problems that are not adequately addressed by traditional approaches. <p> It is common in these systems to not re-use addresses to ensure uniqueness over time. This results in a sparse trail of currently allocated objects in a large virtual address space, whereas a reuse policy results in a much smaller virtual address range which is densely packed <ref> [Elp93, HERV94] </ref>. In either case, persistent systems lead to a large number of potentially quite small objects within a virtual address space. <p> As has been argued earlier, current translation schemes cannot support such sparse small-grain address spaces efficiently. 3.3.3 Single Address Space OS A particular class of persistent systems are single address space operating systems <ref> [HERV94, CLBHL92, MWO + 93] </ref>. These use permanent address allocation and thus suffer from the virtual address space fragmentation described above. One of the major properties of these systems is their separation of translation and protection.
Reference: [HH93] <author> Jerry Huck and Jim Hays. </author> <title> Architectural Support for Translation Table Management in Large Address Space Machines. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Inadequate TLB coverage causes more frequent TLB misses. Applications that spend 5-20% of their run time servicing TLB misses are now common, with extremes of 40% not unheard of <ref> [HH93] </ref>. Large physical memories also increase the size of the per-page translation entries in the page table. The number of bits required to index all physical frames is 3 now greater than the 26 or so 2 available in a 32 bit PT entry. <p> This requires physical memory to be contiguous, as holes in the physical memory layout require empty entries in the page table in order to preserve the indexing. This can be expensive if memory mapped I/O devices cannot be packed efficiently. 4.1.2 Hashed Page Table The hashed page table (HPT) <ref> [HH93] </ref> has been proposed to overcome some of the limitations of the IPT. It is a more general variant of the hardware technique for TLB replacement in the MONADS-PC [RKA92]. In a HPT (see Fig. 3) only a single table is used. <p> TLB refill simply becomes a matter of hashing the faulting address to find the TLB entry and loading it into the TLB. The HPT is demonstrably faster than the IPT <ref> [HH93] </ref>. This is mainly due to the IPT requiring access to two tables with two possible cache misses. The HPT only accesses one table with both the virtual address and frame number lying in the same cache line. <p> Mapping the occupied portions of the array will have a detrimental impact on TLB performance if many objects need to be mapped. Refill times using hashed page tables are slightly higher, but this method is better suited to large or sparse address spaces <ref> [HH93] </ref>. Software control of placement policy does have an effect on performance. Operating systems reserve TLB entries for either very frequently used pages, or pages whose TLB refill costs would be excessive.
Reference: [IBM78] <institution> IBM. IBM System/38 technical developments. Order no. G580-0237. IBM, </institution> <address> Atlanta, Ga., </address> <year> 1978. </year>
Reference-contexts: Two basic techniques have been used: inverted page tables and directly hashed page tables. 4.1.1 Inverted Page Tables Inverted page tables (IPT) are characteristic of large address space architectures such as the System/38 <ref> [IBM78] </ref>, the 801 [CM88] and the HP Precision Architecture [Lee89]. These architectures used short form addresses together with address space registers to generate long form addresses of 40 to 64 bits in length. Translation of these long form addresses was the motivation for using hashing techniques.
Reference: [KTM91] <author> Toyohiko Kagimasa, Kikuo Takahashi, and Toshiaki Mori. </author> <title> Adaptive storage management for very large virtual/real storage systems. </title> <booktitle> In 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: However these studies, together with others [KTNW93, Mog93], are quick to point out that very little research has been done to investigate the issues raised and overheads involved in managing multiple page sizes. Kagi-masa et al. <ref> [KTM91] </ref> describe a system using multiple page sizes in a partitioned address space, though their aim is to reduce storage costs, not TLB overheads. Talluri [TH94] proposes two sub-block TLB designs which provide some advantages of multiple pages sizes while requiring little or no operating system modifications.
Reference: [KTNW93] <author> Yousef A. Khaldi, Madhusudhan Talluri, Michael N. Nelson, and Dock Williams. </author> <title> Virtual Memory Support for Multiple Page Sizes. </title> <booktitle> In 4th Int'l Workshop on Workstation Operating Systems, </booktitle> <address> Napa, Califor-nia, </address> <month> October </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: The use of multiple page sizes to combat both TLB coverage limitations and internal fragmentation seems to be the best approach. Investigations have shown [TKHP92, CBJ92, TH94] a dramatic decrease in TLB miss overheads with only a moderate increase in working set size. However these studies, together with others <ref> [KTNW93, Mog93] </ref>, are quick to point out that very little research has been done to investigate the issues raised and overheads involved in managing multiple page sizes.
Reference: [Lee89] <author> Ruby B. Lee. </author> <title> Precision architecture. </title> <booktitle> Computer, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Two basic techniques have been used: inverted page tables and directly hashed page tables. 4.1.1 Inverted Page Tables Inverted page tables (IPT) are characteristic of large address space architectures such as the System/38 [IBM78], the 801 [CM88] and the HP Precision Architecture <ref> [Lee89] </ref>. These architectures used short form addresses together with address space registers to generate long form addresses of 40 to 64 bits in length. Translation of these long form addresses was the motivation for using hashing techniques. An IPT consists of two parts (see Fig. 2).
Reference: [Lie94] <author> J. Liedtke. </author> <title> Address space sparsity and fine granularity. </title> <booktitle> In 6th SIGOPS European Workshop, </booktitle> <address> SchloDagstuhl, Germany, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: As a result, it is relatively expensive to perform operations such as modifying protection, or inserting or deleting objects, because they need to be done frame by frame. An alternative approach has been proposed by Liedtke <ref> [Lie94] </ref>. It is a tree-based scheme called guarded page tables (GPT) that combines the advantages of relatively fast translation table updates for variable size objects without suffering from excessive fragmentation as a result of sparse allocation policies. <p> A 9 Virtual Address Guard 0 10111 101011101 10111 0 1 1 0 1 Data Page traditional page table tree can be considered to be a GPT with empty guards. A more detailed description of the translation process can be found in <ref> [Lie94] </ref>. It can be shown that at most 2k entries are need to map k pages. Page table size is thus a function of the number of mapped pages, and is independent of address space size and layout.
Reference: [Mil90] <author> Milan Milenkovic. </author> <title> Microprocessor memory management units. </title> <journal> IEEE Micro, </journal> <volume> 10(2), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Section 4 examines recent alternatives that have been proposed to overcome these problems. As we argue in our conclusion, however, none of the existing approaches have so far met the demands of modern OS design. 2 Background In the late 1980's microprocessors had remarkably similar memory management architectures <ref> [Mil90] </ref>. They all supported 32 bit paged virtual memory management via a translation lookaside buffer (TLB) and hierarchical page tables. In conventional tree-based page tables (PT), a virtual address is broken into several parts.
Reference: [Mog93] <author> Jeffrey C. Mogul. </author> <title> Big Memories on the Desktop. </title> <booktitle> In 4th Int'l Workshop on Workstation Operating Systems, </booktitle> <address> Napa, California, </address> <month> October </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: The use of multiple page sizes to combat both TLB coverage limitations and internal fragmentation seems to be the best approach. Investigations have shown [TKHP92, CBJ92, TH94] a dramatic decrease in TLB miss overheads with only a moderate increase in working set size. However these studies, together with others <ref> [KTNW93, Mog93] </ref>, are quick to point out that very little research has been done to investigate the issues raised and overheads involved in managing multiple page sizes.
Reference: [MWO + 93] <author> Kevin Murray, Tim Wilkinson, Peter Osmon, Ashley Saulsbury, Tom Stiemerling, and Paul Kelly. </author> <title> Design and Implementation of an Object-Oriented 64-bit Single Address Space Microkernel. </title> <type> Technical Report 9, </type> <institution> SARC, Dept. Computer Science, City University, </institution> <address> London, </address> <year> 1993. </year>
Reference-contexts: As has been argued earlier, current translation schemes cannot support such sparse small-grain address spaces efficiently. 3.3.3 Single Address Space OS A particular class of persistent systems are single address space operating systems <ref> [HERV94, CLBHL92, MWO + 93] </ref>. These use permanent address allocation and thus suffer from the virtual address space fragmentation described above. One of the major properties of these systems is their separation of translation and protection.
Reference: [NUS + 93] <author> David Nagle, Richard Uhlig, Tim Stanely, Stuart Sechrest, Trevor Mudge, and Richard Brown. </author> <title> Design Tradeoffs for Software-Managed TLBs. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year> <month> 14 </month>
Reference-contexts: The TLB is effectively divided into a region for operating system use and the rest is used for user-level mapping. The reserved entries typically map user page tables and kernel data. Changes in memory usage described in Section 3.3, and also identified by <ref> [NUS + 93, UNS + 94] </ref>, have increased the amount of kernel data and active page tables. The optimal number of reserved entries is a function of application and user-level server activity. Dynamically changing the number of reserved entries is proposed by Uhlig et al. [UNS + 94]. <p> Software improvements to TLB performance appear to be limited to data structure optimisations for refill and TLB placement policy optimisations. Careful consideration of TLB interaction with operating system design and activity <ref> [NUS + 93, UNS + 94] </ref> has shown that performance gains are possible in this area. 4.2.2 Hardware Currently, much research effort has been directed towards increasing TLB coverage, the focus being on increasing the number of TLB entries, increasing the page size, or using multiple page sizes.
Reference: [PH90] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture: a quantitative approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Today's systems ship with 32 MB of RAM and expand to at least 128 MB, and 512 MB is not uncommon. The rate of increase is unlikely to slow as DRAM size quadruples every three years and memory requirements of programs increase 1.5 to 2 times every year <ref> [PH90] </ref>. The increase in memory requirements of applications has led to larger application working set sizes. Current TLB's have not kept pace with this increase in memory usage. The average microprocessor TLB size was 64 entries several years ago, with a 4 KB page size.
Reference: [RA85] <author> John Rosenberg and David Abramson. </author> <title> MONADS-PC A capability-based workstation to support software engineering. </title> <booktitle> In Proc 18th Hawaii Int'l Conf. on System Sciences, </booktitle> <year> 1985. </year>
Reference-contexts: These systems have similar impact on address-space management as the memory-mapped files described above. Other systems, like Monads <ref> [RA85] </ref>, allocate a permanent address for each object. It is common in these systems to not re-use addresses to ensure uniqueness over time.
Reference: [RKA92] <author> John Rosenberg, J. L. Keedy, and D Abramson. </author> <title> Address mechanisms for large virtual memories. </title> <journal> The Computer Journal, </journal> <volume> 35(4), </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: It is a more general variant of the hardware technique for TLB replacement in the MONADS-PC <ref> [RKA92] </ref>. In a HPT (see Fig. 3) only a single table is used. The entries contain page number and frame number and protection information, and are indexed using hashing.
Reference: [SAK + 89] <author> Eugene H. Spafford, James E. Allchin, Gregory Kenley, David V. Pitts, and C. Thomas Wilkes. </author> <title> Anatomy of a Multicomputer: The First Six Years of Clouds. </title> <publisher> Academic Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: The Mungi [HERV94] project is currently constructing an OS based on a shared single 64-bit address space. As part of our research, we have identified several important problems that are not adequately addressed by traditional approaches. Other new approaches to OS design, such as object-oriented systems <ref> [SGH + 89, SAK + 89] </ref>, have highlighted similar problems. This paper presents a review of current trends in VM implementation techniques and identifies the problems of traditional tree-based translation schemes. Section 4 examines recent alternatives that have been proposed to overcome these problems.
Reference: [SGH + 89] <author> M. Shapiro, Y. Gourhant, S. Habert, L. Mosseri, M. Ruffin, and C. Valot. </author> <title> Sos: an object-oriented operating system assessment and perspectives. </title> <journal> Computing Systems, </journal> <volume> 2(4) </volume> <pages> 287-338, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The Mungi [HERV94] project is currently constructing an OS based on a shared single 64-bit address space. As part of our research, we have identified several important problems that are not adequately addressed by traditional approaches. Other new approaches to OS design, such as object-oriented systems <ref> [SGH + 89, SAK + 89] </ref>, have highlighted similar problems. This paper presents a review of current trends in VM implementation techniques and identifies the problems of traditional tree-based translation schemes. Section 4 examines recent alternatives that have been proposed to overcome these problems.
Reference: [Sit92] <author> R. L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <institution> Digital Equipment Corporation, Maynard, </institution> <address> M.A., </address> <year> 1992. </year>
Reference-contexts: The simple structure of VM management described above is no longer efficient, or even adequate. The following is a description of recent advances in system architecture and how they effect traditional VM systems. 3.1 64 bit Processors The advent of 64 bit microprocessors <ref> [Hei93, Sit92] </ref> has an immediate effect on the VM system. The need to translate a larger number of virtual address bits enlarges the silicon required for the TLB and affects the data structure used to store mappings between virtual and physical memory. <p> The methods used in traditional 32-bit systems result in a five-level PT tree for 64-bit address spaces. The additional levels increase the cost of performing translations in the case of TLB misses. In order to reduce the effect of these overheads, DEC suggest for the Alpha <ref> [Sit92] </ref> a three-level tree which is mapped in the kernel virtual address space. Nodes in this tree are large virtual structures but are only partially allocated.
Reference: [TH94] <author> Madhusudhan Talluri and Mark D. Hill. </author> <title> Surpassing the TLB performance of superpages with less operating system support. </title> <booktitle> In Sixth Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Increasing the number of TLB entries is an obvious solution, as illustrated by Chen [CBJ92]. However, large fully associative structures are difficult to build. Reducing the associativityto increase the number of entries is a valid technique <ref> [UNS + 94, TH94] </ref>. However, it is not clear whether the number of entries could be increased sufficiently to cover a significant proportion of larger memory sizes. <p> Much larger page sizes appear unusable for general use. The use of multiple page sizes to combat both TLB coverage limitations and internal fragmentation seems to be the best approach. Investigations have shown <ref> [TKHP92, CBJ92, TH94] </ref> a dramatic decrease in TLB miss overheads with only a moderate increase in working set size. <p> Kagi-masa et al. [KTM91] describe a system using multiple page sizes in a partitioned address space, though their aim is to reduce storage costs, not TLB overheads. Talluri <ref> [TH94] </ref> proposes two sub-block TLB designs which provide some advantages of multiple pages sizes while requiring little or no operating system modifications. In a sub-block TLB, each TLB entry maps a large superpage to a number of smaller frames, not all of which need to be resident.
Reference: [TKHP92] <author> Madhusudhan Talluri, Shing Kong, Mark D. Hill, and David A. Pat-terson. </author> <title> Tradeoffs in supporting two page sizes. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, to cover 10% of 512 MB would require 6,554 entries for 8 KB pages. 11 Larger page sizes appear to have the most potential to increase TLB coverage. Unfortunately, larger page sizes also have the potential to dramatically increase working set sizes due to internal fragmentation. Talluri <ref> [TKHP92] </ref> illustrated that a moderate increase in page size from 4 KB to 32 KB resulted in an average increase of 60% in working set size. <p> Much larger page sizes appear unusable for general use. The use of multiple page sizes to combat both TLB coverage limitations and internal fragmentation seems to be the best approach. Investigations have shown <ref> [TKHP92, CBJ92, TH94] </ref> a dramatic decrease in TLB miss overheads with only a moderate increase in working set size.
Reference: [UNS + 94] <author> Richard Uhlig, David Nagle, Tim Stanley, Trevor Mudge, Stuart Sechrest, and Richard Brown. </author> <title> Design tradeoffs for software-managed TLBs. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> August </month> <year> 1994. </year>
Reference-contexts: The likely improvements 10 in performance that can be achieved by replacement policy are minor <ref> [UNS + 94, CBJ92] </ref>. Hardware designers typically choose random replacement, as the cost involved in implementing other policies in hardware outweigh the performance benefits to be gained. Performance improvements through software involvement in replacement policy are doubtful. The costs of software misses are a tradeoff between speed and memory consumption. <p> The TLB is effectively divided into a region for operating system use and the rest is used for user-level mapping. The reserved entries typically map user page tables and kernel data. Changes in memory usage described in Section 3.3, and also identified by <ref> [NUS + 93, UNS + 94] </ref>, have increased the amount of kernel data and active page tables. The optimal number of reserved entries is a function of application and user-level server activity. Dynamically changing the number of reserved entries is proposed by Uhlig et al. [UNS + 94]. <p> The optimal number of reserved entries is a function of application and user-level server activity. Dynamically changing the number of reserved entries is proposed by Uhlig et al. <ref> [UNS + 94] </ref>. They use a simple scheme to tune the partition boundary between reserved and user-mapping entries depending on the activity of the machine. Their results indicate that dynamic partitioning performs better than static partitioning even when the partition is statically tuned to the application. <p> Software improvements to TLB performance appear to be limited to data structure optimisations for refill and TLB placement policy optimisations. Careful consideration of TLB interaction with operating system design and activity <ref> [NUS + 93, UNS + 94] </ref> has shown that performance gains are possible in this area. 4.2.2 Hardware Currently, much research effort has been directed towards increasing TLB coverage, the focus being on increasing the number of TLB entries, increasing the page size, or using multiple page sizes. <p> Increasing the number of TLB entries is an obvious solution, as illustrated by Chen [CBJ92]. However, large fully associative structures are difficult to build. Reducing the associativityto increase the number of entries is a valid technique <ref> [UNS + 94, TH94] </ref>. However, it is not clear whether the number of entries could be increased sufficiently to cover a significant proportion of larger memory sizes.
Reference: [VRH93] <author> J. Vochteloo, S. Russell, and G. Heiser. </author> <title> Capability based protection in the Mungi operating system. </title> <booktitle> In 3rd Int'l Workshop on Object-Orientation in Operating Systems. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: This leads to a drastic increase of the number of objects that must be managed, and further fragments the page table. The protection models in these systems are generally based on lists of capability that are maintained for each process <ref> [VRH93] </ref>. The capabilities are used at run-time to establish mappings with the appropriate protection bits. As activity switches between processes it is necessary to quickly change the protection bits without affecting the translation.
Reference: [Wil91] <author> P. R. Wilson. </author> <title> Pointer swizzling at page fault time: Efficiently supporting huge address spaces on standard hardware. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 19(4), </volume> <month> June </month> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Some persistent systems do not permanently allocate addresses to objects but simply map them into a process' address space at a suitable free region and then swizzle <ref> [Wil91] </ref> the pointers to compensate for the changed location. These systems have similar impact on address-space management as the memory-mapped files described above. Other systems, like Monads [RA85], allocate a permanent address for each object. It is common in these systems to not re-use addresses to ensure uniqueness over time.
References-found: 29

