URL: http://www.cs.umd.edu/~keleher/papers/iwoos.sparks.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/papers.html
Root-URL: 
Email: keleher@cs.umd.edu  
Title: Sparks: Coherence as an Abstract Type  
Author: Peter J. Keleher 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Sparks is a protocol construction framework that treats records of coherence actions as abstract types. Sparks' central abstraction is the coherence history, an object that summarizes past coherence actions to shared segments. Histories provide high-level access to coherence guarantees. We motivate our work by discussing synchronization design in distributed shared memory systems, and show how histories can be used to cleanly create more efficient synchronization than is currently used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Since coherence in LRC systems like CVM is driven by synchronization, it is also entirely proper to view Sparks as a toolkit with which to write DSM protocols. 3 Synchronization Support DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 2, 6, 4] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them. <p> While a segment in a page-based DSM consists of a set of pages, segments could also be composed of arbitrarily-shaped objects in distributed object systems such as Midway <ref> [1] </ref>, or CRL [4]. class History f TemporalExtent temporal; SegmentExtent segment; ThreadExtent thread; void register (int on or off); void operator += (History *); void operator -= (History *); void apply (); UpdateData *get data (); g; The thread extent names the set of threads whose write notices may be contained
Reference: [2] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Since coherence in LRC systems like CVM is driven by synchronization, it is also entirely proper to view Sparks as a toolkit with which to write DSM protocols. 3 Synchronization Support DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 2, 6, 4] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them.
Reference: [3] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: While early systems strictly emulated the sequentially consistent [8] programming model of tightly-coupled multiprocessors, most recent systems support relaxed consistency models such as lazy release consistency (LRC) [5], a close relative to the eager release consistency (ERC) <ref> [3] </ref> memory model. DSMs that implement ERC delay propagating modifications of shared data until they execute a release, and then the modifications are performed globally. Under LRC protocols, processors further delay performing modifications remotely until subsequent acquires by other processors.
Reference: [4] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> To appear in The Proceedings of the 15th ACM Symposium on Operating Systems Principles. </booktitle>
Reference-contexts: However, most system typically provide support for only a very limited set of synchronization types, such as barrier and locks. Some systems additionally provide support for reduction types <ref> [4] </ref>, but, in general, application programmers are expected to implement high-level synchronization types on top of low-level synchronization types. <p> Since coherence in LRC systems like CVM is driven by synchronization, it is also entirely proper to view Sparks as a toolkit with which to write DSM protocols. 3 Synchronization Support DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 2, 6, 4] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them. <p> While a segment in a page-based DSM consists of a set of pages, segments could also be composed of arbitrarily-shaped objects in distributed object systems such as Midway [1], or CRL <ref> [4] </ref>. class History f TemporalExtent temporal; SegmentExtent segment; ThreadExtent thread; void register (int on or off); void operator += (History *); void operator -= (History *); void apply (); UpdateData *get data (); g; The thread extent names the set of threads whose write notices may be contained in the history.
Reference: [5] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While early systems strictly emulated the sequentially consistent [8] programming model of tightly-coupled multiprocessors, most recent systems support relaxed consistency models such as lazy release consistency (LRC) <ref> [5] </ref>, a close relative to the eager release consistency (ERC) [3] memory model. DSMs that implement ERC delay propagating modifications of shared data until they execute a release, and then the modifications are performed globally. Under LRC protocols, processors further delay performing modifications remotely until subsequent acquires by other processors. <p> By applying one node's current history at another node, the second node's view of shared state is brought up to date with respect to events seen by the first. More formally, a history is a partially ordered set of intervals <ref> [5] </ref>, where an interval describes a portion of the execution of a single processor. Intervals contain write notices, which are generally just indications that a given page has been modified. Applying such a notice usually invalidates the associated page.
Reference: [6] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Since coherence in LRC systems like CVM is driven by synchronization, it is also entirely proper to view Sparks as a toolkit with which to write DSM protocols. 3 Synchronization Support DSMs typically separate synchronization support from shared address space support in order to achieve good performance <ref> [1, 2, 6, 4] </ref>. Such systems provide a limited set of synchronization primitives (locks, barriers), and expect application programmers to build sophisticated synchronization constructs in terms of them.
Reference: [7] <author> Povl T. Koch, Robert J. Fowler, and Eric Jul. </author> <title> Message-driven relaxed consistency in a software distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 75-86, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The Sparks class library can be used to build high level synchronization objects that accurately reflect the synchronization objects' coherence semantics. Our approach is related to the causality annotations of CarlOS <ref> [7] </ref>, but Sparks will provide a much richer set of mechanisms and finer control over the scope of consistency actions. Sparks will replace the top layer of CVM.
Reference: [8] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Septem-ber </month> <year> 1979. </year>
Reference-contexts: While early systems strictly emulated the sequentially consistent <ref> [8] </ref> programming model of tightly-coupled multiprocessors, most recent systems support relaxed consistency models such as lazy release consistency (LRC) [5], a close relative to the eager release consistency (ERC) [3] memory model.
Reference: [9] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the 1995 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Hence, such replay mechanisms will be a a form of prefetch, and will help hide the latency of the shared accesses. This technique has been used before <ref> [9] </ref>, but only as an ad hoc technique written specifically for a single application. By using histories, the process is semi-automated. Recordings are made essentially by taking snapshots of a process's read history before and after the region to be recorded. <p> During the next iteration, barrierX will use H local to disseminate the read miss information on barrier releases, allowing other processors to stream in data they produce before the misses occur again. Recording and playing back data transfers was first used by the Mukherjee <ref> [9] </ref> in the context of a sequentially consistent DSM. Our work differs in two ways. First, our recording mechanisms will be part of the synchronization type definitions. The playbacks will be initiated by automatic heuristics, making them more reliable and easier to apply.
References-found: 9

