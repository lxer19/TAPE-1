URL: http://www.cs.indiana.edu/hyplan/shockema/icassp.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/shockema/resume.html
Root-URL: http://www.cs.indiana.edu
Email: fharper,mjohnson,lhj,hockema,robotg@ecn.purdue.edu  
Title: INTERFACING A CDG PARSER WITH AN HMM WORD RECOGNIZER USING WORD GRAPHS  
Author: M. P. Harper, M. T. Johnson, L. H. Jamieson, S. A. Hockema, and C. M. White 
Address: West Lafayette, IN 47907  
Affiliation: Purdue University, School of Electrical and Computer Engineering  
Abstract: In this paper, we describe a prototype spoken language system that loosely integrates a speech recognition component based on hidden Markov models with a constraint dependency grammar (CDG) parser using a word graph to pass sentence candidates between the two modules. This loosely coupled system was able to improve the sentence selection accuracy and concept accuracy over the level achieved by the acoustic module with a stochastic grammar. Timing profiles suggest that a tighter coupling of the modules could reduce parsing times of the system, as could the development of better acoustic models and tighter parsing constraints for conjunctions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Boros, W. Eckert, F. Gallwitz, G. Goerz, G. Han-rieder, and H. Niemann. </author> <title> Towards understanding spontaneous speech: Word accuracy vs. concept accuracy. </title> <booktitle> In Proc. of ICSLP, </booktitle> <volume> volume 2, </volume> <pages> pages 1009-1012, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, in the ten cases that the recognizer plus stochastic grammar failed to return the target sentence, four of the returned sentences differed only slightly from the meaning of the target sentence (substitution or deletion of a determiner), resulting in a concept accuracy <ref> [1] </ref> of 94%. The word graphs constructed by this recognizer placed the target sentence as the highest scoring path in 90 of the word graphs, second highest in nine, and third highest in one.
Reference: [2] <author> M. P. Harper and R. A. Helzerman. </author> <title> Managing multiple knowledge sources in constraint-based parsing spoken language. </title> <note> Fundamenta Informaticae, 23(2,3,4):303-353, </note> <year> 1995. </year>
Reference-contexts: These constraints can be used to express legal syntactic, prosodic, semantic relations, as well as context-dependent relations. Constraints can be ordered for efficiency or they can be withheld. The presence of ambiguity can trigger the propagation of additional constraints to further refine the parse for a sentence <ref> [2] </ref>. This flexibility can be utilized to create a smart language processing system: one that decides when and how to use its constraints based on the state of the parse. Fourth, a CDG parser is highly parallelizable [4].
Reference: [3] <author> Mary P. Harper and Randall A. Helzerman. </author> <title> Extensions to constraint dependency parsing for spoken language processing. </title> <booktitle> Computer Speech and Language, </booktitle> <pages> pages 187-234, </pages> <year> 1995. </year>
Reference-contexts: How best to achieve this goal is an important problem that deserves careful study. The question of how to integrate the language models with speech recognition systems is gaining in importance as speech recognizers are increasingly used in human/computer interfaces and dialog systems <ref> [3, 10] </ref>. <p> A language processing module that is more powerful than a regular grammar can be loosely, moderately, or tightly integrated with the spoken language system, and there are advantages and disadvantages associated with each choice <ref> [3] </ref>. A loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advantage. However, such a module cannot directly reduce the search space of the acoustic module. <p> CDG has been adapted to support the simultaneous analysis of sentences with multiple alternative lexical categories (e.g., can is a noun, verb, or modal) and multiple feature values (e.g., the as a determiner can modify nouns that are third person singular or third person plural) <ref> [3] </ref>. The resulting parser can also simultaneously process the sentence hypotheses resulting from the word segmentation ambiguity of a speech recognizer [3]. The CDG parsing algorithm offers a flexible and powerful parsing framework for our spoken language system. <p> (e.g., can is a noun, verb, or modal) and multiple feature values (e.g., the as a determiner can modify nouns that are third person singular or third person plural) <ref> [3] </ref>. The resulting parser can also simultaneously process the sentence hypotheses resulting from the word segmentation ambiguity of a speech recognizer [3]. The CDG parsing algorithm offers a flexible and powerful parsing framework for our spoken language system. In addition to the fact that it can simultaneously analyze all sentences in a word graph [3], there are a variety of other features that make it attractive. <p> can also simultaneously process the sentence hypotheses resulting from the word segmentation ambiguity of a speech recognizer <ref> [3] </ref>. The CDG parsing algorithm offers a flexible and powerful parsing framework for our spoken language system. In addition to the fact that it can simultaneously analyze all sentences in a word graph [3], there are a variety of other features that make it attractive. First, the generative capacity of a CDG is beyond context-free languages [7]. <p> Second, free-order languages can be handled by a CDG parser without enumerating all permutations because order among constituents is not a requirement of the grammatical formalism <ref> [3] </ref>. Third, the CDG parser uses sets of constraints which operate on role values assigned to roles to determine whether or not a string of terminals is in the grammar. These constraints can be used to express legal syntactic, prosodic, semantic relations, as well as context-dependent relations.
Reference: [4] <author> R. A. Helzerman and M. P. Harper. </author> <title> Log time parsing on the MasPar MP-1. </title> <booktitle> In Proc. of the International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 209-217, </pages> <year> 1992. </year>
Reference-contexts: This flexibility can be utilized to create a smart language processing system: one that decides when and how to use its constraints based on the state of the parse. Fourth, a CDG parser is highly parallelizable <ref> [4] </ref>. Finally, we have developed methods for learning CDGs directly from labelled sentences in a corpus [11]. This work was enabled by the fact that CDG constraints are PAC learnable from positive examples of dependency relations.
Reference: [5] <author> M. T. Johnson, M. P. Harper, and L. H. Jamieson. </author> <title> Effectiveness of word graphs for interfacing speech recognition and language models. </title> <booktitle> In Proc. of ICSLP, </booktitle> <year> 1998. </year>
Reference-contexts: The first algorithm was chosen because it creates only slightly larger word graphs containing only legal lattice paths. We have explored the effects of different types of pruning and stochastic grammars on word graphs and N-best lists produced by our acoustic module trained on the RM corpus <ref> [5] </ref>. It was determined that by allowing an unlimited number of active models in our HMM that the word graph contains the correct sentence in 98% of the test cases.
Reference: [6] <institution> Entropic Cambridge Research Laboratory Ltd. </institution> <note> HTK version 2.1, </note> <year> 1997. </year>
Reference-contexts: The Word Recognizer The underlying structure for the recognition network is provided by a multiple-mixture triphone Hidden Markov Model (HMM) [9], with a simple integrated grammar (either bi-gram or word pair). Our system was implemented using HTK Version 2.1 by Entropic <ref> [6, 12] </ref>. Observation output distributions under this system are represented by multiple stream Gaussian Mixture Models (GMMs), with distribution parameters and state transition probabilities re-estimated using the Baum-Welch algorithm.
Reference: [7] <author> H. Maruyama. </author> <title> Structural disambiguation with constraint propagation. </title> <booktitle> In Proc. of the Assoc. for Comp. Ling., </booktitle> <pages> pages 31-38, </pages> <year> 1990. </year>
Reference-contexts: The Parser Our language module is based on Constraint Dependency Grammar, which was originally conceived by Maruyama <ref> [7] </ref>. In contrast to context-free grammars, CDG uses constraints rather than production rules for its grammar rules, and the sentence structure is recorded by assigning role values consisting of dependency links (called modifiees) and tags (called labels) to named slots (called roles) associated with each word. <p> In addition to the fact that it can simultaneously analyze all sentences in a word graph [3], there are a variety of other features that make it attractive. First, the generative capacity of a CDG is beyond context-free languages <ref> [7] </ref>. There is evidence for the need to develop parsers for grammars that are more expressive than the class of context-free grammars but less expressive than context-sensitive grammars.
Reference: [8] <author> P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. </author> <title> A database for continuous speech recognition in a 1000-word domain. </title> <booktitle> In Proc. of ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 651-654, </pages> <year> 1988. </year>
Reference-contexts: EXPERIMENTAL EVALUATION 3.1. The Corpus and the CDG Grammar In order to evaluate the use of word graphs as an effective interface between our speech recognition and natural language components, we have performed initial experiments on the DARPA Resource Management (RM) corpus <ref> [8] </ref>, a 1000-word task domain defined for speech recognition evaluation.
Reference: [9] <author> Lawrence R. Rabiner. </author> <title> Tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: THE MODULES In this section we describe the components of our system. 2.1. The Word Recognizer The underlying structure for the recognition network is provided by a multiple-mixture triphone Hidden Markov Model (HMM) <ref> [9] </ref>, with a simple integrated grammar (either bi-gram or word pair). Our system was implemented using HTK Version 2.1 by Entropic [6, 12]. Observation output distributions under this system are represented by multiple stream Gaussian Mixture Models (GMMs), with distribution parameters and state transition probabilities re-estimated using the Baum-Welch algorithm.
Reference: [10] <author> Ludwig A. Schmid. </author> <title> Parsing word graphs using a linguistic grammar and a statistical language model. </title> <booktitle> Proc. of ICASSP, </booktitle> <volume> 2 </volume> <pages> 41-44, </pages> <year> 1994. </year>
Reference-contexts: How best to achieve this goal is an important problem that deserves careful study. The question of how to integrate the language models with speech recognition systems is gaining in importance as speech recognizers are increasingly used in human/computer interfaces and dialog systems <ref> [3, 10] </ref>.
Reference: [11] <author> C. M. White, M. P. Harper, T. Lane, and R. A. Helzer-man. </author> <title> Inductive learning of abstract role values derived from a constraint dependency grammar. In Proc. of the Automata Induction, Grammatical Inference, </title> <booktitle> and Language Acquisition Workshop, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: Fourth, a CDG parser is highly parallelizable [4]. Finally, we have developed methods for learning CDGs directly from labelled sentences in a corpus <ref> [11] </ref>. This work was enabled by the fact that CDG constraints are PAC learnable from positive examples of dependency relations. This work provides a foundation for speeding the grammar development cycle and for creating constraints from corpora that will be tightly constraining. 3. EXPERIMENTAL EVALUATION 3.1.
Reference: [12] <author> Steve Young, Julian Odell, Dave Ollason, Valtcho Valtchev, and Phil Woodland. </author> <title> The HTK Book. </title> <institution> Entropic Cambridge Research Laboratory Ltd., </institution> <address> 2.1 edition, </address> <year> 1997. </year>
Reference-contexts: The Word Recognizer The underlying structure for the recognition network is provided by a multiple-mixture triphone Hidden Markov Model (HMM) [9], with a simple integrated grammar (either bi-gram or word pair). Our system was implemented using HTK Version 2.1 by Entropic <ref> [6, 12] </ref>. Observation output distributions under this system are represented by multiple stream Gaussian Mixture Models (GMMs), with distribution parameters and state transition probabilities re-estimated using the Baum-Welch algorithm.
References-found: 12

