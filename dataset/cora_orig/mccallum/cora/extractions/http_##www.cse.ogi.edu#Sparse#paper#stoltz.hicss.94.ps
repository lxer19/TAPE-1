URL: http://www.cse.ogi.edu/Sparse/paper/stoltz.hicss.94.ps
Refering-URL: http://www.cse.ogi.edu/~stoltz/
Root-URL: http://www.cse.ogi.edu
Email: fstoltz,gerlek,mwolfeg@cse.ogi.edu  
Title: Extended SSA with Factored Use-Def Chains to Support Optimization and Parallelism  
Author: Eric Stoltz Michael P. Gerlek Michael Wolfe 
Address: Portland, Oregon  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract: This paper describes our implementation of the Static Single Assignment (SSA) form of intermediate program representation in our parallelizing Fortran 90 compiler, Nascent. Although the traditional SSA form algorithm renames variables uniquely at every definition point, it is not practical to add new names to the symbol table at all assignments. Thus, most implementations actually provide def-use chains for each definition. In contrast, we provide use-def chains, so that in the intermediate representation the link at each use points to its unique reaching definition. We discuss how our approach improves the implementation and efficiency of optimization and analysis techniques such as induction variable recognition and scalar dependence identification, used in the detection of parallelism. We also support parallelism by extending the traditional SSA form into languages with parallel constructs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: For technical reasons, a flow graph edge connects Entry to Exit [5]. Each basic block contains a list of intermediate code tuples, described in the next section. We restrict our attention to reducible flow graphs <ref> [1] </ref>. SSA form. After a program has been converted into this form, it has two distinguishing properties: 1. Every use of a variable in the program has exactly one reaching definition, and 2. At confluence points in the CFG, merge functions called -functions are introduced. <p> Although this traditional SSA form renames variables uniquely at every definition point, it is not really practical (and certainly not desirable) to add new names to the symbol table for all assignments. Thus, the common implementation [5, 14, 25] actually provides def-use links <ref> [1] </ref> for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. <p> This routine is called recursively on the dominator tree children, popping definitions off the stack when returning. A traditional use-def chain would list all definitions of a variable which reach the current use <ref> [1] </ref>. <p> Traditional iterative solvers for optimizations such as constant propagation require many passes, recomputing information until a fixed point is reached <ref> [1, 10, 15] </ref>. In our approach we classify each node in the SSA data-flow graph in terms of its graph descendants using a depth-first search.
Reference: [2] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 617-640, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: An example of parallel section constructs, similar to ideas advocated by Hansen [13] and Allen et al. <ref> [2] </ref>, is given in Figure 7. We needed to design our intermediate representation to translate these programs into SSA form, preserving the unique use-def link when applicable, but dealing with multiple (perhaps anomalous) updates among parallel sections. <p> Other systems have been developed, such as the PTRAN system <ref> [2] </ref>, aimed at automatic restructuring of sequential code for parallel execution. However, the intermediate form is not described in much detail, so it is difficult to assess the methods they use.
Reference: [3] <author> R. A. Ballance, A. B. Maccabe, and K. J. Ot-tenstein. </author> <title> The Program Dependence Web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code <ref> [3, 7, 14, 21, 25] </ref>. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> If the graph successor nodes have not yet been evaluated, the algorithm is recursively called on those tuples corresponding to the targets of the links. This approach requires only one pass in the absence of cycles. We apply techniques suggested by Ballance et al. <ref> [3] </ref> in order to correctly propagate values down conditional branches. This entails associating a predicate with each -function so that, if possible, we can determine which branch will be taken to reach the current confluence point. <p> Many other intermediate representations for compiler analysis and optimizations exist, though most don't specifically address their application for enhancing parallelism. We briefly examine several of these. The Program Dependence Web (PDW) <ref> [3] </ref> attempts to provide a framework for integrating control dependence as well as SSA within a consistent model.
Reference: [4] <author> Ron Cytron and Jeanne Ferrante. </author> <title> What's in a name? -or- the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Cytron et al. [5] established the foundation for an intermediate form which significantly reduces the complexity of def-use links. Their method describes efficient techniques to compute the location of, and minimally construct, -function augmentation to data-flow graphs. Additional work by Cytron and Ferrante <ref> [4] </ref> to enhance parallelism using these constructs on imperative programs showed that a delicate balance needs to be maintained between managing storage and increasing parallelism. An algorithm was presented by Cytron et al. [5] which essentially applies a demand-driven analysis to the problem of dead code elimination.
Reference: [5] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, and Mark N Wegman. </author> <title> Efficiently computing Static Single Assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: For technical reasons, a flow graph edge connects Entry to Exit <ref> [5] </ref>. Each basic block contains a list of intermediate code tuples, described in the next section. We restrict our attention to reducible flow graphs [1]. SSA form. After a program has been converted into this form, it has two distinguishing properties: 1. <p> All -functions receive a unique name since they are considered definitions, while their arguments are renamed to the definition that reaches the corresponding control flow predecessor. Additional details are provided in the paper by Cytron et al. <ref> [5] </ref>. Although this traditional SSA form renames variables uniquely at every definition point, it is not really practical (and certainly not desirable) to add new names to the symbol table for all assignments. <p> Additional details are provided in the paper by Cytron et al. [5]. Although this traditional SSA form renames variables uniquely at every definition point, it is not really practical (and certainly not desirable) to add new names to the symbol table for all assignments. Thus, the common implementation <ref> [5, 14, 25] </ref> actually provides def-use links [1] for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. <p> Thus, to perform an operation associated with any tuple, a request (or demand) is made for the information at the target of these pointers. In converting intermediate code into SSA form, we generally follow the algorithm given by Cytron et al. <ref> [5] </ref>, which relies extensively upon the concepts of dominators and dominance frontiers for nodes in the CFG. Briefly, node X dominates node Y if all paths from Entry to Y must pass through X. X strictly dominates Y if X dominates Y and X 6= Y. <p> The advantage of this approach is a consistent framework with which to sensibly reason about such parallel programs. This framework is achieved in two ways. First, augmenting classical analysis techniques of the control flow graph (such as dominator computation, dominance frontiers, and join sets <ref> [5] </ref>) to an extended control flow graph captures the abstraction of control flow and parallelism. <p> We have implemented these explicit parallel constructs and analysis methods in Nascent, with code generation planned for both distributed and shared memory architectures. 5 Related Work Some related work also addresses program representation with which to perform intermediate optimization and analysis while supporting parallelism. Cytron et al. <ref> [5] </ref> established the foundation for an intermediate form which significantly reduces the complexity of def-use links. Their method describes efficient techniques to compute the location of, and minimally construct, -function augmentation to data-flow graphs. <p> Additional work by Cytron and Ferrante [4] to enhance parallelism using these constructs on imperative programs showed that a delicate balance needs to be maintained between managing storage and increasing parallelism. An algorithm was presented by Cytron et al. <ref> [5] </ref> which essentially applies a demand-driven analysis to the problem of dead code elimination. This algorithm includes a set called Definers (S), the set of statements that provide values used by a statement S.
Reference: [6] <author> Ron Cytron, Jeanne Ferrante, and Vivek Sarkar. </author> <title> Compact representations for control dependence. </title> <booktitle> In ACM SIGPLAN Conference on Progamming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: We note that all variables are assumed to be initialized (hence defined) at Entry. 2. Dominator Computation We compute the immediate dominator tree of the CFG using the algorithm by Lengauer and Tarjan [18]. The dominance frontier of all nodes is then quickly assembled using a compact technique <ref> [6] </ref> which is just as fast as previous techniques and an improvement in terms of space. In addition, it is much simpler to code; our original 150 lines of code for this phase was reduced to just 10. 3.
Reference: [7] <author> Ron Cytron and Reid Gershbein. </author> <title> Efficient acco-modation of may-alias information in SSA form. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code <ref> [3, 7, 14, 21, 25] </ref>. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph.
Reference: [8] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of four Perfect-Benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 65-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> LNCS no. 589. </note>
Reference-contexts: array references requires array subscripts be characterized in terms of the iteration count of the enclosing loop (s); these are usually linear induction sequences, but our technique allows for the inclusion of more complex sequences as well, including generalized induction variables (shown to be important in parallelizing the Perfect benchmarks <ref> [8] </ref>) and monotonic variables (resulting in dependence information within the range of con ditionals [19]).
Reference: [9] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The Program Dependence Graph and its use in optimization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The SSA form is extended to include gating functions which provide predicate information for -functions (which are divided into two classes: loop-header and non-loop-header functions) as well as exit values for variables within loops. The PDW is considered an augmented Program Dependence Graph <ref> [9] </ref>, a graph which captures both control and data-dependence information. Another method employed to generalize SSA is the Dependence Flow Graph [14] which combines control information with def-use SSA chains by defining control regions which consist of single-entry, single-exit areas of the control flow graph.
Reference: [10] <author> Charles N. Fischer and Richard J. LeBlanc. </author> <title> Crafting a Compiler. </title> <publisher> Benjamin Cummings, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: Traditional iterative solvers for optimizations such as constant propagation require many passes, recomputing information until a fixed point is reached <ref> [1, 10, 15] </ref>. In our approach we classify each node in the SSA data-flow graph in terms of its graph descendants using a depth-first search.
Reference: [11] <author> Parallel Computing Forum. </author> <title> PCF Parallel Fortran Extensions. </title> <journal> Fortran Forum, </journal> <volume> 10(3), </volume> <month> September </month> <year> 1991. </year> <note> (special issue). </note>
Reference-contexts: Anti-dependence, on the other hand, is a more complex task and the subject of future work. 4.3 Explicit Parallel Sections We have designed Nascent to accept explicit parallel section constructs consistent with the Parallel Computing Forum extensions to Fortran <ref> [11] </ref>. An example of parallel section constructs, similar to ideas advocated by Hansen [13] and Allen et al. [2], is given in Figure 7.
Reference: [12] <author> Michael P. Gerlek, Eric Stoltz, and Michael Wolfe. </author> <title> Beyond induction variables: Detecting and classifying sequences using a demand-driven SSA form. </title> <note> submitted for publication, </note> <month> September </month> <year> 1993. </year>
Reference-contexts: In 3 (b), employing use-def links, we recursively visit all descendants using Tar-jan's algorithm, thus insuring that the cycle can be correctly classified when first detected. More details on this work can be found in a related paper <ref> [12] </ref>. Accurate characterization of various induction forms provides information necessary for many program transformations useful in automatic paralleliza-tion.
Reference: [13] <author> Per Brinch Hansen. </author> <title> Operating System Principles. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: An example of parallel section constructs, similar to ideas advocated by Hansen <ref> [13] </ref> and Allen et al. [2], is given in Figure 7. We needed to design our intermediate representation to translate these programs into SSA form, preserving the unique use-def link when applicable, but dealing with multiple (perhaps anomalous) updates among parallel sections.
Reference: [14] <author> Richard Johnson and Keshav Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In ACM SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code <ref> [3, 7, 14, 21, 25] </ref>. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> Additional details are provided in the paper by Cytron et al. [5]. Although this traditional SSA form renames variables uniquely at every definition point, it is not really practical (and certainly not desirable) to add new names to the symbol table for all assignments. Thus, the common implementation <ref> [5, 14, 25] </ref> actually provides def-use links [1] for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. <p> The PDW is considered an augmented Program Dependence Graph [9], a graph which captures both control and data-dependence information. Another method employed to generalize SSA is the Dependence Flow Graph <ref> [14] </ref> which combines control information with def-use SSA chains by defining control regions which consist of single-entry, single-exit areas of the control flow graph.
Reference: [15] <author> Ken Kennedy. </author> <title> Global data flow analysis. </title> <editor> In Steven S. Muchnick and Neil D. Jones, editors, </editor> <title> Program Flow Analysis: Theory and Practice, page 18. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Traditional iterative solvers for optimizations such as constant propagation require many passes, recomputing information until a fixed point is reached <ref> [1, 10, 15] </ref>. In our approach we classify each node in the SSA data-flow graph in terms of its graph descendants using a depth-first search.
Reference: [16] <author> G. A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the First ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: In our approach we classify each node in the SSA data-flow graph in terms of its graph descendants using a depth-first search. Constant propagation is performed in Nascent using a standard constant propagation lattice <ref> [16] </ref>, with each node being assigned a lattice value as a function of the lattice value of its SSA data-flow graph successors. If the graph successor nodes have not yet been evaluated, the algorithm is recursively called on those tuples corresponding to the targets of the links.
Reference: [17] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Second, a well defined consistency for values flowing into and out of parallel sections is maintained by using a copy-in/copy-out semantics [23], as opposed to a sequentially consistent model of execution <ref> [17] </ref> which al lows possible non-determinism. The copy-in/copy-out model allows the compiler to perform optimizations such as forward substitution and constant propagation. More detail on this project can be found in a preliminary paper [22].
Reference: [18] <author> Thomas Lengauer and Robert Endre Tarjan. </author> <title> A fast algorithm for finding dominators in a flow-graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> July </month> <year> 1979. </year>
Reference-contexts: We note that all variables are assumed to be initialized (hence defined) at Entry. 2. Dominator Computation We compute the immediate dominator tree of the CFG using the algorithm by Lengauer and Tarjan <ref> [18] </ref>. The dominance frontier of all nodes is then quickly assembled using a compact technique [6] which is just as fast as previous techniques and an improvement in terms of space.
Reference: [19] <author> David A. Padua and Michael Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <month> December </month> <year> 1986. </year>
Reference-contexts: the enclosing loop (s); these are usually linear induction sequences, but our technique allows for the inclusion of more complex sequences as well, including generalized induction variables (shown to be important in parallelizing the Perfect benchmarks [8]) and monotonic variables (resulting in dependence information within the range of con ditionals <ref> [19] </ref>). As another example, the recognition of wrap-around variables can enable a compiler to "peel off" the first iteration of a loop so the remaining iterations may be parallelized. 4.2 Data Dependence In order to execute code in parallel, most notably loops, dependences must be preserved [27]. <p> Given two statements S i and S j , if a dependence exists between these two statements we denote that relationship: S i ffi S j . Specific dependences annotate this notation. In general, one must preserve the following data dependences <ref> [19] </ref>: * Flow-dependence: read after write, S i ffi S j * Anti-dependence: write after read, S i ffi S j * Output-dependence: write after write, S i ffi o S j When loops are considered, we are often interested in distance and/or direction of the dependence. <p> For example, if a flow dependence exists between S i and S j with a distance of d, it is denoted S i ffi (d) S j , while if its direction is (&lt;), it is denoted S i ffi (&lt;) S j <ref> [19, 27] </ref>. To understand how variable values are carried around loops and merged at confluence points, we note that -functions come in two "flavors": merge nodes as the result of conditional branches, and those at the top or header of a loop.
Reference: [20] <author> Barry K. Rosen, Mark N. Wegman, and Ken-neth F. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Fifteenth Annual ACM SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: Thus, the common implementation [5, 14, 25] actually provides def-use links [1] for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. The def-use chain style of SSA implementation lends itself well to forward data-flow problems <ref> [20, 25] </ref> due to consistency of direction between program flow and def-use links. However, a demand-driven data-flow problem will typically request information at a program point from its data-flow predecessors. As we shall see, use-def chains admirably match a demand-driven style of data-flow analysis.
Reference: [21] <author> Barry K. Rosen, Mark N. Wegman, and Ken-neth F. Zadeck. </author> <title> Global value numbers and redundant computation. </title> <booktitle> In Fifteenth Annual ACM SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code <ref> [3, 7, 14, 21, 25] </ref>. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering <ref> [21] </ref>, and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph.
Reference: [22] <author> Harini Srinivasan, James Hook, and Michael Wolfe. </author> <title> Static Single Assignment for explicitly parallel programs. </title> <booktitle> In Twentieth Annual ACM SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: The copy-in/copy-out model allows the compiler to perform optimizations such as forward substitution and constant propagation. More detail on this project can be found in a preliminary paper <ref> [22] </ref>. We have implemented these explicit parallel constructs and analysis methods in Nascent, with code generation planned for both distributed and shared memory architectures. 5 Related Work Some related work also addresses program representation with which to perform intermediate optimization and analysis while supporting parallelism.
Reference: [23] <author> Harini Srinivasan and Michael Wolfe. </author> <title> Analyzing programs with explicit parallelism. </title> <editor> In Ut-pal Banerjee, David Gelernter, Alexandru Nico-lau, and David A. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, number 589 in Lecture Notes in Computer Science, </booktitle> <pages> pages 403-419. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Second, a well defined consistency for values flowing into and out of parallel sections is maintained by using a copy-in/copy-out semantics <ref> [23] </ref>, as opposed to a sequentially consistent model of execution [17] which al lows possible non-determinism. The copy-in/copy-out model allows the compiler to perform optimizations such as forward substitution and constant propagation. More detail on this project can be found in a preliminary paper [22].
Reference: [24] <author> Robert Tarjan. </author> <title> Depth-first search and linear graph algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 1(2) </volume> <pages> 146-160, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: Based on the observation that statements describing induction variables correspond to strongly connected components (SCCs) in the data-flow graph, we use Tar-jan's depth-first algorithm <ref> [24] </ref> to detect these regions. Nodes in trivial SCCs have values determined as functions of their SSA data-flow descendants, as in constant propagation; however for induction variable classification we assign not just constant values but symbolic expressions.
Reference: [25] <author> Mark N. Wegman and Kenneth F. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code <ref> [3, 7, 14, 21, 25] </ref>. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> 1 Introduction The Static Single Assignment (SSA) form for intermediate program flow representation has become a popular and powerful framework with which to analyze and optimize code [3, 7, 14, 21, 25]. SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation <ref> [25] </ref>, global value numbering [21], and induction variable detection [26], among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> Additional details are provided in the paper by Cytron et al. [5]. Although this traditional SSA form renames variables uniquely at every definition point, it is not really practical (and certainly not desirable) to add new names to the symbol table for all assignments. Thus, the common implementation <ref> [5, 14, 25] </ref> actually provides def-use links [1] for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. <p> Thus, the common implementation [5, 14, 25] actually provides def-use links [1] for each new definition (see Figure 2). Since each use is the head of exactly one link, the semantics of SSA are preserved. The def-use chain style of SSA implementation lends itself well to forward data-flow problems <ref> [20, 25] </ref> due to consistency of direction between program flow and def-use links. However, a demand-driven data-flow problem will typically request information at a program point from its data-flow predecessors. As we shall see, use-def chains admirably match a demand-driven style of data-flow analysis.
Reference: [26] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: SSA form has been used to simplify optimization techniques and data-flow problems including constant propagation [25], global value numbering [21], and induction variable detection <ref> [26] </ref>, among others. Static Single Assignment form is based upon a program represented as a control flow graph. <p> We begin our analysis of programs within a framework consisting of the control-flow graph and an SSA data-flow graph. The SSA data-flow graph consists of tuples of the form &lt;op,left,right,ssalink&gt;, as described by Wolfe <ref> [26] </ref>, where op is the operation code and left and right are the two operands (both are not always required, e.g. a unary minus). The ssalink is used for fetches, including arguments of -functions, as well as indexed stores (which are not discussed further in this paper). <p> We can generalize from the detection of constant values to the detection of many forms of sequences, including linear and nonlinear induction variables, as well as variables that describe monotonic or periodic sequences <ref> [26] </ref>. Based on the observation that statements describing induction variables correspond to strongly connected components (SCCs) in the data-flow graph, we use Tar-jan's depth-first algorithm [24] to detect these regions.
Reference: [27] <author> Michael Wolfe and Utpal Banerjee. </author> <title> Data dependence and its applications to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <month> April </month> <year> 1987. </year>
Reference-contexts: As another example, the recognition of wrap-around variables can enable a compiler to "peel off" the first iteration of a loop so the remaining iterations may be parallelized. 4.2 Data Dependence In order to execute code in parallel, most notably loops, dependences must be preserved <ref> [27] </ref>. Induction variable detection, as previously explained, aids in the analysis of subscripts for array references. Given two statements S i and S j , if a dependence exists between these two statements we denote that relationship: S i ffi S j . Specific dependences annotate this notation. <p> For example, if a flow dependence exists between S i and S j with a distance of d, it is denoted S i ffi (d) S j , while if its direction is (&lt;), it is denoted S i ffi (&lt;) S j <ref> [19, 27] </ref>. To understand how variable values are carried around loops and merged at confluence points, we note that -functions come in two "flavors": merge nodes as the result of conditional branches, and those at the top or header of a loop.
Reference: [28] <author> Michael Wolfe, Michael P. Gerlek, and Eric Stoltz. Nascent: </author> <title> A next-generation, high performance compiler. </title> <institution> Oregon Graduate Institute of Science & Technology unpublished, </institution> <year> 1993. </year>
Reference-contexts: Most of these techniques are used in the detection of parallelism. We also describe an extension to SSA that allows correct analysis of explicit parallel sections. We have implemented SSA form in Nascent, our parallelizing Fortran 90 compiler <ref> [28] </ref>. Nascent accepts explicit parallel section constructs consistent with the Parallel Computing Forum extensions and will also conform to the High Performance Fortran (HPF) standard.
References-found: 28

