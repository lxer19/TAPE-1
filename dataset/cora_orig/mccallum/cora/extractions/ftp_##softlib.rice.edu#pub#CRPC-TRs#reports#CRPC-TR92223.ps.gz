URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92223.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Efficient Call Graph Analysis  
Author: Mary W. Hall Ken Kennedy 
Keyword: Categories and Subject Descriptors: D.3.3 [Programming Languages]: Language constructs and features procedures, functions and subroutines, D.3.4 [Programming Languages]: Processors compilers General Terms: Algorithms, Languages Additional Key Words and Phrases: interprocedural data-flow analysis  
Address: Stanford, CA 94305 Houston, TX 77251  
Affiliation: Center for Integrated Systems Department of Computer Science Stanford University Rice University  
Abstract: We present an efficient algorithm for computing the procedure call graph, the program representation underlying most interprocedural optimization techniques. The algorithm computes the possible bindings of procedure variables in languages where such variables only receive their values through parameter passing, such as Fortran. We extend the algorithm to accommodate a limited form of assignments to procedure variables. The resulting algorithm can also be used in analysis of functional programs that have been converted to Continuation Passing Style. We discuss the algorithm in relationship to other call graph analysis approaches. Many less efficient techniques produce essentially the same call graph. A few algorithms are more precise, but they may be prohibitively expensive depending on language features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Burke, M. </author> <title> An interval-based approach to exhaustive and incremental interprocedural analysis. </title> <type> Research Report RC 12702, </type> <institution> IBM Yorktown Heights, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: We extend the algorithm to provide a simple solution when assignment to procedure-valued variables is allowed. Our algorithm produces essentially the same call graph described by other less efficient algorithms <ref> [1, 18, 21, 23, 24] </ref>. More precise algorithms exist, but they may be prohibitively expensive depending on the language features [3, 17, 20]. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. <p> Weihl extends Walter's method to handle aliasing, assignment and pointer manipulation [24]. Spillman presents a bit matrix formulation to analyze PL/I that is at least O (N + E) for each cycle [21]. Burke presents an interval analysis formulation <ref> [1] </ref>. One cycle of interval analysis and updates requires O (N E p + dE) steps [9]. 11 Shivers describes his approach by defining simultaneous set equations [18].
Reference: [2] <author> Callahan, C.D., Cooper, K.D., Hood, R.T., Kennedy, K., and Torczon, L. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-89, </pages> <year> 1988. </year>
Reference-contexts: These steps are then bounded by the size of Worklist or O (c p EP ). 6 Implementation Results Our implementation of the algorithm provides the basis for interprocedural optimization in ParaS-cope, an environment designed for supporting scientific Fortran programmers <ref> [2] </ref>. The research in ParaScope has focused on aggressive compilation of scientific Fortran codes for a variety of architectures and has pioneered the incorporation of interprocedural optimization into an efficient compilation system. We discovered an important convenience that distinguishes the implementation from the presentation in Figure 1.
Reference: [3] <author> Callahan, D., Carle, A., Hall, M.W., and Kennedy, K. </author> <title> Constructing the procedure call multigraph. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-16(4):483-487, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Our algorithm produces essentially the same call graph described by other less efficient algorithms [1, 18, 21, 23, 24]. More precise algorithms exist, but they may be prohibitively expensive depending on the language features <ref> [3, 17, 20] </ref>. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. The paper is organized into eight remaining sections and a conclusion. <p> The algorithm computes a call graph that is precise, assuming all call sites are invoked. It cannot analyze languages permitting recursion. In previous work with Carle and Callahan, we converted Ryder's algorithm to analyze recursive programs using a worklist approach similar to the one presented here <ref> [3] </ref>. The time complexity of the precise algorithm is bounded by the number of simultaneous bindings of procedure formals across all procedures in the program.
Reference: [4] <author> Chambers, C. and Ungar, D. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 24(7) </volume> <pages> 146-160, </pages> <year> 1989. </year>
Reference-contexts: Inheritance and function overloading make it difficult to understand what procedures are being invoked even when the function name appears at the call. This problem is addressed by customization in self in cases where the compiler can statically determine the unique binding of a call site <ref> [4] </ref>. Support for the above language features will increase the cost of the algorithm. Acknowledgments.
Reference: [5] <author> Cooper, K.D. and Kennedy, K. </author> <title> Interprocedural side-effect analysis in linear time. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(7) </volume> <pages> 57-66, </pages> <year> 1988. </year>
Reference-contexts: We assume c p is bounded by a small constant, consistent with assumptions made in other work <ref> [5, 6] </ref>. 5.1 Correctness Lemma 1 The algorithm Build terminates after no more than c p EP iterations of the while loop. Proof. Each iteration of the while loop removes a pair from Worklist.
Reference: [6] <author> Cooper, K.D. and Kennedy, K. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Conference Record of the Sixteenth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1989. </year>
Reference-contexts: We assume c p is bounded by a small constant, consistent with assumptions made in other work <ref> [5, 6] </ref>. 5.1 Correctness Lemma 1 The algorithm Build terminates after no more than c p EP iterations of the while loop. Proof. Each iteration of the while loop removes a pair from Worklist.
Reference: [7] <author> Eigenmann, R. and Blume, W. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, pages II-17|II-24. </booktitle> <publisher> CRC Press, Inc., </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although most compilers optimize procedures as separate units, there is increasing evidence that optimization across procedure boundaries can yield significant improvements in program execution times. Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls <ref> [7, 11, 12, 14, 22] </ref> and compiling for distributed-memory multiprocessors [10]. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [8] <author> Fischer, M.J. </author> <title> Lambda calculus schemata. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 7(1) </volume> <pages> 104-109, </pages> <year> 1972. </year>
Reference-contexts: However, this algorithm is also applicable to languages with procedure-valued returns if the programs are first converted to continuation passing style (CPS). Continuation passing style (CPS) provides an intermediate program representation where all transfers of control are represented by tail recursive procedure calls <ref> [8, 16] </ref>. A procedure is passed a continuation as one of its parameters. A continuation is another procedure representing the position in the code in which to transfer control; rather than returning, a procedure simply invokes its continuation on exit.
Reference: [9] <author> Hall, M.W. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, Houston, TX, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The propagation at these calls corresponds to the rules set forth in Section 3. In each case, it can be shown that for any generated Worklist pair hb; f i, b 2 Boundto (f ) is true. (A detailed proof appears elsewhere <ref> [9] </ref>.) If F p is invoked, then a may have become a reachable procedure as a result of this call. In this case, InitializeN ode will add bindings from any static calls through a. <p> Spillman presents a bit matrix formulation to analyze PL/I that is at least O (N + E) for each cycle [21]. Burke presents an interval analysis formulation [1]. One cycle of interval analysis and updates requires O (N E p + dE) steps <ref> [9] </ref>. 11 Shivers describes his approach by defining simultaneous set equations [18]. He does not present an algorithm, but the technique was used in his implementation of data-flow analysis in Scheme [19]. 9.2 Precise Algorithms Ryder describes an algorithm to propagate simultaneous bindings of values to procedure parameters [17].
Reference: [10] <author> Hall, M.W., Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92. IEEE Computer Society, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls [7, 11, 12, 14, 22] and compiling for distributed-memory multiprocessors <ref> [10] </ref>. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [11] <author> Hall, M.W., Kennedy, K., and McKinley, </author> <title> K.S. Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 424-434. </pages> <publisher> IEEE Computer Society, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although most compilers optimize procedures as separate units, there is increasing evidence that optimization across procedure boundaries can yield significant improvements in program execution times. Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls <ref> [7, 11, 12, 14, 22] </ref> and compiling for distributed-memory multiprocessors [10]. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [12] <author> Havlak, P. and Kennedy, K. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although most compilers optimize procedures as separate units, there is increasing evidence that optimization across procedure boundaries can yield significant improvements in program execution times. Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls <ref> [7, 11, 12, 14, 22] </ref> and compiling for distributed-memory multiprocessors [10]. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [13] <author> Kildall, G. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1973. </year>
Reference-contexts: Much greater precision is possible if we maintain separate fl sets for each procedure variable [24]. Weihl's approach could be made even more precise by analyzing control flow within procedures, using something similar to constant propagation (but with [ as the meet function rather than ") <ref> [13] </ref>. However, separate fl sets mean that we must propagate an additional O (V ) global procedure variables on Worklist and track newly propagated bindings through the assignments in the procedure. Examining control flow within the procedure will prevent initialization from being performed in a single pass over each procedure.
Reference: [14] <author> Li, Z. and Yew, P. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(9) </volume> <pages> 85-99, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Although most compilers optimize procedures as separate units, there is increasing evidence that optimization across procedure boundaries can yield significant improvements in program execution times. Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls <ref> [7, 11, 12, 14, 22] </ref> and compiling for distributed-memory multiprocessors [10]. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [15] <author> Loeliger, J., Metzger, R., Seligman, M., and Stroud, S. </author> <title> Pointer target tracking: an empirical study. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 14-23. </pages> <publisher> IEEE Computer Society, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: In the case of Fortran, the two algorithms almost always yield the same call graph. We have not addressed a number of language features affecting call graph analysis. Pointer manipulation of procedure variables requires analysis similar to assignments. This issue has been addressed by other researchers <ref> [15, 24] </ref>. Aliasing may arise from features other than pointer manipulation. Aliasing due to call-by-reference parameter passing has been widely discussed in the literature. Spillman and Weihl consider its effect on call graph analysis [21, 24]. Compiling object-oriented languages contributes more difficult problems to constructing the call graph.
Reference: [16] <author> Reynolds, </author> <title> J.C. Definitional interpreters for higher-order programming languages. </title> <booktitle> In Proceedings of the ACM Annual Conference, </booktitle> <pages> pages 717-740, </pages> <year> 1972. </year>
Reference-contexts: However, this algorithm is also applicable to languages with procedure-valued returns if the programs are first converted to continuation passing style (CPS). Continuation passing style (CPS) provides an intermediate program representation where all transfers of control are represented by tail recursive procedure calls <ref> [8, 16] </ref>. A procedure is passed a continuation as one of its parameters. A continuation is another procedure representing the position in the code in which to transfer control; rather than returning, a procedure simply invokes its continuation on exit.
Reference: [17] <author> Ryder, B. </author> <title> Constructing the call graph of a program. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(3):216-225, </volume> <year> 1979. </year>
Reference-contexts: Our algorithm produces essentially the same call graph described by other less efficient algorithms [1, 18, 21, 23, 24]. More precise algorithms exist, but they may be prohibitively expensive depending on the language features <ref> [3, 17, 20] </ref>. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. The paper is organized into eight remaining sections and a conclusion. <p> He does not present an algorithm, but the technique was used in his implementation of data-flow analysis in Scheme [19]. 9.2 Precise Algorithms Ryder describes an algorithm to propagate simultaneous bindings of values to procedure parameters <ref> [17] </ref>. The algorithm computes a call graph that is precise, assuming all call sites are invoked. It cannot analyze languages permitting recursion. In previous work with Carle and Callahan, we converted Ryder's algorithm to analyze recursive programs using a worklist approach similar to the one presented here [3].
Reference: [18] <author> Shivers, O. </author> <title> Control flow analysis in Scheme. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(7) </volume> <pages> 164-174, </pages> <year> 1988. </year> <month> 16 </month>
Reference-contexts: We extend the algorithm to provide a simple solution when assignment to procedure-valued variables is allowed. Our algorithm produces essentially the same call graph described by other less efficient algorithms <ref> [1, 18, 21, 23, 24] </ref>. More precise algorithms exist, but they may be prohibitively expensive depending on the language features [3, 17, 20]. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. <p> Burke presents an interval analysis formulation [1]. One cycle of interval analysis and updates requires O (N E p + dE) steps [9]. 11 Shivers describes his approach by defining simultaneous set equations <ref> [18] </ref>. He does not present an algorithm, but the technique was used in his implementation of data-flow analysis in Scheme [19]. 9.2 Precise Algorithms Ryder describes an algorithm to propagate simultaneous bindings of values to procedure parameters [17].
Reference: [19] <author> Shivers, O. </author> <title> Control-Flow Analysis of higher-order languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently. This idea inspired research in applying traditional data-flow analysis to functional languages; in this context, data-flow analysis must be formulated as an interprocedural problem <ref> [19] </ref>. fl This research has been supported by the Center for Research on Parallel Computation, a National Science Foundation Science and Technology Center, by IBM Corporation and by the state of Texas. 1 Any technique performing analysis or optimization across procedure boundaries requires some underlying representation of the program structure. <p> One cycle of interval analysis and updates requires O (N E p + dE) steps [9]. 11 Shivers describes his approach by defining simultaneous set equations [18]. He does not present an algorithm, but the technique was used in his implementation of data-flow analysis in Scheme <ref> [19] </ref>. 9.2 Precise Algorithms Ryder describes an algorithm to propagate simultaneous bindings of values to procedure parameters [17]. The algorithm computes a call graph that is precise, assuming all call sites are invoked. It cannot analyze languages permitting recursion.
Reference: [20] <author> Shivers, O. </author> <title> The semantics of Scheme control flow analysis. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 26(9) </volume> <pages> 190-198, </pages> <year> 1991. </year>
Reference-contexts: Our algorithm produces essentially the same call graph described by other less efficient algorithms [1, 18, 21, 23, 24]. More precise algorithms exist, but they may be prohibitively expensive depending on the language features <ref> [3, 17, 20] </ref>. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. The paper is organized into eight remaining sections and a conclusion. <p> The algorithms will perform differently only for languages where procedure variables occur frequently, such as in Scheme. Shivers also presents precision improvements for his algorithm, but it still may be less precise than the modified Ryder algorithm <ref> [20] </ref>. He retains a bounded amount of path information and 3 Using these rules, the compiler could detect potential losses in precision. Imprecision is only possible in procedures with multiple procedure formals where at least two formals have Boundto sets containing two or more bindings.
Reference: [21] <author> Spillman, </author> <title> T.C. Exposing side-effects in a PL/I optimizing compiler. </title> <booktitle> In Proceedings of the IFIP Congress 1971, </booktitle> <pages> pages 376-381. </pages> <publisher> North Holland, </publisher> <year> 1971. </year>
Reference-contexts: We extend the algorithm to provide a simple solution when assignment to procedure-valued variables is allowed. Our algorithm produces essentially the same call graph described by other less efficient algorithms <ref> [1, 18, 21, 23, 24] </ref>. More precise algorithms exist, but they may be prohibitively expensive depending on the language features [3, 17, 20]. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. <p> Weihl extends Walter's method to handle aliasing, assignment and pointer manipulation [24]. Spillman presents a bit matrix formulation to analyze PL/I that is at least O (N + E) for each cycle <ref> [21] </ref>. Burke presents an interval analysis formulation [1]. One cycle of interval analysis and updates requires O (N E p + dE) steps [9]. 11 Shivers describes his approach by defining simultaneous set equations [18]. <p> This issue has been addressed by other researchers [15, 24]. Aliasing may arise from features other than pointer manipulation. Aliasing due to call-by-reference parameter passing has been widely discussed in the literature. Spillman and Weihl consider its effect on call graph analysis <ref> [21, 24] </ref>. Compiling object-oriented languages contributes more difficult problems to constructing the call graph. Inheritance and function overloading make it difficult to understand what procedures are being invoked even when the function name appears at the call.
Reference: [22] <author> Triolet, R., Irigoin, F., and Feautrier, P. </author> <title> Direct parallelization of call statements. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 21(7) </volume> <pages> 176-185, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction Although most compilers optimize procedures as separate units, there is increasing evidence that optimization across procedure boundaries can yield significant improvements in program execution times. Interprocedural analysis and optimization have proven to be important to automatic parallelization of loops containing procedure calls <ref> [7, 11, 12, 14, 22] </ref> and compiling for distributed-memory multiprocessors [10]. The above research focuses on analyzing languages used by scientific programmers, usually Fortran. However, interprocedural optimization is perhaps even more important for functional languages, where functions are small and calls occur quite frequently.
Reference: [23] <author> Walter, K. </author> <title> Recursion analysis for compiler optimization. </title> <journal> Communications of the ACM, </journal> <volume> 19(9) </volume> <pages> 514-516, </pages> <year> 1976. </year>
Reference-contexts: We extend the algorithm to provide a simple solution when assignment to procedure-valued variables is allowed. Our algorithm produces essentially the same call graph described by other less efficient algorithms <ref> [1, 18, 21, 23, 24] </ref>. More precise algorithms exist, but they may be prohibitively expensive depending on the language features [3, 17, 20]. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. <p> Walter describes the call graph using boolean relations on the procedures, performing transitive closure and composition of the relations on each cycle <ref> [23] </ref>. Weihl extends Walter's method to handle aliasing, assignment and pointer manipulation [24]. Spillman presents a bit matrix formulation to analyze PL/I that is at least O (N + E) for each cycle [21]. Burke presents an interval analysis formulation [1].
Reference: [24] <author> Weihl, </author> <title> W.E. Interprocedural data flow analysis in the presence of pointers, procedure variables, and label variables. </title> <booktitle> In Conference Record of the Seventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 83-94. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1980. </year> <month> 17 </month>
Reference-contexts: We extend the algorithm to provide a simple solution when assignment to procedure-valued variables is allowed. Our algorithm produces essentially the same call graph described by other less efficient algorithms <ref> [1, 18, 21, 23, 24] </ref>. More precise algorithms exist, but they may be prohibitively expensive depending on the language features [3, 17, 20]. While most of these approaches were designed for imperative languages, Shivers used a similar approach to analyze control flow of Scheme in Continuation Passing Style (CPS) form. <p> This involves propagating new values for fl through previously located calls and tracking values of procedure formals that appear on the right-hand side of assignment statements. The details of this change are fairly complex. Much greater precision is possible if we maintain separate fl sets for each procedure variable <ref> [24] </ref>. Weihl's approach could be made even more precise by analyzing control flow within procedures, using something similar to constant propagation (but with [ as the meet function rather than ") [13]. <p> Walter describes the call graph using boolean relations on the procedures, performing transitive closure and composition of the relations on each cycle [23]. Weihl extends Walter's method to handle aliasing, assignment and pointer manipulation <ref> [24] </ref>. Spillman presents a bit matrix formulation to analyze PL/I that is at least O (N + E) for each cycle [21]. Burke presents an interval analysis formulation [1]. <p> In the case of Fortran, the two algorithms almost always yield the same call graph. We have not addressed a number of language features affecting call graph analysis. Pointer manipulation of procedure variables requires analysis similar to assignments. This issue has been addressed by other researchers <ref> [15, 24] </ref>. Aliasing may arise from features other than pointer manipulation. Aliasing due to call-by-reference parameter passing has been widely discussed in the literature. Spillman and Weihl consider its effect on call graph analysis [21, 24]. Compiling object-oriented languages contributes more difficult problems to constructing the call graph. <p> This issue has been addressed by other researchers [15, 24]. Aliasing may arise from features other than pointer manipulation. Aliasing due to call-by-reference parameter passing has been widely discussed in the literature. Spillman and Weihl consider its effect on call graph analysis <ref> [21, 24] </ref>. Compiling object-oriented languages contributes more difficult problems to constructing the call graph. Inheritance and function overloading make it difficult to understand what procedures are being invoked even when the function name appears at the call.
References-found: 24

