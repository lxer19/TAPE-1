URL: http://www.research.microsoft.com/~mhwang/di.ps
Refering-URL: http://www.research.microsoft.com/~mhwang/
Root-URL: http://www.research.microsoft.com
Title: DELETED INTERPOLATION AND DENSITY SHARING FOR CONTINUOUS HIDDEN MARKOV MODELS  
Author: X.D. Huang, Mei-Yuh Hwang, Li Jiang, and Milind Mahajan 
Address: One Microsoft Way Redmond, Washington 98052, USA  
Affiliation: Microsoft Corporation  
Abstract: As one of the most powerful smoothing techniques, deleted interpolation has been widely used in both discrete and semi-continuous hidden Markov model (HMM) based speech recognition systems. For continuous HMMs, most smoothing techniques are carried out on the parameters themselves such as Gaussian mean or covariance parameters. In this paper, we propose to smooth the probability density values instead of the parameters of continuous HMMs. This allows us to use most of the existing smoothing techniques for both discrete and continuous HMMs. We also point out that our deleted interpolation can be regarded as a parameter sharing technique. We further generalize this sharing to the probability density function (PDF) level, in which each PDF becomes a basic unit and can be freely shared across any Markov state. For a wide range of dictation experiments, deleted interpolation reduced the word error rate by 11% to 23% over other simple parameter smoothing techniques like flooring. Generic PDF sharing further reduced the error rate by 3%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jelinek F., and Mercer, R. </author> <title> Interpolated Estimation of Markov Source Parameters from Sparse Data." </title> <booktitle> Proc. the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, </address> <publisher> North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: Deleted interpolation (DI) has been successfully used for this purpose for both discrete and semi-continuous HMM based speech recognizers <ref> [1, 2] </ref>. In discrete HMMs, the output probability values in the output probability distributions are identical to the model parameters. However, in continuous HMMs, the PDF parameters (e.g. Gaussian means and covariances) and the values of the PDF are different. <p> Finally we summarize our major findings. 2. DELETED INTERPOLATION AND PDF SHARING 2.1 Deleted Interpolation for PDFs Deleted interpolation was first used to smooth a less well trained, but more detailed discrete output probability distribution with a better trained, but less detailed one <ref> [1] </ref>. The interpolation weights are often estimated using cross-validation data with the EM algorithm, to maximize the probability of the model generating the unseen data. In discrete HMMs, output probability distributions can be interpolated directly, as the value of the output distribution and the output probability parameter are identical.
Reference: [2] <author> Huang X., </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Deleted interpolation (DI) has been successfully used for this purpose for both discrete and semi-continuous HMM based speech recognizers <ref> [1, 2] </ref>. In discrete HMMs, the output probability values in the output probability distributions are identical to the model parameters. However, in continuous HMMs, the PDF parameters (e.g. Gaussian means and covariances) and the values of the PDF are different.
Reference: [3] <author> Gauvain J., and Lee C. </author> <title> Maximum a Posterior Estimation of Multivariate Gaussian Mixture Observations of Markov Chains, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> Vol. 2, No. 2, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: However, in continuous HMMs, the PDF parameters (e.g. Gaussian means and covariances) and the values of the PDF are different. Most smoothing techniques such as Maximum a Posterior Probability (MAP) smooth only model parameters such as Gaussian means, variances, and mixture coefficients <ref> [3] </ref>. Smoothing techniques invented for discrete HMMs may not be applied directly. In this paper, we point out that we can carry out smoothing in the probability value rather than on the parameter itself, as conventionally used for continuous HMMs.
Reference: [4] <author> Hwang, M.Y., Huang X., and Alleva F. </author> <title> Predicting Unseen Triphones with Senones, </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1993. </year>
Reference-contexts: In general, each individual PDF in the mixture of any Markov state can be regarded as a basic unit. We can use these basic units to form any mixture function for modeling any Markov state. Thus, we advance ourselves from state sharing (senones <ref> [4] </ref>) to PDF sharing, which increases the freedom to balance the amount of training data vs. the number of detailed parameters to capture acoustic variability. We experimented with combining the continuous PDFs of the context-dependent (CD) and context-independent (CI) phone models using deleted interpolation instead of MAP-based parameter smoothing. <p> This two-level (CD senone and CI senone) density DI scheme can be extended to multiple levels when the decision-tree based senones <ref> [4] </ref> are used. For example, in the following figure, senone c, e, f can share the same set of PDFs in b, in addition to having their own PDFs; similarly, senone h and i can share the set of PDFs in g.
Reference: [5] <author> Huang X., Acero A., Alleva F., Hwang, M.Y.., Jiang L., Mahajan M. </author> <title> Microsoft Windows Highly Intelligent Speech Recognizer: Whisper, </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1995. </year>
Reference-contexts: This paper is organized as follows. We first discuss deleted interpolation and how it can be used to smooth the density values of continuous HMMs. We then discuss a generic PDF sharing scheme, followed by experimental results to illustrate the utility of these techniques in Whisper <ref> [5] </ref>. Finally we summarize our major findings. 2. DELETED INTERPOLATION AND PDF SHARING 2.1 Deleted Interpolation for PDFs Deleted interpolation was first used to smooth a less well trained, but more detailed discrete output probability distribution with a better trained, but less detailed one [1]. <p> Each phonetic model had the 3-state Bakis topology without any skip arcs. Lexicon pronunciations were obtained from Carnegie Mellon University [7]. The MFCC coefficients were normalized with augmented mean normalization procedure. Detailed Whisper system descriptions can be found in <ref> [5] </ref>. 3.1 Deleted Interpolation for Speaker-Independent Speech Recognition The baseline acoustic model consisted of 6000 context-dependent decision-tree based senones 1 for modeling position-dependent and context-dependent within-word and crossword triphones [7].
Reference: [6] <author> Lee, K.F., </author> <title> Context-Dependent Phonetic Hidden Markov Models for Continuous Speech Recognition, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <month> April, </month> <year> 1990. </year>
Reference-contexts: Parameter sharing is thus advanced from a phone unit (generalized triphones <ref> [6] </ref>) to a Markov state unit (senones), to a density component unit. Parameter sharing at a finer granularity provides more flexibility to capture different acoustic phonetic variations. 3.
Reference: [7] <author> Huang X., Alleva F., Hwang M., and Rosenfeld R. </author> <title> An Overview of Sphinx-II Speech Recognition System, </title> <booktitle> Proc. of ARPA Human Language Technology Workshop, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: The acoustic model consisted of 42 context independent phones (without deletable stops, the flap and the phone TS). Each phonetic model had the 3-state Bakis topology without any skip arcs. Lexicon pronunciations were obtained from Carnegie Mellon University <ref> [7] </ref>. The MFCC coefficients were normalized with augmented mean normalization procedure. Detailed Whisper system descriptions can be found in [5]. 3.1 Deleted Interpolation for Speaker-Independent Speech Recognition The baseline acoustic model consisted of 6000 context-dependent decision-tree based senones 1 for modeling position-dependent and context-dependent within-word and crossword triphones [7]. <p> Mellon University <ref> [7] </ref>. The MFCC coefficients were normalized with augmented mean normalization procedure. Detailed Whisper system descriptions can be found in [5]. 3.1 Deleted Interpolation for Speaker-Independent Speech Recognition The baseline acoustic model consisted of 6000 context-dependent decision-tree based senones 1 for modeling position-dependent and context-dependent within-word and crossword triphones [7]. The HMM output probability density function at each CD and each CI senone was a mixture of 20 Gaussian probability density functions. Diagonal covariance was used for each mixture component. This baseline system used simple flooring techniques for smoothing the variances and mixture weights.
Reference: [8] <author> Young, S.J. </author> <title> and Woodland P.C. The Use of State Tying in Continuous Speech Recognition, </title> <booktitle> Proc. of EuroSpeech, </booktitle> <year> 1993. </year>
Reference-contexts: Unified mapping for the Markov state and PDF sharing thus provides the maximum flexibility to speech recognition. We experimented with clustering PDFs to build a unified senone and PDF mapping structure. Our experiments (using various distortion metrics including <ref> [8] </ref>) indicated that there was no significant error reduction; but computational complexity (mainly from memory reduction) could be modestly reduced with the same level of word error rate constraint. 4. SUMMARY We proposed an algorithm to extend the concept of deleted interpolation to the probability domain for continuous HMMs.
Reference: [9] <author> Takahashi, S. and Sagayama, S. </author> <title> Four-level Tied-Structure for Efficient Representation of Acoustic Modeling, </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1995 </year>
Reference-contexts: The shared PDF scheme opened many possible parameter sharing structures that include senones or generalized triphones as its special cases. We believe that this architecture could be well used to build compact yet accurate acoustic models for advanced speech recognition <ref> [9, 10] </ref>.
Reference: [10] <author> Dugast, C. Beyerlein, P. and Haeb-Umbach, R. </author> <title> Application of Clustering Techniques to Mixture Density Modeling for Continuous Speech Recognition, </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1995. </year>
Reference-contexts: The shared PDF scheme opened many possible parameter sharing structures that include senones or generalized triphones as its special cases. We believe that this architecture could be well used to build compact yet accurate acoustic models for advanced speech recognition <ref> [9, 10] </ref>.
References-found: 10

