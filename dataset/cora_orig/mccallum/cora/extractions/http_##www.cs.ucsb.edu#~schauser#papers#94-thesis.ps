URL: http://www.cs.ucsb.edu/~schauser/papers/94-thesis.ps
Refering-URL: http://www.cs.ucsb.edu/~schauser/papers/
Root-URL: http://www.cs.ucsb.edu
Title: Compiling Lenient Languages for Parallel Asynchronous Execution  
Author: Klaus Erik Schauser 
Note: This work was supported in part by an IBM Graduate Fellowship, by the National Science Foundation Presidential Faculty Fellowship CCR-925370, and LLNL Grant UCB-ERL-92/172. Computational resources were provided, in part, under NSF Infrastructure Grant CDA-8722788.  
Address: Berkeley  
Affiliation: Department of Electrical Engineering and Computer Science Division of Computer Science University of California,  
Abstract-found: 0
Intro-found: 1
Reference: [AA89] <author> Z. Ariola and Arvind. P-TAC: </author> <title> A parallel intermediate language. </title> <booktitle> In Proceedings of the 1989 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 230-242, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: We will use the term inlet node for any node with an inlet annotation, be it a receive node or I-fetch response node. Likewise we are going to use the term outlet node for any send or I-store node. As proposed by <ref> [AA89] </ref>, conditionals can also be represented by function calls. A conditional with two arms can be viewed as a function call, where depending on the result of the predicate, one of two basic blocks are called. <p> He presents a proof of the correctness of the algorithm, based on the semantics of the simple parallel intermediate functional language P-TAC <ref> [AA89] </ref>. The algorithm has not been implemented, so its effectiveness cannot be compared to our approach. 7.2 Future Work There are many fruitful avenues for future work. We list just a few.
Reference: [ACI + 83] <author> Arvind, D. E. Culler, R. A. Iannucci, V. Kathail, K. Pingali, and R. E. Thomas. </author> <title> The Tagged Token Dataflow Architecture. </title> <type> Technical Report FLA memo, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: Therefore, the emerging implicitly parallel languages were accompanied early on by the development of specialized computer architectures, e.g., dataflow machine such as the TTDA <ref> [ACI + 83] </ref>, Manchester dataflow machine [GKW85], Epsilon-2 [GH90], ADAM [MF93], EM-4 [SYH + 89], and Monsoon [PC90]. These architecture not only support dynamic scheduling and synchronization in hardware but also support fine-grained communication. <p> Other backends translate TAM directly into machine code, such as the backend developed for the J-Machine [Spe92]. as their intermediate form. The original MIT compiler translates program graphs into dataflow graphs which can then be executed on the Tagged Token Dataflow Architecture (TTDA) <ref> [ACI + 83] </ref> or be interpreted by a graph interpreter (GITA). Other backends translate program graphs for the Monsoon dataflow machine [PC90] and for the P-RISC machine [NA89]. <p> Care has to be taken to generate threads which obey all dynamic data dependencies and avoid deadlock. We now present related work, future directions, and finish with our conclusions. 7.1 Related Work Over the past two decades much research has been done in compiling lenient languages for dataflow architectures <ref> [ACI + 83, Tra86, AN90, GKW85, Cul90, GDHH89] </ref>. Only in recent years have researchers come to understand that language and architecture can be studied separately and have begun to study compilation aspects of these languages for commodity 166 processors.
Reference: [AE88] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: This combination, termed lenient evaluation by Ken Traub [Tra88], exhibits more parallelism than lazy evaluation while retaining much of its expressive power. Let us take a look at an example. Figure 1.1 shows the top-level function calls and dependencies for the hydrodynamics and heat conduction code called Simple <ref> [CHR78, AE88] </ref>. This problem can be expressed very naturally in a functional language. The 3 algorithm consists of two parts hydrodynamics and heat conduction. 1 To solve the whole problem (simple x), we first solve the hydrodynamic component (hydro x) and then apply the heat conduction function heat. <p> Particles are independent, but statistics from all particle traces are combined into a set of histograms represented as M-structures. The input consists of 40,000 initial particles. Paraffins [AHN88] enumerates the distinct isomers of paraffins up to size 19. Simple <ref> [AE88, CHR78] </ref> is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id. It integrates the solution to several PDEs forward in time over a collection of roughly 25 large rectangular grids.
Reference: [AHN88] <author> Arvind, S. K. Heller, and R. S. Nikhil. </author> <title> Programming Generality and Parallel Computers. </title> <booktitle> In Proc. of the Fourth Int. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <pages> pages 255-286. </pages> <address> ESCOM (Leider), Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Splitting is handled by recursive calls to the trace particle routine. Particles are independent, but statistics from all particle traces are combined into a set of histograms represented as M-structures. The input consists of 40,000 initial particles. Paraffins <ref> [AHN88] </ref> enumerates the distinct isomers of paraffins up to size 19. Simple [AE88, CHR78] is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id. It integrates the solution to several PDEs forward in time over a collection of roughly 25 large rectangular grids.
Reference: [AI87] <author> Arvind and R. A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In Proc. of DFVLR - Conf. 1987 on Par. Proc. in Science and Eng., </booktitle> <address> Bonn-Bad Godesberg, </address> <publisher> W. </publisher> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: The processor just waits until the reply returns and then continues executing. If processors switch to some other work during long latency communication, synchronization becomes more complicated. If the implementation supports a large number of outstanding communication 33 events, it must provide a large enough synchronization name space <ref> [AI87] </ref>. With I-structure accesses, multiple fetches may defer before one is finally satisfied. Independently of the lenient language, fine grain synchronization may also be required because the network may re-order messages.
Reference: [AN90] <author> Arvind and R. S. Nikhil. </author> <title> Executing a Program on the MIT Tagged-Token Dataflow Architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 300-318, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Care has to be taken to generate threads which obey all dynamic data dependencies and avoid deadlock. We now present related work, future directions, and finish with our conclusions. 7.1 Related Work Over the past two decades much research has been done in compiling lenient languages for dataflow architectures <ref> [ACI + 83, Tra86, AN90, GKW85, Cul90, GDHH89] </ref>. Only in recent years have researchers come to understand that language and architecture can be studied separately and have begun to study compilation aspects of these languages for commodity 166 processors.
Reference: [ANP87] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report CSG Memo 269, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> (Also in Proc. of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986.). </year> <month> 176 </month>
Reference-contexts: One limitation is that in order to be purely functional, the contents of the array has to be defined completely within the array comprehension. This ensures that referential transparency is preserved. There are certain problems where this is not sufficient <ref> [ANP87] </ref>, therefore, Id also provides two non-functional data structures, I-structures and M-structures. I-structures are write-once data structures, which separate the creation of the structure from the definition of its elements. <p> It can be used to naturally express the recursive dependencies that arise when implementing parsers [Hut92], attribute grammars [Joh87a], cooperating processes [FH88], or physical simulations, such as successive over-relaxation <ref> [ANP87] </ref>. 2.2.2 Speculative Computation Lazy evaluation only evaluates expressions which contribute to the final answer. It therefore only performs computation that is really required. Strict and lenient on the other hand might do unnecessary computation since they are eager. <p> Heap storage is assumed to be distributed over processors and is accessed through split-phase fetch and store operations, described below. In addition to data, each heap location holds a small number of tag bits, providing element-by-element synchronization as required for I-structures, M-structures, and thunks <ref> [ANP87, Hel89] </ref>. 41 3.1.3 Execution Model Invoking a code-block involves allocating a frame, depositing arguments into it, and initiating execution of the code-block relative to the newly allocated frame. The caller does not suspend upon invoking a child code-block, so it may have multiple concurrent children.
Reference: [App92] <author> A. W. Appel. </author> <title> Compiling with continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: There are 4 Of course, lazy evaluation would only produce as much of the flattened list as is actually required. 5 Additional issues arise due to the use of higher-order functions, partial applications, parallelism, and single assignment limitations. Optimizations of continuations <ref> [Kra88, App92] </ref> and update analysis [HB85] are important techniques to deal with them. 21 several ways to implement lazy evaluation: using explicit delay and force [Hen80], graph reduction [PvE93], or abstract machines [Joh87b].
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: The reason is that the algorithm starts with no knowledge about the program (the trivial annotations) and then iteratively refines this. This refinement may fail to improve at recursive function calls because propagation rule are conservative. Fortunately, techniques developed for dataflow analysis <ref> [ASU86, WZ85] </ref> provide a solution: abstract interpretation combined with techniques for finding fixed points [CC77]. <p> Repeat from Step 3 until ToVisit = ;. It is obvious that this algorithm will find a fixed point after a finite number of steps. Of course the algorithm should employ techniques developed for efficiently finding these fixed points <ref> [ASU86, CPJ85] </ref>, for example, we want to select the basic blocks in Step 3 in the right order starting with basic blocks which do not contain any basic blocks, and only update those parts of matrix which change and trace only those effects on other basic block. 5.7.2 Annotations Deriving squiggly <p> instructions. 3 As already discussed earlier, interprocedural partitioning also slightly decreases the number of instructions related to communication, as the grouping of arguments and results reduces the number of messages. 2 Note, that here the term basic block is used to describe a sequence of instructions without any control transfer <ref> [ASU86] </ref>, which is different from our previous use of basic block for structured dataflow graphs. 3 This is similar to the fraction of control instructions observed for imperative programs on commodity processors.
Reference: [Bar61] <author> R. S. Barton. </author> <title> A new approach to the functional design of a computer. </title> <booktitle> In Proc. Western Joint Computer Conf., </booktitle> <pages> pages 393-396, </pages> <year> 1961. </year>
Reference-contexts: This evolution is similar in spirit to other high level languages requiring complex mechanisms, where initial language and hardware developments where closely related. Historical examples include the Burroughs B5000 <ref> [Bar61] </ref>, a stack architecture with displays to support lexical level addressing of ALGOL like languages, Lisp machines with tag hardware to support dynamic operand types [Moo85], and Prolog machines with hard 5 ware support for unification [Dob87].
Reference: [BCFO91] <author> A.P.W. Bohm, D.C. Cann, J.T. Feo, and R.R. Oldehoeft. </author> <title> Sisal 2.0 reference manual. </title> <type> Technical Report CS-91-118, </type> <institution> Colorado State University, </institution> <month> November 12 </month> <year> 1991. </year>
Reference-contexts: Instead we rely on the run-time system to make some of these decisions (e.g., when to spawn off work). It should be noted though, that other parallel implementations of functional languages use the former approach, most notably SISAL implementations <ref> [BCFO91] </ref>. SISAL limits itself to completely static computation structures, and, therefore, the compiler can statically partition the program for parallel execution [SH86].
Reference: [BCS + 89] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Simmons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark Gamteb. </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Quicksort is a simple quick-sort using accumulation lists. The input is a list of 10,000 random numbers. Gamteb is a Monte Carlo neutron transport code <ref> [BCS + 89] </ref>. It is highly recursive with many conditionals. The work associated with a particle is unpredictable, since particles may be absorbed or scattered due to collisions with various materials, or may split into multiple particles. Splitting is handled by recursive calls to the trace particle routine.
Reference: [Bir84] <author> R.S. Bird. </author> <title> Using circular programs to eliminate multiple traversals of data. </title> <journal> Acta Informatica, </journal> <volume> 21(4) </volume> <pages> 239-250, </pages> <year> 1984. </year>
Reference-contexts: As our small examples show, non-strictness provides a powerful tool to the programmer, since it permits defining data structures recursively and creating circular data structures. This can improve the time and space requirements of a program substantially, by avoiding recomputing results or traversing data structures multiple times <ref> [Bir84] </ref>. It can be used to naturally express the recursive dependencies that arise when implementing parsers [Hut92], attribute grammars [Joh87a], cooperating processes [FH88], or physical simulations, such as successive over-relaxation [ANP87]. 2.2.2 Speculative Computation Lazy evaluation only evaluates expressions which contribute to the final answer. <p> Expressiveness: Non-strictness, as present in lenient and lazy evaluation, significantly increases the expressiveness. The programmer can create circular data structures and define data structures recursively in terms of their own elements. This results in more efficient programs both in space and time requirements <ref> [Hug88, Bir84, Joh87a] </ref> In addition, lazy evaluation provides the control structure to handle infinite data structures. Using explicit delays, programmers can obtain the same benefit under lenient evaluation. Lazy evaluation is the only strategy which completely avoids speculative computation by only evaluating expressions which contribute to the final result.
Reference: [BP89] <author> M. Beck and K. Pingali. </author> <title> From Control Flow to Dataflow. </title> <type> Technical Report TR 89-1050, </type> <institution> CS Dep., Cornell University, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Similar intermediate representations are commonly found in optimizing compilers <ref> [FOW87, BP89, SG85] </ref>. A structured dataflow graph consists of a collection of basic block and interfaces. Typically, each function and each arm of a conditional of the program is represented by a separate basic block. <p> Other backends translate program graphs for the Monsoon dataflow machine [PC90] and for the P-RISC machine [NA89]. Some other approaches compile FORTRAN programs into representations similar to program graphs <ref> [BP89, FOW87] </ref>. 6.1.2 Machine Characteristics Our main experimental platforms are the Thinking Machines CM-5 and sequential machines, especially Sparc-based Sun workstations. The CM-5 is a massively parallel MIMD computer based on the Sparc processor. <p> Structured dataflow graphs are a fully parallel intermediate representation. Other researchers have shown, that it is possible, and also desirable, to translate sequential languages, such as FORTRAN, into representations very similar to ours <ref> [BP89, FOW87] </ref>.
Reference: [CA88] <author> D. E. Culler and Arvind. </author> <title> Resource Requirements of Dataflow Programs. </title> <booktitle> In Proc. of the 15th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 141-150, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The assignment of iterations to frames can be addressed by a variety of policies. A very general form of parallel loop structure, called k-bounded loops <ref> [CA88] </ref>, is used in compiling Id. In this scheme, the amount of parallelism, i.e., the number of frames, is determined at the time the loop is invoked, possibly depending on values within the program. The loop builds a ring of k frames and cycles through them.
Reference: [CC77] <author> P. Cuosot and R. Cuosot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Proc. 4th ACM Symp. on Principles of Programming Languages, </booktitle> <address> Los Angeles, CA, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: This refinement may fail to improve at recursive function calls because propagation rule are conservative. Fortunately, techniques developed for dataflow analysis [ASU86, WZ85] provide a solution: abstract interpretation combined with techniques for finding fixed points <ref> [CC77] </ref>. We illustrate this concept by showing how abstract interpretation can be used by global analysis to derive squiggly edges. 5.7.1 Squiggly Edges Each function and each arm of a conditional is represented by a basic block with n-inputs for the arguments and m-outputs for the results.
Reference: [CGSvE92] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> Empirical Study of Dataflow Languages on the CM-5. </title> <booktitle> In Proc. of the Dataflow Workshop, 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <month> 177 </month>
Reference-contexts: The I-structure allocation policy only differs from Gamteb in that caching is employed. For a more detailed discussion about the effect of parallelism scaling (k-bounds), frame allocation policy, and I-structure management policy on the execution time, see <ref> [CGSvE92] </ref>. In both applications, we observe a linear speedup beyond a small number of processors. Going from one processor to two, we see the effect of message handling overhead. Roughly half the time is lost to message handling overhead in Gamteb and three-quarters of the time in Simple.
Reference: [CGSvE93] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <note> TAM </note>
Reference-contexts: TAM was specifically designed to deal efficiently with the dynamic scheduling required by non-strictness and inter-processor communication <ref> [CSS + 91, CGSvE93] </ref>. As shown below, TAM exposes the scheduling, synchronization, communication, and storage hierarchy to the compiler in a structured form. By making the cost of these operations explicit, the compiler can optimize against them and use the least expensive primitive. <p> Similar specializations can be derived for the other TL0 instructions, e.g., SEND (local/remote), POST (idle/ready/running, sync/no-sync, succ/fail), I-FETCH (present/deferred, local/remote), etc. For a extended discussion of the methodology, including a description of the special cases of the various TL0 instruction and their precise cycle counts, see <ref> [CGSvE93, SGS + 93] </ref>. We have analyzed the implementation of each of the primitives for a Sparc processor, by studying the best possible implementation in Sparc assembly code. <p> Their code-generation approach is based on the Threaded Abstract Machine (TAM), the first threaded execution model to refine Traub's definition of threads by disallowing suspension <ref> [CSS + 91, CGSvE93] </ref>. This notion, which has been picked up by most other researchers, has the advantage of capturing the cost of switching between threads. This work resulted in the first full-fledged Id compiler for conventional architectures, running on various sequential as well as parallel machines.
References-found: 18

