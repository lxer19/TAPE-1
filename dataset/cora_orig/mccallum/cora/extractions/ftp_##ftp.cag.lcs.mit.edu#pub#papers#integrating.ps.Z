URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/integrating.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/integrating.html
Root-URL: 
Title: Integrating Message-Passing and Shared-Memory: Early Experience  
Author: David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper discusses some of the issues involved in implementing a shared-address space programming model on large-scale, distributed-memory multiprocessors. While such a programming model can be implemented on both shared-memory and message-passing architectures, we argue that the transparent, coherent caching of global data provided by many shared-memory architectures is of crucial importance. Because message-passing mechanisms are much more efficient than shared-memory loads and stores for certain types of interprocessor communication and synchronization operations, however, we argue for building multiprocessors that efficiently support both shared-memory and message-passing mechanisms. We describe an architecture, Alewife, that integrates support for shared-memory and message-passing through a simple interface; we expect the compiler and runtime system to cooperate in using appropriate hardware mechanisms that are most efficient for specific operations. We report on both integrated and exclusively shared-memory implementations of our runtime system and two applications. The integrated runtime system drastically cuts down the cost of communication incurred by the scheduling, load balancing, and certain synchronization operations. We also present preliminary performance results comparing the two systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: In turn, this suggests that multiprocessor architectures might be designed such that processors are able to communicate with one another via either shared-memory or message-passing interfaces, using whichever is likely to be most efficient for the communication in question. In fact, the MIT Alewife machine <ref> [1] </ref> does exactly that interprocessor communication can be effected either through a sequentially-consistent shared-memory interface or by way of a messaging mechanism as efficient as those found in many present day message-passing architectures. <p> We then investigate the extent to which we can use Alewife's message-passing functionality to address the defects of shared-memory described above. 3 Architectural Interfaces for Messages Most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 9, 13] </ref>. Therefore any additional cost to support both message-passing in addition to shared-memory must only be paid at the processor-network interface.
Reference: [2] <author> Bob Beck, Bob Kasten, and Shreekant Thakkar. </author> <title> VLSI Assist for a Multiprocessor. </title> <booktitle> In Proceedings Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <address> Washington, DC, </address> <month> October </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general. Some shared-memory machines have implemented message-like primitives in hardware. For example, Beck, Kasten, and Thakkar <ref> [2] </ref> describe the implementation of SLICa system link and interrupt controller chipfor use with the Sequent Balance system. Each SLIC chip is coupled with a processing node and communicates with the other SLIC chips on a special SLIC bus that is separate from the memory system bus.
Reference: [3] <author> Daniel G. Bobrow, Linda G. DeMichiel, Richard P. Gabriel, Sonya E. Keene, Gregor Kiczales, and David A. Moon. </author> <title> Common Lisp Object System Specification. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23, </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: In addition, we note that a shared-object space with messages is the basis for implementing a parallel object-oriented language. In this sense shared-memory and message-passing might be integrated at the language level by integrating object-oriented and procedural programming as in T [18, 19], CLOS <ref> [3] </ref> and, more recently, Dylan [20]. 7 Acknowledgments The research reported on herein was funded in part by NSF grant # MIP-9012773, in part by DARPA contract # N00014-87-K-0825, and in part by a NSF Presidential Young Investigator Award.
Reference: [4] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: For static applications which share data at too fine a grain, even if overhead related to local/remote checks can be eliminated, performance suffers because messages typically aren't large enough to amortize any fixed overheads inherent in message-based communication. It is worth noting, however, that for architectures like iWarp <ref> [4] </ref> which in effect support extremely low overhead messaging for static communication patterns, this is not a problem. Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler.
Reference: [5] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <pages> 2(151-169), </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler. There has been a great deal of work in this area for scientific programs written in various dialects of FORTRAN and targeted at message-passing machines <ref> [5, 11, 14, 21, 24] </ref>. Of course, the constraints are the same as when the optimizations are programmer applied: applications must be primarily static and only coarse-grained data sharing can be exploited efficiently. For dynamic applications, the compiler can't help much.
Reference: [6] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: In a sense, the machine allows controlled breaking of the shared-memory abstraction by the programmer, runtime system, or compiler when doing so can yield higher performance. In fact, Alewife's coherence protocol, LimitLESS directories <ref> [6] </ref>, relies on the processor's ability to directly send and receive coherence packets into the network. With Alewife's integrated interface, a message can be sent with just a few user-level instructions.
Reference: [7] <author> A. Cox and R. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year> <note> Also as a Univ. </note> <institution> Rochester TR-263, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Another example of a shared-memory machine that also supports a message-like primitive is the BBN Butterfly, which provides hardware support for block transfers. In an implementation of distributed shared-memory on this machine, Cox and Fowler <ref> [7] </ref> conclude that an effective block transfer mechanism was critical to performance. They argue that a mechanism that allows more concurrency between processing and block transfer would make a bigger impact.
Reference: [8] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In IFIP Congress, </booktitle> <year> 1989. </year>
Reference-contexts: Finally, it is worth noting that some message-passing machines also provide limited support for the shared-address space programming model. For example, the J-machine <ref> [8] </ref> provides a global name space for objects and an object name cache which speeds up (but does not eliminate) the local/remote checks suggested in 6 Conclusions and Future Work In this paper, we argue that multiprocessors should provide efficient support for both shared-memory and message-passing communication styles.
Reference: [9] <author> Thomas H. Dunigan. </author> <title> Kendall Square Multiprocessor: Early Experiences and Performance. </title> <type> Technical Report ORNL/TM-12065, </type> <institution> Oak Ridge National Laboratory, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: We then investigate the extent to which we can use Alewife's message-passing functionality to address the defects of shared-memory described above. 3 Architectural Interfaces for Messages Most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 9, 13] </ref>. Therefore any additional cost to support both message-passing in addition to shared-memory must only be paid at the processor-network interface. <p> By comparison, typical software implementations (e.g. Intel DELTA and iPSC/860, Kendall Square KSR1) take well over 400 sec <ref> [9] </ref>. 4.3 Remote Thread Invocation To invoke a thread on a remote processor, a pointer to the thread's code and any arguments must be placed atomically on the task queue of another processor.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: In some cases where we argue messages are better that shared-memory, such as the barrier example, a similar effect could be achieved by using shared-memory with a weaker consistency model. For example, the Dash multiprocessor <ref> [13, 10] </ref> has a mechanism to deposit a value from one processor's cache directly into the cache of another processor, avoiding cache coherence overhead. This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general.
Reference: [11] <author> K. Knobe, J. Lukas, and G. Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler. There has been a great deal of work in this area for scientific programs written in various dialects of FORTRAN and targeted at message-passing machines <ref> [5, 11, 14, 21, 24] </ref>. Of course, the constraints are the same as when the optimizations are programmer applied: applications must be primarily static and only coarse-grained data sharing can be exploited efficiently. For dynamic applications, the compiler can't help much.
Reference: [12] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <note> Submitted for publication. Also available as MIT Laboratory for Computer Science Tech Memo, 1993. 9 </note>
Reference-contexts: In this section, we describe the manner in which Alewife integrates shared-memory and message-passing communication facilities; we provide this detail in part to demonstrate that the complexity of integrating both message-passing and shared-memory is not unreasonable. See <ref> [12] </ref> for additional details. Figures 2 and 3 show typical interfaces provided by message-passing multicomputers and shared-memory machines,respectively. 3 In the message-passing version, the processor has direct access to the network and can send (or receive) packets directly into (or from) the network.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hen--nessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We then investigate the extent to which we can use Alewife's message-passing functionality to address the defects of shared-memory described above. 3 Architectural Interfaces for Messages Most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 9, 13] </ref>. Therefore any additional cost to support both message-passing in addition to shared-memory must only be paid at the processor-network interface. <p> In some cases where we argue messages are better that shared-memory, such as the barrier example, a similar effect could be achieved by using shared-memory with a weaker consistency model. For example, the Dash multiprocessor <ref> [13, 10] </ref> has a mechanism to deposit a value from one processor's cache directly into the cache of another processor, avoiding cache coherence overhead. This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general.
Reference: [14] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2 </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler. There has been a great deal of work in this area for scientific programs written in various dialects of FORTRAN and targeted at message-passing machines <ref> [5, 11, 14, 21, 24] </ref>. Of course, the constraints are the same as when the optimizations are programmer applied: applications must be primarily static and only coarse-grained data sharing can be exploited efficiently. For dynamic applications, the compiler can't help much.
Reference: [15] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in Message Passing and Shared Memory Implementations of a Standard Cell Router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages III 88-96, </pages> <year> 1989. </year>
Reference-contexts: Martonosi and Gupta <ref> [15] </ref> compare the performance of a message-passing and a shared-memory implementation of a standard cell router. However, their comparison focused on the two programming styles, and not on the architectural mechanisms provided by the two styles. In contrast, our work focuses on mechanism, not programming style.
Reference: [16] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The CM-5 port of our simulator has proved invaluable, especially for running simulations of large Alewife systems (64 to 512 nodes). 4.2 Combining Tree Barriers In order to implement barrier synchronization on Alewife, we use a combining tree scheme <ref> [16] </ref>. In such a scheme, a k-ary tree with n leaves is laid out across the n processors participating in the barrier such that exactly one tree leaf resides on each processor.
Reference: [17] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: application which accesses data in a particularly regular fashionthis allows data to be prefetched into the cache such that virtually all actual accesses to remote data hit in the cache, effectively hiding all communication latency. 6 details. 4.5 Thread Scheduler The Alewife runtime system is based on lazy task creation <ref> [17] </ref>. This system performs the tasks of load-balancing and dynamic partitioning of programs that cannot be statically partitioned effectively. As part of this research, we have implemented a version of this runtime system that uses message-based communication in both searching for work and thread migration. <p> We benchmarked both the original shared-memory and the hybrid implementations using one synthetic application (grain) and an adaptive numerical integration code (aq). As described in <ref> [17] </ref>, the grain application enumerates a complete binary tree of depth n and sums the values found at the leaves using a recursive divide-and-conquer structure. Before obtaining the value found at each leaf, a delay loop of l cycles in duration is executed.
Reference: [18] <author> J. Rees and N. Adams. </author> <title> T: A Dialect of LISP. </title> <booktitle> In Proceedings of Symposium on Lisp and Functional Programming, </booktitle> <month> August </month> <year> 1982. </year>
Reference-contexts: In addition, we note that a shared-object space with messages is the basis for implementing a parallel object-oriented language. In this sense shared-memory and message-passing might be integrated at the language level by integrating object-oriented and procedural programming as in T <ref> [18, 19] </ref>, CLOS [3] and, more recently, Dylan [20]. 7 Acknowledgments The research reported on herein was funded in part by NSF grant # MIP-9012773, in part by DARPA contract # N00014-87-K-0825, and in part by a NSF Presidential Young Investigator Award.
Reference: [19] <author> J. Rees, N. Adams, and J. Meehan. </author> <title> The T Manual, Fourth Edition. </title> <type> Technical report, </type> <institution> Yale University, Computer Science Department, </institution> <month> Jan-uary </month> <year> 1984. </year>
Reference-contexts: In addition, we note that a shared-object space with messages is the basis for implementing a parallel object-oriented language. In this sense shared-memory and message-passing might be integrated at the language level by integrating object-oriented and procedural programming as in T <ref> [18, 19] </ref>, CLOS [3] and, more recently, Dylan [20]. 7 Acknowledgments The research reported on herein was funded in part by NSF grant # MIP-9012773, in part by DARPA contract # N00014-87-K-0825, and in part by a NSF Presidential Young Investigator Award.
Reference: [20] <institution> Apple Computer Eastern Research and Technology. </institution> <month> Dylan: </month> <title> an object-oriented dynamic language. </title> <institution> Apple Computer, Inc., </institution> <year> 1992. </year>
Reference-contexts: In addition, we note that a shared-object space with messages is the basis for implementing a parallel object-oriented language. In this sense shared-memory and message-passing might be integrated at the language level by integrating object-oriented and procedural programming as in T [18, 19], CLOS [3] and, more recently, Dylan <ref> [20] </ref>. 7 Acknowledgments The research reported on herein was funded in part by NSF grant # MIP-9012773, in part by DARPA contract # N00014-87-K-0825, and in part by a NSF Presidential Young Investigator Award.
Reference: [21] <author> Anne Rogers and Keshav Pingali. </author> <title> Process Decomposition through Locality of Reference. </title> <booktitle> In SIGPLAN '89, Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler. There has been a great deal of work in this area for scientific programs written in various dialects of FORTRAN and targeted at message-passing machines <ref> [5, 11, 14, 21, 24] </ref>. Of course, the constraints are the same as when the optimizations are programmer applied: applications must be primarily static and only coarse-grained data sharing can be exploited efficiently. For dynamic applications, the compiler can't help much.
Reference: [22] <author> Alfred Z. Spector. </author> <title> Performing Remote Operations Efficiently on a Local Computer Network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4), </volume> <month> April </month> <year> 1982. </year> <pages> Pages 246-260. </pages>
Reference-contexts: It is for these reasons that we believe that some hardware support for shared-memory programs is important; the fundamental hardware mechanism is coherent caching of global addresses. Note that from this perspective, systems like <ref> [22] </ref> and Active Messages [23] provide just an efficient messaging interfacean implementation of a shared-address space programming model on top of such systems will face the same problems as those described above for implementations on message-passing architectures. 2.2 Defects of Shared-memory The previous section discussed the aspects of application behavior that
Reference: [23] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: It is for these reasons that we believe that some hardware support for shared-memory programs is important; the fundamental hardware mechanism is coherent caching of global addresses. Note that from this perspective, systems like [22] and Active Messages <ref> [23] </ref> provide just an efficient messaging interfacean implementation of a shared-address space programming model on top of such systems will face the same problems as those described above for implementations on message-passing architectures. 2.2 Defects of Shared-memory The previous section discussed the aspects of application behavior that benefit from hardware support
Reference: [24] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semiautomatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6(1), </volume> <year> 1988. </year>
Reference-contexts: Optimizations similar to those a programmer might use to eliminate the overhead of local/remote checks can be applied by a compiler. There has been a great deal of work in this area for scientific programs written in various dialects of FORTRAN and targeted at message-passing machines <ref> [5, 11, 14, 21, 24] </ref>. Of course, the constraints are the same as when the optimizations are programmer applied: applications must be primarily static and only coarse-grained data sharing can be exploited efficiently. For dynamic applications, the compiler can't help much.
References-found: 24

