URL: http://www.cs.umd.edu/~keleher/papers/sigarch93.ps.gz
Refering-URL: http://www.cs.umd.edu/~keleher/papers.html
Root-URL: 
Title: Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology  
Author: Sandhya Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel 
Affiliation: Department of Computer Science Rice University  
Abstract: We evaluate the effect of processor speed, network bandwidth, and software overhead on the performance of release-consistent software distributed shared memory. We examine five different protocols for implementing release consistency: eager update, eager invalidate, lazy update, lazy invalidate, and a new protocol called lazy hybrid. This lazy hybrid protocol tries to combine the benefits of both lazy update and lazy invalidate. Our simulations indicate that with the processors and networks that are becoming available, coarse-grained applications, such as Jacobi and TSP perform well, more or less independent of the protocol used. Medium-grained applications, such as Water, can achieve good performance, but the choice of protocol is critical. For sixteen processors, the best protocol, lazy hybrid, performed more than three times better than the worst, the eager update. Fine-grained applications such as Cholesky achieve little speedup regardless of the protocol because of the frequency of synchronization operations and the high latency involved. While the use of relaxed memory models, lazy implementations, and multiple-writer protocols has significantly reduced the impact of false sharing, synchronization latency remains a serious problem for software distributed shared memory systems. These results suggest that future work on software DSMs should concentrate on reducing the amount of synchronization or its effect. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report CS-1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Lazy implementations of RC further delay the propagation of modifications until the acquire. At that time, the last releaser piggybacks on the lock grant message sent to the acquirer a set of write notices describing those shared data modifications that precede the acquire according to the happened-before-1 partial order <ref> [1] </ref>. A write notice is an indication that a page has been modified. The happened-before-1 partial order is essentially the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubia-towicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, there are at least two drawbacks to multithreading. The work will be divided up at an even finer granularity which will introduce local synchronization overhead between threads on the same processor. There is also the potential for increased communication resulting from capacity induced misses <ref> [2] </ref>. However, we still believe that multithread-ing is a promising approach for masking latency in DSM systems because capacity misses are rare and local synchronization is cheap. To date, no one has evaluated the effectiveness of multithreading in a DSM setting.
Reference: [3] <author> M. Ahamad, P.W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: However, some of the improvements are masked by the corresponding changes in software overheads. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [3, 5, 9, 10] </ref>). We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford [13], because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [5].
Reference: [4] <author> H.E. Bal and A.S. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of the 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Lo [15]), performance reports have been relatively rare. The few performance results that have been published consist of measurements of a particular implementation in a particular hardware and software environment <ref> [4, 6, 7, 14] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete.
Reference: [5] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: However, some of the improvements are masked by the corresponding changes in software overheads. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [3, 5, 9, 10] </ref>). We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford [13], because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [5]. <p> We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford [13], because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas <ref> [5] </ref>. EC differs from RC because it requires all shared data to be explicitly associated with some synchronization variable. On a lock acquisition EC only needs to propagate the shared data associated with the lock.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Lo [15]), performance reports have been relatively rare. The few performance results that have been published consist of measurements of a particular implementation in a particular hardware and software environment <ref> [4, 6, 7, 14] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete. <p> Regardless of the considerable bandwidth available on these networks, Cholesky's performance is constrained by the very high number of synchronization operations. Among the protocols for implementing software release consistency, we distinguish between eager and lazy protocols. Eager protocols push modifications to all cachers at synchronization variable releases <ref> [6] </ref>. In contrast, lazy protocols [12] pull the modifications at synchronization variable acquires, and only to the processor executing the acquire. Both eager and lazy release consistency can be implemented using either invalidate or update protocols. <p> Section 5 discusses our simulation methodology, and Section 6 presents the simulation results. We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) [10], eager release consistency (ERC) <ref> [6] </ref>, and lazy release consistency (LRC) [12]. RC [10] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses oc cur. <p> While this strategy masks latency, in a software implementation it is also important to reduce the number of messages sent because of the high per message cost. In an eager software implementation of RC such as Munin's multiple-writer protocol <ref> [6] </ref>, a processor delays propagating its modifications of shared data until it executes a release (see Figures 1 and 2). Lazy implementations of RC further delay the propagation of modifications until the acquire. <p> This contrasts with the exclusive-writer protocol used, for instance, in DASH [10], where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin <ref> [6] </ref> indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic between synchronization points. All of the protocols support the use of exclusive locks and global barriers to synchronize access to shared memory. <p> In this table, the concurrent last modifiers for a page are the processors that created modifications that do not causally (via happened-before-1 ) precede any other known modifications to that page. 4.1 The Eager Protocols 4.1.1 Locks We base our eager RC algorithms on Munin's multiple-writer protocol <ref> [6] </ref>. A processor delays propagating its modifications of shared data until it comes to a release. <p> Although all messages are simulated, protocol-specific consistency information is not reflected in the amount of data sent. Only the actual shared data moved by the protocols is included in message lengths. 6 Simulation Results 6.1 DSM on an Ethernet Although prior work <ref> [6] </ref> showed that Ethernet-based software DSMs can achieve significant speedups, we find that for modern processors the Ethernet is no longer a viable option. Table 2 shows the speedup of Ja-cobi, a coarse-grained program. Jacobi's speedup peaks at 5.2 for eight processors, and declines rapidly thereafter. <p> On a lock acquisition EC only needs to propagate the shared data associated with the lock. EC, however, requires the programmer to insert additional synchronization in shared memory programs to execute correctly on an EC memory. Typically, RC does not require additional synchronization. Ivy [14] and Munin <ref> [6] </ref> are two implementations of software DSMs for which performance measurements have been published. Both achieve good speedups on many of the applications studied. The slow processors used in the implementations prevented the network from becoming a bottleneck in achieving these speedups. With faster processors, faster networks are Pr.
Reference: [7] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Lo [15]), performance reports have been relatively rare. The few performance results that have been published consist of measurements of a particular implementation in a particular hardware and software environment <ref> [4, 6, 7, 14] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete.
Reference: [8] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, S. Madala, and J. B. Sinclair. </author> <title> The Efficient Simulation of Parallel Computer Systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The evaluation is done by execution-driven simulation <ref> [8] </ref>. The application programs we use have been written for (hardware) shared memory multiprocessors.
Reference: [9] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: However, some of the improvements are masked by the corresponding changes in software overheads. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [3, 5, 9, 10] </ref>). We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford [13], because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [5].
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washing-ton, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Furthermore, the hardware environments of many of these implementations are by now obsolete. Much faster processors are commonplace, and much faster networks are becoming available. We are focusing on DSMs that support release consistency <ref> [10] </ref>, i.e., where memory is guaranteed to be consistent only following certain synchronization operations. <p> Section 5 discusses our simulation methodology, and Section 6 presents the simulation results. We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) <ref> [10] </ref>, eager release consistency (ERC) [6], and lazy release consistency (LRC) [12]. RC [10] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses oc cur. <p> We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) <ref> [10] </ref>, eager release consistency (ERC) [6], and lazy release consistency (LRC) [12]. RC [10] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses oc cur. <p> All five are multiple-writer protocols. Multiple processors can concurrently write to their own copy of a page with their separate modifications being merged at a subsequent release, in accordance with the RC model. This contrasts with the exclusive-writer protocol used, for instance, in DASH <ref> [10] </ref>, where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin [6] indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic between synchronization points. <p> However, some of the improvements are masked by the corresponding changes in software overheads. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [3, 5, 9, 10] </ref>). We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford [13], because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [5].
Reference: [11] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The pere--grine high-performance rpc system. </title> <note> Accepted for publication in Software: Practice and Experience. Available as Technical Report RICE COMP TR91-151, </note> <month> August </month> <year> 1992. </year>
Reference-contexts: This cost is set at (1000 + message lengthfl1:5=4) processor cycles at both the destination and source of each message. These figures were modeled after the Peregrine <ref> [11] </ref> implementation overheads. Peregrine is an RPC system implemented at Rice that provides performance close to optimal using techniques such as avoiding intermediate copying. The lazy implementation's extra complexity is modeled by doubling the per-byte message overhead both at the sender and at the receiver.
Reference: [12] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Among the protocols for implementing software release consistency, we distinguish between eager and lazy protocols. Eager protocols push modifications to all cachers at synchronization variable releases [6]. In contrast, lazy protocols <ref> [12] </ref> pull the modifications at synchronization variable acquires, and only to the processor executing the acquire. Both eager and lazy release consistency can be implemented using either invalidate or update protocols. <p> We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) [10], eager release consistency (ERC) [6], and lazy release consistency (LRC) <ref> [12] </ref>. RC [10] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses oc cur. <p> The happened-before-1 partial order is essentially the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs. The happened-before-1 partial order can be represented efficiently by tagging write notices with vector timestamps <ref> [12] </ref>. At acquire time, the acquiring processor checks for which pages the incoming write notices contain vector timestamps larger than the timestamp of its copy of that page in memory.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Essentially, RC requires ordinary shared memory accesses to be performed only when a subsequent release by the same processor is performed. RC implementations can delay the effects of shared memory accesses as long as they meet this constraint. For instance, the DASH <ref> [13] </ref> implementation of RC buffers and pipelines writes without blocking the processor. A subsequent release is not allowed to perform (i.e., the corresponding lock cannot be granted to another processor) until acknowledgments have been received for all outstanding invalidations. <p> We have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford <ref> [13] </ref>, because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [5]. EC differs from RC because it requires all shared data to be explicitly associated with some synchronization variable.
Reference: [14] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Lo [15]), performance reports have been relatively rare. The few performance results that have been published consist of measurements of a particular implementation in a particular hardware and software environment <ref> [4, 6, 7, 14] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete. <p> The new diffs are then merged into the page and the processor is allowed to proceed. The lazy protocols determine the location of a page or updates to the page entirely on the basis of local information. Additional messages that are required by other DSM systems <ref> [14] </ref> to find a page are unnecessary. 5 Methodology 5.1 Application Suite We simulated four programs, covering three different classes of applications. TSP and Jacobi are coarse-grained programs with a large amount of computation relative to synchronization (20,240 and 1,130,750 cycles between external synchronizations, respectively, at 16 processors). <p> On a lock acquisition EC only needs to propagate the shared data associated with the lock. EC, however, requires the programmer to insert additional synchronization in shared memory programs to execute correctly on an EC memory. Typically, RC does not require additional synchronization. Ivy <ref> [14] </ref> and Munin [6] are two implementations of software DSMs for which performance measurements have been published. Both achieve good speedups on many of the applications studied. The slow processors used in the implementations prevented the network from becoming a bottleneck in achieving these speedups.
Reference: [15] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although several models and algorithms for software distributed shared memory (DSM) have been published (see, for instance, the survey article by Nitzberg and fl This work was supported in part by NSF Grants CCR-9116343 and CCR-9211004, Texas ATP Grant No. 0036404013 and by a NASA Graduate Fellowship. Lo <ref> [15] </ref>), performance reports have been relatively rare. The few performance results that have been published consist of measurements of a particular implementation in a particular hardware and software environment [4, 6, 7, 14].
Reference: [16] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The application programs are Jacobi, Traveling Salesman Problem (TSP), and Water and Cholesky from the SPLASH benchmark suite <ref> [16] </ref>. Jacobi and TSP exhibit coarse-grained parallelism, with little synchronization relative to the amount of computation, whereas Water may be characterized as medium-grained, and Cholesky as fine-grained. <p> TSP solves the traveling salesman problem for 18-city tours. Our Jacobi program is a simple Successive Over-Relaxation program that works on grids of 512 by 512 elements. Water, from the SPLASH suite <ref> [16] </ref>, is a medium grained molecular dynamics simulation (1200 cycles between external synchronizations). We ran Water with the default parameters: 288 molecules for 2 steps.
References-found: 16

