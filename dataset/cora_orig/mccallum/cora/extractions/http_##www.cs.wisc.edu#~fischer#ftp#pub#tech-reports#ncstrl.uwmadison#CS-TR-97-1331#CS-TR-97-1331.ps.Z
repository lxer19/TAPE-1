URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-97-1331/CS-TR-97-1331.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-97-1331/
Root-URL: http://www.cs.wisc.edu
Email: fjussi,mirong@cs.wisc.edu  
Title: Relational Joins for Data on Tertiary Storage  
Author: Jussi Myllymaki Miron Livny 
Keyword: tertiary storage, join methods, parallel I/O  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Despite the steady decrease in secondary storage prices and continuous, sometimes drastic, improvements in storage density, the data storage requirements of many organizations cannot be met economically using secondary storage alone. Tertiary storage solutions, especially automated magnetic tape libraries, offer a much lower storage cost at a level of functionality that satisfies the needs of many application types. Database management systems (DBMS) typically do not incorporate tertiary storage devices such as magnetic tapes and optical disks as a first-class citizen in the storage hierarchy. The typical solution in bringing tertiary-resident data under the control of a DBMS is to use operating system facilities to copy the data to secondary storage, and then to perform query optimization and execution as if the data had been in secondary storage all along. This approach fails to recognize the opportunities for saving execution time and storage space if the data were accessed on tertiary devices directly and in parallel with other I/Os. In this paper we examine ways of joining two relations stored on magnetic tapes. Our earlier work has shown that when one relation is stored on tape and the other on disk, a parallel I/O variant of Nested Block Join performs quite well, given sufficient main memory space. Work presented in this paper extends by considering the case where both relations are larger than available disk space. To tackle main memory size limitations, we focus on hashing-based solutions. We modified Grace Hash Join to handle a range of tape relation sizes and to exploit parallelism between disk and tape I/Os. We show how disk and main memory space affect the performance of tertiary joins and demonstrate how parallel I/O helps these methods save execution time as well as memory and disk space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI X3T9.2 Committee. </author> <booktitle> Small Computer System Interface 2 (SCSI-2), </booktitle> <month> Nov. </month> <year> 1993. </year> <note> Working Draft, Project 375D. </note>
Reference-contexts: than available main memory and the combined size of the relations exceeds available disk 1 Disk caching would reduce seek and latency costs even if requests were smaller than 30 blocks. 2 The SCSI standard defines the command READ REVERSE but its implementation by tape drive manufacturers is left optional <ref> [1] </ref>. On a historical note, Knuth also assumes bi-directional tape drives in his work on tape sorting [12]. 6 space.
Reference: [2] <author> M. Blasgen and K. Eswaran. </author> <title> Storage and access in relational data bases. </title> <journal> IBM Systems Journal, </journal> <volume> 16(4) </volume> <pages> 363-377, </pages> <year> 1977. </year>
Reference-contexts: Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively. The database literature contains an extensive collection of work on optimizing joins, started by the seminal paper by Blasgen and Eswaran <ref> [2] </ref>. Much of the work has been devoted to optimizing ad hoc equi-joins, i.e. joins where an exact match of join attributes is requested and which do not rely on the existence of pre-computed access structures such as indices. <p> Most studies consider main memory and secondary storage as the only forms of storage, but few studies include tertiary storage in the system model. Studies on disk-based joins typically employ a transfer-only cost model <ref> [2, 3, 5, 11, 17] </ref> where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7]. <p> Therefore, the total cost of the join (response time) is roughly the same as I/O cost, like in most other cost models for joins. Join method analyses typically do not consider query output costs since they are deemed the same for all methods <ref> [2, 5, 7, 8, 17] </ref>. In our study, however, it seems necessary to include the output costs in the cost model because the input cost may be affected by the output cost.
Reference: [3] <author> K. Bratbergsengen. </author> <title> Hashing methods and relational algebra operations. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 323-333, </pages> <address> Singapore, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Most studies consider main memory and secondary storage as the only forms of storage, but few studies include tertiary storage in the system model. Studies on disk-based joins typically employ a transfer-only cost model <ref> [2, 3, 5, 11, 17] </ref> where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7].
Reference: [4] <author> M. J. Carey, L. M. Haas, and M. Livny. </author> <title> Tapes hold data too: Challenges of tuples on tertiary store. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 413-417, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Only the most frequently used data is stored in secondary storage, for instance, meta-data, indices, small datasets, or subsets of larger datasets. Database management systems (DBMS) are typically not capable of operating on tertiary-resident data in the same manner as they handle secondary storage <ref> [4] </ref>. One reason is the wide variety and dissimilarity of tertiary storage devices, another is the difference in how tertiary storage and secondary storage are accessed at the device level. Magnetic tape storage, for instance, is inherently sequential with slow random-access capabilities. <p> The results of the experiments are described and analyzed in Sections 7 through 9. Section 10 concludes the paper. 2 Related Work Our investigation of database joins on tertiary storage was prompted by a SIGMOD Database Challenges paper <ref> [4] </ref> which pointed out that DBMS control of tertiary storage was becoming ever more important because of the large volumes of tertiary-resident data that are available for analysis and manipulation.
Reference: [5] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 1-8, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: To join relations much larger than main memory or disk space, one has to employ hashing techniques. In this paper we present modifications to Grace Hash Join <ref> [5] </ref> to make it work on tape storage and to exploit parallel I/O and double-buffering techniques described in [14]. We describe and evaluate both disk-tape and tape-tape join methods based on Grace Hash Join. <p> Most studies consider main memory and secondary storage as the only forms of storage, but few studies include tertiary storage in the system model. Studies on disk-based joins typically employ a transfer-only cost model <ref> [2, 3, 5, 11, 17] </ref> where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7]. <p> Therefore, the total cost of the join (response time) is roughly the same as I/O cost, like in most other cost models for joins. Join method analyses typically do not consider query output costs since they are deemed the same for all methods <ref> [2, 5, 7, 8, 17] </ref>. In our study, however, it seems necessary to include the output costs in the cost model because the input cost may be affected by the output cost. <p> Each R hash bucket is read back into memory in turn and joined with the corresponding S hash bucket by scanning it. Step II is iterated until S is exhausted. The number of hash buckets is B = jRj M where M &gt; p jRj (see <ref> [5] </ref>). We assume that hash values are uniformly distributed, that is, the hash buckets for R are equal-sized.
Reference: [6] <author> S. Ghandeharizadeh, A. Dashti, and C. Shahabi. </author> <title> Pipelining mechanism to minimize the latency time in hierarchical multimedia storage managers. </title> <journal> Computer Communications, </journal> <volume> 18(3) </volume> <pages> 170-184, </pages> <month> Mar. </month> <year> 1995. </year> <note> Also available as Technical Report 94-584, </note> <institution> Computer Science Department, University of Southern California. </institution>
Reference-contexts: While multimedia systems such as video-on-demand servers have successfully integrated tertiary storage into their storage hierarchies <ref> [6, 10, 18] </ref>, relational database systems still have little notion of incorporating tertiary storage as an integral part of the system. Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively.
Reference: [7] <author> L. M. Haas, M. J. Carey, and M. Livny. </author> <title> SEEKing the truth about ad hoc join costs. </title> <type> Technical Report 1148, </type> <institution> Department of Computer Science, University of Wisconsin at Madison, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in <ref> [7] </ref>. The optimization and execution of queries on tertiary-resident data in the Postgres database system is described in [15, 16]. In that architecture, a query on large tertiary relations is broken into smaller, independent subqueries on fragments of the relations. <p> Therefore, the total cost of the join (response time) is roughly the same as I/O cost, like in most other cost models for joins. Join method analyses typically do not consider query output costs since they are deemed the same for all methods <ref> [2, 5, 7, 8, 17] </ref>. In our study, however, it seems necessary to include the output costs in the cost model because the input cost may be affected by the output cost. <p> We assume that all disk accesses are multi-page I/O requests. The cost of a disk access is therefore derived by counting the number of blocks transferred. The seek cost and rotational latency are ignored. As shown in <ref> [7] </ref>, disk seeks and rotational latency play a relatively minor role compared to transfer cost when disk requests are at least moderately large. In our model, the size of all disk requests is assumed to be at least 30 blocks, making seek and latency costs negligible 1 .
Reference: [8] <author> R. B. Hagmann. </author> <title> An observation on database buffering performance metrics. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <pages> pages 289-293, </pages> <address> Kyoto, Japan, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Studies on disk-based joins typically employ a transfer-only cost model [2, 3, 5, 11, 17] where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in <ref> [8] </ref>, and a detailed cost model combining both cost models was developed in [7]. The optimization and execution of queries on tertiary-resident data in the Postgres database system is described in [15, 16]. <p> Therefore, the total cost of the join (response time) is roughly the same as I/O cost, like in most other cost models for joins. Join method analyses typically do not consider query output costs since they are deemed the same for all methods <ref> [2, 5, 7, 8, 17] </ref>. In our study, however, it seems necessary to include the output costs in the cost model because the input cost may be affected by the output cost.
Reference: [9] <author> B. K. Hillyer and A. Silberschatz. </author> <title> Random I/O scheduling in online tertiary storage systems. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Our performance results are from an experimental implementation rather than a simulation. Whereas [16] considers different types of queries, our focus is on joins without indices. A study on tape I/O scheduling <ref> [9] </ref> presents an interesting and detailed model of the random access behavior of the same tape drive model we use in our studies. The tertiary join algorithms we study exhibit mostly sequential tape I/O patterns, complementing the picture drawn by [9]. 3 System Model for Tertiary Joins We now present our <p> A study on tape I/O scheduling <ref> [9] </ref> presents an interesting and detailed model of the random access behavior of the same tape drive model we use in our studies. The tertiary join algorithms we study exhibit mostly sequential tape I/O patterns, complementing the picture drawn by [9]. 3 System Model for Tertiary Joins We now present our system model for analyzing relational joins in a tertiary storage environment.
Reference: [10] <author> M. G. Kienzle, A. Dan, D. Sitaram, and W. Tetzlaff. </author> <title> Using tertiary storage in video-on-demand servers. </title> <booktitle> In Proc. CompCon, </booktitle> <pages> pages 225-233, </pages> <address> San Francisco, CA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: While multimedia systems such as video-on-demand servers have successfully integrated tertiary storage into their storage hierarchies <ref> [6, 10, 18] </ref>, relational database systems still have little notion of incorporating tertiary storage as an integral part of the system. Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively.
Reference: [11] <author> W. Kim. </author> <title> A new way to compute the product and join of relation. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 179-187, </pages> <address> Santa Monica, CA, </address> <month> May </month> <year> 1980. </year>
Reference-contexts: In our earlier work [13], we have examined the case where one relation is stored on tape and the other on disk. Using an analytical model, we have shown that a parallel I/O variant of Nested Block Join <ref> [11] </ref> performs very well when at least half the smaller relation fits in main memory but very poorly when little main memory is available. To join relations much larger than main memory or disk space, one has to employ hashing techniques. <p> Most studies consider main memory and secondary storage as the only forms of storage, but few studies include tertiary storage in the system model. Studies on disk-based joins typically employ a transfer-only cost model <ref> [2, 3, 5, 11, 17] </ref> where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7].
Reference: [12] <author> D. Knuth. </author> <title> The Art of Computer Programming, Vol. III: Sorting and Searching. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Redwood City, CA, </address> <year> 1973. </year>
Reference-contexts: On a historical note, Knuth also assumes bi-directional tape drives in his work on tape sorting <ref> [12] </ref>. 6 space. An iterative way to compute the join is to read relation S in pieces, denoted by S i , and compute a mini-join S i 1 R in each iteration i.
Reference: [13] <author> J. Myllymaki and M. Livny. </author> <title> Disk-tape joins: Synchronizing disk and tape access. </title> <booktitle> In Proc. ACM SIGMETRICS, </booktitle> <pages> pages 279-290, </pages> <address> Ottawa, Canada, </address> <month> May </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: Our goal is to show how disk and main memory space affect the performance of tertiary joins and to demonstrate how parallel I/O helps save execution time as well as memory and disk space. In our earlier work <ref> [13] </ref>, we have examined the case where one relation is stored on tape and the other on disk. <p> We describe and evaluate both disk-tape and tape-tape join methods based on Grace Hash Join. We also review algorithms developed in <ref> [13] </ref> and apply them in the context of two tape relations. For all tertiary join methods examined, we show what the resource requirements are and analyze the performance 2 via an experimental implementation. <p> In our earlier work, we have reported on ways to join a tape relation with a disk relation when direct access to tertiary storage by the join methods is made possible <ref> [13] </ref>. The results indicated that significant savings in execution time and buffer space (both main memory and disk space) can be achieved by customizing existing relational join algorithms in two ways. <p> Main memory size and the speed ratio between the disk and tape devices were identified as the key components determining the performance of a join of a disk and tape relation. The buffering techniques discussed in <ref> [13] </ref> were applied in a more general context, extensible to data mining and data visualization applications, in [14]. The paper also provided an analysis of how the performance characteristics and interaction of main memory, process scheduling, I/O bus and storage devices affect parallel I/O throughput. <p> The tertiary join algorithms we study exhibit mostly sequential tape I/O patterns, complementing the picture drawn by [9]. 3 System Model for Tertiary Joins We now present our system model for analyzing relational joins in a tertiary storage environment. As in our earlier work <ref> [13] </ref>, the model is a simplified view of computer system components and their interaction because the goal of our studies is to perform qualitative analysis and to understand the interdependency of system resource requirements and join execution time. 4 Table 1: Summary of Notation R smaller relation (tape R) S larger <p> These two conditions guarantee that each R hash bucket fits into memory when read back from disk. 5.1.3 Concurrent Disk-Tape Nested Block Join (CDT-NB) Concurrent Disk-Tape Nested Block Join (CDT-NB) is a parallel I/O variant of DT-NB and was first described in <ref> [13] </ref>. As in DT-NB, relation R is first copied from tape to disk. CDT-NB performs the join by reading a chunk of S into a main memory or disk buffer and joining the previous chunk simultaneously with R. This method has two variations. <p> The aggregate disk speed was assumed to be twice the tape speed (X D = 2X T ). The response times were calculated using cost formulas derived for each join method. The derivation and the resulting 10 formulas are based on <ref> [13] </ref> but are beyond the scope of this paper. In Figures 1 through 3 we examine three interesting ranges of jRj. Figure 1 shows the case where jRj is comparable to M (M appears at value 1). <p> We have presented modifications to Grace Hash Join to make it operate on tape-resident relations, and also reviewed Nested Block Join-based methods developed in our earlier work <ref> [13] </ref>. An experimental implementation and performance analysis with relation sizes ranging up to 10,000 MB allowed us to validate the strengths and weaknesses of these methods.
Reference: [14] <author> J. Myllymaki and M. Livny. </author> <title> Efficient buffering for concurrent disk and tape I/O. </title> <booktitle> In Proc. Performance '96, </booktitle> <pages> pages 453-471, </pages> <address> Lausanne, Switzerland, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: To join relations much larger than main memory or disk space, one has to employ hashing techniques. In this paper we present modifications to Grace Hash Join [5] to make it work on tape storage and to exploit parallel I/O and double-buffering techniques described in <ref> [14] </ref>. We describe and evaluate both disk-tape and tape-tape join methods based on Grace Hash Join. We also review algorithms developed in [13] and apply them in the context of two tape relations. <p> The buffering techniques discussed in [13] were applied in a more general context, extensible to data mining and data visualization applications, in <ref> [14] </ref>. The paper also provided an analysis of how the performance characteristics and interaction of main memory, process scheduling, I/O bus and storage devices affect parallel I/O throughput. In our current studies we focus on joins between two tertiary-resident relations. <p> One particular technique join methods can use is double-buffering, in which a reader process reads S i from one memory or disk buffer and joins it with R while a writer process fetches S i+1 from tape and stores it in another buffer <ref> [14] </ref>. The simple approach is to split existing buffer space in two halves, but this makes each S i half the original size. This in turn doubles the number of iterations needed and doubles the number of times R is scanned. <p> To study the join methods' sensitivity to variations in the disk/tape speed ratio, we repeated 14 Experiment 3 with tape relations containing data with different compression ratios. As discussed in <ref> [14] </ref>, a higher compression ratio yields a higher data transfer speed of the tape drive, and a lower compression ratio yields a lower data transfer speed. Each compression ratio also yields a different optimum join time.
Reference: [15] <author> S. Sarawagi. </author> <title> Database systems for efficient access to tertiary memory. </title> <booktitle> In Proc. IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 120-126, </pages> <address> Monterey, CA, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7]. The optimization and execution of queries on tertiary-resident data in the Postgres database system is described in <ref> [15, 16] </ref>. In that architecture, a query on large tertiary relations is broken into smaller, independent subqueries on fragments of the relations. The system caches fragments in secondary storage and improves tape access efficiency by reordering the I/O requests resulting from the subqueries.
Reference: [16] <author> S. Sarawagi. </author> <title> Query processing in tertiary memory databases. </title> <booktitle> In Proc. Conf. Very Large Databases, </booktitle> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7]. The optimization and execution of queries on tertiary-resident data in the Postgres database system is described in <ref> [15, 16] </ref>. In that architecture, a query on large tertiary relations is broken into smaller, independent subqueries on fragments of the relations. The system caches fragments in secondary storage and improves tape access efficiency by reordering the I/O requests resulting from the subqueries. <p> Our goal is to show how disk and main memory space affect the performance of these joins, and demonstrate how parallel I/O helps them save execution time as well as memory and disk space. The distinction between our line of work and that presented in <ref> [16] </ref> is that our join algorithms access data directly on tertiary devices, using available disk space both as a speed-matching buffer and as a cache. Our performance results are from an experimental implementation rather than a simulation. Whereas [16] considers different types of queries, our focus is on joins without indices. <p> The distinction between our line of work and that presented in <ref> [16] </ref> is that our join algorithms access data directly on tertiary devices, using available disk space both as a speed-matching buffer and as a cache. Our performance results are from an experimental implementation rather than a simulation. Whereas [16] considers different types of queries, our focus is on joins without indices. A study on tape I/O scheduling [9] presents an interesting and detailed model of the random access behavior of the same tape drive model we use in our studies.
Reference: [17] <author> L. Shapiro. </author> <title> Join processing in database systems with large main memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: Most studies consider main memory and secondary storage as the only forms of storage, but few studies include tertiary storage in the system model. Studies on disk-based joins typically employ a transfer-only cost model <ref> [2, 3, 5, 11, 17] </ref> where the number of pages transferred is the cost metric while the latency penalty of small I/O requests is disregarded. The number of multi-page I/O requests was the cost metric in [8], and a detailed cost model combining both cost models was developed in [7]. <p> Therefore, the total cost of the join (response time) is roughly the same as I/O cost, like in most other cost models for joins. Join method analyses typically do not consider query output costs since they are deemed the same for all methods <ref> [2, 5, 7, 8, 17] </ref>. In our study, however, it seems necessary to include the output costs in the cost model because the input cost may be affected by the output cost.
Reference: [18] <author> H. Suzuki et al. </author> <title> Storage hierarchy for video-on-demand systems. </title> <booktitle> In Proc. Storage and Retrieval for Image and Video Databases II, </booktitle> <pages> pages 198-207, </pages> <address> San Jose, CA, </address> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: While multimedia systems such as video-on-demand servers have successfully integrated tertiary storage into their storage hierarchies <ref> [6, 10, 18] </ref>, relational database systems still have little notion of incorporating tertiary storage as an integral part of the system. Joining two relations is one of the most common operations in a relational DBMS and one of the most costly if done naively.
Reference: [19] <author> J.-B. Yu and D. J. DeWitt. </author> <title> Query pre-execution and batching in Paradise: A two-pronged approach to the efficient processing of queries on tape-resident data sets. </title> <note> Document available at http://www.cs.wisc.edu/~ jiebing/tape.ps. 20 </note>
Reference-contexts: The system caches fragments in secondary storage and improves tape access efficiency by reordering the I/O requests resulting from the subqueries. A similar reordering of tape I/O requests is performed by the tertiary storage interface of the Paradise database system <ref> [19] </ref>. The system pre-executes queries, collects and 3 reorders the tape I/O references, and then re-executes the queries. As in the Postgres system, the ordering and batching of tape I/O requests yields improved tape data transfer efficiency.
References-found: 19

