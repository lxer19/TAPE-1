URL: http://www.cs.cmu.edu/~softagents/papers/kbcs96.ps.gz
Refering-URL: http://www.cs.cmu.edu/~softagents/publications_old.html
Root-URL: 
Email: pannu+@cs.cmu.edu katia@cs.cmu.edu  
Title: A Learning Personal Agent for Text Filtering and Notification  
Author: Anandeep S. Pannu Katia Sycara 
Keyword: Content Areas: software agents, information retrieval Word Count 4833  
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute School of Computer Science Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: [Armstrong et al.1995] <author> Robert Armstrong, Dayne Freitag, Thorsten Joachims, and Tom Mitchell. WebWatcher: </author> <title> A Learning Apprentice for the World Wide Web. </title> <booktitle> In Proceedings of AAAI Spring Symposium on Information Gathering from Heterogenous Distributed Environments, </booktitle> <year> 1995. </year>
Reference-contexts: This ensures that when the LPA starts interacting with the user it correctly classifies a large proportion of the conference announcements and requests for proposals. Learning Personal Agents have been used for the information filtering from the WWW [Lang1995], <ref> [Armstrong et al.1995] </ref>, [Pazzani et al.1995]. In case of the WebWatcher [Armstrong et al.1995] and the agent described in [Pazzani et al.1995] the agent tries to find an "interesting" link in a Web Page that has already been preselected by a user, which means that the probability of finding relevant links <p> Learning Personal Agents have been used for the information filtering from the WWW [Lang1995], <ref> [Armstrong et al.1995] </ref>, [Pazzani et al.1995]. In case of the WebWatcher [Armstrong et al.1995] and the agent described in [Pazzani et al.1995] the agent tries to find an "interesting" link in a Web Page that has already been preselected by a user, which means that the probability of finding relevant links is relatively high. <p> Hence we ruled them out. Dynamic Learning Filtering Models could easily incorporate evolving user preferences since they are constantly updated. User feedback is incorporated while carrying out the task, and filtering and learning take place simultaneously. The WebWatcher <ref> [Armstrong et al.1995] </ref> and the Learning Interface Agents [Maes and Kozierok1993] construct user preference models for filtering which are Dynamic Learning Filtering Models. The owner looks at a document and decides whether it is relevant or not, giving appropriate feedback to his/her Learning Personal agent.
Reference: [Foltz and Dumais1992] <author> P W Foltz and S T Dumais. </author> <title> Personalized Information delivery : An Analysis of Information Filtering Methods. </title> <journal> Communications of the Association for Computing Machinery, </journal> <month> December </month> <year> 1992. </year>
Reference-contexts: The granularity of feature size has been kept to word level to avoid the enormous task of developing linguistic knowledge needed for Natural Language Processing. The assumption is that there is some underlying or "latent" structure in the pattern of word usage across documents <ref> [Foltz and Dumais1992] </ref>. The need therefore is to come up with a method to estimate the latent structure. Thus words are used as the building blocks of the structures that result in the prediction of the relevance of text to a user.
Reference: [Lang1994] <author> Ken Lang. Newsweeder: </author> <title> An Adaptive Multi-User Text filter. Research Summary, </title> <institution> Carnegie Mellon University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: This is a simple technique that is hard to beat <ref> [Lang1994] </ref> and provides a well tested benchmark to which other learning models can be compared. This technique relies on the occurrence properties of the terms in document collections. <p> The results obtained by us are not completely disappointing however. Despite the small number of training examples used (357) compared to those reported in other literature (for instance <ref> [Lang1994] </ref> which used 20000 examples), we were able to get performance approaching 60% recall and a precision of 94%. For the Cascade 2 neural network growing algorithm trained on a 1000 articles, Lang [Lang1994] reported 25% correct classification. <p> the small number of training examples used (357) compared to those reported in other literature (for instance <ref> [Lang1994] </ref> which used 20000 examples), we were able to get performance approaching 60% recall and a precision of 94%. For the Cascade 2 neural network growing algorithm trained on a 1000 articles, Lang [Lang1994] reported 25% correct classification. In our case the Neural Network has approximately 3 times the performance achieved by the Cascade 2 algorithm. This is no doubt due to the fact that our text filtering task is more specialized than the task for [Lang1994]. <p> algorithm trained on a 1000 articles, Lang <ref> [Lang1994] </ref> reported 25% correct classification. In our case the Neural Network has approximately 3 times the performance achieved by the Cascade 2 algorithm. This is no doubt due to the fact that our text filtering task is more specialized than the task for [Lang1994]. The precision of increases with user feedback as is expected. 10 7 Conclusions and future work We described a Learning Personal Assistant that learns a model of the user's preferences in order to notify a user when relevant information becomes available.
Reference: [Lang1995] <author> K Lang. </author> <title> Learning to Filter Netnews. </title> <booktitle> In Proceedings of the Machine Learning Conference 1995, </booktitle> <year> 1995. </year>
Reference-contexts: In particular, our agent is learning profiles for notifying users about conference announcements and requests for proposals that match their research interests. In contrast to environments where the goal is to avoid information overload (eg., the Learning Interface Agent [Maes and Kozierok1993] and NewsWeeder <ref> [Lang1995] </ref>) the filtering task for our agent involves judging whether an article is relevant or irrelevant to the user based on the user profile, in an environment where the prior probability of encountering a relevant document is very low compared to the probability of encountering an irrelevant document. <p> This ensures that when the LPA starts interacting with the user it correctly classifies a large proportion of the conference announcements and requests for proposals. Learning Personal Agents have been used for the information filtering from the WWW <ref> [Lang1995] </ref>, [Armstrong et al.1995], [Pazzani et al.1995]. <p> For learning from text, the features we picked to be included in the vectors were the words occurring in a document. For the purposes of this paper a word is a sequence of letters, delimited by non-letters. This is similar to NewsWeeder's <ref> [Lang1995] </ref> approach of using tokens which are generalized words. The granularity of feature size has been kept to word level to avoid the enormous task of developing linguistic knowledge needed for Natural Language Processing.
Reference: [Maes and Kozierok1993] <editor> Pattie Maes and Robyn Kozierok. </editor> <booktitle> Learning Interface Agents. In Proceedings of the Eleventh National Conference on AI. </booktitle> <publisher> AAAI, AAAI Press/MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: In particular, our agent is learning profiles for notifying users about conference announcements and requests for proposals that match their research interests. In contrast to environments where the goal is to avoid information overload (eg., the Learning Interface Agent <ref> [Maes and Kozierok1993] </ref> and NewsWeeder [Lang1995]) the filtering task for our agent involves judging whether an article is relevant or irrelevant to the user based on the user profile, in an environment where the prior probability of encountering a relevant document is very low compared to the probability of encountering an <p> Hence we ruled them out. Dynamic Learning Filtering Models could easily incorporate evolving user preferences since they are constantly updated. User feedback is incorporated while carrying out the task, and filtering and learning take place simultaneously. The WebWatcher [Armstrong et al.1995] and the Learning Interface Agents <ref> [Maes and Kozierok1993] </ref> construct user preference models for filtering which are Dynamic Learning Filtering Models. The owner looks at a document and decides whether it is relevant or not, giving appropriate feedback to his/her Learning Personal agent.
Reference: [Pazzani et al.1995] <author> Michael Pazzani, Larry Nguyen, and Stefanus Mantik. </author> <title> Learning from Hotlists and Coldlists : Towards a WWW Information Filtering and Seeking Agent. </title> <booktitle> In Proceedings of the Seventh International IEEE conference on Tools with AI. </booktitle> <publisher> IEEE Computer Press, </publisher> <year> 1995. </year>
Reference-contexts: This ensures that when the LPA starts interacting with the user it correctly classifies a large proportion of the conference announcements and requests for proposals. Learning Personal Agents have been used for the information filtering from the WWW [Lang1995], [Armstrong et al.1995], <ref> [Pazzani et al.1995] </ref>. In case of the WebWatcher [Armstrong et al.1995] and the agent described in [Pazzani et al.1995] the agent tries to find an "interesting" link in a Web Page that has already been preselected by a user, which means that the probability of finding relevant links is relatively high. <p> Learning Personal Agents have been used for the information filtering from the WWW [Lang1995], [Armstrong et al.1995], <ref> [Pazzani et al.1995] </ref>. In case of the WebWatcher [Armstrong et al.1995] and the agent described in [Pazzani et al.1995] the agent tries to find an "interesting" link in a Web Page that has already been preselected by a user, which means that the probability of finding relevant links is relatively high.
Reference: [Salton1989] <author> G Salton. </author> <title> Automatic Text Processing. The Transformation, Analysis and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <month> 11 </month>
Reference-contexts: Statistical criteria and heuristics are used to decide which words contribute significantly. In the techniques described later in this paper we used the vector space information retrieval paradigm, where documents are represented as vectors <ref> [Salton1989] </ref>. Assume some dictionary vector ~ D where each element d i is a word. Each docu ment then has a vector representation ~ V , where element v i is the weight of the word d i for that document. <p> 4 Using an Information Retrieval Based Technique for Filtering Once we decided on a representation of the documents as described in Section 3, one of the techniques adopted for learning a user preference model was a standard well tested technique from Information Retrieval called term frequency-inverse document frequency weighting (tf-idf) <ref> [Salton1989] </ref>. This is a simple technique that is hard to beat [Lang1994] and provides a well tested benchmark to which other learning models can be compared. This technique relies on the occurrence properties of the terms in document collections.
References-found: 7

