URL: http://www.cs.ucsd.edu/~calder/papers/HPCA-96-PSA.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/papers.html
Root-URL: http://www.cs.ucsd.edu
Email: calder,grunwald@cs.colorado.edu  emer@vssad.hlo.dec.com  
Title: Predictive Sequential Associative Cache  
Author: Brad Calder Dirk Grunwald Joel Emer 
Address: Campus Box 430 Boulder, CO 80309-0430  77 Reed Road (HLO2-3/J3) Hudson, MA 01749  
Affiliation: Department of Computer Science, University of Colorado,  Digital Semiconductor,  
Abstract: This paper appeared in the 2nd International Symposium on High Performance Computer Architecture, San Jose, February 1996. Abstract In this paper, we propose a cache design that provides the same miss rate as a two-way set associative cache, but with a access time closer to a direct-mapped cache. As with other designs, a traditional direct-mapped cache is conceptually partitioned into multiple banks, and the blocks in each set are probed, or examined, sequentially. Other designs either probe the set in a fixed order or add extra delay in the access path for all accesses. We use prediction sources to guide the cache examination, reducing the amount of searching and thus the average access latency. A variety of accurate prediction sources are considered, with some being available in early pipeline stages. We feel that our design offers the same or better performance and is easier to implement than previous designs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, John Hennesy, and Mark Horowitz. </author> <title> Cache performance of operating systems and multiprogramming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6 </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Usually a least-recently-used mechanism is employed. A set-associative cache typically has a lower miss rate than a direct-mapped cache, but is more difficult to implement and increases the cache access time [5]. 2.1 Statically Ordered Cache Probes Agarwal et al <ref> [1] </ref> proposed the Hash-Rehash cache (HR-Cache) to reduce the miss rate of direct-mapped caches. Although the Hash-Rehash cache can be used to implement arbitrary associativity, we only consider a two-way set-associative cache, since that is the most cost-effective configuration.
Reference: [2] <author> Anant Agarwal and Steven D. Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct mapped caches. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 179-190. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: The next reference is to 001 10, which was just replaced. Index six is examined first, and index two is examined second. Again, a miss occurs. The contents of index six are moved to index two, and 001 10 is loaded into index six. Cache simulations by Agarwal <ref> [2] </ref>, and our own simulations, show that the Hash-Rehash cache has a higher miss rate than a two-way set-associative cache with LRU replacement. There are two obstacles that limit the performance or practicality of the HR-Cache: the higher miss rate and the need to exchange entire cache lines. <p> Furthermore, a special exchange operation could interfere with the pipelining of normal cache accesses, especially in multi-ported designs, and would introduce more bookkeeping for deferred write and fill operations. The Column-Associative Cache (CA-Cache) of Agarwal and Pudar <ref> [2] </ref> improved the miss rate and average access time of the HR-Cache, but still requires that entire cache blocks be exchanged. Like the HR-Cache, the CA-Cache, shown diagrammatically in Figure 1 (b), divides the cache into two banks. <p> We decided to compare the techniques using a timing model that separates the latency encountered by the pipeline and the time the cache is busy. Agarwal <ref> [2] </ref> used a simple timing model to demonstrate the performance of the CA-Cache. His model provides an average access time and can be used to compare all cache organizations that have the same cycle time. However, Agarwal's timing model did not distinguish between loads and stores. <p> The HR-Cache and CA-Cache exchange cache lines to improve the average access latency and we assume it takes T S cycles to swap two cache lines. Agarwal's timing model <ref> [2] </ref> combines the notion of latency and occupancy, and argued that T S should be a single cycle. We feel a larger value is more reasonable, particularly for large cache lines, such as 32 or 128 bytes. <p> Table 4: Conservative and Optimistic Latency For Different Programs cache in some situations. Agarwal <ref> [2] </ref> used the rehash bit to reduce the latency by initiating misses one cycle earlier. In our Optimistic timing model, the rehash bit has no effect on latency because misses are always initiated early, but rehash bits influence occupancy in all configurations.
Reference: [3] <author> Todd M. Austin, Dionisios N. Pnevmatikatos, and Gurindar S. Sohi. </author> <title> Skewed associativity enhances performance predictability. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 369-380. </pages> <publisher> IEEE, </publisher> <year> 1995. </year>
Reference-contexts: This is similar in flavour to the prediction method of Austin et al <ref> [3] </ref>. The RegNum model forms the prediction address by concatenating the register number and the lower five bits of the offset ( (b t 5)j ((offset 5)&0x1F) ). The Proc configuration extends RegNum using an exclusive or of the destination address from the previous procedure call.
Reference: [4] <author> J. H. Chang, H. Chao, and K. </author> <title> So. Cache design of a sub-micron CMOS System/370. </title> <booktitle> In 14th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 208-213. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Chang et al <ref> [4] </ref> proposed a novel organization for a multi-chip 32-way set-associative cache. Their study, and an earlier study by So and Rechtschaffen [12], found that most requests reference the most recently used blocks. Each set of cache blocks had associated most recently used (MRU) information. <p> On the next reference, block two is examined first, followed by block six. The MRU bit is updated to indicate that block six was more recently used. The design in [8] focused on large, secondary caches, and the lower cycle time seen by <ref> [4] </ref> did not apply. In the design by Kessler et al, the MRU bit must be fetched prior to accessing the cache contents to determine what cache line should be examined first since the effective address is used index the MRU table.
Reference: [5] <author> Mark Hill. </author> <title> A case for direct-mapped caches. </title> <journal> IEEE Computer, </journal> <volume> 21(12) </volume> <pages> 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: New entries may be loaded into either block, as determined by the block allocation policy. Usually a least-recently-used mechanism is employed. A set-associative cache typically has a lower miss rate than a direct-mapped cache, but is more difficult to implement and increases the cache access time <ref> [5] </ref>. 2.1 Statically Ordered Cache Probes Agarwal et al [1] proposed the Hash-Rehash cache (HR-Cache) to reduce the miss rate of direct-mapped caches. Although the Hash-Rehash cache can be used to implement arbitrary associativity, we only consider a two-way set-associative cache, since that is the most cost-effective configuration.
Reference: [6] <author> Norm Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 191-201. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Thus, even the Eff technique described below can be used with instruction caches. We examined an 8 KByte cache with 32-byte cache lines. We assume the cache uses a write-around or no-store-allocate write policy, since earlier work by Jouppi <ref> [6] </ref> found this to be more effective than a fetch-on-write or store-allocate policy. The study by Jouppi found an overall lower miss rate using write-around.
Reference: [7] <author> David R. Kaeli and Philip G. Emma. </author> <title> Branch history table prediction of moving target branches due to subroutine returns. </title> <booktitle> In 18th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 34-42. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: We included the stack depth to reduce interference between register usage in different procedures. We further improved this by including the address of the current procedure. Some of this information is already retained by many machines to implement a return-address stack <ref> [7] </ref>, a branch prediction mechanism used to predict procedure return addresses. We also tried separate steering bit tables for certain registers. Table Index 4. Instruction and Previous References. We also used the address of the instruction issuing the reference and variants of the previous cache reference.
Reference: [8] <author> R. R. Kessler, Richard Jooss, Alvin Lebeck, and Mark D. Hill. </author> <title> Inexpensive implementations of set-associativity. </title> <booktitle> In 16th Annual International Symposium on Computer Architecture, SIGARCH Newsletter. IEEE, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: This organization allowed most references (85%-95%) to complete in a single cycle, and reduced the cycle time of their multi-chip implementation by 30-35%. Kessler et al <ref> [8] </ref> proposed a similar organization. Rather than swap the cache locations like the HR-Cache and CA-Cache, each pair of cache blocks uses an MRU bit to indicate the most recently used block. When searching for data, the block indicated by the MRU bit is probed first. <p> The MRU bit is set to indicate that block two is now more recent than block six. On the next reference, block two is examined first, followed by block six. The MRU bit is updated to indicate that block six was more recently used. The design in <ref> [8] </ref> focused on large, secondary caches, and the lower cycle time seen by [4] did not apply. <p> When fetching a cache line entry, the effective address is used to index into the actual cache. Likewise, a prediction index is used to select a particular steering bit. As Kessler et al <ref> [8] </ref> indicated, the steering bits need to be accessed prior to the cache access. If we use the effective address to select a steering bit, this may lengthen the cache access time arguably, if the effective address were available earlier, cache accesses would be initiated at an earlier pipeline stage. <p> However, the Eff column demonstrates how to improve the MRU-Cache design of Kessler et al for the domains considered in <ref> [8] </ref>. The next most effective configuration is XOR-5-5, followed by the CA-Cache. The XOR-5-5 configuration requires an exclusive-or of the contents of the register and offset before the SBT is accessed; this may not be possible in some designs.
Reference: [9] <author> Scott McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: In this paper, we are primarily concerned with first-level data cache references, because data references are difficult to predict, and first level caches must be both fast and have low miss rates. Furthermore, instruction cache misses can be reduced using a number of software techniques <ref> [9, 10] </ref> and instruction references are usually very predictable. Thus, even the Eff technique described below can be used with instruction caches. We examined an 8 KByte cache with 32-byte cache lines.
Reference: [10] <author> Wen mei W. Hwu and Pohua P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> In 16th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 242-251. </pages> <publisher> ACM, ACM, </publisher> <year> 1989. </year>
Reference-contexts: In this paper, we are primarily concerned with first-level data cache references, because data references are difficult to predict, and first level caches must be both fast and have low miss rates. Furthermore, instruction cache misses can be reduced using a number of software techniques <ref> [9, 10] </ref> and instruction references are usually very predictable. Thus, even the Eff technique described below can be used with instruction caches. We examined an 8 KByte cache with 32-byte cache lines.
Reference: [11] <author> Andre Seznec. </author> <title> A case for two-way skewed-associative caches. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, SIGARCH Newsletter, </booktitle> <pages> pages 169-168. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: The Steering Bit Table can be small, and implemented on the processor, while the MRU Table, cache tags and data can be implemented off-chip. In the future, we hope to apply prediction sources to skewed-associative caches <ref> [11] </ref>. Acknowledgements Brad Calder was supported by an ARPA Fellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland. This work was funded in part by NSF grant No. ASC-9217394, NSF grant No.
Reference: [12] <author> Kimming So and Rudolph N. Rechtschaffen. </author> <title> Cache operations by MRU change. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(6) </volume> <pages> 700-709, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Chang et al [4] proposed a novel organization for a multi-chip 32-way set-associative cache. Their study, and an earlier study by So and Rechtschaffen <ref> [12] </ref>, found that most requests reference the most recently used blocks. Each set of cache blocks had associated most recently used (MRU) information. When accessing a cache set, the cache provided the block selected by the MRU information.
Reference: [13] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the SIG-PLAN'94 Conference on Programming Language Design and Implementation. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: We collected information from 26 C and Fortran programs. We instrumented the programs from the SPEC92 benchmark suite and other programs, including many from the Perfect Club. We used ATOM <ref> [13] </ref> to instrument the programs. Due to the structure of ATOM, we did not need to record traces and traced the full execution of each program.
Reference: [14] <author> Steven J. E. Wilton and Norman P. Jouppi. </author> <title> An enhanced access and cycle time model for on-chip caches. </title> <type> Report 93/5, </type> <institution> DEC Western Research Lab, </institution> <year> 1993. </year>
Reference-contexts: By contrast, in a direct-mapped cache, a fetch address can only access a single block, and the data from that block can be speculatively dispatched while the tags are compared, thus reducing the critical path length. The CACTI cache timing simulator <ref> [14] </ref> can be used to approximate the increased access time for a two-way set-associative cache.
References-found: 14

