URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P682.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: INCOMPLETE CHOLESKY FACTORIZATIONS WITH LIMITED MEMORY  
Phone: 60439  
Author: Chih-Jen Lin and Jorge J. More 
Note: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. De partment of Energy, under Contract W-31-109-Eng-38.  
Date: August 1997  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P682-0897  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. A. Ajiz and A. Jennings, </author> <title> A robust incomplete Cholesky-conjugate gradient algorithm, </title> <journal> Int. J. Num. Meth. Eng., </journal> <volume> 20 (1984), </volume> <pages> pp. 949-966. </pages>
Reference-contexts: Dickinson and Forsyth [11] reported that oe = 1 is generally an overestimate and that the shifted incomplete Cholesky factorization was preferable for elasticity analysis problems. Hladik, Reed, and Swoboda [23] suggested using a parameter ! 2 <ref> [0; 1] </ref> with a ii = a ii + !ja ij j; a jj = a jj + !ja ij j; and used a search procedure to determine an appropriate !. Carr [7] tested a variation of this approach with an incomplete LU factorization and a drop tolerance strategy. <p> These two codes are representative of codes that rely on drop tolerances. Other implementations of the incomplete Cholesky factorization include the Ajiz and Jennings <ref> [1] </ref> code (drop tolerances) and the Meschach [43] and SLAP [40] codes (fixed fill). The main aim in these computational experiments is to emphasize the difficulty of choosing appropriate drop tolerances while keeping memory requirements predictable.
Reference: [2] <author> B. M. Averick, R. G. Carter, J. J. Mor e, and G.-L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Preprint MCS-P153-0694, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: We also selected three matrices that required an excessive number of conjugate gradient during the solution of an optimization problem with a truncated Newton method [6]. Matrices jimack and nlmsurf are from the CUTE collection [5], while dgl2 is from the MINPACK-2 collection <ref> [2] </ref>. Table 4.1 describes the test set. In this table n is the order of the matrix and nnz is the number of nonzeros in the lower triangular part of A.
Reference: [3] <author> O. Axelsson, </author> <title> Iterative Solution Methods, </title> <publisher> Cambridge Univeristy Press, </publisher> <year> 1994. </year>
Reference-contexts: For additional information on incomplete Cholesky factorizations, see Axelsson <ref> [3] </ref> and Saad [36]. Section 3 extends the incomplete Cholesky factorization of Section 2 to indefinite matrices. Two main issues arise: scaling the matrix and modifying the matrix so that the factorization is possible. We use a symmetric scaling of the matrix and Manteuffel's [27] shifted approach. <p> This result is a direct consequence of the characterization of M-matrices as those matrices with nonpositive off-diagonal entries such that Ax &gt; 0 for some x &gt; 0. Axelsson <ref> [3, Section 6.1] </ref> has proofs of these results, as well as additional information on M-matrices.
Reference: [4] <author> M. Benzi, C. D. Meyer, and M. Tuma, </author> <title> A sparse approximate inverse precondi-tioner for the conjugate gradient method, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 17 (1996), </volume> <pages> pp. 1135-1149. </pages>
Reference-contexts: The first five matrices are the matrices used by Jones and Plassmann [25] to test their algorithm. We added bcsstk18, a large problem from the bcsstk set, and 1138bus, the hardest problem in the set of matrices used by Benzi, Meyer, and Tuma <ref> [4] </ref> to test their inverse preconditioner. We also selected three matrices that required an excessive number of conjugate gradient during the solution of an optimization problem with a truncated Newton method [6]. Matrices jimack and nlmsurf are from the CUTE collection [5], while dgl2 is from the MINPACK-2 collection [2].
Reference: [5] <author> I. Bongartz, A. R. Conn, N. I. M. Gould, and P. L. Toint, CUTE: </author> <title> Constrained and Unconstrained Testing Environment, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 21 (1995), </volume> <pages> pp. 123-160. </pages>
Reference-contexts: We also selected three matrices that required an excessive number of conjugate gradient during the solution of an optimization problem with a truncated Newton method [6]. Matrices jimack and nlmsurf are from the CUTE collection <ref> [5] </ref>, while dgl2 is from the MINPACK-2 collection [2]. Table 4.1 describes the test set. In this table n is the order of the matrix and nnz is the number of nonzeros in the lower triangular part of A.
Reference: [6] <author> A. Bouaricha, J. J. Mor e, and Z. Wu, </author> <title> Newton's method for large-scale optimization, </title> <type> Preprint MCS-P635-0197, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1997. </year>
Reference-contexts: In the implementation of a truncated Newton method proposed by Bouaricha, More and Wu <ref> [6] </ref>, the ellipsoidal trust region is transformed into a spherical trust region, and then the conjugate gradient method is applied to the transformed problem min bg T w + 1 o where bg = D T g; b B = D T BD 1 : Given an approximate solution w of <p> As suggested in <ref> [6] </ref>, we can use an incomplete Cholesky factorization to generate a scaling matrix D that clusters the eigenvalues of b B. This is a desirable goal because then the conjugate gradient method is able to solve (1.2) in a few iterations. <p> This is certainly the case if R = 0, and tends to happen for reasonable choices of S. If clustering occurs, then the choice in <ref> [6] </ref> of D = L T as the scaling matrix is reasonable. Unfortunately, it is not clear how to choose the sparsity pattern S so that L 1 BL T has clustered eigenvalues. <p> In an optimization application it seems reasonable to avoid implementations that choose S statically, independent of the numerical entries of B, for example, choosing S as the sparsity pattern of B. In the implementation of a truncated trust region Newton method in <ref> [6] </ref>, the precondi-tioner proposed by Jones and Plassmann [25] was chosen because this preconditioner has a number of advantages: The sparsity pattern S depends on the numerical entries of the matrix. The memory requirements of the incomplete Cholesky factorization are predictable. <p> We also note that the strategy of incrementing ff k by a constant is not likely to be efficient in general. An early version of Bouaricha, More, and Wu <ref> [6] </ref> used Algorithm 3.1 with ff 0 = ff S if min (a ii ) 0 and ff S = 1 2 k b Ak 1 . <p> We also selected three matrices that required an excessive number of conjugate gradient during the solution of an optimization problem with a truncated Newton method <ref> [6] </ref>. Matrices jimack and nlmsurf are from the CUTE collection [5], while dgl2 is from the MINPACK-2 collection [2]. Table 4.1 describes the test set. In this table n is the order of the matrix and nnz is the number of nonzeros in the lower triangular part of A. <p> The maximal number of conjugate gradient iterations allowed was n, the order of the matrix. The setting of oe = 10 3 is used in at least one large-scale Newton code <ref> [6] </ref> but is not typical of other codes, or in linear algebra applications. We will discuss how our results change when oe is chosen smaller. The computational experiments were done on a Sun UltraSPARC1-140 workstation with 128 MB RAM.
Reference: [7] <author> E. Carr, </author> <title> Preconditioning and performance issues for the solution of ill-conditioned three-dimensional structural analysis problems, </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1997. </year>
Reference-contexts: Hladik, Reed, and Swoboda [23] suggested using a parameter ! 2 [0; 1] with a ii = a ii + !ja ij j; a jj = a jj + !ja ij j; and used a search procedure to determine an appropriate !. Carr <ref> [7] </ref> tested a variation of this approach with an incomplete LU factorization and a drop tolerance strategy. He showed that this approach was competitive with the shifted incomplete Cholesky factorization on three-dimensional structure analysis problems. Algorithm 3.1 specifies our strategy in detail.
Reference: [8] <author> S. H. Cheng and N. J. Higham, </author> <title> A modified Cholesky algorithm based on a symmetric indefinite factorization, Numerical Analysis Report No. </title> <type> 289, </type> <institution> University of Manchester, </institution> <address> Manchester M13 9PL, England, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The main approaches are due to Gill, Murray, and Wright [17, Section 4.4.2.2] and Schnabel and Eskow [39]. Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham <ref> [8] </ref>, and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. <p> Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham <ref> [8] </ref>, and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. Section 4 presents the results of our computational experiments. We test an implementation icfm of the proposed incomplete Cholesky factorization on a set of ten problems.
Reference: [9] <author> A. R. Conn, N. I. M. Gould, and P. L. Toint, LANCELOT, </author> <title> Springer Series in Computational Mathematics, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The main approaches are due to Gill, Murray, and Wright [17, Section 4.4.2.2] and Schnabel and Eskow [39]. Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, <ref> [9, Section 3.3.8] </ref>, [16], [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. Section 4 presents the results of our computational experiments.
Reference: [10] <author> E. F. D'Azevedo, P. A. Forsyth, and W. P. Tang, </author> <title> Towards a cost effective ILU preconditioner with high level fill, </title> <journal> BIT, </journal> <volume> 31 (1992), </volume> <pages> pp. 442-463. </pages>
Reference-contexts: Guidelines for the use of these factorizations and descriptions of several implementations can be found in <ref> [10, 29, 44] </ref>. Drop-tolerance strategies have the advantage that they depend on the entries of A. In these strategies nonzeros are included in the incomplete factor if they are larger than some threshold parameter.
Reference: [11] <author> J. K. Dickinson and P. A. Forsyth, </author> <title> Preconditioned conjugate gradient methods for three-dimensional linear elasticity, </title> <journal> Int. J. Num. Meth. Eng., </journal> <volume> 37 (1994), </volume> <pages> pp. 2211-2234. </pages>
Reference-contexts: Dickinson and Forsyth <ref> [11] </ref> reported that oe = 1 is generally an overestimate and that the shifted incomplete Cholesky factorization was preferable for elasticity analysis problems.
Reference: [12] <author> I. S. Duff, R. Grimes, J. Lewis, and B. Poole, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 15 (1989), </volume> <pages> pp. 1-14. </pages> <note> Currently available in http://math.nist.gov/MatrixMarket. </note>
Reference-contexts: We set ff S = 10 3 in Algorithm 3.1, but we also experimented with ff S = 10 6 , with little change in our results. We selected ten problems for the test set. The first seven problems are from the Harwell-Boeing sparse matrix collection <ref> [12] </ref>. The first five matrices are the matrices used by Jones and Plassmann [25] to test their algorithm. We added bcsstk18, a large problem from the bcsstk set, and 1138bus, the hardest problem in the set of matrices used by Benzi, Meyer, and Tuma [4] to test their inverse preconditioner.
Reference: [13] <author> I. S. Duff and G. A. Meurant, </author> <title> The effect of ordering on preconditioned conjugate gradients, </title> <journal> BIT, </journal> <volume> 29 (1989), </volume> <pages> pp. 635-657. 22 </pages>
Reference-contexts: This is an important issue since the ordering of the matrix affects the fill in the matrix, and thus the incomplete Cholesky factorization. In particular, Duff and Meurant <ref> [13] </ref> and Eijkhout [14] have shown that the number of conjugate gradient iterations can double if the minimum degree ordering is used to reorder the matrix. <p> In particular, Figures 4.2 and 4.3 look similar when oe is smaller. The improvement is most noticeable for bcsstk09, the easiest problem in the test set. The decrease in computational time for an incomplete Cholesky factorization is not guaranteed. For example, the results of Duff and Meurant <ref> [13] </ref> comparing a level 1 factorization with a level 0 incomplete Cholesky factorization on grid problem with five-point and nine-point stencils showed that the extra work in the factorization and in the computation of the conjugate gradient iterates was greater than the work saved by the reduction (if any) 15 Table
Reference: [14] <author> V. Eijkhout, </author> <title> Analysis of parallel incomplete point factorizations, Linear Algebra and its Application, </title> <month> 154-156 </month> <year> (1991), </year> <pages> pp. 723-740. </pages>
Reference-contexts: This is an important issue since the ordering of the matrix affects the fill in the matrix, and thus the incomplete Cholesky factorization. In particular, Duff and Meurant [13] and Eijkhout <ref> [14] </ref> have shown that the number of conjugate gradient iterations can double if the minimum degree ordering is used to reorder the matrix.
Reference: [15] <author> A. Forsgren, P. E. Gill, and W. Murray, </author> <title> Computing modified Newton directions using a partial Cholesky factorization, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 16 (1995), </volume> <pages> pp. 139-150. </pages>
Reference-contexts: Modifications to the Cholesky factorization in the presence of indefiniteness have received considerable attention in the optimization literature. The main approaches are due to Gill, Murray, and Wright [17, Section 4.4.2.2] and Schnabel and Eskow [39]. Recent work in this area includes Forsgren, Gill, and Murray <ref> [15] </ref>, Cheng and Higham [8], and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. <p> The example above clearly shows that we need to modify the diagonal elements before we encounter a negative pivot. This observation has led to several proposed modifications to the Cholesky factorization of the form A + E, with E a diagonal matrix. See, for example, <ref> [17, 39, 38, 15, 32] </ref>. These approaches are applicable to general indefinite matrices but only if all the elements in the factorization are retained. Thus, the advantage of having predictable storage requirements is lost.
Reference: [16] <author> P. E. Gill, W. Murray, D. B. Poncel eon, and M. A. Saunders, </author> <title> Preconditioners for indefinite systems arising in optimization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 (1992), </volume> <pages> pp. 292-311. </pages>
Reference-contexts: Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], <ref> [16] </ref>, [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. Section 4 presents the results of our computational experiments.
Reference: [17] <author> P. E. Gill, W. Murray, and M. H. Wright, </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: We also study how the shift depends on the scaling of the matrix. Modifications to the Cholesky factorization in the presence of indefiniteness have received considerable attention in the optimization literature. The main approaches are due to Gill, Murray, and Wright <ref> [17, Section 4.4.2.2] </ref> and Schnabel and Eskow [39]. Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier [32]. <p> The example above clearly shows that we need to modify the diagonal elements before we encounter a negative pivot. This observation has led to several proposed modifications to the Cholesky factorization of the form A + E, with E a diagonal matrix. See, for example, <ref> [17, 39, 38, 15, 32] </ref>. These approaches are applicable to general indefinite matrices but only if all the elements in the factorization are retained. Thus, the advantage of having predictable storage requirements is lost.
Reference: [18] <author> K. Goebel, </author> <title> Getting more out of your new UltraSPARC TM machine, Sun Developer News, </title> <month> 1 </month> <year> (1996). </year>
Reference-contexts: The computational experiments were done on a Sun UltraSPARC1-140 workstation with 128 MB RAM. The incomplete Cholesky factorization and the preconditioned conjugate gradient method are written in FORTRAN and linked to MATLAB (version 5.0) drivers through C subroutines and cmex scripts. We follow the recommendations in <ref> [18] </ref> by using -fast, -xO5, -xdepend, -xchip=ultra, -xarch=v8plus, -xsafe=mem, as the compiler options. The results of our computational experiments are shown in Figures 4.1 to 4.3. We present the number of conjugate gradient iterations, the time required for the conjugate gradient iterations, and the total computational time.
Reference: [19] <author> I. Gustafsson, </author> <title> A class of first order factorization methods, </title> <journal> BIT, </journal> <volume> 18 (1978), </volume> <pages> pp. </pages> <month> 142-156. </month> <title> [20] , Modified incomplete Cholesky (MIC) methods, in Preconditioning Methods: Theory and Applications, </title> <editor> D. Evans, ed., Gordon and Breach, </editor> <year> 1983, </year> <pages> pp. </pages> <month> 265-293. </month> <title> [21] , A class of preconditioned conjugate gradient methods applied to the finite element equations, in Preconditioning Conjugate Gradient Methods, </title> <editor> O. Axelsson and L. Y. Koltilina, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1990, </year> <pages> pp. 44-57. </pages>
Reference-contexts: Many variations are possible. For example, we could define S so that L is a banded matrix with predetermined bandwidth. These strategies have predictable memory requirements but are independent of the entries of A because the dropped elements depend only on the structure of A. Gustafson <ref> [19] </ref> introduced a level k factorization where the sparsity pattern S k is defined by setting S 0 to be the sparsity pattern of A, and defining S k+1 = S k [ R k ; where R k is the sparsity pattern of L k L T k . <p> In particular, the product LU produced by ILUT is unlikely to be symmetric for a symmetric matrix A. There are several variations of the approaches that we have presented. In particular, in the modified incomplete Cholesky factorization of Gustafsson <ref> [19] </ref>, dropped elements are added to the diagonal entries of the column. With this modification Re = 0, where e is the vector of all ones. For additional information on modified incomplete Cholesky factorizations, see Gustafsson [20, 21], Hackbusch [22], and Saad [36].
Reference: [22] <author> W. Hackbusch, </author> <title> Iterative Solution of Large Sparse Systems of Equations, </title> <booktitle> Applied Mathematical Sciences 95, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: In particular, in the modified incomplete Cholesky factorization of Gustafsson [19], dropped elements are added to the diagonal entries of the column. With this modification Re = 0, where e is the vector of all ones. For additional information on modified incomplete Cholesky factorizations, see Gustafsson [20, 21], Hackbusch <ref> [22] </ref>, and Saad [36]. Other variations arise from the way that the matrix is scaled or from the method used to deal with breakdowns.
Reference: [23] <author> I. Hlad ik, M. B. Reed, and G. Swoboda, </author> <title> Robust preconditioners for linear elasticity FEM analysis, </title> <journal> Int. J. Num. Meth. Eng., </journal> <volume> 40 (1997), </volume> <pages> pp. 2109-2117. </pages>
Reference-contexts: Dickinson and Forsyth [11] reported that oe = 1 is generally an overestimate and that the shifted incomplete Cholesky factorization was preferable for elasticity analysis problems. Hladik, Reed, and Swoboda <ref> [23] </ref> suggested using a parameter ! 2 [0; 1] with a ii = a ii + !ja ij j; a jj = a jj + !ja ij j; and used a search procedure to determine an appropriate !.
Reference: [24] <author> A. Jennings and G. M. Malik, </author> <title> Partial elimination, </title> <journal> J. Inst. Maths. Appl., </journal> <volume> 20 (1977), </volume> <pages> pp. 307-316. </pages>
Reference-contexts: However, he did not recommend a procedure for determining a suitable ff. There have also been proposed modifications of the form A + E, with E a diagonal matrix. Jennings and Malik <ref> [24] </ref> proposed setting a ii = a ii + oeja (k) (k) (k) 1 ja ij j; (k) ij is dropped during the kth pivot step, and proved that if the original matrix A is positive definite, then this modification guarantees that the incomplete factorization succeeds for any oe &gt; 0.
Reference: [25] <author> M. T. Jones and P. E. Plassmann, </author> <title> An improved incomplete Cholesky factorization, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 21 (1995), </volume> <pages> pp. 5-17. </pages>
Reference-contexts: In the implementation of a truncated trust region Newton method in [6], the precondi-tioner proposed by Jones and Plassmann <ref> [25] </ref> was chosen because this preconditioner has a number of advantages: The sparsity pattern S depends on the numerical entries of the matrix. The memory requirements of the incomplete Cholesky factorization are predictable. <p> In Section 2 we contrast the proposed incomplete Cholesky factorization with other approaches, in particular, the fixed fill factorization of Meijerink and van der Vorst [28], the ILUT factorization of Saad [35, 36], the drop tolerance strategy of Munksgaard [31], and the factorization proposed by Jones and Plassmann <ref> [25] </ref>. For additional information on incomplete Cholesky factorizations, see Axelsson [3] and Saad [36]. Section 3 extends the incomplete Cholesky factorization of Section 2 to indefinite matrices. Two main issues arise: scaling the matrix and modifying the matrix so that the factorization is possible. <p> a lower triangular matrix L such that A = LL T + R; l ij = 0 if (i; j) =2 S; r ij = 0 if (i; j) 2 S: In this section we propose an incomplete Cholesky factorization that combines the best features of the Jones and Plassmann <ref> [25] </ref> factorization and the ILUT factorization of Saad [35, 36]. We also compare this approach with other approaches used to compute incomplete Cholesky factorizations. Fixed fill strategies fix the nonzero structure of the incomplete factor prior to the factorization. <p> If o is large, then L will have few nonzero elements but will also tend to be a poor preconditioner. The strategies described above have unpredictable memory requirements, or the factorization is independent of the entries of A. Jones and Plassmann <ref> [25] </ref> proposed an incomplete 4 Cholesky factorization that avoids these requirements. <p> Algorithm 2.2 outlines the incomplete Cholesky factorization with limited memory. An advantange of implementations of Algorithm 2.2 is that, unlike factorizations based on drop tolerances, they require no dynamic memory management. This advantage translates into superior performance. Our implementation follows the Jones and Plassmann <ref> [25] </ref> implementation. <p> Algorithm 3.1 specifies our strategy in detail. Note, in particular, that we scale the initial matrix by the l 2 norm of the columns of A; if A has zero columns we can just apply the algorithm to the submatrix with nonzero columns. Jones and Plassmann <ref> [25] </ref> used a similar approach for positive definite matrices but with p = 0 in Algorithm 3.1. In their approach the matrix A is scaled as in (3.1), and ff F is generated by starting with ff 0 = 0, and incrementing ff k by a constant (10 2 ). <p> We selected ten problems for the test set. The first seven problems are from the Harwell-Boeing sparse matrix collection [12]. The first five matrices are the matrices used by Jones and Plassmann <ref> [25] </ref> to test their algorithm. We added bcsstk18, a large problem from the bcsstk set, and 1138bus, the hardest problem in the set of matrices used by Benzi, Meyer, and Tuma [4] to test their inverse preconditioner. <p> In these figures we present results for p = 0; 2; 5; 10; the value p = 0 is of interest because this corresponds to the choice made by Jones and Plassmann <ref> [25] </ref>.
Reference: [26] <author> T. A. Manteuffel, </author> <title> Shifted incomplete Cholesky factorization, in Sparse Matrix Proceedings, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979, </year> <pages> pp. </pages> <month> 41-61. </month> <title> [27] , An incomplete factorization technique for positive definite linear systems, </title> <journal> Math. Comp., </journal> <volume> 34 (1980), </volume> <pages> pp. 307-327. 23 </pages>
Reference-contexts: Thus, the advantage of having predictable storage requirements is lost. There have been several proposed modifications to the incomplete Cholesky factorization that are applicable to general positive definite matrices. The shifted incomplete 7 Cholesky factorization of Manteuffel <ref> [26, 27] </ref> for the scaled matrix b A = D 1=2 AD 1=2 ; D = diag (a ii ); (3.1) requires the computation of a suitable ff 0 for which the incomplete Cholesky factorization of b A + ffI succeeds.
Reference: [28] <author> J. A. Meijerink and H. A. Van der Vorst, </author> <title> An iterative solution method for linear equations systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp., </journal> <volume> 31 (1977), </volume> <pages> pp. </pages> <month> 148-162. </month> <title> [29] , Guidelines for the usage of incomplete decompositions in solving sets of linear equations as they occur in practical problems, </title> <journal> J. Comput. Phys., </journal> <volume> 44 (1981), </volume> <pages> pp. 134-155. </pages>
Reference-contexts: In Section 2 we contrast the proposed incomplete Cholesky factorization with other approaches, in particular, the fixed fill factorization of Meijerink and van der Vorst <ref> [28] </ref>, the ILUT factorization of Saad [35, 36], the drop tolerance strategy of Munksgaard [31], and the factorization proposed by Jones and Plassmann [25]. For additional information on incomplete Cholesky factorizations, see Axelsson [3] and Saad [36]. Section 3 extends the incomplete Cholesky factorization of Section 2 to indefinite matrices. <p> We also compare this approach with other approaches used to compute incomplete Cholesky factorizations. Fixed fill strategies fix the nonzero structure of the incomplete factor prior to the factorization. Meijerink and van der Vorst <ref> [28] </ref> considered two choices of S, the standard setting of S to the sparsity pattern of A, and a setting that allowed more fill. Many variations are possible. For example, we could define S so that L is a banded matrix with predetermined bandwidth. <p> This result shows, in particular, that any strictly diagonally dominant matrix is an H-matrix. Maijerink and van der Vorst <ref> [28] </ref> proved that if A is an M-matrix, then the incomplete LU (Cholesky) factorization exists for any predetermined sparsity pattern S, and Manteuffel [27] extended this result to H-matrices with positive diagonal elements.
Reference: [30] <author> J. J. Mor e and D. C. Sorensen, </author> <title> Computing a trust region step, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 4 (1983), </volume> <pages> pp. 553-572. </pages>
Reference-contexts: These algorithms obtain the global minimum of (1.1) even if B is indefinite. See, for example, the algorithm of More and Sorensen <ref> [30] </ref>. This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S.
Reference: [31] <author> N. Munksgaard, </author> <title> Solving sparse symmetric sets of linear equations by preconditioned conjugate gradients, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 6 (1980), </volume> <pages> pp. 206-219. </pages>
Reference-contexts: In Section 2 we contrast the proposed incomplete Cholesky factorization with other approaches, in particular, the fixed fill factorization of Meijerink and van der Vorst [28], the ILUT factorization of Saad [35, 36], the drop tolerance strategy of Munksgaard <ref> [31] </ref>, and the factorization proposed by Jones and Plassmann [25]. For additional information on incomplete Cholesky factorizations, see Axelsson [3] and Saad [36]. Section 3 extends the incomplete Cholesky factorization of Section 2 to indefinite matrices. <p> In Section 5 we compare the icfm implementation with two incomplete Cholesky factorizations that rely on drop tolerances to reduce fill. We use the ma31 code of Munksgaard <ref> [31] </ref> and the cholinc command of MATLAB (version 5.0). <p> Drop-tolerance strategies have the advantage that they depend on the entries of A. In these strategies nonzeros are included in the incomplete factor if they are larger than some threshold parameter. For example, Munksgaard <ref> [31] </ref> drops a (k) ij during the kth step if ja ij j o a ii a jj ; where o is the drop tolerance. A disadvantage of drop tolerance strategies is that their memory requirements depend in an unpredictable manner on the drop tolerance. <p> Hence, if p increases, then the computing time for the incomplete Cholesky factorization may actually decrease. This situation happens with some of our test cases. 16 5 Software Evaluation We evaluate the incomplete Cholesky factorization of Algorithm 3.1 by comparing our results with those obtained with the ma31 code <ref> [31] </ref> in the Harwell subroutine library (release 10) and the cholinc command of MATLAB (version 5.0). These two codes are representative of codes that rely on drop tolerances.
Reference: [32] <author> A. Neumaier, </author> <title> On satisfying second-order optimality conditions using modified cholesky factorizations, </title> <type> Technical Report, </type> <institution> Universitat Wien, Vienna, Austria, </institution> <year> 1997. </year>
Reference-contexts: The main approaches are due to Gill, Murray, and Wright [17, Section 4.4.2.2] and Schnabel and Eskow [39]. Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier <ref> [32] </ref>. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], [38], and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. Section 4 presents the results of our computational experiments. <p> The example above clearly shows that we need to modify the diagonal elements before we encounter a negative pivot. This observation has led to several proposed modifications to the Cholesky factorization of the form A + E, with E a diagonal matrix. See, for example, <ref> [17, 39, 38, 15, 32] </ref>. These approaches are applicable to general indefinite matrices but only if all the elements in the factorization are retained. Thus, the advantage of having predictable storage requirements is lost.
Reference: [33] <author> J. M. Ortega, </author> <title> Introduction to Parallel and Vector Solution of Linear Systems, </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: This factorization is in place with the jth column of L overwriting the jth column of A. Note that diagonal elements are updated as the factorization proceeds. For an extensive discussion of other forms of the Cholesky factorization for dense matrices, see Ortega <ref> [33] </ref>. Algorithm 2.2 outlines the incomplete Cholesky factorization with limited memory. An advantange of implementations of Algorithm 2.2 is that, unlike factorizations based on drop tolerances, they require no dynamic memory management. This advantage translates into superior performance. Our implementation follows the Jones and Plassmann [25] implementation.
Reference: [34] <author> F. Rendel and H. Wolkowicz, </author> <title> A semidefinite framework for trust region subprob-lems with applications to large scale minimization, </title> <journal> Math. Programming, </journal> <volume> 77 (1997), </volume> <pages> pp. 273-299. </pages>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 In many large-scale problems, time and memory constraints do not permit a direct fac-torization, and we must then use an iterative scheme. Rendel and Wolkowicz <ref> [34] </ref>, Sorensen [41], and Santos and Sorensen [37] have recently proposed iterative algorithms that require only matrix-vector operations to determine the global minimum of (1.1), but these algorithms do not use preconditioning and thus are unlikely to perform well on difficult problems.
Reference: [35] <author> Y. Saad, ILUT: </author> <title> A dual threshold incomplete LU factorization, </title> <journal> Numer. Linear Algebra Appl., </journal> <volume> 4 (1994), </volume> <pages> pp. </pages> <month> 387-402. </month> <title> [36] , Iterative Methods for Sparse Linear Systems, </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: In Section 2 we contrast the proposed incomplete Cholesky factorization with other approaches, in particular, the fixed fill factorization of Meijerink and van der Vorst [28], the ILUT factorization of Saad <ref> [35, 36] </ref>, the drop tolerance strategy of Munksgaard [31], and the factorization proposed by Jones and Plassmann [25]. For additional information on incomplete Cholesky factorizations, see Axelsson [3] and Saad [36]. Section 3 extends the incomplete Cholesky factorization of Section 2 to indefinite matrices. <p> = LL T + R; l ij = 0 if (i; j) =2 S; r ij = 0 if (i; j) 2 S: In this section we propose an incomplete Cholesky factorization that combines the best features of the Jones and Plassmann [25] factorization and the ILUT factorization of Saad <ref> [35, 36] </ref>. We also compare this approach with other approaches used to compute incomplete Cholesky factorizations. Fixed fill strategies fix the nonzero structure of the incomplete factor prior to the factorization. <p> Another approach that has predictable storage requirements and depends on the entries of A is the ILUT factorization of Saad <ref> [35, 36] </ref>. The ILUT factorization of a general matrix A depends on a memory parameter p and on a drop tolerance o .
Reference: [37] <author> S. A. Santos and D. C. Sorensen, </author> <title> A new matrix-free algorithm for the large-scale trust-region subproblem, </title> <type> Technical Report TR95-20, </type> <institution> Rice University, Houston, Texas, </institution> <year> 1995. </year>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 In many large-scale problems, time and memory constraints do not permit a direct fac-torization, and we must then use an iterative scheme. Rendel and Wolkowicz [34], Sorensen [41], and Santos and Sorensen <ref> [37] </ref> have recently proposed iterative algorithms that require only matrix-vector operations to determine the global minimum of (1.1), but these algorithms do not use preconditioning and thus are unlikely to perform well on difficult problems.
Reference: [38] <author> T. Schlick, </author> <title> Modified Cholesky factorizations for sparse preconditioners, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 424-445. </pages>
Reference-contexts: Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], <ref> [38] </ref>, and [8]) but only if all the elements in the factorization are retained. Thus, these approaches lose the advantage of having predictable storage requirements. Section 4 presents the results of our computational experiments. <p> The example above clearly shows that we need to modify the diagonal elements before we encounter a negative pivot. This observation has led to several proposed modifications to the Cholesky factorization of the form A + E, with E a diagonal matrix. See, for example, <ref> [17, 39, 38, 15, 32] </ref>. These approaches are applicable to general indefinite matrices but only if all the elements in the factorization are retained. Thus, the advantage of having predictable storage requirements is lost.
Reference: [39] <author> R. B. Schnabel and E. Eskow, </author> <title> A new modified Cholesky factorization, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 1136-1158. </pages>
Reference-contexts: We also study how the shift depends on the scaling of the matrix. Modifications to the Cholesky factorization in the presence of indefiniteness have received considerable attention in the optimization literature. The main approaches are due to Gill, Murray, and Wright [17, Section 4.4.2.2] and Schnabel and Eskow <ref> [39] </ref>. Recent work in this area includes Forsgren, Gill, and Murray [15], Cheng and Higham [8], and Neumaier [32]. These approaches can be extended to sparse problems (see, for example, [9, Section 3.3.8], [16], [38], and [8]) but only if all the elements in the factorization are retained. <p> The example above clearly shows that we need to modify the diagonal elements before we encounter a negative pivot. This observation has led to several proposed modifications to the Cholesky factorization of the form A + E, with E a diagonal matrix. See, for example, <ref> [17, 39, 38, 15, 32] </ref>. These approaches are applicable to general indefinite matrices but only if all the elements in the factorization are retained. Thus, the advantage of having predictable storage requirements is lost.
Reference: [40] <author> M. K. Seager, </author> <title> A SLAP for the Masses, </title> <type> Technical Report UCRL-100267, </type> <institution> Lawrence Livermore National Laboratory, Livermore, California, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: These two codes are representative of codes that rely on drop tolerances. Other implementations of the incomplete Cholesky factorization include the Ajiz and Jennings [1] code (drop tolerances) and the Meschach [43] and SLAP <ref> [40] </ref> codes (fixed fill). The main aim in these computational experiments is to emphasize the difficulty of choosing appropriate drop tolerances while keeping memory requirements predictable.
Reference: [41] <author> D. C. Sorensen, </author> <title> Minimization of a large scale quadratic function subject to a spherical constraint, </title> <journal> SIAM J. Optimization, </journal> <volume> 7 (1997), </volume> <pages> pp. 141-161. 24 </pages>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 In many large-scale problems, time and memory constraints do not permit a direct fac-torization, and we must then use an iterative scheme. Rendel and Wolkowicz [34], Sorensen <ref> [41] </ref>, and Santos and Sorensen [37] have recently proposed iterative algorithms that require only matrix-vector operations to determine the global minimum of (1.1), but these algorithms do not use preconditioning and thus are unlikely to perform well on difficult problems.
Reference: [42] <author> T. Steihaug, </author> <title> The conjugate gradient method and trust regions in large scale optimiza-tion, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 626-637. </pages>
Reference-contexts: An interesting approach, based on the work of Steihaug <ref> [42] </ref>, is to use a preconditioned conjugate gradient method, with suitable modifications that take into account the trust region constraint and the possible indefiniteness of B to determine an approximate solution of (1.1).
Reference: [43] <author> D. E. Stewart and Z. Leyk, </author> <title> Meschach : Matrix computations in C, </title> <booktitle> vol. 32 of Proceedings of the Center of Mathematics and Its Application, </booktitle> <institution> Austrian National University, </institution> <year> 1994. </year>
Reference-contexts: These two codes are representative of codes that rely on drop tolerances. Other implementations of the incomplete Cholesky factorization include the Ajiz and Jennings [1] code (drop tolerances) and the Meschach <ref> [43] </ref> and SLAP [40] codes (fixed fill). The main aim in these computational experiments is to emphasize the difficulty of choosing appropriate drop tolerances while keeping memory requirements predictable.
Reference: [44] <author> D. P. Young, R. Melvin, F. T. Johnson, J. E. Bussoletti, and S. S. Samant, </author> <title> Application of sparse matrix solvers as effective preconditioners, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 10 (1989), </volume> <pages> pp. 1186-1199. </pages>
Reference-contexts: Guidelines for the use of these factorizations and descriptions of several implementations can be found in <ref> [10, 29, 44] </ref>. Drop-tolerance strategies have the advantage that they depend on the entries of A. In these strategies nonzeros are included in the incomplete factor if they are larger than some threshold parameter.
References-found: 39

