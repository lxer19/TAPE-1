URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-010.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu, nemo@cis.ufl.edu  
Title: A Fast and Low Overhead Distributed Priority Lock  
Author: Theodore Johnson and Richard Newman-Wolfe 
Address: Gainesville, Fl 32611-2024  
Affiliation: Dept. of CIS, University of Florida  
Abstract: Distributed synchronization is necessary to coordinate the diverse activities of a distributed system. Priority synchronization is needed for real time systems, or to improve the performance of critical tasks. We present a distributed priority lock that uses Li and Hudak's path compression methods to achieve a theoretical O(log n) messages per critical section request, where n is the number of processors. In addition, our algorithm requires only O(log n) bits of storage per processor, by making use of distributed lists. We present performance results to show that the expected message complexity of the algorithm is indeed O(log n) per critical section request. The low storage and overhead requirements of the algorithm make it scalable and practical for implementation. In addition to its use in synchronization, our algorithm has applications to distributed shared virtual memory consistency with novel check-in/check-out semantics.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Birman, A. Schiper, and P. Stephenson, </author> <title> Lightweight causal and atomic group multi-cast, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9 (1991), </volume> <pages> pp. 272-314. </pages>
Reference-contexts: In our example, the last message from A to B tells B that it is in the waiting ring, and the first message from C to B is the token. Considerable work has been done on implementing causal communications <ref> [1] </ref>, but this work requires that all messages are broadcast, which in general requires O (n) messages. The messages that need to be processed in order involve waiting a processor's participation in waiting ring maintenance.
Reference: [2] <author> O. Carvalho and G. Roucairol, </author> <title> On mutual exclusion in computer networks, </title> <journal> Comm. of the ACM, </journal> <volume> 26 (1983), </volume> <pages> pp. 146-147. </pages>
Reference-contexts: Lamport [10] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [16] and Carvalho and Roucairol <ref> [2] </ref> improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [17] introduces the idea of quorum consensus for distributed synchronization.
Reference: [3] <author> K. Chandy and L. Lamport, </author> <title> Distributed snapshots: Determining global states of distributed systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3 (1985), </volume> <pages> pp. 63-75. </pages>
Reference-contexts: While global time does not exist in an asynchronous distributed system, we can view the events in the system as being totally ordered using Lamport timestamps [10], and view a point in time as being a consistent cut <ref> [3] </ref>. We note that all processors that are not requesting the token lie on a path that leads a processor that either holds or is requesting the token. This property can be seen by induction. We assume the property holds initially (and this is required for correctness).
Reference: [4] <author> T. Craig, </author> <title> Queuing spin lock alternatives to support timing predictability, </title> <type> tech. rep., </type> <institution> University of Washington, </institution> <year> 1993. </year> <month> 16 </month>
Reference-contexts: However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc [13], Craig <ref> [4] </ref>, and Johnson and Harathi [6]. Our priority synchronization algorithm uses the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block process externally instead of internally.
Reference: [5] <author> A. Goscinski, </author> <title> Two algorithms for mutual exclusion in real-time distributed computer systems, </title> <journal> The Journal of Parallel and Distributed Computing, </journal> <volume> 9 (1990), </volume> <pages> pp. 77-82. </pages>
Reference-contexts: Li and Hudak show that their path compression algorithm requires O (n + K log q) messages if only q of the n processors use the page. Some work has been done to develop prioritized critical section algorithms. Goscinski <ref> [5] </ref> has proposed a fully distributed priority synchronization algorithm. However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request.
Reference: [6] <author> K. Harathi and T. Johnson, </author> <title> A priority synchronization algorithm for multiprocessors, </title> <type> Tech. Rep. </type> <institution> tr93.005, UF, </institution> <year> 1991. </year> <note> available at ftp.cis.ufl.edu:cis/tech-reports. </note>
Reference-contexts: However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc [13], Craig [4], and Johnson and Harathi <ref> [6] </ref>. Our priority synchronization algorithm uses the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block process externally instead of internally.
Reference: [7] <author> M. Hill, J. Larus, S. Reinhardt, and D. Wood, </author> <title> Cooperative shared memory: Software and hardware for scalable multiprocessors, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11 (1993), </volume> <pages> pp. 300-318. </pages>
Reference-contexts: Previous dynamic path compression algorithms require O (n) storage. In DSVM algorithms, the owner of a page releases it very quickly, so little blocking occurs and the large potential storage overhead is not considered a problem. New DSVM algorithms use check-in/check-out semantics <ref> [7] </ref> and blocking might become significant. Furthermore, the techniques described in this paper can be used to support DSVM efficiently with novel sharing semantics.
Reference: [8] <author> D. James, A. Laundrie, S. Gjessing, and G. Sohi, </author> <title> Scalable coherent interface, </title> <booktitle> Computer, 23 (1990), </booktitle> <pages> pp. 74-77. </pages>
Reference-contexts: To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block process externally instead of internally. Some work has been done on distributed lists, primarily in the context of directory-based cache coherence algorithms. For example, the Scalable Coherent Interface (SCI) <ref> [8] </ref> uses a distributed queue to chain together all of the processors that are requesting access to a memory block. The algorithm is greatly simplified because the pointer to the head of the list is stored in a standard place (the home memory block).
Reference: [9] <author> A. Kumar, </author> <title> Hierarchical quorum consensus: A new algorithm for managing replicated data, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40 (1991), </volume> <pages> pp. 994-1004. </pages>
Reference-contexts: Maekawa [12] presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar <ref> [9] </ref> presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak [11] present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system.
Reference: [10] <author> L. Lamport, </author> <title> Time, clocks, and the ordering of events in a distributed system, </title> <journal> Communications of the ACM, </journal> <volume> 21 (1978), </volume> <pages> pp. 558-564. </pages>
Reference-contexts: The low space and message passing overhead make it scalable and practical for imple-mentation. Considerable attention has been paid to the problem of distributed synchronization. Lamport <ref> [10] </ref> proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. <p> We loosely refer to events as occurring at a point in `time'. While global time does not exist in an asynchronous distributed system, we can view the events in the system as being totally ordered using Lamport timestamps <ref> [10] </ref>, and view a point in time as being a consistent cut [3]. We note that all processors that are not requesting the token lie on a path that leads a processor that either holds or is requesting the token. This property can be seen by induction.
Reference: [11] <author> K. Li and P. Hudak, </author> <title> Memory coherence in shared virtual memory systems, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7 (1989), </volume> <pages> pp. 321-359. 17 </pages>
Reference-contexts: In this paper, we present a distributed priority synchronization algorithm that requires O (log n) bits of storage per processor (the O (log n) bits are required to store the names of O (1) processors). The algorithm uses the path compression technique of Li and Hudak <ref> [11] </ref> to obtain a theoretical O (log n) messages per request. Our performance results show that O (log n) messages are required fl We acknowledge the support of USRA grant #5555-19 and NSF grant DMS-9223088 1 in practice. <p> Kumar [9] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm. Li and Hudak <ref> [11] </ref> present a distributed synchronization algorithm to enforce coherence in a distributed shared virtual memory (DSVM) system. In DSVM, a page of memory in a processor is treated as a cached version of a globally shared memory page.
Reference: [12] <author> M. Maekawa, </author> <title> A sqrt(n) algorithm for mutual exlcusion in decentralized systems, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 3 (1985), </volume> <pages> pp. 145-159. </pages>
Reference-contexts: The number of votes that are required to obtain the token can be reduced by observing that the only requirement for mutual exclusion is that any pair of processors require a vote from the same processor. Maekawa <ref> [12] </ref> presents an algorithm that requires O ( p n) messages per request and O ( p n log n) space per processor. Kumar [9] presents the hierarchical quorum consensus protocol, which requires O (n :63 ) votes for consensus, but is more fault tolerant than Maekawa's algorithm.
Reference: [13] <author> E. Markatos and T. LeBlanc, </author> <title> Multiprocessor synchronization primitives with priorities, </title> <type> tech. rep., </type> <institution> University of Rochester, </institution> <year> 1991. </year>
Reference-contexts: Goscinski [5] has proposed a fully distributed priority synchronization algorithm. However, this algorithm requires O (n log n) storage per processor and O (n) messages per critical section request. Recent work on multiprocessor priority synchronization algorithms has focused on contention-free algorithms, with algorithms proposed by Markatos and LeBlanc <ref> [13] </ref>, Craig [4], and Johnson and Harathi [6]. Our priority synchronization algorithm uses the path compression technique of Li and Hudak to achieve low message passing overhead. To avoid the O (n) storage cost of blocking, the algorithm uses distributed lists to block process externally instead of internally.
Reference: [14] <author> J. Mellor-Crummey and M. Scott, </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors, </title> <journal> ACM Trans. Computer Systems, </journal> <volume> 9 (1991), </volume> <pages> pp. 21-65. </pages>
Reference-contexts: We note that Li and Hudak found that their path compression algorithm had far superior performance to algorithms that required fixed site managers. A shared memory synchronization algorithm that is quite similar in nature to the SCI algorithm is the contention-free lock of Mellor-Crummey and Scott <ref> [14] </ref>. The recent contention-free priority locks are based on the Mellor-Crummey and Scott lock.
Reference: [15] <author> K. Raymond, </author> <title> A tree-based algorithm for distributed mutual exclusion, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7 (1989), </volume> <pages> pp. 61-77. </pages>
Reference-contexts: However, the blocking that the algorithm requires incurs a O (n) space overhead, to store the identities of the blocked requests. Raymond <ref> [15] </ref> has proposed a simple synchronization algorithm that can be configured to require O (log n) storage per processor and O (log n) messages per critical section request. The algorithm organizes the participating processors in a fixed tree.
Reference: [16] <author> G. Ricart and A. Agrawala, </author> <title> An optimal algorithm for mutual exclusion in computer networks, </title> <journal> Comm. of the ACM, </journal> <volume> 24 (1981), </volume> <pages> pp. 9-17. </pages>
Reference-contexts: Lamport [10] proposes a timestamp-based distributed synchronization algorithm. A processor broadcasts its request for the token to all of the other processors, which reply with a permission. A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala <ref> [16] </ref> and Carvalho and Roucairol [2] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas [17] introduces the idea of quorum consensus for distributed synchronization.
Reference: [17] <author> R. H. Thomas, </author> <title> A majority consensus approach to concurrency control for multiple copy databases, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4 (1979), </volume> <pages> pp. 180-209. </pages>
Reference-contexts: A processor implicitly receives the token when it receives permissions from all other processors. Ricart and Agrawala [16] and Carvalho and Roucairol [2] improve on Lamport's algorithm by reducing the message passing overhead. However, all of these algorithms require O (n) messages per request. Thomas <ref> [17] </ref> introduces the idea of quorum consensus for distributed synchronization. When a processor requests the token, it sends a vote request to all of the other processors in the system. A processor will vote for the critical section entry of at most one processor at a time.
Reference: [18] <author> T. Woo and R. Newman-Wolfe, </author> <title> Huffman trees as a basis for a dynamic mutual exclusion algorithm for distributed systems, </title> <booktitle> in Proceedings of the 12th IEEE International Conference on Distributed Computing Systems, </booktitle> <year> 1992, </year> <pages> pp. 126-133. 18 19 </pages>
Reference-contexts: The algorithm organizes the participating processors in a fixed tree. The execution of the algorithm is similar to that of the Li and Hudak algorithm. Woo and Newman-Wolfe <ref> [18] </ref> use a fixed tree based on a Huffman code. Because the tree is fixed, however, it does not adapt to the pattern of requests in the system. Often, only a small population of the processors make requests for the token.
References-found: 18

