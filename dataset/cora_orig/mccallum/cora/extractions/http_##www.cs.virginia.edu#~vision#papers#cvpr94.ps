URL: http://www.cs.virginia.edu/~vision/papers/cvpr94.ps
Refering-URL: http://www.cs.virginia.edu/~vision/papers/
Root-URL: http://www.cs.virginia.edu
Email: taylor@virginia.edu, olson@virginia.edu, wnm@virginia.edu  
Title: Accurate Vergence Control in Complex Scenes  
Author: John R. Taylor, Thomas J. Olson, and Worthy N. Martin 
Keyword: Categories: Active and real-time vision, Vision-guided robotics  
Address: Charlottesville, VA 22903  
Affiliation: Department of Computer Science The University of Virginia  
Abstract: In binocular visual systems, vergence is the process of directing the gaze so that the optical axes intersect at a surface point. Correlation-based methods of disparity analysis provide fast estimates of the vergence error. Unfortunately, most correlation techniques do not provide mechanisms to determine which image locations contributed to a given correlation peak. The result is that large correlation peaks may have contributions from image areas not relevant to the vergence task. This paper presents a vergence system that applies a cepstral filter to multiscale images obtained from a dominant-eye binocular sensor. As used by this system, the cepstral filter has two main advantages: it enhances targets through narrow-band signal suppression, and it supports a back-projection operation to determine the image locations associated with particular peaks. The use of multiscale images allows the system to have both high resolution for precision in the final vergence and a large field of view for a wide range of initial camera orientations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. L. Abbott and N. Ahuja, </author> <title> Surface Reconstruction by Dynamic Integration of Focus, Camera Vergence, and Stereo, </title> <booktitle> Proceedings of the Second International Conference on Computer Vision, </booktitle> <pages> pp. 532-543, </pages> <year> 1988. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate [6], or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms <ref> [1, 14, 15, 30] </ref>. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target). <p> Any method that computes depth can be used to estimate the vergence error. In humans, ver-gence can be driven by accommodation (focus) or binocular disparity [13]. Both of these cues have been used for robot vergence control <ref> [8, 1] </ref>. Most researchers rely primarily on disparity, however, since error estimates based on focus tend to be unreliable at intermediate and larger distances. <p> Vergence: Krotkov et al. [21] used a verging camera system in range estimation, but did not attempt to fixate a specific scene point. Ahuja and Abbott <ref> [1] </ref> integrated vergence, focus, and ste Focal Points Optical Axes Vergence Angle Fixation Point Baseline Right Image Plane Epipolar Line 8 reo in a surface reconstruction system. Their system verged on surface points by using a combination of focus, zoom, and binocular disparity.
Reference: [2] <author> J. Aloimonos, I. Weiss, and A. Bandopadhay, </author> <title> Active vision, </title> <booktitle> Proceedings of the 1st International Conference on Computer Vision, </booktitle> <pages> pages 35-54, </pages> <year> 1987. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate [6], or purposive <ref> [2] </ref> vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms [1, 14, 15, 30]. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. <p> Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target). It can be shown that fixation simplifies many early vision problems <ref> [2, 32] </ref> and establishes a useful coordinate system for intermediate and high-level vision [6]. Formally, fixation occurs when the intersection of the optical axes remains positioned on a surface point, called the fixation point.
Reference: [3] <author> P. Anandan, </author> <title> A Unified Perspective on Computational Techniques for the Measurement of Visual Motion, </title> <booktitle> Proceedings of the First International Conference on Computer Vision, </booktitle> <pages> pp. 219-230, </pages> <year> 1987. </year>
Reference-contexts: Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution. <p> Due to the effects of the damping windows, we consider 20 only horizontal disparities in the range <ref> [-3, 3] </ref> to be small enough to appear in the next finer level. (Theoretically, disparities in the range of -16 to 16 could appear in the next finer level.) If a confirming peak does exist, then at least some of the features that contributed to the original peak at level i
Reference: [4] <author> N. Ayache and B. Faverjon, </author> <title> Efficient registration of stereo images by matching graph descriptions of edge segments, </title> <journal> International Journal of Computer Vision, </journal> <pages> pp. 107-131, </pages> <year> 1987. </year>
Reference-contexts: The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments <ref> [26, 4] </ref>, and edge points [17]. In many systems, initial matches are made using epipolar constraints. A feature point in one image will be compared to feature points along the corresponding epipolar line in the other image.
Reference: [5] <author> R. </author> <title> Bajcsy, Passive Perception vs. Active Perception, </title> <booktitle> Proceedings IEEE Workshop on Computer Vision, </booktitle> <address> Ann Arbor, </address> <year> 1986. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active <ref> [5] </ref>, animate [6], or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms [1, 14, 15, 30]. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks.
Reference: [6] <author> D. H. Ballard, </author> <title> Reference Frames For Animate Vision, </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence, AAAI, </booktitle> <year> 1989. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate <ref> [6] </ref>, or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms [1, 14, 15, 30]. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. <p> The parameters are typically adjusted to maintain fixation on an object of interest (the target). It can be shown that fixation simplifies many early vision problems [2, 32] and establishes a useful coordinate system for intermediate and high-level vision <ref> [6] </ref>. Formally, fixation occurs when the intersection of the optical axes remains positioned on a surface point, called the fixation point. Fixation in general is a complex problem involving various control parameters and sources of information [15, 11].
Reference: [7] <author> S. T. Barnard and W. B. Thompson, </author> <title> Disparity Analysis of Images, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. PAMI-2, no. 4, </volume> <pages> pp. 333-340, </pages> <month> July </month> <year> 1980. </year>
Reference-contexts: The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators <ref> [7] </ref> such as the Moravec interest operator [27], linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints. A feature point in one image will be compared to feature points along the corresponding epipolar line in the other image.
Reference: [8] <author> S. T. Barnard and M. A. Fischler, </author> <title> Computational stereo, </title> <journal> Computing Surveys, </journal> <volume> vol. 14, no. 4, </volume> <pages> pp. 553-572, </pages> <month> De-cember </month> <year> 1982. </year>
Reference-contexts: Any method that computes depth can be used to estimate the vergence error. In humans, ver-gence can be driven by accommodation (focus) or binocular disparity [13]. Both of these cues have been used for robot vergence control <ref> [8, 1] </ref>. Most researchers rely primarily on disparity, however, since error estimates based on focus tend to be unreliable at intermediate and larger distances. <p> Binocular Disparity: Over the past two decades there has been a lot of work on the estimation 6 of binocular disparity and on the stereo correspondence problem in general. Dhond and Aggarwal [16] and Barnard and Fischler <ref> [8] </ref> present thorough reviews of these techniques. All correspondence techniques include preprocessing, matching, and disparity determination and can be grouped into feature-based and area-based techniques. Feature-based techniques: Feature-based methods extract features from the binocular input.
Reference: [9] <author> B. P. Bogert, M. J. R. Healy, and J. W. Tukey, </author> <title> The quefrency alanysis of time series for echoes: Cepstrum, </title> <editor> pseudo-autocovariance, cross-cepstrum, and saphe cracking, </editor> <booktitle> Proceedings of the Symposium on Time Series Analysis, </booktitle> <pages> pp. 209-243, </pages> <address> New York, </address> <year> 1963. </year>
Reference-contexts: The (power) cepstrum is the inverse Fourier transform of the log of the power spectrum. Bogert et al. <ref> [9] </ref> first used cepstral filtering to examine signals containing echoes. Yeshurun and Schwartz [33] used two-dimensional cepstral filtering to determine the disparity between image patches and showed that cepstral filtering produces correlation peaks with higher signal-to-noise ratios than those produced by cross or auto-correlation.
Reference: [10] <author> R. A. Brooks, </author> <title> A Robust Layered Control System for a Mobile Robot, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <pages> pp. 14-23, </pages> <month> April </month> <year> 1986. </year>
Reference: [11] <author> C. M. Brown, </author> <title> Gaze Controls Cooperating through Prediction, </title> <journal> Image and Vision Computing, </journal> <volume> 8(1) </volume> <pages> 10-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Formally, fixation occurs when the intersection of the optical axes remains positioned on a surface point, called the fixation point. Fixation in general is a complex problem involving various control parameters and sources of information <ref> [15, 11] </ref>. In this paper we assume that the agent has two cameras that can pan independently but are mounted on a common tilt platform, and that each camera provides high resolution only in the central region (or fovea) of the field of view.
Reference: [12] <author> P. J. Burt, E. H. Adelson, </author> <title> The Laplacian Pyramid as a Compact Image Code, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 31, no. 4, </volume> <month> April, </month> <year> 1983. </year>
Reference-contexts: It also allows the system to use the highest resolution available in the input images. The system windows the input to the cepstrum with Hamming functions in order to prevent corruption of the disparity computations by artifacts at the image borders. 3.1 The Scale Space A Gaussian pyramid <ref> [12] </ref> is a sequence of versions of an image at successively lower resolution and sampling density. From a uniform resolution image, a Gaussian pyramid can be formed by successively low-pass filtering and subsampling the image.
Reference: [13] <author> R. H. S. Carpenter, </author> <title> Movements of the eyes, </title> <address> London, Pion, </address> <year> 1977. </year>
Reference-contexts: We call this angle the vergence error. With an estimate of the vergence error, the process can construct a corrective pan command. Any method that computes depth can be used to estimate the vergence error. In humans, ver-gence can be driven by accommodation (focus) or binocular disparity <ref> [13] </ref>. Both of these cues have been used for robot vergence control [8, 1]. Most researchers rely primarily on disparity, however, since error estimates based on focus tend to be unreliable at intermediate and larger distances.
Reference: [14] <author> J. Clark and N. Ferrier, </author> <title> Modal control of an attentive visual system, </title> <booktitle> Proceedings of the Second International Conference on Computer Vision, </booktitle> <year> 1988. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate [6], or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms <ref> [1, 14, 15, 30] </ref>. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target).
Reference: [15] <author> D. Coombs and C. Brown, </author> <title> Real-time Smooth Pursuit Tracking for a Moving Binocular Robot, </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 23-28, </pages> <address> Champagne-Urbana, IL, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate [6], or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms <ref> [1, 14, 15, 30] </ref>. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target). <p> Formally, fixation occurs when the intersection of the optical axes remains positioned on a surface point, called the fixation point. Fixation in general is a complex problem involving various control parameters and sources of information <ref> [15, 11] </ref>. In this paper we assume that the agent has two cameras that can pan independently but are mounted on a common tilt platform, and that each camera provides high resolution only in the central region (or fovea) of the field of view. <p> Olson and Coombs [30] created a real-time vergence system that used cepstral filtering [33] to estimate the vergence error indicated by the stereo image pair. In a velocity control loop, the cameras were panned to minimize the computed vergence error. Finally, Coombs and Brown <ref> [15] </ref> used the vergence system described in [30] in conjunction with a disparity filter to track moving targets with the cameras of a moving robot. 3. Accurate Vergence Control The vergence algorithm described in this paper is an extension of that described in [30].
Reference: [16] <author> U. R. Dhond and J. K. Aggarwal, </author> <title> Structure from Stereo - A Review, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 19, no. 6, </volume> <month> November/December </month> <year> 1989. </year>
Reference-contexts: Binocular Disparity: Over the past two decades there has been a lot of work on the estimation 6 of binocular disparity and on the stereo correspondence problem in general. Dhond and Aggarwal <ref> [16] </ref> and Barnard and Fischler [8] present thorough reviews of these techniques. All correspondence techniques include preprocessing, matching, and disparity determination and can be grouped into feature-based and area-based techniques. Feature-based techniques: Feature-based methods extract features from the binocular input.
Reference: [17] <author> W. E. L. </author> <title> Grimson, A computer implementation of a theory of human stereo vision, </title> <journal> Phil. Transactions of Royal Society London, </journal> <volume> vol B292, </volume> <pages> pp. 217-253, </pages> <year> 1981. </year> <month> 32 </month>
Reference-contexts: The extracted features are matched using local similarity, and false matches are eliminated by enforcing global criteria. The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels <ref> [17] </ref>, zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints. <p> been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels <ref> [17] </ref>, zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints. A feature point in one image will be compared to feature points along the corresponding epipolar line in the other image. <p> Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [18] <author> H. J. Hannah, </author> <title> SRIs baseline stereo system, </title> <booktitle> Proceedings of the DARPA Image Understanding Workshop, </booktitle> <address> Miami Beach, FL, </address> <pages> pp. 149-155, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The epipolar line for points along the center scan line of one camera correspond to the center scan line of the other camera. (See figure 2.) Area-based techniques: As opposed to matching localized features, area-based methods attempt to match entire regions. Moravec [27] and Hannah <ref> [18] </ref> selected neighborhoods (regions) whose center pixel was identified as interesting by the Moravec interest operator. The regions were then correlated to determine similarity. Matthies [25] used the sum-of-square differences (SSD) to determine the disparities in band-pass filtered images for a planetary rover. <p> Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [19] <author> W. Hoff and N. Ahuja, </author> <title> Surfaces from stereo: Integrating feature matching, disparity estimation, and contour detection, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. PAMI-11, no. 2, </volume> <pages> pp. 121-136, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [20] <author> A. D. Jepson and M. Jenkin, </author> <title> The Fast Computation of Disparity from Phase Differences, </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 398-403, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [21] <author> E. Krotkov, K. Henriksen, and R. Kories, </author> <title> Stereo Ranging with Verging Cameras, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. PAMI-12, no. 12, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution. Vergence: Krotkov et al. <ref> [21] </ref> used a verging camera system in range estimation, but did not attempt to fixate a specific scene point. Ahuja and Abbott [1] integrated vergence, focus, and ste Focal Points Optical Axes Vergence Angle Fixation Point Baseline Right Image Plane Epipolar Line 8 reo in a surface reconstruction system.
Reference: [22] <author> C. D. Kuglin and D. C. Hines, </author> <title> The Phase Correlation Image Alignment Method, </title> <booktitle> Proceedings of the IEEE International Conference on Cybernetics and Society, </booktitle> <pages> pp. 163-165, </pages> <year> 1975. </year>
Reference-contexts: Okutomi and 7 Kanade [29] determined precise distance estimates by using the sums of SSDs (SSSD-in-inverse-distance) computed from multiple binocular image pairs taken from cameras with differing base-lines. Kuglin and Hines <ref> [22] </ref> used phase-correlation to find the global disparity between pairs of images for registration. Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches.
Reference: [23] <author> S. Mallat, </author> <title> Zero-Crossings of a Wavelet Transform, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 37, no. 4, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: The extracted features are matched using local similarity, and false matches are eliminated by enforcing global criteria. The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform <ref> [23] </ref>, feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints.
Reference: [24] <author> D. Marr and T. Poggio, </author> <title> A Computational Theory of Human Stereo Vision, </title> <journal> Proceedings of Royal Society London, </journal> <volume> vol. B204, </volume> <pages> pp. 301-328, </pages> <year> 1979. </year>
Reference-contexts: Feature-based techniques: Feature-based methods extract features from the binocular input. The extracted features are matched using local similarity, and false matches are eliminated by enforcing global criteria. The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels <ref> [24] </ref> or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints. <p> Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [25] <author> L. Matthies, </author> <title> Stereo Vision for Planetary Rovers: Stochastic Modeling to Near Real-Time Implementation, </title> <journal> International Journal of Computer Vision, </journal> <volume> 8:1, </volume> <pages> pp. 71-91, </pages> <year> 1992. </year>
Reference-contexts: Moravec [27] and Hannah [18] selected neighborhoods (regions) whose center pixel was identified as interesting by the Moravec interest operator. The regions were then correlated to determine similarity. Matthies <ref> [25] </ref> used the sum-of-square differences (SSD) to determine the disparities in band-pass filtered images for a planetary rover. Okutomi and 7 Kanade [29] determined precise distance estimates by using the sums of SSDs (SSSD-in-inverse-distance) computed from multiple binocular image pairs taken from cameras with differing base-lines.
Reference: [26] <author> G. Medioni and R. Nevatia, </author> <title> Segment-based stereo matching, Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 31, </volume> <pages> pp. 2-18, </pages> <year> 1985. </year>
Reference-contexts: The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator [27], linear edge segments <ref> [26, 4] </ref>, and edge points [17]. In many systems, initial matches are made using epipolar constraints. A feature point in one image will be compared to feature points along the corresponding epipolar line in the other image.
Reference: [27] <author> H. P. Moravec, </author> <title> Towards automatic visual obstacle avoidance, </title> <booktitle> Proceedings of the 5th International Joint Conference on Artificial Intelligence, </booktitle> <address> p. 584, </address> <year> 1977. </year>
Reference-contexts: The features that have been used in feature-based methods include: zero-crossings resulting from the application of difference-of-Gaussian (DOG) kernels [24] or Lapla-cian-of-Gaussian (LOG) kernels [17], zero-crossings extracted from the wavelet transform [23], feature points extracted with interest operators [7] such as the Moravec interest operator <ref> [27] </ref>, linear edge segments [26, 4], and edge points [17]. In many systems, initial matches are made using epipolar constraints. A feature point in one image will be compared to feature points along the corresponding epipolar line in the other image. <p> The epipolar line for points along the center scan line of one camera correspond to the center scan line of the other camera. (See figure 2.) Area-based techniques: As opposed to matching localized features, area-based methods attempt to match entire regions. Moravec <ref> [27] </ref> and Hannah [18] selected neighborhoods (regions) whose center pixel was identified as interesting by the Moravec interest operator. The regions were then correlated to determine similarity. Matthies [25] used the sum-of-square differences (SSD) to determine the disparities in band-pass filtered images for a planetary rover. <p> Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [28] <author> H. K. Nishihara, </author> <title> Practical real-time imaging stereo matcher, </title> <journal> Optical Engineering, </journal> <volume> vol. 23, no 5, </volume> <pages> pp. 536-545, </pages> <month> September/October </month> <year> 1984. </year>
Reference-contexts: Finally, Yeshurun and Swartz [33] used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates <ref> [3, 17, 18, 19, 20, 24, 27, 28] </ref>. The coarser scales allow the systems to handle large disparities effectively by limiting the number of number of possible matches while the finer scales provide higher resolution.
Reference: [29] <author> M. Okutomi, T. Kanade, </author> <title> A Multiple-Baseline Stereo, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 15, no. 4, </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: The regions were then correlated to determine similarity. Matthies [25] used the sum-of-square differences (SSD) to determine the disparities in band-pass filtered images for a planetary rover. Okutomi and 7 Kanade <ref> [29] </ref> determined precise distance estimates by using the sums of SSDs (SSSD-in-inverse-distance) computed from multiple binocular image pairs taken from cameras with differing base-lines. Kuglin and Hines [22] used phase-correlation to find the global disparity between pairs of images for registration.
Reference: [30] <author> T. J. Olson and D. J. Coombs, </author> <title> Real-Time Vergence Control for Binocular Robots, </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 67-89, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: 1. Introduction Recent interest in what is variously called active [5], animate [6], or purposive [2] vision has led a number of researchers to consider the computational advantages of anthropomorphic binocular camera platforms <ref> [1, 14, 15, 30] </ref>. Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target). <p> Ahuja and Abbott [1] integrated vergence, focus, and ste Focal Points Optical Axes Vergence Angle Fixation Point Baseline Right Image Plane Epipolar Line 8 reo in a surface reconstruction system. Their system verged on surface points by using a combination of focus, zoom, and binocular disparity. Olson and Coombs <ref> [30] </ref> created a real-time vergence system that used cepstral filtering [33] to estimate the vergence error indicated by the stereo image pair. In a velocity control loop, the cameras were panned to minimize the computed vergence error. Finally, Coombs and Brown [15] used the vergence system described in [30] in conjunction <p> and Coombs <ref> [30] </ref> created a real-time vergence system that used cepstral filtering [33] to estimate the vergence error indicated by the stereo image pair. In a velocity control loop, the cameras were panned to minimize the computed vergence error. Finally, Coombs and Brown [15] used the vergence system described in [30] in conjunction with a disparity filter to track moving targets with the cameras of a moving robot. 3. Accurate Vergence Control The vergence algorithm described in this paper is an extension of that described in [30]. <p> Finally, Coombs and Brown [15] used the vergence system described in <ref> [30] </ref> in conjunction with a disparity filter to track moving targets with the cameras of a moving robot. 3. Accurate Vergence Control The vergence algorithm described in this paper is an extension of that described in [30]. The principal failing of the previous system was its inability to deal with scenes containing objects at a variety of disparities. Since it sought to minimize the disparity over the whole scene, it tended to verge on the background rather than on the desired fixation target. <p> Hereafter, the levels of the scale space will be referred to as indicated in figure 3b. Starting at the finest level, the levels of the truncated pyramid will be numbered from 0 to the number of levels minus one. 3.2 Cepstral Filtering Our system uses the cepstral filter <ref> [33, 30] </ref> to determine the disparity between the features in a binocular image pair. The (power) cepstrum is the inverse Fourier transform of the log of the power spectrum. Bogert et al. [9] first used cepstral filtering to examine signals containing echoes. <p> Cepstral filtering performs relatively well in the presence of repeating patterns, because taking the log of the power spectrum has the effect of reducing the contribution of narrow-band signals (repetitive patterns and smooth blobs) while leaving broad-band components relatively unaltered <ref> [30] </ref>. The cepstrum is computed as follows. A pair of images of size h x w are first windowed with damping functions (two-dimensional Hamming functions) to prevent the image borders from corrupting the computations. <p> Subpixel precision for the disparity measurements can be achieved by interpolating between the peak pixel and the larger of its two horizontal neighbors as in <ref> [30] </ref>. <p> A second peak corresponding to the disparity (9, 1) also appears on the right side of the central peak. [Note: in the working system one of the original binocular images is shifted vertically (as described in <ref> [30] </ref>) in order to differentiate between positive and negative disparities in the cepstral filtered image. <p> Thus, adjusting the vergence angle so that the highest correlation peak appears at a disparity of zero (as in <ref> [30] </ref>) does not guarantee that the disparity between the features of the target in the binocular image pairs is zero. Our system uses a number of strategies to determine which peak should be used as the basis for the vergence error estimate. <p> In order to do this we must first examine in detail what the cepstrum actually computes. As shown in <ref> [30] </ref>, the cepstrum can be regarded as the auto-correlation of prefiltered images. Without the intermediate step of taking the logarithm of the Fourier transform, cepstral filtering would simply compute the auto-correlation of the concatenated image. <p> The far right hand side is the Fourier transform of the auto-correlation of the filtered image. Therefore, taking the logarithm can also be viewed as applying an adaptive, non-linear prefilter that suppresses narrow-band signals before auto-correlating <ref> [30] </ref>. The modulation transfer function of the prefilter, the cepstral-equivalent prefilter (CEP), is: The CEP is the basis for our second method of verifying peaks in the cepstrum. <p> To provide a baseline for comparison purposes, many of the experiments were also performed with the algorithm of Olson and Coombs <ref> [30] </ref> with and without windowing (using Hamming functions). 4.1 Disparity accuracy The first experiment was designed to test the accuracy of the disparity measurements. It consisted of panning the non-dominant camera over a scene and plotting the measured disparity versus pan angle. The disparity measurements were interpolated for subpixel accuracy. <p> Figures 7b, 7c, and 7d are plots of the horizontal disparity corresponding to the highest correlation peak versus pan angle at level 3 without 22 windowing (as in <ref> [30] </ref>), at level 3 with windowing, and in the scale space with windowing, respectively. It is important to note that the full vergence algorithm as described in Section 3 was not used for the scale space data. <p> Assuming that the optical axis of each camera passes through the center pixel (255, 255) of the full-sized (512 x 512) images, then the optical axes intersect at the surface point if this disparity is zero. Smaller absolute disparities correspond to higher accuracies. The vergence algorithm of <ref> [30] </ref> computed the vergence error by converting the disparity corresponding to the highest correlation peak to a pan angle. Their system only used one spatial scale corresponding in resolution and sampling density to our level 3. <p> Figures 9b and 9c show the resulting zero disparity filtered [31] images after vergence of a system using only level 3 (as in <ref> [30] </ref>) with windowing and our multiscale system, respectively. The level 3 system appears to have verged incorrectly on the white spray can, while the multiscale system appears to have verged correctly 25 on the target. In the remainder of this section, we focus on the multiscale system.
Reference: [31] <author> T. J. Olson and R. J. Lockwood, </author> <title> Fixation-based filtering, </title> <booktitle> Proceedings of the SPIE Intelligent Robots and Computer Vision XI Conference, </booktitle> <pages> pp. 685-696, </pages> <year> 1992. </year>
Reference-contexts: The errors in pixels are computed for the plots in figure 7 over the specified ranges in degrees. 24 (ZDF) <ref> [31] </ref>. The ZDF produces an image in which the features at a disparity of zero appear sharp, and all others are blurred in proportion to their disparity. <p> A baseline of three inches was used, the non-dominant camera was initially at a pan angle of 0 degrees (the optical axes were initially parallel), and the distance between the target and the dominant camera was approximately four feet. Figures 9b and 9c show the resulting zero disparity filtered <ref> [31] </ref> images after vergence of a system using only level 3 (as in [30]) with windowing and our multiscale system, respectively. The level 3 system appears to have verged incorrectly on the white spray can, while the multiscale system appears to have verged correctly 25 on the target.
Reference: [32] <author> T. J. Olson, </author> <title> Stereopsis for Verging System, </title> <booktitle> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 55-60, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Given appropriate gaze control algorithms, these systems allow agents to alter the imaging system parameters to facilitate the current tasks. The parameters are typically adjusted to maintain fixation on an object of interest (the target). It can be shown that fixation simplifies many early vision problems <ref> [2, 32] </ref> and establishes a useful coordinate system for intermediate and high-level vision [6]. Formally, fixation occurs when the intersection of the optical axes remains positioned on a surface point, called the fixation point.
Reference: [33] <author> Y. Yeshurun and E. Schwartz, </author> <title> Cepstral Filtering on a Columnar Image Architecture: A Fast Algorithm for Binocular Stereo Segmentation, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 11, no. 7, </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Okutomi and 7 Kanade [29] determined precise distance estimates by using the sums of SSDs (SSSD-in-inverse-distance) computed from multiple binocular image pairs taken from cameras with differing base-lines. Kuglin and Hines [22] used phase-correlation to find the global disparity between pairs of images for registration. Finally, Yeshurun and Swartz <ref> [33] </ref> used cepstral filtering to determine the disparity between binocular image patches. Scale Spaces: Many binocular disparity estimation techniques employ some form of a hierarchical or coarse-to-fine control strategy to refine the disparity estimates [3, 17, 18, 19, 20, 24, 27, 28]. <p> Their system verged on surface points by using a combination of focus, zoom, and binocular disparity. Olson and Coombs [30] created a real-time vergence system that used cepstral filtering <ref> [33] </ref> to estimate the vergence error indicated by the stereo image pair. In a velocity control loop, the cameras were panned to minimize the computed vergence error. <p> Hereafter, the levels of the scale space will be referred to as indicated in figure 3b. Starting at the finest level, the levels of the truncated pyramid will be numbered from 0 to the number of levels minus one. 3.2 Cepstral Filtering Our system uses the cepstral filter <ref> [33, 30] </ref> to determine the disparity between the features in a binocular image pair. The (power) cepstrum is the inverse Fourier transform of the log of the power spectrum. Bogert et al. [9] first used cepstral filtering to examine signals containing echoes. <p> The (power) cepstrum is the inverse Fourier transform of the log of the power spectrum. Bogert et al. [9] first used cepstral filtering to examine signals containing echoes. Yeshurun and Schwartz <ref> [33] </ref> used two-dimensional cepstral filtering to determine the disparity between image patches and showed that cepstral filtering produces correlation peaks with higher signal-to-noise ratios than those produced by cross or auto-correlation. They also showed that cepstral filtering performs well in the presence of noise and other image degradations.
References-found: 33

