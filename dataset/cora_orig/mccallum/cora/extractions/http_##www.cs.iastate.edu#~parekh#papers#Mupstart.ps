URL: http://www.cs.iastate.edu/~parekh/papers/Mupstart.ps
Refering-URL: http://www.cs.iastate.edu/~parekh/resume.html
Root-URL: 
Email: fparekhjyangjhonavarg@cs.iastate.edu  
Title: MUpstart A Constructive Neural Network Learning Algorithm for Multi-Category Pattern Classification  
Author: Rajesh Parekh, Jihoon Yang Vasant Honavar 
Address: Ames IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science Iowa State University  
Abstract: Constructive learning algorithms offer an approach for dynamically constructing near-minimal neural network architectures for pattern classification tasks. Several such algorithms proposed in the literature are shown to converge to zero classification errors on finite non-contradictory datasets. However, these algorithms are restricted to two-category pattern classification and (in most cases) they require the input patterns to have binary (or bipolar) valued attributes only. We present a provably correct extension of the Upstart algorithm to handle multiple output classes and real-valued pattern attributes. Results of experiments with several artificial and real-world datasets demonstrate the feasibility of this approach in practical pattern classification tasks and also suggest several interesting directions for future research. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Burgess. </author> <title> A constructive algorithm that converges for real-valued input patterns. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(1):5966, </volume> <year> 1994. </year>
Reference-contexts: A number of constructive learning algorithms for 2-category pattern classification have been proposed in the literature Tower , Pyramid [5], Tiling [7], Upstart [3], and Perceptron Cascade <ref> [1] </ref>. <p> However, extensions of the constructive learning algorithms to handle patterns with real-valued attributes have only been studied for the Upstart [12] and the Perceptron Cascade <ref> [1] </ref> algorithms. We present MUpstart, an extension of the Upstart algorithm. MUpstart is a provably correct constructive learning algorithm that handles multiple output classes, real-valued attributes, and facilitates both independent and WTA training of the output neurons. <p> Experiments have demonstrated the feasibility of this algorithm on practical pattern classification tasks. The convergence proof for MUpstart can be easily adapted to establish the convergence of a multi-category extension of the Perceptron Cascade algorithm <ref> [1, 8, 9] </ref>.
Reference: [2] <author> C.-H. Chen, R. Parekh, J. Yang, K. Balakrishnan, and V. Honavar. </author> <title> Analysis of decision boundaries generated by constructive neural network learning algorithms. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 628635, </pages> <year> 1995. </year>
Reference-contexts: The interested reader is referred to <ref> [2] </ref> for an analysis (in geometrical terms) of the decision boundaries generated by some of these constructive learning algorithms.
Reference: [3] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 2:198209, </address> <year> 1990. </year>
Reference-contexts: A number of constructive learning algorithms for 2-category pattern classification have been proposed in the literature Tower , Pyramid [5], Tiling [7], Upstart <ref> [3] </ref>, and Perceptron Cascade [1]. <p> MUpstart is a provably correct constructive learning algorithm that handles multiple output classes, real-valued attributes, and facilitates both independent and WTA training of the output neurons. Preliminary experiments on several artificial and real-world datasets demonstrate the practical applicability of this algorithm. 2. The MUpstart Algorithm The 2-category Upstart algorithm <ref> [3] </ref> constructs a binary tree of threshold neurons. A simple extension of this idea to deal with M output categories would be to construct M independent binary trees (one for each output class). This approach fails to exploit the inter-relationships that may exist between the different outputs.
Reference: [4] <author> M. Frean. </author> <title> A thermal perceptron learning rule. </title> <booktitle> Neural Computation, </booktitle> <address> 4:946957, </address> <year> 1992. </year>
Reference-contexts: However when S is not linearly separable, the Perceptron algorithm behaves poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule (e.g., the Pocket algorithm with ratchet modification [5], the Thermal perceptron algorithm <ref> [4] </ref>, and the Barycentric correction procedure [10]) are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable.
Reference: [5] <author> S. Gallant. </author> <title> Perceptron based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2):179191, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: However when S is not linearly separable, the Perceptron algorithm behaves poorly (i.e., the classification accuracy on the training set can fluctuate wildly from iteration to iteration). Several extensions to the perceptron weight update rule (e.g., the Pocket algorithm with ratchet modification <ref> [5] </ref>, the Thermal perceptron algorithm [4], and the Barycentric correction procedure [10]) are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable. <p> A number of constructive learning algorithms for 2-category pattern classification have been proposed in the literature Tower , Pyramid <ref> [5] </ref>, Tiling [7], Upstart [3], and Perceptron Cascade [1].
Reference: [6] <author> S. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: or TLU) or multi-layer perceptrons (MLP) offer an attractive framework for the design of pattern classification and inductive knowledge acquisition systems for a number of reasons including: potential for parallelism and fault tolerance; significant representational and computational efficiency that they offer over disjunctive normal form (DNF) functions and decision trees <ref> [6] </ref>; and simpler digital hardware realizations than their continuous counterparts. A single TLU, also known as perceptron, can be trained to classify a set of input patterns into one of two classes. <p> For pattern sets that involve multiple output classes, training can be performed either independently or by means of the winner take all (WTA) strategy <ref> [6] </ref>. In the former, each output neuron is trained independently of the others using one of the TLU weight training algorithms mentioned earlier.
Reference: [7] <author> M. Mezard and J. Nadal. </author> <title> Learning feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 22:21912203, </volume> <year> 1989. </year>
Reference-contexts: A number of constructive learning algorithms for 2-category pattern classification have been proposed in the literature Tower , Pyramid [5], Tiling <ref> [7] </ref>, Upstart [3], and Perceptron Cascade [1].
Reference: [8] <author> R. G. Parekh, J. Yang, and V. G. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category classification. </title> <type> Technical Report ISU-CS-TR95-15a, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1995. </year>
Reference-contexts: Experiments have demonstrated the feasibility of this algorithm on practical pattern classification tasks. The convergence proof for MUpstart can be easily adapted to establish the convergence of a multi-category extension of the Perceptron Cascade algorithm <ref> [1, 8, 9] </ref>.
Reference: [9] <author> R. G. Parekh, J. Yang, and V. G. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical Report ISU-CS-TR97-06, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1997. </year>
Reference-contexts: Further it is easy to show that the same weight setting for the daughter and output neurons can be used to show convergence even when the outputs are computed according to the WTA strategy <ref> [9] </ref>. We have thus proved the convergence of the MUpstart algorithm. 2 3. Experimental Results We have conducted experiments on the MUpstart algorithm using a variety of artificial and real-world datasets. <p> Further, the networks constructed exhibit good generalization on test sets. For a detailed analysis of the different performance issues and a comparison of the MUpstart algorithm with other constructive learning algorithms see <ref> [9] </ref>. 4. Discussion Constructive neural network learning algorithms offer a powerful approach to inductive learning for pattern classification applications. This paper has developed MUpstart, a provably convergent extension of the Upstart algorithm to handle multi-category classification and real-valued pattern attributes. <p> Experiments have demonstrated the feasibility of this algorithm on practical pattern classification tasks. The convergence proof for MUpstart can be easily adapted to establish the convergence of a multi-category extension of the Perceptron Cascade algorithm <ref> [1, 8, 9] </ref>.
Reference: [10] <author> H. Poulard. </author> <title> Barycentric correction procedure: A fast method of learning threshold units. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 710713, </pages> <year> 1995. </year>
Reference-contexts: Several extensions to the perceptron weight update rule (e.g., the Pocket algorithm with ratchet modification [5], the Thermal perceptron algorithm [4], and the Barycentric correction procedure <ref> [10] </ref>) are designed to find a reasonably good weight vector that correctly classifies a large fraction of the training set S when S is not linearly separable and to converge to zero classification errors when S is linearly separable.
Reference: [11] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65:386408, </volume> <year> 1958. </year>
Reference-contexts: If such a weight vector ( ^ W) exists for the pattern set S then S is said to be linearly separable. The Perceptron weight update rule <ref> [11] </ref>: W W + (C p O p )X p (where C p is the desired output for pattern X p and &gt; 0 is the learning rate) is an iterative algorithm for determining ^ W if one exists.
Reference: [12] <author> J. Saffery and C. Thornton. </author> <title> Using stereographic projection as a preprocessing technique for upstart. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, pages II 441446. </booktitle> <publisher> IEEE Press, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: The TLU weight training algorithms like the Pocket algorithm with ratchet modification , the Thermal perceptron algorithm , and the Barycentric correction procedure do handle patterns with real-valued attributes. However, extensions of the constructive learning algorithms to handle patterns with real-valued attributes have only been studied for the Upstart <ref> [12] </ref> and the Perceptron Cascade [1] algorithms. We present MUpstart, an extension of the Upstart algorithm. MUpstart is a provably correct constructive learning algorithm that handles multiple output classes, real-valued attributes, and facilitates both independent and WTA training of the output neurons. <p> This attribute takes on a value equal to the sum of squares of the values of all other attributes in the pattern. This idea of considering projections of input patterns was first described in <ref> [12] </ref>.
Reference: [13] <author> J. Yang, R. G. Parekh, and V. G. Honavar. </author> <title> Empirical comparison of graceful variants of the perceptron learning algorithm on non-separable data sets. </title> <note> In preparation, </note> <year> 1997. </year>
Reference-contexts: For a detailed comparison of the algorithms for training TLUs see <ref> [13] </ref>. When S is not linearly separable, a multi-layer network of TLUs is needed to learn a non-linear decision boundary that correctly classifies all the training examples. <p> It is thus of interest to apply the WTA strategy for computing the outputs in constructive learning algorithms. For details on the adaptation of the TLU training algorithms to the WTA strategy see <ref> [13] </ref>. Additionally, practical classification tasks often involve patterns with real-valued attributes. The TLU weight training algorithms like the Pocket algorithm with ratchet modification , the Thermal perceptron algorithm , and the Barycentric correction procedure do handle patterns with real-valued attributes. <p> The design of suitable threshold neuron training algorithms that (with a high probability) satisfy the requirements imposed on A and are at least approximately optimal remains an open research problem. Detailed theoretical and experimental analysis of the performance of single threshold neuron training algorithms is in progress <ref> [13] </ref>. Since our primary focus was on a provably convergent multi-category extension of the Upstart algorithm, we have not addressed a number of important issues in this paper.
References-found: 13

