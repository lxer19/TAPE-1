URL: file://net.cs.utexas.edu/pub/rvdg/shpcc94-cg.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/conference.html
Root-URL: 
Title: Matrix-Vector Multiplication and Conjugate Gradient Algorithms on Distributed Memory Computers  
Author: John G. Lewis David G. Payne Robert A. van de Geijn 
Address: 15201 N.W. Greenbrier Pkwy Seattle, WA 98124-0346 Beaverton, Oregon 97006  Austin, Texas 78712-1188  
Affiliation: Mathematics and Engineering Analysis Supercomputer Systems Division Research and Technology Division Intel Corporation Boeing Computer Services  Department of Computer Sciences The University of Texas at Austin  
Abstract: The critical bottlenecks in the implementation of the conjugate gradient algorithm on distributed memory computers are the communication requirements of the sparse matrix-vector multiply and of the vector recurrences. In a previous paper, we described the data distribution and communication patterns of several implementations of parallel matrix-vector multiplication, demonstrating that on hypercubes, the cost of communication can be overcome to a much larger extent than is often assumed. In this paper, we generalize the best of those implementations to mesh architectures. We make no assumptions about the mesh being square or power-of-two. We also comment on the implications of our results for structured problems and on the scalability of our approach. Results are presented for the implementation of these algorithms on the Intel Touchstone Delta and Paragon mesh multicomputers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> The NAS Parallel Benchmarks. David Bailey, John Barton, Thomas Lasinski and Horst Simon (editors). </author> <type> NASA Technical Memorandum 103863, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Performing an arithmetic operation like floating point addition or multiplication is assumed to require time fl. 3 The NAS Parallel CG Benchmark The NAS Parallel CG Benchmark <ref> [1] </ref> is formally an application of the inverse iteration algorithm for finding eigenvalues. However, the essential part of the benchmark is the solution of unstructured sparse positive definite linear equations.
Reference: [2] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Si-mon. </author> <title> NAS Parallel Benchmark Results, </title> <booktitle> Proceedings of SHPCC94. </booktitle>
Reference-contexts: used 1 This time is worse than that reported in [7] due to a change in the operating system of the Delta 2 The current benchmark now requires 25 iterations. to benchmark the Intel family of parallel computers, the most up-to-date performance numbers can be found elsewhere in these proceedings <ref> [2] </ref>. For the interested reader, we include some Paragon numbers for the redistribution-free method in Table 2. (The Paragon numbers were obtained by Dr. Satya Gupta using a derivative of our codes running under release R1.2 of the Paragon O/S, with an assembly coded matrix-vector multiply.
Reference: [3] <author> M. Barnett, R. Littlefield, D.G. Payne, and R. van de Geijn. </author> <title> Efficient Communication Primitives on Mesh Architectures with Hardware Routing, </title> <booktitle> Sixth SIAM Conf. on Par. Proc. for Sci. Comp., </booktitle> <address> Norfolk, Virginia, March 22-24, </address> <year> 1993. </year>
Reference-contexts: On a linear array, for long vectors, we use a "bucket" algorithm <ref> [3] </ref>, for which the time required is (p 1)ff + p We illustrate this method in Fig. 2 for a four processor linear array.
Reference: [4] <author> M. Barnett, R. Littlefield, D.G. Payne, and R. van de Geijn, </author> <title> Global Combine on Mesh Architectures with Wormhole Routing, </title> <booktitle> in the proceedings of the 7th International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April 13-16, </month> <year> 1993. </year>
Reference-contexts: In this application, we shall see it suffices to leave the result distributed. This implementation avoids network conflicts on mesh architectures. See <ref> [4] </ref> for details of a mesh implementation that reduces the communication startup requirements. We illustrate this method in Fig. 3 for a four processor linear array.
Reference: [5] <editor> G. Fox, et al., </editor> <booktitle> Solving Problems on Concurrent Processors: </booktitle> <volume> Volume 1, </volume> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: It should be noted that this is essentially a sparse version of the dense matrix-vector multiplication algorithm in Fox, et al. <ref> [5] </ref> for hypercubes, with the additional observation that redistribution is needed to complete the data movement for the iteration.
Reference: [6] <author> B. Hendrickson, R. Leland, and S. Plimpton, </author> <title> A Parallel Algorithm for Matrix-Vector Multiplication, </title> <type> Tech. Rep. SAND 92-2765, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: This method was independently discovered for hypercubes by Hendrickson, Leland and Plimpton <ref> [6] </ref>, who observed the need to avoid network conflicts for the transpose in the mapping of the logical mesh of processors to a hypercube.
Reference: [7] <author> J.G. Lewis and R.A. van de Geijn, </author> <title> Distributed Memory Matrix-Vector Multiplication and Conjugate Gradient Algorithms, </title> <booktitle> in the proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November 15-19, </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Matrix-vector multiplication is an important computation in many sparse linear algebraic systems. The primary bottleneck in sparse matrix-vector multiplication on distributed memory multicomputers is communication. A common approach to this problem is to minimize messages. Our conclusions from a previous paper <ref> [7] </ref> lead us to argue against that approach. There we met the communications issue head on, and found that appropriate communication algorithms and suitable data layouts allowed us to achieve high performance. <p> This method was independently discovered for hypercubes by Hendrickson, Leland and Plimpton [6], who observed the need to avoid network conflicts for the transpose in the mapping of the logical mesh of processors to a hypercube. See also Lewis and van de Geijn <ref> [7] </ref> for a discussion of the implementation of these algorithms on a hypercube. 5.2 Redistribution-Free Method Our second implementation is very similar to the first, except that the redistribution operation is avoided by using a different assignment of the matrix A to processors. <p> This was part of a larger study in which we considered a number of factors influencing performance. More details on this study were reported in <ref> [7] </ref>, and we hope to further report in a more comprehensive paper [8]. Table 1 provides results for both methods from the Intel Touchstone Delta mesh multicomputer. We give matrices that have been decomposed as described for these methods. The thick lines indicate structured nonzeroes. <p> The implementations used forced messages and asynchronous (isend/irecv) messaging. As expected, the redistribution-free method outperforms the standard method. Since our implementation is the one currently used 1 This time is worse than that reported in <ref> [7] </ref> due to a change in the operating system of the Delta 2 The current benchmark now requires 25 iterations. to benchmark the Intel family of parallel computers, the most up-to-date performance numbers can be found elsewhere in these proceedings [2].
Reference: [8] <author> J.G. Lewis, S. Ouyang, J. Patterson, D.G. Payne, Y. Shen and R.A. van de Geijn, </author> <title> Sparse Matrix Vector Multiply and Conjugate Gradient Algorithms on Distributed Memory Computers, </title> <note> in preparation. </note>
Reference-contexts: In the end processor P ij computes its own contribution to y ij last and all c 1 stages of the summation occur during computation. However, the performance results in x7 do not reflect this technique; such results will be reported in <ref> [8] </ref>. 6 Comparison of the Methods. We now compare the relative strengths of the two dimensional data decompositions. * Work Balance: For the randomly sparse problem, both data decompositions lead to reasonable load balance. However, for structured problems, this is clearly not the case. <p> This was part of a larger study in which we considered a number of factors influencing performance. More details on this study were reported in [7], and we hope to further report in a more comprehensive paper <ref> [8] </ref>. Table 1 provides results for both methods from the Intel Touchstone Delta mesh multicomputer. We give matrices that have been decomposed as described for these methods. The thick lines indicate structured nonzeroes.
References-found: 8

