URL: ftp://ftp.idsia.ch/pub/juergen/fki-194-94.ps.gz
Refering-URL: http://vita.mines.edu:3857/0/lpratt/transfer.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: DISCOVERING PROBLEM SOLUTIONS WITH LOW KOLMOGOROV COMPLEXITY AND HIGH GENERALIZATION CAPABILITY  
Author: Jurgen Schmidhuber 
Keyword: Generalized Kolmogorov complexity, Solomonoff-Levin distribution, generalization, univer sal search, self-sizing programs, neural networks.  
Address: 80290 Munchen, Germany  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  
Pubnum: Technical Report FKI-194-94  
Email: schmidhu@informatik.tu-muenchen.de  
Web: http://papa.informatik.tu-muenchen.de/mitarbeiter/schmidhu.html  
Date: August 1994  
Abstract: Many machine learning algorithms aim at finding "simple" rules to explain training data. The expectation is: the "simpler" the rules, the better the generalization on test data (! Occam's razor). Most practical implementations, however, use measures for "simplicity" that lack the power, universality and elegance of those based on Kolmogorov complexity and Solomonoff's algorithmic probability. Likewise, most previous approaches (especially those of the "Bayesian" kind) suffer from the problem of choosing appropriate priors. This paper addresses both issues. It first reviews some basic concepts of algorithmic complexity theory relevant to machine learning, and how the Solomonoff-Levin distribution (or universal prior) deals with the prior problem. The universal prior leads to a probabilistic method for finding "algorithmically simple" problem solutions with high generalization capability. The method is based on Levin complexity (a time-bounded generalization of Kolmogorov complexity) and inspired by Levin's optimal universal search algorithm. With a given problem, solution candidates are computed by efficient "self-sizing" programs that influence their own runtime and storage size. The probabilistic search algorithm finds the "good" programs (the ones quickly computing algorithmically probable solutions fitting the training data). Simulations focus on the task of discovering "algorithmically simple" neural networks with low Kolmogorov complexity and high generalization capability. It is demonstrated that the method, at least with certain toy problems where it is computationally feasible, can lead to generalization results unmatchable by previous neural net algorithms. Much remains do be done, however, to make large scale applications and "incremental learning" feasible. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adleman, L. </author> <year> (1979). </year> <title> Time, space, and randomness. </title> <type> Technical Report MIT/LCS/79/TM-131, </type> <institution> Laboratory for Computer Science, MIT. </institution>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see e.g. (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also <ref> (Adleman, 1979) </ref>). Other generalizations of Kolmogorov complexity have been proposed, e.g. (Hartmanis, 1983), but see the contributions in (Watanabe, 1992) for more. Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Allender, A. </author> <year> (1992). </year> <title> Application of time-bounded Kolmogorov complexity in complexity theory. </title> <editor> In Watanabe, O., editor, </editor> <booktitle> Kolmogorov complexity and computational complexity, </booktitle> <pages> pages 6-22. </pages> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: The presentation above is partly inspired by presentations found in (Chaitin, 1987), (Li and Vitanyi, 1993), and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see e.g. <ref> (Allender, 1992) </ref> and (Li and Vitanyi, 1993)). However, it seems that nobody implemented it for experimental applications, perhaps in fear of the ominous "constant factor" which may be large.
Reference: <author> Amari, S. and Murata, N. </author> <year> (1993). </year> <title> Statistical theory of learning curves under entropic loss criterion. </title> <journal> Neural Computation, </journal> <volume> 5(1) </volume> <pages> 140-153. </pages>
Reference: <author> Atick, J. J., Li, Z., and Redlich, A. N. </author> <year> (1992). </year> <title> Understanding retinal color coding from first prinicples. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 559-572. </pages>
Reference-contexts: Various "neural" methods for compressing input data are known. See (Schmidhuber, 1992b) for a "neural" method designed to generate factorial codes. See <ref> (Atick et al., 1992) </ref> for a focus on visual inputs. See (Schmidhuber, 1992a) for loss-free sequence compression. See (Becker, 1991) for numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of un-derfitting the training data.
Reference: <author> Barlow, H. B. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 295-311. </pages>
Reference: <author> Barron, A. R. </author> <year> (1988). </year> <title> Complexity regularization with application to artificial neural networks. </title> <booktitle> In Non-parametric Functional Estimation and Related Topics, </booktitle> <pages> pages 561-576. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist approaches for control. </title> <type> Technical Report COINS Technical Report 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003. </address>
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference: <author> Barzdin, Y. M. </author> <year> (1988). </year> <title> Algorithmic information theory. </title> <editor> In Reidel, D., editor, </editor> <booktitle> Encyclopaedia of Mathematics, </booktitle> <volume> volume 1, </volume> <pages> pages 140-142. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Baum, E. B. and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160. </pages>
Reference: <author> Becker, S. </author> <year> (1991). </year> <title> Unsupervised learning procedures for neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(1 </volume> & 2):17-33. 
Reference-contexts: Various "neural" methods for compressing input data are known. See (Schmidhuber, 1992b) for a "neural" method designed to generate factorial codes. See (Atick et al., 1992) for a focus on visual inputs. See (Schmidhuber, 1992a) for loss-free sequence compression. See <ref> (Becker, 1991) </ref> for numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of un-derfitting the training data. MDL approaches in other areas of machine learning include (Quinlan and Rivest, 1989; Gao and Li, 1989; Milosavljevic and Jurka, 1993; Pednault, 1989).
Reference: <author> Bennett, C. H. </author> <year> (1988). </year> <title> Logical depth and physical complexity. In The Universal Turing Machine: A Half Century Survey, </title> <booktitle> volume 1, </booktitle> <pages> pages 227-258. </pages> <publisher> Oxford University Press, Oxford and Kammerer & Unverzagt, </publisher> <address> Hamburg. </address>
Reference-contexts: The solution. The only solution to the problem is: make all w i equal to 1. The Kolmogorov complexity of this solution is small, since there is a short program that computes it. Its Levin complexity is small, too, since its "logical depth" (the runtime of its shortest program <ref> (Bennett, 1988) </ref>) is less than 400 time steps. The difficulty. If the training set is very small (e.g. if there are just four or five training examples), then conventional perceptron algorithms will not solve this apparently simple problem. They will not achieve good generalization on unseen test data.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference: <author> Chaitin, G. </author> <year> (1966). </year> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal of the ACM, </journal> <volume> 13 </volume> <pages> 547-569. </pages>
Reference: <author> Chaitin, G. </author> <year> (1969). </year> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159. </pages>
Reference-contexts: Both Solomonoff and Kolmogorov observed K's machine independence. Today, even Solomonoff himself refers to K as "Kolmogorov complexity", e.g. (Solomonoff, 1986). In 1969, G. J. Chaitin independently also published the essential concepts <ref> (Chaitin, 1969) </ref> (some hints were already 5 provided at the end of his 1966 paper). Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Schnorr, 1971). Apparently, L. A.
Reference: <author> Chaitin, G. </author> <year> (1975). </year> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 329-340. </pages> <note> 19 Chaitin, </note> <author> G. </author> <year> (1987). </year> <title> Algorithmic Information Theory. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Cover, T. M., Gacs, P., and Gray, R. M. </author> <year> (1989). </year> <title> Kolmogorov's contributions to information theory and algorithmic complexity. </title> <journal> Annals of Probability Theory, </journal> <volume> 17 </volume> <pages> 840-865. </pages>
Reference-contexts: Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993). See also <ref> (Cover et al., 1989) </ref>. See (Schmidhuber, 1994) for an application to fine arts.
Reference: <author> Dayan, P. and Sejnowski, T. </author> <year> (1994). </year> <title> TD(): Convergence with probability 1. Machine Learning. </title> <publisher> In press. </publisher>
Reference: <author> Deco, G., Finnoff, W., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Elimination of overtraining by a mutual information network. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 744-749. </pages> <publisher> Springer. </publisher>
Reference: <author> Dickmanns, D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1986). </year> <title> Der genetische Algorithmus: Eine Imple-mentierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference: <author> Dietterich, T. G. </author> <year> (1989). </year> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, </address> <pages> pages 124-128. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gacs, P. </author> <year> (1974). </year> <title> On the symmetry of algorithmic information. </title> <journal> Soviet Math. Dokl., </journal> <volume> 15 </volume> <pages> 1477-1480. </pages>
Reference: <author> Gao, Q. and Li, M. </author> <year> (1989). </year> <title> The minimum description length principle and its application to online learning of handprinted characters. </title> <booktitle> In Proc. 11th IEEE International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Mi, </address> <pages> pages 843-848. </pages>
Reference: <author> Guyon, I., Vapnik, V., Boser, B., Bottou, L., and Solla, S. A. </author> <year> (1992). </year> <title> Structural risk minimization for character recognition. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 471-479. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate. See (LeCun et al., 1991) for a related approach. See (Vapnik, 1992) and <ref> (Guyon et al., 1992) </ref> for some theoretical analysis. * "Non-algorithmic" MDL methods based on Gaussian priors.
Reference: <author> Hartmanis, J. </author> <year> (1983). </year> <title> Generalized Kolmogorov complexity and the structure of feasible computations. </title> <booktitle> In Proc. 24th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 439-445. </pages>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see e.g. (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g. <ref> (Hartmanis, 1983) </ref>, but see the contributions in (Watanabe, 1992) for more. Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 164-171. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Haussler, D. </author> <year> (1988). </year> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221. </pages>
Reference: <author> Hinton, G. E. and van Camp, D. </author> <year> (1993). </year> <title> Keeping neural networks simple. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 11-18. </pages> <publisher> Springer. </publisher>
Reference-contexts: A special error term (in addition to the standard term enforcing matches between desired outputs and actual outputs) encourages weights close to zero. The idea is that a zero weight does not cost many bits to be specified, thus being "simple" <ref> (Hinton and van Camp, 1993) </ref>. Pearlmutter and Hinton were probably the first to propose weight decay, while Rumelhart was perhaps the first to suggest its use for reducing overfitting.
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1994). </year> <title> Simplifying networks by discovering "flat" minima. </title> <type> Technical Report FKI- -94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> To be presented at NIPS'94. </note>
Reference: <author> Hoffmeister, F. and Back, T. </author> <year> (1991). </year> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <editor> In Manner, R. and Schwefel, H. P., editors, </editor> <booktitle> Proc. of 1st International Conference on Parallel Problem Solving from Nature, </booktitle> <address> Berlin. </address> <publisher> Springer. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor. </address>
Reference: <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11. </pages>
Reference-contexts: History spotlights / Selected references In 1965, A. N. Kolmogorov (1903-1987), founder of modern axiomatic probability theory (Kolmogorov, 1933), was the first to introduce a variant of the complexity measure K for its own sake <ref> (Kolmogorov, 1965) </ref>. Levin (1984) cites announcements of Kolmogorov's lectures on this subject dating back to 1961. In independent and even earlier work, R. J.
Reference: <author> Kolmogorov, A. N. </author> <year> (1933). </year> <editor> Grundbegriffe der Wahrscheinlichkeitsrechnung. </editor> <publisher> Springer, </publisher> <address> Berlin. </address> <note> 20 Koza, </note> <author> J. R. </author> <year> (1992). </year> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 313-324. </pages> <publisher> Addison Wesley Publishing Company. </publisher>
Reference-contexts: In the context of machine learning, conditional complexity measures how much additional information has to be acquired, given what is already known. History spotlights / Selected references In 1965, A. N. Kolmogorov (1903-1987), founder of modern axiomatic probability theory <ref> (Kolmogorov, 1933) </ref>, was the first to introduce a variant of the complexity measure K for its own sake (Kolmogorov, 1965). Levin (1984) cites announcements of Kolmogorov's lectures on this subject dating back to 1961. In independent and even earlier work, R. J.
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 950-957. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> LeCun, Y. </author> <year> (1985). </year> <title> Une procedure d'apprentissage pour reseau a seuil asymetrique. </title> <booktitle> Proceedings of Cognitiva 85, Paris, </booktitle> <pages> pages 599-604. </pages>
Reference: <author> LeCun, Y., Kanter, I., and Solla, S. A. </author> <year> (1991). </year> <title> Second order properties of error surfaces: Learning time and generalization. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 918-924. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon. Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate. See <ref> (LeCun et al., 1991) </ref> for a related approach. See (Vapnik, 1992) and (Guyon et al., 1992) for some theoretical analysis. * "Non-algorithmic" MDL methods based on Gaussian priors.
Reference: <author> Levin, L. A. </author> <year> (1973a). </year> <title> On the notion of a random sequence. </title> <journal> Soviet Math. Dokl., </journal> <volume> 14(5) </volume> <pages> 1413-1416. </pages>
Reference: <author> Levin, L. A. </author> <year> (1973b). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: Levin proved equation (3): K (s) = H (s)+O (1). The importance of prefix codes was independently seen by Chaitin (1975), who also proved (3) and attributes part of the argument to N. Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see e.g. <ref> (Levin, 1973b) </ref> and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g. (Hartmanis, 1983), but see the contributions in (Watanabe, 1992) for more.
Reference: <author> Levin, L. A. </author> <year> (1974). </year> <title> Laws of information (nongrowth) and aspects of the foundation of probability theory. </title> <journal> Problems of Information Transmission, </journal> <volume> 10(3) </volume> <pages> 206-210. </pages>
Reference-contexts: Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Schnorr, 1971). Apparently, L. A. Levin was the first to introduce and analyze today's "standard form" of Kolmogorov complexity based on halting programs and prefix codes <ref> (Levin, 1974) </ref>, see also (Gacs, 1974; Levin, 1973a; Levin, 1976; Zvonkin and Levin, 1970). Levin proved equation (3): K (s) = H (s)+O (1). The importance of prefix codes was independently seen by Chaitin (1975), who also proved (3) and attributes part of the argument to N. Pippenger.
Reference: <author> Levin, L. A. </author> <year> (1976). </year> <title> Various measures of complexity for finite objects (axiomatic description). </title> <journal> Soviet Math. Dokl., </journal> <volume> 17(2) </volume> <pages> 522-526. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference-contexts: Levin proved equation (3): K (s) = H (s)+O (1). The importance of prefix codes was independently seen by Chaitin (1975), who also proved (3) and attributes part of the argument to N. Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see e.g. (Levin, 1973b) and <ref> (Levin, 1984) </ref>, where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g. (Hartmanis, 1983), but see the contributions in (Watanabe, 1992) for more.
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1989). </year> <title> A theory of learning simple concepts under simple distributions and average case complexity for the universal distribution. </title> <booktitle> In Proc. 30th American IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 34-39. </pages>
Reference-contexts: Problems that humans consider to be typical are atypical when compared to the general set of all well-defined problems (see also <ref> (Li and Vitanyi, 1989) </ref>). Indeed, for all "interesting" problems, the bias towards algorithmic simplicity seems justified! This may be a miracle. Or perhaps a consequence of the possibility that our universe is run by a short algorithm (every electron behaves the same way).
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference-contexts: An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993). See also (Cover et al., 1989). See (Schmidhuber, 1994) for an application to fine arts. The presentation above is partly inspired by presentations found in (Chaitin, 1987), <ref> (Li and Vitanyi, 1993) </ref>, and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see e.g. (Allender, 1992) and (Li and Vitanyi, 1993)). <p> The presentation above is partly inspired by presentations found in (Chaitin, 1987), <ref> (Li and Vitanyi, 1993) </ref>, and (Solomonoff, 1986). 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see e.g. (Allender, 1992) and (Li and Vitanyi, 1993)). However, it seems that nobody implemented it for experimental applications, perhaps in fear of the ominous "constant factor" which may be large.
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117. </pages>
Reference: <author> Maass, W. </author> <year> (1994). </year> <title> Perspectives of current research about the complexity of learning on neural nets. </title> <editor> In Roychowdhury, V. P., Siu, K. Y., and Orlitsky, A., editors, </editor> <booktitle> Theoretical Advances in Neural Computation and Learning. </booktitle> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference-contexts: MacKay evaluates hyper-parameters (such as weight-decay rates) with respect to their probabilities of generating the observed data <ref> (MacKay, 1992) </ref>. Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon. Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate.
Reference: <author> Martin-Lof, P. </author> <year> (1966). </year> <title> The definition of random sequences. </title> <journal> Information and Control, </journal> <volume> 9 </volume> <pages> 602-619. </pages>
Reference: <author> Milosavljevic, A. and Jurka, J. </author> <year> (1993). </year> <title> Discovery by minimal length encoding: A case study in molecular evolution. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 96-87. </pages>
Reference: <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 847-854. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. C. and Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 107-115. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 21 Nowlan, </publisher> <editor> S. J. and Hinton, G. E. </editor> <year> (1992). </year> <title> Simplifying neural networks by soft weight sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 173-193. </pages>
Reference: <author> Parker, D. B. </author> <year> (1985). </year> <title> Learning-logic. </title> <type> Technical Report TR-47, </type> <institution> Center for Comp. Research in Economics and Management Sci., MIT. </institution>
Reference: <author> Paul, W. and Solomonoff, R. J. </author> <year> (1991). </year> <title> Autonomous theory building systems. </title> <type> Manuscript, </type> <note> revised 1994. </note>
Reference: <author> Pearlmutter, B. A. and Rosenfeld, R. </author> <year> (1991). </year> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 925-931. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pednault, E. P. D. </author> <year> (1989). </year> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In 11th IJCAI, </booktitle> <pages> pages 1603-1609. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. and Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248. </pages>
Reference: <author> Rechenberg, I. </author> <year> (1971). </year> <title> Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biol-ogischen Evolution. Dissertation. </title> <note> Published 1973 by Fromman-Holzboog. </note>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431. </pages>
Reference: <author> Rissanen, J. </author> <year> (1986). </year> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1987). </year> <title> Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... </title> <type> hook. Report, </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. H. </author> <year> (1989). </year> <title> A local learning algorithm for dynamic feedforward and recurrent networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 403-412. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992a). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference-contexts: Various "neural" methods for compressing input data are known. See (Schmidhuber, 1992b) for a "neural" method designed to generate factorial codes. See (Atick et al., 1992) for a focus on visual inputs. See <ref> (Schmidhuber, 1992a) </ref> for loss-free sequence compression. See (Becker, 1991) for numerous additional references. There are also numerous heuristic constructive methods, where network size grows in case of un-derfitting the training data.
Reference: <author> Schmidhuber, J. H. </author> <year> (1992b). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference-contexts: From the standpoint of classical information theory, an optimal compression algorithm is one that builds 10 a factorial code of the input data (a code with statistically independent components, e.g. (Bar--low, 1989)). Various "neural" methods for compressing input data are known. See <ref> (Schmidhuber, 1992b) </ref> for a "neural" method designed to generate factorial codes. See (Atick et al., 1992) for a focus on visual inputs. See (Schmidhuber, 1992a) for loss-free sequence compression. See (Becker, 1991) for numerous additional references.
Reference: <author> Schmidhuber, J. H. </author> <year> (1993a). </year> <title> On decreasing the ratio between learning complexity and number of time-varying variables in fully recurrent nets. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 460-463. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1993b). </year> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1994). </year> <title> Algorithmic art. </title> <type> Technical report, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993). See also (Cover et al., 1989). See <ref> (Schmidhuber, 1994) </ref> for an application to fine arts. <p> The experimental results obtained with the probabilistic algorithm (see section 4) are very similar to those obtained by the original universal search procedure <ref> (Schmidhuber and Jankowski, 1994) </ref>. Overview. The method described in this section searches and finds algorithms that compute solutions to a given problem specified by possibly very limited "training data". The goal is to discover solutions with high generalization performance on "test data" unavailable during the search phase. <p> When speed is an issue, then we will prefer systematic enumeration, or a slightly more complicated probabilistic variant whose expected search time equals the one of systematic enumeration. Variants of systematic universal search based on the primitives above were implemented in collaboration with Norbert Jankowski <ref> (Schmidhuber and Jankowski, 1994) </ref>. With the examples below, however, total 2 In principle, it is possible to run a variant of universal search on a neural net architecture instead of a conventional digital machine. <p> Although the focus of the experiments was on perceptron-like neural nets, the presented methods are general enough to be applied to a wide variety of problems. For instance, in work done in collaboration with Norbert Jankowski, variants of universal search were successfully applied to path finding problems in mazes <ref> (Schmidhuber and Jankowski, 1994) </ref>. Much work on "incremental" learning in real world applications remains to be done, however. The bias towards algorithmic simplicity is a very general one. It is weaker than most kinds of problem specific inductive bias, e.g. (Utgoff, 1986; Haussler, 1988).
Reference: <author> Schmidhuber, J. H. and Jankowski, N. </author> <year> (1994). </year> <title> Applications of Levin's optimal universal search algorithm. </title> <type> Technical report, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> In preparation. 22 Schnorr, </note> <author> C. P. </author> <year> (1971). </year> <title> A unified approach to the definition of random sequences. </title> <journal> Mathematical Systems Theory, </journal> <volume> 5 </volume> <pages> 246-258. </pages>
Reference-contexts: Meanwhile, the theory of Kolmogorov complexity has split into many subfields. An excellent overview and many additional details on the history are given in Li and Vitanyi's book (1993). See also (Cover et al., 1989). See <ref> (Schmidhuber, 1994) </ref> for an application to fine arts. <p> The experimental results obtained with the probabilistic algorithm (see section 4) are very similar to those obtained by the original universal search procedure <ref> (Schmidhuber and Jankowski, 1994) </ref>. Overview. The method described in this section searches and finds algorithms that compute solutions to a given problem specified by possibly very limited "training data". The goal is to discover solutions with high generalization performance on "test data" unavailable during the search phase. <p> When speed is an issue, then we will prefer systematic enumeration, or a slightly more complicated probabilistic variant whose expected search time equals the one of systematic enumeration. Variants of systematic universal search based on the primitives above were implemented in collaboration with Norbert Jankowski <ref> (Schmidhuber and Jankowski, 1994) </ref>. With the examples below, however, total 2 In principle, it is possible to run a variant of universal search on a neural net architecture instead of a conventional digital machine. <p> Although the focus of the experiments was on perceptron-like neural nets, the presented methods are general enough to be applied to a wide variety of problems. For instance, in work done in collaboration with Norbert Jankowski, variants of universal search were successfully applied to path finding problems in mazes <ref> (Schmidhuber and Jankowski, 1994) </ref>. Much work on "incremental" learning in real world applications remains to be done, however. The bias towards algorithmic simplicity is a very general one. It is weaker than most kinds of problem specific inductive bias, e.g. (Utgoff, 1986; Haussler, 1988).
Reference: <author> Schwefel, H. P. </author> <year> (1974). </year> <title> Numerische Optimierung von Computer-Modellen. Dissertation. </title> <note> Published 1977 by Birkhauser, Basel. </note>
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, XXVII:379-423. </journal>
Reference-contexts: The weights are taken to be generated by mixtures of Gaussians. The fewer the number of Gaussians and the closer some weight is to the center of some Gaussian, the higher its probability, and the fewer bits are needed to encode it (according to classical information theory <ref> (Shannon, 1948) </ref>). * Bayesian strategies for backprop nets. MacKay evaluates hyper-parameters (such as weight-decay rates) with respect to their probabilities of generating the observed data (MacKay, 1992). Estimates of the probabilities are computed on the basis of Gaussian assumptions. * Optimal Brain Surgeon.
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: Both Solomonoff and Kolmogorov observed K's machine independence. Today, even Solomonoff himself refers to K as "Kolmogorov complexity", e.g. <ref> (Solomonoff, 1986) </ref>. In 1969, G. J. Chaitin independently also published the essential concepts (Chaitin, 1969) (some hints were already 5 provided at the end of his 1966 paper). Important related early work is described in (Martin-Lof, 1966; Gacs, 1974; Schnorr, 1971). Apparently, L. A. <p> See also (Cover et al., 1989). See (Schmidhuber, 1994) for an application to fine arts. The presentation above is partly inspired by presentations found in (Chaitin, 1987), (Li and Vitanyi, 1993), and <ref> (Solomonoff, 1986) </ref>. 3 PROBABILISTIC SEARCH FOR USEFUL SELF-SIZING PROGRAMS WITH LOW LEVIN COMPLEXITY Levin's universal search algorithm was considered of interest for theoretical purposes (see e.g. (Allender, 1992) and (Li and Vitanyi, 1993)).
Reference: <author> Solomonoff, R. </author> <year> (1990). </year> <title> A system for incremental learning based on algorithmic probability. </title> <editor> In Pednault, E. P. D., editor, </editor> <booktitle> The Theory and Application of Minimal-Length Encoding (Preprint of Symposium papers of AAAI 1990 Spring Symposium). </booktitle>
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <booktitle> In Machine Learning, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Valiant, L. G. </author> <year> (1987). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142. </pages>
Reference: <author> Vapnik, V. </author> <year> (1992). </year> <title> Principles of risk minimization for learning theory. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 831-838. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hassibi and Storck (1993) use second order information to obtain "simple" nets by pruning weights whose influence on the error is minimal, and changing other weights to compensate. See (LeCun et al., 1991) for a related approach. See <ref> (Vapnik, 1992) </ref> and (Guyon et al., 1992) for some theoretical analysis. * "Non-algorithmic" MDL methods based on Gaussian priors.
Reference: <author> Wallace, C. S. and Boulton, D. M. </author> <year> (1968). </year> <title> An information theoretic measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 185-194. </pages>
Reference: <author> Watanabe, O. </author> <year> (1992). </year> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference-contexts: Pippenger. Levin introduced Kt complexity and the universal optimal search algorithm (see e.g. (Levin, 1973b) and (Levin, 1984), where related ideas are attributed to Adleman see also (Adleman, 1979)). Other generalizations of Kolmogorov complexity have been proposed, e.g. (Hartmanis, 1983), but see the contributions in <ref> (Watanabe, 1992) </ref> for more. Easily computable approximations of the MDL principle were formulated by Wallace and Boulton (1968) and Rissanen (1978, 1983, 1986).
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College. </institution>
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference: <author> Williams, R. J. </author> <year> (1988). </year> <title> Toward a theory of reinforcement-learning connectionist systems. </title> <type> Technical Report NU-CCS-88-3, </type> <institution> College of Comp. Sci., Northeastern University, </institution> <address> Boston, MA. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On overfitting avoidance as bias. </title> <type> Technical Report SFI TR 93-03-016, </type> <institution> Santa Fe Institute, </institution> <address> NM 87501. </address>
Reference: <author> Zvonkin, A. K. and Levin, L. A. </author> <year> (1970). </year> <title> The complexity of finite objects and the algorithmic concepts of information and randomness. </title> <journal> Russian Math. Surveys, </journal> <volume> 25(6) </volume> <pages> 83-124. 23 </pages>
Reference-contexts: Such approximations build the basis of most if not all current machine learning applications, e.g. (Quinlan and Rivest, 1989; Gao and Li, 1989; Milosavljevic and Jurka, 1993; Pednault, 1989). Barzdin, referred to in <ref> (Zvonkin and Levin, 1970) </ref>, related Kolmogorov complexity to a variant of Godel's incompleteness theorem, a subject which became a central theme of Chaitin's research (Chaitin, 1987). Meanwhile, the theory of Kolmogorov complexity has split into many subfields.
References-found: 85

