URL: http://www.hcs.eng.fsu.edu/pubs/HCS_SCI6.ps
Refering-URL: http://www.hcs.eng.fsu.edu/Pubs.html
Root-URL: 
Title: SCI and the Scalable Cluster Architecture Latency-hiding Environment (SCALE) Project  
Author: Alan George, William Phipps, Robert Todd, David Zirpoli, Kyla Justice, Mark Giacoboni, and Mushtaq Sarwar 
Date: Abstract  
Address: Florida located in Gainesville, Florida.  
Note: 1 In January, Dr. George and the HCS Research Lab will join the  
Affiliation: High-performance Computing and Simulation (HCS) Research Laboratory 1 Electrical Engineering Department, FAMU-FSU College of Engineering Florida State University and Florida A&M University  Department of Electrical and Computer Engineering at the University of  
Abstract: Future high-performance computing systems for both general-purpose and embedded applications must be able: scalable, portable, dependable, programmable, and affordable. The Scalable Cluster Architecture L a-tency-hiding Environment (SCALE) project underway in the HCS Research Laboratory hopes to contribute to this end by bridging the gap between the research worlds of parallel computing and high-performance interconnects by employing concurrent research methods that are both theoretical and experimental in nature. This paper pr o-vides an overview of the SCALE concepts and project, current developments and results in modeling and sim u-lation tasks, current developments and results in experimental testbed tasks, as well as conclusions and future research plans. 
Abstract-found: 1
Intro-found: 1
Reference: [ANDE95] <author> Anderson, Duane, </author> <title> Proposal to the P1596.6 (SCI/RT) Working Group for A Preemptive Pr i-ority Queue Protocol, </title> <note> White Paper for IEEE P1596.6 WG, April 5,1995. </note>
Reference-contexts: In addition to modeling and simulation of base SCI, development is continuing on several real-time protocols based on SCI. Models have been developed and are cu r-rently being tested simulating the Preemptive Priority Queue <ref> [ANDE95] </ref> and TRAIN [SCOT95] protocols. Also, development is continuing on a Directed Flow Co n-trol protocol model [LACH96] and a 2-bit Priority prot o-col model [GUST96]. Preliminary results and detailed model descriptions are given in [TODD96]. 3.
Reference: [CARR92] <author> Carriero, N. and Gelernter, D., </author> <title> How to Write Parallel Programs: A First Course . The MIT Press, </title> <address> Cambridge, MA. </address> <year> 1992. </year>
Reference-contexts: Interaction with developers at Berkeley will help to validate this implementation. HCS_Linda Linda, developed at Yale, is a simplistic yet powerful parallel coordination language that offers a co ntent-addressable virtual shared memory across a network of workstations [SCA95] <ref> [CARR92] </ref>. Researchers at the HCS Lab have begun the design and implementation of a multithreaded, lightweight version of Linda. This is i m-plemented using active messages due to the similarities of remote procedure call execution and synchronization that are inherent within Linda.
Reference: [DOLP95] <author> Dolphin Inc., </author> <title> 1 Gbit/sec SBus-SCI Cluster Adapter Card, </title> <type> White Paper, </type> <institution> Dolphin Interco n-nect Solutions, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Some of these developments are: HCS_LIB, HCS_AM, and HCS_Linda. MPI was also used in development of parallel applications. HCS_LIB The lowest-level API available for use with the Do l-phin SCI/Sbus-1 cards is a set of library routines called SCI_LIB <ref> [DOLP95] </ref>. This set of functions provides a level of abstraction to the programmer and facilitates the creation of shared-memory services and message-passing support without the programmer being overly concerned with the specific idiosyncrasies of the current revision of SCI drivers. The SCI_LIB functions are shown in Table 4.
Reference: [FREU93] <editor> Fruend, R.F. and Siegel, H.J., </editor> <booktitle> Heterogeneous Processing, IEEE Computer, </booktitle> <month> June </month> <year> 1993, </year> <pages> pp. 13-17. </pages>
Reference-contexts: Research topics in heterogeneous computing are diverse and involve issues related to co n-nectivity, bandwidth, granularity, high-level orchestration tools, and programming paradigms. Heterogeneous co m-puting methods and tools like SmartNet <ref> [FREU93] </ref> can help to harness the dynamic performance requirements associated with sequential and parallel job sets. The research, development and demonstration of the SCALE system will take place in terms of both modeling and simulation tasks and experimental testbed tasks.
Reference: [GEOR95] <author> A.D. George, R.W. Todd, and W. Rosen, </author> <title> "A Cluster Testbed for SCI-based Parallel Proces sing," </title> <booktitle> Proceedings of the 4 th International. Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <month> November </month> <year> 1995, </year> <pages> pp. 109-114. </pages>
Reference-contexts: While all of these parallel computing mechanisms, both for high-level parallel pr o gramming and low-level, lightweight, highspeed comm u-nications, are still quite new and continue to be perfected, they have already been successful in achieving a reason ably high degree of parallel system efficiency <ref> [GEOR95, GEOR96] </ref>. These tools will be extended, leveraged, and complimented with mechanisms to design, develop, and demonstrate the SCALE system. <p> Throughput and Latency Tests In order to gauge the potential performance benefits of the SCALE environment, a number of basic bench-marking programs have been developed and their results measured and collected. In <ref> [GEOR95] </ref> these measur e-ments were presented for a ring of SS5/85 workstations running SunOS 4.1.3. In [GEOR96] the same measur e-ments were shown using a heterogeneous cluster of SS20/85, SS20/50, and SS5/85 workstations.
Reference: [GEOR96] <author> A.D. George, R.W. Todd, W. Phipps, M. Miars, and W. Rosen, </author> <title> Parallel Processing Experiments on an SCI-based Workstation Cluster, </title> <booktitle> Proceedings of the 5 th International Workshop on SCI-based High-Performance Low-Cost Co m-puting, </booktitle> <month> March </month> <year> 1996, </year> <pages> pp. 29-39. </pages>
Reference-contexts: While all of these parallel computing mechanisms, both for high-level parallel pr o gramming and low-level, lightweight, highspeed comm u-nications, are still quite new and continue to be perfected, they have already been successful in achieving a reason ably high degree of parallel system efficiency <ref> [GEOR95, GEOR96] </ref>. These tools will be extended, leveraged, and complimented with mechanisms to design, develop, and demonstrate the SCALE system. <p> Throughput and Latency Tests In order to gauge the potential performance benefits of the SCALE environment, a number of basic bench-marking programs have been developed and their results measured and collected. In [GEOR95] these measur e-ments were presented for a ring of SS5/85 workstations running SunOS 4.1.3. In <ref> [GEOR96] </ref> the same measur e-ments were shown using a heterogeneous cluster of SS20/85, SS20/50, and SS5/85 workstations. <p> Parallel Processing Experiments The parallel programs developed for this study were designed and implemented in C using HCS_LIB or MPI function calls over SCI. In order to compare the results with conventional workstation clusters, a comparable set of programs was also developed using MPICH. In <ref> [GEOR96] </ref>, the matrix multiply and parallel sort applic a tions were run on a heterogeneous cluster of workstations.
Reference: [GUST95] <author> Gustavson, D.B. and Q. Li, </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface, </title> <booktitle> Proceedings of the Second International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <pages> pp. 131-154, </pages> <month> March, </month> <year> 1995. </year>
Reference: [GUST96] <author> Gustavson, </author> <title> D.B., RealTime Architecture for the Scalable Coherent Interface, </title> <booktitle> White Paper for IEEE P1596.6 WG, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: Models have been developed and are cu r-rently being tested simulating the Preemptive Priority Queue [ANDE95] and TRAIN [SCOT95] protocols. Also, development is continuing on a Directed Flow Co n-trol protocol model [LACH96] and a 2-bit Priority prot o-col model <ref> [GUST96] </ref>. Preliminary results and detailed model descriptions are given in [TODD96]. 3. Experimental Testbed Research The HCS Lab is researching, developing, and stud y-ing new lightweight communication protocols and coord i-nation languages to enhance portability and ease the wri t-ing of parallel programs.
Reference: [HPC96] <author> Netperf: </author> <title> A Network Performance Benchmark , Information Networks Division, Hewlett-Packard Company. Revision 2.1, </title> <month> February 15, </month> <year> 1996. </year>
Reference-contexts: The r e-sults of these basic experiments are provided in the form of both latency and effective throughput measurements. All TCP/IP-based throughput and latency statistics were obtained using a public domain benchmarking utility by Hewlett-Packard called netperf <ref> [HPC96] </ref>. Throughput metrics were obtained using the TCP_STREAM test and latency metrics were obtained using the TCP_RR test. The shared-memory benc hmark using SCI examines the latency between two machines for an n-byte payload transfer.
Reference: [KLIN93] <author> Kline, T., Rosen, W., and Harris, J., </author> <title> Application of IEEE 1596-1992 Scalable Coherent Interface to Sensor/Video Interconnect Subsystems. </title> <type> White Paper, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: The top-level block diagram of the connectionless switch is shown in Figure 6. as the Switch Fabric The third switch is the multi-service switch proposed by the Navy <ref> [KLIN93] </ref> which combines the multiple-port connectionless switch above with an internal crossbar and controller. The crossbar controller is addressable by any externally-connected node by residing on the internal switch fabric.
Reference: [LACH96] <author> Lachenmaier, R. and Stretch, T., </author> <title> A Draft Pr o-posal for a SCI/Real-Time Protocol using D i-rected Flow Control Symbols, </title> <note> White Paper for IEEE P1596.6, draft 0.21, </note> <month> April 12, </month> <year> 1996. </year>
Reference-contexts: Models have been developed and are cu r-rently being tested simulating the Preemptive Priority Queue [ANDE95] and TRAIN [SCOT95] protocols. Also, development is continuing on a Directed Flow Co n-trol protocol model <ref> [LACH96] </ref> and a 2-bit Priority prot o-col model [GUST96]. Preliminary results and detailed model descriptions are given in [TODD96]. 3.
Reference: [MAIN95] <author> Mainwaring, </author> <title> A.M., Active Message Applic a-tions Programming Interface and Communication Subsystem Organization, </title> <type> Draft Technical R e-port. </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: HCS_AM To build upon the successes of other universities and in the spirit of simple, portable and low-latency commun i-cations, the HCS Lab is actively pursuing an Active Me s-sage (AM) implementation over SCI called HCS_AM. This ongoing work will be compatible with Berkeleys Generic Active Message <ref> [MAIN95] </ref> specification to e n-able SCI clusters access to the newest applications d e-signed for Active Messages. AM offers a low-latency communication method which allows remote procedure execution and bulk data transfers in a simple, unified package.
Reference: [MPIF93] <author> Message Passing Interface Forum, </author> <title> MPI: A Me s-sage Passing Interface, </title> <booktitle> Proceedings of Supe r-computing 1993 , IEEE Computer Society Press, </booktitle> <pages> pp. 878-883, </pages> <year> 1993. </year>
Reference-contexts: The Message Passing Interface (MPI) The MPI specification was created by the Message Passing Interface Forum <ref> [MPIF93] </ref> with the goal of pr o-viding a portable parallel API that allows for efficient communication, heterogeneous implementations, conve n-ient C and FORTRAN-77 language bindings, and consi s-tency with current message-passing paradigm practices (such as PVM, NX, p4, etc.).
Reference: [PHIP96] <author> W. Phipps, </author> <title> Switching and Parallel Processing Techniques for the Scalable Coherent Interface, </title> <journal> B.S.E.E. </journal> <note> Honors Thesis, </note> <author> Major Professor: A. George, </author> <month> Spring </month> <year> 1996. </year>
Reference-contexts: The crossbar controller is addressable by any externally-connected node by residing on the internal switch fabric. To improve the performance of streaming data, the crossbar is switched to physically connect two external ringlets into a single larger ring. All of the switch models and descriptions are featured in <ref> [PHIP96] </ref>. Figure 7 shows the block diagram for the BONeS model of the multi-service switch.
Reference: [JUST96] <author> K. Justice and W. Phipps, </author> <title> Design and Impl e-mentation of Lindas TupleSpace over Active Messages, </title> <note> HCS Research Laboratory Technical Report, </note> <month> August </month> <year> 1996. </year>
Reference-contexts: Development is also ta king place to expand upon the HCS_Linda paradigm to Piranha or Paradise network models, which extend the basic Linda memory model to either adaptive parallelism or to fault-tolerant shared-memory servers, respectively <ref> [JUST96] </ref>.
Reference: [SARW96] <author> M.A. Sarwar and J.A. Herbert, </author> <title> Fast Fourier Transform over Scalable Coherent Interface, </title> <note> HCS Research Laboratory Technical Report, </note> <month> April </month> <year> 1996. </year>
Reference-contexts: After each stage, the results are sent to the master who then distributes them to the remaining active workers. This process continues until the last stage in which only the master performs calculations which produce the FFT results <ref> [SARW96] </ref>. 1 31 29 27 25 23 21 19 17 15 13 11 9 7 5 3 1 st Stage 5 th Stage 4 th Stage 3 rd Stage 2 nd Stage 1 31 29 27 25 23 21 19 17 15 13 11 9 7 5 3 All of the
Reference: [SCA95] <institution> Linda Users Guide and Reference Manual , Scientific Computing Associates. </institution> <note> Manual version 3.0, </note> <month> January </month> <year> 1995. </year>
Reference-contexts: Interaction with developers at Berkeley will help to validate this implementation. HCS_Linda Linda, developed at Yale, is a simplistic yet powerful parallel coordination language that offers a co ntent-addressable virtual shared memory across a network of workstations <ref> [SCA95] </ref> [CARR92]. Researchers at the HCS Lab have begun the design and implementation of a multithreaded, lightweight version of Linda. This is i m-plemented using active messages due to the similarities of remote procedure call execution and synchronization that are inherent within Linda.
Reference: [SCI93] <institution> Scalable Coherent Interface , ANSI/IEEE Sta n-dard 1596-1992, IEEE Service Center, </institution> <address> Piscata-way, New Jersey, </address> <year> 1993. </year>
Reference-contexts: The parallel architecture of a SCALE system is co n-structed by combining high-performance interconnects in a multilevel fashion. The scalable topology we propose is a cluster which starts with shared-bus, shared-memory symmetric multiprocessors (SMPs) with UMA. These SMPs are combined via the Scalable Coherent Interface (SCI) <ref> [SCI93] </ref> with NUMA and CC-NUMA across local-area distances, and then clusters of these SCI-based mult i-processors are combined via Asynchronous Transfer Mode (ATM) across metropolitan-area and wide-area distances.
Reference: [SCOT95] <author> Scott, Tim, </author> <title> Full Performance TRAIN Protocol for SCI/RT, </title> <note> White Paper for IEEE P1596.6 WG, draft 0.11, September 30, </note> <year> 1995. </year>
Reference-contexts: In addition to modeling and simulation of base SCI, development is continuing on several real-time protocols based on SCI. Models have been developed and are cu r-rently being tested simulating the Preemptive Priority Queue [ANDE95] and TRAIN <ref> [SCOT95] </ref> protocols. Also, development is continuing on a Directed Flow Co n-trol protocol model [LACH96] and a 2-bit Priority prot o-col model [GUST96]. Preliminary results and detailed model descriptions are given in [TODD96]. 3.
Reference: [STRM89] <author> Strum, R.D. and Kirk, D.E., </author> <title> Discrete Systems and Digital Signal Processing , Addison-Wesley Publications, </title> <booktitle> Nuew Your, </booktitle> <pages> pp. 493-512, </pages> <year> 1989. </year>
Reference-contexts: The approach this program uses is the radix-2 FFT that was developed by J. Cooley and J.W. Tukey <ref> [STRM89] </ref>. A divide-and-conquer algorithm is used which places the restriction that the number of data points N is an integer power of two. vector. The parallel FFT is an example of agenda para l-lelism where a master divides the vector into blocks that are sent to the workers.
Reference: [TODD96] <author> R.W. Todd, </author> <title> A Simulation CaseStudy of Pr o-posed Real-Time Protocols Based on the Scalable Coherent Interface, M.S.E.E. </title> <type> Thesis, </type> <note> Major Professor: </note> <author> A. George, </author> <month> Summer </month> <year> 1996. </year>
Reference-contexts: Also, development is continuing on a Directed Flow Co n-trol protocol model [LACH96] and a 2-bit Priority prot o-col model [GUST96]. Preliminary results and detailed model descriptions are given in <ref> [TODD96] </ref>. 3. Experimental Testbed Research The HCS Lab is researching, developing, and stud y-ing new lightweight communication protocols and coord i-nation languages to enhance portability and ease the wri t-ing of parallel programs. Some of these developments are: HCS_LIB, HCS_AM, and HCS_Linda.
Reference: [ZIRP96] <author> D. Zirpoli, </author> <title> Parallel Computing on Workstation Clusters via the Scalable Coherent Interface, M.S.E.E. </title> <type> Thesis, </type> <note> Major Professor: </note> <author> A. George, </author> <month> Fall </month> <year> 1996. </year>
References-found: 22

