URL: http://www.lsi.upc.es/dept/techreps/ps/R95-14.ps.gz
Refering-URL: http://www.lsi.upc.es/dept/techreps/1995.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mmartin@lsi.upc.es, ia@lsi.upc.es  
Title: Learning to Solve Complex Tasks by Reinforcement: A New Algorithm  
Author: Mario Martin, Ulises Cortes 
Keyword: Machine Learning, Reinforcement Learning, Robotics, Reactive sys tems.  
Date: Abstract  
Address: Gargallo 5, 08028, Barcelona (Catalunya), SPAIN  
Affiliation: Dept. Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya C Pau  
Abstract: In this paper, a new approach for learning to solve complex problems by reinforcement is proposed. In order to solve complex tasks the system is guided by a teacher who previously proposes intermediate general tasks to learn. The learnt behaviors to solve these tasks are added to the system's set of actions increasing its skills until it is able to easily solve the desired complex task. This approach uses a new reinforcement learning mechanism, robust to ambiguous information and able to learn general behaviors. These mechanisms are studied, described and finally tested with a set of experiments in a complex environment. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Ballard. </author> <title> Reference frames for animate vision. </title> <booktitle> In Proceedings of IJCAI89, </booktitle> <year> 1989. </year>
Reference-contexts: probability to get a positive reinforcement when executing an action a, given the actual situation s, is estimated as follows 2 : P (s; a) = S A (s; a) + F A (s; a) S N A (s; a) 1 This kind of activity has been called animate perception <ref> [1] </ref>. 2 This measure has been compared with other ones and presents a more successful performance.
Reference: 2. <author> AG Barto, RS Sutton, and CJ. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report TR-89-95, </type> <institution> University of Massachusets, </institution> <year> 1989. </year>
Reference-contexts: policy table which will be used to increase the system's set of actions and to achieve solving more complex problems. 3.1 Relation of the new algorithm to other works The proposed algorithm presents several differences respect to other well known reinforcement learning mechanisms, specially with the TD family [16], [18], <ref> [2] </ref>. Most of these reinforcement algorithms find optimum solutions for a task, for instance, finding the shortest path to goal position in a concrete maze. Unfortunately, these mechanism are not well suited to learn general behaviors with incomplete perceptual information. Under these constraints, the concept of optimization loses its sense.
Reference: 3. <author> A.R. Cassandra, </author> <title> L.P. Kaelbling, and M.L. Littman. Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: Other works in reinforcement learning deal with the perceptual aliasing problem by extending the TD methods. Some try to disambiguate the state where the system currently is by a memory of past actions [12], others by building a model of the environment [13], <ref> [3] </ref> and others avoiding the ambiguous states [19]. These methods cannot help in the mentioned situation for the task of finding a block. Memory cannot help because we can face this situation with independence of the past actions taken.
Reference: 4. <author> D. Chapman. </author> <title> Penguins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 45-50, </pages> <month> winter </month> <year> 1989. </year>
Reference-contexts: The intermediate tasks the system must learn in order to solve a complex task will be given opportunely to the system by a teacher. On the other hand, it is known from the debate Ginsberg-Chapman [8] <ref> [4] </ref> that, for solving of complex tasks, it is necessary to generalize situations. In complex tasks, the number of different situations is too large to consider a response for each of them. Chap-man [4] shows that a limited world perception by means of deictic sensors can be indicated for reactive systems <p> On the other hand, it is known from the debate Ginsberg-Chapman [8] <ref> [4] </ref> that, for solving of complex tasks, it is necessary to generalize situations. In complex tasks, the number of different situations is too large to consider a response for each of them. Chap-man [4] shows that a limited world perception by means of deictic sensors can be indicated for reactive systems solving complex tasks, producing a very profitable generalization process. This limited perception implies that the information of the environment the system has, is always partial, incomplete and ambiguous. <p> This environment has also been used previously by Chapman <ref> [4] </ref> and Whitehead and Ballard [19] to show the possibilities and drawbacks of reactive systems in complex domains. A standard setting of the environment (see figure 2) is composed of a window where we can see a table and blocks of different colors resting on it or forming stacks. <p> The learning cycle consists in presenting randomly generated new situations (settings) of the environment to the system from which to solve the task. 5 With this action, substituting colors by letters, it is not very hard to solve the fruitcake problem <ref> [4] </ref>. 9 In the graphic for this task (figure 3a), the learning of the system can be observed. Near the 2.000 presentations of different settings the learning rate is stabilized at the 90% of successes.
Reference: 5. <author> D. Chapman and L. Kaelbling. </author> <title> Learning from delayed reinforcement in a complex domain. </title> <type> Technical Report 90-11, </type> <institution> Teleos Research, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction One important issue in reactive systems research is the learning ability to solve complex tasks. Several mechanisms has been proposed to make reactive systems learn, for instance [16] <ref> [5] </ref> (for a review see [10]). These mechanisms consist in obtaining, usually by means of a trialanderror process guided by a reinforcement signal, a mapping between the set of possible perceptions and the set of actions, that describes an adequate behavior.
Reference: 6. <author> M. Colombetti and M. Dorigo. </author> <title> Robot shaping: Developing situated agents through learning. </title> <type> Technical Report 92-040 revised, </type> <institution> International Computer Science Institute, </institution> <year> 1993. </year>
Reference-contexts: An example of this construction is exposed in the next section. The using of hierarchies of actions to solve complex problems is not a new idea. The approach we use is similar to the "shaping" of animats <ref> [6] </ref> or the construction of complex behaviors from low level ones [11] [14]. These methods has been used for learning from a immediate reinforcement or for learning optimal actions, but not for the learning of general behaviors from a delayed reinforcement.
Reference: 7. <author> P. Dayan and G.E. Hinton. </author> <title> Feudal reinforcement learning. </title> <booktitle> In Proceedings of NIPS'5. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: These methods has been used for learning from a immediate reinforcement or for learning optimal actions, but not for the learning of general behaviors from a delayed reinforcement. Other works as <ref> [7] </ref> [9] also learn to solve complex tasks by decomposing it not in simpler behaviors but in the state space. Nevertheless, in all these methods (including our method) is necessary to give a decomposition of the 7 task.
Reference: 8. <author> M.L. Ginsberg. </author> <title> Universal planning: An (almost) universally bad idea. </title> <journal> AI Magazine, </journal> <volume> 10 </volume> <pages> 41-44, </pages> <month> winter </month> <year> 1989. </year>
Reference-contexts: The intermediate tasks the system must learn in order to solve a complex task will be given opportunely to the system by a teacher. On the other hand, it is known from the debate Ginsberg-Chapman <ref> [8] </ref> [4] that, for solving of complex tasks, it is necessary to generalize situations. In complex tasks, the number of different situations is too large to consider a response for each of them.
Reference: 9. <author> L.P. Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: These methods has been used for learning from a immediate reinforcement or for learning optimal actions, but not for the learning of general behaviors from a delayed reinforcement. Other works as [7] <ref> [9] </ref> also learn to solve complex tasks by decomposing it not in simpler behaviors but in the state space. Nevertheless, in all these methods (including our method) is necessary to give a decomposition of the 7 task.
Reference: 10. <author> L.P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction One important issue in reactive systems research is the learning ability to solve complex tasks. Several mechanisms has been proposed to make reactive systems learn, for instance [16] [5] (for a review see <ref> [10] </ref>). These mechanisms consist in obtaining, usually by means of a trialanderror process guided by a reinforcement signal, a mapping between the set of possible perceptions and the set of actions, that describes an adequate behavior. Unfortunately, these methods are not well suited to learn how to solve complex tasks.
Reference: 11. <author> L.J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year> <month> 13 </month>
Reference-contexts: An example of this construction is exposed in the next section. The using of hierarchies of actions to solve complex problems is not a new idea. The approach we use is similar to the "shaping" of animats [6] or the construction of complex behaviors from low level ones <ref> [11] </ref> [14]. These methods has been used for learning from a immediate reinforcement or for learning optimal actions, but not for the learning of general behaviors from a delayed reinforcement.
Reference: 12. <author> L.J. Lin and T.M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non--markovian domains. </title> <type> Technical Report CS-92-138, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Other works in reinforcement learning deal with the perceptual aliasing problem by extending the TD methods. Some try to disambiguate the state where the system currently is by a memory of past actions <ref> [12] </ref>, others by building a model of the environment [13], [3] and others avoiding the ambiguous states [19]. These methods cannot help in the mentioned situation for the task of finding a block. Memory cannot help because we can face this situation with independence of the past actions taken.
Reference: 13. <author> R.A. McCallum. </author> <title> Learning with incomplete selective perception. </title> <type> Technical Report TR-453, </type> <institution> University of Rochester, </institution> <year> 1993. </year>
Reference-contexts: Other works in reinforcement learning deal with the perceptual aliasing problem by extending the TD methods. Some try to disambiguate the state where the system currently is by a memory of past actions [12], others by building a model of the environment <ref> [13] </ref>, [3] and others avoiding the ambiguous states [19]. These methods cannot help in the mentioned situation for the task of finding a block. Memory cannot help because we can face this situation with independence of the past actions taken.
Reference: 14. <author> P.S. Singh. </author> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> (8):323-340, 1992. 
Reference-contexts: An example of this construction is exposed in the next section. The using of hierarchies of actions to solve complex problems is not a new idea. The approach we use is similar to the "shaping" of animats [6] or the construction of complex behaviors from low level ones [11] <ref> [14] </ref>. These methods has been used for learning from a immediate reinforcement or for learning optimal actions, but not for the learning of general behaviors from a delayed reinforcement.
Reference: 15. <author> M.I. Jordan S.P. Singh, T. Jaakkola. </author> <title> Learning without state-estimation in partially observable markovian decision processes. </title> <booktitle> In Proceedings of the Eleventh Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: As we have seen, the TD () family is not well suited to the framework we are describing. The main problem of this approach is the perceptual aliasing problem that implies to face with a non-Markovian task <ref> [15] </ref>. This problem difficults to use as an estimation of the future reinforcement, the maximum estimation of the neighbor states, because the system does not know which are these states. Nevertheless, these methods are used in robotics in tasks with this perceptual problem.
Reference: 16. <author> R.S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction One important issue in reactive systems research is the learning ability to solve complex tasks. Several mechanisms has been proposed to make reactive systems learn, for instance <ref> [16] </ref> [5] (for a review see [10]). These mechanisms consist in obtaining, usually by means of a trialanderror process guided by a reinforcement signal, a mapping between the set of possible perceptions and the set of actions, that describes an adequate behavior. <p> in the policy table which will be used to increase the system's set of actions and to achieve solving more complex problems. 3.1 Relation of the new algorithm to other works The proposed algorithm presents several differences respect to other well known reinforcement learning mechanisms, specially with the TD family <ref> [16] </ref>, [18], [2]. Most of these reinforcement algorithms find optimum solutions for a task, for instance, finding the shortest path to goal position in a concrete maze. Unfortunately, these mechanism are not well suited to learn general behaviors with incomplete perceptual information.
Reference: 17. <author> S. Thrun. </author> <title> Finding structure in reinforcement learning. </title> <booktitle> In Proceedings of NIPS'7, </booktitle> <year> 1995. </year>
Reference-contexts: We are planning to automatize this process with genetics algorithms to give us the necessary information for the construction of the hierarchy of behaviors. The only work we are aware that automatically learns general behaviors from delayed reinforcement is <ref> [17] </ref>. Unfortunately, it is necessary to know how to solve complex tasks for find these general behaviors.
Reference: 18. <author> C. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <year> 1989. </year>
Reference-contexts: This limited perception implies that the information of the environment the system has, is always partial, incomplete and ambiguous. Most learning mechanisms, for instance Q-learning <ref> [18] </ref>, suppose the system having complete information about its environment. 2 When these methods have to deal with incomplete information or ambiguities, the prob-lem of perceptual aliasing [19] appears, disabling the learning process. <p> the policy table which will be used to increase the system's set of actions and to achieve solving more complex problems. 3.1 Relation of the new algorithm to other works The proposed algorithm presents several differences respect to other well known reinforcement learning mechanisms, specially with the TD family [16], <ref> [18] </ref>, [2]. Most of these reinforcement algorithms find optimum solutions for a task, for instance, finding the shortest path to goal position in a concrete maze. Unfortunately, these mechanism are not well suited to learn general behaviors with incomplete perceptual information. <p> Nevertheless, these methods are used in robotics in tasks with this perceptual problem. An empirical comparison has been made between a close version of TD (0), Q-learning <ref> [18] </ref>, and our method. It can be seen in section 4, figure 3. The Q-learning mechanism has been modified to make it resistant to perceptual aliasing and optimization. The modifications prevents exploration and annuls the need for optimization by setting fl = 1.
Reference: 19. <author> S.D. Whitehead and D.H. Ballard. </author> <title> Learning to percive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Most learning mechanisms, for instance Q-learning [18], suppose the system having complete information about its environment. 2 When these methods have to deal with incomplete information or ambiguities, the prob-lem of perceptual aliasing <ref> [19] </ref> appears, disabling the learning process. Then, an important feature that our learning process must show is the ability for learning with limited world perception in order to solve complex tasks. <p> Some try to disambiguate the state where the system currently is by a memory of past actions [12], others by building a model of the environment [13], [3] and others avoiding the ambiguous states <ref> [19] </ref>. These methods cannot help in the mentioned situation for the task of finding a block. Memory cannot help because we can face this situation with independence of the past actions taken. <p> This environment has also been used previously by Chapman [4] and Whitehead and Ballard <ref> [19] </ref> to show the possibilities and drawbacks of reactive systems in complex domains. A standard setting of the environment (see figure 2) is composed of a window where we can see a table and blocks of different colors resting on it or forming stacks.
References-found: 19

