URL: http://ai.eecs.umich.edu/people/durfee/courses/592winter98/readings/ijcai95-apn.ps
Refering-URL: http://ai.eecs.umich.edu/people/durfee/courses/592winter98/index.html
Root-URL: http://www.cs.umich.edu
Title: Local learning in probabilistic networks with hidden variables  
Author: Stuart Russell, John Binder, Daphne Koller, Keiji Kanazawa 
Address: Berkeley, CA 94720, USA  
Affiliation: Computer Science Division University of California  
Abstract: Probabilistic networks, which provide compact descriptions of complex stochastic relationships among several random variables, are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. We show that networks with fixed structure containing hidden variables can be learned automatically from data using a gradient-descent mechanism similar to that used in neural networks. We also extend the method to networks with intensionally represented distributions, including networks with continuous variables and dynamic probabilistic networks. Because probabilistic networks provide explicit representations of causal structure, human experts can easily contribute prior knowledge to the training process, thereby significantly improving the learning rate. Adaptive probabilistic networks (APNs) may soon compete directly with neural networks as models in computational neuroscience as well as in industrial and financial applications.
Abstract-found: 1
Intro-found: 1
Reference: [ Buntine, 1994 ] <author> Wray L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: The existence of localized gradient descent algorithms for both adaptive probabilistic networks and back-propagation neural networks is no accident. In other work, we have established general conditions under which any distributed computational system is amenable to local learning (see also <ref> [ Buntine, 1994 ] </ref> ). Such results suggest that the class of abstract models considered in computational neuroscience can be broadened considerably.
Reference: [ Cooper and Herskovits, 1992 ] <author> G. Cooper and E. </author> <note> Herskovits. </note>
Reference-contexts: The case of unknown structure and fully observable variables has also received some attention. In this case, the problem is to reconstruct the topology of the networka discrete optimization problem usually solved by a greedy search in the space of structures <ref> [ Cooper and Herskovits, 1992; Heckerman et al., 1994 ] </ref> . For any given proposed structure, the CPTs can be reconstructed as described above. The resulting algorithms are capable of recovering fairly large networks from large data sets with a high degree of accuracy.
References-found: 2

