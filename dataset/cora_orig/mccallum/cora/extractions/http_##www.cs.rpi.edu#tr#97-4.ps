URL: http://www.cs.rpi.edu/tr/97-4.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Email: bubnak@cs.rpi.edu stewart@cs.rpi.edu  
Title: Model Selection and Surface Merging in Reconstruction Algorithms  
Author: Kishore Bubna Charles V. Stewart 
Note: 1 The authors would like to thank James Miller for helpful discussions and comments. They would also like to acknowledge the financial support of the National Science Foundation under grant IRI-9408700.  
Address: 110, 8th Street Troy, NY 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> Information theory and an extension of the maximum likelihood principle. </title> <editor> In B. N. Petrov and F. Csaki, editors, </editor> <booktitle> 2nd International Symposium of Information Theory, </booktitle> <pages> pages 267-281. </pages> <publisher> Akademiai Kiado, </publisher> <year> 1973. </year>
Reference-contexts: The Akaike Information Criterion (AIC) <ref> [1] </ref> approximates (6) by While AIC has not been used in surface reconstruction, [9] uses a popular variant of AIC, CAIC [10] d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1): (8) We study both CAIC and AIC here.
Reference: [2] <author> F. Arman and J. K. Aggarwal. </author> <title> Model-based object recognition in dense-range images a review. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(1) </volume> <pages> 5-43, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: But, experiments with our sensors show that for relatively small fields of view (a viewing cone of 25 degrees or less), the errors can be approximated to be along the depth (z) direction. Almost all other algorithms assume the same <ref> [2] </ref>, and have not reported any problems. 3 the n data points, and satisfies the relation 2 n X p (x i ) q (x j ) = 0; for p 6= q: (2) Parameter estimation for model selection: Model selection techniques fit each of the candidate models to the data,
Reference: [3] <author> R. H. Bartels and J. J. Jezioranski. </author> <title> Least-squares fitting using orthogonal multinomials. </title> <journal> ACM Trans actions on Mathematical Software, </journal> <volume> 11(3) </volume> <pages> 201-217, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: First, when fitting higher order models to data belonging to a low order surface, the design matrix may be ill-conditioned. Second, fitting different models to a data set is computationally expensive. To alleviate these problems, we fit using orthogonal polynomials <ref> [3, 4] </ref>. Least-squares estimation using orthogonal polynomials gives well-conditioned matrices, and is efficient because the fit to high order models builds on fits to lower order models. <p> The models in M use discrete orthogonal polynomials as basis functions <ref> [3, 5] </ref>, and are given by z (x) = j=0 where d m is the number of parameters in the model. The parameter vector then is given by m = [ 0 1 : : : d m 1 ] T . <p> For perspective projection measurements this assumption is violated, and we derive orthogonal polynomials using the method given in <ref> [3] </ref>. 3 Model selection This section describes model selection criteria already used in reconstruction algorithms and introduces several promising criteria from the statistics literature.
Reference: [4] <author> P. J. Besl. </author> <title> Surfaces in Range Image Understanding. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: First, when fitting higher order models to data belonging to a low order surface, the design matrix may be ill-conditioned. Second, fitting different models to a data set is computationally expensive. To alleviate these problems, we fit using orthogonal polynomials <ref> [3, 4] </ref>. Least-squares estimation using orthogonal polynomials gives well-conditioned matrices, and is efficient because the fit to high order models builds on fits to lower order models. <p> When in (5) is unknown, the log likelihood is written as log L ( m ; m ) and ^ m is estimated using IRLS (see [9] for details). 2 Besl <ref> [4, 5] </ref> derived orthogonal polynomials by assuming that x 1 ; : : : ; x n are equally spaced. <p> The test rejects model m if r m is not within a 95% level of confidence. Since the RUNS test does not generalize to 3D range images, Besl <ref> [4, pages 150-152] </ref> introduces a heuristic approximation.
Reference: [5] <author> P. J. Besl, J. B. Birch, and L. T. Watson. </author> <title> Robust window operators. </title> <booktitle> In ICCV, </booktitle> <pages> pages 591-600, </pages> <year> 1988. </year>
Reference-contexts: The models in M use discrete orthogonal polynomials as basis functions <ref> [3, 5] </ref>, and are given by z (x) = j=0 where d m is the number of parameters in the model. The parameter vector then is given by m = [ 0 1 : : : d m 1 ] T . <p> When in (5) is unknown, the log likelihood is written as log L ( m ; m ) and ^ m is estimated using IRLS (see [9] for details). 2 Besl <ref> [4, 5] </ref> derived orthogonal polynomials by assuming that x 1 ; : : : ; x n are equally spaced.
Reference: [6] <author> P. J. Besl and R. C. Jain. </author> <title> Segmentation through variable-order surface fitting. </title> <journal> IEEE PAMI, </journal> <volume> 10:167 192, </volume> <year> 1988. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic <ref> [6, 8, 36, 43] </ref> and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria [7, 9, 16, 27, 29, 30, 41, 46, 47]. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods [22, 36] or by region growing <ref> [6, 9, 29, 43] </ref>. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. <p> For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds <ref> [6, 29, 43] </ref>, Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test <ref> [6, 8] </ref>, Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries <ref> [6, 13, 36, 43, 44] </ref>. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> Merging decisions, on the other hand, must balance between bridging true surface boundaries and leaving artificial surface boundaries, a difficult problem illustrated by the examples shown in Figure 1. In an attempt to avoid the model selection problem, many merging techniques only join surfaces fit to the same model <ref> [6, 36, 44, 13, 26] </ref>, potentially limiting the effectiveness of the merging process. Surface reconstruction, therefore, provides a good context for studying the model selection problem in computer vision. <p> The intuition is that low order incorrect models will produce a significant over-estimate to the error in the data. BESL: This test combines RUNS and CHI, and rejects model m if both of them fail. This is the model selection criteria used by Besl and Jain <ref> [6] </ref>.
Reference: [7] <author> R. M. Bolle and D. B. Cooper. </author> <title> Bayesian recognition of local 3-D shape by approximating image intensity functions with quadric polynomials. </title> <journal> IEEE PAMI, </journal> <volume> 6(4) </volume> <pages> 418-429, </pages> <year> 1984. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule <ref> [7, 14, 41, 47] </ref>, Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [8] <author> R. C. Bolles and M. A. Fischler. </author> <title> A RANSAC-based approach to model fitting and its applications to finding cylinders in range data. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 637-643, </pages> <year> 1981. </year>
Reference-contexts: In each of these domains, the appropriate model must be chosen automatically if it is not dictated by prior knowledge. This model selection problem has received much less attention than the associated problem of estimating model parameters <ref> [8, 9, 32, 33, 39] </ref>, yet without a proper model, the estimated parameters have little meaning. The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. <p> The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic <ref> [6, 8, 36, 43] </ref> and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria [7, 9, 16, 27, 29, 30, 41, 46, 47]. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test <ref> [6, 8] </ref>, Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> BESL: This test combines RUNS and CHI, and rejects model m if both of them fail. This is the model selection criteria used by Besl and Jain [6]. RANSAC: This test <ref> [8] </ref> rejects model m for any one of three reasons: (a) CHI fails (this replaces the error-tolerance test using the RANSAC metric), (b) Reject m at 95% confidence level when jp m n m j &gt; 2 p n, and (c) Reject m at 95% confidence level when the longest run
Reference: [9] <author> K. L. Boyer, M. J. Mirza, and G. Ganguly. </author> <title> The robust sequential estimator: A general approach and its application to surface organization in range data. </title> <journal> IEEE PAMI, </journal> <volume> 16(10) </volume> <pages> 987-1001, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In each of these domains, the appropriate model must be chosen automatically if it is not dictated by prior knowledge. This model selection problem has received much less attention than the associated problem of estimating model parameters <ref> [8, 9, 32, 33, 39] </ref>, yet without a proper model, the estimated parameters have little meaning. The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. <p> The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> Finally, model selection criteria in vision must tolerate outliers <ref> [9, 16] </ref>, unknown noise distributions, and other kinds of unmodeled errors in the data [15]. One computer vision problem where model selection techniques are crucial is surface reconstruction. <p> This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods [22, 36] or by region growing <ref> [6, 9, 29, 43] </ref>. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance <ref> [9] </ref>, and minimum description lengths (MDL) [16, 27, 29, 30]. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44]. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces <ref> [9, 29] </ref>, or creates artificial surface boundaries [6, 13, 36, 43, 44]. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> We use ordinary least-squares for data with Gaussian errors, and M-estimators, solved using iteratively reweighted least squares (IRLS) [23], for data with random errors and outliers. Only M-estimators that are maximum likelihood estimators [20, page 361-362] can be used in model selection. Following Boyer, Mirza, and Ganguly <ref> [9] </ref>, we use the M-estimator based on a t-distribution. The loglikelihood when e ~ N (0; 2 I n ) and when is known a priori is log L ( m ) = 2 1 T Maximization yields the estimate ^ m . <p> When in (4) is not known, the log likelihood is written as log L ( m ; m ) and maximization yields both ^ m and ^ m . In the presence of outliers, the errors are reasonably represented by a t-distribution <ref> [9] </ref>, having degree of freedom f and scaled by , giving log L ( m ) = 2 i=1 e 2 f 2 ; w mi = f + (e mi =) 2 : (5) Here, e mi is the error for the ith observation. ^ m is estimated from (5) <p> When in (5) is unknown, the log likelihood is written as log L ( m ; m ) and ^ m is estimated using IRLS (see <ref> [9] </ref> for details). 2 Besl [4, 5] derived orthogonal polynomials by assuming that x 1 ; : : : ; x n are equally spaced. <p> The Akaike Information Criterion (AIC) [1] approximates (6) by While AIC has not been used in surface reconstruction, <ref> [9] </ref> uses a popular variant of AIC, CAIC [10] d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1): (8) We study both CAIC and AIC here. <p> For model selection, we test each ground-truth segment, as well as regions of different sizes within certain segments. Similarly, for merging, we test each ground-truth segment with its adjacent segments, and also test adjacent regions within certain segments. As mentioned before, we assume errors are t-distributed (following <ref> [9] </ref>, f = 1:5), and is unknown. As a result, criteria that require knowledge of a priori or assume a Gaussian error distribution cannot be used. Also, the current versions of our bootstrap based criteria cannot be used in the presence of outliers.
Reference: [10] <author> H. Bozdogan. </author> <title> Model selection and Akaike's information criterion (AIC): The general theory and its analytical extensions. </title> <journal> Psychometrika, </journal> <volume> 52 </volume> <pages> 345-370, </pages> <year> 1987. </year>
Reference-contexts: The Akaike Information Criterion (AIC) [1] approximates (6) by While AIC has not been used in surface reconstruction, [9] uses a popular variant of AIC, CAIC <ref> [10] </ref> d ( ^ m ; fl ) 2 log L ( ^ m ) + d m (log n + 1): (8) We study both CAIC and AIC here. When is unknown, L ( ^ ; ^) replaces L ( ^ ) in (7) and (8).
Reference: [11] <author> K. A. Brownlee. </author> <title> Statistical Theory and Methodology in Science and Engineering. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1960. </year>
Reference-contexts: For 2D range images, the total number of runs, r m , for any fit ^ m , is asymptotically 3 normally distributed and is given by <ref> [11, pages 164-170] </ref> r m ~ N 2p m q m + 1; (p m + q m ) 2 (p m + q m 1) : Here, p m is the number of positive residuals, and n m is the number of negative residuals in the fit.
Reference: [12] <author> J. Cabrera and P. Meer. </author> <title> Unbiased estimation of ellipses by bootstrapping. </title> <journal> IEEE PAMI, </journal> <volume> 18(7):752 756, </volume> <year> 1996. </year>
Reference-contexts: The bootstrap is a method for estimating an unknown distribution from available data. Not yet popular in computer vision <ref> [12] </ref>, this technique was introduced in statistics by Efron [18]. In linear regression, the bootstrap technique can be used to obtain an empirical distribution of errors in the data, and this distribution can then be used to generate different statistics on Z and ^ m .
Reference: [13] <author> F. S. Cohen and R. D. Rimey. </author> <title> A maximum likelihood approach to segmenting range data. </title> <booktitle> In IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 1696-1701, </pages> <year> 1988. </year>
Reference-contexts: This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid <ref> [13, 44] </ref>, by clustering methods [22, 36] or by region growing [6, 9, 29, 43]. Model selection is an important part of this initial estimation step. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries <ref> [6, 13, 36, 43, 44] </ref>. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> Merging decisions, on the other hand, must balance between bridging true surface boundaries and leaving artificial surface boundaries, a difficult problem illustrated by the examples shown in Figure 1. In an attempt to avoid the model selection problem, many merging techniques only join surfaces fit to the same model <ref> [6, 36, 44, 13, 26] </ref>, potentially limiting the effectiveness of the merging process. Surface reconstruction, therefore, provides a good context for studying the model selection problem in computer vision.
Reference: [14] <author> F. S. Cohen and J.-Y. Wang. </author> <title> Modeling image curves using 3-D object curve models a path to 3-D recognition and shape estimation from image contours. </title> <journal> IEEE PAMI, </journal> <volume> 16(1) </volume> <pages> 1-12, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule <ref> [7, 14, 41, 47] </ref>, Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [15] <author> B. Curless and M. Levoy. </author> <title> Better optical triangulation through spacetime analysis. </title> <booktitle> In ICCV, </booktitle> <pages> pages 987-993, </pages> <address> Boston, MA, </address> <year> 1995. </year> <month> 24 </month>
Reference-contexts: Finally, model selection criteria in vision must tolerate outliers [9, 16], unknown noise distributions, and other kinds of unmodeled errors in the data <ref> [15] </ref>. One computer vision problem where model selection techniques are crucial is surface reconstruction. Many reconstruction algorithms use a local-to-global approach in which parameter estimation techniques and local decision criteria are combined in a greedy surface recovery strategy. This approach generally consists of two steps.
Reference: [16] <author> T. Darrell and A. Pentland. </author> <title> Cooperative robust estimation using layers of support. </title> <journal> IEEE PAMI, </journal> <volume> 17(5) </volume> <pages> 474-487, </pages> <year> 1995. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> Finally, model selection criteria in vision must tolerate outliers <ref> [9, 16] </ref>, unknown noise distributions, and other kinds of unmodeled errors in the data [15]. One computer vision problem where model selection techniques are crucial is surface reconstruction. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) <ref> [16, 27, 29, 30] </ref>. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [17] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> Wiley Publications, </publisher> <year> 1973. </year>
Reference-contexts: len m = log 2 L ( ^ m ) + 2 T where log 2 fl (t) = log 2 t+ log 2 log 2 t+ : : : , including only its positive terms, and V d m is the volume of the d m -dimensional unit hypersphere <ref> [17, page 24] </ref>. When is not known, (14) becomes len m = log 2 L ( ^ m ; ^) + 2 T T Here H ( ^ m ; ^) is the Hessian of log 2 L ( m ; ) at the maximum likelihood estimates.
Reference: [18] <author> B. Efron and R. J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: In computer vision problems, however, error distributions are often unknown and difficult to model accurately. Because of this, it is crucial to develop model selection criteria that depend on only weak assumptions about error distributions. We address this problem in this section by deriving bootstrap <ref> [18] </ref> versions of the criteria in Section 3.1. The only assumptions are that the errors are zero-mean and independent. <p> The bootstrap is a method for estimating an unknown distribution from available data. Not yet popular in computer vision [12], this technique was introduced in statistics by Efron <ref> [18] </ref>. In linear regression, the bootstrap technique can be used to obtain an empirical distribution of errors in the data, and this distribution can then be used to generate different statistics on Z and ^ m . The idea of bootstrap is simple. <p> The plug-in bootstrap principle <ref> [18, chapter 4] </ref> substitutes P with ^ P m and this is used to generate R bootstrap error vectors, e fl 1m ; : : : ; e fl Rm . <p> Our sensor has a of about 0.02 cm at a depth of 100 cm, in our experiments we vary from 0.02 to 0.1 cm. The results are based on 500 simulations, and for bootstrap criteria, the number of bootstrap replications, R, is set to 200 <ref> [18] </ref>. Effect of region size and on performance: In this set of experiments, a 0 = 100 and a 1 = 1 (for both the models), and a 2 = 0:1 for the quadratic model.
Reference: [19] <author> A. W. Fitzgibbon and R. B. Fisher. </author> <title> Lack-of-fit detection using the run-distribution test. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 173-178, </pages> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: He creates binary images of positive and negative residuals, erodes the images using a 3 fi 3 kernel, finds the largest connected component in each image, and rejects the null hypotheses 3 For small samples, techniques from [42] and <ref> [19] </ref> may be used. 7 if the larger of these components is greater than 2% of n. We follow this heuristic for 3D range images. The runs test is advantageous when and the noise distribution of the data are unknown.
Reference: [20] <author> C. Goodall. </author> <title> M-estimators of location: an outline of the theory. </title> <editor> In D. C. Hoaglin, F. Mosteller, and J. W. Tukey, editors, </editor> <title> Understanding Robust and Exploratory Data Analysis, chapter 11. </title> <publisher> John Wiley and Sons, </publisher> <year> 1983. </year>
Reference-contexts: We use ordinary least-squares for data with Gaussian errors, and M-estimators, solved using iteratively reweighted least squares (IRLS) [23], for data with random errors and outliers. Only M-estimators that are maximum likelihood estimators <ref> [20, page 361-362] </ref> can be used in model selection. Following Boyer, Mirza, and Ganguly [9], we use the M-estimator based on a t-distribution.
Reference: [21] <author> F. Gustafsson and H. Hjalmarsson. </author> <title> Twenty-one ML estimators for model selection. </title> <journal> Automatica, </journal> <volume> 31 </volume> <pages> 1377-1392, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: P ( m jm; I) is the prior probability of m . Since reconstruction applications generally lack prior information on parameters, we use a uniform prior on m (see <ref> [21, appendix A] </ref>). When is known, its prior, P (jI), is a delta function at the known , and (9) reduces to an integral with respect to m only. <p> For t-distributed errors, assuming a uniform prior on <ref> [21] </ref>, (9) reduces to P (Djm; I) = (2) d m =2 L ( ^ m ; ^) [j H ( ^ m ; ^)j] 1=2 ; (12) where H ( ^ m ; ^) is the Hessian of log L ( m ; ) at the maximum likelihood estimates ^
Reference: [22] <author> R. Hoffman and A. Jain. </author> <title> Segmentation and classification of range images. </title> <journal> IEEE PAMI, </journal> <volume> 9 </volume> <pages> 608-620, </pages> <year> 1987. </year>
Reference-contexts: This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods <ref> [22, 36] </ref> or by region growing [6, 9, 29, 43]. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model.
Reference: [23] <author> P. W. Holland and R. E. Welsch. </author> <title> Robust regression using iteratively reweighted least-squares. Com mun. </title> <journal> Statist.-Theor. Meth., </journal> <volume> A6:813-827, </volume> <year> 1977. </year>
Reference-contexts: Since many model selection criteria are based on the loglikelihood of the estimated parameters, maximum likelihood estimators must be used. We use ordinary least-squares for data with Gaussian errors, and M-estimators, solved using iteratively reweighted least squares (IRLS) <ref> [23] </ref>, for data with random errors and outliers. Only M-estimators that are maximum likelihood estimators [20, page 361-362] can be used in model selection. Following Boyer, Mirza, and Ganguly [9], we use the M-estimator based on a t-distribution.
Reference: [24] <author> A. Hoover, G. Jean-Baptiste, X. Jiang, P. Flynn, H. Bunke, D. Goldgof, K. Bowyer, D. Eggert, A. Fitzgibbon, and R. Fisher. </author> <title> An experimental comparison of range image segmentation algorithms. </title> <journal> IEEE PAMI, </journal> <volume> 18 </volume> <pages> 673-689, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Table 6 gives a qualitative summary of relative performances. 7 Results using real data This section compares different model selection criteria and merging rules using one of the Perceptron test data sets from the University of South Florida's Segmentation Comparison Project <ref> [24] </ref> (see Figure 10 (a)). This data set, consisting of planar surfaces, is particularly suitable because it provides ground-truth segmentation.
Reference: [25] <author> E. T. Jaynes. </author> <booktitle> Probability Theory the Logic of Science. Physics, </booktitle> <address> Washington University, St. Louis, MO 63130, USA, http://omega.albany.edu:8008/JaynesBook.html, 1994. </address>
Reference-contexts: When is known, its prior, P (jI), is a delta function at the known , and (9) reduces to an integral with respect to m only. Solving this reduced integral using a second order Taylor's expansion of log L ( m ) at ^ m <ref> [25, chapter 24] </ref> yields P (Djm; I) (2) d m =2 L ( ^ m )[j H ( ^ m )j] 1=2 ; (10) where H ( ^ m ) is the Hessian of log L ( m ) at ^ m . <p> When is not known, we need to assign P (jI). Again, we use non-informative priors on P (jI). For the Gaussian case, using the non-informative 5 prior 1= for (see <ref> [25, chapter 6, page 29] </ref>), and assigning other probabilities as before, (9) reduces to P (Djm; I) = 2 (d m =2)+1 n=2 jX T (nd m )=2 ; (11) where RSS m is the residual sum of squares for model m. <p> H ( ^ m ) is also needed for BAYES (10) and for RISS (14). To obtain a distribution-free measure of H ( ^ m ), observe that H ( ^ m ) [V ( ^ m )] 1 , the covariance matrix of ^ m <ref> [25, chapter 24] </ref>. This makes the problem easy because the bootstrap estimate of V ( ^ m ), V fl ( ^ m ) can be calculated from ^ fl fl Rm .
Reference: [26] <author> S. M. LaValle and S. A. Hutchinson. </author> <title> A Bayesian segmentation methodology for parametric image models. </title> <journal> IEEE PAMI, </journal> <volume> 17(2) </volume> <pages> 211-217, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Merging decisions, on the other hand, must balance between bridging true surface boundaries and leaving artificial surface boundaries, a difficult problem illustrated by the examples shown in Figure 1. In an attempt to avoid the model selection problem, many merging techniques only join surfaces fit to the same model <ref> [6, 36, 44, 13, 26] </ref>, potentially limiting the effectiveness of the merging process. Surface reconstruction, therefore, provides a good context for studying the model selection problem in computer vision. <p> Based on this observation, the rule merges A and B to C if a model from M is selected for C, otherwise, it concludes 5 In surface reconstruction, a Bayesian merging approach has been used by LaValle and Hutchinson <ref> [26] </ref>. However, they only merge surfaces corresponding to the same model. They also restrict the parameter space such that jj m jj = 1. As such, their work can be considered as a special case of ours.
Reference: [27] <author> Y. G. Leclerc. </author> <title> Constructing simple stable descriptions for image partitioning. </title> <journal> IJCV, </journal> <volume> 3 </volume> <pages> 73-102, </pages> <year> 1989. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) <ref> [16, 27, 29, 30] </ref>. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [28] <author> A. Leonardis. </author> <title> Image Analysis Using Parametric Models : Model-Recovery and Model-Selection Paradigm. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, </institution> <year> 1993. </year>
Reference-contexts: The second term in (16), K 2 RSS m , is equivalent to log 2 L ( ^ i ) (see <ref> [28, page 65] </ref>), yielding K 1 n + log 2 L ( ^ m ) K 3 d m (17) For model selection using a fixed data set, n is also fixed, allowing (17) to be simplified to log 2 L ( ^ m ) K 3 d m (18) Although
Reference: [29] <author> A. Leonardis, A. Gupta, and R. </author> <title> Bajcsy. Segmentation of range images as the search for geometric parametric models. </title> <journal> IJCV, </journal> <volume> 14 </volume> <pages> 253-277, </pages> <year> 1995. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods [22, 36] or by region growing <ref> [6, 9, 29, 43] </ref>. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. <p> For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds <ref> [6, 29, 43] </ref>, Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) <ref> [16, 27, 29, 30] </ref>. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44]. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces <ref> [9, 29] </ref>, or creates artificial surface boundaries [6, 13, 36, 43, 44]. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> Criteria (14) and (15), later referred as RISS, have not been used in the vision literature. In surface reconstruction, Leonardis, Gupta and Bajcsy <ref> [29] </ref> uses a MDL based criterion in a quadratic optimization function. <p> For model selection using a fixed data set, n is also fixed, allowing (17) to be simplified to log 2 L ( ^ m ) K 3 d m (18) Although theoretically K 3 is the number of bits required to encode d m , in practice the algorithm in <ref> [29] </ref> chooses the value of K 3 empirically. To compare the performance of (18) with other criteria, we need to choose a value for K 3 . <p> In this regard, comparing (18) with AIC (7), we see that when K 3 = 1= log 2, maximizing (18) is equivalent to minimizing (7). Thus, the model selection criterion used in <ref> [29] </ref> is likely to show similar properties as that obtained using AIC. As such, we use AIC to study the properties of (16). 3.2 Model selection using Chi-square, F, and runs test A number of model selection criteria that have been used in reconstruction algorithms are based on hypothesis tests. <p> As such, their work can be considered as a special case of ours. Another difference is that they use a precise non-informative prior on m , and solve the integral (9) numerically. 6 Section 3.1 shows how the model selection criteria used in the optimization function in <ref> [29] </ref> is equivalent to AIC when K 3 = 1= log 2. This function can be used for merging surfaces, and turns out to be equivalent to (22) used with AIC. 11 (a) (b) (c) (d) from quadratic model at different a 2 .
Reference: [30] <author> M. Li. </author> <title> Minimum description length based 2D shape description. </title> <booktitle> In ICCV, </booktitle> <pages> pages 512-517, </pages> <year> 1993. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) <ref> [16, 27, 29, 30] </ref>. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [31] <author> K. V. Mardia, J. T. Kent, and J. M. Bibby. </author> <title> Multivariate Analysis. </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: Model selection using Bayes rule: Criteria based on Bayes rule choose the model that maximizes the probability of the data, D, given the model m and prior information I. This probability is denoted by P (Djm; I). Using Bayes rule (and assuming m and are independent <ref> [31, page 109] </ref>), Z Z P (Dj m ; ; m; I) in (9) is just the likelihood L ( m ). P ( m jm; I) is the prior probability of m . <p> First consider the accuracy term, which is given by the likelihood. When errors are unknown, ordinary least-squares is used for parameter estimation. More sophisticated estimators are unnecessary because our weak assumptions on sensor noise are sufficient to yield unbiased, minimum variance estimates <ref> [31, page 172] </ref>. The accuracy of the model given the data can then be measured using the normalized residual sum of squares, making prior knowledge of error distribution unnecessary. Therefore, we replace the model accuracy term log L ( ^ m ) with RSS m = 2 .
Reference: [32] <author> P. Meer, D. Mintz, A. Rosenfeld, and D. Y. Kim. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> IJCV, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: In each of these domains, the appropriate model must be chosen automatically if it is not dictated by prior knowledge. This model selection problem has received much less attention than the associated problem of estimating model parameters <ref> [8, 9, 32, 33, 39] </ref>, yet without a proper model, the estimated parameters have little meaning. The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. <p> log L ( m ) = 2 i=1 e 2 f 2 ; w mi = f + (e mi =) 2 : (5) Here, e mi is the error for the ith observation. ^ m is estimated from (5) using IRLS initialized by the least median of squares (LMS) <ref> [32] </ref> estimate of m .
Reference: [33] <author> J. V. Miller and C. V. Stewart. </author> <title> MUSE: Robust surface fitting using unbiased scale estimates. </title> <booktitle> In CVPR, </booktitle> <pages> pages 300-306, </pages> <year> 1996. </year>
Reference-contexts: In each of these domains, the appropriate model must be chosen automatically if it is not dictated by prior knowledge. This model selection problem has received much less attention than the associated problem of estimating model parameters <ref> [8, 9, 32, 33, 39] </ref>, yet without a proper model, the estimated parameters have little meaning. The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds.
Reference: [34] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 468-471, </pages> <year> 1978. </year> <month> 25 </month>
Reference-contexts: The quantities len ( ^ e m ) and len ( ^ m ) are calculated using different assumptions giving rise to different model selection criteria. The most common of these criteria is due to Rissanen <ref> [34] </ref>, and is equivalent to BIC, equation (13).
Reference: [35] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: The quantities len ( ^ e m ) and len ( ^ m ) are calculated using different assumptions giving rise to different model selection criteria. The most common of these criteria is due to Rissanen [34], and is equivalent to BIC, equation (13). In <ref> [35] </ref>, Rissanen derives an improved criterion which chooses the model minimizing len m = log 2 L ( ^ m ) + 2 T where log 2 fl (t) = log 2 t+ log 2 log 2 t+ : : : , including only its positive terms, and V d m
Reference: [36] <author> B. Sabata, F. Arman, and J. K. Aggarwal. </author> <title> Segmentation of 3D range images using pyramidal data structures. </title> <journal> CVGIP:IU, </journal> <volume> 57 </volume> <pages> 373-387, </pages> <year> 1993. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic <ref> [6, 8, 36, 43] </ref> and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria [7, 9, 16, 27, 29, 30, 41, 46, 47]. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods <ref> [22, 36] </ref> or by region growing [6, 9, 29, 43]. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries <ref> [6, 13, 36, 43, 44] </ref>. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> Merging decisions, on the other hand, must balance between bridging true surface boundaries and leaving artificial surface boundaries, a difficult problem illustrated by the examples shown in Figure 1. In an attempt to avoid the model selection problem, many merging techniques only join surfaces fit to the same model <ref> [6, 36, 44, 13, 26] </ref>, potentially limiting the effectiveness of the merging process. Surface reconstruction, therefore, provides a good context for studying the model selection problem in computer vision.
Reference: [37] <author> K. Sato and S. </author> <title> Inokuchi. Range-imaging system utilizing nematic liquid crystal mask. </title> <booktitle> In ICCV, </booktitle> <pages> pages 657-661, </pages> <year> 1987. </year>
Reference-contexts: The data contains Gaussian errors and are generated using focal length=1.77 cm and pixel size=0.0016 cm, the calibration parameters of our range sensor <ref> [37] </ref>. Simulated data allows us to test selection criteria on data from different region sizes and values, and to test merging rules under different step heights and crease angles.
Reference: [38] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: These criteria, (10), (11) and (12), will be referred to as BAYES. To avoid the expense of computing H ( ^ m ; ^), several asymptotic approximations of (10) have been introduced. A common one is BIC <ref> [38] </ref>: P (Djm; I) L ( ^ m ) n d m =2 ; (13) (Once again L ( ^ m ; ^ m ) replaces L ( ^ m ) in (13) when is unknown.) None of these Bayesian techniques have been used in surface reconstruction.
Reference: [39] <author> C. V. Stewart. MINPRAN: </author> <title> A new robust estimator for computer vision. </title> <journal> IEEE PAMI, </journal> <volume> 17(10):925 938, </volume> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: In each of these domains, the appropriate model must be chosen automatically if it is not dictated by prior knowledge. This model selection problem has received much less attention than the associated problem of estimating model parameters <ref> [8, 9, 32, 33, 39] </ref>, yet without a proper model, the estimated parameters have little meaning. The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds.
Reference: [40] <author> C. V. Stewart, R. Y. Flatland, and K. Bubna. </author> <title> Geometric constraints and stereo disparity computation. </title> <journal> IJCV, </journal> <volume> 20(3) </volume> <pages> 143-168, </pages> <year> 1996. </year>
Reference-contexts: Our merging rule based on the F-test, therefore, works in two steps. In the first step, it checks if the parameters of surface A are within the 95% confidence interval of the parameters of B (or vice-versa only one must succeed) <ref> [40] </ref> using the F statistic in [45, page 97]. When A and B belong to different models, the technique only checks if the lower order model fits within the confidence interval of the higher order model.
Reference: [41] <author> J. Subrahmonia, D. B. Cooper, and D. Keren. </author> <title> Practical reliable Bayesian recognition of 2D and 3D objects using implicit polynomials and algebraic invariants. </title> <journal> IEEE PAMI, </journal> <volume> 18(5) </volume> <pages> 505-519, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule <ref> [7, 14, 41, 47] </ref>, Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
Reference: [42] <author> F. S. Swed and C. Eisenhart. </author> <title> Tables for testing randomness of grouping in a sequence of alternatives. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 14 </volume> <pages> 66-87, </pages> <year> 1943. </year>
Reference-contexts: He creates binary images of positive and negative residuals, erodes the images using a 3 fi 3 kernel, finds the largest connected component in each image, and rejects the null hypotheses 3 For small samples, techniques from <ref> [42] </ref> and [19] may be used. 7 if the larger of these components is greater than 2% of n. We follow this heuristic for 3D range images. The runs test is advantageous when and the noise distribution of the data are unknown.
Reference: [43] <author> G. Taubin. </author> <title> Estimation of planar curves, surfaces, and nonplanar space curves defined by implicit equations with applications to edge and range segmentation. </title> <journal> IEEE PAMI, </journal> <volume> 13(11) </volume> <pages> 1115-1138, </pages> <year> 1991. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic <ref> [6, 8, 36, 43] </ref> and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria [7, 9, 16, 27, 29, 30, 41, 46, 47]. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid [13, 44], by clustering methods [22, 36] or by region growing <ref> [6, 9, 29, 43] </ref>. Model selection is an important part of this initial estimation step. For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. <p> For example, when expanding seed regions, at each iteration, it must be decided whether to continue growing using the same model or to switch to a different model. This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds <ref> [6, 29, 43] </ref>, Chi-square tests [46], runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries <ref> [6, 13, 36, 43, 44] </ref>. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface.
Reference: [44] <author> R. Taylor, M. Savini, and A. Reeves. </author> <title> Fast Segmentation of Range Imagery Into Planar Regions. </title> <journal> CVGIP, </journal> <volume> 45 </volume> <pages> 42-60, </pages> <year> 1989. </year>
Reference-contexts: This approach generally consists of two steps. In the first step, initial surface patches are fit to the data either by dividing the image into a grid <ref> [13, 44] </ref>, by clustering methods [22, 36] or by region growing [6, 9, 29, 43]. Model selection is an important part of this initial estimation step. <p> Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries <ref> [6, 13, 36, 43, 44] </ref>. Therefore, in the second step, surface patches need to be pruned or surface 1 (a) (b) or quadratic surface. <p> Merging decisions, on the other hand, must balance between bridging true surface boundaries and leaving artificial surface boundaries, a difficult problem illustrated by the examples shown in Figure 1. In an attempt to avoid the model selection problem, many merging techniques only join surfaces fit to the same model <ref> [6, 36, 44, 13, 26] </ref>, potentially limiting the effectiveness of the merging process. Surface reconstruction, therefore, provides a good context for studying the model selection problem in computer vision.
Reference: [45] <author> S. Weisberg. </author> <title> Applied Linear Regression. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: For 3D range images, we replace the longest run with size of the largest connected component created by the process described in RUNS. FTEST: In this test, any model m i is rejected in favor of m i+1 if <ref> [45, page96] </ref> (RSS m i RSS m i+1 )=((n d m i ) (n d m i+1 )) &gt; F (d m i+1 d m i ;nd m i+1 );0:95 : (19) Starting with the zeroth-order model, this test continues switching to a higher order model until (19) is not satisfied <p> Our merging rule based on the F-test, therefore, works in two steps. In the first step, it checks if the parameters of surface A are within the 95% confidence interval of the parameters of B (or vice-versa only one must succeed) [40] using the F statistic in <ref> [45, page 97] </ref>. When A and B belong to different models, the technique only checks if the lower order model fits within the confidence interval of the higher order model.
Reference: [46] <author> P. Whaite and F. P. Ferrie. </author> <title> Active exploration: knowing when we're wrong. </title> <booktitle> In ICCV, </booktitle> <pages> pages 41-48, </pages> <year> 1993. </year>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests <ref> [46] </ref>, runs test [6, 8], Bayes rule [7, 14, 41, 47], Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. <p> The runs test is advantageous when and the noise distribution of the data are unknown. CHI: This test is based on a one-way Chi-square test and rejects model m at a 95% confidence level. It has been used by Whaite and Ferrie <ref> [46] </ref>. The intuition is that low order incorrect models will produce a significant over-estimate to the error in the data. BESL: This test combines RUNS and CHI, and rejects model m if both of them fail. This is the model selection criteria used by Besl and Jain [6].
Reference: [47] <author> S. C. Zhu and A. Yuille. </author> <title> Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation. </title> <journal> IEEE PAMI, </journal> <volume> 18(9) </volume> <pages> 884-900, </pages> <year> 1996. </year> <month> 26 </month>
Reference-contexts: The model selection criteria that have been used in vision have many origins. Many of these criteria are heuristic [6, 8, 36, 43] and some rely on user defined thresholds. Others, especially recent ones, are applications of statistical and information theoretic criteria <ref> [7, 9, 16, 27, 29, 30, 41, 46, 47] </ref>. Unfortunately, the advantages and limitations of these criteria in computer vision algorithms have not been carefully analyzed. <p> This difficult problem has been addressed in vision with a variety of techniques, including heuristic criteria based on user-defined thresholds [6, 29, 43], Chi-square tests [46], runs test [6, 8], Bayes rule <ref> [7, 14, 41, 47] </ref>, Kullback-Liebler (K-L) distance [9], and minimum description lengths (MDL) [16, 27, 29, 30]. Depending on the algorithm, this first, surface growing step of the local-to-global reconstruction process either gives redundant surfaces [9, 29], or creates artificial surface boundaries [6, 13, 36, 43, 44].
References-found: 47

