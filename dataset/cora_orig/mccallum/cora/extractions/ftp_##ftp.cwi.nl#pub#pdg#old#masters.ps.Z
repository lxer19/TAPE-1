URL: ftp://ftp.cwi.nl/pub/pdg/old/masters.ps.Z
Refering-URL: http://www.cwi.nl/~pdg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Automatic Grammar Induction using the Minimum Description Length Principle 1  
Author: Peter Grunwald 
Date: April 20, 1994  
Abstract: 1 This thesis reports on the project leading to the author's graduation as a `doctorandus' at the Free University of Amsterdam. The project was performed at the Centre for Mathemathics and Computer Science, Amsterdam, the Netherlands, under supervision of Prof. Dr. Ir. P.M.B. Vitanyi. 
Abstract-found: 1
Intro-found: 1
Reference: [Adr92] <author> P.W. Adriaans. </author> <title> Language Learning from a Categorial Perspective. </title> <type> PhD thesis, </type> <institution> University of Amsterdam, </institution> <address> Amsterdam, </address> <year> 1992. </year>
Reference: [Ang80] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Inform. Contr., </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference: [Atk92] <author> M. Atkinson. </author> <title> Children's Syntax, an Introduction to Principles and Parameters Theory. </title> <publisher> Blackwell, Oxford, </publisher> <year> 1992. </year>
Reference: [Ber85] <author> R.C. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1985. </year>
Reference: [BP87] <author> R.C. Berwick and S. Pilato. </author> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 9-38, </pages> <year> 1987. </year>
Reference-contexts: These algorithms have been used successfully by linguists, for instance in a model of learning the English auxiliary verb system <ref> [BP87] </ref>. It may very well be that with the extension described here, this auxiliary verb system and some other subsets of natural language grammars can also be learnt by our system.
Reference: [BPd + 92] <author> P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18 </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: This does not affect the length): At first glance, it seems that H 1;n (c L 0 1 ) is very time-intensive to compute. Using ideas of <ref> [BPd + 92] </ref>, we manage to compute H 1;n (c L 0 1 ) in O (jCj 2 ) steps, therefore reducing the whole computation of T 21 to O (jCj 2 ). We will have a glance at the efficient subalgorithm in chapter 5, section 5.3. <p> We then put, at each step i, 47 the n + i-th most frequent word into an existing class, where we make sure to pick the class such that the total increase in description length is minimal. This is essentially the route taken by <ref> [BPd + 92] </ref>. 2. or we can restrict our training text beforehand to all sentences in which each word is one of the n most frequent. The first method still uses a lot of memory, so we will always use the second. <p> For each of these classes, a logarithm has to be computed. In the natural language case, the number of immediate `neighbors' will be much lower than the possible number (which is C + C. For a typical very large corpus of English text, it is reported in <ref> [BPd + 92] </ref> that on average a word has only 2 % of its possible successors. (and thus also 2 % of its possible predecessors). <p> This takes no time at all because we already knew T 2 or else we could not have decided on this merge anyway. The critical step is again the computation of the new values of T 2 for all pairs of classes. In <ref> [BPd + 92] </ref>, a clever algorithm is described that can do a similar task in amazingly little time. We adjusted it a little bit to fit in our model in order to compute mer T 21 fast. <p> Computing the new T 2 's, we can use these terms and the old T 2 's and find the answers in time 0:04 fi C 2 steps 4 . These steps do not even involve a logarithm, so this will be very fast (details can be found in <ref> [BPd + 92] </ref>). On the other hand, for all neighbors of the newly formed class, we have to perform a computation that does involve a logarithm. This will amount to about 0:04 fi C logarithms, which can sometimes take more time than the quadratic operation. <p> at least 1 fi C 2 steps to update all T 2 values but we know beforehand that most of them remain unchanged. 51 5.3.3 The Critical Loop: Constructing The recomputation of T 2 after a construct has taken place costs more time; here, we cannot use the idea of <ref> [BPd + 92] </ref> so we essentially have to recompute mer T and con T for all pairs of classes. (0:08 fi C 2 , fully parallelizable). <p> We have turned to the so-called `Brown Corpus' ([KF67]) which is by our knowledge the largest body of present-day American English around. It has also been used in <ref> [BPd + 92] </ref> to test a model that is very similar to ours (chapter 7). <p> One can see that there is often just one single `wrong' word in a class (like `fire' in class 126). It would be interesting to see whether we could `repair' this by getting words out of classes (which is done quite successfully in <ref> [BPd + 92] </ref> seethe next chapter), but unfortunately there was no time to implement this feature. <p> All other adjective-like words that are allowed to merge are in one of the four shown classes. We can see that often a small subset of a class is completely off track, while the other class members really belong together. In <ref> [BPd + 92] </ref>, where a system very similar to ours is presented, there is also an operation which can put a word out of a class if this reduces the description length, thereby correcting errors introduced by the greediness of the algorithm. <p> They examine a (large) sample of training text, t L 1 , with length L. The parameters are chosen so as to maximize the probability that given the first word of the training text, the rest of this text is reproduced. In <ref> [BPd + 92] </ref> this is called sequential maximum likelihood estimation. 7.1.3 n-gram class-based models Everything about n-gram models we have seen until now had been done before. We now turn to [BPd + 92]'s own contribution. It consists of integrating the concept of `word classes' into their models. <p> In <ref> [BPd + 92] </ref> this is called sequential maximum likelihood estimation. 7.1.3 n-gram class-based models Everything about n-gram models we have seen until now had been done before. We now turn to [BPd + 92]'s own contribution. It consists of integrating the concept of `word classes' into their models. A `word class' is meant to be a set of words with similar meaning and/or syntactic function. <p> By `best' we mean the mapping that maximizes Pr (t L 2 j t 1 ). The following is completely due to <ref> [BPd + 92] </ref>: Let f () = (L 1) 1 log Pr (t L 2 j t 1 ) . <p> At first glance, it seems that finding these two classes is of the order O (C 4 ), where C is the number of classes before the merge step. <ref> [BPd + 92] </ref> manage to reduce the computation of all I (that is, for all combinations of classes c 1 ; c 2 ) in time O (C 2 ), thereby reducing the whole algorithm to this complexity. <p> Now we have seen how this works in practice. This means that the results described in chapter 6, section 6.3.1 should be exactly the same as those of <ref> [BPd + 92] </ref>. 7.3 Problems with this model We have seen that our model is essentially an extension to [BPd + 92]'s. If we try to use their model as part of a method for grammar induction 5 , some problems arise. <p> Now we have seen how this works in practice. This means that the results described in chapter 6, section 6.3.1 should be exactly the same as those of <ref> [BPd + 92] </ref>. 7.3 Problems with this model We have seen that our model is essentially an extension to [BPd + 92]'s. If we try to use their model as part of a method for grammar induction 5 , some problems arise. Some of these are addressed by our own algorithm, some have not been solved yet. <p> Some of these are addressed by our own algorithm, some have not been solved yet. The most important problems are: 5 Note however that this is not the intention expressed in <ref> [BPd + 92] </ref>. <p> We will go a little into the remaining problematic points: 7.3.1 Brown's use of the Maximum Likelihood Principle The Best Partition is the Trivial Partition In <ref> [BPd + 92] </ref> it is claimed that there exists an optimal partition of all words in the training text into classes, a partition that maximizes the average mutual information I between the classes. (And `maximizing the average mutual information between classes' means finding classes according to the Maximum Likelihood method) However, <p> How many merges should we admit so as to obtain a 'maximally interesting partition' ? In <ref> [BPd + 92] </ref>, this problem is not discussed. Maybe I will suddenly drop dramatically after a certain number of merge steps. <p> Notice that the previous-but-one and next-but-one relations are neither used in our own model nor in <ref> [BPd + 92] </ref>'s. 7.4.1 Computing the Rank Correlation Coefficient Now an n fi n distance matrix is computed for all words in the rows. The idea is that the more distant two words are, the less similar their context is. <p> But is this enough? And why use the Spearman Rank Correlation Coefficient? no linguistic principles The problems are the same as for <ref> [BPd + 92] </ref>'s and our own model. 1 class per word The clusters effectively partition the words of the input alphabet into classes. <p> As we shall see later, one can try to solve these problems partially by combining our own model with that of Finch and Chater. 88 7.5.2 Nice Properties The results obtained with Finch and Chater's model seem just a little bit better than those of <ref> [BPd + 92] </ref> 10 The main reason for this is clear: while in [BPd + 92] only 2-grams are considered with no words inbetween, Finch and Chater also incorporate dependencies between words with one intervening word, which may just as well be something that humans are able to do, too. <p> to solve these problems partially by combining our own model with that of Finch and Chater. 88 7.5.2 Nice Properties The results obtained with Finch and Chater's model seem just a little bit better than those of <ref> [BPd + 92] </ref> 10 The main reason for this is clear: while in [BPd + 92] only 2-grams are considered with no words inbetween, Finch and Chater also incorporate dependencies between words with one intervening word, which may just as well be something that humans are able to do, too. <p> From a theoretical point of view, we have corrected a mistake in <ref> [BPd + 92] </ref> about the increase of mutual information in a text; we have shown that Brown's model ([BPd + 92]) can be seen as a specific version of a variation on Solomonoff's model ([Sol64]) and we have given an analysis of Finch & Chater's method ([FC91, FC92]) in which we <p> Only because it would take a long time to implement and because it is not completely clear to us how to implement it efficiently this has not been done yet. From <ref> [BPd + 92] </ref> we know that if we check whether removal of a word from a class and putting it into another class could reduce the description length, and if so, we really do it, then this may often compensate for the errors introduced by the greediness of the algorithm.
Reference: [Car77] <author> S. Carey. </author> <title> The child as word learner. </title> <editor> In M. Halle, J. Bresnan, and G.A. Miller, editors, </editor> <booktitle> Linguistic theory and psychological reality, </booktitle> <pages> pages 264-293. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1977. </year>
Reference: [Cho81] <author> N. Chomsky. </author> <title> Lectures on government and binding. </title> <publisher> Foris, </publisher> <address> Dordrecht, </address> <year> 1981. </year>
Reference: [FC91] <author> S. Finch and N. Chater. </author> <title> A hybrid approach to the automatic learning of linguistic categories. </title> <journal> AISB Quarterly, </journal> <volume> 78 </volume> <pages> 16-24, </pages> <year> 1991. </year>
Reference-contexts: We will suppose that they have exactly the same frequency f of occurring in the training text. Furthermore we will assume that `begin' and `commence' share exactly the same context with the same frequencies. One of the four tables (successor ) that we should construct according to <ref> [FC91] </ref> has entries that look as follows: 10 The results are very similar in nature so we have not included them here. 89 Here x stands for the probability Pr (ww n = `begin now') which is equal to the other three probabilities that come into play.
Reference: [FC92] <author> S. Finch and N. Chater. </author> <title> Bootstrapping syntactic categories using statistical methods. In Background and Experiments in Machine Learning of Natural Language: </title> <booktitle> Proc. 1st SHOE-Workshop, </booktitle> <pages> pages 229-235, </pages> <address> Tilburg, </address> <year> 1992. </year>
Reference-contexts: Finch and N. Chater ([FC91], <ref> [FC92] </ref>). The basic idea is again to map words into classes if they share the same context within a large body of text, but the way this is done is quite different. Suppose we have a large corpus of natural language text.
Reference: [Fin] <author> S. </author> <title> Finch. </title> <type> Personal communication. </type>
Reference-contexts: The key idea is to compute the correlation between variables X i and X j in an 8 The actual formulae are not mentioned in the cited articles and were obtained directly from <ref> [Fin] </ref>. 9 Notation as in previous chapters. 87 indirect way: First record for all observed values of X i their rank in the range of observed X i -values. I.e. the highest value observed will be nr. 1, the second highest value nr. 2 etc.
Reference: [Fis25] <author> R.A. </author> <title> Fisher. </title> <journal> On the mathematical foundations of theoretical statistics. Philos. Trans. Roy. Soc. London, Ser. A, </journal> <volume> 222 </volume> <pages> 309-368, </pages> <year> 1925. </year>
Reference: [Gal68] <author> R.G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: A Markov Chain is called ergodic if all of its states form one ergodic set. Coding Theorem We have the following theorems: (proofs can be found in e.g. <ref> [Gal68] </ref>) Theorem 2 The entropy of a stationary ergodic Markov source U is given by H 1 (U ) = a2S X Pr (s n = b j s = a) log Pr (s n = b j s = a)) (3:4) where Pr (a) is the limiting probability of a <p> we must have, in the limit, f () = w2W X Pr (c 1 c 2 ) log Pr (c 2 ) The only part that is dependent on the mapping of words unto classes is the second part, which is the average mutual information between all classes! (see e.g. <ref> [Gal68] </ref>) 7.1.4 A practical algorithm So it is found that the `best' (in a maximum likelihood sense) way to partition a group of words into classes is that which maximizes the average mutual information between the classes. It seems not possible to find this partition efficiently. <p> = P Pr (c 1 c 2 ) Pr (c 1 c 2 ) Pr (c 1 ) Pr (c 2 ) ) then independent of m; n and of the elements of each c 2 C, we have I (C 0 ) I (C) Proof (a slight variation of <ref> [Gal68] </ref>, pag. 35) We define random variables X and Y that can take on as values elements of C and also X 0 and Y 0 that can take on as values elements of C 0 as follows: X takes on the value c 1 , w U 2 c 1 <p> It is useful to view X; Y and Y 0 as `cascaded channels', as is done in figure 7.1. We state without proof (see e.g. <ref> [Gal68] </ref>) the basic fact that in general (i.e. for all stochastic variables X, Y 0 , Y , not just under their definitions here) I (X; Y 0 Y ) = I (X; Y 0 ) + I (X; Y j Y 0 ) = I (X; Y ) + I
Reference: [JaJa57] <author> A.M. -glom i I.M. -glom. Verotnost~ i Informaci. Gosularstven-noe Izdatel~stvo Tehniko-teoretiqesko i Literatur~fi,Moskva,1957. </author> <month> 101 </month>
Reference: [Jay68] <author> E.T. Jaynes. </author> <title> Prior probabilities. </title> <journal> IEEE Trans. Systems Sci. Cybernet., </journal> <volume> SSC--4:227-241, </volume> <year> 1968. </year>
Reference: [KF67] <author> H. Kucera and W. Francis. </author> <title> Computational Analysis of Present Day American English. </title> <publisher> Brown University Press, </publisher> <year> 1967. </year>
Reference: [Lan87] <author> P. Langley. </author> <title> Machine learning and grammar induction. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 5-8, </pages> <year> 1987. </year> <note> Editorial of special issue on language learning. </note>
Reference: [LV92] <author> M. Li and P.M.B. Vitanyi. </author> <title> Inductive reasoning and Kolmogorov complexity. </title> <journal> J. Comput. Syst. Sci., </journal> <volume> 44(2) </volume> <pages> 343-384, </pages> <year> 1992. </year>
Reference-contexts: More practically, we have to encode in the description of some examples (data items) * their correct description unaided by any theory. (see e.g. the system for handwritten character recognition described in <ref> [LV92] </ref>) and * the fact itself that they are not covered by (viz. wrongly described by) the theory As an example, consider a very large set of n points in a plane on which a linear regression is done. Suppose that all points fit except one that is terribly misplaced.
Reference: [McN66] <author> D. McNeill. </author> <title> Developmental psycholinguistics. </title> <editor> In F. Smith and G.A. Miller, editors, </editor> <title> The genesis of language: a psycholinguistic approach. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1966. </year>
Reference: [Oli68] <author> D. Olivier. </author> <title> Stochastic Grammars and Language Acquisition Mechanisms. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, Mass., </address> <year> 1968. </year>
Reference-contexts: Or was it?' 6.1 The Test Corpora and the Platform 6.1.1 A Stochastic Toy-Grammar We started our empirical work using a toy-grammar. It is called `stochastic' because when generating sentences, each production in the grammar is fired with a certain probability (see <ref> [Oli68] </ref>). The toy grammar is shown in fig. 6.1. This grammar produces sentences like: the big boy that likes the weird ill apple says that the fish is tired. It is clear that this grammar contains recursion, both of one word structures and of sentences.
Reference: [Pos64] <author> P. </author> <title> Postal. Limitations of phrase-structure grammars. </title> <editor> In J.A. Fodor and J.J. Katz, editors, </editor> <booktitle> The Structure of Language: Readings in the philosophy of language. </booktitle> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1964. </year>
Reference: [PW80] <author> F. Pereira and S. Warren. </author> <title> DCG grammars. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 231-278, </pages> <year> 1980. </year>
Reference-contexts: the restricted training text. 48 Chapter 5 The Program: Implementation Issues 5.1 Three Programs We have written three programs of interest for our model: grammar.pro A Prolog Program that produces `random sentences' that belong to the language of a Stochastic Context Free Grammar that is defined in the `DCG'-formalism (see <ref> [PW80] </ref>). filter.c A C Program that 1. reads a file F containing a body of text S 2. counts the frequencies of all different words occurring in it 3. produces a file F 0 with text S 0 that contains only those sentences which fully consist of the n most frequent
Reference: [Ris82] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Ann. Statist., </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1982. </year>
Reference: [Sol64] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference, part 1 and part 2. </title> <journal> Inform. Contr., </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <year> 1964. </year>
Reference-contexts: Of the latter two things, we can always keep track in our implementation. Efficient Approximation of con T 21 Using a slight variation of a formula introduced in <ref> [Sol64] </ref>, we can compute an approximation for T 21 that gives very good results for construct operands that have a reasonable high frequency of occurring. (and we do not want to use operands that have a low frequency anyway because the data we have about them is unreliable: see section 4.4.4).
Reference: [WC80] <author> K. Wexler and P.W. Culicover. </author> <title> Formal Principles of Language Acquisition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1980. </year>
Reference: [Wol82] <author> J.G. Wolff. </author> <title> Language acquisition, data compression, and generalization. </title> <journal> Language and Communication, </journal> <volume> 2 </volume> <pages> 57-89, </pages> <year> 1982. </year> <month> 102 </month>
Reference-contexts: On the other hand, if k is too large the algorithm does not work precisely anymore. 2. We will find better classes (= clusters) because we make use of more dependencies in the text. 7.6 Data Compression and the SNPR system In <ref> [Wol82] </ref>, J.G. Wolff describes a grammar learning system called `SNPR14' that is, in some respects, very similar to ours. His concept of `folding' is very similar to our concept of `merging'; his concept of `building' is very similar to our concept of `constructing'. <p> His concept of `folding' is very similar to our concept of `merging'; his concept of `building' is very similar to our concept of `constructing'. We 93 will not take the trouble to explain this in detail because the similarities are quite obvious. The interested reader is referred to <ref> [Wol82] </ref> to make a comparison him/herself. Here, we will notice two important differences to our model. Regarding one of them, our model may be seen as a solution to problems posed by Wolff's approach. The other difference however goes beyond our own model. <p> text t L 1 (l + k L) that, when parsed by the partial grammar C at some step yields c 1 c 2 : : : c i : : : c n . (i.e. v 1 : : : v j w k fl An example (inspired by <ref> [Wol82] </ref>), will show that this is simpler than it seems. Consider the following C and t L 1 : b 2 = fP; Q; Rg b 4 = fA; Bg t L 1 = RY. AXPY. BXPY. AXQY. <p> Now the rebuilding process does not remove anything from any class: although the string 11 Because of the differences between SNPR14 and our own model, this description will only be a rough one, not totally equivalent to what is done in <ref> [Wol82] </ref>. Our main concern is just to explain the basic idea. 94 BXQY never occurred in the training text, it remains a `generalization' (i.e. a string that can be recognized by the partial grammar). J.G. Wolff writes about this ([Wol82]): The attraction of this kind of mechanism for eliminating overgeneralizations is <p> There are also other computations which can get very time consuming and therefore limit the use of SNPR14 to toy grammars (no `real' natural language is tried in <ref> [Wol82] </ref>). One of those is, alas, the rebuilding function described earlier. If we really have to check all words and classes that have meta classes (to use our own terminology) every time a construct takes place, each construct will make the algorithm a lot slower.
References-found: 26

