URL: ftp://io.nosc.mil/pub/gherrity/gherrity.thesis.ps.Z
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Game-Learning Machine  
Author: Michael Gherrity Professor Walter J. Savitch Professor Anthony V. Sebald Professor Maxwell B. Stinchcombe 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree Doctor of Philosophy in Computer Science by  Committee in charge: Professor Paul R. Kube, Chairperson Professor Richard K. Belew  
Date: 1993  
Affiliation: UNIVERSITY OF CALIFORNIA, SAN DIEGO  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. Abramson. </author> <title> The Expected-outcome model of two-player games. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: In general, the program performs only a one-ply search. HOYLE has demonstrated an ability to learn a number of simple games, but its ability to learn more complex games such as chess is questionable. Bruce Abramson <ref> [1] </ref> proposed a method for training evaluation functions which could be used for arbitrary games, however the functions were only used in one-ply searches during actual play, and the training was not done during play.
Reference: [2] <editor> L. V. Allis, M. van der Meulen, and H. J. van den Herik. fffi conspiracy-number search. In D. F. Beal, editor, </editor> <booktitle> Advances in Computer Chess 6, </booktitle> <pages> pages 73-95. </pages> <publisher> Ellis Horwood Limited, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* [47], conspiracy number search [32, 55], min/max approximation [50], singular extension [23], and fffi conspiracy number search <ref> [2] </ref>. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function. Palay [46] proposed a search procedure where the evaluation function returns a probability distribution as the value of a position rather than a single number. <p> A naive estimate of the size of the full game tree would be roughly 9! = 362; 880 nodes, although a more careful count including early termination and forced moves gives 75; 482 nodes <ref> [2] </ref>. A game tree of this size could easily be searched completely in a reasonable length of time, however SAL's search 1 Connect-Four is a trademark of the Milton-Bradley company 72 73 tree was limited to about 100 nodes.
Reference: [3] <author> I. Althofer. </author> <title> An incremental negamax algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 57-65, </pages> <year> 1990. </year>
Reference-contexts: Various off-line procedures are suggested for 27 collecting this data. Such methods are not practical for a game-learning program that can only learn from actually playing the game. The incremental negamax algorithm described by Ingo Althofer <ref> [3] </ref> is proven to exponentially decrease the effects of the evaluation function errors on the value of the root node as the depth of the search is increased. Although this algorithm is effective for erroneous evaluation functions, it does not perform a selective search.
Reference: [4] <author> T. Anantharaman, M. S. Campbell, and F. Hsu. </author> <title> Singular extensions: Adding selectivity to brute-force searching. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 99-109, </pages> <year> 1990. </year>
Reference-contexts: Alexander Kronrod [33] Typical chess-playing programs select a move by searching through thousands of moves, countermoves, counter-countermoves, etc., forming a vast game tree of possible board positions. For example, the DEEP THOUGHT program examines over 500,000 positions per second <ref> [4] </ref>. To ensure that key moves are not missed, every legal move from each position in the tree must be considered. <p> Each 5 additional move added to the depth of the search tree is called a ply. The DEEP THOUGHT program generally searches over 10 ply before making a move [23]. Several efforts have been made to grow the search tree in a more selective fashion <ref> [4, 50] </ref>. A selective search would determine which sequences of moves should be considered based on characteristics of the board positions examined in the game tree. Promising lines of play would be continued to great depth, whereas bad lines would be terminated early to avoid wasting time.
Reference: [5] <editor> W. Aspray and A. Burks, editors. </editor> <title> Papers of John Von Neumann on Computing and Computer Theory. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Also in 1950, Claude Shannon [59] proposed using chess playing ability as a means of answering the question; "Could a machine be designed that would be capable of `thinking' ?" After the first electronic digital computer became operational in 1951 <ref> [5] </ref>, discussions concerning machines, games, and thinking were no longer purely academic. Computer programs for playing games became a major focus of research in Artificial Intelligence (AI). The first programs for playing chess played at a novice level. However today, many chess programs play extremely well.
Reference: [6] <editor> A. Barr and E. Feigenbaum, editors. </editor> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> volume 1. </volume> <publisher> William Kaufmann, Inc., </publisher> <address> Los Altos, </address> <year> 1981. </year>
Reference-contexts: Developments toward solving the chess-playing problem have proven to be applicable towards the solution of many more practical problems. The technique of searching a state space was first proposed by Shannon, Turing, and Wiener for the game of chess. In their series The Handbook of Artificial Intelligence <ref> [6] </ref>, Barr and 4 Feigenbaum consider such search methods to constitute some of the core ideas of AI [volume 1, p. 21]. Applications which have used search methods include robot planning, visual scene analysis, mathematical theorem proving, symbolic integration, and puzzle solving.
Reference: [7] <author> E. B. Baum. </author> <title> Minimax is not optimal for imperfect game players. </title> <year> 1993. </year>
Reference-contexts: Using the evaluation value of the root node as the threshold causes the consistency search procedure to be most sensitive to values better than this threshold. 2. Analyses of the case of multivalued evaluation functions performed by both Pearl [47] and Baum <ref> [7] </ref> showed that if pathology occurs for the bi-valued case, then it also occurs for the multivalued case. This indicates an insensitivity in the analyses between the use of bi-valued or multivalued evaluation functions. For simplicity, only the bi-valued case will be analyzed here.
Reference: [8] <author> E. B. Baum and W. D. Smith. </author> <title> Best play for imperfect players and game tree search. </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Palay [46] proposed a search procedure where the evaluation function returns a probability distribution as the value of a position rather than a single number. This idea was extended by both Russell and Wefald [52] and Baum and Smith <ref> [8] </ref> to include information about the costs and potential benefits of further node expansions. Although these methods claim various types of optimality, they are not usable for the game-learning task for several reasons: 1. The probability distributions are computed using a small number of features chosen by the programmer.
Reference: [9] <author> D. F. Beal. </author> <title> An analysis of minimax. </title> <editor> In M. R. B. Clarke, editor, </editor> <booktitle> Advances in computer chess 2, </booktitle> <pages> pages 103-109. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1980. </year>
Reference-contexts: As a result, a modification of a search procedure proposed by Don Beal <ref> [9] </ref>, called consistency search, was designed and shown to be beneficial even for 16 pathological games. The description and analysis of the consistency search procedure is given in Chapter III. <p> Chapter III Consistency Search This chapter describes and analyzes the search procedure used by the SAL program. This procedure is called consistency search since it shares the basic principle behind the consistency search procedure proposed by Don Beal <ref> [9, 11] </ref>. There are several important differences between the procedure described here and Beal's procedure however, and these will be highlighted. <p> Section III.H contains the details of the analysis. III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased [23]. However, analyses of this procedure performed by Beal <ref> [9] </ref>, Nau [36, 37, 39], and Pearl [47] show that the performance should actually decrease with increasing search depth for many types of games. Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. <p> Unfortunately, this procedure has only been shown to be beneficial in games like chess. There are many games in which a capture is not even a legal move, and it is difficult to identify a quiescent position. III.D The Consistency Search Procedure The consistency search procedure proposed by Beal <ref> [9] </ref> was intended to reduce the error in the value of a node by selective search. A consistent position is defined as one where the evaluation value of the position is equal to its minimax value after a one-ply search. The search proceeds depth-first, stopping only at consistent positions. <p> The negamax values of the root child nodes are used to select a move. Adding the alpha-beta tree pruning technique to the consistency search procedure is straightforward and an example is given by Beal in <ref> [9] </ref>. III.E Extending the Procedure to Multivalued Evaluation Functions The consistency search procedure described in the previous section applies to a be-valued evaluation function. Most evaluation functions can return a large 32 1. <p> This comparison is done for various different uniform branching factors in very deep game trees with independent, randomly assigned terminal outcomes. Recall that for these trees a full-width, fixed-depth, minimax search is pathological <ref> [9, 36] </ref>, so a one-ply search gives a lower probability of making an incorrect move than a deeper search. The analysis shows that for sufficiently accurate evaluation functions, the consistency search significantly decreases the probability of making an incorrect move when compared to a one-ply search.
Reference: [10] <author> D. F. Beal. </author> <title> Benefits of minimax search. </title> <editor> In M. R. B. Clarke, editor, </editor> <booktitle> Advances in computer chess 3, </booktitle> <pages> pages 17-24. </pages> <publisher> Pergamon Press, Oxford, </publisher> <year> 1982. </year>
Reference-contexts: This assumption was made by Samuel for his checkers-learning program [53], since the results of a deeper search were used as training values for the evaluations of the positions generated at the first ply. Mathematical analysis by both Dana Nau [36, 37] and Don Beal <ref> [10] </ref> indicate that the assumption that a deeper search yields a better choice of move is not correct for many games. In fact, their analyses showed the opposite is true, that a deeper search results in worse play. <p> The implication is that typical games like chess are not pathological, whereas games for which it can be shown that standard game tree search is ineffective are in some way contrived and unusual [24]. Many researchers <ref> [47, 14, 10, 57] </ref> have modified the mathematical assumptions used by Nau and Beal in an attempt to explain why games like chess are not pathological. None of these modifications seem compelling. <p> Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal <ref> [10] </ref>, Nau [37, 41, 40], and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological.
Reference: [11] <author> D. F. Beal. </author> <title> Recent progress in understanding minimax search. </title> <booktitle> In Proceedings of the Association for Computing Machinery annual conference, </booktitle> <pages> pages 164-169, </pages> <address> New York, </address> <year> 1983. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Chapter III Consistency Search This chapter describes and analyzes the search procedure used by the SAL program. This procedure is called consistency search since it shares the basic principle behind the consistency search procedure proposed by Don Beal <ref> [9, 11] </ref>. There are several important differences between the procedure described here and Beal's procedure however, and these will be highlighted. <p> Applying a simple threshold to these values to generate the values 0 or 1 could lose a significant amount of information. Beal <ref> [11] </ref> suggested defining a consistent node for a multivalued evaluation function as one whose evaluation value differed from its one-ply negamax value by no more than some small constant. The question is then what to choose for the small constant.
Reference: [12] <author> D. F. Beal. </author> <title> A generalized quiescence search algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 85-98, </pages> <year> 1990. </year> <month> 93 </month>
Reference-contexts: Only those procedures that are game independent are potentially useful for a game-learning program. For example, the quiescent search procedure proposed by Don Beal <ref> [12] </ref> assumes that it is illegal for a player to pass. However, for a game such as go passing is a legal move, hence a game-learning program using quiescent search would perform poorly on the game of go.
Reference: [13] <author> R. A. Becker, J. M. Chambers, and A. R. Wilks. </author> <title> The New S Language. Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Pacific Grove, Cali-fornia, </address> <year> 1988. </year>
Reference-contexts: If the selective expansion performed by the consistency search was not beneficial, the full-width, two-ply search results 2 The figure was made using the S program <ref> [13] </ref> and the curves drawn using the lowess scatterplot smoothing function. 74 Figure V.1: SAL's performance against a tic-tac-toe program. would be superior to the consistency search results. In pathological games, the one-ply search would perform better than the two-ply search.
Reference: [14] <author> I. Bratko and M. </author> <title> Gams. Error analysis of the minimax principle. </title> <editor> In M. R. B. Clarke, editor, </editor> <booktitle> Advances in computer chess 3, </booktitle> <pages> pages 1-15. </pages> <publisher> Pergamon Press, Oxford, </publisher> <year> 1982. </year>
Reference-contexts: The implication is that typical games like chess are not pathological, whereas games for which it can be shown that standard game tree search is ineffective are in some way contrived and unusual [24]. Many researchers <ref> [47, 14, 10, 57] </ref> have modified the mathematical assumptions used by Nau and Beal in an attempt to explain why games like chess are not pathological. None of these modifications seem compelling. <p> Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal [10], Nau [37, 41, 40], and Bratko and Gams <ref> [14] </ref> show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological.
Reference: [15] <author> H. L. Dreyfus. </author> <title> What Computers Can't Do: </title> <booktitle> The Limits of Artificial Intelligence. </booktitle> <publisher> Harper and Row, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: This has been an insidious problem 8 in AI research. A method is shown to work well in a small domain, but when tried in the larger domain requires excessive computational resources to yield a practical solution. Such domains have been termed micro-worlds <ref> [15] </ref>. It is difficult to defend any choice of domain from the micro-worlds criticism. However, the chosen domain includes games like checkers, chess, othello, and go.
Reference: [16] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern classification and scene analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year>
Reference-contexts: In addition, criteria for winning the game would be available from the final board position of each game played. A game-learning program was developed which learned the rules of a game using simple linear discriminant functions. The Perceptron Convergence Theorem <ref> [16] </ref> can be used to argue that given sufficient information about the board position and move, the rules of any game could be learned after a finite number of illegal moves. Information about a board position and move is sufficient if legal moves can be linearly separated from illegal moves.
Reference: [17] <author> S. Epstein. </author> <title> Prior knowledge strengthens learning to control search in weak theory domains. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7 </volume> <pages> 547-586, </pages> <year> 1992. </year>
Reference-contexts: Rote-learning has been used in a competition chess program [56], but the learning was used only to prevent the program from repeating the same moves in human tournaments. More recently, a game-learning program named HOYLE was developed by Susan Epstein <ref> [17] </ref>. HOYLE is provided with a variety of general game playing methods and uses various rote-learning techniques to improve the performance of these methods for specific games. In general, the program performs only a one-ply search.
Reference: [18] <editor> E. A. Feigenbaum and J. Feldman, editors. </editor> <booktitle> Computers and thought. </booktitle> <publisher> McGraw-Hill Book Company, Inc., </publisher> <year> 1963. </year>
Reference-contexts: For a computer program to behave intelligently, it must search problem mazes in a highly selective way, exploring paths relatively fertile with solutions and ignoring paths relatively sterile. - Feigenbaum and Feldman <ref> [18] </ref> In board games of perfect information, the current board position provides all the information necessary for the player whose turn it is to more to choose the correct next move.
Reference: [19] <author> J. J. Gillogly. </author> <title> The technology chess program. </title> <journal> Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 145-163, </pages> <year> 1972. </year>
Reference-contexts: This allows the analysis to assume a fixed-depth search, rather than the variable depth that results from the capture tree. The importance of quiescence was realized by Shannon [58] and Turing [66] and experimentally verified by Gillogly <ref> [19] </ref> for the game of chess. Unfortunately, this procedure has only been shown to be beneficial in games like chess. There are many games in which a capture is not even a legal move, and it is difficult to identify a quiescent position.
Reference: [20] <author> A. K. Griffith. </author> <title> A comparison and evaluation of three machine learning procedures as applied to the game of checkers. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 137-148, </pages> <year> 1974. </year>
Reference-contexts: All other programs can also be characterized as learning an evaluation function, but have only been designed to learn a specific game. For example, Arthur Samuel [53] and Arnold Griffith <ref> [20] </ref> designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games <ref> [53, 20, 28, 62] </ref>, and learning by playing the game [53, 63, 30]. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players.
Reference: [21] <author> D. Hartmann. </author> <title> Notions of evaluation functions tested against grandmaster games. </title> <editor> In D. F. Beal, editor, </editor> <booktitle> Advances in Computer Chess 5, </booktitle> <pages> pages 91-143. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: The total material value of each player in a given board position is an example of a feature. Other features in the game of chess include piece mobility, center control, and piece development <ref> [21] </ref>. Chess players have observed a strong correlation between the eventual outcome of a chess game and the values of these features for positions that occur in the game.
Reference: [22] <author> D. R. Hofstadter. Metamagical themas: </author> <title> Questing for the essence of mind and pattern. </title> <publisher> Basic Books, Inc., </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Instead, he boiled the problem of motion down to the most pristine case he could imagine planets coasting through a vacuum. - Douglas Hofstadter <ref> [22, p. 566] </ref> Typical board games require a player to make a sequence of decisions in a limited amount of time. There is usually insufficient time to investigate all the possible 3 consequences of these decisions. Once made, the decisions cannot be revoked. <p> I.D Two-Person, Deterministic, Zero-Sum Board Games of Perfect Information The domain of all games is exceedingly large. Aside from the common parlor games such as chess and checkers, it extends to esoteric games such as Nomic <ref> [22, pp. 70-86] </ref>, where the rules of the game are constantly changed by the players.
Reference: [23] <author> F. Hsu, T. Anantharaman, M. Campbell, and A. Nowatzyk. </author> <title> A grandmaster chess machine. </title> <journal> Scientific American, </journal> <volume> 263(4) </volume> <pages> 44-50, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Computer programs for playing games became a major focus of research in Artificial Intelligence (AI). The first programs for playing chess played at a novice level. However today, many chess programs play extremely well. The current computer chess champion, DEEP THOUGHT <ref> [23] </ref>, has a U.S. Chess Federation rating of about 2600 (the world champion currently has a rating of 2900 and an average tournament player has a rating of 1500). Some people contend that a program like DEEP THOUGHT will never be able to become the world chess champion. <p> This would prove, according to Shannon, that 1 2 machines are capable of thinking. However, many people remain unconvinced. Referring to a future match between the world chess champion and their next generation program, the authors of DEEP THOUGHT state in <ref> [23] </ref>, ... the ingenuity of one supremely talented individual will be pitted against the work of generations of mathematicians, computer scientists and engineers. We believe the result will not reveal whether machines can think but rather whether collective human effort can outshine the best achievements of the ablest human beings. <p> Each 5 additional move added to the depth of the search tree is called a ply. The DEEP THOUGHT program generally searches over 10 ply before making a move <ref> [23] </ref>. Several efforts have been made to grow the search tree in a more selective fashion [4, 50]. A selective search would determine which sequences of moves should be considered based on characteristics of the board positions examined in the game tree. <p> The game of chess became the test domain of choice for new selective search algorithms. Since the early 1950's, computer speeds have increased by over 4 orders of magnitude. A brute force search can now extend to over 10 ply in the game of chess <ref> [23] </ref>. A program searching to such a depth can beat all but the best chess players. In 1977, a program performing a brute force search became the best chess-playing 6 program in the world [60]. <p> It is found that these errors increase as the depth of the search increases. Contrary to these mathematical results, experience with chess playing programs has shown that the deeper the program searches, the better it plays <ref> [23] </ref>. Since this seems to be the case for most common games, games where deep search is not beneficial have been called pathological. <p> Section III.H contains the details of the analysis. III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased <ref> [23] </ref>. However, analyses of this procedure performed by Beal [9], Nau [36, 37, 39], and Pearl [47] show that the performance should actually decrease with increasing search depth for many types of games. <p> There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* [47], conspiracy number search [32, 55], min/max approximation [50], singular extension <ref> [23] </ref>, and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function. Palay [46] proposed a search procedure where the evaluation function returns a probability distribution as the value of a position rather than a single number.
Reference: [24] <author> H. Kaindl. </author> <title> Minimaxing theory and practice. </title> <journal> AI Magazine, </journal> <pages> pages 69-76, </pages> <month> Fall </month> <year> 1988. </year>
Reference-contexts: The implication is that typical games like chess are not pathological, whereas games for which it can be shown that standard game tree search is ineffective are in some way contrived and unusual <ref> [24] </ref>. Many researchers [47, 14, 10, 57] have modified the mathematical assumptions used by Nau and Beal in an attempt to explain why games like chess are not pathological. None of these modifications seem compelling.
Reference: [25] <author> H. Kaindl, R. Shams, and H. Horacek. </author> <title> Minimax search algorithms with and with-ould aspiration windows. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(12) </volume> <pages> 1225-1235, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The program was not designed to compete in computer game-playing tournaments, and hence does not include many techniques that have been shown to improve the playing strength of competition programs, such as transposition tables, aspiration windows, and various heuristics <ref> [54, 25] </ref>. Although these techniques would improve SAL's performance, they were not necessary to test the search methods and learning issues described in this dissertation. This chapter provides a detailed description of the SAL program. Section IV.A gives a top-level description of the the major modules of the program.
Reference: [26] <author> D. E. Knuth and R. W. Moore. </author> <title> An analysis of alpha-beta pruning. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 293-326, </pages> <year> 1975. </year>
Reference-contexts: The search procedure determines whether these positions are consistent by generating and evaluating all the legal moves from these two positions (nodes D, E, F, and G). The evaluation values of nodes D and E are propagated up the tree to node B using the negamax procedure <ref> [26] </ref>. If the negamax value of node B is equal to its evaluation value, node B is considered consistent, and no further searching from this position is performed. Similarly, the evaluation values of nodes F and G are used to determine whether node C is consistent. <p> The initial board configuration must be provided in a file that is read during execution. IV.B Search Algorithm SAL uses the consistency search algorithm described in Chapter III combined with the standard alpha-beta search procedure <ref> [26] </ref>. For each position at which it is SAL's turn to move, a full-width, two-ply search tree is generated. An incremental consistency search is performed to determine which nodes of this search tree to expand further. Nodes that are cutoff by the alpha-beta algorithm are not considered for expansion.
Reference: [27] <author> K. Lee and S. Mahajan. </author> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 1-25, </pages> <year> 1988. </year> <month> 94 </month>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan <ref> [27, 28] </ref>), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search.
Reference: [28] <author> K. Lee and S. Mahajan. </author> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36, </pages> <year> 1990. </year>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan <ref> [27, 28] </ref>), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games <ref> [53, 20, 28, 62] </ref>, and learning by playing the game [53, 63, 30]. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players.
Reference: [29] <author> R. Levinson. </author> <title> Experience-based creativity. </title> <year> 1991. </year>
Reference-contexts: An interesting feature of both Figures V.4 and V.5 is that the performance curve is continuing to rise somewhat linearly. This is notably different than what would be expected from a one-ply search, which would level off <ref> [29] </ref>. Although this 81 Figure V.4: SAL versus GNUCHESS | The number of moves SAL makes before the game is over. Each data point is the average of 10 games. 82 Figure V.5: SAL versus GNUCHESS | The number of points SAL captures before the game is over.
Reference: [30] <author> R. Levinson and R. Snyder. </author> <title> Adaptive pattern-oriented chess. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pages 601-606. </pages> <address> Morgan-Kaufman, </address> <year> 1991. </year>
Reference-contexts: Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder <ref> [30] </ref>, and Nitsche [45]). Most of these programs only perform a one-ply search. This chapter will describe some of the difficulties with using only a one-ply search, as well as some of the difficulties with using a deeper search. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games [53, 20, 28, 62], and learning by playing the game <ref> [53, 63, 30] </ref>. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players.
Reference: [31] <author> H. H. Martens. </author> <title> Two notes on machine 'learning'. </title> <journal> Information and Control, </journal> <volume> 2 </volume> <pages> 364-379, </pages> <year> 1959. </year>
Reference-contexts: A full-width, two-ply search would generate all the positions that result from a legal move from the given position, as well as all the positions that result from these 11 12 generated positions. There were several early attempts to learn simple games using rote-learning methods <ref> [49, 31] </ref>, but these were not extended to more complex games. Rote-learning has been used in a competition chess program [56], but the learning was used only to prevent the program from repeating the same moves in human tournaments.
Reference: [32] <author> D. A. McAllester. </author> <title> Conspiracy numbers for min-max search. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 287-310, </pages> <year> 1988. </year>
Reference-contexts: There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* [47], conspiracy number search <ref> [32, 55] </ref>, min/max approximation [50], singular extension [23], and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function.
Reference: [33] <author> J. McCarthy. </author> <title> Chess as the drosophila of AI. </title> <editor> In T. A. Marsland and J. Schaeffer, editors, </editor> <title> Computers, Chess, </title> <journal> and Cognition, </journal> <pages> pages 197-216. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It is reasonable to expect that the game-learning domain will also prove fruitful. I.B Selective Search and Brute Force Search Chess is the Drosophila of Artificial Intelligence. Alexander Kronrod <ref> [33] </ref> Typical chess-playing programs select a move by searching through thousands of moves, countermoves, counter-countermoves, etc., forming a vast game tree of possible board positions. For example, the DEEP THOUGHT program examines over 500,000 positions per second [4].
Reference: [34] <editor> D. Michie. King and rook against king: </editor> <title> Historical background and a problem on the infinite board. </title> <editor> In M. R. B. Clarke, editor, </editor> <booktitle> Advances in Computer Chess 1, </booktitle> <pages> pages 30-58. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1977. </year>
Reference-contexts: Although it might be possible to develop, or even learn, such an accurate evaluation function in some games, there are numerous examples in the chess literature that illustrate how difficult this would be in the case of chess. Donald Michie, in <ref> [34] </ref>, shows two board positions, reproduced here as Figures II.1 and II.2, that are identical except for the position of one pawn. In Figure II.1, d3h7 is a winning move, whereas in Figure II.2 this move loses.
Reference: [35] <author> M. Minsky. </author> <booktitle> Steps toward artificial intelligence. Proceedings of the Institute of Radio Engineers, </booktitle> <volume> 49 </volume> <pages> 8-30, </pages> <month> January </month> <year> 1961. </year> <note> also in Computers and Thought. </note>
Reference-contexts: And on the other hand, systems like chess, or nontrivial parts of mathematics, are too complicated for complete analysis. Without complete analysis, there must always remain some core of search, or "trial and error." - Marvin Minsky <ref> [35] </ref> It appears that the clue to intelligent behavior, whether of men or machines, is highly selective search, the drastic pruning of the tree of possibilities explored.
Reference: [36] <author> D. S. Nau. </author> <title> Pathology on game trees: a summary of results. </title> <booktitle> In Proceedings of the First National Conference on Artificial Intelligence, </booktitle> <pages> pages 102-104, </pages> <address> Stanford, CA, </address> <year> 1980. </year> <institution> Stanford University. </institution>
Reference-contexts: This assumption was made by Samuel for his checkers-learning program [53], since the results of a deeper search were used as training values for the evaluations of the positions generated at the first ply. Mathematical analysis by both Dana Nau <ref> [36, 37] </ref> and Don Beal [10] indicate that the assumption that a deeper search yields a better choice of move is not correct for many games. In fact, their analyses showed the opposite is true, that a deeper search results in worse play. <p> Section III.H contains the details of the analysis. III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased [23]. However, analyses of this procedure performed by Beal [9], Nau <ref> [36, 37, 39] </ref>, and Pearl [47] show that the performance should actually decrease with increasing search depth for many types of games. Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. <p> This comparison is done for various different uniform branching factors in very deep game trees with independent, randomly assigned terminal outcomes. Recall that for these trees a full-width, fixed-depth, minimax search is pathological <ref> [9, 36] </ref>, so a one-ply search gives a lower probability of making an incorrect move than a deeper search. The analysis shows that for sufficiently accurate evaluation functions, the consistency search significantly decreases the probability of making an incorrect move when compared to a one-ply search.
Reference: [37] <author> D. S. Nau. </author> <title> An investigation of the causes of pathology in games. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 257-278, </pages> <year> 1982. </year>
Reference-contexts: This assumption was made by Samuel for his checkers-learning program [53], since the results of a deeper search were used as training values for the evaluations of the positions generated at the first ply. Mathematical analysis by both Dana Nau <ref> [36, 37] </ref> and Don Beal [10] indicate that the assumption that a deeper search yields a better choice of move is not correct for many games. In fact, their analyses showed the opposite is true, that a deeper search results in worse play. <p> Section III.H contains the details of the analysis. III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased [23]. However, analyses of this procedure performed by Beal [9], Nau <ref> [36, 37, 39] </ref>, and Pearl [47] show that the performance should actually decrease with increasing search depth for many types of games. Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. <p> Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal [10], Nau <ref> [37, 41, 40] </ref>, and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological. <p> Nau <ref> [37] </ref> has shown that such dependencies can eliminate pathology in full-width searches. Accordingly, suppose the terminal nodes of the game tree are assigned a WIN with probability p, and a LOSE with probability 1 p.
Reference: [38] <author> D. S. Nau. </author> <title> The last player theorem. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 53-65, </pages> <year> 1982. </year>
Reference-contexts: This means that at nodes with an evaluation value of 0 the search was terminated, and the node coerced, if two or more children also had evaluation values of 0. For hazardous, practically-infinite game trees, two types of trees are much more probable than any others <ref> [38] </ref>: either all but one of the children of the root node have LOSE statuses, or all but one have WIN statuses. If all but one of the children have LOSE statuses, then there is only one possible incorrect move. This is an "easy win" for the root node player.
Reference: [39] <author> D. S. Nau. </author> <title> Decision quality as a function of search depth on game trees. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 30(4) </volume> <pages> 687-708, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Section III.H contains the details of the analysis. III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased [23]. However, analyses of this procedure performed by Beal [9], Nau <ref> [36, 37, 39] </ref>, and Pearl [47] show that the performance should actually decrease with increasing search depth for many types of games. Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes.
Reference: [40] <author> D. S. Nau. </author> <title> On game graph structure and its influence on pathology. </title> <journal> International Journal of Computer and Information Sciences, </journal> <volume> 12(6) </volume> <pages> 367-383, </pages> <year> 1983. </year>
Reference-contexts: Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal [10], Nau <ref> [37, 41, 40] </ref>, and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological.
Reference: [41] <author> D. S. Nau. </author> <title> Pathology on game trees revisited, and an alternative to minimaxing. </title> <journal> Artificial Intelligence, </journal> <volume> 21 </volume> <pages> 221-224, </pages> <year> 1983. </year>
Reference-contexts: Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal [10], Nau <ref> [37, 41, 40] </ref>, and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological.
Reference: [42] <author> J. Von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1947. </year>
Reference-contexts: Assume the game ends after a finite number of moves and one of the two players is the winner, the other the loser. All the possible moves of such a game can be characterized by a game tree <ref> [42] </ref>. A portion of such a tree for a game with only two legal moves from each position is shown in Figure III.1. The consistency search procedure assumes there is an evaluation function f , which is a function of a position.
Reference: [43] <author> A. Newell. </author> <title> The chess machine: An example of dealing with a complex task by adaptation. </title> <booktitle> In 1955 Western Joint Computer Conference, </booktitle> <pages> pages 101-108, </pages> <year> 1955. </year> <month> 95 </month>
Reference-contexts: doubtful whether there is enough information in "win, lose, or draw" when referred to the whole play of the game to permit any learning at all over available time scales: : : For learning to take place, each play of the game must yield much more information. - Alan Newell <ref> [43] </ref> In a game of chess, a player might make over one hundred moves before the outcome of the game is known. Even though the player might have won the game, some of the moves may have been poor. Perhaps the opponent was simply unable to capitalize on the mistakes.
Reference: [44] <author> A. Newell, J. C. Shaw, and H. Simon. </author> <title> Chess playing programs and the problem of complexity. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 2 </volume> <pages> 320-335, </pages> <month> October </month> <year> 1958. </year> <note> also in Computers and Thought. </note>
Reference-contexts: These search methods have been referred to as brute force. When chess programs were first being developed, the speed of the available computers would achieve depths of less than four ply using a brute force search under tournament conditions <ref> [44] </ref>. It was found that a four ply search resulted in poor chess play. It became clear that some method of selective search was required to achieve better play.
Reference: [45] <author> T. Nitsche. </author> <title> A learning chess program. </title> <editor> In M. R. B. Clarke, editor, </editor> <booktitle> Advances in Computer Chess 3, </booktitle> <pages> pages 113-120. </pages> <publisher> Pergamon Press, Oxford, </publisher> <year> 1982. </year>
Reference-contexts: Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche <ref> [45] </ref>). Most of these programs only perform a one-ply search. This chapter will describe some of the difficulties with using only a one-ply search, as well as some of the difficulties with using a deeper search. In addition, the problems with relying on learning methods will be described.
Reference: [46] <author> A. J. Palay. </author> <title> Searching with probabilities. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1985. </year>
Reference-contexts: These procedures include SSS* [47], conspiracy number search [32, 55], min/max approximation [50], singular extension [23], and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function. Palay <ref> [46] </ref> proposed a search procedure where the evaluation function returns a probability distribution as the value of a position rather than a single number.
Reference: [47] <author> J. Pearl. </author> <title> Heuristics: Intelligent search strategies for computer problem solving. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1984. </year>
Reference-contexts: The implication is that typical games like chess are not pathological, whereas games for which it can be shown that standard game tree search is ineffective are in some way contrived and unusual [24]. Many researchers <ref> [47, 14, 10, 57] </ref> have modified the mathematical assumptions used by Nau and Beal in an attempt to explain why games like chess are not pathological. None of these modifications seem compelling. <p> III.A Analyses of Traditional Game Tree Search Computer chess programs using a full-width, fixed-depth, minimax search procedure show improved performance as the depth of the search is increased [23]. However, analyses of this procedure performed by Beal [9], Nau [36, 37, 39], and Pearl <ref> [47] </ref> show that the performance should actually decrease with increasing search depth for many types of games. Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. <p> Such behavior has been termed "pathological." The original analyses by Beal, Nau, and Pearl assume a uniform game tree with random, independently assigned terminal outcomes. Beal [10], Nau [37, 41, 40], and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl <ref> [47] </ref> showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological. Schrufer [57] showed that trees where every won node has at least two lost children are not pathological. <p> There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* <ref> [47] </ref>, conspiracy number search [32, 55], min/max approximation [50], singular extension [23], and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function. <p> Using the evaluation value of the root node as the threshold causes the consistency search procedure to be most sensitive to values better than this threshold. 2. Analyses of the case of multivalued evaluation functions performed by both Pearl <ref> [47] </ref> and Baum [7] showed that if pathology occurs for the bi-valued case, then it also occurs for the multivalued case. This indicates an insensitivity in the analyses between the use of bi-valued or multivalued evaluation functions. For simplicity, only the bi-valued case will be analyzed here. <p> III.F.1 The Status of a Node Assume a two-player game with b legal moves from each position where a fixed number of moves must be made before the outcome of the game is known. The board splitting game described by Pearl <ref> [47, pp. 270-273] </ref> is such a game. The game tree for the board splitting game is a uniform b-ary tree of depth d. <p> Such a tree will be called practically-infinite. This assumption is intuitively appealing since it represents what would normally be considered a very complex position. Pearl <ref> [47] </ref> has shown that if a fixed-depth, full-width search can reach terminal nodes, then pathology is eliminated. By restricting the analysis to practically-infinite game trees, it is shown that the effectiveness of the consistency search procedure is not due to encountering terminal nodes. <p> This analysis only considers bi-valued evaluation functions. The case of continuous evaluation functions was considered by Pearl <ref> [47] </ref>. For a full-width, fixed-depth search, the results for the continuous case were similar to the bi-valued results. For simplicity, only the bi-valued case will be considered for the consistency search procedure to be described. III.F.4 Error Probabilities A notation similar to Pearl's [47] is adopted. <p> continuous evaluation functions was considered by Pearl <ref> [47] </ref>. For a full-width, fixed-depth search, the results for the continuous case were similar to the bi-valued results. For simplicity, only the bi-valued case will be considered for the consistency search procedure to be described. III.F.4 Error Probabilities A notation similar to Pearl's [47] is adopted.
Reference: [48] <author> P. I. Richards. </author> <title> Machines which can learn. </title> <journal> American Scientist, </journal> <volume> 39 </volume> <pages> 711-716, </pages> <year> 1951. </year>
Reference-contexts: Specifically, the machine must be able to learn how to play any two-player board game well by simply playing the game. This idea of a game-learning machine was first suggested by Paul Richards <ref> [48] </ref> and Marvin Weinberg [68] in 1951 .
Reference: [49] <author> P. I. Richards. </author> <title> On game-learning machines. </title> <journal> The Scientific Monthly, </journal> <pages> pages 201-205, </pages> <month> April </month> <year> 1952. </year>
Reference-contexts: Specifically, the machine must be able to learn how to play any two-player board game well by simply playing the game. This idea of a game-learning machine was first suggested by Paul Richards [48] and Marvin Weinberg [68] in 1951 . Richards states in <ref> [49] </ref>, Can one conceive of a machine that has absolutely no initial built-in knowledge but does have an "intelligent" ability to learn almost any game through experience alone? I.A Why Play Games? Newton couldn't have discovered his laws of motion if he had concentrated on trying to understand the laws governing <p> A full-width, two-ply search would generate all the positions that result from a legal move from the given position, as well as all the positions that result from these 11 12 generated positions. There were several early attempts to learn simple games using rote-learning methods <ref> [49, 31] </ref>, but these were not extended to more complex games. Rote-learning has been used in a competition chess program [56], but the learning was used only to prevent the program from repeating the same moves in human tournaments.
Reference: [50] <author> R. L. Rivest. </author> <title> Game tree searching by min/max approximation. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 77-96, </pages> <year> 1988. </year>
Reference-contexts: Each 5 additional move added to the depth of the search tree is called a ply. The DEEP THOUGHT program generally searches over 10 ply before making a move [23]. Several efforts have been made to grow the search tree in a more selective fashion <ref> [4, 50] </ref>. A selective search would determine which sequences of moves should be considered based on characteristics of the board positions examined in the game tree. Promising lines of play would be continued to great depth, whereas bad lines would be terminated early to avoid wasting time. <p> There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* [47], conspiracy number search [32, 55], min/max approximation <ref> [50] </ref>, singular extension [23], and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function. <p> It has simple rules which enabled the move generator for SAL to be written in a couple of hours. In addition, connect-four was used to test the min/max approximation procedure in <ref> [50] </ref> where a simple heuristic evaluation function was described. Lastly, it is a considerably more complex game than tic-tac-toe. V.B.1 SAL versus C4 An opponent program, called C4, was written to play connect-four against SAL. C4 performs the standard alpha-beta, iterative deepening search used in most game playing programs. <p> The move is selected based on the completed search with the greatest depth. Unlike the program described in <ref> [50] </ref>, no move ordering was performed. The C4 program was limited to search trees with less than 1000 nodes. The first move made by the C4 program was made at random. The evaluation function described by Ronald Rivest in [50] was used. <p> Unlike the program described in <ref> [50] </ref>, no move ordering was performed. The C4 program was limited to search trees with less than 1000 nodes. The first move made by the C4 program was made at random. The evaluation function described by Ronald Rivest in [50] was used. This evaluation function is based on the number of partially filled "segments" a move would create. A segment is a group of four slots in a line. The sum of scores from all the segments a move would create is the resulting position's evaluation value.
Reference: [51] <author> D. E. Rumelhart, J. L. McClelland, and et.al. </author> <title> Parallel distributed Processing: </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Unfortunately, no satisfactory scheme for doing this has yet been devised. Neural network learning algorithms have been shown to discover features in many simple problem domains <ref> [51] </ref>. Since it is assumed that only games of perfect 19 information will be played, the current board position must contain sufficient information to allow the best move to be selected. <p> A temporal difference algorithm [61] was used to provide the target value for each example. The value of was set to 0. The evaluation value of the next board position was used as the target value for the evaluation value of the current position. The backpropagation algorithm <ref> [51] </ref> was used to determine the amount each weight of the network should be changed. The weights of the network were actually changed after each game was over.
Reference: [52] <author> S. Russell and E. Wefald. </author> <title> Do the right thing: Studies in limited rationality. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Palay [46] proposed a search procedure where the evaluation function returns a probability distribution as the value of a position rather than a single number. This idea was extended by both Russell and Wefald <ref> [52] </ref> and Baum and Smith [8] to include information about the costs and potential benefits of further node expansions. Although these methods claim various types of optimality, they are not usable for the game-learning task for several reasons: 1.
Reference: [53] <author> A. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <month> July </month> <year> 1959. </year> <note> also in Computers and Thought. </note>
Reference-contexts: All other programs can also be characterized as learning an evaluation function, but have only been designed to learn a specific game. For example, Arthur Samuel <ref> [53] </ref> and Arnold Griffith [20] designed programs that learned the game of checkers. <p> This assumption is based partly on the notion that a deep search reveals winning combinations and traps, and leads to positions that can be more accurately evaluated. This assumption was made by Samuel for his checkers-learning program <ref> [53] </ref>, since the results of a deeper search were used as training values for the evaluations of the positions generated at the first ply. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games <ref> [53, 20, 28, 62] </ref>, and learning by playing the game [53, 63, 30]. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games [53, 20, 28, 62], and learning by playing the game <ref> [53, 63, 30] </ref>. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players. <p> Unfortunately, there can be no teacher to identify which moves are which. A program which only learns from playing games must solve this credit assignment problem. Samuel <ref> [53] </ref> developed an algorithm for his checkers-learning program that provided a training value for each position that occurred in the game. This training value was given by the evaluation of the position after the next move. <p> Since it is not possible to compute all possible features of all possible games, a game-learning program must discover its own features relevant to the game currently being played. The feature-discovery problem was recognized by Samuel during the design of his checkers-learning program <ref> [53] </ref>. For this program, Samuel choose a group of 38 features that might be relevant to the game of checkers, and designed a learning algorithm that would select the 16 best features from among this group. The evaluation function would use these 16 features. <p> One method of improving this learning rate would be to learn from some of the information in the search tree, similar to Samuel's checker program <ref> [53] </ref>. This was not done in the current version of SAL. The alpha-beta search procedure used by SAL reduces the size of the search tree, but leaves some nodes with values different from their negamax values.
Reference: [54] <author> J. Schaeffer. </author> <title> The history heuristic and alpha-beta search enhancements in practice. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(11) </volume> <pages> 1203-1212, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The program was not designed to compete in computer game-playing tournaments, and hence does not include many techniques that have been shown to improve the playing strength of competition programs, such as transposition tables, aspiration windows, and various heuristics <ref> [54, 25] </ref>. Although these techniques would improve SAL's performance, they were not necessary to test the search methods and learning issues described in this dissertation. This chapter provides a detailed description of the SAL program. Section IV.A gives a top-level description of the the major modules of the program.
Reference: [55] <author> J. Schaeffer. </author> <title> Conspiracy numbers. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 67-84, </pages> <year> 1990. </year>
Reference-contexts: There have been several search procedures which attempt to achieve better play by selectively growing the search tree based on the values of the nodes already evaluated. These procedures include SSS* [47], conspiracy number search <ref> [32, 55] </ref>, min/max approximation [50], singular extension [23], and fffi conspiracy number search [2]. Such procedures are not directly applicable to the game-learning task since they assume an error-free evaluation function.
Reference: [56] <author> T. Scherzer, L. Wcherzer, and D. Tjaden. </author> <title> Learning in bebe. </title> <editor> In T. A. Mars-land and J. Schaeffer, editors, </editor> <title> Computers, Chess, </title> <journal> and Cognition, </journal> <pages> pages 197-216. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: There were several early attempts to learn simple games using rote-learning methods [49, 31], but these were not extended to more complex games. Rote-learning has been used in a competition chess program <ref> [56] </ref>, but the learning was used only to prevent the program from repeating the same moves in human tournaments. More recently, a game-learning program named HOYLE was developed by Susan Epstein [17].
Reference: [57] <author> G. Schrufer. </author> <title> Presence and absence of pathology on game trees. </title> <editor> In D. F. Beal, editor, </editor> <booktitle> Advances in computer chess 4, </booktitle> <pages> pages 101-112. </pages> <publisher> Pergamon Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: The implication is that typical games like chess are not pathological, whereas games for which it can be shown that standard game tree search is ineffective are in some way contrived and unusual [24]. Many researchers <ref> [47, 14, 10, 57] </ref> have modified the mathematical assumptions used by Nau and Beal in an attempt to explain why games like chess are not pathological. None of these modifications seem compelling. <p> Beal [10], Nau [37, 41, 40], and Bratko and Gams [14] show that various dependencies between terminal outcomes can prevent pathology. Similarly, Pearl [47] showed that game trees which contain a number of short move sequences that lead to terminal positions, called "traps," are not pathological. Schrufer <ref> [57] </ref> showed that trees where every won node has at least two lost children are not pathological. Although there are certainly chess positions with traps or dependencies, it is not certain that every chess position has these features.
Reference: [58] <author> C. E. Shannon. </author> <title> A Chess-playing machine. </title> <publisher> Scientific American, </publisher> <month> February </month> <year> 1950. </year>
Reference-contexts: Board games are pristine in that they are simple to implement on a computer, have a clear-cut criteria for success or failure, and do not require a large database of facts. Defending his work on developing a chess-playing program, Shannon <ref> [58] </ref> states The chess machine is an ideal one to start with for several reasons. The problem is sharply defined, both in the allowed operations (the moves of chess) and in the ultimate goal (checkmate). It is neither so simple as to be trivial nor too difficult for satisfactory solution. <p> This allows the analysis to assume a fixed-depth search, rather than the variable depth that results from the capture tree. The importance of quiescence was realized by Shannon <ref> [58] </ref> and Turing [66] and experimentally verified by Gillogly [19] for the game of chess. Unfortunately, this procedure has only been shown to be beneficial in games like chess.
Reference: [59] <author> C. E. Shannon. </author> <title> Programming a computer for playing Chess. </title> <journal> Philosophical Mag--azine, </journal> <volume> 41(7) </volume> <pages> 256-275, </pages> <month> March </month> <year> 1950. </year>
Reference-contexts: He even suggested using chess problems as part of this test. Also in 1950, Claude Shannon <ref> [59] </ref> proposed using chess playing ability as a means of answering the question; "Could a machine be designed that would be capable of `thinking' ?" After the first electronic digital computer became operational in 1951 [5], discussions concerning machines, games, and thinking were no longer purely academic. <p> Perhaps the most distinguishing feature of decision problems is their combinatorial nature. Specifically, even though it is theoretically possible to exhaustively search through all sequences of actions for a solution, such a search is not feasible since there are exponentially many sequences. For example, in <ref> [59] </ref> Shannon estimates there are about 10 120 different games of chess. Since it is not possible to exhaustively search for a solution, some other method is required for finding adequate answers to these problems. <p> Applications which have used search methods include robot planning, visual scene analysis, mathematical theorem proving, symbolic integration, and puzzle solving. In <ref> [59] </ref> Shannon listed a number of such applications, stating, Machines of this general type are an extension over the ordinary use of numerical computers in several ways. First, the entities dealt with are not primarily numbers, but rather chess positions, circuits, mathematical expressions, words, etc. <p> The software was not examined to determine if this were the case. In any event, SAL's performance is much improved over its earlier games. Chapter VI Conclusion As described in Chapter I, the SAL program was designed as a step toward an answer to Shannon's question <ref> [59] </ref>: "Could a machine be designed that would be capable of `thinking' ?" Specifically, it was proposed that a "thinking" machine would need to be able to learn to play games well by simply playing the game. <p> Finally, Section VI.D presents some aspects of the full game-learning domain that are not present in the domain selected for the SAL program but which would make a successful program more useful in practical applications. VI.A Summary of Important Ideas and Results * In 1950, Shannon <ref> [59] </ref> proposed the chess-playing problem as a means of an 87 88 swering the question: "Could a machine be designed that would be capable of `thinking' ?" Research to develop a good chess-playing program has helped to improve computer performance in numerous more practical applications.
Reference: [60] <author> D. J. Slate and L. R. Atkin. </author> <title> CHESS 4.5 The Northwestern University chess program. </title> <editor> In P. W. Frey, editor, </editor> <booktitle> Chess Skill in Man and Machine, chapter 4, </booktitle> <pages> pages 82-118. </pages> <publisher> Springer-Verlag, </publisher> <address> 2 edition, </address> <year> 1983. </year>
Reference-contexts: A program searching to such a depth can beat all but the best chess players. In 1977, a program performing a brute force search became the best chess-playing 6 program in the world <ref> [60] </ref>. Since that time, brute force search has continued to perform much better than any known selective search technique. The assumption that a successful method of selective search must be found before a program would be able to play a good game of chess has been proven to be incorrect.
Reference: [61] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This training value was given by the evaluation of the position after the next move. A modified version of this procedure, called the temporal difference method, was formalized and analyzed by Richard Sutton <ref> [61] </ref>. He proved convergence and optimality for this procedure in the linear case. The temporal difference method was used by Gerald Tesauro in his backgammon-learning program [63]. <p> In chess this would correspond to a check. IV.E Temporal Difference Learning Each move of the game being played provided a training example for one of the neural network evaluation functions. A temporal difference algorithm <ref> [61] </ref> was used to provide the target value for each example. The value of was set to 0. The evaluation value of the next board position was used as the target value for the evaluation value of the current position.
Reference: [62] <author> G. Tesauro and T. J. Sejnowski. </author> <title> A parallel network that learns to play Backgammon. </title> <journal> Artificial Intelligence, </journal> <volume> 39 </volume> <pages> 357-390, </pages> <year> 1989. </year>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski <ref> [62, 63] </ref>), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games <ref> [53, 20, 28, 62] </ref>, and learning by playing the game [53, 63, 30]. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players. <p> A raw board position is some simple encoding of the positions of each piece on the board into a set of numbers to be given to the evaluation function. The backgammon program developed by Tesauro and Sejnowski <ref> [62, 63] </ref> uses only the raw board position as inputs to a neural network evaluation function. A problem with only using the raw board position as inputs to the evaluation function is that it requires the learning algorithm to solve the very difficult feature-discovery problem.
Reference: [63] <author> G. J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <type> Technical Report RC 17223 (#76307), </type> <institution> IBM Research Division, T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski <ref> [62, 63] </ref>), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search. <p> The description and analysis of the consistency search procedure is given in Chapter III. II.C Learning an Evaluation Function There have been two main approaches to learning an evaluation function: learning from positions that have occurred in expert games [53, 20, 28, 62], and learning by playing the game <ref> [53, 63, 30] </ref>. Learning from positions that occur in expert games is an off-line training procedure. The evaluation function is isolated from the game-playing portion of the program and is trained to mimic the decisions made by expert human players. <p> A modified version of this procedure, called the temporal difference method, was formalized and analyzed by Richard Sutton [61]. He proved convergence and optimality for this procedure in the linear case. The temporal difference method was used by Gerald Tesauro in his backgammon-learning program <ref> [63] </ref>. He found that the program was able to learn to play backgammon at a "fairly strong intermediate level of performance" which surpassed the performance of the program when trained on human expert games. <p> A raw board position is some simple encoding of the positions of each piece on the board into a set of numbers to be given to the evaluation function. The backgammon program developed by Tesauro and Sejnowski <ref> [62, 63] </ref> uses only the raw board position as inputs to a neural network evaluation function. A problem with only using the raw board position as inputs to the evaluation function is that it requires the learning algorithm to solve the very difficult feature-discovery problem. <p> This choice, as with the choice of learning rate and the range of the initial weights was rather arbitrary, but was based somewhat on the results reported by Tesauro in <ref> [63] </ref>. Chapter II presented the argument that the evaluation function learning should not be relied on to achieve expert play in games such as chess. Search is also required. Consistent with this argument, a minimal amount of effort was spent tuning the neural networks.
Reference: [64] <author> K. Thompson. </author> <title> Retrograde analysis of certain endgames. </title> <journal> Journal of the International Computer Chess Association, </journal> <volume> 9(3) </volume> <pages> 131-139, </pages> <year> 1986. </year>
Reference-contexts: This is not at all clear, even for games as old as chess. For example, in 1986, a computer retrograde analysis of chess endgames discovered that most KQKBB and KQKNN endgame positions can be won, whereas it was previously thought that such positions were draws <ref> [64] </ref>. * A program which only learns from the games of better players, and cannot learn from its own games, raises questions about how the better players learned to play so well.
Reference: [65] <author> A. M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> 59 </volume> <pages> 433-460, </pages> <month> October </month> <year> 1950. </year> <note> also in Computers and Thought. </note>
Reference-contexts: Introduction The relationships between machines, games, and thinking has been a topic of many discussions. In 1948, Norbert Wiener [69] considered whether chess playing ability "represents an essential difference between the potentialities of the machine and the mind [page 193]." In 1950, Alan Turing <ref> [65] </ref> suggested the "imitation game" as a test for whether a machine could think. He even suggested using chess problems as part of this test.
Reference: [66] <author> A. M. </author> <title> Turing. Digital computers applied to games. </title> <editor> In B. V. Bowden, editor, </editor> <title> Faster than Thought: </title> <booktitle> A Symposium on Digital Computing Machines, </booktitle> <pages> pages 286-310. </pages> <publisher> Sir Isaac Pitman and Sons, Ltd., </publisher> <address> London, </address> <year> 1953. </year>
Reference-contexts: This allows the analysis to assume a fixed-depth search, rather than the variable depth that results from the capture tree. The importance of quiescence was realized by Shannon [58] and Turing <ref> [66] </ref> and experimentally verified by Gillogly [19] for the game of chess. Unfortunately, this procedure has only been shown to be beneficial in games like chess. There are many games in which a capture is not even a legal move, and it is difficult to identify a quiescent position.
Reference: [67] <author> L. Uhr. </author> <title> Pattern recognition, learning, and thought: Computer-programmed models of higher mental processes. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1973. </year>
Reference-contexts: I.C Game Learning A common criticism of chess-playing machines is that they can only play chess, and have limited abilities to modify their playing style. It is argued that a thinking machine would not be so limited. In his book Pattern Recognition, Learning, and Thought <ref> [67] </ref>, published in 1973, Leonard Uhr addressed the game-playing problem. What I am arguing is that, very simply, games are far harder than they may look on the surface, and that, further, we are not yet ready to handle them properly.
Reference: [68] <author> M. Weinberg. </author> <title> Mechanism in neurosis. </title> <journal> American Scientist, </journal> <volume> 39 </volume> <pages> 74-98, </pages> <month> January </month> <year> 1951. </year>
Reference-contexts: Specifically, the machine must be able to learn how to play any two-player board game well by simply playing the game. This idea of a game-learning machine was first suggested by Paul Richards [48] and Marvin Weinberg <ref> [68] </ref> in 1951 .
Reference: [69] <author> N. Wiener. </author> <title> Cybernetics: Or control and communication in the animal and the machine. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1948. </year>
Reference-contexts: Introduction The relationships between machines, games, and thinking has been a topic of many discussions. In 1948, Norbert Wiener <ref> [69] </ref> considered whether chess playing ability "represents an essential difference between the potentialities of the machine and the mind [page 193]." In 1950, Alan Turing [65] suggested the "imitation game" as a test for whether a machine could think. He even suggested using chess problems as part of this test.
Reference: [70] <author> S. Yakowitz. </author> <title> A statistical foundation for machine learning, with application to Go-Moku. </title> <journal> Computers Math. Applic., </journal> <volume> 17(7) </volume> <pages> 1095-1102, </pages> <year> 1989. </year>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz <ref> [70] </ref>), and chess (Zobrist and Carlson [71], Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search. This chapter will describe some of the difficulties with using only a one-ply search, as well as some of the difficulties with using a deeper search.
Reference: [71] <author> A. L. Zobrist and Jr. F. R. Carlson. </author> <title> An advice-taking chess computer. </title> <journal> Scientific American, </journal> <volume> 228 </volume> <pages> 92-105, </pages> <month> June </month> <year> 1973. </year> <month> 96 </month>
Reference-contexts: For example, Arthur Samuel [53] and Arnold Griffith [20] designed programs that learned the game of checkers. Other board games in which learning programs have been designed include othello (Lee and Mahajan [27, 28]), backgammon (Tesauro and Sejnowski [62, 63]), go-moku (Yakowitz [70]), and chess (Zobrist and Carlson <ref> [71] </ref>, Levinson and Snyder [30], and Nitsche [45]). Most of these programs only perform a one-ply search. This chapter will describe some of the difficulties with using only a one-ply search, as well as some of the difficulties with using a deeper search.
References-found: 71

