URL: http://www.ri.cmu.edu/afs/cs/project/nnspeech-10/tpolzin/WWW/publications/cmc-98.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/nnspeech-10/tpolzin/WWW/publications.html
Root-URL: 
Phone: 2  
Title: Detecting Emotions in Speech  
Author: Thomas S. Polzin and Alex H. Waibel ; 
Keyword: multimodal communication, user modelling, emotion detection  
Address: Pittsburgh, PA 15213, USA  Germany  
Affiliation: 1 School of Computer Science, Carnegie Mellon University,  Fakultat fur Informatik, University of Karlsruhe,  
Abstract: Human language carries various kinds of information. In human computer interaction the detection of the emotional state of a speaker as reflected in his or her utterances is crucial. In this investigation we will explore how acoustic and prosodic information can be used to detect the emotional state of a speaker. We will show how prosodic information can be combined and integrated with acoustic information within a hidden Markov model architecture, which allows one to make observations at a rate appropriate for the phenomena to be modeled. Using this architecture, we will demonstrate that prosodic information adds discriminative power to the overall system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> C. Bregler, S. Manke, H. Hild, and A. Waibel. </author> <title> Improving connected letter recognition by lip reading. </title> <booktitle> In ICASSP, Minneapolis, 1993. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing. </booktitle>
Reference: <author> F. Dellaert, T.S. Polzin, and A. Waibel. </author> <title> Recognizing emotions in speech. </title> <booktitle> In ICSLP, </booktitle> <year> 1996. </year>
Reference: <author> R. Frick. </author> <title> Communicating emotion. the role of prosodic features. </title> <journal> Psychological Bulletin, </journal> <volume> 97(3) </volume> <pages> 412-429, </pages> <year> 1985. </year>
Reference: <author> M.A.K. Halliday. </author> <title> A course in spoken English. </title> <publisher> Oxford University Press, </publisher> <address> London, England, </address> <year> 1970. </year>
Reference: <author> S. Kaiser and T. Wehre. </author> <title> Automated coding of facial behavior in human-computer interactions with FACS. </title> <journal> Journal of Nonverbal Behaviour, </journal> <volume> 16(2) </volume> <pages> 67-83, </pages> <year> 1992. </year>
Reference: <author> I.R. Murray and J.L. Arnott. </author> <title> Synthesizing emotions in speech: Is it time to get excited? In ICLSP, </title> <type> 1816-1819, </type> <year> 1996. </year>
Reference: <author> T.S. Polzin. </author> <title> Suprasegmental hidden Markov models. </title> <type> Technical report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> 5000 Forbes Avenue, Pittsburgh PA 15213, USA, </address> <note> to appear. </note>
Reference: <author> R. Stiefelhagen, U. Meier, and J. Yang. </author> <title> Real-time lip-tracking for lip reading. </title> <booktitle> In Eurospeech 9, </booktitle> <year> 1997. </year>
Reference: <author> K.R. Scherer, D.R. Ladd, and K.E.A. Silverman. </author> <title> Vocal cues to speaker affect: Testing two models. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 76 </volume> <pages> 1346-1356, </pages> <year> 1984. </year>
Reference: <author> K.R. Scherer, R. Banse, H.G. Wallbott, and T. Goldbeck. </author> <title> Vocal cues in emotion encoding and de-coding. </title> <journal> Motivation & Emotion, </journal> <volume> 2(15) </volume> <pages> 123-148, </pages> <year> 1991. </year>
Reference: <author> K.R. Scherer and R. Banse. </author> <title> Acoustic profiles in vocal emotion expression. </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> 70, </volume> <pages> 614-636, </pages> <year> 1996. </year>
Reference: <author> S. Kaiser and K.R. Scherer. </author> <title> Models of 'normal' emotions applied to facial and vocal expressions in clinical disorders. </title> <editor> In: W.F. Flack and J.D. Laird (Eds.), Emotions in psychopathology, </editor> <address> New York:Oxford University Press, </address> <note> to appear. </note>
Reference: <author> A. Takeuchi and K. Nagao. </author> <title> Communicative facial display as a new conversational display. </title> <type> Technical report, </type> <institution> SCSL-TR-92-019, Sony Computer Science Laboratory, </institution> <address> Tokyo, </address> <year> 1992. </year>
Reference: <author> T. Zeppenfeld, M. Finke, K. Ries, M. Westphal, and A. Waibel. </author> <title> Recognition of conversational telephone speech using the Janus speech engine. </title> <booktitle> In IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Munich , Germany, </address> <year> 1997. </year>
References-found: 14

