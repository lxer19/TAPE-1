URL: http://www.cf.ac.uk/uwcc/maths/sahu/papers/em.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On Convergence of the EM Algorithm and the Gibbs Sampler  SUMMARY  
Author: Sujit K. Sahu Gareth O. Roberts 
Keyword: Key words: EM algorithm; Gaussian distribution; Generalized linear mixed models; Gibbs sampler; Markov chain Monte Carlo; Parameterization; Rates of convergence.  
Date: April 15, 1998  
Address: Cardiff, CF2 4YH, UK.  Cambridge, CB2 1SB, UK.  
Affiliation: School of Mathematics University of Wales, Cardiff  Statistical Laboratory University of Cambridge  
Abstract: In this article we investigate the relationship between the two popular algorithms, the EM algorithm and the Gibbs sampler. We show that the approximate rate of convergence of the Gibbs sampler by Gaussian approximation is equal to that of the corresponding EM type algorithm. This helps in implementing either of the algorithms as improvement strategies for one algorithm can be directly transported to the other. In particular, by running the EM algorithm we know approximately how many iterations are needed for convergence of the Gibbs sampler. We also obtain a result that under conditions, the EM algorithm used for finding the maximum likelihood estimates can be slower to converge than the corresponding Gibbs sampler for Bayesian inference which uses proper prior distributions. We illustrate our results in a number of realistic examples all based on the generalized linear mixed models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breslow, N. E. and Clayton, D. G. </author> <title> (1993) Approximate inference in generalized linear mixed models. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 88, </volume> <pages> 9-25. </pages>
Reference-contexts: This confirms results from similar empirical studies by Gilks (1997) for a different problem. 2 As a second example, we consider the more general class of generalized linear mixed models (GLMM) <ref> (Breslow and Clayton, 1993) </ref>. From the EM implementation we show how to obtain approximate rate of convergence of the Gibbs sampler (by Gaussian approximation) which help determining a burn-in period.
Reference: <author> Box, G. E. P. and Tiao, G. C. </author> <title> (1992) Bayesian Inference in Statistical Analysis. </title> <publisher> London: Addison-Wesley. </publisher>
Reference: <author> Crowder, M. J. </author> <title> (1978) Beta-binomial ANOVA for proportions. </title> <journal> Appl. Statist., </journal> <volume> 27, </volume> <pages> 34-37. </pages>
Reference: <author> Dellaportas, P. and Smith, A. F. M. </author> <title> (1993) Bayesian inference for generalised linear and proportional hazards models via Gibbs sampling. </title> <journal> Appl. Statist., </journal> <volume> 42, </volume> <pages> 443-459. </pages>
Reference: <author> Dempster, A. P., Laird, N. M. and Rubin, D. B. </author> <title> (1977) Maximum likelihood from from incomplete data via the EM algorithm (with discussion). </title> <journal> J. R. Statist. Soc., B, </journal> <volume> 39, </volume> <pages> 1-38. </pages> <note> 18 Gelfand, </note> <author> A. E., Sahu, S. K. and Carlin, B. P. </author> <title> (1995) Efficient parametrization for normal linear mixed models. </title> <journal> Biometrika, </journal> <volume> 82, </volume> <pages> 479-488. </pages>
Reference-contexts: 1 Introduction The EM algorithm <ref> (Dempster, Laird and Rubin, 1977) </ref> and the Gibbs sampler have both been highly successful in extending the scope of statistical models that are computationally feasible. Accessible discussions about these algorithms are found in recent texts, e.g., Gilks, Richardson and Spiegelhalter (1996) and Tanner (1996).
Reference: <author> Gelman, A. </author> <title> (1997) Discussion of the paper by Meng and van Dyk. </title> <journal> J. R. Statist. Soc., </journal> <volume> B 59, </volume> <pages> 554. </pages>
Reference-contexts: Moreover, this parameterization is an example of positive association where all the partial correlations between parameters are non-negative. Therefore the results of Theorem 2 can be expected to apply to it. The third parameterization we consider was suggested in Meng and van Dyk (1997), <ref> (see also Gelman, 1997) </ref>, and assumes y ij = + t a=2 b i + * ij ; i = 1; : : : ; I; j = 1; : : : ; J; and b i ~ N (0; t (1+a) ); where a is a given constant.
Reference: <author> Gilks, W. R. </author> <title> (1997) Discussion of the paper by Meng and van Dyk. </title> <journal> J. R. Statist. Soc., </journal> <volume> B 59, </volume> <pages> 543-545. </pages>
Reference: <author> Gilks, W. R., Best, N. G. and Tan, K. K. C. </author> <title> (1995) Adaptive Rejection Metropoils Sampling within Gibbs Sampling. </title> <journal> Appl. Statist., </journal> <pages> 455-472. </pages>
Reference-contexts: The ECM M-step for t under AUXA requires finding the root of polynomial like equations with non-integer powers. The Gibbs updating step for t in the AUXA parameterization requires the ability to sample from non-log-concave distributions and we used the adaptive rejection Metropolis sampling <ref> (Gilks et al., 1995) </ref> for this. We follow the usual method of detecting convergence for the ECM algorithm. If the squared Euclidean distance between the current and the last iterate is less than a pre-specified quantity (10 20 in our case) then we declare convergence and stop the algorithm.
Reference: <author> Gilks, W. R., Richardson, S. and Spiegelhalter, D. G. </author> <title> (1996) Markov Chain Monte Carlo In Practice, </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <editor> Jacobs, D. (Eds.) </editor> <booktitle> (1977) The State of the Art in Numerical Analysis. </booktitle> <address> London: </address> <publisher> Academic Press. </publisher>
Reference: <author> Liu, J. S. </author> <title> (1996) Fraction of Missing Information and Convergence Rate of Data Augmentation. </title> <type> Preprint. </type>
Reference: <author> McCullagh, P. and Nelder, J. A. </author> <title> (1989) Generalized Linear Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> McCulloch, C, E. </author> <title> (1997) Maximum Likelihood Algorithms for Generalized Linear Mixed Models. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 92, </volume> <pages> 162-170. </pages>
Reference: <author> Meng, X.-L. </author> <title> (1994) On the Rate of Convergence of the ECM Algorithm. </title> <journal> Ann. Statist., </journal> <volume> 22, </volume> <pages> 326-339. </pages>
Reference: <author> Meng, X.-L. and Rubin, D. B. </author> <title> (1993) Maximum likelihood estimation via the ECM algorithm: a general framework. </title> <journal> Biometrika, </journal> <volume> 80, </volume> <pages> 267-278. </pages> <editor> Meng, X-L. and van Dyk, D. </editor> <title> (1997) The EM algorithm-an old folk-song sung to a fast new tune (with discussion). </title> <journal> J. R. Statist. Soc., </journal> <volume> B 59, </volume> <pages> 511-567. </pages>
Reference-contexts: Introduce constraints g i (); i = 1; : : : ; m and maximize Q (j (t) ) subject to g i (); i = 1; : : : ; m. The constraints should satisfy a `space filling' requirement <ref> (Meng and Rubin, 1993) </ref> which essentially says that the maximization of Q (j (t) ) by repeated conditional maximizations is over the whole parameter space fi, say, not a subspace of it.
Reference: <author> Roberts, G. O. and Sahu, S. K. </author> <title> (1996) Rate of Convergence of the Gibbs Sampler by Gaussian Approximation. </title> <type> Technical Report, </type> <institution> University of Cambridge. </institution>
Reference-contexts: These results do indicate that the rates of convergence of the Gibbs samplers by Gaussian approximations <ref> (Roberts and Sahu, 1996) </ref> have value as these rates of convergence are those of the corresponding EM-type algorithms. The general formulation outlined above lends itself to many interesting practical implications.
Reference: <author> Roberts, G. O. and Sahu, S. K. </author> <title> (1997) Updating Schemes, Correlation Structure, Blocking and Parameter-isation for the Gibbs Sampler. </title> <journal> J. R. Statist. Soc. B. </journal> <volume> 59, </volume> <pages> 291-317. </pages>
Reference-contexts: Here typical choices of the divergence measure include the total variation distance; the 2 distance and the Kullback-Liebler distance, <ref> (see e.g., Roberts and Sahu, 1997) </ref>. Although the two types of algorithms are very different in nature, the role of in each case is very similar. The following argument applies in both cases. <p> Hence, combining Theorems 1 and 2 we can conclude that the EM type algorithms for classical inference will be slower than the Gibbs samplers for Bayesian inference with proper prior distribution, under positive association. The above result <ref> (and many similar results in Roberts and Sahu, 1997) </ref> can be extended to other equivalent correlation structures as follows. <p> This parameterization introduces additional conditional independence relationships into the model, a commonly beneficial ingredient for acceptable mixing of the Gibbs sampler <ref> (see for example Roberts amd Sahu, 1997) </ref>. Moreover, this parameterization is an example of positive association where all the partial correlations between parameters are non-negative. Therefore the results of Theorem 2 can be expected to apply to it.
Reference: <author> Roberts, G. O. and Sahu, S. K. </author> <title> (1997b) Discussion of the paper by Meng and van Dyk. </title> <journal> J. R. Statist. Soc., </journal> <volume> B 59, </volume> <pages> 558-559. </pages>
Reference: <author> Sahu, S. K. </author> <title> (1997) Bayesian Estimation and Model Choice in the Item Response Models. </title> <note> Available from http://www.cf.ac.uk//uwcc/maths/sahu/. </note>
Reference-contexts: Here typical choices of the divergence measure include the total variation distance; the 2 distance and the Kullback-Liebler distance, <ref> (see e.g., Roberts and Sahu, 1997) </ref>. Although the two types of algorithms are very different in nature, the role of in each case is very similar. The following argument applies in both cases. <p> Hence, combining Theorems 1 and 2 we can conclude that the EM type algorithms for classical inference will be slower than the Gibbs samplers for Bayesian inference with proper prior distribution, under positive association. The above result <ref> (and many similar results in Roberts and Sahu, 1997) </ref> can be extended to other equivalent correlation structures as follows. <p> This parameterization introduces additional conditional independence relationships into the model, a commonly beneficial ingredient for acceptable mixing of the Gibbs sampler <ref> (see for example Roberts amd Sahu, 1997) </ref>. Moreover, this parameterization is an example of positive association where all the partial correlations between parameters are non-negative. Therefore the results of Theorem 2 can be expected to apply to it.
Reference: <author> Schafer, D. W. </author> <title> (1987) Covariate measurement error in generalized linear models. </title> <journal> Biometrika, </journal> <volume> 74, </volume> <pages> 385-391. </pages> <note> 19 Steele, </note> <author> B. M. </author> <title> (1996) A Modified EM Algorithm for Estimation in Generalized Mixed Models. </title> <journal> Biometrics, </journal> <volume> 52, </volume> <pages> 1295-1310. </pages>
Reference: <author> Spiegelhalter, D. J., Thomas, A. and Best, N. G. </author> <title> (1996) Computation on Bayesian graphical models. In Bayesian Statistics 5, </title> <editor> (Eds. J. M. Bernardo, J. O. Berger, A. P. Dawid and A. F. M. Smith). </editor> <publisher> Oxford: Oxford University Press, </publisher> <pages> pp. 407-426. </pages>
Reference-contexts: Some of the problems can be alleviated by using clever parameterizations. As an example data set for these models (described below) we consider the dyestuff data set found in Box and Tiao (1992, page 246). The BUGS <ref> (Spiegelhalter et al., 1996) </ref> implementation for this data set reports extremely slow convergence, the package recommending using 50,000 Gibbs sampled values for estimation after discarding first 5000 iterates. <p> We found that the exact rate of convergence of the EM algorithm using the nlminb routine was quite close to our estimate in every simulation. Returning to the observed data set, we run a Gibbs sampler <ref> (using BUGS, Spiegelhalter et al., 1996) </ref> with the converged EM estimates as the starting values. Since the EM algorithm converges quickly (rate of convergence is about 0.57) we can expect quick convergence of the Gibbs sampler. <p> After finding the mode we evaluate the Hessian matrix at the mode. This matrix is then used to evaluate the rate of convergence of the Gibbs sampler. The rate of convergence so obtained is 0.97. Now we run a Gibbs sampler <ref> (using BUGS, Spiegelhalter et al., 1996) </ref> starting at the posterior mode. Empirical evidence, see e.g., Figure 2, suggests that the Gibbs sampler does not converge quite as rapidly as the previous example and mixing is rather slow.
Reference: <author> Tanner, M. A. </author> <title> (1996) Tools for Statistical Inference. </title> <publisher> Springer-Verlag: Heidelberg. </publisher> <editor> van Dyk, D. and Meng, X.-L. </editor> <title> (1997) On the Orderings and Groupings of Conditional Maximizations within ECM-type Algorithms. </title> <journal> J. Comput. Graph. Statist., </journal> <volume> 6, </volume> <pages> 202-223. </pages>
Reference: <author> Wedderburn, R. W. M. </author> <title> (1976) On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models. </title> <journal> Biometrika, </journal> <volume> 63, </volume> <pages> 27-32. </pages>
Reference: <author> Whitmore, A. S. and Keller J. B. </author> <title> (1989) Approximations for Regression With Covariate Measurement Error. </title> <journal> J. Amer. Statist. Assoc., </journal> <volume> 83, </volume> <pages> 1057-1066. 20 </pages>
References-found: 24

