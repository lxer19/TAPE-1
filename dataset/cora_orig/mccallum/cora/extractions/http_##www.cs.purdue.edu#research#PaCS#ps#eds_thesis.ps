URL: http://www.cs.purdue.edu/research/PaCS/ps/eds_thesis.ps
Refering-URL: http://www.cs.purdue.edu/research/PaCS/parasol.html
Root-URL: http://www.cs.purdue.edu
Title: A SYSTEM FOR MULTITHREADED PARALLEL SIMULATION AND COMPUTATION WITH MIGRANT THREADS AND OBJECTS  
Author: by Edward Mascarenhas 
Degree: A Thesis Submitted to the Faculty of  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Date: August 1996  
Affiliation: Purdue University  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 199 BIBLIOGRAPHY </institution>
Reference: [1] <author> Marc Abrams. </author> <title> The object library for parallel simulations(OLPS). </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 210-219, </pages> <year> 1988. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> Some PDES systems are targeted primarily towards shared memory multiprocessors, for example, Fujimoto's Time Warp system [40], OLPS <ref> [1] </ref>, and SimKit [47]. In a shared memory environment, simulator kernel data structures may be shared between LPs. Because this results in significantly smaller communication costs, shared memory execution is more efficient than distributed memory execution. <p> The algorithm locates the j-th smallest array element through a scan which splits an N element array A in such as way that A [0]; A <ref> [1] </ref>; :::; A [j 2] A [j 1] A [j]; :::A [N 1]. FIND can be used to locate the median and halve an array. The idea is to give newly created quicksort threads equal workloads.
Reference: [2] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Both numbers drop when recursion is introduced. Fewer threads implies reduced threads management and context-switching costs, which is why performance improves. 4.3.3 Quicksort The well-known quicksort algorithm <ref> [2] </ref> operates recursively, to sort a list S of n integers. A partition () procedure uses a pivot to split S and each recursively obtained subarray into three pieces.
Reference: [3] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Other systems attempt to combine the advantages of kernel-space and user-space threads <ref> [3, 71] </ref>. A POSIX standard for a threads interface is also available [55]. Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors. <p> Each approach has its advantages and disadvantages; a hybrid implementation is also possible <ref> [3, 71] </ref>. Among the major advantages of user-space threads are cheap thread management, portability, and customization. The chief disadvantage is that a blocking system call in a thread causes its host process to block, preventing other ready threads from running.
Reference: [4] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [5] <author> Y. Arsty and R. Finkel. </author> <title> Designing a process migration facility: </title> <booktitle> The Charlotte experience. IEEE Computer, </booktitle> <pages> pages 47-56, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others [106, 107]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [117], DEMOS/MP [88], Charlotte <ref> [5] </ref>, Sprite [29]. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support. Creating maps of run-time images of threads on heterogeneous machines is sufficiently complicated to render the effort impractical, particularly with respect to portability.
Reference: [6] <author> D. Baezner, G. Lomow, and B. W. Unger. </author> <title> Sim++: The transition to distributed simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 211-218, </pages> <year> 1990. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie [8], SPEEDES [110], CPSim [49], Sim++ <ref> [6] </ref>, ModSim [123], and TWOS [57]. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations.
Reference: [7] <author> R. L. Bagrodia. </author> <title> Iterative design of efficient simulations using Maisie. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 243-247, </pages> <year> 1991. </year>
Reference-contexts: Such a distinct process is required due to the need for coordination among multiple logical processes. 5.1.2 Example: A Closed Queuing Network We provide a ParaSol model of a closed queuing network (CQN) using the queuing domain library. The same example can be developed in Maisie <ref> [7] </ref> based on the active-server approach. <p> Moreover, migration is a direct form of active-transaction flow, resulting in a powerful threads-based modeling abstraction. In contrast, the Maisie model for the same application requires the definition of message types to denote the flowing components of the system <ref> [7] </ref>. 5.2.5 ParaSol Process Mechanisms SP mechanisms provide parallel computing support to LPs within the process, and also involve aspects of the simulation that require coordination between multiple SPs. 5.2.5.1 Initialization Initialization occurs when the controller process spawns off processes on other processors specified by the user.
Reference: [8] <author> R. L. Bagrodia and Wen-Toh Liao. Maisie: </author> <title> A language and optimizing environment for distributed simulation. </title> <booktitle> In Proceedings of SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 205-210, </pages> <year> 1990. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie <ref> [8] </ref>, SPEEDES [110], CPSim [49], Sim++ [6], ModSim [123], and TWOS [57]. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations. <p> Some threads systems provide special kinds of threads that are stateless (without stacks) [39] and some do not allow context-switching from functions called by the thread-function <ref> [8] </ref>. Needless to say, these systems cannot be used to build general purpose discrete event simulators without additional restrictions on modeling. There is an analogous function to the one performed by the simulation driver in a threads system.
Reference: [9] <author> D. Ball and S. Hoyt. </author> <title> The adaptive time-warp concurrency control algorithm. </title> <booktitle> In Proceedings of the SCS MultiConference on Distributed Simulation, </booktitle> <pages> pages 174-177, </pages> <year> 1990. </year>
Reference-contexts: That is, arriving messages do not cause rollbacks at a destination [26, 44]. In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior <ref> [9, 94] </ref>. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations [69, 89]. <p> Indeed, the system was designed for practical use and for experimentation. The proposed method is based, in part, on some ideas presented in <ref> [9] </ref>. In essence, the proposal is to minimize loss in either waiting for late transactions, or in undoing work. Before processing a transaction, an LP examines its input channels and computes an optimal delay interval. Transaction processing is then suspended for a period based on this interval. <p> If the delay interval computed turns out to be too small, the LP may simply spin in a busy loop. We present a model based on a rollback cost and delay analysis, similar to that presented in <ref> [9] </ref> and [36]. Our work differs from theirs in some respects, such as in how expected cost is computed. We estimate transaction interarrival distributions (both virtual time and real time) on input channels and use these, along with other costs, to estimate rollback probability and cost.
Reference: [10] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software-Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55]. <p> For our experiments, g 1 (x) = x fl x + 1:0, g 2 (x) = sin (x), with x 2 <ref> [10; 10] </ref>, and ffi = 10 7 . All runs were made on a 4-processor SPARCstation 20. Measured execution time and speedup are shown in Figure 4.17. Measured time excludes initializations, and speedup is measured relative to sequential (single-threaded) execution.
Reference: [11] <author> A. S. Birell. </author> <title> An introduction to programming with threads. </title> <type> Technical Report 35, </type> <institution> DEC Systems Research Center, </institution> <month> January </month> <year> 1989. </year> <month> 200 </month>
Reference-contexts: Using threads also has some disadvantages. Since one thread can access the stack of any other thread and can therefore affect it in unpredictable ways, it is more complex to develop applications based on threads <ref> [11] </ref>. Process-oriented simulation systems like CSIM and Maisie use some form of threads to represent entities in the system. However, threads in these systems are integrated with the simulation system. In our approach the threads system is separate from the simulator.
Reference: [12] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: The time required to migrate a thread is only 1-3% larger than the time required to move a message of the same size. 4.2.3.1 Data Access Methods Remote Procedure Calls and Data Shipping are traditional ways to access remote data. Each RPC-style <ref> [12, 19, 25] </ref> access requires two messages, a send and a receive. The former delivers parameters to a procedure, and the latter returns results. In the traditional process-oriented model the sender typically blocks on the send. With threads, the sending process is free to continue with other work.
Reference: [13] <author> R. D. Blumofe, C. E. Leiserson, C. F. Joerg, K. H. Randall, B. C. Kuszmaul, and Y. Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY [64], and Filaments [31, 39]. Other systems, for example, Cid [83], Cilk <ref> [13] </ref>, Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms.
Reference: [14] <author> P. A. Buhr and R. A. Stroobosscher. </author> <title> The System: Providing light-weight con-currency on shared-memory multiprocessor computers running Unix. </title> <journal> Software-Practice and Experience, </journal> <volume> 20(9) </volume> <pages> 929-964, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [15] <author> C. Burdorf and J. Marti. </author> <title> Load balancing strategies for time warp on multi-user workstations. </title> <journal> The Computer Journal, </journal> <volume> 36(2) </volume> <pages> 168-176, </pages> <year> 1993. </year>
Reference-contexts: One of the factors that affect performance is load balance. Given that load may become unbalanced, a run-time mechanism is required to balance the load. Dynamic load-balancing schemes in the context of PDES have only recently begun to appear <ref> [15, 45, 95] </ref>. In the absence of an automatic partitioning and load balancing scheme the PDES system must provide adequate support to control these aspects manually at program initialization time. The granularity of the computation in relation to communication is also decided by the partitioning.
Reference: [16] <author> M. Carlisle and A. Rogers. </author> <title> Software caching and computation migration in Olden. </title> <type> Technical Report TR-483-95, </type> <institution> Department of Computer Science, Prince-ton University, </institution> <year> 1995. </year>
Reference-contexts: Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY [64], and Filaments [31, 39]. Other systems, for example, Cid [83], Cilk [13], Olden <ref> [16] </ref>, provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms. <p> Complex object migration is nontrivial, given that internal objects and/or pointers to other objects require packing and unpacking at source and destination processes, respectively. Another alternative to the two schemes mentioned above is computation migration <ref> [16, 53] </ref>. Here, part of a computation (e.g., the topmost function in a sequence 65 of nested calls) is moved to a process hosting required data.
Reference: [17] <author> K. M. Chandy and J. Misra. </author> <title> Distributed simulation: A case study in design and verification of distributed programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(5) </volume> <pages> 440-452, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: The major focus of parallel simulation research, in recent years, has centered around assessment of these two synchronization methods. 1.2.1 Synchronization in PDES In the conservative approach <ref> [17] </ref>, events are executed strictly in order of their occurrence in simulation time. While this prevents causality errors, it may lead to deadlock, since an LP may block forever, waiting for input messages. <p> not met) f read input messages if necessary block until safe to process select and remove the process with minimum reactivation time from the calendar local clock = selected process timestamp pass control to the process send messages to other LPs as required g called Null messages, to prevent deadlocks <ref> [17] </ref>. Figure 1.5 shows the driver for a conservative parallel execution at one LP. Conservative mechanisms must be able to determine when it is safe to process the next event. Messages arriving on input channels to an LP are buffered in FIFO order and must be in nondecreasing timestamp order. <p> The CPSim programming model is a directed graph of simulated objects, with event messages being passed between objects. 2.1.1 Adaptive Synchronization The study of synchronization protocols is a major area of research in PDES. The well known conservative protocols are based on null messages <ref> [17] </ref>, deadlock detection and recovery [18]. Optimistic synchronization is based on the notion of virtual time [58], and state-saving and rollback mechanisms. 20 Both conservative and optimistic methods have their advantages and disadvantages [41].
Reference: [18] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11) </volume> <pages> 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: The CPSim programming model is a directed graph of simulated objects, with event messages being passed between objects. 2.1.1 Adaptive Synchronization The study of synchronization protocols is a major area of research in PDES. The well known conservative protocols are based on null messages [17], deadlock detection and recovery <ref> [18] </ref>. Optimistic synchronization is based on the notion of virtual time [58], and state-saving and rollback mechanisms. 20 Both conservative and optimistic methods have their advantages and disadvantages [41].
Reference: [19] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield. </author> <title> The Amber System: Parallel programming on a network of multiprocessors. </title> <booktitle> In Symposium on Operating System Principles, </booktitle> <pages> pages 147-158, </pages> <year> 1989. </year>
Reference-contexts: Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors. Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber <ref> [19] </ref>, Clouds [25], IVY [64], and Filaments [31, 39]. Other systems, for example, Cid [83], Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms. <p> The time required to migrate a thread is only 1-3% larger than the time required to move a message of the same size. 4.2.3.1 Data Access Methods Remote Procedure Calls and Data Shipping are traditional ways to access remote data. Each RPC-style <ref> [12, 19, 25] </ref> access requires two messages, a send and a receive. The former delivers parameters to a procedure, and the latter returns results. In the traditional process-oriented model the sender typically blocks on the send. With threads, the sending process is free to continue with other work. <p> Further, this slot must be reserved for the thread on all machines in the distributed system, just in case the thread makes an appearance at a machine. Such an approach is adopted by the Amber system <ref> [19] </ref>. A significant advantage of this approach is that migration of a thread from one machine to another does not require address translation of pointers on the stack. But this advantage is had at the expense of address space. <p> Transaction flow enables transactions to access servers hosted in other LPs in the system. Most threads systems do not support migration as a means of accessing remote objects; some provide remote procedure call or data shipping to achieve the same effect <ref> [19, 64] </ref>. The advantages of thread migration have already been discussed in Section 4.2.3. Migration entails moving a transaction from one processor to another. Ariadne provides the low-level thread migration support required by ParaSol .
Reference: [20] <author> K. Chung, J. Sang, and V. Rego. </author> <title> A performance comparison of event calendar algorithms: An empirical approach. </title> <journal> Software-Practice and Experience, </journal> <volume> 23(10) </volume> <pages> 1107-1138, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Figure 5.6 shows the calendar, calendar entries, the Thread Context Module, and thread context entries. Other more efficient organizations of the calendar particularly when the number of entries is large, can be accommodated because of the modular design. One such organization is the tree structure <ref> [20] </ref>. The calendar stores events that have executed in the past, and events scheduled for future execution.
Reference: [21] <author> Douglas Comer and David L. Stevens. </author> <title> Internetworking with TCP/IP: Client-Server Programming and Applications. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: use of the system for monitoring of parallel applications in EcliPSe is described in [60], and examples of its application in simulation, and parallel programming with the Ariadne threads system can be found in [77]. 5.4.1 The DISplay System Architecture The DISplay software architecture is based on the client-server paradigm <ref> [21] </ref>. The simulation application, which is created by an analyst or programmer, is treated as a client. Client calls are made by invoking functions resident in the DISplay client library.
Reference: [22] <author> B. A. Cota and R. G. Sargent. </author> <title> A modification of the process interaction world view. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(2) </volume> <pages> 109-129, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In general, when the behavior of the servers in the system is complex the active server approach is considered to be more useful <ref> [22] </ref>. Neither one of the two modeling views is clearly preferable to the other. In our experience, however, the modeling of transaction flow via active processes (threads), and passive servers through objects, can potentially enhance the entire modeling effort, from model design and validation, to code debugging and maintenance.
Reference: [23] <author> S. Crane. </author> <title> The Rex lightweight process library. </title> <type> Technical report, </type> <institution> Department of Computing, Imperial College of Science, Technology and Medicine, </institution> <address> London, England, </address> <year> 1993. </year> <month> 201 </month>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [24] <author> S. R. Das and R. M. Fujimoto. </author> <title> An adaptive memory management protocol for time warp parallel simulation. </title> <booktitle> In Proceedings of the 1994 ACM Sigmetrics Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 201-210, </pages> <year> 1994. </year>
Reference-contexts: These include estimates based on the arithmetic mean, median, exponentially smoothed average, and computationally intensive auto-regressive and integrated moving-average forecasting methods. Some success was reported with these methods [34, 35]. Another proposed strategy for limiting optimism is based on memory management <ref> [24] </ref>. Here, optimism is limited in an indirect manner, by controlling the amount of memory provided to an LP. The adaptive protocol attempts to provide each LP with only as much memory as necessary for optimal performance. <p> Adaptive synchronization methods, in particular, provide a framework for tailoring synchronization to the peculiarities of a given application. Based on decisions made with run-time data, such methods offer a dynamic combination of optimistic and conservative synchronization, and have been reported to exhibit poor to reasonable performance in previous studies <ref> [24, 36, 50, 108] </ref>. Of particular importance is the fact that extreme forms of optimistic progress and conservative blocking can be avoided. In this section, we present a new and dynamic method for adaptive synchronization.
Reference: [25] <author> P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen. </author> <title> Distributed programming with objects and threads in the Clouds system. </title> <type> Technical Report GIT-GC 91/26, </type> <institution> Distributed Systems Laboratory, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors. Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds <ref> [25] </ref>, IVY [64], and Filaments [31, 39]. Other systems, for example, Cid [83], Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms. <p> The time required to migrate a thread is only 1-3% larger than the time required to move a message of the same size. 4.2.3.1 Data Access Methods Remote Procedure Calls and Data Shipping are traditional ways to access remote data. Each RPC-style <ref> [12, 19, 25] </ref> access requires two messages, a send and a receive. The former delivers parameters to a procedure, and the latter returns results. In the traditional process-oriented model the sender typically blocks on the send. With threads, the sending process is free to continue with other work. <p> In the traditional process-oriented model the sender typically blocks on the send. With threads, the sending process is free to continue with other work. Data-shipping is a form of remote memory copy: remote data is moved to a computation or simply moved from one process to another <ref> [25, 64] </ref>. Shipping data to a computation requires copy coherency, an expensive proposition when changes are frequent. Moving data between processes to improve locality is beneficial when remote accesses are frequent, but is expensive for large data blocks or memory pages.
Reference: [26] <author> P. M. Dickens and P. F. Reynolds Jr. </author> <title> SRADS with local rollback. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 161-164, </pages> <year> 1990. </year>
Reference-contexts: In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination <ref> [26, 44] </ref>. In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior [9, 94]. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations [69, 89].
Reference: [27] <author> E. Dijkstra and C. Scholten. </author> <title> Termination detection for Diffusing Computations. </title> <journal> Information Processing Letters, </journal> <volume> 11(1) </volume> <pages> 1-4, </pages> <month> August </month> <year> 1980. </year>
Reference-contexts: The usual strategy is to base termination on a local predicate B i which becomes true when every process P i in the distributed system has terminated. Typical termination algorithms, such as described in <ref> [27] </ref>, assume that processes are configured in special ways rings, trees or predefined cycles using topological information to aid in termination. A number of different algorithms have been proposed based on diffusing computations, ring structures, value-carrying tokens and time-stamping.
Reference: [28] <author> B. Dimitrov. Arachne: </author> <title> A compiler based portable threads architecture supporting migration on heterogeneous networked systems. </title> <type> Master's thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Thus each migration requires a return. Computation migration is useful when thread stacks are large, enabling low migration costs because only necessary stack is migrated. Compiler based techniques for heterogeneous thread migration have also been developed <ref> [28, 102] </ref>. An additional "stack" consisting of a linked list of data structures one for each activation frame in the actual stack is maintained. Initial experiments have demonstrated the viability of this approach.
Reference: [29] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice and Experience, </journal> <volume> 21 </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others [106, 107]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [117], DEMOS/MP [88], Charlotte [5], Sprite <ref> [29] </ref>. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support. Creating maps of run-time images of threads on heterogeneous machines is sufficiently complicated to render the effort impractical, particularly with respect to portability.
Reference: [30] <author> D. L. Eager and J. Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This helps prevent stack overflow and increases computation granularity. A related technique for increasing the granularity of fine-grained applications is found in Lazy Task Creation [78]. It is also possible to enhance stateful threads with support for low-cost fine-grained operations as in the Chores system <ref> [30] </ref>. An interesting feature of programming with Ariadne, as demonstrated above, is the relative ease with which one can move from sequential code to parallel code. This occurs because sequential and parallel versions of code do not differ much.
Reference: [31] <author> D. R. Engler, G. R. Andrews, and D. K. Lowenthal. Filaments: </author> <title> Efficient support for fine-grain parallelism. </title> <type> Technical Report TR 93-13a, </type> <institution> Department of Computer Science, The University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY [64], and Filaments <ref> [31, 39] </ref>. Other systems, for example, Cid [83], Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms.
Reference: [32] <author> A. Wallquist et al. </author> <title> Exploiting physical parallelism using superomputers: Two examples from chemical physics. </title> <journal> IEEE Computer, </journal> <volume> 2(5) </volume> <pages> 9-21, </pages> <year> 1987. </year>
Reference-contexts: Here we are concerned with the use of computer programs that serve as models of interacting objects that are physical systems. Simulation is widely used for system modeling. Areas where simulation is used include molecular 2 dynamics methods in chemical physics <ref> [32] </ref>, models of computer and communications systems [68], battle management models [33], and genetic algorithms [46]. Systems fall into one of two categories: discrete and continuous. In this thesis, we focus on simulations of discrete systems.
Reference: [33] <editor> F. Weiland et al. </editor> <title> Distributed combat simulation and time warp: The model and its performance. </title> <booktitle> In Proceedings of the SCS MultiConference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 14-20, </pages> <year> 1989. </year>
Reference-contexts: Simulation is widely used for system modeling. Areas where simulation is used include molecular 2 dynamics methods in chemical physics [32], models of computer and communications systems [68], battle management models <ref> [33] </ref>, and genetic algorithms [46]. Systems fall into one of two categories: discrete and continuous. In this thesis, we focus on simulations of discrete systems.
Reference: [34] <author> A. Ferscha. </author> <title> Probabilistic adaptive direct optimism control in time warp. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 120-129, </pages> <year> 1995. </year>
Reference-contexts: Yet another proposed approach is to predict the time-stamp of the next message arrival on a given channel, using message-arrival history. Only available events with time-stamps smaller than the predicted time-stamp are processed <ref> [34] </ref>. A confidence level assigned to each estimate enables the execution of each event with a certain probability. Several different methods of predicting the time-stamp of the next arrival have been proposed. <p> Several different methods of predicting the time-stamp of the next arrival have been proposed. These include estimates based on the arithmetic mean, median, exponentially smoothed average, and computationally intensive auto-regressive and integrated moving-average forecasting methods. Some success was reported with these methods <ref> [34, 35] </ref>. Another proposed strategy for limiting optimism is based on memory management [24]. Here, optimism is limited in an indirect manner, by controlling the amount of memory provided to an LP. The adaptive protocol attempts to provide each LP with only as much memory as necessary for optimal performance.
Reference: [35] <author> A. Ferscha and G. Chiola. </author> <title> Self adaptive logical processes: The probabilistic distributed simulation protocol. </title> <booktitle> In Proceedings of the 27th Annual Simulation Symposium, </booktitle> <pages> pages 78-88. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <month> 202 </month>
Reference-contexts: Adaptive control is achieved via a channel specific constant c, enabling various "degrees" of optimism. The constant c is chosen in a way that maximizes the rate of simulation time advance. In experimental work, this protocol has been shown to exhibit good performance [50]. Ferscha and Chiola <ref> [35] </ref> propose a probabilistic control of optimism, based on the determination of a local virtual time window. Events within the window are executed with a given probability, computed using the virtual time-stamps of arriving messages. <p> Several different methods of predicting the time-stamp of the next arrival have been proposed. These include estimates based on the arithmetic mean, median, exponentially smoothed average, and computationally intensive auto-regressive and integrated moving-average forecasting methods. Some success was reported with these methods <ref> [34, 35] </ref>. Another proposed strategy for limiting optimism is based on memory management [24]. Here, optimism is limited in an indirect manner, by controlling the amount of memory provided to an LP. The adaptive protocol attempts to provide each LP with only as much memory as necessary for optimal performance.
Reference: [36] <author> A. Ferscha and J. Luthi. </author> <title> Estimating rollback overhead for optimism control in time warp. </title> <booktitle> In Proceedings of the 28th Annual Simulation Symposium, </booktitle> <pages> pages 2-12. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Adaptive synchronization methods, in particular, provide a framework for tailoring synchronization to the peculiarities of a given application. Based on decisions made with run-time data, such methods offer a dynamic combination of optimistic and conservative synchronization, and have been reported to exhibit poor to reasonable performance in previous studies <ref> [24, 36, 50, 108] </ref>. Of particular importance is the fact that extreme forms of optimistic progress and conservative blocking can be avoided. In this section, we present a new and dynamic method for adaptive synchronization. <p> If the delay interval computed turns out to be too small, the LP may simply spin in a busy loop. We present a model based on a rollback cost and delay analysis, similar to that presented in [9] and <ref> [36] </ref>. Our work differs from theirs in some respects, such as in how expected cost is computed. We estimate transaction interarrival distributions (both virtual time and real time) on input channels and use these, along with other costs, to estimate rollback probability and cost. In Ferscha and Luthi's approach [36], 166 <p> and <ref> [36] </ref>. Our work differs from theirs in some respects, such as in how expected cost is computed. We estimate transaction interarrival distributions (both virtual time and real time) on input channels and use these, along with other costs, to estimate rollback probability and cost. In Ferscha and Luthi's approach [36], 166 transaction executed at LP k from channel c j;k had a time-stamp y j;M1 &lt; y i;N . <p> The optimal delay interval must exceed the average per event cost of adaptive synchronization. We have found a minimum delay in the range of 100 - 500 -seconds to be acceptable. An upper bound for this delay is given by b t <ref> [36] </ref>. Linear search is the simplest method to use in computing the optimal delay. Other search methods include Fibonacci search, Golden section search, and Bisection search [103], all of which exhibit a first order rate of convergence.
Reference: [37] <author> J. Fleischmann and P. A. Wilsey. </author> <title> Comparative analysis of periodic state saving techniques in time warp simulators. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 50-58, </pages> <year> 1995. </year>
Reference-contexts: In the figure, the checkpoint interval giving a minimum execution time is between 8 and 12 events. 6.2.0.1 Related Work A number of models have been proposed for determining the checkpoint interval that yields the minimum overhead <ref> [37, 66, 85, 97] </ref>. An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval [65]. <p> A heuristic for selecting the checkpoint interval is based on the cost function E c = C SS + C CF , where C SS is the cost of state saving and C CF is the cost of coast forwarding <ref> [37] </ref>. In this method, initially a checkpoint interval of one is used. In successive observation intervals the checkpoint interval is incremented by one if E c did not significantly increase. <p> In successive observation intervals the checkpoint interval is incremented by one if E c did not significantly increase. If the costs in the current observation cycle become greater 186 than in previous cycles, the "adaptation direction" is changed and the checkpoint interval is decremented by one. An empirical study <ref> [37] </ref> comparing the performance of various methods using digital logic circuit simulations concluded that the last two methods (Equation 6.11 and the heuristic [37]) performed well on most examples. Methods based on Equation 6.9 perform well depending on the application. <p> An empirical study <ref> [37] </ref> comparing the performance of various methods using digital logic circuit simulations concluded that the last two methods (Equation 6.11 and the heuristic [37]) performed well on most examples. Methods based on Equation 6.9 perform well depending on the application.
Reference: [38] <author> W. R. Franta. </author> <title> The Process View of Simulation. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1977. </year>
Reference-contexts: The process will eventually suspend itself, passing control back to the driver. Each item in the event calendar maintains process identifiers or contexts ordered by their reactivation times. This world view is generally considered to be the most "natural" in a variety of modeling situations <ref> [38] </ref>. Its operational efficiency, however, lies between the event scheduling view and the activity scanning view. 1.1.2 The Active Transaction View A system consists of objects that may be considered to be either active or passive. Active objects act upon passive objects, modifying the state of the system [124]. <p> special driver () thread (or function) selects the next transaction to run based on the smallest time-stamp, advances the simulation clock to the transaction time-stamp and passes control to the thread that represents this transaction. 28 A simulation based on this model may be programmed using the basic simulation primitives <ref> [38] </ref> hold (t), suspend () resume (x), cancel (x), create (), and destroy (x), considered to be part of the core simulation system or the Kernel. In essence these primitives manage the global event calendar by inserting and deleting entries from the calendar.
Reference: [39] <author> V. W. Freeh, D. K. Lowenthal, and G. R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <type> Technical Report TR 94-11a, </type> <institution> Department of Computer Science, The University of Arizona, </institution> <year> 1994. </year>
Reference-contexts: Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY [64], and Filaments <ref> [31, 39] </ref>. Other systems, for example, Cid [83], Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms. <p> Some threads systems provide special kinds of threads that are stateless (without stacks) <ref> [39] </ref> and some do not allow context-switching from functions called by the thread-function [8]. Needless to say, these systems cannot be used to build general purpose discrete event simulators without additional restrictions on modeling.
Reference: [40] <author> R. Fujimoto. </author> <title> Time warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> Some PDES systems are targeted primarily towards shared memory multiprocessors, for example, Fujimoto's Time Warp system <ref> [40] </ref>, OLPS [1], and SimKit [47]. In a shared memory environment, simulator kernel data structures may be shared between LPs. Because this results in significantly smaller communication costs, shared memory execution is more efficient than distributed memory execution. <p> Various methods for efficient state saving of objects have been proposed. These include copy state saving the complete state is saved <ref> [40] </ref>, incremental state saving only the state that is modified is saved [109], 137 and hardware based state saving [43]. <p> Efficiency, is defined as the number of correct event executions (i.e, the number of events that were neither rolled back nor cancelled) divided by the total number of 157 events executed by the simulator <ref> [40] </ref>. In each case the experiment was terminated when the simulated time reached 20,000. Each application exhibits a different rollback behavior. In general, a higher connectivity in the queuing network results in a larger proportion of rollbacks.
Reference: [41] <author> R. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> CACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <year> 1990. </year>
Reference-contexts: When such a decomposition allows simultaneous computations on different processors, the result is a parallel execution. We use the term parallel simulation to imply both shared-memory 9 (e.g., SPARC multiprocessor) and distributed-memory (i.e., workstation cluster, Intel Paragon) executions. We resort to standard parallel simulation terminology <ref> [41] </ref> to present our ideas. If a physical system of interest is viewed as a system of interacting physical processes, a simulator for such a system consists of interacting logical processes (LPs). <p> The well known conservative protocols are based on null messages [17], deadlock detection and recovery [18]. Optimistic synchronization is based on the notion of virtual time [58], and state-saving and rollback mechanisms. 20 Both conservative and optimistic methods have their advantages and disadvantages <ref> [41] </ref>. From the ease-of-use point of view the optimistic protocol is better because it hides synchronization from the user and is therefore easier to use. The conservative protocol requires application specific information in order to perform well. <p> The receiving LP can also optimize the manner in which the rollback is done. Essentially, each terminator annihilates the corresponding positive-transaction and the minimum time-stamped terminator in the batch of terminators, causes a rollback at the destination LP. The coast-forwarding phase <ref> [41] </ref> is necessary when state is saved infrequently, as is done in ParaSol . This phase begins at the calendar entry corresponding to the checkpoint time t p and stops at the straggler's time t s . During the coast-forward, user commands are executed but kernel primitives execute code selectively. <p> All models presented here are novel and have been incorporated in the ParaSol system. We report on the efficacy of these models using several performance measurements. 6.1 Adaptive Synchronization Both conservative as well as optimistic synchronization protocols have their advantages and disadvantages <ref> [41] </ref>. Which protocol is better depends on the characteristics 165 of a given application (number of simulation objects, density of messages, etc.), input data, and the run-time environment (processor speeds, communication latency, etc.). There is a need for synchronization schemes that work well in an application-independent way.
Reference: [42] <author> R. M. Fujimoto. </author> <title> Parallel discrete event simulation: </title> <journal> Will the field survive? ORSA Journal of Computing, </journal> <volume> 5(3) </volume> <pages> 213-230, </pages> <month> Summer </month> <year> 1993. </year>
Reference-contexts: These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. Despite the effort that went into their development, these tools have met with limited success primarily because they do not simplify model development <ref> [42, 82] </ref>.
Reference: [43] <author> R.M. Fujimoto, J. Tsai, and G. Gopalakrishnan. </author> <title> The roll back chip: Hardware support for distributed simulation using Time Warp. </title> <booktitle> In Proceedings of the SCS Distributed Simulation Conference, </booktitle> <pages> pages 81-86, </pages> <year> 1988. </year>
Reference-contexts: Global objects defined at the domain level are already provided with state-saving/restore functions. Thus, users coding above the domain level are protected from having to deal with state management. State-saving overheads are known to have serious impact on the performance of optimistic protocols <ref> [43] </ref>. Minimizing such overheads, whenever possible, is critical. We describe a new save-if-modified method for state saving in ParaSol . Figure 5.9 shows the pseudocode for the save-if-modified algorithm. ParaSol 's state-saving method is incremental and infrequent. <p> Various methods for efficient state saving of objects have been proposed. These include copy state saving the complete state is saved [40], incremental state saving only the state that is modified is saved [109], 137 and hardware based state saving <ref> [43] </ref>. Efficient schemes, such as those employed in SPEEDES for incremental state saving, require the user to write state saving code because state saving depends on the type of event that was executed.
Reference: [44] <author> R. L. Gimarc. </author> <title> Distributed simulation using hierarchical rollback. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 621-629, </pages> <year> 1989. </year>
Reference-contexts: In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination <ref> [26, 44] </ref>. In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior [9, 94]. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations [69, 89].
Reference: [45] <author> D. W. Glazer and C. Tropper. </author> <title> On process migration and load balancing in time warp. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(3) </volume> <pages> 318-327, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: One of the factors that affect performance is load balance. Given that load may become unbalanced, a run-time mechanism is required to balance the load. Dynamic load-balancing schemes in the context of PDES have only recently begun to appear <ref> [15, 45, 95] </ref>. In the absence of an automatic partitioning and load balancing scheme the PDES system must provide adequate support to control these aspects manually at program initialization time. The granularity of the computation in relation to communication is also decided by the partitioning.
Reference: [46] <author> D. Goldberg. </author> <title> Genetic Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Simulation is widely used for system modeling. Areas where simulation is used include molecular 2 dynamics methods in chemical physics [32], models of computer and communications systems [68], battle management models [33], and genetic algorithms <ref> [46] </ref>. Systems fall into one of two categories: discrete and continuous. In this thesis, we focus on simulations of discrete systems.
Reference: [47] <author> F. Gomes, S. Franks, Brian Unger, Zhong-e Xiao, J. Cleary, and A. Covington. SimKit: </author> <title> A high performance logical process simulation class library in C++". </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 706-713, </pages> <year> 1995. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> Some PDES systems are targeted primarily towards shared memory multiprocessors, for example, Fujimoto's Time Warp system [40], OLPS [1], and SimKit <ref> [47] </ref>. In a shared memory environment, simulator kernel data structures may be shared between LPs. Because this results in significantly smaller communication costs, shared memory execution is more efficient than distributed memory execution.
Reference: [48] <author> J. C. Gomez, V. Rego, and V. Sunderam. </author> <title> Tailoring receive threads to suit network loads: Techniques and experiments. </title> <type> Technical Report 96-018, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: Support-layer software is an intrinsic part of the threads library, while the customization modules are meant to flexibly cater to application needs. The layered design enables the system to interface well with application-level software. The ParaSol system [72] is one example of this, and the Clam active-messaging system <ref> [48] </ref> is another. Consider, for example, threads support in ParaSol. Custom modules in ParaSol are mapped into Ariadne's customization layer, as shown in Figure 4.2. The global object locator is an Object Manager that maps unique object identifiers onto LP identifiers. <p> The flexibility of the Ariadne system is amply demonstrated by its use in shared- and distributed-memory environments, as a basis for the development of parallel computing systems such as ParaSol , and for user-space implementation of network protocols <ref> [48] </ref>. Ariadne has proven to be portable; it currently runs on the SPARC (SunOS 4.x, Solaris), Sequent Symmetry, Intel Paragon, Silicon Graphics IRIX workstations, and IBM RS/6000. The basic, shared and distributed threads interface and customization primitives provide the necessary functionality for threads-based parallel programming.
Reference: [49] <author> Bojan Groselj. CPSim: </author> <title> A tool for creating scalable discrete event simulations. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 579-583, </pages> <year> 1995. </year> <month> 203 </month>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie [8], SPEEDES [110], CPSim <ref> [49] </ref>, Sim++ [6], ModSim [123], and TWOS [57]. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations.
Reference: [50] <author> D. O. Hamnes and A. Tripathi. </author> <title> Investigations in adaptive distributed simulation. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 20-23, </pages> <year> 1994. </year>
Reference-contexts: Proposals for adaptive synchronization, based either on local channel-state information or on global information, have recently begun to appear. In the Local 21 Adaptive Protocol (LAP), LPs compute a real-time blocking window (RTBW) based on the average event-arrival rates in real and virtual time <ref> [50] </ref>. An LP blocks if it finds an empty channel for which the increment in virtual time between the last event processed and the next candidate event is larger than the average virtual inter-arrival time. Adaptive control is achieved via a channel specific constant c, enabling various "degrees" of optimism. <p> Adaptive control is achieved via a channel specific constant c, enabling various "degrees" of optimism. The constant c is chosen in a way that maximizes the rate of simulation time advance. In experimental work, this protocol has been shown to exhibit good performance <ref> [50] </ref>. Ferscha and Chiola [35] propose a probabilistic control of optimism, based on the determination of a local virtual time window. Events within the window are executed with a given probability, computed using the virtual time-stamps of arriving messages. <p> Adaptive synchronization methods, in particular, provide a framework for tailoring synchronization to the peculiarities of a given application. Based on decisions made with run-time data, such methods offer a dynamic combination of optimistic and conservative synchronization, and have been reported to exhibit poor to reasonable performance in previous studies <ref> [24, 36, 50, 108] </ref>. Of particular importance is the fact that extreme forms of optimistic progress and conservative blocking can be avoided. In this section, we present a new and dynamic method for adaptive synchronization. <p> Then, LP k can be made to delay processing for time w = max `2E w i;j ` . This method was followed in <ref> [50] </ref>. This strategy will not give a correct value for the delay a delay that will minimize the cost because the computational expense is too large. An exact expected cost function that simultaneously accounts for all empty channels can be developed.
Reference: [51] <author> J. O. Henrikson. </author> <title> GPSS finding the appropriate world view. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 505-516, </pages> <year> 1981. </year>
Reference-contexts: Using an active transaction approach to model flowing system-objects as processes is very natural and appealing from the modeling point or view. Moreover, a simulator that can support the active transaction approach can also support the active server approach <ref> [51] </ref> (though the reverse may not always be true).
Reference: [52] <author> C. A. R. Hoare. </author> <title> Proof of a program: Find. </title> <journal> Communications of the ACM, </journal> <volume> 14(1) </volume> <pages> 39-45, </pages> <month> January </month> <year> 1971. </year>
Reference-contexts: The uniprocessor, shared memory version of Ariadne takes about 12-15% longer than a sequential program, giving a rough indication of threads system overheads. 4.3.3.2 Quicksort with FIND In early work, Hoare <ref> [52] </ref> proposes a FIND algorithm for locating the order-statistics of an array.
Reference: [53] <author> W. C. Hsieh, P. Wang, and W. E. Weihl. </author> <title> Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems. </title> <booktitle> Proceedings of the Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 239-248, </pages> <year> 1993. </year>
Reference-contexts: In IVY, object migration requires the maintenance of coherent copies of data on distinct processors. Maintenance overheads and contention for multiple access to data can degrade performance. Another form of thread migration, called computation migration <ref> [53] </ref>, has been proposed as part of the Prelude language. This technique exploits compile-time transformations to migrate only the topmost frame of a thread's stack. This frame may move from one node to another, repeatedly accessing objects, but must finally return to its source. Thus each migration requires a return. <p> Complex object migration is nontrivial, given that internal objects and/or pointers to other objects require packing and unpacking at source and destination processes, respectively. Another alternative to the two schemes mentioned above is computation migration <ref> [16, 53] </ref>. Here, part of a computation (e.g., the topmost function in a sequence 65 of nested calls) is moved to a process hosting required data. <p> Another alternative to the two schemes mentioned above is computation migration [16, 53]. Here, part of a computation (e.g., the topmost function in a sequence 65 of nested calls) is moved to a process hosting required data. Using compile-time transforms, the scheme described in <ref> [53] </ref> effects this by transferring single activation frames between hosts; the function is forced to return to the sender, entailing a two-way transfer. <p> Because a migrant need never return to its sender, transfers are one-way instead of two-way; intermediate results are generally stored in an object within the thread. Thread migration is known to require the least number of messages for a typical series of remote access requests <ref> [53] </ref>. With potentially low consumption of network bandwidth, relative to other access schemes, thread migration offers good potential for improved performance. Choice of an appropriate access scheme depends, however, on the needs of the application and on the underlying environment.
Reference: [54] <institution> IBM. </institution> <note> AIX Version 4.1 Technical Reference. SC23-2618. </note>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads <ref> [54, 87, 116] </ref>, and many user-space [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [55] <author> IEEE. </author> <title> Threads extension for portable operating systems (draft 9), </title> <year> 1995. </year>
Reference-contexts: Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available <ref> [55] </ref>. Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors.
Reference: [56] <author> R. Jain and I. Chlamtac. </author> <title> The P 2 algorithm for dynamic calculation of quanties and histograms without storing observations. </title> <journal> Communications of the ACM, </journal> <volume> 28(10) </volume> <pages> 1076-1085, </pages> <year> 1985. </year>
Reference-contexts: Assuming stationarity, part or all of the data fx j;n ; n &lt; M g and fy j;n ; n &lt; N g may be used in computing a simple probability statement involving R j;M and T j;N , respectively. The P 2 algorithm <ref> [56] </ref>, is used to estimate the real and virtual time probabilities in Equation 6.3. This algorithm enables on-the-fly computation of quantiles and histograms, without the need for storing observations.
Reference: [57] <author> D. Jefferson and S. Bellenot. </author> <title> Distributed simulation and the time warp operating system. </title> <booktitle> In ACM Operating System Review, </booktitle> <pages> pages 77-93, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie [8], SPEEDES [110], CPSim [49], Sim++ [6], ModSim [123], and TWOS <ref> [57] </ref>. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations.
Reference: [58] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: The null message guarantees the receiver that the sender will not send a message with a smaller timestamp. In the optimistic approach <ref> [58] </ref>, an LP processes events as event messages become available, hoping that the physical order in which event messages arrive corresponds to order of nondecreasing time. <p> The GVT also defines when I/O operations may be committed. The original optimistic approach, called the Time-Warp protocol, was the first to introduce this notion of virtual time <ref> [58] </ref>. 1.3 Lightweight Process Systems In traditional operating systems, a process has an address space and a single thread of control. Interprocess communication may be used to provide data-sharing between 12 two or more processes, allowing for either true concurrency on a multiprocessor or pseudo-concurrency on a uniprocessor. <p> The well known conservative protocols are based on null messages [17], deadlock detection and recovery [18]. Optimistic synchronization is based on the notion of virtual time <ref> [58] </ref>, and state-saving and rollback mechanisms. 20 Both conservative and optimistic methods have their advantages and disadvantages [41]. From the ease-of-use point of view the optimistic protocol is better because it hides synchronization from the user and is therefore easier to use. <p> It is a centralized algorithm similar to the GVT algorithm used in TWOS <ref> [58] </ref>. Basically, the controller process decides when the next GVT computation is to be done, and broadcasts a GVT-START message to all SPs, requesting LP-based LVT and messages-in-transit information. After all processes reply, a minimum is determined, and this becomes the new GVT value.
Reference: [59] <author> D. Keppel. </author> <title> Tools and techniques for building fast portable threads packages. </title> <type> Technical Report UWCSE 93-05-06, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [60] <author> F. Knop, E. Mascarenhas, V. Rego, and V. Sunderam. </author> <title> Fail-Safe Concurrent Simulation with EcliPSe: An introduction. </title> <journal> Simulation Practice & Theory, </journal> <volume> 3 </volume> <pages> 121-146, </pages> <year> 1995. </year>
Reference-contexts: In this section we provide a brief overview of the DISplay system: architecture, synchronization capability, tasks and dialogs. A more detailed description can be found in [74]. The use of the system for monitoring of parallel applications in EcliPSe is described in <ref> [60] </ref>, and examples of its application in simulation, and parallel programming with the Ariadne threads system can be found in [77]. 5.4.1 The DISplay System Architecture The DISplay software architecture is based on the client-server paradigm [21].
Reference: [61] <author> D. P. Briscoe L. M. Sokol and A. P. Wieland. MTW: </author> <title> A strategy for scheduling discrete simulation events for concurrent execution. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 19, </volume> <pages> pages 34-42, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based [108]. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs <ref> [61, 67, 94, 111, 120] </ref>. In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination [26, 44].
Reference: [62] <author> A. M. Law and W. D. </author> <title> Kelton. Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill, </publisher> <year> 1982. </year>
Reference-contexts: Define R j;n to be random variables denoting the interarrival time and time-stamp of the n-th transaction coming in on c j;k , respectively, for any j, and n 1. We assume that fR j;n ; n 1g and fT j;n ; n 1g are stationary sequences <ref> [62] </ref>.
Reference: [63] <author> J. M. Lemme and J. H. Rice. </author> <title> Adaptive quadrature algorithms for the ILLIAC IV. </title> <journal> International Journal of Computer and Information Sciences, </journal> <volume> 9(1) </volume> <pages> 63-72, </pages> <month> February </month> <year> 1980. </year> <month> 204 </month>
Reference-contexts: 150 fi fi fi Recursion Maximum 4 4 4 0:5 1:5 2:5 3:5 Processors Speedup No recursion 3 3 3 Recursion Level 50 + + + Recursion Level 150 2 2 2 Recursion Maximum fi fi fi 4.3.2.1 Performance We tested the adaptive threads-based quadrature on an integrand proposed in <ref> [63] </ref> to be a worthwhile test. The function f (x) = sign (g 1 (x); g 2 (x)) is defined as jg 1 (x)j if g 2 (x) 0, and as g 1 (x) if g 2 (x) &lt; 0.
Reference: [64] <author> K. Li. IVY: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors. Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY <ref> [64] </ref>, and Filaments [31, 39]. Other systems, for example, Cid [83], Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms. <p> In the traditional process-oriented model the sender typically blocks on the send. With threads, the sending process is free to continue with other work. Data-shipping is a form of remote memory copy: remote data is moved to a computation or simply moved from one process to another <ref> [25, 64] </ref>. Shipping data to a computation requires copy coherency, an expensive proposition when changes are frequent. Moving data between processes to improve locality is beneficial when remote accesses are frequent, but is expensive for large data blocks or memory pages. <p> Transaction flow enables transactions to access servers hosted in other LPs in the system. Most threads systems do not support migration as a means of accessing remote objects; some provide remote procedure call or data shipping to achieve the same effect <ref> [19, 64] </ref>. The advantages of thread migration have already been discussed in Section 4.2.3. Migration entails moving a transaction from one processor to another. Ariadne provides the low-level thread migration support required by ParaSol .
Reference: [65] <author> Y. Lin. </author> <title> Understanding the limits of optimistic and conservative parallel simulation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval <ref> [65] </ref>. This work was later extended to include the effect of rollback behavior [66].
Reference: [66] <author> Y. B. Lin, B. R. Priess, W. M. Loucks, and E. D. Lazowska. </author> <title> Selecting the checkpoint interval in time warp simulations. </title> <booktitle> In Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 3-8, </pages> <year> 1993. </year>
Reference-contexts: In the figure, the checkpoint interval giving a minimum execution time is between 8 and 12 events. 6.2.0.1 Related Work A number of models have been proposed for determining the checkpoint interval that yields the minimum overhead <ref> [37, 66, 85, 97] </ref>. An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval [65]. <p> An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval [65]. This work was later extended to include the effect of rollback behavior <ref> [66] </ref>. Based on empirical studies the authors suggest the use of the upper bound + = 6 6 (2ff 1 + 1)ffi s 3 7 (6.9) 185 as the initial checkpoint interval computed after N events are committed.
Reference: [67] <author> B. D. Lubachevsky, A. Shwartz, and A. Weiss. </author> <title> Rollback sometimes works ... if filtered. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 630-639, </pages> <year> 1989. </year>
Reference-contexts: Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based [108]. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs <ref> [61, 67, 94, 111, 120] </ref>. In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination [26, 44].
Reference: [68] <author> M. H. MacDougall. </author> <title> Simulating Computer Systems: Techniques and Tools. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Here we are concerned with the use of computer programs that serve as models of interacting objects that are physical systems. Simulation is widely used for system modeling. Areas where simulation is used include molecular 2 dynamics methods in chemical physics [32], models of computer and communications systems <ref> [68] </ref>, battle management models [33], and genetic algorithms [46]. Systems fall into one of two categories: discrete and continuous. In this thesis, we focus on simulations of discrete systems. <p> In operation, this is considered to be less efficient than the event scheduling view, because of the repeated need to evaluate event occurrence conditions. * A third world view is the process interaction view whose chief advantage lies in its ease of modeling <ref> [68] </ref>. Instead of defining types of events that may occur, the modeler defines processes which describe the behavior of objects in the system as a sequence of events or a sequence of activities. Thus events are implicitly defined by processes.
Reference: [69] <author> V. Madisetti, J. Walrand, and D. Messerschmitt. WOLF: </author> <title> A rollback algorithm for optimistic distributed simulation systems. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 296-305, </pages> <year> 1988. </year>
Reference-contexts: In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior [9, 94]. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations <ref> [69, 89] </ref>. Probabilistic synchronization is performed as follows: at discrete points in time each processor determines whether it should synchronize with others based on the outcome of a probabilistic experiment (e.g., flipping a coin) [70].
Reference: [70] <author> V. K. Madisetti, D. A. Hardaker, and R. M. Fujimoto. </author> <title> The MIMDIX operating system for parallel simulation. </title> <booktitle> In Proceedings of the 6th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 65-74, </pages> <year> 1992. </year>
Reference-contexts: In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations [69, 89]. Probabilistic synchronization is performed as follows: at discrete points in time each processor determines whether it should synchronize with others based on the outcome of a probabilistic experiment (e.g., flipping a coin) <ref> [70] </ref>. Proposals for adaptive synchronization, based either on local channel-state information or on global information, have recently begun to appear. In the Local 21 Adaptive Protocol (LAP), LPs compute a real-time blocking window (RTBW) based on the average event-arrival rates in real and virtual time [50].
Reference: [71] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos. </author> <title> First class user-level threads. </title> <booktitle> In Proceedings of the Symposium on Operating System Principles, </booktitle> <pages> pages 110-121, </pages> <year> 1991. </year>
Reference-contexts: Other systems attempt to combine the advantages of kernel-space and user-space threads <ref> [3, 71] </ref>. A POSIX standard for a threads interface is also available [55]. Ariadne provides not only the basic features of a threads system, but also provides a parallel programming interface that can be used on uniprocessor, shared and distributed memory multiprocessors. <p> Each approach has its advantages and disadvantages; a hybrid implementation is also possible <ref> [3, 71] </ref>. Among the major advantages of user-space threads are cheap thread management, portability, and customization. The chief disadvantage is that a blocking system call in a thread causes its host process to block, preventing other ready threads from running.
Reference: [72] <author> E. Mascarenhas, F. Knop, and V. Rego. ParaSol: </author> <title> A multithreaded system for parallel simulation based on mobile threads. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <address> Washington D.C., </address> <pages> pages 690-697, </pages> <month> December 4-7 </month> <year> 1995. </year>
Reference-contexts: Support-layer software is an intrinsic part of the threads library, while the customization modules are meant to flexibly cater to application needs. The layered design enables the system to interface well with application-level software. The ParaSol system <ref> [72] </ref> is one example of this, and the Clam active-messaging system [48] is another. Consider, for example, threads support in ParaSol. Custom modules in ParaSol are mapped into Ariadne's customization layer, as shown in Figure 4.2. <p> Both rely on the underlying communications environment and may be generalized to envelop other computing models. As another example, mobile threads in the ParaSol <ref> [72] </ref> parallel simulation system may communicate with other mobile threads through a mailbox facility. Such messaging enables threads to exchange and share data, update results, schedule computations and synchronize. Thread migration overheads in Ariadne are small, compared to the total time required for migration.
Reference: [73] <author> E. Mascarenhas and V. Rego. </author> <title> Ariadne User Manual. </title> <type> Technical Report 94-081, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1994. </year>
Reference-contexts: After initialization, the user may create Ariadne threads and perform various thread-related tasks. The basic system primitives are shown in Table 4.1. More details on the use of these and other parallel programming primitives, and illustrative examples, can be found in the Ariadne User Manual <ref> [73] </ref>. The main system thread may use a_exit () to signal termination. If no other user-created threads exist, the threads system terminates and the application may continue to run without Ariadne threads on a return from a_exit (). Otherwise, the main thread blocks until all other user-created threads terminate.
Reference: [74] <author> E. Mascarenhas and V. Rego. </author> <title> An architecture for visualization and user interaction in parallel environments. </title> <journal> Computer & Graphics, </journal> <volume> 19(5) </volume> <pages> 739-753, </pages> <year> 1995. </year>
Reference-contexts: We simulate the movement of the walker for T time-steps, and finally compute his mean square displacement (msd). The goal is to (empirically) determine msd as a function of q and T . Details on the model can be found in <ref> [74] </ref> and [80]. 95 In this example, both thread migration and data transfer are used. The uniprocessor version of the application is simple to code. A 2-D grid is created to represent the lattice, and a set of sites is marked as inaccessible. <p> If his original position became occupied by another walker, in the meantime, a cascade of corrections may occur. For simplicity, we do not implement cascading retractions. Synchronization is based on a decentralized protocol, with each processor synchronizing only with its neighbors <ref> [74] </ref>. Though the application is regular and well-suited to distribution, load imbalance is a certainty. Initially balanced process loads become unbalanced as walkers begin to migrate across slice boundaries. Redistributing grid slices to balance load is possible, but is not pursued here. <p> In this section we provide a brief overview of the DISplay system: architecture, synchronization capability, tasks and dialogs. A more detailed description can be found in <ref> [74] </ref>.
Reference: [75] <author> E. Mascarenhas and V. Rego. </author> <title> Migrant threads on process farms: Parallel programming with ariadne. </title> <type> Technical Report TR-95-081, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1995. </year>
Reference-contexts: The iteration terminates upon converging, i.e., when j i;j i;j j &lt; ", for all (i; j) and small, positive ". As mentioned earlier, threads are ideal for implementing iterations, and the red-black scheme is no exception. Pseudo-code for Ariadne threads can be found in <ref> [75] </ref>. By distributing vertical slices (sets of rows) of a mesh across a set of distributed processors, threads governing mesh point computations are also distributed. Each distributed process manages mesh points within its slice.
Reference: [76] <author> E. Mascarenhas and V. Rego. Ariadne: </author> <title> Architecture of a portable threads system supporting thread migration. </title> <journal> Software-Practice and Experience, </journal> <volume> 26(3) </volume> <pages> 327-356, </pages> <month> March </month> <year> 1996. </year> <month> 205 </month>
Reference-contexts: Contemporary operating systems support lightweight processes or threads as computational units. Threads may be supported in user-space (user-space threads) or within the operating system kernel (kernel-space threads). Each approach has its advantages and disadvantages <ref> [76, 115] </ref>. Threads are essentially mini-processes: each has a program counter and a stack to indicate its current state of execution. Each thread begins execution at the start of a function called the thread-function. <p> Another important requirement due to optimistic synchronization in PDES, is thread checkpointing for state saving and thread restoration on the occurrence of a rollback. 40 4. ARIADNE: ARCHITECTURE, DESIGN, IMPLEMENTATION, AND PERFORMANCE We propose the use of cheap, user-space threads <ref> [76, 115] </ref> as the unit of computation in shared- and distributed-memory environments. <p> Ariadne is a portable, efficient, and flexible threads system supporting multithreaded parallel computing on uniproces-sors, shared- and distributed-memory multiprocessors. Basic primitives in Ariadne perform well compared to commercially available threads systems <ref> [76] </ref>. A new and efficient mechanism for thread migration that permits thread stack relocation on distributed processes is presented, along with performance measures. <p> As before, each recursive call that operates on a subarray of S is turned into a thread. Code describing the development of a parallel, multithreaded quicksort can be found in <ref> [76] </ref>. In the following section we show how pivoting to get balanced loads can enhance parallel execution performance, even though such pivoting requires more work. Using Ariadne to perform a quicksort helps illustrate its automatically balanced processor loads. <p> At the destination, the message is "unpacked" into a thread-shell and is ready for execution. The cost of migrating a thread in this manner is roughly the same as the cost of sending and receiving a message of the same length <ref> [76] </ref>. This point is of great significance, because ParaSol does not curtail the number of migrations. The active-transaction approach may result in a large number of threads, each of which can migrate. Since migrant 128 threads may trigger rollback, efficient migration is important.
Reference: [77] <author> E. Mascarenhas, V. Rego, and J. Sang. </author> <title> DISplay: A system for visual interaction in distributed environments. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 698-705, </pages> <year> 1995. </year>
Reference-contexts: A more detailed description can be found in [74]. The use of the system for monitoring of parallel applications in EcliPSe is described in [60], and examples of its application in simulation, and parallel programming with the Ariadne threads system can be found in <ref> [77] </ref>. 5.4.1 The DISplay System Architecture The DISplay software architecture is based on the client-server paradigm [21]. The simulation application, which is created by an analyst or programmer, is treated as a client. Client calls are made by invoking functions resident in the DISplay client library.
Reference: [78] <author> E. Mohr, D. A. Kranz, and R. Halstead Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: New threads are created only when the depth of recursion reaches or exceeds some user-set threshold RECURSE LEVEL. This helps prevent stack overflow and increases computation granularity. A related technique for increasing the granularity of fine-grained applications is found in Lazy Task Creation <ref> [78] </ref>. It is also possible to enhance stateful threads with support for low-cost fine-grained operations as in the Chores system [30]. An interesting feature of programming with Ariadne, as demonstrated above, is the relative ease with which one can move from sequential code to parallel code.
Reference: [79] <author> F. Mueller. </author> <title> A library implementation of POSIX threads under UNIX. </title> <booktitle> In Proceedings of the 1993 USENIX Winter Conference, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [80] <author> H. Nakanishi. </author> <title> Anomalous diffusion in disordered clusters. </title> <editor> In P. J. Reynolds, editor, </editor> <title> On clusters and clustering, from atoms to fractals, </title> <booktitle> chapter 27, </booktitle> <pages> pages 373-382. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1993. </year>
Reference-contexts: These phenomena are sometimes modeled as random walks on disordered clusters. The model described below is known as the ant in the labyrinth and is attributed to deGennes <ref> [80] </ref>. Briefly, a random walker (ant) moves randomly over certain accessible sites of a given lattice, where the fraction of accessible sites is q, 0 &lt; q &lt; 1. We simulate the movement of the walker for T time-steps, and finally compute his mean square displacement (msd). <p> We simulate the movement of the walker for T time-steps, and finally compute his mean square displacement (msd). The goal is to (empirically) determine msd as a function of q and T . Details on the model can be found in [74] and <ref> [80] </ref>. 95 In this example, both thread migration and data transfer are used. The uniprocessor version of the application is simple to code. A 2-D grid is created to represent the lattice, and a set of sites is marked as inaccessible.
Reference: [81] <author> H. Nakanishi, V. Rego, and S. Sunderam. </author> <title> On the effectiveness of superconcur-rent computations on heterogeneous networks. </title> <journal> Journal of Parallel & Distributed Computing, </journal> <volume> 24(2) </volume> <pages> 177-190, </pages> <year> 1995. </year>
Reference-contexts: This demand comes in one or both of two standard forms: speed of model execution, and size of executing model. For example, in particle-physics simulations <ref> [81] </ref>, hundreds of statistical samples from hundred-hour (uniprocessor) runs are required for estimating measures of scale-invariant phenomena at critical disorder.
Reference: [82] <author> D. Nicol and P. Heidelberger. </author> <title> On extending parallelism to serial simulators. </title> <type> Technical Report ICASE 94-95, </type> <institution> NASA Langley Research Center, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. Despite the effort that went into their development, these tools have met with limited success primarily because they do not simplify model development <ref> [42, 82] </ref>.
Reference: [83] <author> R. S. Nikhil. Cid: </author> <title> A parallel, "shared-memory" C for distributed-memory machines. </title> <booktitle> In Proceedings of the 7th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 376-390, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Some of the systems that provide parallel programming with threads in a shared/distributed environment include Amber [19], Clouds [25], IVY [64], and Filaments [31, 39]. Other systems, for example, Cid <ref> [83] </ref>, Cilk [13], Olden [16], provide a multithreaded system for distributed memory architectures by extending existing sequential languages. The emphasis here is on providing fine grained parallelism using granularity management and scheduling algorithms.
Reference: [84] <author> J. M. Ortega and R. G. Voigt. </author> <title> Solution of Partial Differential Equations on Vector and Parallel Computers. </title> <publisher> SIAM, </publisher> <year> 1985. </year>
Reference-contexts: Such iteration occurs, for example, in determining steady state heat distribution over a thin square metal plate, given temperature on the boundary. The behavior of this system is governed by Laplace's equation with Dirichlet boundary conditions. It can be discretized using a five-point difference equation <ref> [84] </ref>, with the 90 Table 4.7 Time (Quicksort)/time (Quicksort+FIND) ratios Number of Number of Processors Integers 1 2 4 500000 1.88 1.57 1.34 2000000 1.73 1.57 1.26 square replaced by a mesh. The problem is one of determining the values of heat distribution at mesh intersection points.
Reference: [85] <author> A. Palaniswamy and P. A. Wilsey. </author> <title> An analytical comparison of periodic check-pointing and incremental state saving. </title> <booktitle> In Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 127-134, </pages> <year> 1993. </year>
Reference-contexts: In the figure, the checkpoint interval giving a minimum execution time is between 8 and 12 events. 6.2.0.1 Related Work A number of models have been proposed for determining the checkpoint interval that yields the minimum overhead <ref> [37, 66, 85, 97] </ref>. An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval [65]. <p> Another approach used was to assume that a large number of events are committed, and to minimize the average overhead. This leads to the expression opt = 6 6 2ffi s k r 7 7 for the checkpoint interval <ref> [85] </ref>. Here k r is the number of rollbacks when N events are committed, and fl is the average number of events undone in a rollback (average rollback length). The two approaches described above are based on observation intervals involving N committed events.
Reference: [86] <author> C. D. Pegden, R. E. Shannon, and R. P. Sadowski. </author> <title> Introduction to Simulation using SIMAN. </title> <address> Mc-Graw Hill, New York, </address> <year> 1990. </year>
Reference-contexts: If processes in the model define the behavior of the servers in the system, then the model supports active servers. If the transactions are modeled as processes, then the model supports active transactions. Popular sequential simulation languages, GPSS [104], SIMAN <ref> [86] </ref> advocate the use of active transaction or transaction flow modeling. Using an active transaction approach to model flowing system-objects as processes is very natural and appealing from the modeling point or view.
Reference: [87] <author> M. L. Powell, S. R. Kleiman, S. Barton, D. Shah, D. Stein, and M. Weeks. </author> <title> SunOS multi-thread architecture. </title> <booktitle> In Proceedings of the 1991 USENIX Winter Conference, </booktitle> <pages> pages 65-79, </pages> <year> 1991. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads <ref> [54, 87, 116] </ref>, and many user-space [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55]. <p> Timings for setjmp and longjmp operations are also reported, to give an indication of the additional work done by Ariadne during context-switching. Synchronization time is measured by forcing two threads to repeatedly synchronize with one another, as done in <ref> [87] </ref>. Timings involving Ariadne were measured on the SPARCclas-sic (SunOS 4.1), SPARCstation 20 (SunOS 5.3), Sequent Symmetry, IBM RS/6000, and Intel i860 processors. Timings involving Sun-LWP were measured on the SPAR-Cclassic, and timings involving Sun-MT were measured on a SPARCstation 20. All measurements are reported in Table 4.4.
Reference: [88] <author> M. L. Powell and B. P. Miller. </author> <title> Process migration in DEMOS/MP. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-119, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others [106, 107]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [117], DEMOS/MP <ref> [88] </ref>, Charlotte [5], Sprite [29]. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support. Creating maps of run-time images of threads on heterogeneous machines is sufficiently complicated to render the effort impractical, particularly with respect to portability.
Reference: [89] <author> A. Prakash and R. Subramanian. </author> <title> An efficient optimistic distributed simulation scheme based on conditional knowledge. </title> <booktitle> In Proceedings of the 24th Annual Simulation Symposium, </booktitle> <pages> pages 85-94, </pages> <month> April </month> <year> 1991. </year> <month> 206 </month>
Reference-contexts: In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior [9, 94]. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations <ref> [69, 89] </ref>. Probabilistic synchronization is performed as follows: at discrete points in time each processor determines whether it should synchronize with others based on the outcome of a probabilistic experiment (e.g., flipping a coin) [70].
Reference: [90] <author> B. Priess. </author> <title> The Yaddes distributed discrete event simulation specification language. </title> <booktitle> In Proceedings of the SCS MultiConference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 139-144, </pages> <year> 1989. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance.
Reference: [91] <institution> Pritsker and Associates, Inc., West Lafayette, Indiana. TESS and SLAMII. </institution>
Reference-contexts: Most simulations produce output in textual form when the simulation terminates. Graphical visualization is a useful simulation aid. Several commercial simulation systems provide run-time animation and graphical views of simulation output <ref> [91, 99, 118] </ref>. Requirements for such support include online graphical display and interaction, inspection of simulation state variables, and specification of model parameters [98]. A Parallel Discrete Event simulation places some special requirements on a Visualization and Interaction system.
Reference: [92] <author> C. Provenzano. Pthreads. </author> <note> http://www.mit.edu:8001/people/proven/pthreads.html. </note>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [93] <author> M. Raynal. </author> <title> Distributed Algorithms and Protocols. </title> <publisher> John Wiley and Sons Ltd., </publisher> <year> 1988. </year>
Reference-contexts: A number of different algorithms have been proposed based on diffusing computations, ring structures, value-carrying tokens and time-stamping. A survey of such algorithms can be found in <ref> [93] </ref>. 73 In Ariadne, each process determines its own status to be either active or inactive.
Reference: [94] <author> P. Reiher and D. Jefferson. </author> <title> Limitation of optimisim in the time warp operationg system. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 765-769, </pages> <year> 1989. </year>
Reference-contexts: Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based [108]. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs <ref> [61, 67, 94, 111, 120] </ref>. In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination [26, 44]. <p> That is, arriving messages do not cause rollbacks at a destination [26, 44]. In penalty based systems for limiting optimism, some LPs are penalized by blocking their execution based on their past rollback behavior <ref> [9, 94] </ref>. In knowledge based protocols, rollback information is used to restrict the propagation of incorrect computations [69, 89].
Reference: [95] <author> P. Reiher and D. Jefferson. </author> <title> Virtual time based dynamic load management in the Time Warp Operating System. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Systems, </booktitle> <volume> volume 22, </volume> <pages> pages 103-111, </pages> <year> 1990. </year>
Reference-contexts: One of the factors that affect performance is load balance. Given that load may become unbalanced, a run-time mechanism is required to balance the load. Dynamic load-balancing schemes in the context of PDES have only recently begun to appear <ref> [15, 45, 95] </ref>. In the absence of an automatic partitioning and load balancing scheme the PDES system must provide adequate support to control these aspects manually at program initialization time. The granularity of the computation in relation to communication is also decided by the partitioning.
Reference: [96] <author> M. J. Rochkind. </author> <title> Advanced Unix Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1985. </year>
Reference-contexts: Context-switching involves saving a running thread's register state, including all floating point registers, and the signal mask (if required), and restoring the next runnable thread's corresponding state. For portability, Ariadne accomplishes all this using the standard setjmp () and longjmp () subroutines 1 available in Unix environments <ref> [96] </ref>. Porting Ariadne entails rewriting only thread initialization code about 5 lines of assembly code on a SPARC! * Ease of Use. The basic user interface is simple. Use of Ariadne requires a basic understanding of concurrent programming and C/C++.
Reference: [97] <author> R. Ronngren and R. Ayani. </author> <title> Adaptive checkpointing in time warp. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 94-100, </pages> <year> 1994. </year>
Reference-contexts: In the figure, the checkpoint interval giving a minimum execution time is between 8 and 12 events. 6.2.0.1 Related Work A number of models have been proposed for determining the checkpoint interval that yields the minimum overhead <ref> [37, 66, 85, 97] </ref>. An analytical model to obtain upper and lower bounds for the checkpoint interval has been developed under the assumption that the rollback behavior of an LP is not affected by the checkpoint interval [65]. <p> obs events (including events that commit and those that are rolled back) minimizes the cost of state saving and coast forwarding, and computes the optimal checkpoint interval opt = 6 6 2 k r ffi c 7 7 where ffi c is the average cost of coast forwarding an event <ref> [97] </ref>. A heuristic for selecting the checkpoint interval is based on the cost function E c = C SS + C CF , where C SS is the cost of state saving and C CF is the cost of coast forwarding [37]. <p> the rollback behavior changes substantially over the length of the simulation run. 6.2.1 The Minimum Overhead Cost Model Our model for adaptive state saving is also based on minimizing the total overhead composed of state saving and coast forwarding costs, and is similar to that proposed by Rongren and Ayani <ref> [97] </ref> (henceforth called the RA model for convenience). <p> We measured the performance of the Minimum Overhead model and compared it with the RA model for the same queuing simulation experiments. The value of the observation interval R obs was set to 250 (in <ref> [97] </ref>, empirical results indicate that a few hundred events are sufficient). The maximum value of the checkpoint interval was set to 30. A number of 8 processor experiments are conducted on the Intel Paragon, and 4 processor experiments were conducted on a cluster of SPARCstation 5's connected by Ethernet.
Reference: [98] <author> Michael Rooks. </author> <title> A unified framework for visual interactive simulation. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 1146-1154, </pages> <year> 1991. </year>
Reference-contexts: Graphical visualization is a useful simulation aid. Several commercial simulation systems provide run-time animation and graphical views of simulation output [91, 99, 118]. Requirements for such support include online graphical display and interaction, inspection of simulation state variables, and specification of model parameters <ref> [98] </ref>. A Parallel Discrete Event simulation places some special requirements on a Visualization and Interaction system. Output from distributed simulations occurring 34 from different logical processes will be performed at different virtual timestamps.
Reference: [99] <author> Edward C. Russell. </author> <title> Building simulation models with SIMSCRIPT II.5. CACI Products Company, </title> <address> La Jolla, CA, </address> <year> 1989. </year>
Reference-contexts: Most simulations produce output in textual form when the simulation terminates. Graphical visualization is a useful simulation aid. Several commercial simulation systems provide run-time animation and graphical views of simulation output <ref> [91, 99, 118] </ref>. Requirements for such support include online graphical display and interaction, inspection of simulation state variables, and specification of model parameters [98]. A Parallel Discrete Event simulation places some special requirements on a Visualization and Interaction system.
Reference: [100] <author> J. Sang. </author> <title> Multithreading in Distributed-Memory Systems and Simulation: Design, Implementation, and Experiments. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1994. </year>
Reference-contexts: This is not true, however, in heterogeneous environments because mapping run-time images of stacks from one architecture to another at run-time is complex. On heterogeneous architectures compile-time transformations have been found to be useful for thread migration <ref> [100] </ref>. As a load balance mechanism, thread migration can be used to move threads from a busy processor to processors that are lightly loaded.
Reference: [101] <author> J. Sang, E. Mascarenhas, and V. Rego. </author> <title> Mobile-process-based parallel simulation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 33(1) </volume> <pages> 12-23, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Thread migration was chosen in favor of remote procedure calls, message passing, or data shipping, based on empirical evidence suggesting that for discrete event simulation applications, thread migration performs equally well while also providing an improved modeling method <ref> [101] </ref>. Thread migration is compared with other methods of remote access in more detail in the next chapter. Every PDES application requires some form of partitioning of the model into LPs. <p> In our experience with parallel simulation applications, the cost of moving threads between processors is roughly the same as the cost of messaging the same event-related information <ref> [101] </ref>. In this chapter we describe the Ariadne threads system and its features, particularly those that are used in parallel applications. Ariadne is a portable, efficient, and flexible threads system supporting multithreaded parallel computing on uniproces-sors, shared- and distributed-memory multiprocessors. <p> To the best of our knowledge, ParaSol is the first optimistic parallel discrete event simulation system that is based on an independent multithreaded system that supports transaction migration to access remote objects. Indeed, ParaSol builds upon the experimental Si system <ref> [101] </ref>. In this chapter, we describe the ParaSol parallel discrete event simulation system. First we discuss the layered architecture of ParaSol , and provide a complete example of a closed queuing network application, to demonstrate the ease-of-modeling idea and to motivate the need for the remaining layers. <p> Transmitting threads instead of messages adds marginal overhead in the form of stack information. But since the dominant component of transmission time is due to transmission latency and not message size or even data packing/unpacking, thread-migration costs are roughly equivalent to message transmission costs <ref> [101] </ref>. Finally, transaction migration does not always involve costly message transmission because both the source and destination LPs may reside within the same process or in the same shared memory multiprocessor. A thread migration capability is not essential for implementing process-based simulations.
Reference: [102] <author> J. Sang, G. Peters, and V. Rego. </author> <title> Thread migration on heterogeneous systems via compile-time transformations. </title> <booktitle> In Proceedings of the 1994 International Conference of Parallel and Distributed Systems, </booktitle> <pages> pages 634-639, </pages> <year> 1994. </year>
Reference-contexts: Thus each migration requires a return. Computation migration is useful when thread stacks are large, enabling low migration costs because only necessary stack is migrated. Compiler based techniques for heterogeneous thread migration have also been developed <ref> [28, 102] </ref>. An additional "stack" consisting of a linked list of data structures one for each activation frame in the actual stack is maintained. Initial experiments have demonstrated the viability of this approach.
Reference: [103] <author> L. E. </author> <title> Scales. Introduction to Non-Linear Optimization. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: An upper bound for this delay is given by b t [36]. Linear search is the simplest method to use in computing the optimal delay. Other search methods include Fibonacci search, Golden section search, and Bisection search <ref> [103] </ref>, all of which exhibit a first order rate of convergence.
Reference: [104] <author> T. J. Schriber. </author> <title> Simulation Using GPSS. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1974. </year> <month> 207 </month>
Reference-contexts: If processes in the model define the behavior of the servers in the system, then the model supports active servers. If the transactions are modeled as processes, then the model supports active transactions. Popular sequential simulation languages, GPSS <ref> [104] </ref>, SIMAN [86] advocate the use of active transaction or transaction flow modeling. Using an active transaction approach to model flowing system-objects as processes is very natural and appealing from the modeling point or view.
Reference: [105] <author> K. Schwan, et al. </author> <title> A C thread library for multiprocessors. </title> <type> Technical Report GIT-ICS-91/02, </type> <institution> Department of Information and Computer Science, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [106] <author> C. Shub. </author> <title> Native code process-originated migration in a heterogeneous environment. </title> <booktitle> In Proceedings of the 18th Annual ACM Computer Science Conference, </booktitle> <pages> pages 266-270, </pages> <year> 1990. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others <ref> [106, 107] </ref>. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [117], DEMOS/MP [88], Charlotte [5], Sprite [29]. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support.
Reference: [107] <author> J. M. Smith. </author> <title> A Survey of Process Migration Mechanisms. </title> <booktitle> ACM Operating System Review, </booktitle> <pages> pages 28-40, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others <ref> [106, 107] </ref>. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [117], DEMOS/MP [88], Charlotte [5], Sprite [29]. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support.
Reference: [108] <author> S. Srinivasan and P. F. Reynolds Jr. </author> <title> NPSI adaptive synchronizaton algorithms for PDES. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 658-665, </pages> <year> 1995. </year>
Reference-contexts: Adaptive protocols can limit the uncontrolled execution of incorrect events, or limit the optimism of the optimistic protocol. Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based <ref> [108] </ref>. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs [61, 67, 94, 111, 120]. <p> The idea is that giving an LP unchecked access to memory each time such memory is requested, subject to machine limitations, may hamper progress at other LPs. In yet another approach, Srinivasan and Reynolds <ref> [108] </ref> introduce a class of adaptive protocols based on near-perfect state information (NPSI). A global reduction network is used to move state information between LPs at low cost. Each LP uses 22 this information to compute an error potential (EP). <p> Adaptive synchronization methods, in particular, provide a framework for tailoring synchronization to the peculiarities of a given application. Based on decisions made with run-time data, such methods offer a dynamic combination of optimistic and conservative synchronization, and have been reported to exhibit poor to reasonable performance in previous studies <ref> [24, 36, 50, 108] </ref>. Of particular importance is the fact that extreme forms of optimistic progress and conservative blocking can be avoided. In this section, we present a new and dynamic method for adaptive synchronization.
Reference: [109] <author> J. Steinman. </author> <title> Incremental state saving in SPEEDES using C++. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 687-696, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Various methods for efficient state saving of objects have been proposed. These include copy state saving the complete state is saved [40], incremental state saving only the state that is modified is saved <ref> [109] </ref>, 137 and hardware based state saving [43]. Efficient schemes, such as those employed in SPEEDES for incremental state saving, require the user to write state saving code because state saving depends on the type of event that was executed.
Reference: [110] <author> J. S. Steinman. SPEEDES: </author> <title> A unified approach to parallel simulation. </title> <booktitle> In Proceedings of the 6th Workshop on Parallel and Distributed Simulation, volume 24 of Simulation Series, </booktitle> <pages> pages 75-84, </pages> <year> 1992. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie [8], SPEEDES <ref> [110] </ref>, CPSim [49], Sim++ [6], ModSim [123], and TWOS [57]. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations. <p> SPEEDES (Synchronous Parallel Environment for Emulation and Discrete Event Simulation) began with the support of window based synchronization protocols, but currently supports multiple synchronization protocols: Time Buckets, Breathing Time Buckets, Time Warp <ref> [110] </ref>, and Breathing Time Warp [111]. These are all essentially windowing algorithms coupled with Time Warp synchronization. For example, in Breathing Time Warp, execution proceeds in three phases.
Reference: [111] <author> J. S. Steinman. </author> <title> Breathing Time Warp. </title> <booktitle> In Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 109-118, </pages> <year> 1993. </year>
Reference-contexts: SPEEDES (Synchronous Parallel Environment for Emulation and Discrete Event Simulation) began with the support of window based synchronization protocols, but currently supports multiple synchronization protocols: Time Buckets, Breathing Time Buckets, Time Warp [110], and Breathing Time Warp <ref> [111] </ref>. These are all essentially windowing algorithms coupled with Time Warp synchronization. For example, in Breathing Time Warp, execution proceeds in three phases. <p> Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based [108]. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs <ref> [61, 67, 94, 111, 120] </ref>. In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination [26, 44]. <p> The results shown in Figure 5.26 (a), for a 32x32 torus and a 64x64 torus, with a transaction density of 1 Job/Server, are similar to those obtained with other PDES systems (for example, <ref> [111] </ref>). Doubling the number of processors reduces the time required for run completion by more than half in some cases. The 1-processor run shown in 158 ratio ((a). CLUS, (b). PGON, and (c).
Reference: [112] <author> Sun Microsystems, Inc., Sun 825-1250-01. </author> <title> SunOS Programming Utilities and Libries: Lightweight Processes, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55]. <p> All measurements are reported in Table 4.4. From the measurements, Ariadne appears competitive with Sun-LWP and Sun-MT threads on most operations. Sun's large creation time with stack allocation via lwp_newstk () may be attributed to setup of red zone stacks protection against stack overflow <ref> [112] </ref>. The Sun-MT library also sets up protected stacks of default size 1MB using several system calls. Ariadne's context-switching is a little more expensive than context-switching in Sun-LWP and Sun-MT, possibly because Ariadne invokes several functions during a context-switch.
Reference: [113] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Threads provide an efficient mechanism for thr realization of processes in process-oriented simulations. Our communication system of choice is the PVM library <ref> [113] </ref>. PVM is a popular tool in the parallel computing community and satisfies ParaSol's requirements 100 of remote process spawning, architecture independent data formats, message passing, blocking and non blocking I/O, and the facility to probe for message types.
Reference: [114] <author> SunSoft, Inc., Sun 801-5294-10. </author> <note> SunOS 5.3 Guide to Multithread Programming, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: The Thread Create operation entails the creation of threads with and without preallocated stacks, respectively. In Sun-LWP, preallocation is done with the aid of lwp_setstkcache () and lwp_newstk () primitives. With Sun-MT, stacks are cached by default so that repetitive thread creation is cheap <ref> [114] </ref>. The Sun-MT threads library measurements are based on the default stack size allocated by the library. 76 Context Switch time is measured by timing context-switches between two specific threads.
Reference: [115] <author> A. S. Tanenbaum. </author> <title> Distributed Operating Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Contemporary operating systems support lightweight processes or threads as computational units. Threads may be supported in user-space (user-space threads) or within the operating system kernel (kernel-space threads). Each approach has its advantages and disadvantages <ref> [76, 115] </ref>. Threads are essentially mini-processes: each has a program counter and a stack to indicate its current state of execution. Each thread begins execution at the start of a function called the thread-function. <p> Another important requirement due to optimistic synchronization in PDES, is thread checkpointing for state saving and thread restoration on the occurrence of a rollback. 40 4. ARIADNE: ARCHITECTURE, DESIGN, IMPLEMENTATION, AND PERFORMANCE We propose the use of cheap, user-space threads <ref> [76, 115] </ref> as the unit of computation in shared- and distributed-memory environments.
Reference: [116] <author> A. Tevanian, R. F. Rashid, D. B. Golub, D. L. Black, E. Cooper, and M. W. Young. </author> <title> Mach threads and the UNIX kernel: The battle for control. </title> <booktitle> In Proceedings of the Summer USENIX Conference, </booktitle> <pages> pages 185-197, </pages> <year> 1987. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads <ref> [54, 87, 116] </ref>, and many user-space [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [117] <author> M. M. Theimer, K. A. Lantz, and D. R. Cheriton. </author> <title> Preemptable remote execution facilities for the V-system. </title> <booktitle> In Poceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <year> 1985. </year>
Reference-contexts: Proposals for its application include load sharing, resource sharing, communication overhead reduction and failure robustness, among others [106, 107]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V <ref> [117] </ref>, DEMOS/MP [88], Charlotte [5], Sprite [29]. In general, thread migration entails a higher level of architectural detail than that encountered in other aspects of thread support. Creating maps of run-time images of threads on heterogeneous machines is sufficiently complicated to render the effort impractical, particularly with respect to portability.
Reference: [118] <author> W.B. Thompson. </author> <title> A tutorial for modelling with the WITNESS visual interactive simulator. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 228-232, </pages> <month> December </month> <year> 1993. </year> <month> 208 </month>
Reference-contexts: Most simulations produce output in textual form when the simulation terminates. Graphical visualization is a useful simulation aid. Several commercial simulation systems provide run-time animation and graphical views of simulation output <ref> [91, 99, 118] </ref>. Requirements for such support include online graphical display and interaction, inspection of simulation state variables, and specification of model parameters [98]. A Parallel Discrete Event simulation places some special requirements on a Visualization and Interaction system.
Reference: [119] <author> B. Topol. Conch: </author> <title> Second generation heterogeneous computing. </title> <type> Technical report, </type> <institution> Department of Mathematics and Computer Science, Emory University, </institution> <year> 1992. </year>
Reference-contexts: Both calls are embedded in the a_thread_pack () primitive. In Figure 4.11 is shown a simplified implementation of the a_migrate () function, using a_thread_pack (), a_thread_unpack (), and related support functions. In this example we utilize the Conch communications library <ref> [119] </ref> for message passing: functions with a c_ prefix are Conch primitives. <p> With reliable communication, this scheme has proven simple to implement, and effective. The controller process is chosen arbitrarily: with Conch we use a front-end (see <ref> [119] </ref>) process, and with PVM we use the first process that is created. The termination algorithm is implemented using Ariadne's customization layer. If Ariadne processes frequently create and destroy threads, state transitions may be frequent. Messages enabling such transitions, and messages sent to the controller may result in high traffic. <p> In this case, Ariadne's messaging support 92 200 600 1000 Processors Execution On 4K stacks (secs) 256x256 3 128x128 + + + 2 200 600 1000 1400 1800 Processors Execution On Common Stack (secs) 256x256 3 3 128x128 + + + 2 came from the Conch <ref> [119] </ref> distributed system, which allows processors to be configured in a ring. The uniprocessor version of the program is also threads-based, but does not incur distributed system overheads. The environment was not dedicated to the application, though experiments were conducted during off-peak hours. <p> PVM has been ported to a large number of environments, thus enhancing availability. ParaSol is designed to work with other communications systems that provide similar functionality. For example, ParaSol also works with the Conch communications system <ref> [119] </ref>. Conch uses TCP as its transport mechanism whereas PVM's primary transport is UDP based (though TCP is also permitted as an option in PVM). ParaSol makes use of Unix based inter process communication, such as shared memory or message queues, when processes are hosted on the same machine.
Reference: [120] <author> S. J. Turner and M. Q. Xu. </author> <title> Performance evaluation of the Bounded Time Warp algorithm. </title> <booktitle> In Proceedings of the 6th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 117-126, </pages> <year> 1992. </year>
Reference-contexts: Protocols that limit optimism have been classified as: window based, space based, penalty based, knowledge based, probabilistic, and state based [108]. In window based protocols, only events within a virtual time window [t; t + T ] are processed, usually followed by some form of synchronization among LPs <ref> [61, 67, 94, 111, 120] </ref>. In some space based protocols, LPs process events optimistically but send messages to other LPs only when these messages are safe. That is, arriving messages do not cause rollbacks at a destination [26, 44].
Reference: [121] <author> M. Vandevoorde and E. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17 </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: A value of s that yields good performance is obtained while the simulation progresses, by observing the effect of a change in s on measurable metrics like the event commitment rate. 2.2 Multithreaded Systems Several operating systems Mach, AIX, and Solaris offer kernel-space threads [54, 87, 116], and many user-space <ref> [4, 10, 14, 23, 59, 79, 92, 105, 112, 121] </ref> threads systems have been developed. Other systems attempt to combine the advantages of kernel-space and user-space threads [3, 71]. A POSIX standard for a threads interface is also available [55].
Reference: [122] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus any existing communications system that meets these requirements can be used. A particular implementation of the interface can change depending on the capabilities of the particular communications system. A more powerful communications system (for example, active messages <ref> [122] </ref>) can provide the required functionality efficiently without requiring changes in the simulator kernel. 3.3.1 Visualization and Interaction Support The objective of any simulation is to study and draw conclusions on the behavior of the system based on the results of the simulation. <p> In the figure, the frequency that gives the minimum cost is between 4 and 8 events. 191 Active-message based communication systems <ref> [122] </ref> allow the execution of a function at a destination processor, depending on the message type received. This mechanism can simplify and reduce the cost of probing for messages. When a message arrives it can execute a function that sets a flag in the simulator kernel.
Reference: [123] <author> J. West and A. Mullarney. ModSim: </author> <title> A language for distributed simulation. </title> <booktitle> In Proceedings of SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 155-159, </pages> <year> 1988. </year>
Reference-contexts: Finally, in Chapter 7, we conclude and discuss plans for future work. 17 2. RELATED WORK 2.1 Parallel Discrete Event Simulation Systems The past two decades has witnessed the development of several experimental as well as commercial parallel discrete event simulation systems <ref> [1, 6, 8, 40, 47, 49, 57, 82, 90, 110, 123] </ref>. These differ with respect to their programming paradigms, synchronization protocol, modeling view, runtime characteristics, and performance. <p> We take advantage of shared memory when ever processes are hosted on a multiprocessor by exploiting shared memory communication. Recently developed experimental systems that run on distributed memory multiprocessors include 18 Maisie [8], SPEEDES [110], CPSim [49], Sim++ [6], ModSim <ref> [123] </ref>, and TWOS [57]. We discuss some of these systems next. The Time Warp Operating system (TWOS) is a multiprocessor operating system directed towards PDES. It runs on a Mark III hypercube and on clusters of Sun workstations.

References-found: 124

