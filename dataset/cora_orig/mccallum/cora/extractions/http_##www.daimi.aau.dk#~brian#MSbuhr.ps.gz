URL: http://www.daimi.aau.dk/~brian/MSbuhr.ps.gz
Refering-URL: http://www.daimi.aau.dk/~brian/DAIP96.html
Root-URL: http://www.daimi.aau.dk
Email: e-mail: buhr@daimi.aau.dk  
Title: Aesthetics of Newspaper Layout and a Survey on Architecture Determining Algorithms  
Author: By Morten Buhr 
Date: December 1995  
Address: DK-8000 Aarhus C, Denmark.  
Affiliation: Computer Science Department, Aarhus University  
Abstract-found: 0
Intro-found: 0
Reference: [Brit 69] <institution> (1969): Encyclopaedia Britannica, Encyclopaedia Britan-nica Inc., Chicago, William Benton. </institution>
Reference-contexts: He is known for his efforts in cutting down the amount of abstract ideas which were used in the philosophy of his time. His ideas are summed up in these two phrases (from <ref> [Brit 69] </ref> and [Gillispie 74]): Multiplicity ought not to be posited without necessity 1 . and What can be accounted for by fewer assumptions is explained in vain by more. 1 Translation of the title of his famous thesis: Plurales non est ponenda sine necessitate. 14 Decremental Algorithms The principle has
Reference: [Buhr et al. 93] <author> M. Buhr, P. Dalsgaard and I. </author> <month> Hoy </month> <year> (1993): </year> <title> MIP Cascade-Correlation, Internal Report (in Danish), </title> <type> DAIMI, </type> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: Realising that limiting the architecture to a two-layer perceptron may not always be a good idea, an extension of the 2nd-order CCA producing a multilayered perceptron with multiple nodes in each layer was proposed in <ref> [Buhr et al. 93] </ref>. Nodes in hidden layers are only receiving inputs from nodes in the layer immediately below, whereas output nodes receive input from all nodes. Only one layer is trained at a time, adding one node at a time to this layer.
Reference: [Christensen 94] <author> H. B. </author> <title> Christensen (1994): A new framework for layout construction, </title> <type> Version 2, Internal Report, </type> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: Thence the set of articles is divided into two smaller sets which are to be placed in the two free areas resulting from the bisection. The algorithm is described in details in [Frandsen et al. 93] and <ref> [Christensen 94] </ref>. The LPA is meant to serve as a tool for the layout operator working in a text driven page composition environment (possibly a combination of a text driven and layout driven environment). <p> Article shapes differing from shapes used by DN are prohibited. E.g. a picture cannot be placed in between the headline and the leading column. This is ensured by the template approach applied to the LPA described in <ref> [Christensen 94] </ref>. 7.4.2 Making Layouts Suitable for ANNs Having the PDL-descriptions of the original DN-pages and the produced pages what is left to do is to decide on a ANN representation of the data, and transform these PDL-descriptions into the chosen representation (the PDL!ANN compiler).
Reference: [Depenau et al. 94] <author> J. Depenau and M. </author> <title> Moller (1994): Aspects of Generalization and Pruning, </title> <booktitle> in Proc. from the World Congress on Neural Networks, </booktitle> <address> San Diego, </address> <booktitle> 1994, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 464-469. </pages>
Reference-contexts: It might also be difficult to split the data set into three (allowing for the use of a validation set) if the amount of data available is very limited. 3.6 Which Pruning Algorithm Is the Best? In <ref> [Depenau et al. 94] </ref> and [Vlk & Knuden 94] experiments have been carried out for comparison of the pruning algorithms MAG, OBD and OBS, using the MONK's problem [Thrun et al. 91] as a benchmark.
Reference: [Depenau 95a] <author> J. </author> <month> Depenau </month> <year> (1995): </year> <title> Automated Design of Neural Network Architecture for Classification, </title> <type> Ph.D. Thesis, DAIMI, </type> <institution> Computer Science Department, Aarhus University. </institution>
Reference-contexts: In general the VC dimension of an ANN is unknown. However, in the case of a linear classifier, the VC dimension can be shown (e.g. [Hertz et al. 91]) to be the number of free parameters (the number of weights plus the threshold). In <ref> [Depenau 95a] </ref> it is further shown that many networks built automatically by a construction algorithm (construction algorithms are to be treated in Chapter 4) have the same upper bound on the VC dimension as a linear classifier. 2.4 Summary Albert Einstein once said: "Make it simple; as simple as possible, But
Reference: [Depenau 95b] <author> J. </author> <month> Depenau </month> <year> (1995): </year> <title> A Glocal-Local Learning Architecture for Classification, </title> <booktitle> in Proc. from the World Congress on Neural Networks, 1995, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 587-590. </pages>
Reference-contexts: Where the Cascade-Correlation Algorithm use Sigmoidal transfer functions, other successful construction algorithms are based on Radial Basis Functions, e.g. [Reilly et al. 82] and [Platt 91]. Recently a new construction algorithm combining nodes with Sigmoid Functions and Radial Basic Functions has been proposed <ref> [Depenau 95b] </ref>.
Reference: [Depenau et al. 95] <author> J. Depenau and M. </author> <title> Moller (1995): The MS-oe Error Function, </title> <booktitle> in Proc. from the World Congress on Neural Networks, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 614-617. </pages>
Reference: [Denker et al. 87] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel and J. </author> <title> Hopfield (1987): Large Automatic Learning, Rule Extraction, and Generalization, </title> <journal> Complex Systems, </journal> <volume> No. 1, </volume> <pages> pp. 877-922. </pages>
Reference-contexts: U U T L G3 In <ref> [Denker et al. 87] </ref> an illustration of learning and generalisation can be found. The task of learning and generalising is explained through the illustrations in pairs, some of which are consistent with a rule, R.
Reference: [Fahlman 88] <author> S. E. </author> <title> Fahlman (1988): Faster-Learning Variations on Back-Propagation: An Empirical Study, </title> <booktitle> in Proceedings of the 1988 Connectionist Summer School, </booktitle> <publisher> Morgan Kauf-mann. 102 BIBLIOGRAPHY </publisher>
Reference-contexts: the network correctly classified the pattern: (i O )g 0 (h ) (1 (1)) g 0 (h ) = 0 g 0 (h ) = 0 This problem can to some extent be overcome by adding a small offset to the derivative, as it is suggested with the QuickProp algorithm <ref> [Fahlman 88] </ref>, or simply by omitting the derivative. Alternatively a better suited error function could be used. It should also be noted that target values around 0 are not to be preferred since it is hard to teach a node to output 0.
Reference: [Fahlman et al. 91] <author> S. E. Fahlman and C. </author> <booktitle> Lebiere (1991): The Cascade-Correlation Learning Architecture, in Advances in Neural Information Systems II, </booktitle> <year> 1990, </year> <pages> pp. 524-532, </pages> <editor> ed. D. E. Touretzky, </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On the face of it, this approach does not seem to produce networks with good generalisation abilities, but the reported results seem encouraging. In 1990 Fahlman and Lebiere proposed the Cascade-Correlation Algorithm (CCA) <ref> [Fahlman et al. 91] </ref>, and this algorithm has turned out to be the most commonly used among the different incremental learning algorithms. <p> thus the input patterns are faithful), ancillary nodes are added in each layer, to make sure that the patterns not correctly classified by the master node are made faithful. 38 Incremental Algorithms 4.3 Continuous Algorithms Various construction algorithms building ANNs with continuous transfer functions exist, but only the Cascade-Correlation Algorithm <ref> [Fahlman et al. 91] </ref> and a few variants of it will be treated here. Where the Cascade-Correlation Algorithm use Sigmoidal transfer functions, other successful construction algorithms are based on Radial Basis Functions, e.g. [Reilly et al. 82] and [Platt 91]. <p> idea of this algorithm is to make a global search for regularities in the training set, using a simple perceptron with Sigmoidal nodes, and combine this with a local search, using Radial Basis nodes added incrementally to take care of the irregularities. 4.3.1 The Cascade-Correlation Algorithm The Cascade-Correlation Algorithm (CCA) <ref> [Fahlman et al. 91] </ref> differs from the construction algorithms presented in Section 4.2 by building nets containing nodes with continuous transfer functions. <p> This way of constructing the network | by training and adding one node at a time and freezing the incoming weights when a node is inserted | should prevent what is known as the Moving Target Problem <ref> [Fahlman et al. 91] </ref>. The term covers the phenomenon which is often observed when training a multilayer perceptron with the Back-Propagation algorithm. Each node in the network is trying to extract some feature from the input that will improve the network's overall performance. <p> The correlation factor, Corr [X; Y ], takes values on the interval [1; 1], thus the goal of the CCA, when training a candidate node, is to maximise the magnitude of the correlation between the output of the node to be inserted and the residual error. Empirical results <ref> [Fahlman et al. 91] </ref> show that performance is improved by using the covariance instead of the correlation. That is, by omitting the normalisation of the covariance, performance is improved. <p> It was clearly illustrated that bringing the output nodes to saturation is unfortunate, as it gives poor target values. 4.3.5 Improvements of the CCA The CCA has proven to be very efficient with the task of learning a variety of classification problems, e.g. <ref> [Fahlman et al. 91] </ref> and [Thrun et al. 91]. The constructed networks' ability to generalise has, however, not shown the same convincing results. It is believed that the reason for this is that the CCA tends to build networks with very high capacity. <p> The drawback with these algorithm is that they build discrete nets with nodes using threshold transfer functions, hence the practical use of the algorithms is limited. However, the Cascade-Correlation Algorithm (CCA) <ref> [Fahlman et al. 91] </ref> builds networks with nodes using continuous transfer functions. The CCA was derived in details in order to make it easier to understand the principle in the CCA and how the algorithm was derived.
Reference: [Frandsen et al. 93] <author> G. S. Frandsen, J. Palsberg, E. M. Schmidt and S. </author> <month> Sjogaard </month> <year> (1993): </year> <title> Layout Construction: A Case Study In Algorithm Engineering, </title> <institution> DAIMI PB-450, Computer Science Department, Aarhus University. </institution>
Reference-contexts: Thence the set of articles is divided into two smaller sets which are to be placed in the two free areas resulting from the bisection. The algorithm is described in details in <ref> [Frandsen et al. 93] </ref> and [Christensen 94]. The LPA is meant to serve as a tool for the layout operator working in a text driven page composition environment (possibly a combination of a text driven and layout driven environment).
Reference: [Frean 90] <author> M. </author> <month> Frean </month> <year> (1990): </year> <title> The Upstart Algorithm: A Method for Constructing and Training Feed-Forward Neural Networks, </title> <journal> Neural Computation, </journal> <volume> Vol. 2, </volume> <pages> pp. 198-209. </pages>
Reference-contexts: In the wake of that, several learning algorithms 34 Incremental Algorithms were proposed which decide on an appropriate network architecture during learning. Among the best known and most successful of these algorithms are: The Upstart Algorithm <ref> [Frean 90] </ref> and the Tiling Algorithm [Mezard et al. 89]. Being restricted to the use of binary nodes (i.e. nodes which can only output 1 or +1) the practical use of both algorithms are, however, rather limited.
Reference: [Gallant 86a] <author> S. I. </author> <title> Gallant (1986): Three Construction Algorithms For Network Learning, </title> <booktitle> in Proc. 8th Annual Conf. of Cognitive Science Society, </booktitle> <pages> pp. 652-660. </pages>
Reference-contexts: The nodes in the constructed networks all use a threshold function as transfer function. None of these algorithms are able to handle inconsistent training sets. 4.2.1 Three Simple Algorithms In <ref> [Gallant 86a] </ref> and [Gallant 90] three different construction algorithms are proposed. Though being very simple and easy to understand, the ideas and concepts introduced with these algorithms applies to more recent and more sophisticated construction algorithms. <p> This is repeated until the problem is solved. See Figure 4.1 (a) for an illustration of the produced architecture: A two-layer perceptron with short-cut connections from input nodes to output nodes. It is argued <ref> [Gallant 86a] </ref> that the inserted nodes will act as feature detectors, but what features they are detecting is unknown | and it does not matter; if the node is useless, another one is simply inserted. (a) (b) (c) Tower Construction Algorithm. (c) The Inverted Pyramid Construction Algorithm. <p> This approach is reported to greatly reduce the number of weights. 4.4 Summary Various learning algorithms which incrementally build the network architecture during training have been presented, starting with three simple algorithms proposed in <ref> [Gallant 86a] </ref> and the Tiling Algorithm [Mezard et al. 89]. The drawback with these algorithm is that they build discrete nets with nodes using threshold transfer functions, hence the practical use of the algorithms is limited.
Reference: [Gallant 86b] <author> S. I. </author> <title> Gallant (1986): Optimal Linear Discriminants, </title> <booktitle> in Proc. 8th International Conf. on Pattern Recognition, </booktitle> <pages> pp. 849-852. </pages>
Reference-contexts: This goes on until the entire training set is learned. The convergence of the algorithms is ensured through the use of a special learning algorithm, called the Pocket Algorithm <ref> [Gallant 86b] </ref>.
Reference: [Gallant 90] <author> S. I. </author> <title> Gallant (1990): Perceptron-Based Learning Algorithms, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 179-191. </pages>
Reference-contexts: The nodes in the constructed networks all use a threshold function as transfer function. None of these algorithms are able to handle inconsistent training sets. 4.2.1 Three Simple Algorithms In [Gallant 86a] and <ref> [Gallant 90] </ref> three different construction algorithms are proposed. Though being very simple and easy to understand, the ideas and concepts introduced with these algorithms applies to more recent and more sophisticated construction algorithms. <p> The Distributed Construction Algorithm This algorithm | the simplest of the three | relies on it being possible to implement any Boolean function by a two-layer perceptron using threshold nodes if enough hidden nodes are present [Minsky & Papert 69]. In the version of the algorithm proposed in <ref> [Gallant 90] </ref> the algorithm starts with a simple perceptron, and if the problem is not linearly separable, hidden nodes connected to all the input nodes and the output node are added one by one.
Reference: [Gillispie 74] <author> (1974): </author> <title> Dictionary of Scientific Biography, </title> <editor> ed. C. C. Gillispie, </editor> <address> New York, </address> <publisher> Charles Scribner's Sons. </publisher>
Reference-contexts: He is known for his efforts in cutting down the amount of abstract ideas which were used in the philosophy of his time. His ideas are summed up in these two phrases (from [Brit 69] and <ref> [Gillispie 74] </ref>): Multiplicity ought not to be posited without necessity 1 . and What can be accounted for by fewer assumptions is explained in vain by more. 1 Translation of the title of his famous thesis: Plurales non est ponenda sine necessitate. 14 Decremental Algorithms The principle has gained a footing
Reference: [Gorodkin et al. 93] <author> J. Gorodkin, L. K. Hansen, A. Krogh, C. Svarer and O. </author> <title> Winther (1993): A Quantitative Study of Pruning By Optimal Brain Damage, </title> <journal> International Journal of Neural Systems, </journal> <volume> No. 4, </volume> <pages> pp. 159-169. </pages>
Reference-contexts: The third term in Equation 3.2 thereby vanishes 2 . By employing these three assumptions Equation 3.2 reduces to: E = q = q 2 The term S q will be called the saliency for weight w q . 2 The validity of this approximation is examined in <ref> [Gorodkin et al. 93] </ref>. 20 Decremental Algorithms The OBD algorithm takes a trained network, computes the saliencies and deletes weights with least saliencies. As with the pruning algorithms presented previously, a retraining procedure has to be applied each time weights have been deleted.
Reference: [Hansen et al. 93] <author> L. K. Hansen and M. W. </author> <title> Pedersen (1993): Controlled Growth of Cascade Correlation Nets, </title> <type> Internal Report, CONNECT, </type> <institution> Electronics Institute, Technical University of Denmark. </institution>
Reference-contexts: Several ad-hoc methods were tried out, but none turned out to be useful. 4.4 Summary 47 Another obvious way of reducing the fan-in of the hidden nodes is to prune the incoming weights. In <ref> [Hansen et al. 93] </ref> Optimal Brain Surgeon is used for pruning the incoming weights to the best candidate node before it is inserted as a new hidden node.
Reference: [Hassibi et al. 94] <author> B. Hassibi and D. G. </author> <month> Stork </month> <year> (1994): </year> <title> Second Order Derivatives for Network Pruning: Optimal Brain Surgeon, </title> <booktitle> in Advances in Neural Information Processiong Systems V, </booktitle> <year> 1993, </year> <pages> pp. 164-171, </pages> <editor> ed. D.S. Touretsky, </editor> <publisher> Morgan-Kaufmann. BIBLIOGRAPHY 103 </publisher>
Reference-contexts: Blur damage: Retrain N to an acceptable error. 6. Employ stopping criterion: If not stop: Goto 2. The OBD algorithm differs from the pruning algorithms previously presented by allowing several weights to be deleted between each retraining procedure, due to the independency approximation. 3.3.4 Optimal Brain Surgeon In <ref> [Hassibi et al. 94] </ref> an improvement of the OBD algorithm: The Optimal Brain Surgeon algorithm is suggested. One of the main objectives against OBD is that the saliency is an estimate of the expected change in error before the network has been retrained. <p> algorithm for computing the inverse Hessian matrix: (H ) = (H ) [] 1 [+1] [+1] T [] 1 [+1] T [] 1 [+1] ; (3.23) with: (H [0] 1 ff I and (H ) = H 1 , where ff is a small constant (10 8 ff 10 4 <ref> [Hassibi et al. 94] </ref>) needed in order to make (H [0] 1 meaningful and easy computable. Moreover, the Sherman-Morrison formula gives the extra advantage of there being no singularities in the computation of the inverse matrix. The ff-parameter thus becomes the only "learning" parameter in the OBS algorithm.
Reference: [Hassibi et al. 95] <author> B. Hassibi and D. G. </author> <month> Stork </month> <year> (1995): </year> <title> Second Order Derivatives for Network Pruning: Optimal Brain Surgeon, </title> <booktitle> in Advances in Neural Information Processing Systems VI, </booktitle> <year> 1994, </year> <pages> pp. 263-270, </pages> <editor> ed. D.S. Touretsky, </editor> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: That is, if the matrix multiplications needed in the algorithm outlined in Figure 3.6 is performed with caution by making use of the symmetry of the inverse Hessian matrix, the information needed to be stored can be reduced to a triangular part of the inverse Hessian matrix. In <ref> [Hassibi et al. 95] </ref> eigenspace decomposition has been used to reduce the dimension of the full Hessian matrix.
Reference: [Hertz et al. 91] <author> J. Hertz, A. Krogh and R. G. </author> <title> Palmer (1991): Introduction To the Theory Of Neural Computation, </title> <address> Santa Fe Institute, </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: The notation used will follow the line of <ref> [Hertz et al. 91] </ref>. The networks considered will be layered feed-forward nets, in which nodes can receive input from arbitrary nodes in underlying layers. The network N consists of nodes n k i arranged in layers k = 0; 1; : : : ; M . <p> Confidence Generalisation Error Capacity Error Interval Upper Bound on the Learning Error VC * (allowing a low training error) and low generalisation error. In general the VC dimension of an ANN is unknown. However, in the case of a linear classifier, the VC dimension can be shown (e.g. <ref> [Hertz et al. 91] </ref>) to be the number of free parameters (the number of weights plus the threshold).
Reference: [Hwang et al. 94] <author> J. Hwang, S. You, S. Lay and I. </author> <title> Jou (1994): What's Wrong with a Cascaded Correlation Learning Network: </title>
References-found: 22

