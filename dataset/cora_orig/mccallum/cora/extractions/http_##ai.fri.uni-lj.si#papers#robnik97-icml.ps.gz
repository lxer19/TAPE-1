URL: http://ai.fri.uni-lj.si/papers/robnik97-icml.ps.gz
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: Marko.Robnik@fri.uni-lj.si  Igor.Kononenko@fri.uni-lj.si  
Title: An adaptation of Relief for attribute estimation in regression  
Author: Marko Robnik- Sikonja Igor Kononenko 
Address: Trzaska 25, 1001 Ljubljana, Slovenia,  Trzaska 25, 1001 Ljubljana, Slovenia,  
Affiliation: University of Ljubljana, Faculty of Computer and Information Science,  University of Ljubljana, Faculty of Computer and Information Science,  
Abstract: Heuristic measures for estimating the quality of attributes mostly assume the independence of attributes so in domains with strong dependencies between attributes their performance is poor. Relief and its extension ReliefF are capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. By exploiting local information provided by different contexts they provide a global view. We present the analysis of Reli-efF which lead us to its adaptation to regression (continuous class) problems. The experiments on artificial and real-world data sets show that Re-gressional ReliefF correctly estimates the quality of attributes in various conditions, and can be used for non-myopic learning of the regression trees. Regressional ReliefF and ReliefF provide a unified view on estimating the attribute quality in regression and classification.
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G., Moore, A. W., and Schall, S. </author> <year> (1996). </year> <title> Locally weighted learning. </title> <type> Technical report, </type> <institution> Georga Institute of Technology. </institution>
Reference: <author> Breiman, L., Friedman, L., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <publisher> Wadsworth Inc., </publisher> <address> Belmont, California. </address>
Reference-contexts: 1 Introduction The majority of current propositional inductive learning systems predict discrete class. They can also solve the regression (also called continuous class) problems by dis-cretizing the prediction (class) in advance. This approach is often inappropriate. Regression learning systems (also called function learning systems), e.g., CART <ref> (Breiman et al., 1984) </ref>, Retis (Karalic, 1992), M5 (Quinlan, 1993), directly predict continuous value. The problem of estimating the quality of attributes seems to be an important issue in both classification and regression and in machine learning in general (e.g., feature selection, constructive induction). <p> Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes, e.g., information gain (Hunt et al., 1966), Gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), and j-measure (Smyth and Goodman, 1990) for discrete class and the mean squared and the mean absolute error (Breiman et al., 1984) for regression. They are therefore less appropriate in domains with strong dependencies between attributes. <p> Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes, e.g., information gain (Hunt et al., 1966), Gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), and j-measure (Smyth and Goodman, 1990) for discrete class and the mean squared and the mean absolute error (Breiman et al., 1984) for regression. They are therefore less appropriate in domains with strong dependencies between attributes. <p> We compare the estimates of RReliefF with the mean squared error (MSE) as a measure of the attribute's quality <ref> (Breiman et al., 1984) </ref>. This measure is standard in regression tree systems. <p> We have run our system with two sets of parameters and procedures. They are named according to the type of mod els used in the leaves. Point mode is similar to CART <ref> (Breiman et al., 1984) </ref> and uses the average prediction (class) value of the examples in each leaf node as the predictor.
Reference: <author> Brodley, C. E. </author> <year> (1995). </year> <title> Automatic selection of split criterion during tree growing based on node location. </title> <booktitle> In Proceedings of the XII International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Domingos, P. </author> <year> (1997). </year> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review. </journal> <note> (to appear). </note>
Reference: <author> Elomaa, T. and Ukkonen, E. </author> <year> (1994). </year> <title> A geometric approach to feature selection. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Proceedings of European Conference on Machine Learning, </booktitle> <pages> pages 351354. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Relief (Kira and Rendell, 1992) and its extension ReliefF (Kononenko, 1994) are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit (Hong, 1994) and the geometrical approach <ref> (Elomaa and Ukkonen, 1994) </ref>. We present the analysis of ReliefF which lead us to adapt it to regression problems. Several other researchers have investigated the use of local information and profited from being aware of it (Domingos, 1997; Atkeson et al., 1996; Friedman, 1994).
Reference: <author> Friedman, J. H. </author> <year> (1994). </year> <title> Flexible metric nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford University. </institution> <note> available by anonomous ftp from playfair.stanford.edu pub/friedman. </note>
Reference: <author> Hong, S. J. </author> <year> (1994). </year> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report RC19664, </type> <institution> IBM. </institution>
Reference-contexts: Relief (Kira and Rendell, 1992) and its extension ReliefF (Kononenko, 1994) are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit <ref> (Hong, 1994) </ref> and the geometrical approach (Elomaa and Ukkonen, 1994). We present the analysis of ReliefF which lead us to adapt it to regression problems. Several other researchers have investigated the use of local information and profited from being aware of it (Domingos, 1997; Atkeson et al., 1996; Friedman, 1994). <p> If A i is the continuous attribute, dif f (A i ; 2; 5) = 7 0:43. So, with this form of dif f function continuous attributes are underestimated. We can overcome this problem with the ramp function as proposed by <ref> (Hong, 1994) </ref>.
Reference: <author> Hunt, E., Martin, J., and Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: The problem of estimating the quality of attributes seems to be an important issue in both classification and regression and in machine learning in general (e.g., feature selection, constructive induction). Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes, e.g., information gain <ref> (Hunt et al., 1966) </ref>, Gini index (Breiman et al., 1984), distance measure (Mantaras, 1989), and j-measure (Smyth and Goodman, 1990) for discrete class and the mean squared and the mean absolute error (Breiman et al., 1984) for regression. They are therefore less appropriate in domains with strong dependencies between attributes.
Reference: <author> Karalic, A. </author> <year> (1992). </year> <title> Employing linear regression in regression tree leaves. </title> <editor> In Neumann, B., editor, </editor> <booktitle> Proceedings of ECAI'92, </booktitle> <pages> pages 440441. </pages> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: They can also solve the regression (also called continuous class) problems by dis-cretizing the prediction (class) in advance. This approach is often inappropriate. Regression learning systems (also called function learning systems), e.g., CART (Breiman et al., 1984), Retis <ref> (Karalic, 1992) </ref>, M5 (Quinlan, 1993), directly predict continuous value. The problem of estimating the quality of attributes seems to be an important issue in both classification and regression and in machine learning in general (e.g., feature selection, constructive induction).
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <editor> In D.Sleeman and P.Edwards, editors, </editor> <booktitle> Proceedings of International Conference on Machine Learning, </booktitle> <pages> pages 249256. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They are therefore less appropriate in domains with strong dependencies between attributes. Relief <ref> (Kira and Rendell, 1992) </ref> and its extension ReliefF (Kononenko, 1994) are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit (Hong, 1994) and the geometrical approach (Elomaa and Ukkonen, 1994). <p> W [A] := W [A] - diff (A,R,H)/m + diff (A,R,M)/m; 7. end; 2 RReliefF 2.1 Relief and ReliefF for classification The key idea of the original Relief algorithm <ref> (Kira and Rendell, 1992) </ref>, given in Figure 1, is to estimate the quality of attributes according to how well their values distinguish between the instances that are near to each other.
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: analysis and extensions of Relief. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Machine Learning: ECML-94, </booktitle> <pages> pages 171 182. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: They are therefore less appropriate in domains with strong dependencies between attributes. Relief (Kira and Rendell, 1992) and its extension ReliefF <ref> (Kononenko, 1994) </ref> are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit (Hong, 1994) and the geometrical approach (Elomaa and Ukkonen, 1994). <p> The total distance is simply the sum of distances over all attributes. Relief's estimate W [A] of the quality of attribute A is an approximation of the following difference of probabilities <ref> (Kononenko, 1994) </ref>: W [A] = P (diff: value of Ajnearest inst: from diff: class) P (diff: value of Ajnearest inst: from same class) (3) The complexity of Relief for n training instances and A attributes is O (m fi n fi A). <p> Its intrinsic contextual nature allows it to recognize contextual attributes. From our experimental results we can conclude that learning regression trees with RReliefF is promising especially in combination with linear models in the leaves of the tree. Both, RReliefF in regression and ReliefF in classification <ref> (Kononenko, 1994) </ref> are estimators of (7), which gives a unified view on the estimation of the quality of attributes for classification and regression. RReliefF's good performance and robustness indicate its appropriateness for feature selection.
Reference: <author> Kononenko, I., Simec, E., and Robnik- Sikonja, M. </author> <year> (1997). </year> <title> Overcoming the myopia of inductive learning algorithms with RELIEFF. </title> <journal> Applied Intelligence, 7:3955. </journal>
Reference-contexts: Relief has commonly been viewed as a feature selection method that is applied in a prepossessing step before the model is learned, however it has recently also been used during the learning process to select splits in the building phase of decision tree <ref> (Kononenko et al., 1997) </ref>. We experimented also with similar use in regression trees. In the next Section we present and analyze the novel RRe-liefF (Regressional ReliefF) algorithm for estimating the quality of attributes in regression problems. <p> These estimates are computed on the subset of the examples that reach current node. Such use of ReliefF on classification problems was shown to be sensible and can significantly outperform impurity measures <ref> (Kononenko et al., 1997) </ref>. We have run our system with two sets of parameters and procedures. They are named according to the type of mod els used in the leaves.
Reference: <author> Lubinsky, D. J. </author> <year> (1995). </year> <title> Increasing the performance and consistency of classification trees by using the accuracy criterion at the leaves. </title> <booktitle> In Proceedings of the XII International Conference on Machine Learning. </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Mantaras, R. </author> <year> (1989). </year> <title> ID3 revisited: A distance based criterion for attribute selection. </title> <booktitle> In Proceedings of Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, USA. </address>
Reference-contexts: Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes, e.g., information gain (Hunt et al., 1966), Gini index (Breiman et al., 1984), distance measure <ref> (Mantaras, 1989) </ref>, and j-measure (Smyth and Goodman, 1990) for discrete class and the mean squared and the mean absolute error (Breiman et al., 1984) for regression. They are therefore less appropriate in domains with strong dependencies between attributes.
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1995). </year> <note> UCI repository of machine learning databases. (http://www.ics.uci.edu/ mlearn/MLRepository.html). </note>
Reference-contexts: We ran our system on the artificial data sets and on domains with continuous prediction value from UCI <ref> (Murphy and Aha, 1995) </ref>. Artificial data sets were used as described above (11 data sets, each consisting of 10 attributes - 2, 3 or 4 important, the rest are random, and containing 1000 examples). For each domain we collected the results as the average of 10 fold cross-validation.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proceedings of the X. International Conference on Machine Learning, </booktitle> <pages> pages 236243. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They can also solve the regression (also called continuous class) problems by dis-cretizing the prediction (class) in advance. This approach is often inappropriate. Regression learning systems (also called function learning systems), e.g., CART (Breiman et al., 1984), Retis (Karalic, 1992), M5 <ref> (Quinlan, 1993) </ref>, directly predict continuous value. The problem of estimating the quality of attributes seems to be an important issue in both classification and regression and in machine learning in general (e.g., feature selection, constructive induction). <p> The '-' sign means that the estimator did not succeed to significantly distinguish between the two groups even with a single random attribute. 4 Building regression trees We have developed a learning system which builds binary regression and model trees (as named by <ref> (Quinlan, 1993) </ref>) by recursively splitting training examples based on the values of attributes. The attribute in each node is selected according to the estimates of the attributes' quality by RRe-liefF or by MSE (9). These estimates are computed on the subset of the examples that reach current node. <p> Instead of cost complexity pruning of CART which demands the cross validation or separate set of examples for setting its complexity parameter we were using the Retis' pruning with m-estimate of probability (Kar-alic, 1992) which produces comparable or better re sults. Linear mode is similar to M5 <ref> (Quinlan, 1993) </ref>, and uses pruned linear models in each leaf node as the predictor. We were using the same procedures for pruning and smoothing of the trees as M5.
Reference: <author> Smyth, P. and Goodman, R. </author> <year> (1990). </year> <title> Rule induction using information theory. </title> <editor> In Piatetsky-Shapiro, G. and Frawley, W., editors, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes, e.g., information gain (Hunt et al., 1966), Gini index (Breiman et al., 1984), distance measure (Mantaras, 1989), and j-measure <ref> (Smyth and Goodman, 1990) </ref> for discrete class and the mean squared and the mean absolute error (Breiman et al., 1984) for regression. They are therefore less appropriate in domains with strong dependencies between attributes.
References-found: 17

