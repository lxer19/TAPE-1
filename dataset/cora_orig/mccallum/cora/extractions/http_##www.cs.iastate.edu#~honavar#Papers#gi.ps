URL: http://www.cs.iastate.edu/~honavar/Papers/gi.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/homepage.html
Root-URL: http://www.cs.iastate.edu
Email: e-mail: rpare@allstate.com  e-mail: honavar@cs.iastate.edu  
Title: Grammar Inference, Automata Induction, and Language Acquisition  
Author: Rajesh Parekh Vasant Honavar 
Address: 321 Middlefield Road, Menlo Park CA 94025  Ames IA 50011-1040. U.S.A.  
Affiliation: Allstate Research and Planning Center  Artificial Intelligence Research Laboratory Department of Computer Science Iowa State University  
Abstract: The natural language learning problem has attracted the attention of researchers for several decades. Computational and formal models of language acquisition have provided some preliminary, yet promising insights of how children learn the language of their community. Further, these formal models also provide an operational framework for the numerous practical applications of language learning. We will survey some of the key results in formal language learning. In particular, we will discuss the prominent computational approaches for learning different classes of formal languages and discuss how these fit in the broad context of natural language learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alquezar and A. Sanfeliu. </author> <title> A hybrid connectionist-symbolic approach to regular grammar inference based on neural learning and hierarchical clustering. </title> <editor> In R. C. Carrasco and J. Oncina, editors, </editor> <booktitle> Proceedings of the Second ICGI-94, Lecture Notes in Artificial Intelligence 862, </booktitle> <pages> pages 203-211. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Therefore, a symbolic description of the learned finite state automaton can be extracted from the trained network using clustering techniques. We describe an approach based on partitioning the space of the state neurons (due to [34]) and a hybrid method combining symbolic and connectionist approaches (due to <ref> [1] </ref>) for extracting state information from trained RNN. <p> We describe an approach based on partitioning the space of the state neurons (due to [34]) and a hybrid method combining symbolic and connectionist approaches (due to [1]) for extracting state information from trained RNN. Giles et al's method [34] divides the output range <ref> [0; 1] </ref> of each recurrent state neuron into q (typically q = 2) equal sized partitions thereby mapping the outputs of the N state neurons to a set of q N states of the FSA. <p> The extracted DFA is minimized (see [42] for a description of the state minimization procedure). Experimental results show that the extracted DFA approximates the target DFA fairly closely. Alquezar and Sanfeliu proposed a method based on hierarchical clustering to extract a FSA from the trained RNN <ref> [1] </ref>. Their algorithm constructs a prefix tree automaton (PTA) from the set of positive training examples (see section 3.3). Simultaneously a set of single-point clusters is initialized to the set of activation vectors of the network's state neurons in response to each symbol of each training string.
Reference: [2] <author> P. J. Angeline, G. M. Saunders, and J. B. Pollack. </author> <title> An evolutionary algorithm that constructs recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5 </volume> <pages> 54-65, </pages> <year> 1994. </year>
Reference-contexts: An inappropriate choice of the network architecture can potentially prevent the network from efficiently learning the target grammar. Angeline et al proposed an evolutionary network induction algorithm for simultaneously acquiring both the network topology and the weight values <ref> [2] </ref>. Their algorithm called GNARL (GeNeralized Acquisition of Recurrent Links) uses evolutionary programming techniques which reply on mutation as the sole genetic reproduction operator (as against genetic algorithms which tend to rely primarily on the crossover operator).
Reference: [3] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: adequate teacher is capable of answering membership queries of the form "Does this sentence belong to the target language?" and equivalence queries of the form "Is this DFA equivalent to the target?" Using labeled examples together with membership and equivalence queries it is possible to correctly identify the target DFA <ref> [3] </ref>. 6 3.3 Search Space Regular grammar inference can be formulated as a search problem in the space of all finite state automata (FSA). Clearly, the space of all FSA is infinite. <p> the algorithm returns the partition ff0g; f1; 4g; f2g; f3; 5gg which is exactly the target DFA we are trying to learn (Fig. 3). 3.5 The L fl Algorithm The L fl algorithm infers a minimum state DFA corresponding to the target with the help of a minimally adequate teacher <ref> [3] </ref>. Unlike the approaches described so far, the L fl algorithm does not search a lattice of FSA. Instead, it constructs a hypothesis DFA by posing membership queries to the teacher. <p> Angluin's L fl learning algorithm can be adapted to the PAC learning framework to show that DFA can be efficiently PAC learned given that the learner is allowed to pose a polynomial number of membership queries <ref> [3] </ref>. However, PAC learning of DFA is proven to be a hard problem in that there exists no polynomial time algorithm that efficiently PAC learns DFA from labeled examples alone [44, 77]. The PAC model's requirement of learnability under all conceivable probability distributions is often considered too stringent in practice.
Reference: [4] <author> D. Bailey. </author> <title> When Push Comes to Shove: A Computational Model of the Role of Motor Control in the Acquisition of Action Verbs. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1997. </year>
Reference-contexts: Bailey proposed a computational model to address the issue of how a child makes use of the concepts that he/she has learned <ref> [5, 4] </ref>. His system learns to produce verb labels for actions and also carries out the actions specified by the verbs it has learned. 33
Reference: [5] <author> D. Bailey, Feldman J., S. Narayanan, and G. </author> <title> Lakoff. Modeling embodied lexical development. </title> <booktitle> In Proceedings of the 19 th Cognitive Science Society Conference, </booktitle> <publisher> pages ??-?? Stanford University Press, </publisher> <year> 1997. </year>
Reference-contexts: Bailey proposed a computational model to address the issue of how a child makes use of the concepts that he/she has learned <ref> [5, 4] </ref>. His system learns to produce verb labels for actions and also carries out the actions specified by the verbs it has learned. 33
Reference: [6] <author> G. Ball, D. Ling, D. Kurlander, J. Miller, D. Pugh, T. Skelly, A. Stankosky, D. Thiel, M. Van Dantzich, and T. Wax. </author> <title> Lifelike computer characters: The persona project at microsoft research. </title> <editor> In J. Bradshaw, editor, </editor> <booktitle> Software Agents. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: Besides obtaining a better understanding of natural language acquisition, interest in studying formal models of language learning stems also from the numerous practical applications of language learning by machines. Research in instructible robots [19] and intelligent software agents and conversational interfaces <ref> [6] </ref> is geared towards the design of agents that can understand and execute verbal instructions given in a natural language (such as English) or some restricted subset of a natural language.
Reference: [7] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41(1) </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: Perhaps the most critical problem with learning HMMs is the adjustment of the model parameters that would maximize the probability of a single observation sequence. In order to determine the model parameters an iterative procedure called the Baum-Welch algorithm can be used <ref> [7] </ref>. Stolcke and Omohundro present a more general approach to the HMM learning problem [88]. Their approach is a Bayesian model merging strategy which facilitates learning the HMM structure as well as the model parameters from a given set of positive examples.
Reference: [8] <author> A. Brazma, I. Jonassen, I. Eidhammer, and D. Gilbert. </author> <title> Approaches to automatic discovery of patterns in biosequences. </title> <journal> Journal of Computational Biology, </journal> <volume> 5(2) </volume> <pages> 277-303, </pages> <year> 1998. </year>
Reference-contexts: Linguistic pattern recognition has been put to several practical uses including speech recognition, discovery of patterns in biosequences, image segmentation, interpretation of ECG, handwriting recognition, recognition of seismic signals and the like <ref> [8, 30, 57] </ref>. The issues and practical difficulties associated with formal language learning models can provide useful insights for the development of language understanding systems.
Reference: [9] <author> M. R. Brent. </author> <title> Advances in the computational study of language acquisition. </title> <journal> Cognition, </journal> <volume> 61 </volume> <pages> 1-38, </pages> <year> 1996. </year>
Reference-contexts: Consequently, natural language acquisition has been, and continues to be, a major focus of research [48, 75]. The past decade has seen significant theoretical as well as experimental advances in the study of natural language acquisition. Examples of some current developments include bootstrapping hypotheses and constraint-based theories <ref> [9] </ref>, optimality theory [80, 89], and neural theory of language [27, 28]. Empirical evidence from these results argues in favor of language learnability to address some of the key problems encountered in child language acquisition. <p> Research in language acquisition has benefited from advances in several disciplines including cognitive psychology <ref> [9] </ref>, linguistics [15, 16, 75], theoretical computer science [42, 55], computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57].
Reference: [10] <author> E. Brill and R. Mooney. </author> <title> An overview of empirical natural language processing. </title> <journal> AI Magazine, </journal> <volume> 18(4, Winter </volume> 1997):13-24, 1997. 
Reference-contexts: The learning of semantics or the meanings of words and sentences is 32 central natural language acquisition. An excellent overview of empirical methods for nat-ural language processing (including techniques for speech recognition, syntactic parsing, semantic processing, information extraction, and machine translation) appears in <ref> [10] </ref>. Thompson et al have designed a system for automatically acquiring a semantic lexicon from a corpus of sentences paired with representations of their meanings [90].
Reference: [11] <author> D. Carmel and S. Markovitch. </author> <title> Learning models of intelligent agents. </title> <booktitle> In Proceedings of the AAAI-96 (vol. </booktitle> <volume> 1), </volume> <pages> pages 62-67. </pages> <publisher> AAAI Press/MIT Press, </publisher> <year> 1996. </year> <month> 34 </month>
Reference-contexts: In such scenarios, an online or incremental model of learning that is guaranteed to eventually converge to the target DFA in the limit is of interest. Particularly, in the case of intelligent autonomous agents, incremental learning offers an attractive framework for characterizing the behavior of the agents <ref> [11] </ref>. 16 Parekh et al have proposed an efficient incremental algorithm for learning regular grammars using membership queries [72]. Their method extends Angluin's ID algorithm to an incremental framework. The learning algorithm is intermittently provided with labeled examples and has access to a knowledgeable teacher capable of answering membership queries.
Reference: [12] <author> R. C. Carrasco and J. Oncina. </author> <title> Learning stochastic regular grammar by means of a state merging method. </title> <editor> In R. C. Carrasco and J. Oncina, editors, </editor> <booktitle> Proceedings of the Second ICGI-94, Lecture Notes in Artificial Intelligence 862, </booktitle> <pages> pages 139-152. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Fig. 10 shows the state transition diagram and associated probabilities of a stochastic finite state automaton. 4.1.1 The Alergia Algorithm for Learning SFA Carrasco and Oncina have developed an algorithm for the inference of deterministic stochastic finite state automata (DSFA) <ref> [12] </ref>. A DSFA is a SFA where for each state q i 2 Q and symbol a 2 there exists at most one state q j such that p ij (a) 6= 0.
Reference: [13] <author> N. Chomsky. </author> <title> Three models for the description of language. </title> <journal> PGIT, </journal> <volume> 2(3) </volume> <pages> 113-124, </pages> <year> 1956. </year>
Reference-contexts: The classes of grammars such as regular or context free grammars that belong to the Chomsky hierarchy of formal grammars <ref> [13, 42] </ref> are often used to model the target grammar. The methods for grammar inference typically identify an unknown grammar (or an approximation of it) from a set of candidate hypotheses (e.g., the set of regular grammars defined over some alphabet). <p> Formal languages were initially used to model natural languages in order to better understand the process of natural language learning <ref> [13] </ref>. Formal languages are associated with recognizing (or equivalently, generating) devices called automata. The task of grammar inference is often modeled as the task of automata induction wherein an appropriate automaton for the target language is learned from a set of labeled examples. <p> Chomsky proposed a hierarchy of formal language grammars based on the types of restrictions placed on the production rules <ref> [13, 42] </ref>. The simplest class of grammars in this hierarchy is the class of regular grammars which are recognized by finite state automata (see section 3.1).
Reference: [14] <author> N. Chomsky. Review of b. f. </author> <title> skinner verbal behavior. </title> <booktitle> Language, </booktitle> <volume> 35 </volume> <pages> 26-58, </pages> <year> 1959. </year>
Reference-contexts: Note that semantics plays a central role in this model of language learning. At the other end of the spectrum is the view advocated by Chomsky that language is not a cultural artifact that can be learned <ref> [14] </ref>. This claim is based on what Chomsky has called the argument from the poverty of stimulus which states that the language stimuli received by the child are insufficient for acquiring abstract grammatical rules solely via inductive learning [15]. Language is thus innate in the biological make-up of the brain.
Reference: [15] <author> N. Chomsky. </author> <title> Reflections on Language. Temple Smith, </title> <address> London, </address> <year> 1976. </year>
Reference-contexts: Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics <ref> [15, 16, 75] </ref>, theoretical computer science [42, 55], computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57]. <p> This claim is based on what Chomsky has called the argument from the poverty of stimulus which states that the language stimuli received by the child are insufficient for acquiring abstract grammatical rules solely via inductive learning <ref> [15] </ref>. Language is thus innate in the biological make-up of the brain. In other words, language is a human instinct [75]. Thus, children must be born with a Universal Grammar or a system of principles, conditions, and rules that are elements or properties of all human languages [15]. <p> via inductive learning <ref> [15] </ref>. Language is thus innate in the biological make-up of the brain. In other words, language is a human instinct [75]. Thus, children must be born with a Universal Grammar or a system of principles, conditions, and rules that are elements or properties of all human languages [15]. Chomsky's subsequent writings [16] argue that the innate universal grammar is organized into modular subsystems of principles concerning government, binding, and thematic roles. Principles or, alternatively, particular lexical items may have parameters associated with them whose values are set according to language experience [56].
Reference: [16] <author> N. Chomsky. </author> <title> Some Concepts and Consequences of the Theory of Government and Binding. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1982. </year>
Reference-contexts: Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics <ref> [15, 16, 75] </ref>, theoretical computer science [42, 55], computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57]. <p> In other words, language is a human instinct [75]. Thus, children must be born with a Universal Grammar or a system of principles, conditions, and rules that are elements or properties of all human languages [15]. Chomsky's subsequent writings <ref> [16] </ref> argue that the innate universal grammar is organized into modular subsystems of principles concerning government, binding, and thematic roles. Principles or, alternatively, particular lexical items may have parameters associated with them whose values are set according to language experience [56].
Reference: [17] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>).
Reference: [18] <author> D. Clouse, C. Giles, B. Horne, and G. Cottrell. </author> <title> Time-delay neural networks: Representation and induction of finite state machines. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8(5) </volume> <pages> 1065-1070, </pages> <year> 1997. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>).
Reference: [19] <author> C. Crangle and P. Suppes. </author> <title> Language and Learning for Robots. </title> <booktitle> CSLI Lecture Notes: </booktitle> <volume> No. 41. </volume> <publisher> CSLI Publications, Stanford, </publisher> <address> CA., </address> <year> 1994. </year>
Reference-contexts: Besides obtaining a better understanding of natural language acquisition, interest in studying formal models of language learning stems also from the numerous practical applications of language learning by machines. Research in instructible robots <ref> [19] </ref> and intelligent software agents and conversational interfaces [6] is geared towards the design of agents that can understand and execute verbal instructions given in a natural language (such as English) or some restricted subset of a natural language. <p> Further, the human may want to summarize this entire sequence of actions by the phrase "Cook dinner". The robot should learn the above sequence of actions and identify it with the phrase "Cook dinner" <ref> [19] </ref>. A satisfactory solution to this problem will necessarily have to build on recent advances in a number of areas in artificial intelligence including natural language processing, machine learning, machine perception, robotics, planning, knowledge representation, and reasoning. <p> This limitation is overcome in the next class of grammars in the hierarchy, the context free grammars. Context free grammars are recognized by pushdown automata which are simply finite state automata augmented with a pushdown stack. Context free grammars are adequate for several practical natural language modeling tasks <ref> [19, 73] </ref>.
Reference: [20] <author> S. Das and R. Das. </author> <title> Induction of discrete-state machine by stabilizing a simple recurrent network using clustering. </title> <journal> Computer Science and Informatics, </journal> <volume> 21(2) </volume> <pages> 35-40, </pages> <year> 1991. </year>
Reference-contexts: Several researchers have studied methods for extracting finite state automata from a trained RNN <ref> [20, 33, 64, 97] </ref>. It is observed that RNN develop an internal state representation in the form of clusters in the activation space of the recurrent state neurons. Therefore, a symbolic description of the learned finite state automaton can be extracted from the trained network using clustering techniques.
Reference: [21] <author> S. Das, C. Giles, and G. Z. Sun. </author> <title> Learning context free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 791-795, </pages> <year> 1992. </year>
Reference-contexts: Das et al proposed an approach for learning deterministic context free grammars using recurrent neural network push down automata (NNPDA) <ref> [21, 22] </ref>. This model uses a recurrent neural network similar to the one described in section 3.6.1 in conjunction with an external stack to learn a proper subset of deterministic context free languages. <p> We briefly describe the NNPDA model proposed by Das et al <ref> [21] </ref>. The NNPDA consists of a recurrent neural network integrated with an external stack through a hybrid error function. It can be trained to simultaneously learn the state transition function of the underlying push down automaton and the actions that are required to control the stack operation.
Reference: [22] <author> S. Das, C. Giles, and G. Z. Sun. </author> <title> Using prior knowledge in a nnpda to learn context free languages. </title> <editor> In C. Giles, S. Hanson, and J. Cowan, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <pages> pages 65-72, </pages> <year> 1993. </year>
Reference-contexts: Das et al proposed an approach for learning deterministic context free grammars using recurrent neural network push down automata (NNPDA) <ref> [21, 22] </ref>. This model uses a recurrent neural network similar to the one described in section 3.6.1 in conjunction with an external stack to learn a proper subset of deterministic context free languages. <p> However, the learning task is computationally intensive. The algorithm does not converge for more nontrivial context free languages such as a n b n cb m a m . Das et al studied the incorporation of prior knowledge in NNPDA to make learning more tractable <ref> [22] </ref>. Two different types of knowledge could be made available to the model: knowledge that depends on the training data alone and partial knowledge about the automaton being inferred. As explained in section 3.8, presenting simpler examples of the target concept can considerably simplify the learning task. <p> Two such methods including pre-determining the initial values of some of the network's weights and presenting structured training examples where the order of generation of each word of the sentence is indicated by parentheses were shown to improve learning speed <ref> [22] </ref>. 5.2 Stochastic CFG The success of HMM in a variety of speech recognition tasks leads one to ask if the more powerful stochastic context free grammars (SCFG) could be employed for speech recognition.
Reference: [23] <author> P. Dupont. </author> <title> Incremental regular inference. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> pages 222-237, </pages> <address> Montpellier, France, 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: + jj and jjS jj denote the sums of lengths of each example in S + and S respectively then it can be shown that the time complexity of the RPNI algorithm is O ((jjS + jj + jjS jj) jjS + jj 2 The interested reader is referred to <ref> [23, 65] </ref> for a detailed description of the algorithm. Example We demonstrate a few steps in the execution of the RPNI algorithm on the task of learning the DFA of Fig. 3. <p> It needs only a finite working storage. However, it is based on an ordered presentation of examples and requires a consistency check with all the previous examples when the hypothesis is modified. An incremental version of the RPNI algorithm for regular grammar inference was proposed by Dupont <ref> [23] </ref>. It is guaranteed to converge to the target DFA when the set of examples seen by the learner includes a characteristic set for the target automaton as a subset. The algorithm runs in time that is polynomial in the sum of lengths of the observed examples.
Reference: [24] <author> P. Dupont, L. Miclet, and E. Vidal. </author> <booktitle> What is the search space of the regular inference? In Proceedings of the Second International Colloquium on Grammatical Inference (ICGI'94), </booktitle> <pages> pages 25-37, </pages> <address> Alicante, Spain, </address> <year> 1994. </year>
Reference-contexts: we guarantee that the target DFA lies within this restricted search space? It can be shown that if the set of positive examples 7 provided to the learner is a structurally complete set then the lattice constructed above is guaranteed to contain the minimum state DFA equivalent to the target <ref> [24, 67, 69] </ref>. <p> The offspring are added to the population and the evolution continues. Over a period of time the individuals tend to converge towards high fitness values (i.e., reasonably good solutions). Dupont has proposed a framework for regular grammar inference using genetic algorithms <ref> [24] </ref>. The learner is given a set S = S + [ S of labeled examples. A PTA is constructed from the set S + as explained in section 3.3.
Reference: [25] <author> J. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year> <month> 35 </month>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>).
Reference: [26] <author> J. Elman. </author> <title> Distributed representation, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 195-225, </pages> <year> 1991. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>).
Reference: [27] <author> J. Feldman, G. Lakoff, D. Bailey, S. Narayanan, T. Regier, and A. Stolcke. Lzero: </author> <title> The first five years. </title> <journal> Artificial Intelligence Review, </journal> <volume> 10 </volume> <pages> 103-129, </pages> <year> 1996. </year>
Reference-contexts: The past decade has seen significant theoretical as well as experimental advances in the study of natural language acquisition. Examples of some current developments include bootstrapping hypotheses and constraint-based theories [9], optimality theory [80, 89], and neural theory of language <ref> [27, 28] </ref>. Empirical evidence from these results argues in favor of language learnability to address some of the key problems encountered in child language acquisition. <p> This argues for a model of language learning that accounts for both syntactic as well as semantic components of the language. Recent research results offer some intriguing preliminary insights on the development of formal models of how children learn language <ref> [27, 28] </ref>. Regier proposed a computational model of how some lexical items describing spatial relations might develop in different languages [82]. His system includes a simple model of the visual system which is common to all human beings and thus must be the source from which all visual concepts arise.
Reference: [28] <author> J. A. Feldman. </author> <title> Real language learning. </title> <booktitle> In Proceedings of the Fourth International Colloquium on Grammatical Inference (ICGI'98), </booktitle> <address> Ames, IA, </address> <year> 1998. </year> <note> (To appear). </note>
Reference-contexts: The past decade has seen significant theoretical as well as experimental advances in the study of natural language acquisition. Examples of some current developments include bootstrapping hypotheses and constraint-based theories [9], optimality theory [80, 89], and neural theory of language <ref> [27, 28] </ref>. Empirical evidence from these results argues in favor of language learnability to address some of the key problems encountered in child language acquisition. <p> This argues for a model of language learning that accounts for both syntactic as well as semantic components of the language. Recent research results offer some intriguing preliminary insights on the development of formal models of how children learn language <ref> [27, 28] </ref>. Regier proposed a computational model of how some lexical items describing spatial relations might develop in different languages [82]. His system includes a simple model of the visual system which is common to all human beings and thus must be the source from which all visual concepts arise.
Reference: [29] <author> K. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice Hall, </publisher> <address> N.J., </address> <year> 1982. </year>
Reference-contexts: A satisfactory solution to this problem will necessarily have to build on recent advances in a number of areas in artificial intelligence including natural language processing, machine learning, machine perception, robotics, planning, knowledge representation, and reasoning. Formal language models are extensively used in syntactic or linguistic pattern classification systems <ref> [29, 30] </ref>. The structural inter-relationships among the linguistic pattern attributes are easily captured by representing the patterns as strings (a collection of syntactic symbols). <p> Incremental approaches guarantee convergence in the limit i.e., when the set of examples seen by the learner satisfy certain properties. Other approaches that attempt to learn the target DFA only from the labeled examples often rely on heuristics and are not guaranteed to converge to the target <ref> [29] </ref>. 17 An interesting question would be to see whether DFA are approximately learnable. Valiant's distribution-independent model of learning also called the probably approximately correct model (PAC model) [93] is a widely used model for approximate learning.
Reference: [30] <author> K. S. Fu and T. L. Booth. </author> <title> Grammatical inference: Introduction and survey (part 1). </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 5 </volume> <pages> 85-111, </pages> <year> 1975. </year>
Reference-contexts: A satisfactory solution to this problem will necessarily have to build on recent advances in a number of areas in artificial intelligence including natural language processing, machine learning, machine perception, robotics, planning, knowledge representation, and reasoning. Formal language models are extensively used in syntactic or linguistic pattern classification systems <ref> [29, 30] </ref>. The structural inter-relationships among the linguistic pattern attributes are easily captured by representing the patterns as strings (a collection of syntactic symbols). <p> Linguistic pattern recognition has been put to several practical uses including speech recognition, discovery of patterns in biosequences, image segmentation, interpretation of ECG, handwriting recognition, recognition of seismic signals and the like <ref> [8, 30, 57] </ref>. The issues and practical difficulties associated with formal language learning models can provide useful insights for the development of language understanding systems. <p> Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics [15, 16, 75], theoretical computer science [42, 55], computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition <ref> [30, 57] </ref>. <p> Regular grammars represent the simplest class of grammars in the Chomsky hierarchy. The study of regular grammar inference methods is of significant practical interest for a number of reasons: every finite language is regular; a context-free language can often 29 be closely approximated by a regular grammar <ref> [30] </ref>. Unfortunately, the regular grammar inference problem is known to be hard (i.e., there exists no efficient algorithm that can learn the target grammar from an arbitrary set of labeled examples).
Reference: [31] <author> C. Giles, D. Chen, H. Miller, and G. Sun. </author> <title> Second-order recurrent neural networks for grammatical inference. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 2 </volume> <pages> 273-281, </pages> <year> 1991. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>). <p> In the following section we describe the second order recurrent neural network architecture due to Giles et al <ref> [31] </ref>. 3.6.1 Second Order RNN for Regular Grammar Inference Recurrent neural networks have feedback connections from the output neurons back to the input that give them the ability to process temporal sequences of arbitrary length. The second order RNN is depicted in Fig. 9.
Reference: [32] <author> C. Giles, B. Horne, and T. Lin. </author> <title> Learning a class of large finite state machines with a recurrent neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(5) </volume> <pages> 1359-1365, </pages> <year> 1995. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>).
Reference: [33] <author> C. Giles and C. Omlin. </author> <title> Extraction, insertion and refinement of symbolic rules in dynamically-driven recurrent neural networks. Connection Science special issue on Architectures for Integrating Symbolic and Neural Processes, </title> <address> 5(3,4):307-337, </address> <year> 1993. </year>
Reference-contexts: Several researchers have studied methods for extracting finite state automata from a trained RNN <ref> [20, 33, 64, 97] </ref>. It is observed that RNN develop an internal state representation in the form of clusters in the activation space of the recurrent state neurons. Therefore, a symbolic description of the learned finite state automaton can be extracted from the trained network using clustering techniques.
Reference: [34] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: A variety of RNN architectures have been investigated for learning grammars from a set of positive and negative examples of the target language (see for example <ref> [17, 18, 25, 26, 31, 34, 32, 58, 78, 85, 97] </ref>). <p> ijk = @W ijk (f) @S 0 14 The results of experiments on the Tomita benchmark for regular grammars [91] showed that second order neural networks converged fairly quickly to compact representations of the target finite state automata and generalized reasonably well on strings not belonging to the target language <ref> [34] </ref>. 3.6.2 Extracting Finite State Automata from Trained RNN One significant disadvantage of connectionist methods is that the learned models are not as transparent as is the case with symbolic approaches. Several researchers have studied methods for extracting finite state automata from a trained RNN [20, 33, 64, 97]. <p> Therefore, a symbolic description of the learned finite state automaton can be extracted from the trained network using clustering techniques. We describe an approach based on partitioning the space of the state neurons (due to <ref> [34] </ref>) and a hybrid method combining symbolic and connectionist approaches (due to [1]) for extracting state information from trained RNN. Giles et al's method [34] divides the output range [0; 1] of each recurrent state neuron into q (typically q = 2) equal sized partitions thereby mapping the outputs of the <p> We describe an approach based on partitioning the space of the state neurons (due to <ref> [34] </ref>) and a hybrid method combining symbolic and connectionist approaches (due to [1]) for extracting state information from trained RNN. Giles et al's method [34] divides the output range [0; 1] of each recurrent state neuron into q (typically q = 2) equal sized partitions thereby mapping the outputs of the N state neurons to a set of q N states of the FSA.
Reference: [35] <author> J. Giordano. </author> <title> Inference of context-free grammars by enumeration: Structural containment as an ordering bias. </title> <editor> In R. C. Carrasco and J. Oncina, editors, </editor> <booktitle> Proceedings of the Second ICGI-94, Lecture Notes in Artificial Intelligence 862, </booktitle> <pages> pages 212-221. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Giordano studied the version space approach to learning context free grammars using a notion of structural containment to define a partial order on the search space of CFG <ref> [35] </ref>. This approach provides another mechanism to circumvent the problem that arises because the operation more general than is not decidable in CFG. A structure generated by a grammar is its derivation tree with all non-terminal labels removed.
Reference: [36] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10(5) </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: say that A is consistent with the sample S = S + [ S if it accepts all positive examples and rejects all negative examples. 3.2 Results in Regular Grammar Inference Regular grammar inference is a hard problem in that regular grammars cannot be correctly identified from positive examples alone <ref> [36] </ref>. Further, it has been shown that there exists no efficient learning algorithm for identifying the minimum state DFA that is consistent with an arbitrary set of positive and negative examples [37]. Efficient algorithms for identification of DFA assume that additional information is provided to the learner.
Reference: [37] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37(3) </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: Further, it has been shown that there exists no efficient learning algorithm for identifying the minimum state DFA that is consistent with an arbitrary set of positive and negative examples <ref> [37] </ref>. Efficient algorithms for identification of DFA assume that additional information is provided to the learner. This information is typically in the form of a set of examples S that satisfies certain properties or a knowledgeable teacher's responses to queries posed by the learner. <p> This work naturally extends the teachability results due to Goldman and Mathias [38] and the equivalent model of learning grammars from characteristic samples due to Gold <ref> [37] </ref> to a probabilistic framework (see [68] for details). This idea of learning from simple examples holds tremendous potential in difficult learning scenarios. <p> Gold proved that regular grammars cannot be learned from positive examples alone <ref> [37] </ref>.
Reference: [38] <author> S. Goldman and H. Mathias. </author> <title> Teaching a smarter learner. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory (COLT'93), </booktitle> <pages> pages 67-76. </pages> <editor> A. C. M. </editor> <publisher> Press, </publisher> <year> 1993. </year> <month> 36 </month>
Reference-contexts: Thus, by using the RPNI algorithm in conjunction with a randomly drawn set of simple examples, it is showed that DFA are efficiently PAC learnable. This work naturally extends the teachability results due to Goldman and Mathias <ref> [38] </ref> and the equivalent model of learning grammars from characteristic samples due to Gold [37] to a probabilistic framework (see [68] for details). This idea of learning from simple examples holds tremendous potential in difficult learning scenarios.
Reference: [39] <author> Colin de la Higuera, J. Oncina, and E. Vidal. </author> <title> Identification of dfa: Data-dependent vs data-independent algorithms. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> pages 313-326, </pages> <address> Montpellier, France, 1996. </address> <publisher> Springer. </publisher>
Reference-contexts: Further, as demonstrated by the results of the recent Abbadingo One DFA Learning Competition several efficient heuristic approaches provide satisfactory solutions to the regular grammar inference problem <ref> [39, 43, 46] </ref>. Context-free and context-sensitive grammars generate languages that are clearly more expressive than those generated by regular grammars. This has motivated a large body of research on algorithms for learning such grammars. However, existing algorithms for induction of these grammars are generally heuristic in nature.
Reference: [40] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <institution> University of Michigan Press, Ann Arbor, Michigan, </institution> <year> 1975. </year>
Reference-contexts: Some query strings have the potential for eliminating huge chunks of the search space. The worst-case time complexity of this method is exponential in the size of the PTA. 3.4.2 Randomized Search Genetic algorithms offer an attractive framework for randomized search in large hypotheses spaces <ref> [40] </ref>. A typical genetic search involves evolving a randomly generated set of individuals (from the hypothesis space) based on the survival of the fittest principle of Darwinian evolution. A population of randomly selected elements of the hypothesis space is evolved over a period of time.
Reference: [41] <author> V. Honavar. </author> <title> Toward learning systems that integrate multiple strategies and representations. </title> <editor> In V. Honavar and L. Uhr, editors, </editor> <booktitle> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, </booktitle> <pages> pages 615-644. </pages> <publisher> Academic Press: </publisher> <address> New York., </address> <year> 1994. </year>
Reference-contexts: Against this background, it is of interest to investigate hybrid algorithms that exploit the strengths of both numeric as well as symbolic representations <ref> [41] </ref>. Research in cognitive psychology, linguistics, and related areas has raised several important issues that must be (at least partially) addressed by any practical system for language acquisition.
Reference: [42] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1979. </year>
Reference-contexts: Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics [15, 16, 75], theoretical computer science <ref> [42, 55] </ref>, computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57]. <p> The classes of grammars such as regular or context free grammars that belong to the Chomsky hierarchy of formal grammars <ref> [13, 42] </ref> are often used to model the target grammar. The methods for grammar inference typically identify an unknown grammar (or an approximation of it) from a set of candidate hypotheses (e.g., the set of regular grammars defined over some alphabet). <p> Chomsky proposed a hierarchy of formal language grammars based on the types of restrictions placed on the production rules <ref> [13, 42] </ref>. The simplest class of grammars in this hierarchy is the class of regular grammars which are recognized by finite state automata (see section 3.1). <p> grammar; there exist efficient (polynomial time) algorithms for several operations (such as minimization of a DFA, determining the equivalence of two DFA, determining whether the language of one DFA is a superset of the language of another DFA and the like) that are frequently used in regular grammar inference methods <ref> [42] </ref>. 3.1 Finite State Automata A deterministic finite state automaton (DFA) is a quintuple A = (Q; ffi; ; q 0 ; F ) where, Q is a finite set of states, is the finite alphabet, q 0 2 Q is the start state, F Q is the set of accepting <p> The partitions in response to the various input strings 1, 00, 01, : : : are recorded and the learned FSA is reconstructed. Clearly, the number of states of the learned FSA can be no more than q N . The extracted DFA is minimized (see <ref> [42] </ref> for a description of the state minimization procedure). Experimental results show that the extracted DFA approximates the target DFA fairly closely. Alquezar and Sanfeliu proposed a method based on hierarchical clustering to extract a FSA from the trained RNN [1]. <p> Similarly, there exists no algorithm that can answer the question "Is L (G 1 ) " L (G 2 ) = ?" <ref> [42] </ref>. Thus, exact learning of the target grammar is seldom attempted in the case of CFG. However, several practically successful approaches have been devised for heuristically learning approximations to the target grammar. 24 5.1.1 Learning Recursive Transitions Networks Recursive Transition Networks are an equivalent representation of CFG.
Reference: [43] <author> H. Juille and J Pollack. Sage: </author> <title> A sampling-based heuristic for tree search. </title> <booktitle> Machine Learning, </booktitle> <year> 1998. </year> <note> (submitted). </note>
Reference-contexts: Further, as demonstrated by the results of the recent Abbadingo One DFA Learning Competition several efficient heuristic approaches provide satisfactory solutions to the regular grammar inference problem <ref> [39, 43, 46] </ref>. Context-free and context-sensitive grammars generate languages that are clearly more expressive than those generated by regular grammars. This has motivated a large body of research on algorithms for learning such grammars. However, existing algorithms for induction of these grammars are generally heuristic in nature.
Reference: [44] <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the 21 st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <address> New York, 1989. </address> <publisher> ACM. </publisher>
Reference-contexts: However, PAC learning of DFA is proven to be a hard problem in that there exists no polynomial time algorithm that efficiently PAC learns DFA from labeled examples alone <ref> [44, 77] </ref>. The PAC model's requirement of learnability under all conceivable probability distributions is often considered too stringent in practice. Pitt identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? [76].
Reference: [45] <author> M. Kearns and U. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics [15, 16, 75], theoretical computer science [42, 55], computational learning theory <ref> [45, 63] </ref>, artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57].
Reference: [46] <author> K. Lang, B. Pearlmutter, and R. Price. </author> <title> Results of the abbadingo one dfa learning competition and a new evidence-driven state merging algorithm. </title> <booktitle> In Proceedings of the Fourth International Colloquium on Grammatical Inference (ICGI'98), </booktitle> <address> Ames, IA, </address> <year> 1998. </year> <note> (To appear). </note>
Reference-contexts: Further, as demonstrated by the results of the recent Abbadingo One DFA Learning Competition several efficient heuristic approaches provide satisfactory solutions to the regular grammar inference problem <ref> [39, 43, 46] </ref>. Context-free and context-sensitive grammars generate languages that are clearly more expressive than those generated by regular grammars. This has motivated a large body of research on algorithms for learning such grammars. However, existing algorithms for induction of these grammars are generally heuristic in nature.
Reference: [47] <author> P. Langley. </author> <title> Simplicity and representation change in grammar induction. </title> <institution> Robotics Laboratory, Stanford Laboratory. </institution> <type> (Unpublished manuscript), </type> <year> 1994. </year>
Reference-contexts: The arcs of the subnetworks to the right of the figure specify individual words from the set of terminals. One approach to learning recursive transition networks based on Langley's GRIDS algorithm <ref> [47] </ref> is similar to the state merging techniques used for learning DFA. Given a set of positive examples a degenerate one level transition network is constructed that accepts exactly the sentences that belong to the training set.
Reference: [48] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kauffman, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Consequently, natural language acquisition has been, and continues to be, a major focus of research <ref> [48, 75] </ref>. The past decade has seen significant theoretical as well as experimental advances in the study of natural language acquisition. Examples of some current developments include bootstrapping hypotheses and constraint-based theories [9], optimality theory [80, 89], and neural theory of language [27, 28]. <p> Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics [15, 16, 75], theoretical computer science [42, 55], computational learning theory [45, 63], artificial intelligence [84], machine learning <ref> [48, 61] </ref>, and pattern recognition [30, 57]. <p> They are essentially simple finite state diagrams where the tests on the arcs refer to either terminal symbols or to entire subnetworks <ref> [48] </ref>. For example, the recursive transition diagram corresponding to the context free grammar of Fig. 1 is shown in Fig. 15. The first network indicates the start symbol S and invokes the subnetworks N P and V P by specifying them as tests on the arcs.
Reference: [49] <author> M. Lankhorst. </author> <title> A genetic algorithm for induction of nondeterministic pushdown automata. </title> <type> Technical Report CS-R 9502, </type> <institution> University of Groningen, </institution> <address> The Netherlands, </address> <year> 1995. </year>
Reference-contexts: set of context free grammars hence applying the version space strategy for learning uniquely invertible grammars does not restrict the capacity of the inference system in any way. 5.1.3 Learning NPDA using Genetic Search Lankhorst presented a scheme for learning non-deterministic pushdown automata (NPDA) from labeled examples using genetic search <ref> [49] </ref>. Push down automata (PDA) are recognizing devices for the class of context free grammars (just as FSA are recognizing devices for regular grammars). A PDA comprises of a FSA and a stack.
Reference: [50] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: The advantages of SCFG lie in their ability to capture the embedded structure within the speech data and their superior predictive power in comparison with regular grammars as measured by prediction entropy <ref> [50] </ref>. The Inside-Outside algorithm can be used to estimate the free parameters of a SCFG. Given a set of positive training sentences and a SCFG whose parameters are randomly initialized, the inside-outside algorithm first computes the most probable parse tree for each training sentence.
Reference: [51] <author> S. Lawrence, S. Fong, and C. Giles. </author> <title> Natural language grammatical inference: A comparison of recurrent neural networks and machine learning methods. </title> <editor> In S. Wermter, E. Riloff, and G. Scheler, editors, </editor> <title> Connectionist, Statistical, and symbolic Approaches 37 to Learning for Natural Language Processing, </title> <booktitle> Lecture Notes in Artificial Intelligence 1040, </booktitle> <pages> pages 33-47. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: The advantages of connectionist methods over the symbolic techniques described above include robust behavior in the presence of limited amount of noise, ability to learn from relatively smaller training sets, and scalability to larger problem sizes <ref> [51, 52] </ref>. Most connectionist language learning approaches specify the learning task in classical terms (for example using an automaton to parse grammatically correct sentences of the language) and then implement these using a suitable neural network architecture such as recurrent neural networks (RNN) [62]. <p> Algorithms that employ purely numeric representations are robust in the presence of limited amounts of noise, are able to learn from relatively smaller training sets, and are known to scale well to larger problem sizes <ref> [51, 52] </ref>. However, their convergence properties are generally poorly understood and the learned models are not as transparent. While it is possible to extract a symbolic description of a learned grammar from a connectionist network, the procedures used for doing so are largely heuristic in nature.
Reference: [52] <author> S. Lawrence, C. Giles, and S. Fong. </author> <title> Natural language grammatical inference with recurrent neural networks. </title> <journal> In IEEE Transactions on knowledge and Data Engineering. </journal> <note> IEEE Press, 1998. (accepted). </note>
Reference-contexts: The advantages of connectionist methods over the symbolic techniques described above include robust behavior in the presence of limited amount of noise, ability to learn from relatively smaller training sets, and scalability to larger problem sizes <ref> [51, 52] </ref>. Most connectionist language learning approaches specify the learning task in classical terms (for example using an automaton to parse grammatically correct sentences of the language) and then implement these using a suitable neural network architecture such as recurrent neural networks (RNN) [62]. <p> Algorithms that employ purely numeric representations are robust in the presence of limited amounts of noise, are able to learn from relatively smaller training sets, and are known to scale well to larger problem sizes <ref> [51, 52] </ref>. However, their convergence properties are generally poorly understood and the learned models are not as transparent. While it is possible to extract a symbolic description of a learned grammar from a connectionist network, the procedures used for doing so are largely heuristic in nature.
Reference: [53] <author> M. Li and P. Vitanyi. </author> <title> Learning simple concepts under simple distributions. </title> <journal> SIAM Journal of Computing, </journal> <volume> 20 </volume> <pages> 911-935, </pages> <year> 1991. </year>
Reference-contexts: of PAC learning under the universal distribution (called the simple PAC model) and demonstrated that several concept classes such as simple k-reversible DFA and log n-term DNF (i.e., boolean formulas in disjunctive normal form) are PAC learnable under this model while their learn-ability under the standard PAC model is unknown <ref> [53] </ref>. Recently, Denis et al proposed a variant of the simple PAC learning model where a teacher might intelligently select simple examples based on his/her knowledge of the target.
Reference: [54] <author> M. Li and P. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications, 2 nd edition. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: Thus, the Kolmogorov complexity of an object with respect to the Universal Turing machine is treated as the Kolmogorov complexity of the object. The interested reader is referred to <ref> [54] </ref> for a complete treatment of Kolmogorov complexity and related topics. The Solomonoff-Levin universal distribution m assigns high probability to objects that are simple i.e., m (x) 2 K (x) .
Reference: [55] <author> J. C. Martin. </author> <title> Introduction to Languages and The Theory of Computation. </title> <publisher> McGraw-Hill Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Research in language acquisition has benefited from advances in several disciplines including cognitive psychology [9], linguistics [15, 16, 75], theoretical computer science <ref> [42, 55] </ref>, computational learning theory [45, 63], artificial intelligence [84], machine learning [48, 61], and pattern recognition [30, 57].
Reference: [56] <author> J. M. Meisel. </author> <title> Parameters in acquisition. </title> <editor> In P. Fletcher and B. MacWhinney, </editor> <publisher> editors, </publisher>
Reference-contexts: Chomsky's subsequent writings [16] argue that the innate universal grammar is organized into modular subsystems of principles concerning government, binding, and thematic roles. Principles or, alternatively, particular lexical items may have parameters associated with them whose values are set according to language experience <ref> [56] </ref>. It is perhaps worth noting that Chomsky's arguments of innateness of language appear to be restricted to language syntax, while the behaviorist view of language acquisition 31 is primarily concerned with language semantics.
References-found: 56

