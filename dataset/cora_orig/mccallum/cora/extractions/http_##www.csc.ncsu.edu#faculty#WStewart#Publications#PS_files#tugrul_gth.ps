URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/PS_files/tugrul_gth.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/WStewart/Publications/Publications.html
Root-URL: http://www.csc.ncsu.edu
Title: ON THE EFFECTS OF USING THE GRASSMANN-TAKSAR-HEYMAN METHOD IN ITERATIVE AGGREGATION-DISAGGREGATION  
Author: TU GRUL DAYAR AND WILLIAM J. STEWART 
Keyword: Key words. Markov chains, decomposability, stationary probability, aggregation-disaggregation, Gaussian elimination, sparsity schemes  
Note: AMS subject classifications. 60J10, 60J27, 65F05, 65F10, 60-04  
Abstract: Iterative aggregation-disaggregation (IAD) is an effective method for solving finite nearly completely decomposable (NCD) Markov chains. Small perturbations in the transition probabilities of these chains may lead to considerable changes in the stationary probabilities; NCD Markov chains are known to be ill-conditioned. During an IAD step, this undesirable condition is inherited by the coupling matrix and one confronts the problem of finding the stationary probabilities of a stochastic matrix whose diagonal elements are close to 1. In this paper, the effects of using the Grassmann-Taksar-Heyman (GTH) method to solve the coupling matrix formed in the aggregation step are investigated. Then, the idea is extended in such a way that the same direct method can be incorporated into the disaggregation step. Finally, the effects of using the GTH method in the IAD algorithm on various examples are demonstrated, and the conditions under which it should be employed are explained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Berman and R. J. Plemmons, </author> <title> Nonnegative Matrices in the Mathematical Sciences, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: But most importantly, as it is explained later in x4, a row-wise sparse storage implementation will spend extra time in the substitution phase of the non-transposed system of equations. (I C (k) ) T is a singular M-matrix (see <ref> [1] </ref>) with 0 column sums, and the unique null vector of unit 1-norm is sought. For such a matrix, Gaussian elimination (GE) preserves column diagonal dominance throughout its computation so that the multiplier element at each step is bounded by 1 thereby eliminating the need for pivoting.
Reference: [2] <author> W. L. Cao and W. J. Stewart, </author> <title> Iterative aggregation/disaggregation techniques for nearly uncoupled Markov chains, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 32 (1985), </volume> <pages> pp. 702-719. 17 </pages>
Reference-contexts: Convergence analysis of the algorithm appears in [17]. Additional discussion may be found in [21] and <ref> [2] </ref>. The Generic Iterative Aggregation-Disaggregation (IAD) Algorithm: 1. Let (0) = ( 1 ; 2 ; : : :; N ) be a given initial approximation of the solution. Set k = 1. 2. Construct the coupling matrix C (k1) c ij = (k1) k i k 1 3.
Reference: [3] <author> P.-J. Courtois, </author> <title> Decomposability: Queueing and Computer System Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: If the matrix is not already in the form (1:1), then symmetric permutations are used to put it into the form in which the diagonal blocks form the strongly connected aggregates. 5.1 Test Problem 1 The first problem that is considered is the 8 fi 8 Courtois matrix <ref> [3] </ref> with all row sums equal to 1.
Reference: [4] <author> W. K. Grassmann, M. I. Taksar and D. P. Heyman, </author> <title> Regenerative analysis and steady state distributions for Markov chains, </title> <journal> Oper. Res., </journal> <volume> 33 (1985), </volume> <pages> pp. 1107-1116. </pages>
Reference-contexts: In this case, all diagonal elements in (I C (k) ) T will be zero, as in the example, and GE will fail in the first step. The advocated approach to finding a remedy for this situation is the GTH algorithm described in <ref> [4] </ref> and the direct method discussed in [16]. The original GTH algorithm emerges from probabilistic arguments, and it is shown that the stationary distribution of a Markov chain can be calculated using only nonnegative numbers and avoiding subtraction operations.
Reference: [5] <author> W. J. Harrod and R. J. Plemmons, </author> <title> Comparison of some direct methods for computing the stationary distributions of Markov chains, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 5 (1984), </volume> <pages> pp. 453-469. </pages>
Reference-contexts: Let P = diag (P 11 ; P 22 ; : : :; P NN ) + E: The quantity kEk 1 is referred to as the degree of coupling and it is taken to be a measure of the decomposability of the matrix (see <ref> [5] </ref>). If it were zero, then P would be reducible. NCD Markov chains that appear in applications are quite large and sparse, possibly having more than thousands of states. <p> The original GTH algorithm emerges from probabilistic arguments, and it is shown that the stationary distribution of a Markov chain can be calculated using only nonnegative numbers and avoiding subtraction operations. This algorithm achieves significantly greater accuracy than other algorithms described in the literature [11], <ref> [5] </ref> since there is no loss of significant digits due to cancellation [6]. Interestingly, the inspiration for the algorithm presented in [16], which is specifically for the solution of NCD Markov chains, is the GTH algorithm.
Reference: [6] <author> D. P. Heyman, </author> <title> Further comparisons of direct methods for computing stationary distributions of Markov chains, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 226-232. </pages>
Reference-contexts: This algorithm achieves significantly greater accuracy than other algorithms described in the literature [11], [5] since there is no loss of significant digits due to cancellation <ref> [6] </ref>. Interestingly, the inspiration for the algorithm presented in [16], which is specifically for the solution of NCD Markov chains, is the GTH algorithm.
Reference: [7] <author> J. R. Koury, D. F. McAllister and W. J. Stewart, </author> <title> Iterative methods for computing stationary distributions of nearly completely decomposable Markov chains, </title> <journal> SIAM J. Alg. Discrete Meth., </journal> <volume> 5 (1984), </volume> <pages> pp. 164-186. </pages>
Reference-contexts: Moreover, when the system is nearly completely decomposable, there are eigenvalues close to 1, and the poor separation of the unit eigenvalue implies a slow rate of convergence for standard matrix iterative methods [12]. IAD methods do not suffer from these limitations <ref> [7] </ref>, [23]. The idea in IAD methods is to observe the system in isolation in each of the diagonal blocks as if the system is completely decomposable (see [15]) and to compute the stationary probability distribution of each diagonal block. However, there are two problems with this approach.
Reference: [8] <author> C. D. Meyer, </author> <title> Stochastic complementation, uncoupling Markov chains, and the theory of nearly reducible systems, </title> <journal> SIAM Rev., </journal> <volume> 31 (1989), </volume> <pages> pp. </pages> <month> 240-272. </month> <title> [9] |||-, Sensitivity of the stationary distribution of a Markov chain, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 (1994), </volume> <pages> pp. 715-728. </pages>
Reference-contexts: Only if these two problems are overcome can one form the stationary distribution of the Markov chain by weighing the subvectors and pasting them together. For the transition probability matrix P , the stochastic complement of P ii <ref> [8] </ref>, is S ii = P ii + P ifl (I P i ) 1 P fli ; where P ifl : n i fi (n n i ) matrix composed of the i th row of blocks of P with P ii removed, P fli : (n n i ) <p> The conditional stationary probability vector of the i th block is i =k i k 1 and it may be computed by solving ( i =k i k 1 )S ii = i =k i k 1 (see <ref> [8] </ref> for details). However, each stochastic complement has an embedded matrix inversion which may require excessive computation. An alternative solution technique is to approximate S ii by accumulating the off-diagonal mass P ifl into the diagonal block P ii on a row by row basis.
Reference: [10] <author> C. A. O'Cinneide, </author> <title> Entrywise perturbation theory and error analysis for Markov chains, </title> <journal> Numer. Math., </journal> <volume> 65 (1993), </volume> <pages> pp. 109-120. </pages>
Reference-contexts: Interestingly, the inspiration for the algorithm presented in [16], which is specifically for the solution of NCD Markov chains, is the GTH algorithm. In a recent paper <ref> [10] </ref>, it is shown that the stationary probability vector of an N fi N irreducible Markov chain stored in floating-point form is close to its exact stationary vector. <p> However, as in the second problem, there are cases where the GTH algorithm does not achieve its worst case bound due to statistical effects in rounding error accumulation although the matrices solved are considerably larger (see Tables 4, 8, 10, 12, and 14). Moreover, just as indicated in <ref> [10] </ref>, it is possible to improve the entrywise relative error in the GTH algorithm even further by forming the pivot element in higher precision (quadruple-precision for the given system parameters).
Reference: [11] <author> C. C. Paige, P. H. Styan and P. G. Wachter, </author> <title> Computation of the stationary distribution of a Markov chain, </title> <journal> J. Statist. Comput. Simulation, </journal> <volume> 4 (1975), </volume> <pages> pp. 173-186. </pages>
Reference-contexts: The original GTH algorithm emerges from probabilistic arguments, and it is shown that the stationary distribution of a Markov chain can be calculated using only nonnegative numbers and avoiding subtraction operations. This algorithm achieves significantly greater accuracy than other algorithms described in the literature <ref> [11] </ref>, [5] since there is no loss of significant digits due to cancellation [6]. Interestingly, the inspiration for the algorithm presented in [16], which is specifically for the solution of NCD Markov chains, is the GTH algorithm.
Reference: [12] <author> B. Philippe, Y. Saad and W. J. Stewart, </author> <title> Numerical methods in Markov chain modelling, </title> <journal> Oper. Res., </journal> <volume> 40 (1992), </volume> <pages> pp. 1156-1179. </pages>
Reference-contexts: Moreover, when the system is nearly completely decomposable, there are eigenvalues close to 1, and the poor separation of the unit eigenvalue implies a slow rate of convergence for standard matrix iterative methods <ref> [12] </ref>. IAD methods do not suffer from these limitations [7], [23]. The idea in IAD methods is to observe the system in isolation in each of the diagonal blocks as if the system is completely decomposable (see [15]) and to compute the stationary probability distribution of each diagonal block.
Reference: [13] <author> S. Pissanetzky, </author> <title> Sparse Matrix Technology, </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1984. </year>
Reference-contexts: On the other hand, Scheme 1 calls for the transposition of the coefficient matrix before executing GE or GTH. What follows is a discussion of the effects of the two implementation schemes on a semisystematic row-wise sparse storage format (see <ref> [13] </ref>). A row-wise sparse storage format is a data structure that stores the nonzero elements in the coefficient matrix row by row. Semisystematic means the elements of row i precede those of row i + 1, but the elements within a given row need not be ordered. <p> Note that both programming difficulties and overhead storage (arrays to store the indices of nonzero array elements and pointers between elements) increase with the sophistication of the storage scheme. Together with the row-wise sparse storage format, experiments with the sparse storage format proposed by Knuth (see <ref> [13] </ref> for a brief explanation), which provides equally fast access to columns and rows of a matrix, are conducted.
Reference: [14] <author> P. J. Schweitzer, </author> <title> A survey of aggregation-disaggregation in large Markov chains, in Numerical Solution of Markov Chains, </title> <editor> W. J. Stewart, ed., </editor> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1991, </year> <pages> pp. 63-88. </pages>
Reference-contexts: Each iteration of the IAD algorithm reduces the residual error (i.e., k (I P )k) by a factor of kEk (see <ref> [14] </ref>, p. 80). The next section discusses how a modified version of Gaussian elimination may be used to enforce stability in the solution of the coupling matrix. In x3, the idea is extended so that it can be used in a nonsingular system of equations with a substochastic coefficient matrix.
Reference: [15] <author> H. Simon and A. Ando, </author> <title> Aggregation of variables in dynamic systems, </title> <journal> Econometrica, </journal> <volume> 29 (1961), </volume> <pages> pp. 111-138. </pages>
Reference-contexts: IAD methods do not suffer from these limitations [7], [23]. The idea in IAD methods is to observe the system in isolation in each of the diagonal blocks as if the system is completely decomposable (see <ref> [15] </ref>) and to compute the stationary probability distribution of each diagonal block. However, there are two problems with this approach. First, since the diagonal blocks are substochastic, the off-diagonal probability mass must somehow be incorporated into the diagonal blocks.
Reference: [16] <author> G. W. Stewart and G. Zhang, </author> <title> On a direct method for the solution of nearly uncoupled Markov chains, </title> <journal> Numer. Math., </journal> <volume> 59 (1991), </volume> <pages> pp. 1-11. </pages>
Reference-contexts: The advocated approach to finding a remedy for this situation is the GTH algorithm described in [4] and the direct method discussed in <ref> [16] </ref>. The original GTH algorithm emerges from probabilistic arguments, and it is shown that the stationary distribution of a Markov chain can be calculated using only nonnegative numbers and avoiding subtraction operations. <p> This algorithm achieves significantly greater accuracy than other algorithms described in the literature [11], [5] since there is no loss of significant digits due to cancellation [6]. Interestingly, the inspiration for the algorithm presented in <ref> [16] </ref>, which is specifically for the solution of NCD Markov chains, is the GTH algorithm. In a recent paper [10], it is shown that the stationary probability vector of an N fi N irreducible Markov chain stored in floating-point form is close to its exact stationary vector.
Reference: [17] <author> G. W. Stewart, W. J. Stewart and D. F. McAllister, </author> <title> A two-stage iteration for solving nearly completely decomposable Markov chains, in The IMA Volumes in Mathematics and its Applications 60: Recent Advances in Iterative Methods, </title> <editor> G. H. Golub, A. Greenbaum, and M. Luskin, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994, </year> <pages> pp. 201-216. </pages>
Reference-contexts: Since the aim is to compute the stationary vector, one can approximate the coupling matrix by using an approximate stationary vector and improve on the approximate solution iteratively [21]. 2 Without further ado, the IAD algorithm is provided. Convergence analysis of the algorithm appears in <ref> [17] </ref>. Additional discussion may be found in [21] and [2]. The Generic Iterative Aggregation-Disaggregation (IAD) Algorithm: 1. Let (0) = ( 1 ; 2 ; : : :; N ) be a given initial approximation of the solution. Set k = 1. 2.
Reference: [18] <author> G. W. Stewart, </author> <type> personal communication, </type> <year> 1994. </year>
Reference-contexts: The second problem investigated is a real-life example and had been studied with a variety of parameters in the past. The scheme suggested by G. W. Stewart <ref> [18] </ref> in x4 is implemented, and the outcome of the related experiments are discussed following the presentation of the numerical results.
Reference: [19] <author> W. J. Stewart, </author> <title> A comparison of numerical techniques in Markov modelling, </title> <journal> Comm. ACM, </journal> <volume> 21 (1978), </volume> <pages> pp. </pages> <month> 144-152. </month> <title> [20] |||-, MARCA: Markov Chain Analyzer, A software package for Markov modeling, in Numerical Solution of Markov Chains, </title> <editor> W. J. Stewart, ed., </editor> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1991, </year> <pages> pp. 37-61. </pages>
Reference-contexts: M agg M disagg Iter T total Err res Err rel GE 1 GE 1 4 0.04 0:286E 16 0:824E 13 GT H 2 GT H 2 4 0.04 0:250E 16 0:420E 15 11 5.2 Test Problem 2 The second problem considered appears in <ref> [19] </ref>. It represents a time-shared, multiprogrammed, paged, virtual memory computer system modeled as a closed queueing network (see Fig. 1). The system consists of a number of users using the terminals, a central processing unit (CPU), a secondary memory device (SMD), and a filing device (FD).
Reference: [21] <author> W. J. Stewart and W. Wu, </author> <title> Numerical experiments with iteration and aggregation for Markov chains, </title> <journal> ORSA J. Comput., </journal> <volume> 4 (1992), </volume> <pages> pp. 336-350. </pages>
Reference-contexts: An alternative solution technique is to approximate S ii by accumulating the off-diagonal mass P ifl into the diagonal block P ii on a row by row basis. There are various ways in which this can be done <ref> [21] </ref>. Thereafter, an approximation of the conditional stationary vector of the block of states may be found by solving the corresponding linear system as before. <p> However, the stationary vector itself is needed to form the coupling matrix. Since the aim is to compute the stationary vector, one can approximate the coupling matrix by using an approximate stationary vector and improve on the approximate solution iteratively <ref> [21] </ref>. 2 Without further ado, the IAD algorithm is provided. Convergence analysis of the algorithm appears in [17]. Additional discussion may be found in [21] and [2]. The Generic Iterative Aggregation-Disaggregation (IAD) Algorithm: 1. <p> aim is to compute the stationary vector, one can approximate the coupling matrix by using an approximate stationary vector and improve on the approximate solution iteratively <ref> [21] </ref>. 2 Without further ado, the IAD algorithm is provided. Convergence analysis of the algorithm appears in [17]. Additional discussion may be found in [21] and [2]. The Generic Iterative Aggregation-Disaggregation (IAD) Algorithm: 1. Let (0) = ( 1 ; 2 ; : : :; N ) be a given initial approximation of the solution. Set k = 1. 2. <p> As indicated in <ref> [21] </ref>, in order to achieve an even better approximation of , at the (k + 1) st iteration of the IAD algorithm, one solves (3:1) (I P T (k+1) where (k+1) i is the (k + 1) st approximation of i and b T i = j&lt;i j P ji +
Reference: [22] <author> W. J. Stewart, </author> <title> Introduction to the Numerical Solution of Markov Chains, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1994. </year>
Reference-contexts: Although the multipliers in Scheme 2 are not necessarily bounded by 1, the growth factor still cannot be greater than 1 as indicated in <ref> [22] </ref>. However, if row-reductions are carried out in Scheme 2, both the upper-triangular matrix and the lower-triangular matrix, which contains the multipliers, have to be stored during the triangularization process.
Reference: [23] <author> Y. Takahashi, </author> <title> A lumping method for the numerical calculation of stationary distributions of Markov chains, </title> <institution> Research Report B-18, Dept. of Information Sciences, Tokyo Institute of Technology, </institution> <year> 1975. </year> <month> 18 </month>
Reference-contexts: Moreover, when the system is nearly completely decomposable, there are eigenvalues close to 1, and the poor separation of the unit eigenvalue implies a slow rate of convergence for standard matrix iterative methods [12]. IAD methods do not suffer from these limitations [7], <ref> [23] </ref>. The idea in IAD methods is to observe the system in isolation in each of the diagonal blocks as if the system is completely decomposable (see [15]) and to compute the stationary probability distribution of each diagonal block. However, there are two problems with this approach.
References-found: 21

