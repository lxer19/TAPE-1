URL: ftp://www-pablo.cs.uiuc.edu/pub/Pablo.Release/Documentation/PorsonifyThesis.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Project/Pablo/PabloSonification.htm
Root-URL: http://www.cs.uiuc.edu
Title: A PORTABLE SYSTEM FOR DATA SONIFICATION  
Author: BY TARA MAJA MADHYASTHA 
Degree: 1990 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1992 Urbana, Illinois  
Affiliation: B.A., Rutgers State University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Aydt, R. A. SDDF: </author> <title> The Pablo Self-Describing Data Format. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: These graphs can be configured, saved and loaded. At the highest analysis level, performance trace events are converted to a self-describing data format <ref> [1] </ref> that includes internal definitions of data types, sizes, and names. A trace file in this format consists of a set of record definitions 62 (much like C structures) and a stream of record tag/data pairs. Each tag identifies the type of the record to follow.
Reference: [2] <author> Blattner, M. M., Sumikawa, D. A., and Greenberg, R. M. Earcons and icons: </author> <title> Their structure and common design principles. Human-Computer Interaction 4 (1989), </title> <type> 11 - 44. </type>
Reference-contexts: This concrete representation is in contrast to the approach taken by Blattner, Sumikawa, and Greenberg <ref> [2] </ref>, who describe a method for encoding information as auditory icons, or earcons, based on rhythmicised sequences of notes, called motives.
Reference: [3] <author> Bly, S. </author> <title> Sound and Computer Information Presentation. </title> <type> PhD thesis, </type> <institution> University of Cali-fornia, Davis, </institution> <year> 1982. </year>
Reference-contexts: Bly's study <ref> [3] </ref> examined possible aural representations of multivariate, logarithmic, and time-varying data. Elements of data sets were mapped to pitch, volume, duration and waveshape, so that each data sample produced a single note. <p> Each face represents an individual sample. Chernoff faces are analogous to Bly's approach to sonifying multivariate data, where the four variables of each data sample were mapped to pitch, volume, duration, and fundamental waveshape of a note <ref> [3] </ref>. The resulting notes enabled listeners to classify each sample into one of three related sets. Sound mappings of this type share many of the same problems as Chernoff faces. Face parameters are not completely independent, and extreme values of some face parameters cause others to lose their effect [7].
Reference: [4] <editor> Boyer, R., and Savageau, D. Places Rated Almanac. </editor> <publisher> Rand McNally, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The data used to experiment with multivariate data sonifica-tion was taken from the Places Rated Almanac <ref> [4] </ref>. In this data set, each of three hundred and twenty-nine cities in the United States has a rating for climate and terrain, housing, health care and environment, crime, transportation, education, the arts, recreation, and economics, in addition to a latitude, longitude, and population. <p> The recording contains samples of the sonifications described in Chapter 5. All of the sonifications were generated by a MIDI-based Yamaha TG33 Tone Generator. A.1 Multivariate Data The following set of sonifications present the data from the Places Rated Almanac <ref> [4] </ref>. A note is played for each data sample (each city), where pitch corresponds to the recreation index, duration is controlled by population, longitude determines the stereo balance, and the climate index is used to select the timbre.
Reference: [5] <author> Chernoff, H. </author> <title> The use of faces to represent points in k-dimensional space graphically. </title> <journal> Journal of the Ameerican Statistical Association 68 (1973), </journal> <volume> 361 - 368. </volume>
Reference-contexts: They illustrate the flexibility of the software and are suggestions to inspire further work in this area. 5.1 Multivariate Data Multivariate data with more than three dimensions is challenging to visualize, because more creative means than plotting must be used to represent the higher dimensions. Chernoff's faces <ref> [5] </ref>, for example, is a technique to aid data clustering by mapping variables of a data sample to facial characteristics, e.g., length of nose, size of eyes. Each face represents an individual sample.
Reference: [6] <author> Dodge, C., and Jerse, T. A. </author> <title> Computer Music: Synthesis, Composition, and Performance. </title> <publisher> Schirmer Books, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: The most general technique relies on software to generate the samples of a waveform. These samples are then made audible using a digital to analog converter. Historically, software synthesis techniques have been the focus of computer music research. Hence, a plethora of software synthesis techniques exist <ref> [6, 16] </ref>. Given the rich history of software-controlled sound synthesis, anything more than a cursory description is beyond the scope of this thesis. The strength of software sound synthesis is its device independence; almost any algorithm can be used to produce sound samples.
Reference: [7] <author> Flury, B., and Riedwyl, H. </author> <title> Graphical representation of multivariate data by means of asymmetrical faces. </title> <journal> Journal of the Ameerican Statistical Association 76 (1981), </journal> <volume> 757 - 765. </volume>
Reference-contexts: The resulting notes enabled listeners to classify each sample into one of three related sets. Sound mappings of this type share many of the same problems as Chernoff faces. Face parameters are not completely independent, and extreme values of some face parameters cause others to lose their effect <ref> [7] </ref>. In the same way, sound characteristics are interrelated, and it can be hard to distinguish the timbre of a very short or high-pitched note, or 68 the pitch of a very short note, etc.
Reference: [8] <author> Francioni, J. M., Albright, L., and Jackson, J. A. </author> <title> The Sounds of Parallel Programs. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference (1991), </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> 84 </pages>
Reference-contexts: This approach is extremely flexible, as the data steam can control any facet of a sound; indeed, it can be interpreted as the waveform itself. However, because the sound synthesis is software controlled, specialized digital signal processing hardware is required to achieve real-time output. Francioni and Jackson <ref> [8, 12] </ref> have experimented with sound specifically to depict parallel program behavior, using relative timing information in event trace files to control timing and duration of sound events.
Reference: [9] <author> Frysinger, S. P. </author> <booktitle> Applied research in auditory data representation. In Proceedings of the SPIE, Conference 1259, Extracting Meaning from Complex Data: Processing, Display, Interaction (1990), </booktitle> <pages> pp. 130 - 139. </pages>
Reference-contexts: Certain patterns of notes are more "melodic" than others. For this reason, the choice of a scale or starting pitch can be particularly significant for a sonification designer, as one choice might generate less melodic, and thus less memorable, patterns than another <ref> [9] </ref>. 1.4 Sonification Goals Previous work in sonification suggests that a general-purpose mechanism to map parameters of data to characteristics of sounds on available sound hardware is essential for experimentation.
Reference: [10] <author> Gaver, W. W. </author> <title> The sonicfinder: An interface that uses auditory icons. Human-Computer Interaction 4 (1989), </title> <type> 67 - 94. </type>
Reference-contexts: The careful listener might be able to deduce a person's appearance or identity from the tapping of shoes or the rustling of clothing. Although still underutilized, sound is not new to the computer interface. Gaver <ref> [10] </ref> describes a representation scheme using sound in the SonicFinder, an auditory interface to the Apple Macintosh that couples auditory icons with graphical feedback. Gaver suggests that sound should be used in the computer interface as it would be used in the real world.
Reference: [11] <author> Geist, G. A. </author> <title> A Portable Instrumented Communication Library, C Reference Manual. </title> <type> Tech. Rep. </type> <institution> ORNL/TM-11130, Oak Ridge National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: This technique was used to sonify an execution trace from PDE code (a partial differential equations solver) running on an Intel iPSC/860 hypercube of 64 nodes, a message-based parallel system. This trace file was captured using the PICL subroutine library <ref> [11] </ref>, and converted into SDDF format to be used with Pablo. The trace file consists of records of different types. Each record includes a timestamp, and other information depending on the record type. <p> A.2.1 Sixty-Four Processors The next two sonifications are designed to illustrate communication patterns in parallel appli cations. The data comes from a 64 node Intel iPSC/860 hypercube, and was generated with the Portable Instrumented Communication Library (PICL) <ref> [11] </ref>. In each sonification, a note is played for each message send and receive. The sending processor identifier (0-63) is mapped to a pitch from (30-93). Notes corresponding to sends are played in the left of the stereo balance field, and receives are played in the right.
Reference: [12] <author> Jackson, J. A., and Francioni, J. M. </author> <title> Aural signatures of parallel programs. </title> <booktitle> In Proceedings of the 25th Hawaii International Conference on System Sciences (1992). </booktitle>
Reference-contexts: This approach is extremely flexible, as the data steam can control any facet of a sound; indeed, it can be interpreted as the waveform itself. However, because the sound synthesis is software controlled, specialized digital signal processing hardware is required to achieve real-time output. Francioni and Jackson <ref> [8, 12] </ref> have experimented with sound specifically to depict parallel program behavior, using relative timing information in event trace files to control timing and duration of sound events.
Reference: [13] <author> Keefe, R. </author> <title> Composing by musical analog: A look at planetary orbits. </title> <booktitle> Computer 24 (1991), </booktitle> <volume> 72 - 75. </volume>
Reference-contexts: This is the technique that was used in Heitor Villa-Lobos's New York Skyline Melody (1940), based on the 1940's New York skyline, and Charles Dodge's Earth's Magnetic Field (1970). Robert Keefe describes in detail the creation of such a musical analog based on planetary orbits <ref> [13] </ref>. Latitudinal coordinates of the planets are mapped onto a pitch letter name, an accidental, and an octave displacement, yielding a musical score. This mapping function is a product of the composer's intuition and creativity.
Reference: [14] <author> Lunney, D., and Morrison, R. C. </author> <title> Auditory presentation of experimental data. </title> <booktitle> In Proceedings of the SPIE, Conference 1259, Extracting Meaning from Complex Data: Processing, Display, Interaction (1990), </booktitle> <pages> pp. 140 - 146. </pages>
Reference-contexts: Duration can also be used to mark the lifetime of timestamped events; it is easy to detect overlapping notes, and to get a qualitative feel for the relative durations of different sounds. 1.3.6 Musical Considerations Most people have a good memory for musical patterns <ref> [14] </ref>. Typically, the melody of a new song is the first thing remembered and the last thing forgotten. However, exactly what constitutes a melody has been the subject of a fair amount of research [16].
Reference: [15] <author> MIDI Manufacturers Association. </author> <title> MIDI Musical Instrument Digital Interface, Specification 1.0. </title> <booktitle> International MIDI Association, </booktitle> <address> Los Angeles, CA, </address> <year> 1988. </year>
Reference-contexts: Mappings of this nature reveal patterns in program execution (such as interprocessor communication patterns and processor busy/idle times) as recognizable rhythms and melodies. These mappings are used to generate MIDI <ref> [15] </ref> files that can be played on instruments conforming to the MIDI protocol. This protocol defines a standard format for representing the components of sound (e.g. pitch, duration, volume); many electronic instruments can play these files.
Reference: [16] <author> Moore, F. R. </author> <title> Elements of Computer Music. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: The most general technique relies on software to generate the samples of a waveform. These samples are then made audible using a digital to analog converter. Historically, software synthesis techniques have been the focus of computer music research. Hence, a plethora of software synthesis techniques exist <ref> [6, 16] </ref>. Given the rich history of software-controlled sound synthesis, anything more than a cursory description is beyond the scope of this thesis. The strength of software sound synthesis is its device independence; almost any algorithm can be used to produce sound samples. <p> It is because of an organizing beat that listeners can anticipate future events and remember more easily what has been heard. Short-term memory limits the number of events that can be immediately remembered; in music it is generally accepted that this can extend upward to fifteen events per second <ref> [16] </ref>. Pulse is perhaps most important in the context of synchronizing human performance. In the context of sonification, it is interesting as a means to generate musical patterns. <p> Typically, the melody of a new song is the first thing remembered and the last thing forgotten. However, exactly what constitutes a melody has been the subject of a fair amount of research <ref> [16] </ref>. For example, a sequence of 10 notes from a single instrument is more likely to be perceived as a melody than the same notes from different instruments. Certain patterns of notes are more "melodic" than others.
Reference: [17] <author> Rabenhorst, D. A., Farrell, E. J., Jameson, D. H., Linton, T. D., and Man-delman, J. A. </author> <title> Complementary visualization and sonification of multi-dimensional data. </title> <booktitle> In Proceedings of the SPIE, Conference 1259, Extracting Meaning from Complex Data: Processing, Display, Interaction (1990), </booktitle> <pages> pp. 147 - 153. </pages>
Reference-contexts: In summary, Bly's experiments showed that sound was at least as useful as traditional graphical techniques in categorizing data samples. 1 See x1.3 for a review of sound characteristics. 3 This mapping technique was later used by Smith et al [22] and Rabenhorst et al <ref> [17] </ref> in concert with visual data exploration environments. Smith et al incorporate the ideas of basic data mapping with an iconographic display where each data sample is represented by an icon on the screen.
Reference: [18] <author> Reed, D. A., Olson, R. D., Aydt, R. A., Madhyastha, T. M., Birkett, T., Jensen, D. W., Nazief, B. A. A., and Totty, B. K. </author> <title> Scalable Performance Environments for Parallel Systems. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference (1991), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 562 - 569. 85 </pages>
Reference-contexts: Ideally, we would like to accompany sonifications with visual displays. More generally, integration of sound into arbitrary applications should be easy to encourage experimentation with sonification. This chapter describes the integration of Porsonify into the Pablo performance analysis environment <ref> [18] </ref>. The device-independent design of Porsonify permits Pablo to utilize any available sound hardware without any device-specific code. Information about available sound devices and how to construct sonifications for them is kept separately in a configurable directory. <p> A.2 Parallel Program Behavior The following sonifications use sound to illustrate aspects of the behavior of various paral lel programs. These were all generated in concert with standard visualizations in the Pablo performance analysis environment <ref> [18] </ref>. A.2.1 Sixty-Four Processors The next two sonifications are designed to illustrate communication patterns in parallel appli cations. The data comes from a 64 node Intel iPSC/860 hypercube, and was generated with the Portable Instrumented Communication Library (PICL) [11].
Reference: [19] <author> Rudolph, D. C., and Reed, D. A. </author> <title> CRYSTAL: Operating System Instrumentation for the Intel iPSC/2. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications (Monterey, </booktitle> <address> CA, </address> <month> Mar. </month> <year> 1989). </year>
Reference-contexts: It provided interesting insight, however, into a trace of simplex code running on an 8 node Intel iPSC/2 hypercube. The simplex method is an algorithm used to solve linear optimization problems. This trace file was generated using the Crystal trace capture library <ref> [19] </ref>. As above, the pitch of a note corresponded to the processor identifier, however, nodes 0-7 were mapped to pitches in a C major scale to create more musically recognizable patterns. A note was started for each send and finished at the corresponding receive.
Reference: [20] <author> Scaletti, C., and Craig, A. B. </author> <title> Using Sound to Extract Meaning from Complex Data. </title> <booktitle> In Proceedings of the SPIE, Conference 1459, Extracting Meaning from Complex Data: Processing, Display, </booktitle> <address> Interaction II (San Jose, CA, </address> <year> 1991). </year>
Reference-contexts: Rabenhorst et al take a similar approach, mapping three-dimensional vector gradients in an exploratory environment to de-tuning and stereo balance of each of three notes in a musical chord. Scaletti and Craig <ref> [20] </ref> developed a preliminary set of standard, high-level sonification tools that they used to add time-varying, data-driven sound to scientific visualizations. The fundamental tool, a Shifter, interprets a stream of data as samples of a waveform. <p> The synthesis time can be reduced with special-purpose digital signal processing hardware. The Capybara digital signal processor (based on nine Motorola 56001 digital signal processors running in parallel) was used by Scaletti and Craig <ref> [20] </ref> to support software synthesis in real-time. The Capybara system is controlled by a functional, data stream language called Kyma, allowing the user to graphically combine sound synthesis building blocks and map data to sound. Although this is a flexible platform for building sonifications, the hardware is specialized and expensive. <p> The resulting sound quality is equivalent to standard telephone service. In contrast, the Capybara sample rate is adjustable up to 28K samples per second <ref> [20] </ref>, and CD quality sound is 13 44.1K samples per second at 16-bit precision per channel. Even at such a low sample rate, the Sun SPARCstation cannot synthesize and play complicated sonifications in real-time. Real-time control of sound synthesis is a necessity.
Reference: [21] <author> Singh, J. P., Weber, W. D., and Gupta, A. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <type> Tech. Rep. </type> <institution> CSL-TR-91-469, Computer Systems Laboratory, DEECS, Stanford University, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: We obtained an execution trace of code from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite <ref> [21] </ref> running under the Choices operating system on the Encore Multimax. The benchmark run is a many-body molecular dynamics simulation (WATER). Choices is an object-oriented operating system designed to support experimentation with resource management policies on shared and distributed memory parallel systems.
Reference: [22] <author> Smith, S., Bergeron, R. D., and Grinstein, G. G. </author> <title> Stereophonic and surface sound generation for exploratory data analysis. </title> <booktitle> In Proceedings of the CHI'90 Conference on Human Factors in Computer Systems (Reading, </booktitle> <address> MA, </address> <year> 1990), </year> <pages> pp. 125 - 132. </pages>
Reference-contexts: In summary, Bly's experiments showed that sound was at least as useful as traditional graphical techniques in categorizing data samples. 1 See x1.3 for a review of sound characteristics. 3 This mapping technique was later used by Smith et al <ref> [22] </ref> and Rabenhorst et al [17] in concert with visual data exploration environments. Smith et al incorporate the ideas of basic data mapping with an iconographic display where each data sample is represented by an icon on the screen.
Reference: [23] <author> Stunkel, C. B., and Reed, D. A. </author> <title> Hypercube Implementation of the Simplex Algorithm. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Computers and Concurrent Applications (Pasadena, </booktitle> <address> CA, </address> <month> Jan. </month> <year> 1988), </year> <journal> Association for Computing Machinery, </journal> <pages> pp. 1473-1482. </pages>
Reference-contexts: With the exception of a few outliers, all 64 pitches are first heard in the left, then in the right as the messages are received. A.2.2 Eight Processors This sonification of part of the execution of simplex linear optimization code running on an 8 node Intel iPSC/2 hypercube <ref> [23] </ref> demonstrates another way of revealing communication patterns. As in the previous sonifications, the identifier of the sending processor is mapped to pitch. However, the smaller number of processors permits notes started at each send to be sustained until their corresponding receives.
Reference: [24] <author> Sun Microsystems. </author> <title> Sun System User's Guide. </title> <address> Mountain View, CA, </address> <year> 1991. </year>
Reference-contexts: This chip contains analog to digital and digital to analog converters; however, the only functionality supported by the SunOS device driver is recording and playback of digital audio data at 8K samples per second with 12-bit precision on one output channel <ref> [24] </ref>. The resulting sound quality is equivalent to standard telephone service. In contrast, the Capybara sample rate is adjustable up to 28K samples per second [20], and CD quality sound is 13 44.1K samples per second at 16-bit precision per channel.
Reference: [25] <author> Wenzel, E. M., Wightman, F. L., and Foster, S. H. </author> <title> A virtual display system for conveying three-dimensional acoustic information. </title> <booktitle> In Proceedings of the Human Factors Society (1988), </booktitle> <pages> pp. 86 - 90. 86 </pages>
Reference-contexts: Implementation of a two-dimensional stereo display, which would allow horizontal and vertical positioning of a sound source, is difficult in real-time. One technique, developed at the NASA Ames Research Center, can position 9 up to four sound sources in three-dimensional space in real-time over headphones <ref> [25] </ref>. This kind of device can greatly increase the potential applications of location in sonification. However, even the one-dimensional display provided by stereo balance (where the sound source can be positioned in discrete steps from far left to far right) can be useful.
References-found: 25

