URL: http://www.cs.berkeley.edu/~yelick/soumen/grobner-ppopp93.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Email: Email: soumen@cs.berkeley.edu, yelick@cs.berkeley.edu  
Title: Implementing an Irregular Application on a Distributed Memory Multiprocessor  
Author: Soumen Chakrabarti Katherine Yelick 
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: Parallelism with irregular patterns of data, communication and computation is hard to manage efficiently. In this paper we present a case study of the Grobner basis problem, a symbolic algebra application. We developed an efficient parallel implementation using the following techniques. First, a sequential algorithm was rewritten in a transition axiom style, in which computation proceeds by nondeterministic invocations of guarded statements at multiple processors. Next, the algebraic properties of the problem were studied to modify the algorithm to ensure correctness in spite of locally inconsistent views of the shared data structures. This was used to design data structures with very little overhead for maintaining consistency. Finally, an application-specific scheduler was designed and tuned to get good performance. Our distributed memory implementation achieves impressive speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering-a new definition. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: The application must use the operations so as to implement the nature of consistency it needs. Thus the abstraction provides a software controlled weak consistency mechanism (see <ref> [1] </ref>, for example). 4.2 The Pair Queue The algorithm needs a priority queue for pairs of polynomials. The priority corresponds to the heuristic merit of a pair. Since this is only a heuristic, it need not be adhered to exactly.
Reference: [2] <author> Bruno Buchberger. </author> <title> Grobner basis: an algorithmic method in polynomial ideal theory. </title> <editor> In N. K. Bose, editor, </editor> <booktitle> Multidimensional Systems Theory, chapter 6, </booktitle> <pages> pages 184-232. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. distributed memory implementation of the Buchberger <ref> [2] </ref> algorithm that equals and often surpasses shared memory performance and scales to a larger number of processors. <p> The original sequential algorithm for computing a Grobner basis was given by Buchberger <ref> [2] </ref>. A survey of the theory can be found in Mishra [5]. Input: F , a finite set of polynomials. <p> These speedups were computed as the ratio between P processor running time (excluding input, output and initialization) and 1 processor running time, both for the parallel algorithm. We used the pair elimination criteria in [3] and the traditional pair selection in <ref> [2] </ref>. Inspection of execution profiles indicated that for small problems, parallelism was limited by under-utilization of processors during startup and termination transients. The total number of tasks is too small to saturate all processors with work for a large fraction of the running time.
Reference: [3] <author> Bruno Buchberger. </author> <title> A Criterion for detecting Unnecessary Reductions in the construction of Grobner Bases. </title> <booktitle> In Proceedings of the EU-ROSAM '79, An International Symposium on Symbolic and Algebraic Manipulation, </booktitle> <pages> pages 3-21, </pages> <address> Marseille, France, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: It has been pointed out that with a good implementation of pruning criteria a large number of pairs are eliminated quickly and only about m instead of m 2 pairs have to be actually reduced. However, * A formal proof of this fact exists in <ref> [3] </ref>, but it only works for restricted inputs with two symbols (n = 2). <p> In comparison, the best curve from Vidal [7] show less scalable speedup. These speedups were computed as the ratio between P processor running time (excluding input, output and initialization) and 1 processor running time, both for the parallel algorithm. We used the pair elimination criteria in <ref> [3] </ref> and the traditional pair selection in [2]. Inspection of execution profiles indicated that for small problems, parallelism was limited by under-utilization of processors during startup and termination transients.
Reference: [4] <author> Soumen Chakrabarti. </author> <title> A distributed memory Grobner basis algorithm. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: We encapsulate the data structures so that the semantics and consistency are exposed through a specified interface to the higher level. We can therefore reason about correctness (safety and liveness properties) using these specifications <ref> [4] </ref>, without having to use the specifics of implementation. The details of the data structure implementations are in x4. Input: F , a finite set of polynomials. <p> These axioms can be interpreted as specifying an interleaving of axioms without any real concurrency. Few axioms actually interfere by sharing data. Using little serialization we can enable the axioms to fire concurrently in the implementation. In <ref> [4] </ref> we have shown that the transition axiom specifications correctly compute a Grobner basis; we omit the proofs. 4 Data Structure Design In this section we describe the design and implementa tion of the two aggregate data structures used in the algorithm. 4.1 The Basis A major decision was the representation
Reference: [5] <author> Bud Mishra and Chee Yap. </author> <title> Notes on Grobner basis. </title> <booktitle> In Information Sciences 48, </booktitle> <pages> pages 219-252. </pages> <publisher> Elsevier Science Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: The original sequential algorithm for computing a Grobner basis was given by Buchberger [2]. A survey of the theory can be found in Mishra <ref> [5] </ref>. Input: F , a finite set of polynomials.
Reference: [6] <author> Kurt Siegl. </author> <title> Parallel Grobner basis computation in jjMAPLEjj. </title> <type> Technical Report 92-11, </type> <institution> Research Institute for Symbolic Computation, Linz, Austria, </institution> <year> 1992. </year>
Reference-contexts: A parallel algorithm employing a ring of reducers with the basis partitioned among them was proposed by Buchberger [7]. A slight variant using a reducing pipeline has been implemented by Siegl <ref> [6] </ref>. We believe that partitioning has less available parallelism and more communication overhead. <p> Poorly Balanced Pipeline Partitioning means that reduction will have to be done in a pipeline as in <ref> [6] </ref>. The basic requirement for a pipelined computation to be efficient is that there should be a large number of stages, each taking about the same time (i.e., no bottlenecks exist). <p> As shown in the performance graphs in x7, except for examples where superlinear speedup results from chancing upon "short-cuts" in the search space, efficiency and scalability is low. Attempts to parallelize this problem on a distributed memory machine have been made by Siegl <ref> [6] </ref>. Reduction of a polynomial is done by a pipeline of processes across which the current basis is partitioned. As discussed earlier, communication costs are an order of magnitude higher.
Reference: [7] <author> Jean-Philippe Vidal. </author> <title> The computation of Grobner bases on a shared memory multiprocessor. </title> <type> Technical Report CMU-CS-90-163, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1990. </year>
Reference-contexts: The problem resembles search in that running time may vary widely depending on heuristic choice. * The amount and pattern of communication is unpre dictable for conceivable parallelizations. Our preliminary profiling experiments on a sequential implementation (from CMU <ref> [7] </ref>) suggested that the problem has significant parallelism: a large fraction of running time is spent in relatively independent polynomial arithmetic with hardly any data dependencies preventing their parallelization. <p> A parallel algorithm employing a ring of reducers with the basis partitioned among them was proposed by Buchberger <ref> [7] </ref>. A slight variant using a reducing pipeline has been implemented by Siegl [6]. We believe that partitioning has less available parallelism and more communication overhead. <p> The point we wish to make is that the problem, while not as amenable to scalable parallelization as, say, matrix multiplication, does have usable parallelism which even a distributed memory multiprocessor can exploit. Benchmarks We have used the set of standard benchmarks collected by Vidal <ref> [7] </ref>. On one processor they run for about half a second to a few minutes, with total degree ordering 1 . The smaller inputs are solved too quickly to 1 Ties are resolved by lexicographic order. (a) Not normalized (b) Normalized shared memory performance. All are superlinear. <p> In figure 7 (a) we give performance on two small examples, arnborg4 and trinks1. The best speedups over 5 runs are shown for our implementation. In comparison, the best curve from Vidal <ref> [7] </ref> show less scalable speedup. These speedups were computed as the ratio between P processor running time (excluding input, output and initialization) and 1 processor running time, both for the parallel algorithm. We used the pair elimination criteria in [3] and the traditional pair selection in [2]. <p> Consider figure 8 (a). We get superlinear speedup according to the prior definition of speedup, with a similar effect reported by Vidal for the shared memory implementation <ref> [7] </ref>. This indicates that the heuristic is not sufficiently discerning for these inputs, so that exploring a few of the best pairs (as against the best) in parallel pays off 2 . <p> Normalized speedup is shown in figure 8 (b). The superlinear nature has been filtered completely and the linear nature of "true" speedup shows clearly. 8 Related Work A review of previous attempts to parallelize Grobner basis can be found in Vidal <ref> [7] </ref>. Vidal's implementation [7] is on a shared memory machine, the basis being still regarded as a reader-writer shared object with the appropriate locks. <p> Normalized speedup is shown in figure 8 (b). The superlinear nature has been filtered completely and the linear nature of "true" speedup shows clearly. 8 Related Work A review of previous attempts to parallelize Grobner basis can be found in Vidal <ref> [7] </ref>. Vidal's implementation [7] is on a shared memory machine, the basis being still regarded as a reader-writer shared object with the appropriate locks. As shown in the performance graphs in x7, except for examples where superlinear speedup results from chancing upon "short-cuts" in the search space, efficiency and scalability is low.
Reference: [8] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: The Environment We used a CM-5 multiprocessor. Each node is a 33 MHz (15-20 MIPS) Sparc processor with 8 MB of memory. The network is a fat-tree supporting at most 20 MB/s data transfer. For our purpose we ignored the topology. Communication was done using the active message layer <ref> [8] </ref>. The implementation is in C; we used gcc-2.2.2 with optimization "-O2" for our measurements. Measurement and Fluctuation The sequential algorithm S is under-specified, even though a sequential program could be designed to be deterministic.
Reference: [9] <author> Katherine Yelick. </author> <title> Using abstraction in explicitly parallel programs. </title> <type> Technical Report MIT/LCS/TR-507, </type> <institution> Massachusetts Institute of Technology, 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Data structures G and gpq as before. Unlike in Algorithm S, Reduce (r; G) need not return a normal form; a partially reduced form will do. 3.2 Transition Axiom Specifications Transition axioms <ref> [9, 11] </ref> are a means to exploit non-determinism in an algorithm description. They help decompose the computation into independently schedu-lable chunks, so that scheduling decisions are deferred as much as possible. This means that significant performance tuning can be done late in the design process without major design changes.
Reference: [10] <author> Katherine Yelick. </author> <title> Programming models for irregular applications. </title> <booktitle> In Proceedings of the Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <month> Oc-tober </month> <year> 1992. </year> <note> To appear in SIGPLAN notices. </note>
Reference-contexts: The priority corresponds to the heuristic merit of a pair. Since this is only a heuristic, it need not be adhered to exactly. In the parallel algorithm, gpq is as much a scheduling control as a data structure. The distributed task queue by Wen et al, described in <ref> [10] </ref>, is designed for applications that can be broken down into tasks. The suggested paradigm is that the task queue starts with some number of tasks, processors dequeue tasks and perform them, possibly generating more tasks.
Reference: [11] <author> Katherine A. Yelick and Steven J. </author> <title> Garland. A parallel completion procedure for term rewriting systems. </title> <booktitle> In Conference on Automated Deduction, </booktitle> <address> Saratoga Springs, NY, </address> <year> 1992. </year>
Reference-contexts: Data structures G and gpq as before. Unlike in Algorithm S, Reduce (r; G) need not return a normal form; a partially reduced form will do. 3.2 Transition Axiom Specifications Transition axioms <ref> [9, 11] </ref> are a means to exploit non-determinism in an algorithm description. They help decompose the computation into independently schedu-lable chunks, so that scheduling decisions are deferred as much as possible. This means that significant performance tuning can be done late in the design process without major design changes.
References-found: 11

