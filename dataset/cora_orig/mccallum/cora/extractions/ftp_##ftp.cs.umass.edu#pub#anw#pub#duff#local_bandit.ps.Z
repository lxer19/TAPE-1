URL: ftp://ftp.cs.umass.edu/pub/anw/pub/duff/local_bandit.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/duff/duff_for_ravi.html
Root-URL: 
Email: fduff,bartog@cs.umass.edu  
Title: Local Bandit Approximation for Optimal Learning Problems  
Author: Michael O. Duff Andrew G. Barto 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: In general, procedures for determining Bayes-optimal adaptive controls for Markov decision processes (MDP's) require a prohibitive amount of computation|the optimal learning problem is intractable. This paper proposes an approximate approach in which bandit processes are used to model, in a certain "local" sense, a given MDP. Bandit processes constitute an important subclass of MDP's, and have optimal learning strategies (defined in terms of Gittins indices) that can be computed relatively efficiently. Thus, one scheme for achieving approximately-optimal learning for general MDP's proceeds by taking actions suggested by strategies that are optimal with respect to local bandit models.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bar-Shalom, Y. & Tse, E. </author> <year> (1976) </year> <month> Caution, </month> <title> probing and the value of information in the control of uncertain systems, </title> <journal> Ann. Econ. Soc. Meas. </journal> <volume> 5 </volume> <pages> 323-337. </pages>
Reference: <author> R. Bellman & R. Kalaba, </author> <title> (1959) On adaptive control processes. </title> <journal> IRE Trans., </journal> <volume> 4 </volume> <pages> 1-9. </pages>
Reference: <author> Bokar, V. & Varaiya, </author> <title> P.P. (1979) Adaptive control of Markov chains I: finite parameter set. </title> <journal> IEEE Trans. Auto. Control 24 </journal> <pages> 953-958. </pages>
Reference: <author> Cozzolino, J.M., Gonzalez-Zubieta, R., & Miller, </author> <title> R.L. (1965) Markov decision processes with uncertain transition probabilities. </title> <type> Tech. </type> <institution> Rpt. 11, Operations Research Center, MIT. </institution>
Reference: <author> Dayan, P. & Sejnowski, T. </author> <title> (1996) Exploration Bonuses and Dual Control. </title> <note> Machine Learning (in press). </note>
Reference: <author> Duff, M.O. </author> <title> (1995) Q-learning for bandit problems. </title> <booktitle> in Machine Learning: Proceedings of the Twelfth International Conference on Machine Learning: </booktitle> <pages> pp. 209-217. </pages>
Reference: <author> Duff, M.O. </author> <title> (1997) Approximate computational methods for optimal learning and dual control. </title> <type> Technical Report, </type> <institution> Deptartment of Computer Science, Univ. of Mas-sachusetts, Amherst. </institution>
Reference: <author> Fel'dbaum, A. </author> <title> (1965) Optimal Control Systems, </title> <publisher> Academic Press. </publisher>
Reference: <author> Gittins, J.C. & Jones, D. </author> <title> (1979) Bandit processes and dynamic allocation indices (with discussion). </title> <journal> J. R. Statist. Soc. </journal> <volume> B 41 </volume> <pages> 148-177. </pages>
Reference: <author> Katehakis, M.H. & Veinott, </author> <title> A.F. (1987) The multi-armed bandit problem: </title> <journal> decomposition and computation Math. </journal> <volume> OR 12: </volume> <pages> 262-268. </pages>
Reference: <author> MacQueen, J. </author> <year> (1966). </year> <title> A modified dynamic programming method for Markov decision problems, </title> <journal> J. Math. Anal. Appl., </journal> <volume> 14 </volume> <pages> 38-43. </pages>
Reference: <author> Robinsion, </author> <title> D.R. (1981) Algorithms for evaluating the dynamic allocation index. </title> <note> Research Report No. </note> <institution> 80/DRR/4, Manchester-Sheffield School of Probability and Statistics. </institution>
Reference: <author> Tsitsiklis, J. </author> <title> (1993) A short proof of the Gittins index theorem. </title> <booktitle> Proc. 32nd Conf. Dec. and Control: </booktitle> <pages> 389-390. </pages>
Reference: <author> Varaiya, P.P., Walrand, J.C., & Buyukkoc, C. </author> <title> (1985) Extensions of the multiarmed bandit problem: the discounted case. </title> <journal> IEEE Trans. Auto. Control 30(5) </journal> <pages> 426-439. </pages>
Reference: <author> Watkins, C. </author> <title> (1989) Learning from Delayed Rewards Ph.D. </title> <type> Thesis, </type> <institution> Cambidge University. </institution>
References-found: 15

