URL: ftp://ftp.cs.washington.edu/tr/1995/02/UW-CSE-95-02-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: virgil@cs.washington.edu, zahorjan@cs.washington.edu  
Title: Implementing Lightweight Remote Procedure Calls in the Mach 3 Operating System  
Author: Virgil Bourassa and John Zahorjan 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: The Mach 3 operating system makes extensive use of remote procedure calls (RPCs) to provide services to client applications. Although the existing Mach 3 RPC facility is highly optimized, the incorporation of a Lightweight Remote Procedure Call (LRPC) facility further reduces this critical cost. This paper describes the implementation of LRPC in the Mach 3 operating system, reviewing the original LRPC concept and implementation in the TAOS operating system, the issues involved with the Mach 3 microkernel operating system, and the resulting design of LRPC Mach3 . Performance measurements indicate that the resulting implementation provides a 31% reduction in latency for a minimal RPC, with even more significant benefits for more complicated RPCs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Accetta, M., R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevenian, and M. Young, </author> <title> Mach: A New Kernel Foundation For UNIX Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Technical Conference and Exhibition, </booktitle> <month> June </month> <year> 1986, </year> <pages> pp. 93-112. </pages>
Reference: [2] <author> Bershad, Brian N., </author> <title> High Performance CrossAddress Space Communication, </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <type> Technical Report No. 900602, </type> <month> June </month> <year> 1990. </year>
Reference-contexts: Historically, RPCs began by using an underlying messagepassing transport layer appropriate for network communications. In time, however, it became clear that the most common use of RPCs was for servers residing on the same machine <ref> [2] </ref>. The Mach 3 operating system takes advantage of this by providing a relatively fast local RPC mechanism that ignores network communication needs. <p> The structure of Mach 3 leads to a different design for its LRPC facility than for the original LRPC facility implemented by Bershad in the TAOS operating system [3]. 2. LRPC Concepts & Implementation Bershad showed in <ref> [2] </ref> that well over 95% of RPC calls in general settings are to servers on the same computer. This implies that the RPC functionality of providing transparent server access across a network of computers is unused in the common case.
Reference: [3] <author> Bershad, Brian N., Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy, </author> <title> Lightweight Remote Procedure Call, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> v8(1), </volume> <month> February </month> <year> 1990, </year> <pages> pp. 3755. </pages>
Reference-contexts: 1. Introduction This paper describes the implementation of Lightweight Remote Procedure Calls <ref> [3] </ref> in the Mach 1 3 operating system [1][15][16]. This implementation shows that a variation of the original LRPC facility tailored to the Mach 3 operating system delivers significant performance improvements over the existing Mach RPC facilitydespite the existing RPC having been optimized for local communications [6][7]. <p> The structure of Mach 3 leads to a different design for its LRPC facility than for the original LRPC facility implemented by Bershad in the TAOS operating system <ref> [3] </ref>. 2. LRPC Concepts & Implementation Bershad showed in [2] that well over 95% of RPC calls in general settings are to servers on the same computer. This implies that the RPC functionality of providing transparent server access across a network of computers is unused in the common case.
Reference: [4] <author> Bershad, Brian N., Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy, </author> <title> UserLevel Interprocess Communication for Shared Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> v9(2), </volume> <month> May </month> <year> 1991, </year> <pages> pp. 175198. </pages>
Reference: [5] <author> Birrell, Andrew D., and Bruce Jay Nelson, </author> <title> Implementing Remote Procedure Calls, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> v2(1), </volume> <month> February </month> <year> 1984, </year> <pages> pp. 3959. </pages>
Reference: [6] <author> Draves, Richard P., </author> <title> A Revised IPC Interface, </title> <booktitle> Proceedings of the First Mach USENIX Workshop, </booktitle> <month> October </month> <year> 1990, </year> <pages> pp. 101121. </pages>
Reference: [7] <author> Draves, Richard P., Brian N. Bershad, Richard F. Rashid, and Randall W. Dean, </author> <title> Using Continuations to Implement Thread Management and Communication in Operating Systems, </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991, </year> <pages> pp. 122136. </pages>
Reference-contexts: Keeping these blocked channel threads lying around is not particularly expensive, thanks to the work done to incorporate continuations into Mach 3s kernel (see <ref> [7] </ref>), relieving each thread of the need for a kernel stack.
Reference: [8] <author> Ford, Bryan, Mike Hibler, and Jay Lepreau, </author> <title> Notes on Thread Models in Mach 3.0, </title> <institution> Department of Computer Science, University of Utah, </institution> <type> Technical Report No. </type> <address> UUCS-93-012, Salt Lake City, Utah, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Doing so also opens up the server task to accidental or intentional assault by entities holding capabilities to the thread. Short of completely redesigning Machs thread scheduling, as in <ref> [8] </ref>, thread migration is not currently a practical solution. Furthermore, establishing a shared region of memory between two distinct tasks is fairly involved in Mach 3 [12], requiring either a) a common ancestor task, b) a common memorymapped file, or c) privileged access. <p> This work led to the exploration of the use of thread migration to speed up directed control transfer in <ref> [8] </ref>. 4.3. Performance LRPC Mach3 is implemented on a Sequent multiprocessor containing 20 Intel i386 microprocessors. A single processor is used for all timing comparisons. The fundamental performance measurement is call latencythe time required for a client to invoke and return from a server procedure.
Reference: [9] <author> Johnson, David B. and Willy Zwaenepoel, </author> <title> The Peregrine Highperformance RPC System, </title> <journal> SoftwarePractice and Experience, </journal> <volume> Vol. 23(2), </volume> <month> February </month> <year> 1993, </year> <pages> pp. </pages> <year> 201221. </year>
Reference: [10] <author> Koontz, Kenneth W., </author> <title> Port Buffers: A Mach IPC Optimization for Handling Large Volumes of Small Messages, </title> <booktitle> Proceedings of the USENIX Mach III Symposium, </booktitle> <address> Sante Fe, New Mexico, U.S.A., </address> <publisher> Usenix Association, </publisher> <month> Spring </month> <year> 1993, </year> <pages> pp. 89102. 12 </pages>
Reference-contexts: And, unlike IPCs fast path, LRPC Mach3 doesnt require special circumstances for its performance improvements. When designing LRPC Mach3 , recent proposed optimizations to the existing Mach 3 RPC mechanism were also reviewed. One such optimization is port buffers <ref> [10] </ref>, a technique of grouping multiple RPC calls which can be performed on top of LRPC Mach3 and benefit from the improved RPC performance.
Reference: [11] <author> Lepreau, Jay, Mike Hibler, Bryan Ford, and Jeffrey Law, </author> <title> InKernel Servers on Mach 3.0: Implementation and Performance, </title> <booktitle> Proceedings of the USENIX Mach III Symposium, </booktitle> <address> Sante Fe, New Mexico, U.S.A., </address> <publisher> Usenix Association, </publisher> <month> Spring </month> <year> 1993, </year> <pages> pp. 3956 </pages>
Reference-contexts: One such optimization is port buffers [10], a technique of grouping multiple RPC calls which can be performed on top of LRPC Mach3 and benefit from the improved RPC performance. Another Mach 3 RPC optimization reviewed was inkernel servers <ref> [11] </ref>, which reduces RPC call latency by putting servers into the kernels address space, limiting the facility to those with kernel access and defeating some of the benefits of the microkernel architecture.
Reference: [12] <author> Loepere, Keith, ed., </author> <title> Mach 3 Kernel Principles, Revision 2.2, Open Software Foundation, </title> <year> 1992 </year>
Reference-contexts: Short of completely redesigning Machs thread scheduling, as in [8], thread migration is not currently a practical solution. Furthermore, establishing a shared region of memory between two distinct tasks is fairly involved in Mach 3 <ref> [12] </ref>, requiring either a) a common ancestor task, b) a common memorymapped file, or c) privileged access. <p> Of course, none of this is necessary for procedures with no pointer parameters, but since for compatibility with MiG/IPC the return value is used exclusively to indicate success or failure of the RPC itself, pointer parameters are the only way to get information back from a server procedure <ref> [12] </ref>. The client stub next performs the forward LRPC Mach3 kernel call, lrpc_call (), conveying the identifier of the requested procedure, pointers to the call frame and the inout heap.
Reference: [13] <author> Loepere, Keith, ed., </author> <title> Mach 3 Server Writers Guide, Revision 2.2, Open Software Foundation, </title> <year> 1992 </year>
Reference: [14] <author> Orman, Hilarie, Edwin Menze III, Sean OMalley, and Larry Peterson, </author> <title> A Fast and General Implementation of Mach IPC in a Network, </title> <booktitle> Proceedings of the USENIX Mach III Symposium, </booktitle> <address> Sante Fe, New Mexico, U.S.A., </address> <publisher> Usenix Association, </publisher> <month> Spring </month> <year> 1993, </year> <pages> pp. 7588. </pages>
Reference-contexts: Note that actual remote host RPC has been implemented on top of the local RPC mechanism in Mach 3 by using proxy ports which transparently provide network communication services, e.g. <ref> [14] </ref>. 4. LRPC Mach3 Section 4.1 describes the design of LRPC Mach3 . Section 4.2 discusses how the design decisions reduce the costs of performing an RPC in Mach 3. Performance measurements comparing LRPC Mach3 to MiG/IPC are provided in section 4.3.
Reference: [15] <author> Rashid, Richard F., </author> <title> Threads of a New System, Unix Review, </title> <type> v4, </type> <month> August </month> <year> 1986, </year> <pages> pp. 3749. </pages>
Reference: [16] <author> Rashid, Richard F., </author> <title> From RIG to Accent to Mach: The Evolution of a Network Operating System, The Ecology of Computation (B.A. </title> <editor> Huberman, ed.), </editor> <publisher> North Holland, </publisher> <year> 1988, </year> <pages> pp. </pages> <year> 207230. </year>
Reference: [17] <author> Rovner, P., R. Levin, and J. Wick, </author> <title> On Extending Modula2 for Building Large, Integrated Systems, </title> <type> Technical Report #3, </type> <institution> Digital Equipment Corporations System Research Center, Palo Alto, California, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: The design goal of these techniques is to emulate, as much as possible, a procedurecall model rather than a messagepassing model. LRPC TAOS 1 for the Modula2+ <ref> [17] </ref> programming language in the TAOS operating system on the DEC Firey [19] achieves a factor of three improvement over the best RPC mechanism available for that environment, namely SRC RPC [18].
Reference: [18] <author> Schroeder, M. D., and M. Burrows, </author> <title> Performance of the Firey RPC, </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating Systems Principles, Litchfield Port, </booktitle> <address> Arizona, U.S.A., </address> <publisher> ACM, </publisher> <month> December 3-6, </month> <year> 1989, </year> <pages> pp. 83-90. </pages>
Reference-contexts: LRPC TAOS 1 for the Modula2+ [17] programming language in the TAOS operating system on the DEC Firey [19] achieves a factor of three improvement over the best RPC mechanism available for that environment, namely SRC RPC <ref> [18] </ref>. In this implementation, the control transfer is made simple by using the clients thread to execute the requested service in the servers address space.
Reference: [19] <author> Thacker, C. P., L. C. Stewart, and E. H. Satterthwaite, Jr., Firey: </author> <title> A Multiprocessor Workstation, </title> <journal> IEEE Transactions on Computers, </journal> <volume> v37(8), </volume> <month> August </month> <year> 1988, </year> <pages> pp. 909920. </pages>
Reference-contexts: The design goal of these techniques is to emulate, as much as possible, a procedurecall model rather than a messagepassing model. LRPC TAOS 1 for the Modula2+ [17] programming language in the TAOS operating system on the DEC Firey <ref> [19] </ref> achieves a factor of three improvement over the best RPC mechanism available for that environment, namely SRC RPC [18]. In this implementation, the control transfer is made simple by using the clients thread to execute the requested service in the servers address space.
Reference: [20] <author> Wahbe, Robert, Steven Lucco, Thomas Anderson, and Susan Graham, </author> <title> Efficient SoftwareBased Fault Isolation, </title> <booktitle> Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993, </year> <pages> pp. 203-216. </pages>
Reference: [21] <author> White, J. E., </author> <title> A Highlevel Framework for Networkbased Resource Sharing, </title> <booktitle> Proceedings of the National Computer Conference, </booktitle> <month> June </month> <year> 1976. </year>
References-found: 21

