URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.PARLE.Mercury.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/kthanasi/
Root-URL: 
Abstract-found: 0
Intro-found: 0
Reference: 1. <author> Gul A. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: For example, Mercury supports user-level threads that can synchronize in arbitrary ways. It also supports work heaps of items ("atomic tasks") that run to completion once they are started. Other abstractions consistent with the underlying mechanisms include coarse-grain dataflow computations, actors <ref> [1] </ref>, remote (interprocessor) object invocation [7], and constructs that reduce overhead by scheduling atoms in groups [10, 22]. In order to minimize the overhead of creating and scheduling tasks Mercury provides two kinds of schedulable entities, both of which can use O-AS.
Reference: 2. <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> "The TERA Computer System". </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: More tasks increase potential overhead and communication cost, but programming with excess parallelism has the benefit that multiprogramming can be used to hide the latency of operations such as I/O, inter-thread synchronization, and even memory access operations <ref> [2] </ref>. The version of Mercury described in this paper is implemented as a parallel runtime library for C++ and has been used for research and graduate courses since 1992. It runs on Silicon Graphics Power 4D and Challenge series multiprocessors.
Reference: 3. <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> "The interaction of architecture and operating system design". </title> <booktitle> In ASPLOS IV, </booktitle> <pages> pages 108-122, </pages> <year> 1991. </year>
Reference-contexts: While these architectural trends have improved the throughput of ordinary instructions, the need to create and save this state has made context switching comparatively expensive <ref> [3] </ref>. Thus, fine-grain parallelism is becoming less attractive over time because of the need to amortize the data communication and context switching overhead of synchronization and scheduling over a large number of instructions.
Reference: 4. <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> "Implementation and Performance of Munin". </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Does object-affinity scheduling help programs in which processes coordinate through synchronized access to such shared data structures? We examined the behavior of a PRESTO program <ref> [4] </ref> for the travelling salesman problem. The main write-shared data structure of the program is a heap used as a priority queue containing the best partial tours found so far.
Reference: 5. <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> "PRESTO: A system for Object-oriented Parallel Programming". </title> <journal> Software Practice and Experience, </journal> <pages> pages 713-732, </pages> <year> 1988. </year>
Reference-contexts: Mercury is derived from and is upwardly compatible with PRESTO <ref> [5] </ref>. Some modifications were made to improve the performance of programs written in the standard PRESTO model 1 . The unique aspects of Mercury are the extensions to implement efficient Object-Affinity Scheduling. Object-affinity scheduling as implemented in Mercury can be used on operations as small as a single object invocation.
Reference: 6. <author> R. Chandra, A. Gupta, and J. L. Henessy. </author> <title> "Data Locality and Load Balancing in COOL". </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 249-259, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: They were also the first to note that the short lifespan of lightweight threads does not allow for cache reuse. Developed independently at the same time as the initial implementation of Mercury, the COOL <ref> [6] </ref> programming language, like PRESTO and Mercury, extends C++. In COOL, calling a function prefixed with the keyword parallel forks a thread to execute the function asynchronously. Join synchronization is done explicitly using condition variables.
Reference: 7. <author> A. L. Cox, R. J. Fowler, and J. E. Veenstra. </author> <title> "Interprocessor invocation on a NUMA multiprocessor". </title> <type> Technical report, </type> <institution> Department of Computer Science - University of Rochester, </institution> <year> 1990. </year>
Reference-contexts: For example, Mercury supports user-level threads that can synchronize in arbitrary ways. It also supports work heaps of items ("atomic tasks") that run to completion once they are started. Other abstractions consistent with the underlying mechanisms include coarse-grain dataflow computations, actors [1], remote (interprocessor) object invocation <ref> [7] </ref>, and constructs that reduce overhead by scheduling atoms in groups [10, 22]. In order to minimize the overhead of creating and scheduling tasks Mercury provides two kinds of schedulable entities, both of which can use O-AS. A Thread 1 These techniques are now common to many well-tuned thread packages.
Reference: 8. <author> M. Devarakonda and A. Mukherjee. </author> <title> "Issues in the Implementation of Cache-Affinity Scheduling". </title> <booktitle> In Proceedings of Winter '92 USENIX Conference, </booktitle> <year> 1992. </year>
Reference-contexts: This strategy is motivated by the assumption that the thread will continue to access the data it has touched in the recent past and that the data will still be in the cache at that processor. These techniques have met with some success <ref> [8, 13, 26] </ref> in the context of heavyweight Unix processes using coarse-grain styles of multiprogramming. In contrast, a lightweight thread created to perform a single task does not accumulate much history. It may require only one scheduling decision over its entire lifetime. <p> Thinning is done by changing isolated 1's to 0's, and processing stops when no more changes are possible. Each phase is divided into two subphases, each of which uses slightly different definition of "isolated". In its coarse-grain version, this application was identified by Devarakonda and Mukherjee <ref> [8] </ref> as being ideally suited to cache-affinity scheduling because threads synchronize four times per phase. A medium-grain version can also benefit when threads are rescheduled between sub-phases, but creating independent threads for each phase limits the potential of this strategy. <p> Vaswani and Zahorjan [26] supported this conclusion by showing that the penalty for reloading the cache is small compared to the system quantum. They also found little benefit in trying to exploit cache affinity in programs that have little reuse of writable shared data. Devarakonda and Mukherjee <ref> [8] </ref> made the point that the negative results of previous studies were for kernel-level affinity scheduling (i.e., coarse-grain kernel threads) and that the kernel may not be the appropriate place to implement affinity scheduling.
Reference: 9. <author> R. P. Draves, B. N. Bershad, R. F. Rashid, and R. W. Dean. </author> <title> "Using Continuations to Implement Thread Management and Communication in Operating Systems". </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium in Operating System Principles, </booktitle> <pages> pages 122-136, </pages> <year> 1991. </year>
Reference-contexts: Continuation passing mechanisms are common in coarse-grain data flow systems [23]. The *T project is developing specialized hardware assistance to reduce system overhead in support of their coarse-grain data flow model, which is nonetheless at a finer granularity than can be accommodated on most MIMD multiprocessors. Draves et al. <ref> [9] </ref> describe the use of continuations to improve the performance of the Mach kernel and to decrease the amount of memory that has to be devoted to kernel stacks. <p> This supports the argument in favor of integrating synchronization and scheduling using continuation-passing in parallel programs. While templates are useful for fine-grain parallel programs, they also are useful for more coarsely-partitioned progams, especially given recent trends in processor architecture. While Draves et al. <ref> [9] </ref> transformed a Mach kernel to continuation-passing style by hand, we do not think that hand transformation is an appropriate long-term strategy. One alternative would be to integrate object-affinity scheduling with compiler-generated continuation-passing parallel programs [24].
Reference: 10. <author> Derek L. Eager and John Zahorjan. "Chores: </author> <title> Enhanced Run-Time Support for Shared-Memory Parallel Computing". </title> <type> Technical Report 91-08-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: It also supports work heaps of items ("atomic tasks") that run to completion once they are started. Other abstractions consistent with the underlying mechanisms include coarse-grain dataflow computations, actors [1], remote (interprocessor) object invocation [7], and constructs that reduce overhead by scheduling atoms in groups <ref> [10, 22] </ref>. In order to minimize the overhead of creating and scheduling tasks Mercury provides two kinds of schedulable entities, both of which can use O-AS. A Thread 1 These techniques are now common to many well-tuned thread packages.
Reference: 11. <author> J. E. Faust and H. M. Levy. </author> <title> "The Performance of an Object-Oriented Threads Package". </title> <booktitle> In OOPSLA/ECOOP 1990 CONF PROC, </booktitle> <pages> pages 278-288. </pages> <note> ACM SIG-PLAN Notices 25 P:10, </note> <month> October </month> <year> 1990. </year>
Reference-contexts: In order to minimize the overhead of creating and scheduling tasks Mercury provides two kinds of schedulable entities, both of which can use O-AS. A Thread 1 These techniques are now common to many well-tuned thread packages. Faust and Levy <ref> [11] </ref> applied a similar set of modifications to their version of PRESTO. is an abstraction of a process with a fully-instantiated execution context, includ-ing a stack and a restorable processor state. Threads can communicate and synchronize with one another using both blocking and spinning primitives.
Reference: 12. <author> R. J. Fowler and L. I. Kontothanassis. </author> <title> "Improving Processor and Cache Locality in Fine-Grained Parallel Computations using Object-Affinity Scheduling and Continuation Passing". </title> <type> Technical Report TR411, </type> <institution> Department of Computer Science - University of Rochester, </institution> <month> June </month> <year> 1992. </year>
Reference: 13. <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> "The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications". </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This strategy is motivated by the assumption that the thread will continue to access the data it has touched in the recent past and that the data will still be in the cache at that processor. These techniques have met with some success <ref> [8, 13, 26] </ref> in the context of heavyweight Unix processes using coarse-grain styles of multiprogramming. In contrast, a lightweight thread created to perform a single task does not accumulate much history. It may require only one scheduling decision over its entire lifetime. <p> The Mercury mechanisms represent the synthesis of many diverse ideas. 4.1 Affinity Scheduling. Using an analytical model, Squillante and Lazowska [25] concluded that poor cache affinity in multiprocessor scheduling can result in significant degradation of program performance. In contrast, Gupta et al. <ref> [13] </ref> concluded that cache affinity has limited impact on systems with reasonably large scheduling quanta. Vaswani and Zahorjan [26] supported this conclusion by showing that the penalty for reloading the cache is small compared to the system quantum.
Reference: 14. <author> C. Hewitt and R. Atkinson. </author> <title> "Parallelism and Synchronization in Actor Systems". </title> <booktitle> In POPL 4, </booktitle> <pages> pages 267-280, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: A continuation is a function that describes what to do when the currently executing computation terminates. The use of continuations to express parallel and distributed computation has a long history, going back to Hewitt's early papers on Actors <ref> [14, 15] </ref>.
Reference: 15. <author> Carl Hewitt. </author> <title> "Viewing Control Structures as Patterns of Passing Messages". </title> <journal> Artificial Intelligence, </journal> <volume> 8 </volume> <pages> 323-364, </pages> <year> 1977. </year>
Reference-contexts: A continuation is a function that describes what to do when the currently executing computation terminates. The use of continuations to express parallel and distributed computation has a long history, going back to Hewitt's early papers on Actors <ref> [14, 15] </ref>.
Reference: 16. <author> W. Hsieh, P. Wang, and W. E. Weihl. </author> <title> "Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems". </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 239-248, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We defer detailed performance comparisons to the future. The idea of migrating a computation to preserve locality, which in Mercury takes the form of remote invocation, also has been addressed recently by Hsieh et al. <ref> [16] </ref>. Based on simulations of large, scalable multiprocessors, they report no benefit from computation migration. This contrasts with the limited but significant improvement that we observe under Mercury on small bus-based machines.
Reference: 17. <author> L. I. Kontothanassis. </author> <title> "The Mercury User's Manual". </title> <type> Technical Report TR465, </type> <institution> Department of Computer Science University of Rochester, </institution> <month> September </month> <year> 1993. </year>
Reference: 18. <author> T. J. LeBlanc. </author> <title> "Shared Memory Versus Message Passing in a Tightly-Coupled Multiprocessor: A Case Study". </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 463-466, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: This involves coarse-grain partitioning of the problem (one process per processor) with explicit communication and synchronization among processes. The use of this coarse-grain, explicitly distributed style of programming usually yields excellent performance, so it is very common in practice and is advocated in the research literature <ref> [18, 20] </ref>. On the other hand, programmer time is extremely expensive; thus, processor performance means little if the system cannot be programmed easily and effectively. To control programming cost, it is vital that programmers be able to a programming style that is natural for the problem at hand.
Reference: 19. <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor". </title> <booktitle> In Proceedings of the 17th International Conference in Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In COOL, calling a function prefixed with the keyword parallel forks a thread to execute the function asynchronously. Join synchronization is done explicitly using condition variables. Intended for use on distributed memory machines such as DASH <ref> [19] </ref>, object-affinity scheduling in COOL associates objects with processors. A thread executing a method is scheduled on the invoked object's processor. The programmer also can specify explicit affinities among objects, as well as create affinity sets of tasks that are scheduled back-to-back on one node to exploit temporal locality.
Reference: 20. <author> Calvin Lin and Lawrence Snyder. </author> <title> "A comparison of Programming Models for Shared Memory Multiprocessors". </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 163-170, </pages> <year> 1990. </year>
Reference-contexts: This involves coarse-grain partitioning of the problem (one process per processor) with explicit communication and synchronization among processes. The use of this coarse-grain, explicitly distributed style of programming usually yields excellent performance, so it is very common in practice and is advocated in the research literature <ref> [18, 20] </ref>. On the other hand, programmer time is extremely expensive; thus, processor performance means little if the system cannot be programmed easily and effectively. To control programming cost, it is vital that programmers be able to a programming style that is natural for the problem at hand.
Reference: 21. <author> H. E. Lu and P. S. P. Wang. </author> <title> "A comment on a Fast Parallel Algorithm for Thinning Digital Patterns". </title> <journal> Communications of the ACM, </journal> <month> March </month> <year> 1986. </year>
Reference-contexts: Digital Thinning speedup on the SGI Challenge under different scheduling policies affinity scheduling has no opportunity to affect execution time one way or the other, while object-affinity scheduling results in performance approaching that of a hand-optimized program. Thinning of Digital Images Thinning of Digital Images <ref> [21] </ref> operates on an image encoded as a Boolean (short integer) matrix. Thinning is done by changing isolated 1's to 0's, and processing stops when no more changes are possible. Each phase is divided into two subphases, each of which uses slightly different definition of "isolated".
Reference: 22. <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> "Load Balancing versus Locality Management in Shared-Memory Multiprocessors". </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 258-267, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: It also supports work heaps of items ("atomic tasks") that run to completion once they are started. Other abstractions consistent with the underlying mechanisms include coarse-grain dataflow computations, actors [1], remote (interprocessor) object invocation [7], and constructs that reduce overhead by scheduling atoms in groups <ref> [10, 22] </ref>. In order to minimize the overhead of creating and scheduling tasks Mercury provides two kinds of schedulable entities, both of which can use O-AS. A Thread 1 These techniques are now common to many well-tuned thread packages.
Reference: 23. <author> R. S. Nikhil and G. M. Papadopoulos. </author> <title> "*T: A Multithreaded Massively Parallel Architecture". </title> <booktitle> In The 19th annual international symposium on computer architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In contrast, Mercury provides mechanisms for the dynamic construction and modification of graphs of templates. Unlike Mercury, Auto-Scheduling does not address the issue of scheduling tasks and threads to improve locality of reference across a distributed memory hierarchy. Continuation passing mechanisms are common in coarse-grain data flow systems <ref> [23] </ref>. The *T project is developing specialized hardware assistance to reduce system overhead in support of their coarse-grain data flow model, which is nonetheless at a finer granularity than can be accommodated on most MIMD multiprocessors.
Reference: 24. <author> Constantine D. Polychronopoulos. </author> <title> "Auto-Scheduling: Control Flow and Data Flow Come Together". </title> <type> Technical Report CSRD RPT 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: A continuation is a function that describes what to do when the currently executing computation terminates. The use of continuations to express parallel and distributed computation has a long history, going back to Hewitt's early papers on Actors [14, 15]. Polychronopoulos <ref> [24] </ref> developed a scheme called Auto-Scheduling in which a program is compiled into a fine-grain, static task 3 This ability stems from the existence of operations to form templates into groups, all of whose members on each node are enabled and enqueued in a single operation. graph in which each node <p> While Draves et al. [9] transformed a Mach kernel to continuation-passing style by hand, we do not think that hand transformation is an appropriate long-term strategy. One alternative would be to integrate object-affinity scheduling with compiler-generated continuation-passing parallel programs <ref> [24] </ref>. Another possibility worth (re)examining is the design of program ming language abstractions that encourage a continuation-passing coordination in otherwise coarse-grain parallel programs. The object-affinity and template mechanisms are cheap enough to consider using them to implement interprocessor invocation.
Reference: 25. <author> M. S. Squillante. </author> <title> Issues in Shared-Memory Multiprocessor Scheduling: A Performance Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Each virtual processor implemented by a Unix process has its own ready queue. A user-level thread is associated by default with the queue of the processor on which it last ran, which effectively results in a "last processor" cache affinity scheduling policy <ref> [25] </ref>. An explicit relocation operation migrates a thread to a specific (virtual) processor. To allow automatic load sharing, processors look for work in remote queues when both their local ready queues (threads and templates) and the central queues are empty. <p> We believe that a more efficient and stable lock implementation on the Challenge would help demonstrate the benefits of remote invocation for locked shared data structures. 4 Related Work. The Mercury mechanisms represent the synthesis of many diverse ideas. 4.1 Affinity Scheduling. Using an analytical model, Squillante and Lazowska <ref> [25] </ref> concluded that poor cache affinity in multiprocessor scheduling can result in significant degradation of program performance. In contrast, Gupta et al. [13] concluded that cache affinity has limited impact on systems with reasonably large scheduling quanta.
Reference: 26. <author> Raj Vaswani and John Zahorjan. </author> <title> "The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed Shared Memory Multiprocessors". </title> <booktitle> In Proceedings of the thirteenth ACM synposium on operating systems principles, </booktitle> <pages> pages 26-40, </pages> <year> 1991. </year>
Reference-contexts: This strategy is motivated by the assumption that the thread will continue to access the data it has touched in the recent past and that the data will still be in the cache at that processor. These techniques have met with some success <ref> [8, 13, 26] </ref> in the context of heavyweight Unix processes using coarse-grain styles of multiprogramming. In contrast, a lightweight thread created to perform a single task does not accumulate much history. It may require only one scheduling decision over its entire lifetime. <p> Using an analytical model, Squillante and Lazowska [25] concluded that poor cache affinity in multiprocessor scheduling can result in significant degradation of program performance. In contrast, Gupta et al. [13] concluded that cache affinity has limited impact on systems with reasonably large scheduling quanta. Vaswani and Zahorjan <ref> [26] </ref> supported this conclusion by showing that the penalty for reloading the cache is small compared to the system quantum. They also found little benefit in trying to exploit cache affinity in programs that have little reuse of writable shared data.
References-found: 26

