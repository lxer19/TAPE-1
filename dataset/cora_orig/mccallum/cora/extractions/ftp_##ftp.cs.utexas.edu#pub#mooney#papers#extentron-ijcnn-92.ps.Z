URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/extentron-ijcnn-92.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: baffes@cs.utexas.edu, zelle@cs.utexas.edu  
Title: Growing Layers of Perceptrons: Introducing the Extentron Algorithm  
Author: Paul T. Baffes and John M. Zelle 
Address: Austin, Texas 78712  
Affiliation: Department of Computer Sciences, University of Texas at Austin,  
Abstract: vations of perceptrons: (1) when the perceptron learning algorithm cycles among hyperplanes, the hyperplanes may be compared to select one that gives a best split of the examples, and (2) it is always possible for the perceptron to build a hyper- plane that separates at least one example from all the rest. We describe the Extentron which grows multi-layer networks capable of distinguishing non- linearly-separable data using the simple perceptron rule for linear threshold units. The resulting algorithm is simple, very fast, scales well to large prob - lems, retains the convergence properties of the perceptron, and can be completely specified using only two parameters. Results are presented comparing the Extentron to other neural network paradigms and to symbolic learning systems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fisher, D. and McKusick, K. </author> <year> (1989). </year> <title> An ex-perimental comparison of ID3 and backprop-agation. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 788-793. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The soybean data set is known not to be linearly separable (though subsets of this data are linearly separable), and the DNA data set has been shown to be difficult for symbolic systems to learn <ref> (Fisher and McKusick, 1989) </ref>. The experiments proceeded as follows. Each data set was divided into training and test sets. Training sets were further divided into subsets, so that the algorithms could be evaluated with varying amounts of training data. After training, each system's accuracy was recorded on the test set. <p> The Extentron also performs better than the simple perceptron on the soybean data set, which was expected since the soybean data set is not linearly separable. In the DNA tests, the Extentron's superiority to ID3 and AQ duplicates the results reported by Fisher and McKusick <ref> (Fisher and McKusick, 1989) </ref> that neural networks are better at learning "N-of- M" concepts than symbolic algorithms. The soybean tests and statistical results were run on a TI Explorer 1.
Reference: <author> Hawley, D. and McClure, W. </author> <year> (1983). </year> <title> Compilation and analysis of Escherichia coli promoter DNA sequences. </title> <journal> Nucleic Acids Research, 11:22372255. </journal>
Reference-contexts: Two differ <p>- ent data sets were used in these experiments, one for soybean diseases (Reinke, 1984) and the other for DNA promoter sequences <ref> (Hawley and McClure, 1983) </ref>. These data sets involve, respectively: 50 and 57 features; 562 and 106 examples; 15 and 2 categories. After translation into a format usable by an Extentron the soybean data set had 79 inputs and the DNA data set had 228 inputs.
Reference: <author> Lang, K. J. and Witbrock, M. J. </author> <year> (1988). </year> <title> Learning to tell two spirals apart. </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle>
Reference-contexts: For the XOR problem, the Extentron completed training in 0.03 seconds running on a SPARC Station 1 under Sun Common Lisp (version 4.0). We also ran an Extentron on Lang and Witbrock's two-spiral problem <ref> (Lang and Witbrock, 1988) </ref>. The Extentron managed to completely learn the exam <p>- 3 ples, but only after growing 65 layers.
Reference: <author> Michalski, R. and Larson, J. </author> <year> (1983). </year> <title> Incremental generation of VL1 hypothesis: The underlying methodology and the description of the pro-gram AQ11. </title> <type> Technical Report ISG 83-5: </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: Additionally, the random seeds for the back propagation algorithm were reset for each run. Training speed, testing speed, training set accuracy and test set accuracy were recorded for each run. Experiments were conducted with implementations of the following algorithms: back propagation, ID3, AQ <ref> (Michalski and Larson, 1983) </ref>, perceptron, and Extentron. For more details on the specific implementations of ID3 and back propagation, see (Mooney et al., 1990).
Reference: <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. Expanded edition. </publisher>
Reference-contexts: 1 Introduction It is well known that the simple perceptron algorithm (Rosenblatt, 1958) is unable to represent classifications which are not linearly separable <ref> (Minsky and Papert, 1988) </ref>. However, the perceptron does exhibit two very useful properties. First, since there are no hidden layers, designing the topology of the network is not an issue for the user. One simply uses the size of the input-output pairs to determine the form of the network.
Reference: <author> Mooney, R., Shavlik, J. W., Towell, G., and Grove, A. </author> <year> (1990). </year> <title> An experimental comparison of sym-bolic and connectionist learning algorithms. </title> <address> pages 171-176. </address>
Reference-contexts: The Extentron managed to completely learn the exam <p>- 3 ples, but only after growing 65 layers. To test the Extentron against other classification algorithms, we duplicated some of the experiments run by Mooney et. al. <ref> (Mooney et al., 1990) </ref> and Shavlik et. al. (Shavlik et al., 1991). Two differ <p>- ent data sets were used in these experiments, one for soybean diseases (Reinke, 1984) and the other for DNA promoter sequences (Hawley and McClure, 1983). <p> Training speed, testing speed, training set accuracy and test set accuracy were recorded for each run. Experiments were conducted with implementations of the following algorithms: back propagation, ID3, AQ (Michalski and Larson, 1983), perceptron, and Extentron. For more details on the specific implementations of ID3 and back propagation, see <ref> (Mooney et al., 1990) </ref>. Statistical significance was measured using a Student t-test for paired difference of means at the 0.05 level of confidence (i.e. 95% certainty that the differences are not due to random chance).
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: Finally, there are numerous symbolic classification algorithms which can be used to perform the same tasks as an Extentron. One such example is Quinlan's ID3 algorithm <ref> (Quinlan, 1986) </ref>. As with perceptron trees, ID3 forms a decision tree to classify the input examples. However the internal nodes of ID3 use an information theoretic measure to determine a split in the data.
Reference: <author> Reinke, R. </author> <year> (1984). </year> <title> Knowledge acquisition and re-finement tools for the advise meta-expert sys-tem. </title> <type> Master's thesis: </type> <institution> University of Illinios at Urbana-Champaign. </institution>
Reference-contexts: To test the Extentron against other classification algorithms, we duplicated some of the experiments run by Mooney et. al. (Mooney et al., 1990) and Shavlik et. al. (Shavlik et al., 1991). Two differ <p>- ent data sets were used in these experiments, one for soybean diseases <ref> (Reinke, 1984) </ref> and the other for DNA promoter sequences (Hawley and McClure, 1983). These data sets involve, respectively: 50 and 57 features; 562 and 106 examples; 15 and 2 categories.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perceptron: A proba-bilistic model for information storage and or-ganization in the brain. </title> <journal> Psychological Review, </journal> (65):386-408. 
Reference-contexts: 1 Introduction It is well known that the simple perceptron algorithm <ref> (Rosenblatt, 1958) </ref> is unable to represent classifications which are not linearly separable (Minsky and Papert, 1988). However, the perceptron does exhibit two very useful properties. First, since there are no hidden layers, designing the topology of the network is not an issue for the user.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Thus two subtrees of the decision tree may be forced to relearn the same hy- perplane split. In short, by extending the inputs, each layer of an Extentron network is able to use any of the features learned to that point. Back propagation <ref> (Rumelhart et al., 1986) </ref> can be compared to the Extentron using several criteria. Back propagation is a more general algorithm, due to the fact that real-valued outputs can be generated (as opposed to the binary outputs of the simple perceptron).
Reference: <author> Shavlik, J. W., Mooney, R. J., and Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and neural learning algo-rithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference-contexts: The Extentron managed to completely learn the exam <p>- 3 ples, but only after growing 65 layers. To test the Extentron against other classification algorithms, we duplicated some of the experiments run by Mooney et. al. (Mooney et al., 1990) and Shavlik et. al. <ref> (Shavlik et al., 1991) </ref>. Two differ <p>- ent data sets were used in these experiments, one for soybean diseases (Reinke, 1984) and the other for DNA promoter sequences (Hawley and McClure, 1983). These data sets involve, respectively: 50 and 57 features; 562 and 106 examples; 15 and 2 categories.
Reference: <author> Towell, G., Shavlik, J., and Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> volume 2, </booktitle> <pages> pages 861-866. </pages>
Reference-contexts: We also tested the Extentron by repeating the standard "leave-one-out" (cross-validation) method performed by Towell et. al. on the DNA data set <ref> (Towell et al., 1990) </ref>. This experiment proceeds by training with N 1 examples (where N is the size of the data set) and testing with the example left out. This process is repeated once for each example in the data set.
Reference: <author> Utgoff, P. E. </author> <year> (1988). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 601-606. </pages>
Reference: <author> Utgoff, P. E. and Brodley, C. E. </author> <year> (1990). </year> <title> An incre-mental method for finding multivariate splits for decision trees. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 58-65. 6 </pages>
References-found: 14

