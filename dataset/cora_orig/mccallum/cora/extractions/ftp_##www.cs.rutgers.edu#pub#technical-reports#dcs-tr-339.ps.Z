URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-339.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fshokoufa,sveng@cs.rutgers.edu  marsic@caip.rutgers.edu  
Title: View-Based Object Recognition Using Saliency Maps  
Author: Ali Shokoufandeh Ivan Marsic and Sven J. Dickinson 
Keyword: View-Based Object Recognition, Shape Representation and Recovery, Graph Matching.  
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Department of Computer Science and Center for Cognitive Science Rutgers University  Department of Electrical and Computer Engineering Rutgers University  
Abstract: We introduce a novel view-based object representation, called the saliency map graph (SMG), which captures the salient regions of an object view at multiple scales using a wavelet transform. This compact representation is highly invariant to translation, rotation (image and depth), and scaling, and offers the locality of representation required for occluded object recognition. To compare two saliency map graphs, we introduce two graph similarity algorithms. The first computes the topological similarity between two SMG's, providing a coarse-level matching of two graphs. The second computes the geometrical similarity between two SMG's, providing a fine-level matching of two graphs. We test and compare these two algorithms on a large database of model object views. ? A condensed version of this paper will appear in the IEEE International Conference on Computer Vision, Bombay, January 1998, under the title, "View-Based Object Matching." 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Belhumeur and D. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 270-277, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Recent results have shown some progress towards solving these problems, e.g., the work of Belheumer and Kriegman <ref> [1] </ref> (limited invariance to illumination changes) and the work of Leonardis and Bischoff [15] and Schmid and Mohr [24] (limited invariance to occlusion). Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. <p> Traditional view-based object representations that are image based, e.g., <ref> [29, 19, 1, 15, 4 24, 23] </ref>, are neither coarse-to-fine nor invariant to the above transformations due to the global nature of their representations (although some offer limited invariance to particular transformations). However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required.
Reference: [2] <author> J. Burns and L. Kitchen. </author> <title> Recognition in 2D images of 3D objects from large model bases using prediction hierarchies. </title> <booktitle> In Proceedings, International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 763-766, </pages> <address> Milan, Italy, </address> <year> 1987. </year>
Reference-contexts: Many approaches to view-based modeling represent each view as a collection of extracted features, such as extracted line segments, curves, corners, line groups, regions, or surfaces (Ikeuchi and Kanade [11], Burns and Kitchen <ref> [2] </ref>, Ullman and Basri [30], Dickinson et al. [7], and Pope and Lowe [21]). The success of these view-based recognition systems depends on the extent to which they can extract their requisite features. <p> However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required. Systems based on more invariant view-based image descriptions, e.g., <ref> [11, 2, 30, 7, 21] </ref>, have relied on complex feature extraction (e.g., edges, lines, regions, etc.) which is not only unreliable but often requires domain-specific parameter tuning.
Reference: [3] <author> P. J. Burt. </author> <title> Attention Mechanisms for Vision in a Dynamic World. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, Vol.1, </booktitle> <pages> pages 977-987, </pages> <address> The Hague, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> The advantage of the wavelet decomposition lies in its effective time (space)- frequency (scale) localization. Unlike other image transforms, e.g., <ref> [3, 4] </ref>, which spread the information across their basis functions, the wavelet transform allows us to compute better localized object representations.
Reference: [4] <author> J. Crowley and A. Parker. </author> <title> A representation for shape based on peaks and ridges in the difference of low-pass transform. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6(2) </volume> <pages> 156-169, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Their bottom-up descriptions of the image are not only global, offering little means for segmenting an image into objects or parts, but offer little invariance to occlusion, object deformation, and other transformations. An approach similar to the approach we will present is due to Crowley et al. <ref> [5, 4, 6] </ref>. From a Laplacian pyramid computed on an image, peaks and ridges at each scale are detected as local maxima. The peaks are then linked together to form a tree structure, from which a set of peaks paths are extracted, corresponding to the branches of the tree. <p> The advantage of the wavelet decomposition lies in its effective time (space)- frequency (scale) localization. Unlike other image transforms, e.g., <ref> [3, 4] </ref>, which spread the information across their basis functions, the wavelet transform allows us to compute better localized object representations. <p> For example, if a non-circular shape's variation in diameter does not reach neighboring scales above or below the current scale, then a circularly-symmetric filter, such as that used by Crowley <ref> [5, 4, 6] </ref>, will give a weak response for the shape. In our approach, however, the 1-D filters are slightly adjusted in width (bounded by neighboring scales). The result is a cluster of oriented peaks from which we compute the 2-D shape's location as the centroid of these peaks. <p> It is imperative that each method support a measure of subgraph similarity in order to support occluded object matching. 1 This graph structure is more compact than Crowley's Laplacian pyramid approach <ref> [5, 4, 6] </ref> since he represents every local peak in his graph, whereas we retain peaks only at their most characteristic scale. 13 3.1 Problem Formulation Two graphs G = (V; E) and G 0 = (V 0 ; E 0 ) are said to be isomorphic if there exists a
Reference: [5] <author> J. L. Crowley. </author> <title> A Multiresolution Representation for Shape. </title> <editor> In A. Rosenfeld, editor, </editor> <booktitle> Multiresolution Image Processing and Analysis, </booktitle> <pages> pages 169-189. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Their bottom-up descriptions of the image are not only global, offering little means for segmenting an image into objects or parts, but offer little invariance to occlusion, object deformation, and other transformations. An approach similar to the approach we will present is due to Crowley et al. <ref> [5, 4, 6] </ref>. From a Laplacian pyramid computed on an image, peaks and ridges at each scale are detected as local maxima. The peaks are then linked together to form a tree structure, from which a set of peaks paths are extracted, corresponding to the branches of the tree. <p> For example, if a non-circular shape's variation in diameter does not reach neighboring scales above or below the current scale, then a circularly-symmetric filter, such as that used by Crowley <ref> [5, 4, 6] </ref>, will give a weak response for the shape. In our approach, however, the 1-D filters are slightly adjusted in width (bounded by neighboring scales). The result is a cluster of oriented peaks from which we compute the 2-D shape's location as the centroid of these peaks. <p> It is imperative that each method support a measure of subgraph similarity in order to support occluded object matching. 1 This graph structure is more compact than Crowley's Laplacian pyramid approach <ref> [5, 4, 6] </ref> since he represents every local peak in his graph, whereas we retain peaks only at their most characteristic scale. 13 3.1 Problem Formulation Two graphs G = (V; E) and G 0 = (V 0 ; E 0 ) are said to be isomorphic if there exists a
Reference: [6] <author> J. L. Crowley and A. C. Sanderson. </author> <title> Multiple Resolution Representation and Probabilis--tic Matching of 2-D Gray-Scale Shape. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 9(1) </volume> <pages> 113-121, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Their bottom-up descriptions of the image are not only global, offering little means for segmenting an image into objects or parts, but offer little invariance to occlusion, object deformation, and other transformations. An approach similar to the approach we will present is due to Crowley et al. <ref> [5, 4, 6] </ref>. From a Laplacian pyramid computed on an image, peaks and ridges at each scale are detected as local maxima. The peaks are then linked together to form a tree structure, from which a set of peaks paths are extracted, corresponding to the branches of the tree. <p> For example, if a non-circular shape's variation in diameter does not reach neighboring scales above or below the current scale, then a circularly-symmetric filter, such as that used by Crowley <ref> [5, 4, 6] </ref>, will give a weak response for the shape. In our approach, however, the 1-D filters are slightly adjusted in width (bounded by neighboring scales). The result is a cluster of oriented peaks from which we compute the 2-D shape's location as the centroid of these peaks. <p> An approach similar to this was proposed by Crowley for detecting ridges in a Laplacian pyramid <ref> [6] </ref>. 2.5 The Saliency Map as a Graph The computed saliency map can be represented as a hierarchical graph with nodes representing saliency regions and specifying region location (in the image), region size, region saliency, and scale level. <p> It is imperative that each method support a measure of subgraph similarity in order to support occluded object matching. 1 This graph structure is more compact than Crowley's Laplacian pyramid approach <ref> [5, 4, 6] </ref> since he represents every local peak in his graph, whereas we retain peaks only at their most characteristic scale. 13 3.1 Problem Formulation Two graphs G = (V; E) and G 0 = (V 0 ; E 0 ) are said to be isomorphic if there exists a
Reference: [7] <author> S. Dickinson, A. Pentland, and A. Rosenfeld. </author> <title> 3-D shape recovery using distributed aspect matching. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 174-198, </pages> <year> 1992. </year>
Reference-contexts: Many approaches to view-based modeling represent each view as a collection of extracted features, such as extracted line segments, curves, corners, line groups, regions, or surfaces (Ikeuchi and Kanade [11], Burns and Kitchen [2], Ullman and Basri [30], Dickinson et al. <ref> [7] </ref>, and Pope and Lowe [21]). The success of these view-based recognition systems depends on the extent to which they can extract their requisite features. With real images of real objects in unconstrained environments, the extraction of such features can be both time consuming and unreliable. <p> However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required. Systems based on more invariant view-based image descriptions, e.g., <ref> [11, 2, 30, 7, 21] </ref>, have relied on complex feature extraction (e.g., edges, lines, regions, etc.) which is not only unreliable but often requires domain-specific parameter tuning.
Reference: [8] <author> G. G. E. Mjolsness and P. Anandan. </author> <title> Optimization in model matching and perceptual organization. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 218-229, </pages> <year> 1989. </year>
Reference-contexts: Excluding the second component, our measure is similar to the objective function used in the f0; 1g integer programming formulation of the largest subgraph isomorphism problem <ref> [13, 8] </ref>.
Reference: [9] <author> E. Edmonds. </author> <title> Paths, trees, and flowers. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 17 </volume> <pages> 449-467, </pages> <year> 1965. </year>
Reference-contexts: (A 1 ; B 1 ; E) with a weight function defined for edge (u; v) (u 2 A 1 and v 2 B 1 ) as w (u; v) = js (v) s (u)j. 2 Next, we find a maximum cardinality, minimum weight matching M 1 in G using <ref> [9] </ref>. All the matched vertices are mapped to each other; that is, we define f (x) = y if (x; y) is a matching edge in M 1 . The remainder of the algorithm proceeds in phases as follows, as shown in Figure 6.
Reference: [10] <author> H. Gabow, M. Goemans, and D. Williamson. </author> <title> An efficient approximate algorithm for survivable network design problems. </title> <booktitle> Proc. of the Third MPS Conference on Integer Programming and Combinatorial Optimization, </booktitle> <pages> pages 57-74, </pages> <year> 1993. </year>
Reference-contexts: The time complexity for finding such a matching in a weighted bipartite graph with n vertices is O (n 2 p n log log n) time, using the scaling algorithm of Gabow, Gomans and Williamson <ref> [10] </ref>.
Reference: [11] <author> K. Ikeuchi and T. Kanade. </author> <title> Automatic generation of object recognition programs. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 1016-1035, </pages> <year> 1988. </year>
Reference-contexts: Many approaches to view-based modeling represent each view as a collection of extracted features, such as extracted line segments, curves, corners, line groups, regions, or surfaces (Ikeuchi and Kanade <ref> [11] </ref>, Burns and Kitchen [2], Ullman and Basri [30], Dickinson et al. [7], and Pope and Lowe [21]). The success of these view-based recognition systems depends on the extent to which they can extract their requisite features. <p> However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required. Systems based on more invariant view-based image descriptions, e.g., <ref> [11, 2, 30, 7, 21] </ref>, have relied on complex feature extraction (e.g., edges, lines, regions, etc.) which is not only unreliable but often requires domain-specific parameter tuning.
Reference: [12] <author> M. Jagersand. </author> <title> Saliency Maps and Attention Selection in Scale and Spatial Coordinates: An Information Theoretic Approach. </title> <booktitle> In Proceedings of the 5th International Conference on Computer Vision, </booktitle> <pages> pages 195-202, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. Jagersand <ref> [12] </ref> uses an information theoretic measure to compute "informativeness" of image regions at different scales, while others have defined some measure of "importance" and used it to drive an attention process [20, 27, 28].
Reference: [13] <author> J. Kobler. </author> <title> The Graph Isomorphism Problem: Its Structural Complexity. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: Excluding the second component, our measure is similar to the objective function used in the f0; 1g integer programming formulation of the largest subgraph isomorphism problem <ref> [13, 8] </ref>.
Reference: [14] <author> J. Koenderink and A. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction The view-based approach to 3-D object recognition represents an object as a collection of 2-D views, sometimes called aspects or characteristic views <ref> [14] </ref>. The advantage of such an approach is that it avoids having to construct a 3-D model of an object as well as having to make 3-D inferences from 2-D features.
Reference: [15] <author> A. Leonardis and H. Bischoff. </author> <title> Dealing with occlusions in the eigenspace approach. </title> <booktitle> In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 453-458, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year> <month> 32 </month>
Reference-contexts: Recent results have shown some progress towards solving these problems, e.g., the work of Belheumer and Kriegman [1] (limited invariance to illumination changes) and the work of Leonardis and Bischoff <ref> [15] </ref> and Schmid and Mohr [24] (limited invariance to occlusion). Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. <p> Traditional view-based object representations that are image based, e.g., <ref> [29, 19, 1, 15, 4 24, 23] </ref>, are neither coarse-to-fine nor invariant to the above transformations due to the global nature of their representations (although some offer limited invariance to particular transformations). However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required.
Reference: [16] <author> T. Lindeberg. </author> <title> Detecting Salient Blob-Like Image Structures and Their Scales With a Scale-Space Primal Sketch|A Method for Focus-of-Attention. </title> <journal> International Journal of Computer Vision, </journal> <volume> 11(3) </volume> <pages> 283-318, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space <ref> [16] </ref>. Jagersand [12] uses an information theoretic measure to compute "informativeness" of image regions at different scales, while others have defined some measure of "importance" and used it to drive an attention process [20, 27, 28].
Reference: [17] <author> S. Mallat and W. L. Hwang. </author> <title> Singularity Detection and Processing with Wavelets. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(2) </volume> <pages> 617-643, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: At this scale, the object's response resembles the wavelet basis function impulse response. The following subsections will explore scale-space cell detection in greater detail. 2.2 Scale-Space Cells in One Dimension To illustrate the detection of scale-space cells, consider the one-dimensional signal in Figure 2, using the wavelets described in <ref> [17] </ref>. Any object behaves like a point at all scales coarser than its characteristic scale I.
Reference: [18] <author> I. Marsic. </author> <title> Data-Driven Shifts of Attention in Wavelet Scale Space. </title> <type> Technical Report CAIP-TR-166, </type> <institution> CAIP Center, Rutgers University, </institution> <address> Piscataway, NJ, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Figure 1), we select the scale which captures the most efficient encoding of an object's salient shape; above the chosen scale, extraneous information is encoded, while below the chosen scale, the object is overly "blurred." The region defining the object at the chosen scale is called the scale-space cell (SSC) <ref> [18] </ref>. <p> Therefore, in the 2-D case, we apply the 1-D procedure in a number of directions and search for clusters of 1-D centers, as shown in Figure 3 (a). Our entire procedure for detecting the SSC's in an image therefore consists of the following four steps <ref> [18] </ref>: 7 analyzing wavelets i;k at the corresponding scales. The middle column shows two examples (A and B) of a function f (x) (top) and their wavelet transforms (below).
Reference: [19] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearance. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14 </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: Turk and Pentland focused on the domain of faces and therefore did not require a large set of model views for each face. Nayar and Murase extended this work to general 3-D objects where a dense set of views was acquired for each object <ref> [19] </ref>. <p> Traditional view-based object representations that are image based, e.g., <ref> [29, 19, 1, 15, 4 24, 23] </ref>, are neither coarse-to-fine nor invariant to the above transformations due to the global nature of their representations (although some offer limited invariance to particular transformations). However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required.
Reference: [20] <author> B. Olshausen, C. Anderson, and D. V. Essen. </author> <title> A Neurobiological Model of Visual Attention and Invariant Pattern Recognition Based on Dynamic Routing of Information. </title> <journal> Journal of Neurosciences, </journal> <volume> 13(11) </volume> <pages> 4700-4719, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. Jagersand [12] uses an information theoretic measure to compute "informativeness" of image regions at different scales, while others have defined some measure of "importance" and used it to drive an attention process <ref> [20, 27, 28] </ref>. Although suitable for locating objects in images for further processing, the above multiscale descriptions, often called saliency maps, lose the detailed shape information required for object recognition. Some multiscale image descriptions have been used to locate a particular target object in the image.
Reference: [21] <author> A. Pope and D. Lowe. </author> <title> Learning object recognition models from images. </title> <booktitle> In Proceedings, IEEE International Conference on Computer Vision, </booktitle> <pages> pages 296-301, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many approaches to view-based modeling represent each view as a collection of extracted features, such as extracted line segments, curves, corners, line groups, regions, or surfaces (Ikeuchi and Kanade [11], Burns and Kitchen [2], Ullman and Basri [30], Dickinson et al. [7], and Pope and Lowe <ref> [21] </ref>). The success of these view-based recognition systems depends on the extent to which they can extract their requisite features. With real images of real objects in unconstrained environments, the extraction of such features can be both time consuming and unreliable. <p> However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required. Systems based on more invariant view-based image descriptions, e.g., <ref> [11, 2, 30, 7, 21] </ref>, have relied on complex feature extraction (e.g., edges, lines, regions, etc.) which is not only unreliable but often requires domain-specific parameter tuning.
Reference: [22] <author> F. Preparata and M. Shamos. </author> <title> Computational Geometry. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Repeat this process until either all the vertices at the i-th level of G 0 are matched or the remaining vertices cannot be matched (due to path constraints). The time complexity of the above procedure for level i is O (n 2 log n) <ref> [22] </ref>. For ` levels, the total complexity is O (`n 2 log n), which compares favorably to the O (`n 2 p n log log n) complexity of our current SMGAT algorithm. 3.5 Limitations of the Matching Algorithms There are two major limitations of both matching algorithms.
Reference: [23] <author> R. P. N. Rao, G. J. Zelinsky, M. M. Hayhoe, and D. H. Ballard. </author> <title> Modeling Saccadic Targeting in Visual Search. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 830-836. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Some multiscale image descriptions have been used to locate a particular target object in the image. For example, Rao et al. use correlation to compare a multiscale saliency map of the target object with a multiscale saliency map of the image in order to fixate on the object <ref> [23] </ref>. Although these approaches are effective in finding a target in the image, they, like any template-based approach, do not scale to large object databases. <p> Traditional view-based object representations that are image based, e.g., <ref> [29, 19, 1, 15, 4 24, 23] </ref>, are neither coarse-to-fine nor invariant to the above transformations due to the global nature of their representations (although some offer limited invariance to particular transformations). However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required.
Reference: [24] <author> C. Schmid and R. Mohr. </author> <title> Combining greyvalue invariants with local constraints for object recognition. </title> <booktitle> In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 872-877, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Recent results have shown some progress towards solving these problems, e.g., the work of Belheumer and Kriegman [1] (limited invariance to illumination changes) and the work of Leonardis and Bischoff [15] and Schmid and Mohr <ref> [24] </ref> (limited invariance to occlusion). Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance.
Reference: [25] <author> E. Simoncelli, W. Freeman, E. Adelson, and D. Heeger. </author> <title> Shiftable multi-scale transforms. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(2) </volume> <pages> 587-607, </pages> <year> 1992. </year> <month> 33 </month>
Reference-contexts: Finally, computing these regions and relations requires the setting of very few parameters. 2.1 The Multiscale Wavelet Transform The scale-space image representation that we have selected is based on a multiscale wavelet transform <ref> [25] </ref>. The advantage of the wavelet decomposition lies in its effective time (space)- frequency (scale) localization. Unlike other image transforms, e.g., [3, 4], which spread the information across their basis functions, the wavelet transform allows us to compute better localized object representations.
Reference: [26] <author> E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger. </author> <title> Shiftable Multiscale Transforms. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(2) </volume> <pages> 587-607, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Step 1|Wavelet Transform: Compute the wavelet pyramid of an image with ` dyadic scales using oriented quadrature bandpass filters tuned to 16 different orientations, i.e. fi = 0 ffi ; 22:5 ffi ; 45 ffi ; :::; 337:5 ffi . See <ref> [26] </ref> for a detailed derivation and description of computing the wavelet pyramid using steerable basis filters.
Reference: [27] <author> B. Takacs and H. Wechsler. </author> <title> A Dynamic and Multiresolution Model of Visual Attention and Its Application to Facial Landmark Detection. Computer Vision and Image Understanding, </title> <publisher> (in press). </publisher>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. Jagersand [12] uses an information theoretic measure to compute "informativeness" of image regions at different scales, while others have defined some measure of "importance" and used it to drive an attention process <ref> [20, 27, 28] </ref>. Although suitable for locating objects in images for further processing, the above multiscale descriptions, often called saliency maps, lose the detailed shape information required for object recognition. Some multiscale image descriptions have been used to locate a particular target object in the image.
Reference: [28] <author> J. K. Tsotsos. </author> <title> An Inhibitory Beam for Attentional Selection. </title> <editor> In L. Harris and M. Jenkin, editors, </editor> <booktitle> Spatial Vision in Humans and Robots, </booktitle> <pages> pages 313-331. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1993. </year>
Reference-contexts: Nevertheless, the lack of abstraction from raw image data to the model means that the model defines a very specific object instance. The concept of computing coarse-to-fine image descriptions has much support in the 2 computer vision community; some examples include <ref> [3, 12, 16, 20, 27, 28] </ref>. In some cases, attention models have been developed that use a multiscale description to decide where in the image to apply some operation. Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. <p> Lindeberg has based this selection process on a quantitative analysis of gray-level blobs in scale space [16]. Jagersand [12] uses an information theoretic measure to compute "informativeness" of image regions at different scales, while others have defined some measure of "importance" and used it to drive an attention process <ref> [20, 27, 28] </ref>. Although suitable for locating objects in images for further processing, the above multiscale descriptions, often called saliency maps, lose the detailed shape information required for object recognition. Some multiscale image descriptions have been used to locate a particular target object in the image.
Reference: [29] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: With real images of real objects in unconstrained environments, the extraction of such features can be both time consuming and unreliable. In contrast to the feature-based view-based recognition paradigm, a number of image-based view-based recognition systems have emerged. Beginning with the eigenface approach proposed by Turk and Pentland <ref> [29] </ref>, these image-based approaches avoid extracting complex features from an image; instead, they retain the entire raw image as a single feature in a high-dimensional space. Turk and Pentland focused on the domain of faces and therefore did not require a large set of model views for each face. <p> Traditional view-based object representations that are image based, e.g., <ref> [29, 19, 1, 15, 4 24, 23] </ref>, are neither coarse-to-fine nor invariant to the above transformations due to the global nature of their representations (although some offer limited invariance to particular transformations). However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required.
Reference: [30] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <month> October </month> <year> 1991. </year> <month> 34 </month>
Reference-contexts: Many approaches to view-based modeling represent each view as a collection of extracted features, such as extracted line segments, curves, corners, line groups, regions, or surfaces (Ikeuchi and Kanade [11], Burns and Kitchen [2], Ullman and Basri <ref> [30] </ref>, Dickinson et al. [7], and Pope and Lowe [21]). The success of these view-based recognition systems depends on the extent to which they can extract their requisite features. With real images of real objects in unconstrained environments, the extraction of such features can be both time consuming and unreliable. <p> However, the advantage of these approaches is that complex feature extraction, grouping, or abstraction is not required. Systems based on more invariant view-based image descriptions, e.g., <ref> [11, 2, 30, 7, 21] </ref>, have relied on complex feature extraction (e.g., edges, lines, regions, etc.) which is not only unreliable but often requires domain-specific parameter tuning.
References-found: 30

