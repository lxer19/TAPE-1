URL: http://www.eecs.umich.edu/~optimus/automatica.ps
Refering-URL: http://www.eecs.umich.edu/~optimus/
Root-URL: http://www.cs.umich.edu
Title: Computational Experiments Using Randomized Algorithms for Robust Stability Analysis  
Author: Albert Yoon and Pramod Khargonekar 
Keyword: Key words: randomized algorithms, global optimization, robust stability analysis, real parametric uncertainty  
Address: Ann Arbor, MI 48109-2122, USA  
Affiliation: Dept. of Electrical Engineering and Computer Science The University of Michigan  
Abstract: In this paper, we take a "computational experiments" approach to robust stability analysis problems. Many robust control problems have been shown to be NP hard but in spite of this, it is important to develop effective techniques for solving them. A typical robust stability analysis problem is taken and formulated as an optimization problem to which several optimization algorithms are applied. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ali, A. Torn, and S. Viitanen. </author> <title> A numerical comparison of some modified controlled random search algorithms. </title> <type> Technical Report 98, </type> <institution> Turku Centre for Computer Science, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: (1) Multi-start of Rosenbrock's method [4] (2) Crude and adaptive random search [10] (3) Adaptive partitioned random search [23] (4) Controlled random search <ref> [1] </ref> and shu*ed complex evolution [7] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> Variations on the pure random search such as partitioning or shrinking the input space are made to improve its efficiency. A third algorithm is the controlled random search (CRS) which has many variants. The version used in this study is from <ref> [1] </ref>. Finally, the Shu*ed Complex Evolution (SCE) approach of [7] which uses elements of the CRS and partitioned random search techniques is implemented and tested. <p> The first step is to generate a group of random points over the entire search space. In general, the strategy is to replace the worst points of the group with better points through a selected operation. The Nelder-Mead simplex method may be considered as one form of CRS. In <ref> [1] </ref>, a numerical comparison of several CRS algorithms is presented and a new CRS algorithm is introduced. In light of the positive results of the algorithm presented in [1], it is used in this study. * Controlled Random Search Inputs: N , ffi , tolerance Initialization: Uniformly generate a group, A <p> The Nelder-Mead simplex method may be considered as one form of CRS. In <ref> [1] </ref>, a numerical comparison of several CRS algorithms is presented and a new CRS algorithm is introduced. In light of the positive results of the algorithm presented in [1], it is used in this study. * Controlled Random Search Inputs: N , ffi , tolerance Initialization: Uniformly generate a group, A of N points and evaluate them. <p> Only the crude random search and ARS I are guaranteed to converge in probability to the global optimum as the number of function evaluations increases. On the other hand, ARS II is not even guaranteed to converge to a local optimum. In <ref> [1] </ref>, it is stated that the CRS algorithm is totally heuristic and lacks theoretical convergence properties. According to Subrahmanyam in [22], convergence theorems for the Nelder-Mead algorithm alone are practically nonexistent even though the algorithm has was presented in 1965. <p> Given this, it is unlikely that any theoretical convergence results for the SCE algorithm exist. Tests for the efficiency of the algorithms have mainly been done through the use of numerical experiments. In <ref> [1] </ref>, the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In [16], they are used to compare the Nelder-Mead algorithm with other direct methods and in [17] they are used to propose modifications to the algorithm to improve its efficiency.
Reference: [2] <author> V. Balakrishnan, S. Balemi, and S. Boyd. </author> <title> Computation of the Minimum Stability Degree of Parameter-dependent Linear Systems by a Branch and Bound Algorithm. </title> <booktitle> In Proceedings of 1st IFAC Symposium on Design Methods of Control Systems, </booktitle> <pages> pages 133-138, </pages> <address> Zurich, Switzerland, </address> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: In contrast, a branch and bound technique has been investigated in [15] using upper and lower bounds to compute . Another branch and bound approach to compute minimum stability degree has been developed in <ref> [2] </ref>. The advantage of our formulation over the computation is that frequency gridding is not necessary. Also, the approach in [2] can be modified to compute the smallest destabilizing perturbation through a one parameter iteration of the branch and bound optimization program that computes the minimum stability degree. <p> Another branch and bound approach to compute minimum stability degree has been developed in <ref> [2] </ref>. The advantage of our formulation over the computation is that frequency gridding is not necessary. Also, the approach in [2] can be modified to compute the smallest destabilizing perturbation through a one parameter iteration of the branch and bound optimization program that computes the minimum stability degree. Thus, a direct comparison of our algorithms and the branch and bound based algorithms is not a straightforward matter.
Reference: [3] <author> B. R. Barmish and C. M. Lagoa. </author> <title> The uniform distribution: a rigorous justification for its use in robustness analysis. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3418-3423, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference: [4] <author> M. Bazaraa, H. Sherali, and C. M. Shetty. </author> <title> Nonlinear Programming Theory and Algorithms. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1993. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: (1) Multi-start of Rosenbrock's method <ref> [4] </ref> (2) Crude and adaptive random search [10] (3) Adaptive partitioned random search [23] (4) Controlled random search [1] and shu*ed complex evolution [7] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. <p> A fairly wide range of random search algorithms is tested in this study. The first algorithm is a multistart version of Rosenbrock's line search method. There are several line search methods such as Zangwill's [27] and Hooke and Jeeve's <ref> [4] </ref> but they are not examined here. Apart from selecting the initial conditions randomly, the algorithm is purely deterministic. Another set of algorithms contains the pure random search and some variants of it. <p> All of these algorithms place an upper bound on fl opt . 3.1 Line Searches and Random Multistart There are many line search methods for solving optimization problems. The algorithm of Rosenbrock <ref> [4] </ref> is presented here as an example. It is deterministic and also differs from the random searches in that it moves only a single point; the random algorithms attempt to move a group of points. <p> Rosenbrock's algorithm is attractive because, unlike say Hooke and Jeeve's algorithm <ref> [4] </ref>, it changes its set of line search directions to suit the cost function. Numerical experiments have shown this algorithm to be more efficient than those that do not adapt its line search directions.
Reference: [5] <author> R. Braatz, P. Young, J. Doyle, and M. Morari. </author> <title> Computational complexity of calculation. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 1682-1683, </pages> <address> San Francisco, California, </address> <year> 1993. </year>
Reference-contexts: Experience has shown that these bounds can be arbitrarily conservative and recently, it has been proven that solving these problems is NP hard <ref> [5] </ref>. Therefore a reasonable course of action is to move our focus towards efficient means of coming to an approximate solution and reduce the conservatism of the bounds.
Reference: [6] <author> R. R. E. de Gaston and M. G. Safonov. </author> <title> Exact calculation of the multiloop stability margin. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 33 </volume> <pages> 156-171, </pages> <year> 1988. </year>
Reference: [7] <author> Q. Duan, S. Sorooshian, and V. Gupta. </author> <title> Effective and efficient global optimization for conceptual rainfall-runoff models. </title> <journal> Water Resources Research, </journal> <volume> 28(4) </volume> <pages> 1015-1031, </pages> <year> 1992. </year>
Reference-contexts: The following methods are investigated here: (1) Multi-start of Rosenbrock's method [4] (2) Crude and adaptive random search [10] (3) Adaptive partitioned random search [23] (4) Controlled random search [1] and shu*ed complex evolution <ref> [7] </ref> These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> A third algorithm is the controlled random search (CRS) which has many variants. The version used in this study is from [1]. Finally, the Shu*ed Complex Evolution (SCE) approach of <ref> [7] </ref> which uses elements of the CRS and partitioned random search techniques is implemented and tested. All of these algorithms place an upper bound on fl opt . 3.1 Line Searches and Random Multistart There are many line search methods for solving optimization problems. <p> Step 4 is a dedicated local search about the current best point. The number of points , N , in A is user-defined and a value of 10 (n + 1) is suggested. 3.5 Shu*ed Complex Evolution (SCE) The so-called Shu*ed Complex Evolution (SCE) approach of <ref> [7] </ref> combines ideas of APRS and CRS. The APRS part of the algorithm separates the group of points into several partitions and for a certain period of time the partitions are treated independently (until the points are shu*ed). <p> A simplex step is interpreted as an evolutionary step of a population while shuffling is interpreted as communication and co-operation between populations. The procedure can be found in <ref> [7] </ref> and is outlined below. * Shu*ing Inputs: s (such that s n + 1 where n is the dimension of the input space), p, * Initialization: Let i=1 and evaluate the cost function at sp points where s is the number of samples and p is the number of complexes <p> In 12 this way, all points in the complex have a chance to participate in evolution, but it is the better points that will participate more often. Finally, randomness can enter when an infeasible point is picked due to reflection or contraction. In <ref> [7] </ref> it is suggested that a point randomly chosen from the smallest hypercube that contains the complex replace the infeasible one. The parameters of this approach are s, p, c reflect , c contract , ff, and fi.
Reference: [8] <author> Q. Duan, S. Sorooshian, and V. Gupta. </author> <title> Optimal use of the SCE-UA global optimization method for calibrating watershed models. </title> <journal> Journal of Hydrology, </journal> <volume> 158 </volume> <pages> 265-284, </pages> <year> 1994. </year>
Reference-contexts: A higher fi may also increase robustness at the expense of decreased efficiency while a higher ff may decrease robustness since more worst-point replacements occur before a new simplex is chosen from the complex. The relationships between these parameters and robustness and efficiency are examined in <ref> [8] </ref> and are based on numerical experiments. 3.6 Convergence Results Almost all of the algorithms presented lack convergence results. Only the crude random search and ARS I are guaranteed to converge in probability to the global optimum as the number of function evaluations increases. <p> The rest of the parameter values chosen were ff = 1, fi = 2n+1 and s = 2n + 1 in accordance with the values recommended in <ref> [8] </ref>.
Reference: [9] <author> M. Fu and R. Barmish. </author> <title> Maximal unidirectional perturbation bounds for stability of polynomials and matrices. </title> <journal> Systems & Control Letters, </journal> <volume> 11 </volume> <pages> 173-179, </pages> <year> 1988. </year>
Reference-contexts: We begin with two different formulations of the robust stability analysis problem as optimization problems. One of these formulations utilizes a result on maximal unidirectional perturbation bounds due to Fu and Barmish <ref> [9] </ref>. For global optimization, we restrict our attention to direct methods, where "direct" refers to methods that use only the value of the 2 objective function but no derivative or any other analytical information. Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. <p> Consider all perturbations such that kk 1 is unity and for M = (A; B; C) and ~ 2 R, define A = A + ~BC. Fu and Barmish <ref> [9] </ref> provide an analytic expression for the smallest j~j such that A 0 + ~A 1 is unstable, where A 0 2 R nfin is stable. <p> Unlike the previous example, the unidirectional perturbation-based cost cannot be computed analytically using the expression in <ref> [9] </ref> due to the size of this problem. Even one attempt to evaluate the cost function results in an out of memory error.
Reference: [10] <author> P. Khargonekar and A. Tikku. </author> <title> Randomized algorithms for robust control analysis and synthesis have polynomial complexity. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3470-3475, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference-contexts: There are some results in the literature [19,10,3,24], which indicate that randomized algorithms may provide tractable approaches to the problems of robust stability analysis and performance. Genetic algorithms for robust stability analysis have been investigated in [12,28]. In <ref> [10] </ref>, it was shown that even simple random search has potentially attractive computational complexity properties. <p> Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: (1) Multi-start of Rosenbrock's method [4] (2) Crude and adaptive random search <ref> [10] </ref> (3) Adaptive partitioned random search [23] (4) Controlled random search [1] and shu*ed complex evolution [7] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. <p> One comes from [20] and represents a linearized model of the closed-loop system of a bank-to-turn, air-to-air missile with a linear H 1 controller. The second is the 55 state, 20 uncertain real parameters multivariable robust stability analysis problem investigated in <ref> [10] </ref>. To measure the computational cost, we use the number of function evaluations and flops as metrics. <p> The price that is paid for this robustness is poor efficiency. Many more function evaluations are likely necessary to arrive at a solution of the same quality as one that is found by a more directed technique. Two random search methods, taken from <ref> [10] </ref>, and a method based on that used in [25] are grouped in this section. The first algorithm simply samples from the region of all perturbations with size less than or equal to fl a designated number of times and records the sample that provides the lowest cost. <p> Although the increase in the number of flops for the unidirectional perturbation-based cost function is about two orders of magnitude, the increase in time is only about one order higher. 4.3 Medium-sized Real Example The LTI system, M , obtained from <ref> [10] </ref> is stable and contains 55 states and 20 inputs and outputs, i.e., the number of real parameters is 20. Unlike the previous example, the unidirectional perturbation-based cost cannot be computed analytically using the expression in [9] due to the size of this problem.
Reference: [11] <author> J. Lagarias, J. Reeds, M. Wright, and P. Wright. </author> <title> Convergence properties of the nelder-mead simplex algorithm in low dimensions. </title> <type> Technical Report 96-4-07, </type> <institution> Lucent Technologies, </institution> <month> May </month> <year> 1997. </year> <month> 27 </month>
Reference-contexts: In [1], it is stated that the CRS algorithm is totally heuristic and lacks theoretical convergence properties. According to Subrahmanyam in [22], convergence theorems for the Nelder-Mead algorithm alone are practically nonexistent even though the algorithm has was presented in 1965. Some very recent work in <ref> [11] </ref> has proven the convergence of the algorithm for strictly convex functions in one dimension and shown the difficulty of finding proofs in higher dimensions. Given this, it is unlikely that any theoretical convergence results for the SCE algorithm exist.
Reference: [12] <author> C. Marrison and R. Stengel. </author> <title> The use of random search and genetic algorithms to optimize stochastic robustness functions. </title> <booktitle> In Proceedings of American Control Conference, </booktitle> <pages> pages 1484-1489, </pages> <address> Baltimore, Maryland, </address> <year> 1994. </year>
Reference: [13] <author> J. A. Nelder and R. Mead. </author> <title> A simplex method for function minimization. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 308-313, </pages> <year> 1965. </year>
Reference-contexts: The APRS part of the algorithm separates the group of points into several partitions and for a certain period of time the partitions are treated independently (until the points are shu*ed). The CRS part of the algorithm uses the Nelder-Mead <ref> [13] </ref> simplex algorithm to replace the worst points in each of the partitions. A simplex is a set of points in the input space which don't all lie in a subspace of lower dimension than the input space.
Reference: [14] <author> A. Nemirovskii. </author> <title> Several NP-hard problems arising in robust stability analysis. </title> <journal> Math. of Control, Signals, and Systems, </journal> <volume> 6 </volume> <pages> 99-105, </pages> <year> 1993. </year>
Reference: [15] <author> M. P. Newlin and P. M. Young. </author> <title> Mixed problems and branch and bound techniques. </title> <booktitle> In Proceedings of the 31st IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3175-3180, </pages> <address> Tucson, Arizona, </address> <year> 1992. </year>
Reference-contexts: In fact, the function must be approximated. * It should be noted that one drawback of the optimization methods used in this study is that they provide only upper bounds on the solution. In contrast, a branch and bound technique has been investigated in <ref> [15] </ref> using upper and lower bounds to compute . Another branch and bound approach to compute minimum stability degree has been developed in [2]. The advantage of our formulation over the computation is that frequency gridding is not necessary.
Reference: [16] <author> J. M. Parkinson and D. Hutchinson. </author> <title> A consideration of nongradient algorithms for the unconstrained optimization of function of high dimensionality. </title> <editor> In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 99-113. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Tests for the efficiency of the algorithms have mainly been done through the use of numerical experiments. In [1], the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In <ref> [16] </ref>, they are used to compare the Nelder-Mead algorithm with other direct methods and in [17] they are used to propose modifications to the algorithm to improve its efficiency. Interestingly enough, several studies have provided different conclusions.
Reference: [17] <author> J. M. Parkinson and D. Hutchinson. </author> <title> An investigation into the efficiency of variants of the simplex method. </title> <editor> In F. A. Lootsma, editor, </editor> <booktitle> Numerical Methods for Nonlinear Optimization, </booktitle> <pages> pages 115-136. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: In [1], the proposed CRS algorithm is compared with other CRS algorithms on a set of benchmark problems using numerical experiments. In [16], they are used to compare the Nelder-Mead algorithm with other direct methods and in <ref> [17] </ref> they are used to propose modifications to the algorithm to improve its efficiency. Interestingly enough, several studies have provided different conclusions.
Reference: [18] <author> M. J. D. Powell. </author> <title> An efficient method for finding the minimum of a function of several variables without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 7 </volume> <pages> 155-162, </pages> <year> 1964. </year>
Reference-contexts: Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input space is high while others claim that it is competitive with other methods such as that of Powell <ref> [18] </ref> especially in higher dimensions. Schwefel in [21] has performed a comprehensive numerical study involving many algorithms on a large number of different problems.
Reference: [19] <author> L. Ray and R. Stengel. </author> <title> Stochastic robustness of linear time-invariant control systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 36 </volume> <pages> 82-87, </pages> <year> 1991. </year>
Reference: [20] <author> C. Schumacher and P. Khargonekar. </author> <title> A comparison of missle autopilot designs using H 1 control with gain scheduling and nonlinear dynamic inversion. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2759-2763, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year>
Reference-contexts: It is very difficult to establish theoretical properties of all but the simplest of these algorithms. Therefore, we evaluate the efficacy of the algorithms on two robust stability analysis problems with real-valued uncertainties for this investigation. One comes from <ref> [20] </ref> and represents a linearized model of the closed-loop system of a bank-to-turn, air-to-air missile with a linear H 1 controller. The second is the 55 state, 20 uncertain real parameters multivariable robust stability analysis problem investigated in [10]. <p> This also renders execution time potentially unsuitable for comparison purposes. For interest, however, execution times are mentioned along with the number of function evaluations. 4.2 Real Missile Controller Example The LTI system, M , comes from <ref> [20] </ref> and represents the closed-loop linearized model of a bank-to-turn, air-to-air missile and a H 1 controller. It is stable, has 7 states and 13 inputs and outputs.
Reference: [21] <author> H. Schwefel. </author> <title> Evolution and Optimum Seeking. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: Interestingly enough, several studies have provided different conclusions. Some claim that the simplex method is inefficient and suffer when the dimension of the input space is high while others claim that it is competitive with other methods such as that of Powell [18] especially in higher dimensions. Schwefel in <ref> [21] </ref> has performed a comprehensive numerical study involving many algorithms on a large number of different problems.
Reference: [22] <author> M. B. Subrahmanyam. </author> <title> An extension of the simplex method to constrained nonlinear optimization. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 62(2) </volume> <pages> 311-319, </pages> <year> 1989. </year>
Reference-contexts: On the other hand, ARS II is not even guaranteed to converge to a local optimum. In [1], it is stated that the CRS algorithm is totally heuristic and lacks theoretical convergence properties. According to Subrahmanyam in <ref> [22] </ref>, convergence theorems for the Nelder-Mead algorithm alone are practically nonexistent even though the algorithm has was presented in 1965. Some very recent work in [11] has proven the convergence of the algorithm for strictly convex functions in one dimension and shown the difficulty of finding proofs in higher dimensions.
Reference: [23] <author> Z. B. Tang. </author> <title> Adaptive partitioned random search to global optimization. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(11) </volume> <pages> 2235-2244, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Since robust control problems often lead to nondifferentiable optimization problems, direct methods will have broader applicability. The following methods are investigated here: (1) Multi-start of Rosenbrock's method [4] (2) Crude and adaptive random search [10] (3) Adaptive partitioned random search <ref> [23] </ref> (4) Controlled random search [1] and shu*ed complex evolution [7] These methods are described in some detail in Section 3 where we also discuss the reasons for choosing to focus on these algorithms. It is very difficult to establish theoretical properties of all but the simplest of these algorithms. <p> The radius is user-specified for each iteration. 3.3 Adaptive Partitioned Random Search (APRS) An adaptive partitioned random search divides the input space into subregions and performs several smaller searches. The version of APRS presented here is from <ref> [23] </ref>. In this approach, the goal of finding the subregion with greatest expectation of containing the maximizer is abandoned and replaced with the goal of finding the partition that provides the greatest expected improvement from the current best estimate of the maximum. This expectation is called the promising index. <p> This expectation is called the promising index. Partitions with greater promising indexes are partitioned further while the remaining, inferior are lumped into one partition called the surrounding region. The algorithm of <ref> [23] </ref> for finding a global maximum in a rectangular input space is given below. * Adaptive Partitioned Random Search (APRS) Inputs: p 0 , p, p index , best p , shrink f actor, tolerance Initialize: Partition the input space into p 0 smaller regions.
Reference: [24] <author> R. Tempo, E. W. Bai, and F. Dabbene. </author> <title> Probabilistic robustness analysis: explicit bounds for the minimum number of samples. </title> <booktitle> In Proceedings of the 35th IEEE International Conference on Decision and Control, </booktitle> <pages> pages 3424-3428, </pages> <address> Kobe, Japan, </address> <year> 1996. </year>
Reference: [25] <author> A. Yoon, P. Khargonekar, and K. Hebbale. </author> <title> Design of computer experiments for open-loop control and robustness analysis of clutch-to-clutch shifts in automatic transmissions. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 3359-3364, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year> <month> 28 </month>
Reference-contexts: Direct methods are chosen since gradient information is computationally expensive to approximate. Moreover, we wish to try to solve several problems in the future which are too complex to hope to have gradient information available. For example, in <ref> [25] </ref> the cost function is nondifferentiable and each evaluation involves the solution of a system of nonlinear differential equations with table look-ups, which is done using simulation tools. A fairly wide range of random search algorithms is tested in this study. <p> Many more function evaluations are likely necessary to arrive at a solution of the same quality as one that is found by a more directed technique. Two random search methods, taken from [10], and a method based on that used in <ref> [25] </ref> are grouped in this section. The first algorithm simply samples from the region of all perturbations with size less than or equal to fl a designated number of times and records the sample that provides the lowest cost.
Reference: [26] <author> P. M. Young, M. P. Newlin, and J. C. Doyle. </author> <title> Let's get real. </title> <journal> ASME, Dynamic Systems and Control Division (Publication) DSC, </journal> <volume> 43 </volume> <pages> 5-12, </pages> <year> 1992. </year>
Reference: [27] <author> W. Zangwill. </author> <title> Minimizing a function without calculating derivatives. </title> <journal> Computer Journal, </journal> <volume> 10 </volume> <pages> 293-296, </pages> <year> 1967. </year>
Reference-contexts: A fairly wide range of random search algorithms is tested in this study. The first algorithm is a multistart version of Rosenbrock's line search method. There are several line search methods such as Zangwill's <ref> [27] </ref> and Hooke and Jeeve's [4] but they are not examined here. Apart from selecting the initial conditions randomly, the algorithm is purely deterministic. Another set of algorithms contains the pure random search and some variants of it.
Reference: [28] <author> X. Zhu, Y. Huang, and J. Doyle. </author> <title> Genetic algorithms and simulated annealing for robustness analysis. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 3756-3760, </pages> <address> Albuquerque, New Mexico, </address> <year> 1997. </year> <month> 29 </month>
References-found: 28

