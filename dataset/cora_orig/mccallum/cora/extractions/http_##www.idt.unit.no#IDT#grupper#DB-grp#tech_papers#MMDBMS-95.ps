URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/MMDBMS-95.ps
Refering-URL: http://www.idt.unit.no/IDT/grupper/DB-grp/tech_papers/tech_papers.html
Root-URL: 
Email: (frhj,roger,olavsag@idt.unit.no)  
Title: Searching and Browsing a Shared Video Database  
Author: Rune Hjelsvold, Roger Midtstraum, and Olav Sandst-a 
Address: Telematics  
Affiliation: Norwegian Institute of Technology Department of Computer Systems and  
Abstract: VideoSTAR (Video STorage And Retrieval) is an experimental video database system that gives support for sharing and reuse of video and meta-data. In this paper we discuss the need for a user to control the degree of meta-data sharing when searching and browsing a shared video database and we propose a way of or-ganising meta-data to allow such functionality. This paper also presents a video query algebra utilizing the way meta-data are organised that allows the user to formulate temporal queries on the video contents. In the last part of the paper we discuss the use of the proposed concepts and the query algebra for video querying and browsing. Here we present a prototype video query tool implemented on top of the VideoSTAR framework that allows users to formulate quite complex queries based on the meta-data available in the VideoSTAR repositories. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.F. Allen. </author> <title> Maintaining Knowledge about Temporal Intervals. </title> <journal> Communications of the ACM, </journal> <month> November </month> <year> 1983. </year>
Reference-contexts: Mandela and Mr. de Klerk. Filter Operations: Allen has shown that there are 13 possible (distinct) relationships that can exist between two temporal intervals <ref> [1] </ref> - e.g., before, over 1 NOT is used to denote the difference between two sets - i.e., A NOT B A AND (NOT B). laps and equals 2 . Filter functions compare two stream intervals to check whether a given temporal relationship exists.
Reference: [2] <editor> J. Ashley et al. </editor> <title> Automatic and Semi-Automatic Image Retrieval Methods in QBIC. </title> <booktitle> In Proceedings of Storage and Retrieval for Image and Video Databases III part of IS&T/SPIE's Symposium on Electronic Imaging: Science and Technology, </booktitle> <address> San Jaso, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: for selecting input sets, they are accessed by the Algebra Operation module to perform annotation, structure, and mapping operations, and they are accessed in the last step when retrieving the objects themselves. 4.2 Experimental Video Query Tool There are several ways to formulate queries - e.g., graphically such as QBIC <ref> [2] </ref>, by using icons [5], or textually such as SQL [13]. The video query tool shown in Figure 7 offers the VideoSTAR query algebra directly to the user. The query tool has been developed with the purpose of being an experimental facility for testing the video query algebra.
Reference: [3] <institution> Avid Technology Inc. </institution> <note> Avid Media Composer User's Guide, </note> <year> 1993. </year>
Reference-contexts: Information like time of recording and exact shot boundaries could easily be provided if one had an integrated system with hard disk based digital cameras and digital editing tools like Avid <ref> [3] </ref>. 2. Image processing techniques like shot detecting tools [15, 7] and (more unlikely at present but still highly desirable) image recognition tools can be used to create some types of information without human involvement.
Reference: [4] <author> J. Clifford and A. Crocker. </author> <title> The Historical Relational Data Model (HRDM) Revisited. In A.U. </title> <editor> Tansel et al., editors, </editor> <title> Temporal Databases: Theory, Design, and Implementation, chapter 1. </title> <publisher> The Ben-jamin/Cummings Publishing Company, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Assuming that A and B are sets of person annotations, A AND B will contain elements from A where an element from B is referring the same person annotation over the same interval. Temporal Set Operations: As noted by Clif-ford and Crocker <ref> [4] </ref>, normal set-theoretic operations of union, intersection and difference produce counter-intuitive results when applied to temporal data. Therefore, temporal variants called tAND, tOR, and tNOT are defined.
Reference: [5] <author> M. Davis. </author> <title> Media Streams: An Iconic Language for Video Annotation. </title> <booktitle> In Proceedings of 1993 IEEE Symposium on Visual Languages, </booktitle> <address> Bergen, Norway, </address> <year> 1993. </year>
Reference-contexts: they are accessed by the Algebra Operation module to perform annotation, structure, and mapping operations, and they are accessed in the last step when retrieving the objects themselves. 4.2 Experimental Video Query Tool There are several ways to formulate queries - e.g., graphically such as QBIC [2], by using icons <ref> [5] </ref>, or textually such as SQL [13]. The video query tool shown in Figure 7 offers the VideoSTAR query algebra directly to the user. The query tool has been developed with the purpose of being an experimental facility for testing the video query algebra.
Reference: [6] <author> T. Dyb-a, T. Holte, and A. Ster. </author> <title> LAVA Report: Analysis of Television Production. </title> <type> Technical report, </type> <institution> SINTEF DELAB, </institution> <month> December </month> <year> 1994. </year> <note> In Nor-wegian. </note>
Reference-contexts: The descriptions are made by humans as part of the production process and afterwards. This is the approach which is used in film and television archives where a staff of trained archivists provide this kind of information at a reasonable high cost <ref> [6] </ref>. Ideally, as much as possible of the meta-data should be captured utilizing the first two approaches but this will far from completely remove the need of human contributions in most applications of video databases.
Reference: [7] <author> A. Hampapur, R. Jain, and T. Weymouth. </author> <title> Digital Video Segmentation. </title> <booktitle> In Proceedings of ACM Multimedia '94, </booktitle> <address> San Francisco, USA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Information like time of recording and exact shot boundaries could easily be provided if one had an integrated system with hard disk based digital cameras and digital editing tools like Avid [3]. 2. Image processing techniques like shot detecting tools <ref> [15, 7] </ref> and (more unlikely at present but still highly desirable) image recognition tools can be used to create some types of information without human involvement. Today the benefits from such techniques are of limited value in the general case but future improvements will hopefully change this. 3.
Reference: [8] <author> R. Hjelsvold. </author> <title> Digital Television Archives - Combining Computer Technology and Video. </title> <booktitle> Presented at the FIAT/IASA annual conference, </booktitle> <address> Bogensee, Germany, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In the last couple of years we have been studying video database issues: We have discussed the video data type [10], we have discussed how applications of a television archive can share a video database <ref> [8] </ref>, we have presented a generic data model for video information [11], we have discussed how integrity and consistency should be maintained in a shared video database [9], and we have presented the VideoSTAR (Video STorage And Retrieval) framework which supports sharing of video data and meta-data [12].
Reference: [9] <author> R. Hjelsvold. </author> <title> Sharing and Reuse of Video Information. </title> <booktitle> In Proceedings of the ACM Multimedia'94 Conference Workshop on Multimedia Database Management Systems, </booktitle> <address> San Francisco, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: issues: We have discussed the video data type [10], we have discussed how applications of a television archive can share a video database [8], we have presented a generic data model for video information [11], we have discussed how integrity and consistency should be maintained in a shared video database <ref> [9] </ref>, and we have presented the VideoSTAR (Video STorage And Retrieval) framework which supports sharing of video data and meta-data [12]. Video is a complex data type that needs a rich data model for representing the important aspects of video information.
Reference: [10] <author> R. Hjelsvold. </author> <title> Video Information Contents and Architecture. </title> <booktitle> In Proceedings of the 4th International Conference on Extending Database Technology, </booktitle> <address> Cambridge, UK, </address> <month> March 28-31 </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In the last couple of years we have been studying video database issues: We have discussed the video data type <ref> [10] </ref>, we have discussed how applications of a television archive can share a video database [8], we have presented a generic data model for video information [11], we have discussed how integrity and consistency should be maintained in a shared video database [9], and we have presented the VideoSTAR (Video STorage
Reference: [11] <author> R. Hjelsvold and R. Midtstraum. </author> <title> Modelling and Querying Video Data. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In the last couple of years we have been studying video database issues: We have discussed the video data type [10], we have discussed how applications of a television archive can share a video database [8], we have presented a generic data model for video information <ref> [11] </ref>, we have discussed how integrity and consistency should be maintained in a shared video database [9], and we have presented the VideoSTAR (Video STorage And Retrieval) framework which supports sharing of video data and meta-data [12].
Reference: [12] <author> R. Hjelsvold and R. Midtstraum. </author> <title> Databases for Video Information Sharing. </title> <booktitle> In Proceedings of the IS&T/SPIE Symposium on Electronic Imaging Science and Technology, Conference on Storage and Retrieval for Image and Video Databases III, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: a video database [8], we have presented a generic data model for video information [11], we have discussed how integrity and consistency should be maintained in a shared video database [9], and we have presented the VideoSTAR (Video STorage And Retrieval) framework which supports sharing of video data and meta-data <ref> [12] </ref>. Video is a complex data type that needs a rich data model for representing the important aspects of video information. A video database becomes even more complex because it has to support different users running different applications in sharing video and meta-data. <p> has been to try to organise meta-data in ways that will allow the user to control the degree of information sharing and to develop a set of operations that can be used to formulate queries. 1.1 The VideoSTAR Data Model The VideoSTAR data model which is more thoroughly discussed in <ref> [12] </ref> is designed to provide users and applications of the VideoSTAR database with the necessary support of storage, retrieval and sharing of: 1. Audio and video recordings (stored media segments) which are stored only once.
Reference: [13] <author> J. Melton and R. Simon. </author> <title> Understanding the New SQL: A Complete Guide. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Operation module to perform annotation, structure, and mapping operations, and they are accessed in the last step when retrieving the objects themselves. 4.2 Experimental Video Query Tool There are several ways to formulate queries - e.g., graphically such as QBIC [2], by using icons [5], or textually such as SQL <ref> [13] </ref>. The video query tool shown in Figure 7 offers the VideoSTAR query algebra directly to the user. The query tool has been developed with the purpose of being an experimental facility for testing the video query algebra.
Reference: [14] <author> L.A. Rowe, J.S. Boreczky, </author> <title> and C.A. Eads. Indexes for User Access to Large Video Databases. </title> <booktitle> In Proceedings of the IS&T/SPIE Symposium on Electronic Imaging Science and Technology, Conference on Storage and Retrieval for Image and Video Databases II, </booktitle> <address> San Jose, CA, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: Typically this is information such as data about the location for the recording and information about persons and objects shown in the frames. In Figure 2 annotations a1 - a6 are made in the basic contexts of StoredMediaSegments III, IV and V. Rowe et Al. <ref> [14] </ref> use the term sensory indexes for this type of information. Descriptions which are made in a primary context will be closely related to the specific composition of the audio and video material and may be largely independent from other use of the same material.
Reference: [15] <author> S.W. Smoliar and H. Zhang. </author> <title> Content-Based Video Indexing and Retrieval. </title> <booktitle> IEEE Multimedia, </booktitle> <month> Summer </month> <year> 1994. </year>
Reference-contexts: Information like time of recording and exact shot boundaries could easily be provided if one had an integrated system with hard disk based digital cameras and digital editing tools like Avid [3]. 2. Image processing techniques like shot detecting tools <ref> [15, 7] </ref> and (more unlikely at present but still highly desirable) image recognition tools can be used to create some types of information without human involvement. Today the benefits from such techniques are of limited value in the general case but future improvements will hopefully change this. 3.
References-found: 15

