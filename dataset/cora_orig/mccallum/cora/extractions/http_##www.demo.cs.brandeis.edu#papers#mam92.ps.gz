URL: http://www.demo.cs.brandeis.edu/papers/mam92.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: kolen-j@cis.ohio-state.edu pollack@cis.ohio-state.edu  
Title: Multiassociative Memory  
Author: John F. Kolen Jordan B. Pollack 
Address: Columbus, OH, 43210 Telephone:(614)292-7402  
Affiliation: The Laboratory for AI Research Department of Computer and Information Science The Ohio State University  
Abstract: This paper discusses the problem of how to implement many-to-many, or multi-associative, mappings within connectionist models. Traditional symbolic approaches wield explicit representation of all alternatives via stored links, or implicitly through enumerative algorithms. Classical pattern association models ignore the issue of generating multiple outputs for a single input pattern, and while recent research on recurrent networks is promising, the field has not clearly focused upon multi-associativity as a goal. In this paper, we define multiassociative memory MM, and several possible variants, and discuss its utility in general cognitive modeling. We extend sequential cascaded networks (Pollack 1987, 1990a) to fit the task, and perform several ini tial experiments which demonstrate the feasibility of the concept. This paper appears in The Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society. August 7-10, 1991. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. A. </author> <year> 1972. </year> <title> A simple neural network generating an interactive memory. </title> <journal> Mathematical Biosciences, </journal> <volume> 14:197 220. </volume>
Reference: <author> Collins, A. M. and Loftus, E. F. </author> <year> 1975. </year> <title> A spreading activation theory of semantic processing. </title> <journal> Psychological Review, 82:240247. </journal>
Reference-contexts: Thus, any possible subset of the range can be captured by this representation. This localist approach has been used to encode possible moves in a tic-tac-toe game (McClelland and Rumelhart, 1986) and the spreading of activation in a semantic network <ref> (Collins and Loftus, 1975) </ref>. Unconstrained representational power, however, is not without its costs. Since individual elements of a range of units can be adequately represented with a binary pattern of length , using units is exponentially more expensive than a sequential retrieval model.
Reference: <author> Elman, J. L. </author> <year> 1988. </year> <title> Finding structure in time. </title> <type> Technical Report CRL 8801, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference: <author> Grossberg, S. </author> <year> 1987. </year> <title> Competitive learning: From interactive activation to adaptive resonance. </title> <journal> Cognitive Science, </journal> <volume> 11:23 63. </volume>
Reference-contexts: The inversion of a multiassociative memory would remain a multiassocia-tive memory. The main difficulty with our initial MM model, and with most recurrent back propagation systems is the reliance on global state information and synchronous update of that information <ref> (Grossberg, 1987) </ref>. As nature teaches us, it is much cheaper to build asynchronous and local parallel processing, which unfortunately can be quite unstable. The multiassociative task is well-suited for an asynchronous and local treatment.
Reference: <author> Hopfield, J. J. </author> <year> 1982. </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <address> 79:25542558. </address>
Reference-contexts: This transformation, or association from a domain set of noisy or partial patterns to a range set of correct patterns is often offered as a model of memory retrieval or recall. Several connectionist models of associative memory exist, such as linear associators (Kohonen, 1972;Anderson, 1972), Hopfield networks <ref> (Hopfield, 1982) </ref>, and feed-forward networks (Rumelhart, Hinton & Williams, 1986), each assuming a many-to-one mapping from domain to range Figure 1a. For noisy or partial memory retrieval, or for perceptual categorization, this assumption might prove valid, however, many other tasks exist, each indescribable within associative frameworks.
Reference: <author> Jordan, M. I. </author> <year> 1987. </year> <title> Supervised learning and systems with excess degrees of freedom. </title> <type> Technical Report COINS Technical Report 88-27, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Boston. </address>
Reference: <author> Kohonen, T. </author> <year> 1972. </year> <title> Correlation matrix memories. </title> <journal> IEEE Transactions on Computers, C-21:353359. </journal>
Reference: <author> Kohonen, T. </author> <year> 1984. </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Reection on the implementation process and its results suggests possible difficulties with the recurrent network, indicating directions for further work. Many to Many Mappings Associative memories take input patterns, process them, and return output patterns <ref> (Kohonen, 1984) </ref>. One way of describing this operation is as a mapping from input patterns of the domain set to the generated output patterns the range set.
Reference: <author> Kolen, J. and Pollack, J. </author> <title> 1990 Back-Propagation is sensitive to Initial Conditions. </title> <booktitle> Complex Systems 4, </booktitle> <pages> 269-280. </pages>
Reference-contexts: We observed different organizations of internal state in different training runs due to back propagations sensitivity to the initial weights <ref> (Kolen & Pollack, 1990) </ref>. In most cases, we also observed that quickly switching between symbols can lead to several cycle delays of the correctness condition as the output dances around the target response.
Reference: <editor> McClelland J., & Rumelhart, D. </editor> <booktitle> 1986 Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <address> Cambridge, </address> <publisher> MIT Press, II, </publisher> <pages> 48-53. </pages>
Reference-contexts: Thus, any possible subset of the range can be captured by this representation. This localist approach has been used to encode possible moves in a tic-tac-toe game <ref> (McClelland and Rumelhart, 1986) </ref> and the spreading of activation in a semantic network (Collins and Loftus, 1975). Unconstrained representational power, however, is not without its costs.
Reference: <author> McCloskey, M. and Glucksberg, S. </author> <year> 1978. </year> <title> Natural categories: Well defined or fuzzy sets? Memory and Cognition, </title> <publisher> 6:462472. </publisher>
Reference-contexts: Consider associating a word with its possible lexical categories or meanings, a chess position to possible next moves, or a category to prototypical members; each input can have a multitude of possible outputs. Even categorization itself is known to vary over time in individual subjects <ref> (McCloskey and Glucksberg, 1978) </ref>. In an attempt to address the issue of multiple output responses, this paper discusses the nature of multiassociative memories capable of responding with one output from a set of possible outputs to an input.
Reference: <author> Pollack, J. B.1987. </author> <title> Cascaded back propagation on dynamic connectionist networks. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference. </booktitle> <address> Seattle, WA, </address> <pages> 391-404. </pages>
Reference: <author> Pollack, J. B. </author> <year> 1990. </year> <title> Language acquisition via strange automata. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society. </booktitle> <address> Cambridge, MA, </address> <pages> 678-685. </pages>
Reference-contexts: Figure 3 contains a schematic representation of a SCN. This paper extends the sequential usage of the cascaded network by utilizing an output vector longer than a single element. Increasing the output width from a single value necessitates a slight modification to the learning rules described in <ref> (Pollack, 1990) </ref>. More output nodes imply each will contribute error to the context node error. <p> We observed different organizations of internal state in different training runs due to back propagations sensitivity to the initial weights <ref> (Kolen & Pollack, 1990) </ref>. In most cases, we also observed that quickly switching between symbols can lead to several cycle delays of the correctness condition as the output dances around the target response.
Reference: <author> Pollack, J. B. </author> <year> 1990. </year> <title> Recursive autoassociative memories. </title> <journal> Artificial Intelligence, </journal> <volume> 46, 1, </volume> <pages> 77-105. </pages>
Reference-contexts: Figure 3 contains a schematic representation of a SCN. This paper extends the sequential usage of the cascaded network by utilizing an output vector longer than a single element. Increasing the output width from a single value necessitates a slight modification to the learning rules described in <ref> (Pollack, 1990) </ref>. More output nodes imply each will contribute error to the context node error. <p> We observed different organizations of internal state in different training runs due to back propagations sensitivity to the initial weights <ref> (Kolen & Pollack, 1990) </ref>. In most cases, we also observed that quickly switching between symbols can lead to several cycle delays of the correctness condition as the output dances around the target response.
Reference: <author> Pollack, J. B. </author> <title> In press The acquisition of dynamical recognizers Machine Learning, </title> <year> 1991. </year>
Reference: <author> Rumelhart D. Hinton, G., and Williams R. </author> <year> 1985. </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323, </volume> <pages> 533-536. </pages>
Reference: <author> Williams, R. </author> <title> 1986 Inverting a connectionist network mapping by back-propagation. </title> <booktitle> Proc. Eight Annual Conf. of the Cognitive Science Society, </booktitle> <address> Amherst. </address> <pages> 859-865. </pages>
Reference-contexts: Several connectionist models of associative memory exist, such as linear associators (Kohonen, 1972;Anderson, 1972), Hopfield networks (Hopfield, 1982), and feed-forward networks <ref> (Rumelhart, Hinton & Williams, 1986) </ref>, each assuming a many-to-one mapping from domain to range Figure 1a. For noisy or partial memory retrieval, or for perceptual categorization, this assumption might prove valid, however, many other tasks exist, each indescribable within associative frameworks. <p> As has been pointed out in the past, inverting a categorization model or an associative memory is a difficult problem, because it requires a many-to-many mapping, not supplied by the standard architectures <ref> (Williams, 1986) </ref>. The inversion of a multiassociative memory would remain a multiassocia-tive memory. The main difficulty with our initial MM model, and with most recurrent back propagation systems is the reliance on global state information and synchronous update of that information (Grossberg, 1987).
References-found: 17

