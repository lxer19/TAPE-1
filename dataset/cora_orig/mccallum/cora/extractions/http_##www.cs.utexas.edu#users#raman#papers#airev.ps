URL: http://www.cs.utexas.edu/users/raman/papers/airev.ps
Refering-URL: http://www.cs.utexas.edu/users/raman/my-papers.html
Root-URL: 
Title: Picture Semantics For Integrating Text And Diagram Input  
Author: Raman Rajagopalan 
Keyword: Key words: Picture Semantics, Coreference, Diagram and Text Integration, Spatial Reasoning  
Address: Austin, Texas, 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: To appear in: AI Review Journal, Vol. 10, No.  
Email: raman@cs.utexas.edu  
Date: 3-4, 1995  
Abstract: The saying `a picture is worth a thousand words' exemplifies the great value of pictures in describing a scenario. Pictures convey spatial information in a compact form, allowing textual descriptions to concentrate on the non-spatial (henceforth, contextual) properties of objects. The difficult task in integrating text and diagrammatic input to a system is to establish coreference matching object references in the text to objects in the diagram. We show that the coreference problem can be greatly simplified if limited contextual information can be provided directly in diagrams. We present a methodology, the Picture Semantics description language, for associating contextual information with objects drawn through graphical editors. Then, we describe our implemented research tool, the Figure Understander, which uses this methodology to integrate the differing information in text and graphically-drawn diagrammatic input into a single unified knowledge base description. We illustrate the utility of our methods through examples from two independent domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ballard, D.H. and Brown, </author> <title> C.M. </title> <booktitle> (1982). Computer Vision, </booktitle> <publisher> Prentice Hall. </publisher>
Reference-contexts: For example, in previous work, it is assumed that the pictorial input is provided in visual (raster) form, such as a camera image. In such cases, it becomes necessary to perform visual scene interpretation <ref> (Ballard et al., 1982) </ref> simply to identify the pictorial objects of interest. There can be many ambiguities. <p> Understanding pictorial input in visual (raster) form requires visual scene interpretation <ref> (Ballard et al., 1982) </ref>, and processing text intended for human-to-human communication can require a sophisticated natural language processing system to resolve the ambiguities that may be present.
Reference: <author> Chang, S. K., Shi, Q. Y., and Yan, C. W. </author> <year> (1987). </year> <journal> Iconic Indexing by 2-D Strings IEEE Transactions on Pattern Recognition and Machine Intelligence, </journal> <volume> 9 </volume> <pages> 413-428. </pages>
Reference: <author> Crawford, J. and Kuipers, B. </author> <year> (1991). </year> <title> Algernon ATractable System for Knowledge-Representation. </title> <booktitle> In Working Notes of The AAAI Spring Symposium on Implemented Knowledge Representation and Reasoning Systems, </booktitle> <address> Palo Alto, CA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference-contexts: The third block specifies the knowledge base assertions to be made for each individual object type (:individual-object-semantics) and for groups as a whole (:object-group-semantics). All contextual information is expressed in terms of predicates in the Algernon knowledge representation language <ref> (Crawford et al., 1991) </ref>. For example, under :object-group-semantics, we have specified that all diagram objects of the group `conducting-disks' are members of the knowledge base class `conducting-disks', and that they have the property of being solid objects. <p> The input to the Figure Understander includes a diagram produced through an object-oriented drawing editor (currently, the Postscript file for a diagram produced using the Interviews drawing editor), a Picture Semantics description for the diagram, one or more sentences of descriptive text, and an Algernon <ref> (Crawford et al., 1991) </ref> knowledge base. It outputs an integrated description of the information in each form of input, diagram and text, into the knowledge base. 3.1. Diagram processing The strength of the Figure Understander lies in its diagram interpretation capabilities. <p> In Figure 8, the function make-variable is used to return a variable name, such as ?V1 or ?V2, to be substituted into the :the from. Algernon's <ref> (Crawford et al., 1991) </ref> definite description facility (:the) is used to connect object references in text to objects in the knowledge base (KB).
Reference: <author> Feiner, S. and McKeown, K. </author> <year> (1990). </year> <title> Coordinating Text and Graphics in Explanation Generation. </title> <booktitle> In Proceedings of The Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 442-449, </pages> <address> Boston, MA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference-contexts: Related issues This paper has focused on a very specific problem providing an efficient method for using text and diagrams as computer input. Some related research issues include the generation of explanations using both text and graphical input <ref> (Feiner et al., 1990) </ref>, generating a pictorial description from a text description of a scene (Gapp, 1994; Latecki et al., 1992; Olivier et al., 1994), and generating a text description from a visual image of a scene (Maass, 1994).
Reference: <author> Freksa, C. </author> <year> (1992). </year> <title> Using Orientation Information for Qualitative Spatial Reasoning. </title> <editor> In Frank, A., Campari, I., and Formentini, U. (Eds.) </editor> <title> Theories and Methods of Spatio-Temporal Reasoning in Geographic Space, </title> <address> 162-178, </address> <publisher> Springer-Verlag: </publisher> <address> Berlin. </address>
Reference: <author> Gapp, K. </author> <year> (1994). </year> <title> Basic Meanings of Spatial Relations: Computation and Evaluation in 3D Space. </title> <booktitle> In Proceedings of The Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1393-1398, </pages> <month> Seat-tle, </month> <title> WA: </title> <journal> American Association for Artificial Intelligence. </journal> <note> aireviewj.tex; 7/11/1995; 14:59; no v.; p.20 20 Raman Rajagopalan He, </note> <author> S., Abe, N., and Kitahashi, T. </author> <title> (1994) Assembly Plan Generation by Integrating Pictorial and Textual Information in an Assembly Illustration. </title> <booktitle> In Working Notes of The AAAI-94 Workshop on Integration of Natural Language and Vision Processing, </booktitle> <pages> 66-73, </pages> <address> Seattle, WA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Herskovits, A. </author> <year> (1985). </year> <title> Semantics and Pragmatics of Locative Expressions. </title> <booktitle> Cognitive Science 9 </booktitle> <pages> 341-378. </pages>
Reference: <author> Jungert, E. </author> <year> (1992). </year> <title> The Observer's Point of View: An Extension of Symbolic Projections. </title> <editor> In Frank, A., Campari, I., and Formentini, U. (Eds.) </editor> <title> Theories and Methods of Spatio-Temporal Reasoning in Geographic Space, </title> <address> 179-195, </address> <publisher> Springer-Verlag: </publisher> <address> Berlin. </address>
Reference: <author> Kuipers, B. </author> <title> (1994) Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge. </title> <publisher> MIT Press: </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Further details of the advantages and limitations of our spatial model, its extensions for reasoning about translational and rotational motion, and details of its implementation within the framework of existing qualitative modeling and simulation systems <ref> (Kuipers, 1994) </ref> may be found in our AAAI-94 article (Rajagopalan, 1994a). 3.2. Text processing The task of attaching semantic information given in any accompanying text sentences to diagram objects requires one to first locate the appropriate diagram objects to which the information is to be attached.
Reference: <author> Landau, B. and Jackendoff, R. </author> <year> (1993). </year> <title> "What" and "Where" in Spatial Language and Spatial Cognition". </title> <booktitle> Behavioral and Brain Sciences 16 </booktitle> <pages> 217-265. </pages>
Reference: <author> Larkin, J. and Simon, H. </author> <year> (1987). </year> <title> Why a Diagram is (Sometimes) Worth 10,000 Words. </title> <booktitle> Cognitive Science 11 </booktitle> <pages> 65-99. </pages>
Reference-contexts: Larkin and Simon describe the benefits of using both diagrams and text to describe a scene in their classic paper, "Why a Diagram is (Sometimes) Worth 10,000 Words" <ref> (Larkin et al., 1987) </ref>. Pictures can be used to compactly describe the spatial properties of objects in a scene. This allows any accompanying textual descriptions to concentrate on the non-spatial domain-specific and problem-specific (henceforth, contextual) properties of objects.
Reference: <author> Latecki, L. and Pribbenow, S. </author> <year> (1992). </year> <title> On Hybrid Reasoning for Processing Spatial Expressions. </title> <booktitle> In Proceedings of The Tenth European Conference on Artificial Intelligence, </booktitle> <pages> 389-393, </pages> <note> Vienna: European Coordinating Committee for Artificial Intelligence. </note>
Reference: <author> Maass, W. </author> <year> (1994). </year> <title> From Visual Perception to Multimodal Communication: Incremental Route Descriptions. </title> <journal> AI Review Journal, </journal> <volume> 8. </volume>
Reference-contexts: research issues include the generation of explanations using both text and graphical input (Feiner et al., 1990), generating a pictorial description from a text description of a scene (Gapp, 1994; Latecki et al., 1992; Olivier et al., 1994), and generating a text description from a visual image of a scene <ref> (Maass, 1994) </ref>. The solutions to these issues focus less on associating pictorial objects with text objects, and instead, concentrate on the modeling and use of spatial relations.
Reference: <editor> McKevitt, P. (Ed.) </editor> <booktitle> (1994) Working Notes of the AAAI Workshop on Integration of Natural Language and Vision Processing. American Association for Artificial Intelligence: </booktitle> <address> Menlo Park, CA. </address>
Reference: <author> Mukerjee, A. and Joe, G. </author> <year> (1990). </year> <title> A Qualitative Model for Space. </title> <booktitle> In Proceedings of The Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Narayanan, N. Hari (Ed.) </author> <year> (1992). </year> <booktitle> Working Notes of the AAAI Spring Symposium Series, Symposium: Reasoning with Diagrammatic Representations. American Association for Artificial Intelligence: </booktitle> <address> Menlo Park, CA. </address>
Reference: <author> Narayanan, N. Hari, Suwa, M., and Motoda, H. </author> <year> (1994). </year> <title> How Things Appear to Work: Predicting Behaviors from Device Diagrams. </title> <booktitle> In Proceedings of The Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1161-1166, </pages> <address> Seattle, WA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Nielsen, P. </author> <year> (1988). </year> <title> A Qualitative Approach to Mechanical Constraint. </title> <booktitle> In Proceedings of The Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 270-274, </pages> <address> Saint Paul, MN: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Novak, G. S., and Bulko, W. </author> <year> (1993). </year> <title> Diagrams and Text as Computer Input. </title> <journal> Journal of Visual Languages and Computing 4 </journal> <pages> 161-175. </pages>
Reference-contexts: The coreference problem <ref> (Novak et al., 1993) </ref> is to associate object references in text with the appropriate diagram objects. Solutions to the coreference problem are important because the unique information in each form of input, diagrams and text, can be integrated only after the mappings between diagram and textual objects are identified. <p> Similar strategies have been adopted in previous work to automate the ability to solve the coreference problem. One approach is to use model libraries which map domain-specific terms to a spatial model for the object <ref> (Novak et al., 1993) </ref>, such as the model that a `car' is a rectangle with four circles attached to it. Another approach is to use an image database to directly associate a picture with a textual term (Rowe et al., 1993; Srihari, 1994).
Reference: <author> Olivier, P., Maeda, T., and Tsujii, J. </author> <year> (1994). </year> <booktitle> Automatic Depiction of Spatial Descriptions In Proceedings of The Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1405-1410, </pages> <address> Seattle, WA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference: <author> Rajagopalan, R. </author> <year> (1994a). </year> <title> A Model for Integrated Spatial and Dynamic Reasoning about Physical Systems. </title> <booktitle> In Proceedings of The Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1411-1417, </pages> <address> Seattle, WA: </address> <booktitle> American Association for Artificial Intelligence. </booktitle>
Reference-contexts: This need was forced on us as we pursued another goal, the development of a problem solver for the magnetic fields domain which performs qualitative reasoning about changes in both spatial and non-spatial properties to solve problems <ref> (Rajagopalan, 1994a) </ref>. As we considered how to input a description of the initial state, we found that special purpose text forms were too cumbersome for providing a complete description of the spatial properties, and that diagrams were a far more natural method of providing the same information. <p> The spatial model The Figure Understander extracts and reasons with a qualitative internal description of the initial spatial state. Our spatial representation, described in greater detail in <ref> (Rajagopalan, 1994a) </ref>, has been used to solve problems involving static scenes as well as problems involving translational and rotational motion. For the purposes of this paper, we will assume that all objects are 2-dimensional and are confined to the XY plane. <p> Further details of the advantages and limitations of our spatial model, its extensions for reasoning about translational and rotational motion, and details of its implementation within the framework of existing qualitative modeling and simulation systems (Kuipers, 1994) may be found in our AAAI-94 article <ref> (Rajagopalan, 1994a) </ref>. 3.2. Text processing The task of attaching semantic information given in any accompanying text sentences to diagram objects requires one to first locate the appropriate diagram objects to which the information is to be attached. <p> Just as we are making use of the Figure Understander to process diagram and text descriptions of a problem in our work on qualitative reasoning about the effects of magnetic fields <ref> (Rajagopalan, 1994a) </ref>, the work of Hari Narayanan, et al. (1994) and He, et al. (1994) could also benefit from the simple approach presented in this paper for using diagrams and text as computer input. <p> The interested reader may consult <ref> (Rajagopalan, 1994a) </ref> or (Rajagopalan, 1995) for further details. 5.2. Related issues This paper has focused on a very specific problem providing an efficient method for using text and diagrams as computer input.
Reference: <author> Rajagopalan, R. </author> <year> (1994b). </year> <title> The Figure Understander: A Tool for the Integration of Text and Graphical Input to a Knowledge Base. </title> <booktitle> In Proceedings of The Sixth IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <pages> 80-87, </pages> <address> New Orleans, LA: </address> <publisher> IEEE Computer Society. </publisher>
Reference: <author> Rajagopalan, R. </author> <year> (1995). </year> <title> Qualitative Reasoning about Dynamic Change in the Spatial Properties of a Physical System. </title> <type> Ph.D. </type> <institution> Diss., Department of Computer Sciences, University of Texas at Austin, Austin, TX (forthcoming). </institution>
Reference-contexts: The interested reader may consult (Rajagopalan, 1994a) or <ref> (Rajagopalan, 1995) </ref> for further details. 5.2. Related issues This paper has focused on a very specific problem providing an efficient method for using text and diagrams as computer input.
Reference: <author> Rajagopalan, R. and Kuipers, B. </author> <year> (1994). </year> <title> The Figure Understander: A System for Integrating Text and Diagram Input to a Knowledge Base. </title> <booktitle> In Proceedings of The Seventh International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, </booktitle> <pages> 211-220, </pages> <address> Austin, TX: </address> <booktitle> International Society of Applied Intelligence. </booktitle>
Reference: <author> Resnick, J., and Halliday, D. </author> <title> (1988) Fundamentals of Physics, </title> <publisher> John Wiley and Sons: </publisher> <address> New York. </address>
Reference-contexts: The coreference problem Diagrams and text typically describe the same object in different ways. Figure 1 shows the diagram and text statement for a Physics problem <ref> (Resnick et al., 1988) </ref>. The diagram objects are lines, polygons, and circles. The text problem statement uses domain-specific terms, such as `disk' and `track', to describe the same objects. Each form of input, diagram and text, contains unique information.
Reference: <author> Retz-Schmidt, G. </author> <year> (1988). </year> <title> Various Views on Spatial Prepositions. </title> <journal> AI magazine 9 </journal> <pages> 95-105. </pages>
Reference: <author> Rowe, N. and Guglielmo, E. </author> <year> (1993). </year> <title> Exploiting captions in retrieval of multimedia data. </title> <booktitle> Information Processing and Management 29 </booktitle> <pages> 453-461. </pages>
Reference: <author> Srihari, R. </author> <year> (1994). </year> <title> Use of Captions and other Collateral Text in Understanding Photographs. </title> <journal> AI Review Journal 8. </journal> <volume> aireviewj.tex; 7/11/1995; 14:59; no v.; </volume> <month> p.21 </month>
Reference-contexts: Overview of our goals and solution In previous work, researchers have addressed the problem of automatically interpreting text and pictorial input intended for human use, such as Srihari's interests in identifying persons in newspaper photographs based on clues in an associated text caption <ref> (Srihari, 1994) </ref>, or Novak and Bulko's (1993) interest in solving textbook Physics problems using the exact text problem statement and associated diagrams that are available to the human reader.
References-found: 28

