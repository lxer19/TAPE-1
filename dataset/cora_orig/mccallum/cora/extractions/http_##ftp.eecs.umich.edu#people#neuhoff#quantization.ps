URL: http://ftp.eecs.umich.edu/people/neuhoff/quantization.ps
Refering-URL: http://ftp.eecs.umich.edu/people/neuhoff/
Root-URL: http://www.eecs.umich.edu
Title: Quantization  
Author: Robert M. Gray and David L. Neuhoff 
Keyword: Quantization, source coding, rate distortion theory, high resolution theory  
Date: 6, OCTOBER 1998 1  
Note: IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO.  
Abstract: The history of the theory and practice of quantization dates to 1948, although similar ideas had appeared in the literature as long ago as 1898. The fundamental role of quantization in modulation and analog-to-digital conversion was first recognized during the early development of pulse code modulation systems, especially in the 1948 paper of Oliver, Pierce, and Shannon. Also in 1948, Bennett published the first high-resolution analysis of quantization and an exact analysis of quantization noise for Gaussian processes, and Shannon published the beginnings of rate distortion theory, which would provide a theory for quantization as analog-to-digital conversion and as data compression. Beginning with these three papers of fifty years ago, we trace the history of quantization from its origins through this decade, and we survey the fundamentals of the theory and many of the popular and promising techniques for quantization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Abaya and G. L. Wise, </author> <title> "Some notes on optimal quantization," </title> <booktitle> Proc. Int'l Conf.Commun., </booktitle> <volume> vol. 2, </volume> <pages> pp. </pages> <address> 30.7.1-30.7.5, </address> <month> June </month> <year> 1981. </year>
Reference: [2] <author> H. Abut, </author> <title> Vector Quantization, IEEE Reprint Collection, </title> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <year> 1990. </year>
Reference-contexts: Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity. Many of the early quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization [484] and Abut's 1990 IEEE Reprint Collection on Vector Quantization <ref> [2] </ref>. We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general formulation of optimal quantization in the next section.
Reference: [3] <author> J. P. Adoul, C. Collin and D. Dalle, </author> <title> "Block encoding and its application to data compression of PCM speech," </title> <booktitle> Proc. Canadian Commun. and EHV Conf., Montreal, </booktitle> <pages> pp. 145-148, </pages> <year> 1978. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle <ref> [3] </ref> used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 [74] and Menez, Boeri, and Esteban in 1979 [353] also considered clustering algorithms for the design of vector quantizers with squared-error and magnitude-error distortion measures.
Reference: [4] <author> J.-P. Adoul, J.-L. Debray, and D. Dalle, </author> <title> "Spectral distance measure applied to the optimum design of DPCM coders with L predictors," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Processing (ICASSP), </booktitle> <address> Denver, </address> <publisher> CO, </publisher> <pages> pp. 512-515, </pages> <year> 1980. </year>
Reference-contexts: The result was an 800 bits per second LPC speech coder with intelligible quality comparable to that of scalar quantized LPC speech coders of four times the rate. (See also [538].) In the same year Adoul, Debray, and Dalle <ref> [4] </ref> also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki [551].
Reference: [5] <author> E. Agrell and T. Eriksson, </author> <title> "Optimization of lattices for quantization," </title> <note> submitted for publication, 1996. This work also appears in "Lattice-based quantization," Part I., Report No. 17, </note> <institution> Department of Information Theory, Chalmers University of Technology, Goteborg, Sweden, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: One upper bound was developed by Zador; others derive from the currently best known tessellations (cf. [106], <ref> [5] </ref>). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for i.i.d. sources. In some cases, simple closed form expressions can be found, e.g. for Gaussian, Laplacian, gamma densities. In other cases numerical integration can be used. <p> These latter give the best sphere packings and coverings in their respective dimensions. Recently, Agrell and Eriksson <ref> [5] </ref> have found improved lattices in dimensions 9 and 10. Though low complexity algorithms have been found for the lossy encoder, there are other issues that affect the performance and complexity of lattice quantizers.
Reference: [6] <author> R. Ahlswede, </author> <title> "The rate-distortion region for multiple descriptions without excess rate," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 31, </volume> <pages> pp. 721-726, </pages> <month> Nov. </month> <year> 1985. </year>
Reference-contexts: The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) <ref> [6] </ref>, and Zhang and Berger (1987) [573]. In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained [509] scalar quantizers for the multiple description problem.
Reference: [7] <author> N. Ahmed and T. Natarajan and K. Rao, </author> <title> "Discrete cosine transform," </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-23, </volume> <pages> pp. 90|93, </pages> <year> 1974. </year>
Reference-contexts: Transform coding has been extensively developed for coding images and video, where the discrete cosine transform (DCT) <ref> [7] </ref>, [429] is most commonly used because of its computational simplicity and its good performance. Indeed DCT coding is the basic approach dominating current image and video coding standards, including H.261, H.263, JPEG, and MPEG.
Reference: [8] <author> V. R. Algazi, </author> <title> "Useful approximation to optimum quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 14, </volume> <pages> pp. 297-301, </pages> <month> June </month> <year> 1966. </year>
Reference-contexts: the Bennett-style asymptotic GRAY AND NEUHOFF: QUANTIZATION 11 approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., [497], [498], [55], [467], <ref> [8] </ref>. An excellent summary of the early work is contained in a 1970 paper by Elias [143]. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization. <p> For example, Lloyd [330] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when multiplied by N 2 . Roe [443], Algazi <ref> [8] </ref> and Wood [539] also used Taylor series.
Reference: [9] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications, </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1973. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represent a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology, cf. Anderberg <ref> [9] </ref>, Harti-gan [238], or Diday and Simon [133].
Reference: [10] <author> J. B. Anderson and J. B. Bodie, </author> <title> "Tree encoding of speech," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 379-387, </pages> <year> 1975. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], [11], [132], [123], <ref> [10] </ref> Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [11] <author> J. B. Anderson and F. Jelinek, </author> <title> "A 2-cycle algorithm for source coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 19, </volume> <pages> pp. 77-92, </pages> <month> Jan. </month> <year> 1973. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], <ref> [11] </ref>, [132], [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [12] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, </author> <title> "Image coding using vector quantization in the wavelet transform domain," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Processing (ICASSP), Albuquerque, </booktitle> <pages> pp. 2297-2300, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], [540]. Early wavelet coding techniques emphasized scalar or lattice vector quantization <ref> [12] </ref>, [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [13] <author> M. Antonini, M. Barlaud, and P. Mathieu, </author> <title> "Image coding using lattice vector quantization of wavelet coefficients," </title> <booktitle> in Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Processing (ICASSP), </booktitle> <address> Vol.4 Toronto, Canada, </address> <pages> pp. 2273-2276, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], [540]. Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], <ref> [13] </ref>, [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [14] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, </author> <title> "Image coding using wavelet transform," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, </volume> <pages> pp. 205-220, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], <ref> [14] </ref>, [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [15] <author> R. Aravind and A. Gersho, </author> <title> "Low-rate image coding with finite-state vector quantization," </title> <booktitle> Proc. Intl. Conf.Acoustics, Speech, and Signal Processing (ICASSP, </booktitle> <pages> pp. 137-140, </pages> <address> Tokyo, </address> <year> 1986. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], [136], [236], <ref> [15] </ref>, [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [16] <author> R. Aravind and A. Gersho, </author> <title> "Image compression based on vector quantization with finite memory," </title> <journal> Optical Engineering, </journal> <volume> vol. 26, </volume> <pages> pp. 570-580, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], [136], [236], [15], <ref> [16] </ref>, [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [17] <author> D. S. Arnstein, </author> <title> "Quantization error in predictive coders," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 23, </volume> <pages> pp. 423-429, </pages> <month> April </month> <year> 1975. </year>
Reference-contexts: Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], <ref> [17] </ref>, [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral
Reference: [18] <author> E. Ayanoglu and R. M. Gray, </author> <title> "The design of predictive trellis waveform coders using the generalized Lloyd algorithm," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 34, </volume> <pages> pp. 1073-1080, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [319], [531], [481], <ref> [18] </ref>, [40].
Reference: [19] <author> E. Ayanoglu and R. M. Gray, </author> <title> "The design of joint source and channel trellis waveform coders," </title> <journal> IEEE Trans. Inform. Theory, pp. </journal> <volume> 855-865, Vol 33, </volume> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: This approach was introduced in 1969 by Kurtenbach and Wintz [304] for scalar quantizers. A Shannon source coding theorem for trellis encoders using this distortion measure was proved in 1981 [135] and a Lloyd-style design algorithm for such encoders provided in 1987 <ref> [19] </ref>. A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa [303] and further studied in [157], [152], [153].
Reference: [20] <author> R. L. Baker and R. M. Gray, </author> <title> "Image compression using nonadaptive spatial vector quantization," </title> <booktitle> Conf. Record of the Sixteenth Asilomar Conf. on Circuits Systems and Computers, Asilomar, </booktitle> <address> CA, </address> <pages> pp. 55-61, </pages> <month> Nov. </month> <year> 1982. </year>
Reference-contexts: Similar ideas can be used for mean-removed VQ <ref> [20] </ref>, [21] and mean/gain/shape VQ [392]. The most general formulation of product codes has been given by Chan and Gersho [82]. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later. Fischer's pyramid VQ [164] is also a kind of shape-gainVQ.
Reference: [21] <author> R. L. Baker and R. M. Gray, </author> <title> "Differential vector quantization of achromatic imagery," </title> <booktitle> Proc. Int'l Picture Coding Symp., </booktitle> <pages> pp. 105-106, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: Similar ideas can be used for mean-removed VQ [20], <ref> [21] </ref> and mean/gain/shape VQ [392]. The most general formulation of product codes has been given by Chan and Gersho [82]. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later. Fischer's pyramid VQ [164] is also a kind of shape-gainVQ.
Reference: [22] <author> M. Balakrishnan, W. A. Pearlman, and L. Lu, </author> <title> "Variable-rate tree-structured vector quantizers," </title> <journal> IEEE Trans. Inform. Theory vol. </journal> <volume> 41, </volume> <pages> pp. 917-930, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see [393], [439], [276], <ref> [22] </ref>, [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [23] <author> A. S. Balamesh, </author> <title> "Block-constrained methods of fixed-rate entropy constrained quantization," </title> <institution> Ph.D.Dissertation, University of Michigan, </institution> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's [305] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization [24], [27], <ref> [23] </ref>, [28] is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook. <p> For i.i.d. Gaussian sources these methods attain SNR within about 2 dB of ffi (R) with k on the order of 100, which is about .5 dB from the goal of 1.53 dB larger than ffi (R). A high resolution analysis is given in [26], <ref> [23] </ref>. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [305].
Reference: [24] <author> A. S. Balamesh and D. L. Neuhoff, </author> <title> "New methods of fixed-rate entropy-coded quantization," </title> <booktitle> Proc. 1992 Conf. on Inform. Sciences and Systems, </booktitle> <address> Princeton, NJ, </address> <pages> pp. 665-670, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's [305] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization <ref> [24] </ref>, [27], [23], [28] is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [25] <author> A. S. Balamesh and D. L. Neuhoff, </author> <note> Unpublished notes, </note> <year> 1992. </year>
Reference-contexts: (say rate 10), after choosing the diameter of the support region to minimize this lower bound, it has been found that the dimension k must be larger than 250 in order that the resulting signal to noise ratio be within 1 dB of that predicted by the Shannon distortion-rate function <ref> [25] </ref>. Similar results were reported by Pepin et al. [409]. On the other hand, as mentioned earlier, a quantizer with dimension 12 can achieve this same distortion.
Reference: [26] <author> A. S. Balamesh and D. L. Neuhoff, </author> <title> "Block-constrained quantization: asymptotic analysis," </title> <journal> DIMACS Series in Discrete Math. and Theoretical Computer Science, </journal> <volume> vol. 14, </volume> <pages> pp. 67-74, </pages> <year> 1993. </year> <journal> 52 IEEE TRANSACTIONS ON INFORMATION THEORY, </journal> <volume> VOL. 44, NO. 6, </volume> <month> OCTOBER </month> <year> 1998 </year>
Reference-contexts: For i.i.d. Gaussian sources these methods attain SNR within about 2 dB of ffi (R) with k on the order of 100, which is about .5 dB from the goal of 1.53 dB larger than ffi (R). A high resolution analysis is given in <ref> [26] </ref>, [23]. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [305].
Reference: [27] <author> A. S. Balamesh and D. L. Neuhoff, </author> <title> "A new fixed-rate quantization scheme based on arithmetic coding," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> San Antonio, p. 435, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's [305] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization [24], <ref> [27] </ref>, [23], [28] is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [28] <author> A. S. Balamesh and D. L. Neuhoff, </author> <title> "Block-constrained methods of fixed-rate entropy-coded, scalar quantization," </title> <note> submitted to IEEE Trans. Inform. Theory. </note>
Reference-contexts: Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's [305] scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization [24], [27], [23], <ref> [28] </ref> is simpler and easier to describe. Here the reproduction codebook is a subset of the k-fold product of some scalar codebook.
Reference: [29] <author> G. B. Ball, </author> <title> "Data analysis in the social sciences: what about the details?," </title> <booktitle> Proc. Fall Joint Computing Conf.," </booktitle> <pages> pp. 533-559, </pages> <address> Washington, D.C., </address> <publisher> Spartan Books, </publisher> <year> 1965. </year>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960's the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey [172], Ball and Hall <ref> [29] </ref>, [230], Jancey [263], and in 1969 by MacQueen [341] (the "k-means" algorithm).
Reference: [30] <author> M. Barlaud, P. Sole, T. Gaidon, M. Antonini, and P. Mathieu, </author> <title> "Pyramidal lattice vector quantization for multiscale image coding," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 3, </volume> <pages> pp. 367-381, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], <ref> [30] </ref>, [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [31] <author> C. F. Barnes, </author> <title> "New multiple path search technique for residual vector quantizers," </title> <booktitle> Proc. Data Compression Conf., </booktitle> <address> Snowbird, UT, </address> <pages> pp. 42-51, </pages> <year> 1994. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], <ref> [31] </ref>, [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [32] <author> C. F. Barnes and R. L. Frost, </author> <title> "Vector quantizers with direct sum codebooks," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 565-580, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal of) increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits <ref> [32] </ref>, [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [33] <author> C. F. Barnes, S. A. Rizvi, and N. M. Nasrabadi, </author> <title> "Advances in residual vector quantization: a review," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 5, </volume> <pages> pp. 226-262, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], <ref> [33] </ref>. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [34] <author> C. W. Barnes, B. N. Tran, S. H. Leung," </author> <title> "On the statistics of fixed-point roundoff error," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 3, </volume> <pages> pp. 595-606, </pages> <month> June </month> <year> 1985. </year>
Reference: [35] <author> E. S. Barnes and N.J.A. Sloane, </author> <title> "The optimal lattice quantizer in three dimensions," </title> <journal> SIAM J. Alg. Discrete Methods, </journal> <volume> vol. 4, </volume> <pages> pp. 30-41, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: For k = 3, it is known that the best lattice tessellation is the body-centered cubic lattice, which is generated by a truncated octahedron <ref> [35] </ref>. It has not been proven that this is the best tessellation, though one would suspect that it is. In summary, Gersho's conjecture is known to be true only for k = 1 and 2.
Reference: [36] <author> P. Bartlett, T. Linder, and G. Lugosi, </author> <title> "The minimax distortion redundancy in empirical quantizer design," </title> <booktitle> Proc. IEEE Intl. Symp. on Inform. Theory, </booktitle> <address> p. 511, Ulm, Germany, </address> <month> June </month> <year> 1997. </year> <note> (Submitted to the IEEE Trans. Inform. Theory.) </note>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], [91], [101], [321], [335], [131], <ref> [36] </ref>.
Reference: [37] <author> W. G. Bath and V. D. Vandelinde, </author> <title> "Robust memoryless quantization for minimum signal distortion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 296-306, </pages> <year> 1982. </year>
Reference-contexts: This can be viewed as a variation on epsilon entropy since the goal is to minimize the maximum distortion. Further results along this line may be found in <ref> [37] </ref>, [275], [491]. Because these are minimax results aimed at scalar quantization, these results apply to any rate or dimension. 48 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 D.
Reference: [38] <author> J.-C. Batllo and V. A. Vaishampayan, </author> <title> "Asymptotic performance of multiple description codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 43, </volume> <pages> pp. 703-707, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger [324]. An alternative approach to multiple description quantization using transform coding has also been considered, e.g., in <ref> [38] </ref>, [211]. I. Other Applications We have not treated many interesting variations and applications of quantization, several of which have been successfully analyzed or designed using the tools described here.
Reference: [39] <author> C. D. Bei and R. M. Gray, </author> <title> "An improvement of the minimum distortion encoding algorithm for vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 33, </volume> <pages> pp. 1132-1133, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [88], <ref> [39] </ref>, [402] and the transform subspace domain approach of [78]. Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be.
Reference: [40] <author> C. D. Bei and R. M. Gray, </author> <title> "Simulation of vector trellis encoding systems," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 34, </volume> <pages> pp. 214-218, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [319], [531], [481], [18], <ref> [40] </ref>.
Reference: [41] <author> P. Bello, R. Lincoln, and H. Gish, </author> <title> "Statistical delta modulation," </title> <journal> Proc. IEEE, </journal> <volume> vol. 55, </volume> <pages> pp. 308-319, </pages> <month> March </month> <year> 1967. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization [161], [203], <ref> [41] </ref>, but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. We will return to this issue when we consider finite-state vector quantizers.
Reference: [42] <author> G. Ben-David and D. Malah, </author> <title> "On the performance of a vector quantizer under channel errors," </title> <booktitle> Signal Proc. VI: Theories and Applications, Proc. EUSIPCO-92, </booktitle> <pages> pp. 1685-1688, </pages> <year> 1992 </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], <ref> [42] </ref>, [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [43] <author> W. R. Bennett, </author> <title> "Spectra of quantized signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 27, </volume> <pages> pp. 446-472, </pages> <month> July </month> <year> 1948. </year>
Reference-contexts: A. Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size [468], [394], <ref> [43] </ref>, which we now derive. GRAY AND NEUHOFF: QUANTIZATION 21 Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + . <p> (q) = N k m (x) 2 f (x) dx: (27) For scalar quantizers (k = 1) with points in the middle of the cells, m (x) = 1 12 and the above reduces to D (q) = 12 N 2 1 f (x) dx (28) which is what Bennett <ref> [43] </ref> found for companders, as restated in terms of point densities by Lloyd [330]. Both (28) and the more general formula (27) are called Bennett's integral. <p> In the following we review the development of rigorous theory. Many analyses | informal and rigorous | explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored <ref> [43] </ref>, [405], [474]. In some cases the source really does have finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature <ref> [43] </ref>, [405], [474] assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. <p> Both theories have been extended to continuous-time random processes. However, the high resolution results are somewhat sketchy <ref> [43] </ref>, [330], [204]. Both can be applied to two or higher dimensional sources such as images or video. Both have been developed the most for Gaussian sources in the context of squared-error distortion, which is not surprising in view of the tractability of squared error and Gaussianity. <p> High resolution theory also has a long tradition of analyzing the error process, beginning with Clavier et al. [99], [100] and Bennett <ref> [43] </ref> and focusing on the distribution of the error, its spectrum and its correlation with the input.
Reference: [44] <author> J. L. Bentley, </author> <title> "Multidimensional binary search trees used for associative searching," </title> <journal> Comm. ACM, </journal> <pages> pp. 209-226, </pages> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantiza-tion, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in <ref> [44] </ref>, [176], [88], [89], [334], [146], [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [45] <author> T. Berger, </author> <title> "Rate distortion theory for sources with abstract alphabet and memory," </title> <journal> Inform. Contr., </journal> <volume> vol. 13, </volume> <pages> pp. 254-273, </pages> <year> 1968. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], <ref> [45] </ref>, [46], [218], [549], [127], [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions.
Reference: [46] <author> T. Berger, </author> <title> Rate Distortion Theory, </title> <address> Prentice-Hall,Englewood Cliffs, NJ, </address> <year> 1971. </year>
Reference-contexts: Accordingly, the theory is often called rate distortion theory, cf. <ref> [46] </ref>. 12 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 for which the mutual information I (X; Y ) is at most R, where we emphasize that X and Y are scalar variables here. <p> This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], <ref> [46] </ref>, [218], [549], [127], [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> It is well known that D slb (R)=D (R) approaches one as R increases [327], [267], <ref> [46] </ref>, [322], which is entirely consistent with the fact that Z (R)=ffi (R) approaches one as R increases. The relationships among the various distortion-rate functions are summarized below. Inequalities marked with a "*" become tight as dimension k increases, and those marked with a "+" become tight as R increases. <p> Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. <ref> [46] </ref>, [494], [560], [217]. For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. <p> Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], [239], <ref> [46] </ref>, [212], [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function. <p> As another example, for a Gaussian source with memory and squared-error distortion, rate distortion theory shows there is a simple relation between the spectra of the source and the spectra of the error produced by an optimal high dimensional quantizer, cf. <ref> [46] </ref>. High resolution theory also has a long tradition of analyzing the error process, beginning with Clavier et al. [99], [100] and Bennett [43] and focusing on the distribution of the error, its spectrum and its correlation with the input. <p> The usefulness of this modified distortion for source coding noisy sources was first seen by Dobrushin and Tsybakov (1962) [134] and was used by Fine (1965) [162] and Sakrison (1968) [452] to obtain information theoretic bounds an quantization and source coding for noisy sources. Berger (1971) <ref> [46] </ref> explicitly used the modified distortion in his study of Shannon source coding theorems for noise corrupted sources.
Reference: [47] <author> T. Berger, </author> <title> "Optimum quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 18, </volume> <pages> pp. 759-765, </pages> <month> November </month> <year> 1972. </year>
Reference-contexts: In a 1972 paper dealing with a vector quantization technique to be discussed later, Berger <ref> [47] </ref> described Lloyd-like conditions for optimality of an entropy-constrained scalar quantizer for squared-error distortion. He formulated the optimization as an unconstrained Lagrangian minimization and developed an iterative algorithm for the design of entropy constrained scalar quantizers. <p> This leads to a quantizer with reduced (but still fairly large) complexity. Dunn compared numerical computations of the performance of this scheme to the Shannon rate-distortion function. As mentioned earlier, this was the first such comparison. In 1972 Berger, Jelinek and Wolf [49], and Berger <ref> [47] </ref> introduced lower complexity encoding algorithms for permutation codes, and Berger [47] showed that for large dimensions, the operational distortion-rate function of permutation codes is approximately equal to that of optimal variable-rate scalar quantizers. <p> Dunn compared numerical computations of the performance of this scheme to the Shannon rate-distortion function. As mentioned earlier, this was the first such comparison. In 1972 Berger, Jelinek and Wolf [49], and Berger <ref> [47] </ref> introduced lower complexity encoding algorithms for permutation codes, and Berger [47] showed that for large dimensions, the operational distortion-rate function of permutation codes is approximately equal to that of optimal variable-rate scalar quantizers. While they do not attain performance beyond that of scalar quantization, permutation codes have the advantage of avoiding the buffering and error propagation problems of variable-rate quantization. <p> We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general formulation of optimal quantization in the next section. In 1984 Farvardin and Modestino [155] extended Berger's <ref> [47] </ref> necessary conditions for optimality of an entropy-constrained scalar quantizer to more general distortion measures and described two design algorithms: the first is similar to Berger's iterative algorithm, but the second was a fixed-point algorithm which can be considered as a natural extension of Lloyd's Method I from fixed-rate to variable-rate <p> In 1989 Chou et al. [93] developed a generalized Lloyd algorithm for entropy constrained vector quantization that generalized Berger's <ref> [47] </ref>, [48] Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [155] to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy. <p> This is a good place to again mention Gish and Pierce's result that if the rate is high, optimal entropy-constrained scalar or vector quantization can provide no more than roughly 1/4 bit improvement over uniform scalar quantization with block entropy coding. Berger <ref> [47] </ref> showed that permutation codes achieved roughly the same performance with a fixed-rate vector quantizer.
Reference: [48] <author> T. Berger, </author> <title> "Minimum entropy quantizers and permutation codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 149-157, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: In 1976 Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint [376]. Yet another approach was taken by Noll and Zelinski (1978) [391]. Berger refined his approach to entropy-constrained quantizer design in <ref> [48] </ref>. Variable-rate quantization was also extended to DPCM and transform coding, where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing [398], [154]. <p> March 1982 special issue on Quantization of these Transactions, which published the Bell Laboratories Technical Memos of Lloyd, Newman and Zador along with Berger's extension of the optimality properties of entropy-constrained scalar quantization to rth power distortion measures and his extensive comparison of minimum entropy quantizers and fixed-rate permutation codes <ref> [48] </ref>, generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima [503], results on the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [418], two seminal papers on lattice quantization by Conway and Sloane [103], [104], rigorous developments of the <p> In 1989 Chou et al. [93] developed a generalized Lloyd algorithm for entropy constrained vector quantization that generalized Berger's [47], <ref> [48] </ref> Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [155] to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy.
Reference: [49] <author> T. Berger, F. Jelinek, and J.K. Wolf, </author> <title> "Permutation codes for sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 18, </volume> <pages> pp. 160-169, </pages> <month> Jan. </month> <year> 1972. </year>
Reference-contexts: This leads to a quantizer with reduced (but still fairly large) complexity. Dunn compared numerical computations of the performance of this scheme to the Shannon rate-distortion function. As mentioned earlier, this was the first such comparison. In 1972 Berger, Jelinek and Wolf <ref> [49] </ref>, and Berger [47] introduced lower complexity encoding algorithms for permutation codes, and Berger [47] showed that for large dimensions, the operational distortion-rate function of permutation codes is approximately equal to that of optimal variable-rate scalar quantizers.
Reference: [50] <author> V. Bhaskaran and K. Konstantinides, </author> <title> Image and Video Compression Standards, </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1995 </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], [374], [196], [424], <ref> [50] </ref>, [458] and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], [208], [408], <ref> [50] </ref>, [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [51] <author> H. S. Black, </author> <title> "Pulse code modulation," Bell Lab. </title> <journal> Record., </journal> <volume> vol. 25, </volume> <pages> pp. 265-269, </pages> <month> July </month> <year> 1947. </year>
Reference: [52] <author> R. E. Blahut, </author> <title> "Computation of channel capacity and rate-distortion functions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 18. </volume> <pages> pp. 460-473, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. [46], [494], [560], [217]. For other sources it can be computed with Blahut's algorithm <ref> [52] </ref>. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. For other cases, the Blahut algorithm [52] can be used to compute D k (R), though its <p> For other sources it can be computed with Blahut's algorithm <ref> [52] </ref>. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. For other cases, the Blahut algorithm [52] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small.
Reference: [53] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: A tree structured quantizer is analogous to a classification or regression tree, and as such unbalanced TSVQs can be designed by algorithms based on a gardening metaphor of growing and pruning. The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone <ref> [53] </ref>, and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm [94], [439], [196]. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned.
Reference: [54] <author> L. K. Brinton, "Nonsubtractive Dither," </author> <title> M.S. </title> <type> Thesis, </type> <institution> Electrical Engineering Department, University of Utah, </institution> <address> Salt Lake City, Utah, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright [542] in 1979 and Brinton <ref> [54] </ref> in 1984 and subsequently extended and refined with a variety of proofs [513], [512], [328], [227].
Reference: [55] <author> J. D. Bruce, </author> <title> "On the optimum quantization of stationary signals," </title> <booktitle> 1964 IEEE Int. Conv. Rec., </booktitle> <volume> Part 1, </volume> <pages> pp. 118-124, </pages> <year> 1964. </year>
Reference-contexts: extensions of the Bennett-style asymptotic GRAY AND NEUHOFF: QUANTIZATION 11 approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., [497], [498], <ref> [55] </ref>, [467], [8]. An excellent summary of the early work is contained in a 1970 paper by Elias [143]. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization.
Reference: [56] <author> J. A. Bucklew, </author> <title> "Companding and random quantization in several dimensions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 27, </volume> <pages> pp. 207-211, </pages> <month> Mar. </month> <year> 1981. </year>
Reference-contexts: This was first mentioned in Panter-Dite [405] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis [193], <ref> [56] </ref>, [57]. So there is no direct way to construct optimal vector quan tizers with the high resolution philosophy.
Reference: [57] <author> J. A. Bucklew, </author> <title> "A note on optimal multidimensional compan-ders," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 29, </volume> <editor> p. </editor> <volume> 279, </volume> <month> March </month> <year> 1983. </year>
Reference-contexts: This was first mentioned in Panter-Dite [405] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis [193], [56], <ref> [57] </ref>. So there is no direct way to construct optimal vector quan tizers with the high resolution philosophy.
Reference: [58] <author> J. A. Bucklew, </author> <title> "Two results on the asymptotic performance of quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-30, </volume> <pages> pp. 341-348, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Cambanis and Gerr (1983) [70] claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems as [64]. A subsequent paper by Bucklew (1984) <ref> [58] </ref> derived a result for vector quantizers that lies between Bennett's integral and Zador's formula. <p> Even assuming Gersho's conjecture is correct, there is no rigorous derivation of the Zador-Gersho formulas (30) and (32) along the lines of the informal derivations that start with Bennett's integral. We also mention that the tail conditions given in some of the rigorous results (e.g. <ref> [58] </ref>, [365]) are very difficult to check. Simpler ones are needed. Finally, as discussed in Section II there are no convincing (let alone rigorous) asymptotic analyses of the operational distortion-rate function of DPCM. I.
Reference: [59] <author> J. A. Bucklew, </author> <title> "A note on the absolute epsilon entropy," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 142-144, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: As in the Shannon case, all these definitions can be made for k-dimensional vectors X k and the limiting behavior can be studied. Results regarding the convergence of such limits and the equality of the information-theoretic and operational notions of epsilon-entropy can be found, e.g., in [421], [420], [278], <ref> [59] </ref>. Much of the theory is concerned with approximating epsilon entropy for small *. Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability.
Reference: [60] <author> J. A. Bucklew and N. C. Gallagher, Jr., </author> <title> "A note on optimum quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 365-366, </pages> <month> May </month> <year> 1979. </year>
Reference-contexts: The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily [137], <ref> [60] </ref>, [193], [184], [196] that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q i (X)(q j (X)X j )] = 0, for all i; j so that each component of the quantizer output is
Reference: [61] <author> J. A. Bucklew and N. C. Gallagher, Jr., </author> <title> "Quantization schemes for bivariate Gaussian random variables," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 25, </volume> <pages> pp. 537-543, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], <ref> [61] </ref>, [62], [530], [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers <ref> [61] </ref>, [62], [483], [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [62] <author> J. A. Bucklew and N. C. Gallagher, Jr., </author> <title> "Two-Dimensional Quantization of Bivariate Circularly Symmetric Densities," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 25, </volume> <pages> pp. 667-671, </pages> <month> Nov. </month> <year> 1979. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], <ref> [62] </ref>, [530], [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], <ref> [62] </ref>, [483], [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [63] <author> J. A. Bucklew and N. C. Gallagher, Jr., </author> <title> "Some properties of uniform step size quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-26, </volume> <pages> pp. 610-613, </pages> <month> September </month> <year> 1980. </year>
Reference-contexts: In 1978 these same authors [581] studied uniform scalar quantization with variable-rate coding, and extended Koshelev's result to r-th power distortion measures. The next contribution is that of Bucklew and Gallagher (1980) <ref> [63] </ref>, who studied asymptotic properties of fixed rate uniform scalar quantization.
Reference: [64] <author> J. A. Bucklew and G. L. Wise, </author> <title> "Multidimensional asymptotic quantization theory with rth power distortion measures, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 239-247, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [418], two seminal papers on lattice quantization by Conway and Sloane [103], [104], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise <ref> [64] </ref>, Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [281], Kieffer's study of the convergence rate of Lloyd's algorithm [280], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was <p> It was not until much later that the asymptotic form of N and D N were found, as will be described later. Formal theory advanced further in papers by Bucklew and Wise, Cambanis and Gerr, and Bucklew. The first of these (1982) <ref> [64] </ref> demonstrated Zador's fixed-rate result for rth power distortion kx yk r , assuming only that E kXk r+ffi fl &lt; 1 for some ffi &gt; 0. It also contained a generalization to random vectors without probability densities, i.e. with distributions that are not absolutely continuous or even continuous. <p> Cambanis and Gerr (1983) [70] claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems as <ref> [64] </ref>. A subsequent paper by Bucklew (1984) [58] derived a result for vector quantizers that lies between Bennett's integral and Zador's formula.
Reference: [65] <author> J. Buhmann and H. Kuhnel, </author> <title> "Vector quantization with complexity costs," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 1133-1145, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], <ref> [65] </ref>. A variety of quantization techniques were introduced by constraining the structure of the vector quantization to better balance complexity with performance and these methods were applied to real signals (especially speech and images) as well as to random sources, which permitted comparison to the theoretical high-resolution and Shannon bounds.
Reference: [66] <author> P. J. Burt and E. H. Adelson, </author> <title> `The Laplacian pyramid as a compact image code," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 31, </volume> <pages> pp. 532-540, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Subband/wavelet/pyramid Quantization Subband codes, wavelet codes, and pyramid codes are intimately related and all are cousins of a transform code. The oldest of these methods (so far as quantization is concerned) is the pyramid code of Burt and Adelsen <ref> [66] </ref> (which is quite different from Fischer's pyramid VQ).
Reference: [67] <author> A. Buzo, R. M. Gray, A. H. Gray, Jr., and J. D. Markel, </author> <title> "Optimal quantizations of coefficient vectors in LPC speech," </title> <booktitle> 1978 Joint Meeting of the Acoustical Society of America and the Acoustical Society of Japan, </booktitle> <address> Honolulu, HI, </address> <month> Dec. </month> <year> 1978. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], <ref> [67] </ref>, [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [68] <author> A. Buzo, A. H. Gray, Jr., R. M. Gray and J. D. Markel, </author> <title> "Optimal quantizations of coefficient vectors in LPC speech," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust. Speech, and Signal Processing (ICASSP), </booktitle> <pages> pp. 52-55, </pages> <address> Washington, D. C., </address> <month> April </month> <year> 1979. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], <ref> [68] </ref>, [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [69] <author> A. Buzo, A. H. Gray, Jr., R. M. Gray, and J. D. Markel, </author> <title> "Speech coding based upon vector quantization," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 28, </volume> <pages> pp. 562-574, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: Later in the same year, Buzo et al. <ref> [69] </ref>, developed a tree-structured vector quan-tizer (TSVQ) for 10-dimensional LPC vectors that greatly reduced the encoder complexity from exponential growth with codebook size to linear growth by searching a sequence of small codebooks instead of a single large codebook. <p> A high resolution analysis is given in [26], [23]. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform [305]. Tree-Structured Quantization In its original and simplest form, a k-dimensional tree-structured vector quantizer (TSVQ) <ref> [69] </ref> is a fixed-rate quantizer with, say, rate R whose encoding is guided by a balanced (fixed-depth) binary tree of depth kR. There is a codevector associated with each of its 2 kR terminal nodes (leaves), and a k-dimensional testvector associated with each of its 2 kR 1 internal nodes. <p> This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in [252] The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ <ref> [69] </ref>, [225] is to first design the testvectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [70] <author> S. Cambanis and N. Gerr, </author> <title> "A simple class of asymptotically optimal quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 29, </volume> <pages> pp. 664-676, </pages> <month> Sept. </month> <year> 1983. </year>
Reference-contexts: However, as pointed out by Linder (1991) [320], there was "a gap in the proof concerning the convergence of Riemann sums with increasing support to a Riemann integral." Linder fixed this and presented a correct derivation with weaker assumptions. Cambanis and Gerr (1983) <ref> [70] </ref> claimed a similar result, but it had more restrictive conditions and suffered from the same sort of problems as [64]. A subsequent paper by Bucklew (1984) [58] derived a result for vector quantizers that lies between Bennett's integral and Zador's formula.
Reference: [71] <author> J. C. Candy and O. J. Benjamin, </author> <title> "The structure of quantization noise from Sigma-Delta modulation," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 29, </volume> <pages> pp. 1316-1323, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: Transform Code sities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier and Grieg to quantizers inside a feedback loop [260], <ref> [71] </ref>, [215], [216], [72].
Reference: [72] <editor> J. Candy and G. Temes, Eds. </editor> <title> Oversampling delta-sigma Data Converters, </title> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: Transform Code sities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier and Grieg to quantizers inside a feedback loop [260], [71], [215], [216], <ref> [72] </ref>.
Reference: [73] <author> R. M. Capocelli and A. DeSantis, </author> <title> "Variations on a theme by Gal-lager," pp. 181-213, Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: Moreover, tighter bounds have been developed. For example Gallager [181] has shown that the entropy can be at most P max +:0861 smaller than the average length of the Huffman code, when P max , the largest of the P i 's, is less than 1/2. See <ref> [73] </ref> for discussion of this and other bounds. Since P max is ordinarily much smaller than 1/2, this shows that H (q (X)) is generally a fairly accurate estimate of the average rate, especially in the high-resolution case.
Reference: [74] <author> J. R. Caprio, N. Westin, and J. Esposito, </author> <title> "Optimum quantization for minimum distortion," </title> <booktitle> Proc. of the Intl Telemetering Conf., </booktitle> <pages> pp. 315-323, </pages> <year> 1978. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 <ref> [74] </ref> and Menez, Boeri, and Esteban in 1979 [353] also considered clustering algorithms for the design of vector quantizers with squared-error and magnitude-error distortion measures. The most important paper on quantization during the 1970's was without a doubt Gersho's paper on "Asymptotically optimal block quantization" [193].
Reference: [75] <author> N. Chaddha, M. Vishwanath and P. A. Chou, </author> <title> "Hierarchical vector quantization of perceptually weighted block transforms," </title> <booktitle> Proc. Compression Conf., </booktitle> <address> Snowbird, UT, </address> <pages> pp. 3-12, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], [358], [357], [518], <ref> [75] </ref>, [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [86], [358]. One technique worth particular mention is called hierarchical table lookup VQ [86], [518], <ref> [75] </ref>, [219]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times. <p> Hence the method is hierarchical. Because each of the quantizers can be implemented entirely with table look up, this method eliminates all arithmetic complexity except memory accesses. It has been successfully used for video coding [518], <ref> [75] </ref>. B. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in [196], [458].
Reference: [76] <author> D. L. Chaffee, </author> <title> "Applications of rate distortion theory to the bandwidth compression," </title> <type> Ph.D. Dissertation, </type> <institution> Electrical Engineering Department, Univ. of California, </institution> <address> Los Angeles, </address> <year> 1975. </year>
Reference-contexts: Notwithstanding the skepticism of some about the feasibility of brute force, unstructured vector quantization, serious studies of such began to appear in the mid-1970's, when several independent results were reported describing applications of clustering algorithms, usually k-means, to problems of vector quantization. In 1974-1975 Chaffee <ref> [76] </ref> and Chaffee and Omura [77] used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification [242].
Reference: [77] <author> D. L. Chaffee and J. K. Omura, </author> <title> "A very low rate voice compression system," </title> <booktitle> Abstracts of Papers of the IEEE Int'l Symp. Inform. Theory, </booktitle> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: In 1974-1975 Chaffee [76] and Chaffee and Omura <ref> [77] </ref> used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification [242]. These papers appear to be the first applications of direct vector quantization for speech and image coding applications.
Reference: [78] <author> C.-K. Chan and L.-M. </author> <title> Po, "A complexity reduction technique for image vector quantization," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, </volume> <pages> pp. 312-321, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [88], [39], [402] and the transform subspace domain approach of <ref> [78] </ref>. Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be.
Reference: [79] <author> W.-Y. Chan and A. Gersho, </author> <title> "High fidelity audio transform coding with vector quantization," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP) Albuquerque, New Mexico, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1109-1112, </pages> <month> April, </month> <year> 1990. </year>
Reference-contexts: In other words, multistage quantization can be used (and often is) with very different kinds of quantizers in its stages (different dimensions and much different structures, e.g. DPCM or wavelet coding). For example, structuring the stage quantizers leads to good performance and further substantial reductions in complexity, e.g. [243], <ref> [79] </ref>. Of course, the multistage structuring leads to a suboptimal VQ for its given dimension.
Reference: [80] <author> W.-Y. Chan and A. Gersho, </author> <title> "Constrained-storage vector quantization in high fidelity audio transform coding," </title> <booktitle> Proc. IEEE ICASSP, Toronto, </booktitle> <pages> pp. 3597-3600, </pages> <month> May </month> <year> 1991. </year> <note> GRAY AND NEUHOFF: QUANTIZATION 53 </note>
Reference-contexts: This approach was further developed by Swaszek in [487]. Another scheme for adapting each stage to the previous is called codebook sharing, as introduced by Chan and Gersho <ref> [80] </ref>, [82]. With this approach, each stage has a finite set of reproduction codebooks, one of which is used to quantize the residual, depending on the sequence of outcomes from the previous stages. <p> with a constraint on storage; and they used this method 13 Since the second stage uniformly refines the first stage cells, the overall point density is approximately that of the first stage. 46 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 to good effect in audio coding <ref> [80] </ref>. In the larger scheme of things, TSVQ, multistage VQ and codebook sharing all fit within the broad family of generalized product codes that they introduced in [82].
Reference: [81] <author> W.-Y. Chan and A. Gersho, </author> <title> "Enhanced multistage vector quantization by joint codebook design," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 40, </volume> <pages> pp. 1693-1697, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], <ref> [81] </ref>, [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [82] <author> W.-Y. Chan and A. Gersho, </author> <title> "Generalized product code vector quantization: a family of efficient techniques for signal compression," </title> <booktitle> Digital Signal Processing, </booktitle> <volume> vol. 4, </volume> <pages> pp. 95-126, </pages> <year> 1994. </year>
Reference-contexts: Similar ideas can be used for mean-removed VQ [20], [21] and mean/gain/shape VQ [392]. The most general formulation of product codes has been given by Chan and Gersho <ref> [82] </ref>. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later. Fischer's pyramid VQ [164] is also a kind of shape-gainVQ. <p> This approach was further developed by Swaszek in [487]. Another scheme for adapting each stage to the previous is called codebook sharing, as introduced by Chan and Gersho [80], <ref> [82] </ref>. With this approach, each stage has a finite set of reproduction codebooks, one of which is used to quantize the residual, depending on the sequence of outcomes from the previous stages. Thus each codebook is shared among some subset of the possible sequences of outcomes from the previous stages. <p> In the larger scheme of things, TSVQ, multistage VQ and codebook sharing all fit within the broad family of generalized product codes that they introduced in <ref> [82] </ref>. Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Figure 3 [235], [116], [85], [417].
Reference: [83] <author> W.-Y. Chan, S. Gupta, and A. Gersho, </author> <title> "Enhanced multistage vector quantization by joint codebook design," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 40, </volume> <pages> pp. 1693-1697, </pages> <month> Nov. </month> <year> 1992. </year>
Reference: [84] <author> Y.-H. Chan and W. Siu, </author> <title> "In search of the optimal searching sequence for VQ encoding," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 43, </volume> <pages> pp. 2891-2893, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], [334], [146], [532], [423], [415], [500], <ref> [84] </ref>. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [85] <author> P. C. Chang and R. M. Gray, </author> <title> "Gradient algorithms for designing predictive vector quantizers," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing vol. </journal> <volume> 34, </volume> <pages> pp. 679-690, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Figure 3 [235], [116], <ref> [85] </ref>, [417]. Alternatively, the encoder and decoder can share a finite set of states and a quantizer custom designed for each state.
Reference: [86] <author> P. C. Chang, J. May, and R. M. Gray, </author> <title> "Hierarchical vector quan-tizers with table-lookup encoders," </title> <booktitle> Proc. 1985 IEEE Int'l Conf. Comm., </booktitle> <volume> vol. 3, </volume> <pages> pp. 1452-1455, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in <ref> [86] </ref>, [358], [357], [518], [75], [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. <p> Thus the question becomes: what is the best partition into N cells, each of which is the union of some number of fine cells. The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in <ref> [86] </ref>, [358]. One technique worth particular mention is called hierarchical table lookup VQ [86], [518], [75], [219]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in <ref> [86] </ref>, [358]. One technique worth particular mention is called hierarchical table lookup VQ [86], [518], [75], [219]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times.
Reference: [87] <author> D. T. S. Chen, </author> <title> "On two or more dimensional optimum quantiz-ers," </title> <booktitle> Proc. IEEE Intl. Conf. Acoust., Speech, and Signal Processing (ICASSP), </booktitle> <pages> pp. 640-643, </pages> <address> Hartford,CT, </address> <year> 1977. </year>
Reference-contexts: These papers appear to be the first applications of direct vector quantization for speech and image coding applications. Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantiz GRAY AND NEUHOFF: QUANTIZATION 15 ers <ref> [87] </ref>. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], [257].
Reference: [88] <author> D.-Y. Cheng, A. Gersho, B. Ramamurthi, and Y. Shoham, </author> <title> "Fast Search Algorithms for Vector Quantization and Pattern Matching," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP), </booktitle> <address> San Diego, </address> <pages> pp. </pages> <address> 911.1-911.4, </address> <month> March </month> <year> 1984. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantiza-tion, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in [44], [176], <ref> [88] </ref>, [89], [334], [146], [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector. <p> Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of <ref> [88] </ref>, [39], [402] and the transform subspace domain approach of [78]. Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be.
Reference: [89] <author> D.-Y. Cheng and A. Gersho, </author> <title> "A fast codebook search algorithm for nearest-neighbor pattern matching," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP), Tokyo, </booktitle> <volume> vol. 1, </volume> <pages> pp. 265-268, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantiza-tion, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in [44], [176], [88], <ref> [89] </ref>, [334], [146], [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [90] <author> P. A. Chou, </author> <title> "Code clustering for weighted universal VQ and other applications," </title> <booktitle> Proc. IEEE Int'l Symp.Inform. Theory, </booktitle> <address> p. 253, Budapest, Hungary, </address> <year> 1991. </year>
Reference-contexts: The clustering of codebooks was originally due to Chou <ref> [90] </ref> in 1991. High resolution quantization theory was used to study rates of convergence with blocklength to the optimal performance, yielding results consistent with earlier convergence results developed by other means, e.g., Linder et al. [321].
Reference: [91] <author> P. A. Chou, </author> <title> "The distortion of vector quantizers trained on n vectors decreases to the optimum as O p (1=n)," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Trondheim, Norway, </address> <year> 1994. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], <ref> [91] </ref>, [101], [321], [335], [131], [36].
Reference: [92] <author> P. A. Chou, M. Effros, and R. M. Gray. </author> <title> "A vector quantization approach to universal noiseless coding and quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 1109-1138, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: A good review of the history of universal source coding through the early 1990s may be found in Kieffer (1993) [283]. Better performance tradeoffs can be achieved by allowing both rate and distortion to vary, and in 1996 Chou et al. <ref> [92] </ref> formulated the universal coding problem as an entropy-constrained vector quantization problem for a family of sources and provided existence proofs and Lloyd-style design algorithms for the collection of codebooks subject to a Lagrangian distortion measure, yielding a fixed rate-distortion slope optimization rather than fixed distortion or fixed rate.
Reference: [93] <author> P. A. Chou and T. Lookabaugh and R. M. Gray, </author> <title> "Entropy-Constrained Vector Quantization," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing vol. </journal> <volume> 37, </volume> <pages> pp. 31-42, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: In 1989 Chou et al. <ref> [93] </ref> developed a generalized Lloyd algorithm for entropy constrained vector quantization that generalized Berger's [47], [48] Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm [155] to vectors.
Reference: [94] <author> P. A. Chou and T. Lookabaugh and R. M. Gray, </author> <title> "Optimal pruning with applications to tree-structured source coding and modeling," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 35, </volume> <pages> pp. 299-315, </pages> <month> Mar. </month> <year> 1989 </year>
Reference-contexts: The resulting optimality properties are summarized below. The proofs are simple and require no calculus of variations or differentiation. Proofs may be found, e.g., in <ref> [94] </ref>, [196]. * For a fixed lossy encoder ff , regardless of the lossless encoder fl , the optimal reproduction decoder fi is given by fi (i) = argmin y the output minimizing the conditional expectation of the distortion between the output and the input given that the encoder produced index <p> We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in [526], [470], <ref> [94] </ref>, [439], [438], [463]. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546]. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQs can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) [342], <ref> [94] </ref>, [439], [196] and with larger branching factors than two or even variable branching factors [460]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [53], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm <ref> [94] </ref>, [439], [196]. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> It can be shown that, for quite general measures of distortion, pruning can be done in an optimal fashion and the optimal subtrees of decreasing rate are nested <ref> [94] </ref>. See also [355]. It seems likely that in the moderate to high rate case, pruning removes leaves corresponding to cells that are oblong such as cubes cut in half, leaving mainly cubic cells.
Reference: [95] <author> P. A. Chou and T. Lookabaugh, </author> <title> "Conditional entropy-constrained vector quantization of linear predictive coefficients," </title> <journal> pp. </journal> <pages> 187-200, </pages> <booktitle> Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1990. </year>
Reference-contexts: One can of course also make the lossless code depend on the state, or be conditional on the previous binary codeword. One can also use a memoryless VQ combined with a conditional lossless code (conditioned on the previous binary codeword) designed with a conditional entropy constraint <ref> [95] </ref>, [188]. A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar.
Reference: [96] <author> J. Chow and T. Berger, </author> <title> "Failure of successive refinement for symmetric Gaussian mixtures," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 43, </volume> <pages> pp. 350-352, </pages> <month> Jan. </month> <year> 1957. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], [291], [292], [557], [147], [437], <ref> [96] </ref> has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [97] <author> T. A. C. M. Claasen and A. Jongepier, </author> <title> "Model for the power spectral density of quantization noise," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 29, </volume> <pages> pp. 914-917, </pages> <month> Aug. </month> <year> 1981. </year>
Reference-contexts: Sripad and Snyder [477] and Claasen and Jongepier <ref> [97] </ref> derived conditions under which the quantization error is white in terms of the joint characteristic functions of pairs of samples, two-dimensional analogs of Widrow's [529] condition. Zador [562] found high resolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers.
Reference: [98] <author> R. J. Clarke, </author> <title> Transform Coding of Images, </title> <publisher> Academic Press, </publisher> <address> Or-landa, FL, </address> <year> 1985. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], <ref> [98] </ref>, [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [99] <author> A. G. Clavier, P. F. Panter, and D. D. Grieg, </author> <title> "Distortion in a Pulse Count Modulation System," </title> <journal> AIEE Trans., </journal> <volume> vol. 66, </volume> <pages> pp. 989-1005, </pages> <year> 1947. </year>
Reference-contexts: High resolution theory also has a long tradition of analyzing the error process, beginning with Clavier et al. <ref> [99] </ref>, [100] and Bennett [43] and focusing on the distribution of the error, its spectrum and its correlation with the input.
Reference: [100] <author> A. G. Clavier, P. F. Panter, and D. D. Grieg, </author> <title> "PCM Distortion Analysis," </title> <booktitle> Electrical Engineering, </booktitle> <pages> pp. 1110-1122, </pages> <month> Nov. </month> <year> 1947. </year>
Reference-contexts: High resolution theory also has a long tradition of analyzing the error process, beginning with Clavier et al. [99], <ref> [100] </ref> and Bennett [43] and focusing on the distribution of the error, its spectrum and its correlation with the input.
Reference: [101] <author> D. Cohn, E. Riskin, and R. Ladner, </author> <title> "Theory and practice of vector quantizers trained on small training sets," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 16, </volume> <pages> pp. 54-65, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], [91], <ref> [101] </ref>, [321], [335], [131], [36].
Reference: [102] <author> R. R. Coifman and M. V. Wickerhauser. </author> <title> "Entropy-based algorithms for best basis selection," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 713-718, </pages> <month> March </month> <year> 1992. </year>
Reference: [103] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "Voronoi regions of lattices,second moments of polytopes,and quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 211-226, </pages> <month> March </month> <year> 1982 </year>
Reference-contexts: quantizers and fixed-rate permutation codes [48], generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima [503], results on the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [418], two seminal papers on lattice quantization by Conway and Sloane <ref> [103] </ref>, [104], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [281], Kieffer's study of <p> of uniformly distributed sources. (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities <ref> [103] </ref>, [104], [105], [459], [106], [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [104] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "Fast quantizing and decoding algorithms for lattice quantizers and codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 227-232, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: and fixed-rate permutation codes [48], generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima [503], results on the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [418], two seminal papers on lattice quantization by Conway and Sloane [103], <ref> [104] </ref>, rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [281], Kieffer's study of the <p> uniformly distributed sources. (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], <ref> [104] </ref>, [105], [459], [106], [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. <p> the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], <ref> [104] </ref>, [105], [459], [106], [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [105] <author> J. H. Conway and N. J. A. Sloane, </author> <title> "A fast encoding method for lattice codes and quantizers, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol 29, </volume> <pages> pp. 820-824, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: distributed sources. (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], [104], <ref> [105] </ref>, [459], [106], [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. <p> If not, then the scaling factor and lattice subset are usually chosen so that the resulting quantizer support region has large probability. In either case a low complexity method is needed for assigning binary sequences to the chosen code-vectors; i.e. for indexing. Conway and Sloane <ref> [105] </ref> found such a method for the important case that the support has the shape of an enlarged cell. For sources with infinite support, such as i.i.d. Gaussian, there is also the difficult question of how to quantize a source vector x lying outside the support region.
Reference: [106] <author> J. H. Conway and N. J. A. Sloane, </author> <title> Sphere Packings,Lattices and Groups, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: In this light, Gersho's conjecture is true if and only if at high rates one may obtain an asymptotically optimal quan-tizer for a uniform distribution by tessellating with T k . The latter statement has been proven for k = 1 (cf. <ref> [106] </ref>, p. 59) and for k = 2 by Fejes Toth (1959) [159]; see also [385]. For k = 3, it is known that the best lattice tessellation is the body-centered cubic lattice, which is generated by a truncated octahedron [35]. <p> One lower bound is the normalized moment of inertia of a sphere of the same di 36 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 mension: M k k + 2 2 2 ! 2 : (58) Another bound is given in <ref> [106] </ref>. One upper bound was developed by Zador; others derive from the currently best known tessellations (cf. [106], [5]). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for i.i.d. sources. <p> of the same di 36 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 mension: M k k + 2 2 2 ! 2 : (58) Another bound is given in <ref> [106] </ref>. One upper bound was developed by Zador; others derive from the currently best known tessellations (cf. [106], [5]). The Zador factors fi k and fl k can be computed straightforwardly for k = 1 and, also, for k 2 for i.i.d. sources. In some cases, simple closed form expressions can be found, e.g. for Gaussian, Laplacian, gamma densities. In other cases numerical integration can be used. <p> (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], [104], [105], [459], <ref> [106] </ref>, [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms. <p> best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], [104], [105], [459], <ref> [106] </ref>, [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [107] <author> P. C. Cosman, R. M. Gray, and M. Vetterli, </author> <title> "Vector quantization of image subbands: A survey," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> Vol.5, </volume> <pages> pp. 202-25. </pages> <month> February, </month> <year> 1996. </year>
Reference-contexts: The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see <ref> [107] </ref> or Chapter 10 of [196]. A combinatorial optimization method is given in [546]. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k-dimensional quantizer because of the constrained structure (transform and product code). <p> Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods <ref> [107] </ref>. A major breakthrough in performance and complexity came with the introduction of zerotrees [315], [466], [457], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [108] <author> P. C. Cosman, K. O. Perlmutter, S. M. Perlmutter, R. M. Gray, and R. A. Olshen, </author> <title> "Training sequence size and vector quantizer performance," </title> <booktitle> Proc. Twenty-Fifth Annual Asilomar Converence on Signals, Systems, and Computers, </booktitle> <address> Pacific Grove, Calif., </address> <pages> pp. 434-438. </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], <ref> [108] </ref>, [91], [101], [321], [335], [131], [36].
Reference: [109] <author> P. C. Cosman, S. M. Perlmutter, and K. O. Perlmutter, </author> <title> "Tree-structured vector quantization with significance map for wavelet image coding," </title> <booktitle> Proc. 1995 IEEE Data Compression Conf. (DCC), </booktitle> <editor> J. A. Storer and M. Cohn, Eds., </editor> <address> Snowbird, Utah, </address> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1995, </year> . 
Reference-contexts: The zerotree approach has been extended to vector quantization (e.g., <ref> [109] </ref>), but the slight improvement comes at a significant cost in added complexity. Rate-distortion ideas have been used to optimize the rate-distortion tradeoffs using wavelet packets by minimizing a Lagrangian distortion over code trees and bit assignments [427].
Reference: [110] <author> T .M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <address> Chichester, UK, </address> <year> 1991. </year>
Reference-contexts: How is it that two such disparate point densities do in fact yield the same distortion? The answer is provided by the asymptotic equipartition property (AEP) <ref> [110] </ref>, which is the key fact upon which most of information theory rests. For a stationary, ergodic source with continuous random variables, the AEP says that when dimension k is large, the k-dimensional probability density is approximately constant, except on a set with small probability.
Reference: [111] <author> D. R. Cox, </author> <title> "Note on grouping," </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> vol. 52, </volume> <pages> 543-547, </pages> <year> 1957. </year>
Reference-contexts: Lukaszewicz and H. Steinhaus [336] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd). Cox in 1957 <ref> [111] </ref> also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later [480], [159], [561]. B.
Reference: [112] <author> T.R . Crimmins, H. M. Horwitz, C. J. Palermo, and R. V. Palermo, </author> <title> "Minimization of Mean-Squared Error for Data Transmitted via Group Codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 15, </volume> <pages> pp. 72-78, </pages> <month> Jan. </month> <year> 1969. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], <ref> [112] </ref>, [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment. <p> Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], <ref> [112] </ref>, [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment. However, this result remained relatively unknown until rederived and generalized in [351].
Reference: [113] <author> R. E. Crochiere, S. M. Webber, and J. K. L. Flanagan. </author> <title> "Digital coding of speech in sub-bands," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 55, </volume> <pages> pp. 1069-1086, </pages> <month> Oct. </month> <year> 1976. </year>
Reference-contexts: Hence we content ourselves with the mention of a few highlights. The interested reader is referred to the book by Vetterli and Kovacevic on wavelets and subband coding [516]. Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. <ref> [113] </ref>. The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], [540].
Reference: [114] <author> I. Csiszar, </author> <title> "Generalized entropy and quantization problems," </title> <booktitle> Proc. Sixth Prague Conf., </booktitle> <pages> pp. 159-174, </pages> <year> 1973. </year>
Reference-contexts: A companion paper [144] considers similar bounds to the performance of vector quantizers with an analogous average-cell-size distortion measure. In 1973 Csiszar <ref> [114] </ref> presented a rigorous generalization of (52) to higher dimensional quantizers.
Reference: [115] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [116] <author> V. Cuperman and A. Gersho, </author> <title> "Vector predictive coding of speech at 16 Kb/s," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 33, </volume> <pages> pp. 685-696, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Figure 3 [235], <ref> [116] </ref>, [85], [417]. Alternatively, the encoder and decoder can share a finite set of states and a quantizer custom designed for each state.
Reference: [117] <author> C. C. Cutler, </author> <title> "Differential quantization of communication signals," </title> <type> U.S. Patent 2 605 361, </type> <month> July 29, </month> <year> 1952. </year>
Reference-contexts: Nevertheless, removing redundancy leads to much improved codes. Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo [129], but the most commonly cited early references are Cutler's patent <ref> [117] </ref> 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips GRAY AND NEUHOFF: QUANTIZATION 7 technical report on delta modulation [128].
Reference: [118] <author> T. Dalenius, </author> <title> "The problem of optimum stratification," </title> <journal> Skandi-navisk Aktuarietidskrift, </journal> <volume> vol. 33, </volume> <pages> pp. 201-213, </pages> <year> 1950. </year>
Reference-contexts: We conclude this subsection by mentioning early work that appeared in the mathematical and statistical literature and which, in hindsight, can be viewed as related to scalar quantization. Specifically, in 1950-1951 Dalenius et al. <ref> [118] </ref>, [119] used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus [336] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd).
Reference: [119] <author> T. Dalenius and M. Gurney, </author> <title> "The problem of optimum stratification II," </title> <journal> Skandinavisk Aktuarietidskrift, </journal> <volume> vol. 34, </volume> <pages> pp. 203-213, </pages> <year> 1951. </year>
Reference-contexts: We conclude this subsection by mentioning early work that appeared in the mathematical and statistical literature and which, in hindsight, can be viewed as related to scalar quantization. Specifically, in 1950-1951 Dalenius et al. [118], <ref> [119] </ref> used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus [336] (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd).
Reference: [120] <author> E. M. Deloraine and A. H. Reeves, </author> <title> "The 25th anniversary of pulse code modulation," </title> <journal> IEEE Spectrum, </journal> <pages> pp. 56-64, </pages> <month> May </month> <year> 1965. </year>
Reference: [121] <author> J. R.B. DeMarca and N. S. Jayant, </author> <title> "An algorithm for assigning binary indices to the codevectors of multidimensional quantiz-ers," </title> <booktitle> Proc. IEEE Int'l Conf. Commun., </booktitle> <pages> pp. 1128-1132, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The codes that do this are often called index assignments. Several specific index assignment methods were considered by Rydbeck and Sundberg [448]. DeMarca and Jayant in 1987 <ref> [121] </ref> introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho [568], who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include [210], [543], [287].
Reference: [122] <author> N. Demir and K. </author> <title> Sayood "Joint source/channel coding for variable length codes," </title> <booktitle> Proc. 1998 IEEE Data Compression Conf., </booktitle> <editor> J. A. Storer, M. Cohn, Eds., </editor> <publisher> Computer Society Press, </publisher> <pages> pp. 139-148, </pages> <month> March </month> <year> 1998. </year>
Reference-contexts: Here we mention only schemes which can be viewed as quantizers which are modified for use on a noisy channel and not those schemes which involve explicit channel codes. More general discussions can be found, e.g., in <ref> [122] </ref>. One approach to designing quantizers for use on noisy channels is to replace the distortion measure with respect to which a quantizer is optimized by the expected distortion over the noisy channel.
Reference: [123] <author> C. R. Davis and M. E. Hellman, </author> <title> "On tree coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 21, </volume> <pages> pp. 373-378, </pages> <month> July </month> <year> 1975. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], [11], [132], <ref> [123] </ref>, [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [124] <author> L. D. Davission, </author> <title> "Information rates for data compression," </title> <journal> IEEE WESCON, </journal> <note> Session 8, Paper 1, </note> <year> 1968. </year>
Reference-contexts: may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes <ref> [124] </ref>, [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including
Reference: [125] <editor> L. D. Davisson and R. M. Gray, Eds., </editor> <booktitle> Data Compression, </booktitle> <volume> vol. 14, </volume> <booktitle> in Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <editor> Dowden, Hutchinson and Ross, Stroudsburg, Penn., </editor> <year> 1976. </year>
Reference: [126] <author> L. D. Davisson, A. Leon-Garcia and D. L. Neuhoff, </author> <title> "New results on coding of stationary nonergodic sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 25, </volume> <pages> pp. 137-144, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], [127], <ref> [126] </ref>, [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [221], [469], <ref> [126] </ref>, [138], [479]. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (33).
Reference: [127] <author> L. D. Davisson and M. B. Pursley, </author> <title> "A direct proof of the coding theorem for discrete sources with memory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 21, </volume> <pages> pp. 301-310, </pages> <month> May </month> <year> 1975. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], <ref> [127] </ref>, [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions.
Reference: [128] <author> F. DeJager, </author> <title> "Delta modulation, a method of PCM transmission using a one-unit code," </title> <journal> Philips Research Rept., </journal> <volume> vol. 7, </volume> <year> 1952. </year>
Reference-contexts: Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo [129], but the most commonly cited early references are Cutler's patent [117] 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips GRAY AND NEUHOFF: QUANTIZATION 7 technical report on delta modulation <ref> [128] </ref>.
Reference: [129] <author> B. Derjavitch, E. M. Deloraine and V. Mierlo, </author> <title> French Patent No. </title> <type> 932, 140, </type> <month> Aug. </month> <year> 1946. </year>
Reference-contexts: Nevertheless, removing redundancy leads to much improved codes. Predictive quantization appears to originate in the 1946 delta modulation patent of Derjavitch, Deloraine, and Van Mierlo <ref> [129] </ref>, but the most commonly cited early references are Cutler's patent [117] 2,605,361 on "Differential quantization of communication signals" and on DeJager's Philips GRAY AND NEUHOFF: QUANTIZATION 7 technical report on delta modulation [128].
Reference: [130] <author> R. A. DeVore, B. Jawerth, and B. Lucier, </author> <title> "Image compression through wavelet transform coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 719-746, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], [540]. Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], <ref> [130] </ref>, [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [131] <author> L. Devroye, L. Gyorfi, and G. Lugosi, </author> <title> A Probabilistic Theory of Pattern Recognition, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], [91], [101], [321], [335], <ref> [131] </ref>, [36].
Reference: [132] <author> R. J. Dick, T. Berger, and F. Jelinek, </author> <title> "Tree encoding of Gaussian sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 332-336, </pages> <month> May </month> <year> 1974. </year> <journal> 54 IEEE TRANSACTIONS ON INFORMATION THEORY, </journal> <volume> VOL. 44, NO. 6, </volume> <month> OCTOBER </month> <year> 1998 </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], [11], <ref> [132] </ref>, [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [133] <author> E. Diday and J. C. Simon, </author> <title> "Clustering analysis," in Digital Pattern Recognition, </title> <editor> K. S. Fu, Ed. </editor> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1976. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represent a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology, cf. Anderberg [9], Harti-gan [238], or Diday and Simon <ref> [133] </ref>. MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimum distortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid.
Reference: [134] <author> R. L. Dobrushin and B. S. Tsybakov, </author> <title> "Information transmission with additional noise," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. IT-18, </volume> <pages> pp. </pages> <address> S293-S304, </address> <year> 1962. </year>
Reference-contexts: The usefulness of this modified distortion for source coding noisy sources was first seen by Dobrushin and Tsybakov (1962) <ref> [134] </ref> and was used by Fine (1965) [162] and Sakrison (1968) [452] to obtain information theoretic bounds an quantization and source coding for noisy sources. Berger (1971) [46] explicitly used the modified distortion in his study of Shannon source coding theorems for noise corrupted sources.
Reference: [135] <author> J. G. Dunham and R. M. Gray, </author> <title> "Joint source and noisy channel trellis encoding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 27, </volume> <pages> pp. 516-519, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Recently the method has been referred to as "channel-optimized quantization," where the quantization might be scalar, vector, or trellis. This approach was introduced in 1969 by Kurtenbach and Wintz [304] for scalar quantizers. A Shannon source coding theorem for trellis encoders using this distortion measure was proved in 1981 <ref> [135] </ref> and a Lloyd-style design algorithm for such encoders provided in 1987 [19]. A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa [303] and further studied in [157], [152], [153].
Reference: [136] <author> M. Ostendorf Dunham and R. M. Gray, </author> <title> "An algorithm for the design of labeled-transition finite-state vector quantizers," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 33, </volume> <pages> pp. 83-89, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], <ref> [136] </ref>, [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [137] <author> J.G. Dunn, </author> <title> "The performance of a class of n dimensional quantizers for a Gaussian source, </title> <booktitle> Proc. Columbia Symp. Signal Transmission Processing, </booktitle> <institution> Columbia Univ., </institution> <address> NY, </address> <pages> pp. 76-81, </pages> <year> 1965. </year> <title> Reprinted in Data Compression, </title> <editor> L. D. Davisson and R. M. Gray, Ed., </editor> <booktitle> Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <volume> vol. 14, </volume> <editor> Dowden, Hutchinson and Ross, Stroudsberg, </editor> <address> PA, </address> <year> 1975. </year>
Reference-contexts: From the above discussion, it should not be surprising that the first VQ intended as a practical technique had a reproduction codebook that was highly structured in order to reduce the complexity of encoding and decoding. Specifically, we speak of the fixed-rate vector quantizer introduced in 1965 by Dunn <ref> [137] </ref> for multidimensional i.i.d. Gaussian vectors. He argued that his code was effectively a permutation code as earlier used by Slepian [472] for channel coding, in that the reproduction codebook contains only codevectors that are permutations of each other. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily <ref> [137] </ref>, [60], [193], [184], [196] that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q i (X)(q j (X)X j )] = 0, for all i; j so that each component of the quantizer output
Reference: [138] <author> M. Effros, P.A. Chou, and R. M .Gray, </author> <title> "Variable-rate source coding theorems for stationary nonergodic sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Volume 40, No. 6, </volume> <pages> pp. </pages> <year> 1920 </year> <month> - </month> <year> 1925, </year> <month> November </month> <year> 1994. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], [127], [126], [282], [283], <ref> [138] </ref>, [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [221], [469], [126], <ref> [138] </ref>, [479]. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (33).
Reference: [139] <author> A. E. El Gamal and T. M. </author> <title> Cover, "Achievable rates for multiple descriptions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 851-857, </pages> <month> November </month> <year> 1982. </year>
Reference-contexts: This problem was first tackled in the information theory community in 1980 by Wolf, Wyner, and Ziv [536] and Ozarow [401] who de GRAY AND NEUHOFF: QUANTIZATION 51 veloped achievable rate regions and lower bounds to performance. The results were extended by El Gamal and Cover (1982) <ref> [139] </ref>, Ahlswede (1985) [6], and Zhang and Berger (1987) [573]. In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained [509] scalar quantizers for the multiple description problem.
Reference: [140] <author> A. E. El Gamal, L. A. Hemachandra, I. Shperling, and V. K. Wei, </author> <title> "Using simulated annealing to design good codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 33, </volume> <pages> pp. 116-123, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing <ref> [140] </ref>, [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65]. <p> It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523]. Simulated annealing has also been used to design such quantizers <ref> [140] </ref>, [152], [354].
Reference: [141] <author> P. Elias, </author> <title> "Predictive coding," </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1950. </year>
Reference-contexts: In 1950 Elias <ref> [141] </ref> provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], [300], [237], [511], [572].
Reference: [142] <author> P. Elias, </author> <title> "Predictive coding I and II," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. 1, </volume> <pages> pp. 16-33, </pages> <month> March </month> <year> 1955. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 <ref> [142] </ref>. Other early references include [395], [300], [237], [511], [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived.
Reference: [143] <author> P. Elias, </author> <title> "Bounds on performance of optimum quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 16, </volume> <pages> pp. 172-184, </pages> <month> March </month> <year> 1970. </year>
Reference-contexts: An excellent summary of the early work is contained in a 1970 paper by Elias <ref> [143] </ref>. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization. It is worth pointing out how the standard resembles and differs from the models considered thus far. <p> But as to the details it offered only that: "The complete proof is surprisingly long and will not be given here." Though Gish and Pierce were the first to informally derive (13), neither this paper nor any paper to date has provided a rigorous derivation. Elias (1970) <ref> [143] </ref> also made a rigorous analysis of scalar quantization, giving asymptotic bounds to the distortion of scalar quantizers with a rather singularly defined measure of distortion, namely, the rth root of the average of the rth power of the cell widths.
Reference: [144] <author> P. Elias, </author> <title> "Bounds and asymptotes for the performance of mul-tivariate quantizers," </title> <journal> Annals of Mathematical Statistics, </journal> <volume> vol. 41, No. </volume> <pages> 4 pp. 1249-1259, </pages> <year> 1970. </year>
Reference-contexts: Elias (1970) [143] also made a rigorous analysis of scalar quantization, giving asymptotic bounds to the distortion of scalar quantizers with a rather singularly defined measure of distortion, namely, the rth root of the average of the rth power of the cell widths. A companion paper <ref> [144] </ref> considers similar bounds to the performance of vector quantizers with an analogous average-cell-size distortion measure. In 1973 Csiszar [114] presented a rigorous generalization of (52) to higher dimensional quantizers.
Reference: [145] <author> Y. Ephraim and R. M. Gray, </author> <title> "A unified approach for encoding clean and noisy sources by means of waveform and autoregres-sive vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 34, </volume> <pages> pp. 826-834, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This result was subsequently extended to a more general class of distortion measures include the input-weighted quadratic distortion of Ephraim and Gray <ref> [145] </ref>, where a generalized Lloyd algorithm for design was presented. Related results and approaches can be found in Witsen-hausen's (1980) [535] treatment of rate distortion theory with modified (or "indirect") distortion measures, and in the Occam filters of Natarajan (1995) [370]. H.
Reference: [146] <author> W. H. Equitz, </author> <title> "A new vector quantization clustering algorithm," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 1568-1575, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor <ref> [146] </ref> (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65]. <p> Techniques of this type may be found in [44], [176], [88], [89], [334], <ref> [146] </ref>, [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [147] <author> W. Equitz and T. </author> <title> Cover, "Successive refinement of information," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 269-275, </pages> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], [291], [292], [557], <ref> [147] </ref>, [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [148] <author> T. Ericson, </author> <title> "A result on delay-less information transmission," </title> <booktitle> Abstracts of the IEEE Int. Symp. Inform. Theory, </booktitle> <address> Grignano, Italy, </address> <month> June, </month> <year> 1979. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], [414], <ref> [148] </ref>, [534], [178], [381], [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [149] <author> T. Eriksson and E. Agrell, </author> <title> "Lattice-based quantization, Part II," </title> <type> Report No. 18, </type> <institution> Department of Information Theory, Chalmers University of Technology, Goteborg, Sweden, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In most dimensions the best known tessellation is a lattice. However, tessellations that are better than the best known lattices have recently been found for dimensions seven and nine by 24 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 Agrell and Eriksson <ref> [149] </ref>. <p> Specifically, Hui and Neuhoff [253], [254], [255] have found that for a Gaussian density with variance oe 2 lim N N ln N N!1 4 N 2 ln N This result was independently found by Eriksson and Agrell <ref> [149] </ref>. Moreover, it was shown that overload distortion is asymptotically negligible and that D N =( 2 N =12) ! 1, which is the first time this has been proved for a source with infinite support. <p> In a related result, the asymptotic form of the optimal scaling factor for lattice quantizers has also been found recently for an i.i.d. Gaussian source [359], <ref> [149] </ref>. We conclude this subsection by mentioning some gaps in rigorous high resolution theory. One, of course, is a proof or counterproof of Gersho's conjecture in dimensions three and higher. Another is the open question of whether the best tessellation in three or more dimensions is a lattice. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the code-book and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. [580], [151], [359], <ref> [149] </ref>, [409]. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [150] <author> A. M. Eskicioglu and P. S. Fisher, </author> <title> "Image Quality Measures and Their Performance," </title> <journal> IEEE Trans. Comm., </journal> <volume> Vol 43, </volume> <pages> pp. 2959-2965, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], [386], <ref> [150] </ref>, [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher <ref> [150] </ref> and Nill [386], [387]. The Bennett integral has been extended to this type of distortion, and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed [186], [316].
Reference: [151] <author> M. Vedat Eyuboglu and G. David Forney, Jr., </author> <title> "Lattice and trellis quantization with lattice- and trellis-bounded codebooks-High-rate theory for memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.39, </volume> <pages> pp. 46-59, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the code-book and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. [580], <ref> [151] </ref>, [359], [149], [409]. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [152] <author> N. Farvardin, </author> <title> "A study of vector quantization for noisy channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 799-809, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa [303] and further studied in [157], <ref> [152] </ref>, [153]. The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412]. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523]. <p> It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523]. Simulated annealing has also been used to design such quantizers [140], <ref> [152] </ref>, [354].
Reference: [153] <author> N. Farvardin, </author> <title> "On the performance and complexity of channel optimized vector quantizers," in Speech Recognition and Coding: </title> <booktitle> New Advances and Trends, </booktitle> <pages> pp. 699-704, </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa [303] and further studied in [157], [152], <ref> [153] </ref>. The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412]. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523].
Reference: [154] <author> N. Farvardin and F. Y. Lin, </author> <title> "Performance of entropy-constrained block transform quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 1433-1439, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Berger refined his approach to entropy-constrained quantizer design in [48]. Variable-rate quantization was also extended to DPCM and transform coding, where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing [398], <ref> [154] </ref>. We note, however, that the variable-rate quantization analysis for DPCM suffers from the same flaws as the fixed-rate quantization analysis for DPCM.
Reference: [155] <author> N. Farvardin and J. W. Modestino, </author> <title> "Optimal quantizer performance for a class of non-Gaussian memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 30, </volume> <pages> pp. 485-497, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general formulation of optimal quantization in the next section. In 1984 Farvardin and Modestino <ref> [155] </ref> extended Berger's [47] necessary conditions for optimality of an entropy-constrained scalar quantizer to more general distortion measures and described two design algorithms: the first is similar to Berger's iterative algorithm, but the second was a fixed-point algorithm which can be considered as a natural extension of Lloyd's Method I from <p> In 1989 Chou et al. [93] developed a generalized Lloyd algorithm for entropy constrained vector quantization that generalized Berger's [47], [48] Lagrangian formulation for scalar quantization and Farvardin and Modestino's fixed-point design algorithm <ref> [155] </ref> to vectors. Optimality properties for minimizing a Lagrangian distortion D (q) + R (q) were derived, where rate could be either average length or entropy.
Reference: [156] <author> N. Farvardin and J. W. Modestino, </author> <title> "Rate-distortion performance of DPCM schemes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 31, </volume> <pages> pp. 402-418, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Indeed, it is still an open question whether this type of analysis, which typically uses Bennett and Panter-Dite formulas, is asymptotically correct. Nevertheless, the results of such high-resolution approximations are widely accepted and often compare well with experimental results [265], <ref> [156] </ref>. <p> Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], <ref> [156] </ref>, [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS
Reference: [157] <author> N. Farvardin and V. Vaishampayan, </author> <title> "Optimal quantizer design for noisy channels: An approach to combined source-channel coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 33, </volume> <pages> pp. 827-838, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa [303] and further studied in <ref> [157] </ref>, [152], [153]. The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412]. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523].
Reference: [158] <editor> L. Fejes Toth, Lagerungen in der Ebene, auf der Kugel und im Raum, </editor> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1953. </year>
Reference-contexts: In 1959 Fejes Toth described the specific application of Steinhaus' problem in two dimensions to a source with a uniform density on a bounded support region and to quantization with an asymptotically large number of points [159]. Using an earlier inequality of his <ref> [158] </ref>, he showed that the optimal two-dimensional quantizer under these assumptions tessellated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. It was rederived in a 1964 Bell Laboratories Technical Memorandum by Newman [385]; its first appearance in English.
Reference: [159] <author> L. Fejes Toth, </author> <title> "Sur la representation d'une population infinie par un nombre fini d'elements," </title> <journal> Acta Math. Acad. Sci. Hung., </journal> <volume> vol. 10, </volume> <pages> pp. 76-81, </pages> <year> 1959. </year>
Reference-contexts: Cox in 1957 [111] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later [480], <ref> [159] </ref>, [561]. B. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable "redundancy" that scalar quantization could not exploit. The term "redundancy" was commonly used in the early days and is still popular in some of the quantization literature. <p> As will be discussed later, this belief is contradicted both by Shan-non's work, which demonstrated strictly improved performance using vector quantizers even for memoryless sources, and by the early work of Fejes Toth (1959) <ref> [159] </ref>. Nevertheless, removing redundancy leads to much improved codes. <p> In addition, Gish and Pierce observed that when coding vectors, performance could be improved by using quan-tizer cells other than the cube implicitly used by uniform scalar quantizers and noted that the hexagonal cell was superior in two dimensions, as originally demonstrated by Fejes Toth <ref> [159] </ref> and Newman [385]. Though uniform quantization is asymptotically best for entropy-constrained quantization, at lower rates nonuniform quantization can do better, and a series of papers explored algorithms for designing them. <p> In 1959 Fejes Toth described the specific application of Steinhaus' problem in two dimensions to a source with a uniform density on a bounded support region and to quantization with an asymptotically large number of points <ref> [159] </ref>. Using an earlier inequality of his [158], he showed that the optimal two-dimensional quantizer under these assumptions tessellated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. <p> The latter statement has been proven for k = 1 (cf. [106], p. 59) and for k = 2 by Fejes Toth (1959) <ref> [159] </ref>; see also [385]. For k = 3, it is known that the best lattice tessellation is the body-centered cubic lattice, which is generated by a truncated octahedron [35]. It has not been proven that this is the best tessellation, though one would suspect that it is. <p> They paper also explores what happens when the distribution is not absolutely continuous. In the second paper, Fejes Toth <ref> [159] </ref> showed that for a two-dimensional random vector that is uniformly distributed on the unit square, the mean squared error of any N point quantizer is bounded from below by M (hexagon)=N . This result was independently rederived in a simpler fashion by Newman (1964) [385].
Reference: [160] <author> Y. S. Feng and N. M. Nasrabadi, </author> <title> "Dynamic address-vector quantisation of RGB colour images," </title> <booktitle> IEE Proc., Part I, Com-mun. Speech Vision, </booktitle> <volume> vol. 138, </volume> <pages> pp. 225-231, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: This is a kind of interblock lossless coding [384], [410], [428]. Address-vector quantization, introduced by Nasrabadi and Feng [371] (see also <ref> [160] </ref>, [373]), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [161] <author> T. L. </author> <title> Fine, "Properties of an Optimal Digital System and Applications," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 10, </volume> <pages> pp. 287-296, </pages> <month> Oct. </month> <year> 1964. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization <ref> [161] </ref>, [203], [41], but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. We will return to this issue when we consider finite-state vector quantizers. <p> Finite-state vector quantizer theory has been developed for finite-state quantizers <ref> [161] </ref>, [178], [179], a variety of design methods exist [174], [175], [136], [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [162] <author> T. </author> <title> Fine, "Optimum mean-square quantization of a noisy input," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-11, </volume> <pages> pp. 293-294, </pages> <month> Apr. </month> <year> 1965. </year>
Reference-contexts: The usefulness of this modified distortion for source coding noisy sources was first seen by Dobrushin and Tsybakov (1962) [134] and was used by Fine (1965) <ref> [162] </ref> and Sakrison (1968) [452] to obtain information theoretic bounds an quantization and source coding for noisy sources. Berger (1971) [46] explicitly used the modified distortion in his study of Shannon source coding theorems for noise corrupted sources.
Reference: [163] <author> T. L. </author> <title> Fine, "The response of a particular nonlinear system with feedback to each of two random processes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 255-264, </pages> <month> Mar. </month> <year> 1968. </year>
Reference-contexts: notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process <ref> [163] </ref>, [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 X X 2 - . . .
Reference: [164] <author> T. R. </author> <title> Fischer "A pyramid vector quantizer," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 32, </volume> <pages> pp. 568-583, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Additional structure can be imposed for faster searches with virtually no loss of performance, as in Fisher's pyramid VQ <ref> [164] </ref>, which takes advantage of the asymptotic equipartition property to choose a structured support region for the quantizer. Tree-structured VQ uses a tree-structured reproduction codebook with a matched tree-structured search algorithm. A tree-structured VQ with far less memory is provided by a multistage or residual VQ. <p> The most general formulation of product codes has been given by Chan and Gersho [82]. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later. Fischer's pyramid VQ <ref> [164] </ref> is also a kind of shape-gainVQ. In this case, the codevectors of the shape code-book are constrained to lie on the surface of a k-dimensional pyramid, namely, the set of all vectors whose components have magnitudes summing to one. Pyramid VQ's are very well suited to i.i.d. Laplacian sources.
Reference: [165] <author> T. R. </author> <title> Fischer "Geometric source coding and vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 35, </volume> <pages> pp. 137-145, </pages> <month> July </month> <year> 1989. </year>
Reference: [166] <author> T. R. Fischer, M. W. Marcellin, and M. Wang, </author> <title> "Trellis-coded vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 1551-1566, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], <ref> [166] </ref>, [167], [522], [343], [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [167] <author> T. R. Fischer and M. Wang, </author> <title> "Entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 415-426, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], [166], <ref> [167] </ref>, [522], [343], [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [168] <author> S. Fix, </author> <title> "Rate distortion functions for continuous alphabet mem-oryless sources," </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <year> 1977. </year>
Reference-contexts: For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms <ref> [168] </ref>, [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. For other cases, the Blahut algorithm [52] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small.
Reference: [169] <author> J. K. Flanagan, D. R. Morrell, R. L. Frost, C.J. Read, and B. E. Nelson, </author> <title> "Vector quantization codebook Generation using simulated annealing," </title> <journal> pp. </journal> <pages> 1759-1762, </pages> <booktitle> Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], <ref> [169] </ref>, [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [170] <author> P. Fleischer, </author> <title> "Sufficient conditions for achieving minimum distortion in a quantizer," </title> <booktitle> IEEE Int. Conv. Rec., </booktitle> <pages> pp. 104-111, </pages> <year> 1964. </year>
Reference-contexts: The latter were essential for later extensions to vector quantizers and to the development of many quantizer optimization procedures. To our knowledge, the first mention of Lloyd's work in the IEEE literature came in 1964 with Fleischer's <ref> [170] </ref> derivation of a sufficient condition (namely, that the log of the source density be concave) in order that the optimal quantizer be the only locally optimal quantizer, and consequently, that Lloyd's Method I yields a globally optimal quantizer. (The condition is satisfied for common densities such as Gaussian and Laplacian.)
Reference: [171] <author> B. A. </author> <title> Flury "Principal points," </title> <journal> Biometrika, </journal> <volume> vol. 77, No. 1, </volume> <pages> pp. 31-41, </pages> <year> 1990. </year>
Reference-contexts: It is difficult to resist pointing out, however, that in 1990 Lloyd's algorithm was rediscovered in the statistical literature under the name of "principal points," which are distinguished from traditional k-means by the assumption of an absolutely continuous distribution instead of an empirical distribution <ref> [171] </ref>, [496], a formulation included in the VQ formulation for a general distribution. Unfortunately, these works reflect no awareness of the rich quantization literature. Most quantizers today are indeed uniform and scalar, GRAY AND NEUHOFF: QUANTIZATION 17 but are combined with prediction or transforms.
Reference: [172] <author> E. Forgey, </author> <title> "Cluster analysis of multivariate data: efficiency vs. interpretability of classification," </title> <journal> Biometrics, </journal> <volume> vol. 21, </volume> <pages> pp. 768, </pages> <year> 1965. </year> <note> (Abstract) </note>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960's the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey <ref> [172] </ref>, Ball and Hall [29], [230], Jancey [263], and in 1969 by MacQueen [341] (the "k-means" algorithm).
Reference: [173] <author> G. D. Forney, Jr., </author> <title> "The Viterbi Algorithm," </title> <journal> Proc. IEEE, </journal> <volume> vol. 61, </volume> <pages> pp. 268-278, </pages> <month> March </month> <year> 1973. </year>
Reference-contexts: Since the shift register is finite, the tree becomes redundant and new nodes will correspond to previously seen states so that the tree diagram becomes a merged tree or trellis, which can be searched by a dynamic programming algorithm, the Viterbi algorithm, cf. <ref> [173] </ref>. In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], [11], [132], [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [174] <author> J. Foster and R. M. Gray, </author> <title> "Finite-state vector quantization," </title> <booktitle> Abstracts of the 1982 IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Les Arcs France, </address> <month> June </month> <year> 1982. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist <ref> [174] </ref>, [175], [136], [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [175] <author> J. Foster, R. M. Gray, and M. Ostendorf Dunham, </author> <title> "Finite-state vector quantization for waveform coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 31, </volume> <pages> pp. 348-359, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], <ref> [175] </ref>, [136], [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [176] <author> J. H. Friedman, F. Baskett and L. J. Shustek, </author> <title> "An algorithm for finding nearest neighbors," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 24, </volume> <pages> pp. 1000-1006, </pages> <month> Oct. </month> <year> 1975. </year>
Reference-contexts: Then to encode a source vector x, one applies the prequantiza-tion, finds the index of the prequantization cell in which x is contained, and performs a full search on the corresponding bucket for the closest codevector to x. Techniques of this type may be found in [44], <ref> [176] </ref>, [88], [89], [334], [146], [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [177] <author> R. L. Frost, C. F. Barnes, and F. Xu, </author> <title> "Design and performance of residual quantizers," </title> <booktitle> in Proc. Data Compression Conf., </booktitle> <editor> J. A. Storer and J. H. Reif, Eds., </editor> <publisher> IEEE Computer Society Press, </publisher> <address> Snow-bird, Utah, </address> <pages> pp. 129-138, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal of) increased complexity. And more sophisticated design algorithms (than the greedy one) can also have benefits [32], <ref> [177] </ref>, [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [178] <author> N. T. Gaarder and D. Slepian, </author> <title> "On optimal finite-state digital transmission systems," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 167-186, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], [414], [148], [534], <ref> [178] </ref>, [381], [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing. <p> Finite-state vector quantizer theory has been developed for finite-state quantizers [161], <ref> [178] </ref>, [179], a variety of design methods exist [174], [175], [136], [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [179] <author> G. Gabor and Z. Gyorfi, </author> <title> "Recursive Source Coding," </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], <ref> [179] </ref>, a variety of design methods exist [174], [175], [136], [236], [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [180] <author> R. G. Gallager, </author> <title> Information Theory and Reliable Communication, </title> <publisher> Wiley, </publisher> <address> NY, </address> <year> 1968. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. <ref> [180] </ref>, [45], [46], [218], [549], [127], [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> It is not known whether or not ffi k+1 (R) is always less than or equal to ffi k (R). However, it can be shown that subadditivity implies (cf. <ref> [180] </ref>, p. 112) ffi (R) = lim ffi k (R): (24) Hence high dimensional quantizers can do as well as any quantizer. Note that (23) and (24) both hold for the special cases of fixed-rate quantizers as well as for variable-rate quantizers. <p> Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder [565] F. Quantization for Noisy Channels The separation theorem of information theory [464], <ref> [180] </ref> states that nearly optimal communication of an information source over a noisy channel can be accomplished by separately quantizing or source coding the source and channel coding or error-control coding the resulting encoded source for reliable transmission over a noisy channel.
Reference: [181] <author> R. G. </author> <title> Gallager "Variations on a theme by Huffman," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 24, </volume> <pages> pp. 668-674, </pages> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: Moreover, tighter bounds have been developed. For example Gallager <ref> [181] </ref> has shown that the entropy can be at most P max +:0861 smaller than the average length of the Huffman code, when P max , the largest of the P i 's, is less than 1/2. See [73] for discussion of this and other bounds.
Reference: [182] <author> N. C. Gallagher, Jr., </author> <title> "Discrete Spectral Phase Coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 22, </volume> <pages> pp. 622-624, </pages> <month> Sept. </month> <year> 1976. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed <ref> [182] </ref>, [183], [407], [406], [61], [62], [530], [489], [490], [483], [485], [488], [360].
Reference: [183] <author> N. C. Gallagher, Jr., </author> <title> "Quantizing Schemes for the Discrete Fourier Transform of a Random Time-Series," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 24, </volume> <pages> pp. 156-163, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], <ref> [183] </ref>, [407], [406], [61], [62], [530], [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase.
Reference: [184] <author> N. C. Gallagher and J. A. Bucklew, </author> <title> "Properties of minimum mean squared error block quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 105-107, </pages> <month> Jan. </month> <year> 1982. </year>
Reference-contexts: The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily [137], [60], [193], <ref> [184] </ref>, [196] that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q i (X)(q j (X)X j )] = 0, for all i; j so that each component of the quantizer output is orthogonal to
Reference: [185] <author> Z. Gao, F. Chen, B. Belzer, and J. Villasenor, </author> <title> "A comparison of the Z, E 8 , and Leech lattices for image subband quantization," </title> <booktitle> in Proc.1995 IEEE Data Compression Conf., </booktitle> <pages> pp. 312-321, </pages> <editor> J. A. GRAY AND NEUHOFF: QUANTIZATION 55 Storer and M. Cohn, Eds., </editor> <publisher> IEEE Computer Society Press, </publisher> <address> Snow-bird, Utah, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], <ref> [185] </ref> and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [186] <author> W. R. Gardner and B. D. Rao, </author> <title> "Theoretical Analysis of the High-Rate Vector Quantization of LPC Parameters," </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> Vol 3, </volume> <pages> pp. 367-381, </pages> <month> Septem-ber </month> <year> 1995. </year>
Reference-contexts: measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], [386], [150], <ref> [186] </ref>, [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> The basic idea for this distortion measure was introduced by Gardner and Rao <ref> [186] </ref> to model a perceptual distortion measure for speech, where the matrix B (y) is referred to as the "sensitivity matrix." The requirement for the existence of the derivatives of third order and for the B (y) to be positive definite were added in [316] as necessary for the analysis. <p> Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher [150] and Nill [386], [387]. The Bennett integral has been extended to this type of distortion, and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed <ref> [186] </ref>, [316]. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently been extended to nondif-ference distortion measures such as (x y) t B x (x y) <ref> [186] </ref>, [316], [325]. In any event both theories are the most fully developed for the squared-error distortion measure, especially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure. For squared-error distortion, it is simply that the variance of the source be finite.
Reference: [187] <author> M. Garey and D. S. Johnson and H. S. Witsenhausen, </author> <title> "The complexity of the generalized Lloyd-Max problem," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 255-266, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [281], Kieffer's study of the convergence rate of Lloyd's algorithm [280], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard <ref> [187] </ref>. Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [195], [214], [342], [372]. 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 F.
Reference: [188] <author> D. P. de Garrido, L. Lu, and W. A. Pearlman, </author> <title> "Conditional entropy-constrained vector quantization of frame difference sub-band signals," </title> <booktitle> Proc. IEEE Int'l Conf. Image Proc. Austin, Part 1 (of 3), </booktitle> <pages> pp. 745-749, </pages> <year> 1994. </year>
Reference-contexts: One can of course also make the lossless code depend on the state, or be conditional on the previous binary codeword. One can also use a memoryless VQ combined with a conditional lossless code (conditioned on the previous binary codeword) designed with a conditional entropy constraint [95], <ref> [188] </ref>. A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar. This is a kind of interblock lossless coding [384], [410], [428].
Reference: [189] <author> N. L. Gerr and S. Cambanis, </author> <title> "Analysis of delayed delta modulation," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 32, </volume> <pages> pp. 496-512, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], <ref> [189] </ref>, [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON
Reference: [190] <author> N. L. Gerr and S. Cambanis, </author> <title> "Analysis of adaptive differential PCM of a stationary Gauss-Markov input," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. </volume> <pages> 3e, pp. 350-359, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], <ref> [190] </ref>, [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION
Reference: [191] <author> A. Gersho, </author> <title> "Stochastic stability of delta modulation," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 51, </volume> <pages> pp. 821-841, </pages> <month> April </month> <year> 1972. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability <ref> [191] </ref>, [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear
Reference: [192] <author> A. Gersho, </author> <title> "Principles of Quantization," </title> <journal> IEEE Trans. Circuits Syst, </journal> <volume> vol. 25, </volume> <pages> pp. 427-436, </pages> <month> July </month> <year> 1978. </year>
Reference: [193] <author> A. Gersho, </author> <title> "Asymptotically optimal block quantization," </title> <journal> IEEE Trans. Inform. Theory., </journal> <volume> vol. 25, </volume> <pages> pp. 373-380, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Unfortunately, the results of Zador's thesis were not published until 1982 [563] and were little known outside of Bell Laboratories until 14 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 Gersho's important paper of 1979 <ref> [193] </ref>, to be described later. Zador's dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. <p> The most important paper on quantization during the 1970's was without a doubt Gersho's paper on "Asymptotically optimal block quantization" <ref> [193] </ref>. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily [137], [60], <ref> [193] </ref>, [184], [196] that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q i (X)(q j (X)X j )] = 0, for all i; j so that each component of the quantizer output is orthogonal <p> Both (28) and the more general formula (27) are called Bennett's integral. The extension of Bennett's integral to vector quan-tizers was first made by Gersho (1979) <ref> [193] </ref> for quantizers with congruent cells for which the concept of inertial profile was not needed, and then to vector quantizers with varying cell shapes (and codevector placements) by Na and Neuhoff (1995) [365]. <p> Unfortunately, it is not GRAY AND NEUHOFF: QUANTIZATION 23 known how to find the best inertial profile. Indeed, it is not even known what functions are allowable as inertial profiles. However, Gersho (1979) <ref> [193] </ref> made the now widely accepted conjecture that when rate is large, most cells of a k-dimensional quantizer with rate R and minimum or nearly minimum MSE are approximately congruent to some basic tessellating 4 k-dimensional cell shape T k . <p> In this case, the optimum inertial profile is a constant and Bennett's integral can be minimized by variational techniques or Holder's inequality [222], <ref> [193] </ref>, resulting in the optimal point density fl f k+2 (x) f k+2 (x 0 ) dx 0 and the following approximation to the operational distortion-rate function: for large R ffi k (R) = M k fi k oe 2 2 2R j Z k (R); (30) where M k j <p> Since fixed-rate coding is a special case of variable-length coding, it must be that fl k is less than or equal to fi k in (30). This can be directly verified using Jensen's inequality <ref> [193] </ref>. In the case of scalar quantization (k = 1), the optimality of the uniform point density and the operational distortion-rate function ffi 1;L (R) were found by Gish and Pierce (1968) [204]. <p> Gersho <ref> [193] </ref> used the argument given above to find the form of ffi k;1 (R) given in (32). As with fixed-rate quantization, we shall proceed under the assumption that Gersho's conjecture is correct, in which case c k = b k = M k . <p> This operational distortion-rate function was also derived by Zador [561], who showed that his unknown factors b k and c k converged to 1 2e . The derivation given here is due to Gersho <ref> [193] </ref>. Notice that in this limiting case, there is no doubt about the constant M . As previously mentioned the M k 's are subadditive, so that they are smallest when k is large. <p> As previously mentioned the M k 's are subadditive, so that they are smallest when k is large. Similarly, for stationary sources it can be shown that the sequence flog fi k g is also subadditive <ref> [193] </ref>, so that they too are smallest when k is large. Therefore another expression for the above Zador-Gersho function is Z (R) = inf k Z k (R). E. <p> Therefore, choosing either k or L large makes fl kL = 2 2h kL as small as possible, namely as small as fl j lim k!1 fl k . Interestingly, fl = fi j lim k!1 fi k , as shown by Gersho <ref> [193] </ref>, who credits Thomas Liggett. It follows immediately that the best possible performance of vector quantizers with block entropy coding is given by ffi (R) = M fioe 2 2 2R , which is the operational distortion-rate function of fixed-rate quantizers. <p> On the one hand, this is like Ben-nett's integral in that f (1) (x), and consequently (x), can be arbitrary. On the other hand, it is like Zador's result (or Gersho's generalization of Bennett's integral <ref> [193] </ref>) in that, in essence, it is assumed that the quantizers have optimal cell shapes. <p> then combined the above with Csiszar's result (53) to show that under fairly weak conditions (finite differential entropy and finite output entropy for some ff &gt; 0) the output entropy H ff and the distortion D ff are asymptotically related via lim D ff which is what Gersho derived informally <ref> [193] </ref>. The generalization of Bennett's integral to fixed-rate vector quantizers with rather arbitrary cell shapes was accomplished by Na and Neuhoff (1995) [365], who presented both informal and rigorous derivations. <p> This was first mentioned in Panter-Dite [405] and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis <ref> [193] </ref>, [56], [57]. So there is no direct way to construct optimal vector quan tizers with the high resolution philosophy. <p> The resulting Voronoi partition is a tessellation with all cells (except for those overlapping the overload region) having the same shape, size and orientation. Lattice quantization was proposed by Gersho <ref> [193] </ref> because of its near optimality for high resolution variable-rate quantization and, also, its near optimality for high resolution fixed-rate quantization of uniformly distributed sources. (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that
Reference: [194] <author> A. Gersho, </author> <title> "Optimal Nonlinear Interpolative Vector Quantization," </title> <journal> IEEE Trans. Commun. </journal> <volume> Vol.38, </volume> <pages> pp. 1285-1287, </pages> <month> Sep. </month> <year> 1990. </year>
Reference-contexts: As an example of other possibilities, one could also extract the first k even samples, followed by the first k odd samples, the next k even samples, and so on. This subsampling could be useful for a multiresolution reconstruction, as in interpolative vector quantization [234], <ref> [194] </ref>. For other types of data there may be no canonical extraction method. For example, in stereo speech the k-dimensional vectors might consist just of left samples, or just of right samples, or half from each, or k from the left followed by k from the right, etc.
Reference: [195] <author> A. Gersho and V. Cuperman, </author> <title> "Vector Quantization: A pattern-matching technique for speech coding," </title> <journal> IEEE Commun. Magazine, </journal> <volume> vol. 21, </volume> <pages> pp. 15-21, </pages> <month> Dec. </month> <year> 1983. </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject <ref> [195] </ref>, [214], [342], [372]. 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 F. The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [196] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], [374], <ref> [196] </ref>, [424], [50], [458] and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], <ref> [196] </ref>, [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200]. <p> The resulting optimality properties are summarized below. The proofs are simple and require no calculus of variations or differentiation. Proofs may be found, e.g., in [94], <ref> [196] </ref>. * For a fixed lossy encoder ff , regardless of the lossless encoder fl , the optimal reproduction decoder fi is given by fi (i) = argmin y the output minimizing the conditional expectation of the distortion between the output and the input given that the encoder produced index i. <p> The centroid property of optimal reproduction decoders has interesting implications in the special case of a squared-error distortion measure, where it follows easily [137], [60], [193], [184], <ref> [196] </ref> that * E [q (X)] = E [X], so that the quantizer output can be considered as an unbiased estimator of the input. * E [q i (X)(q j (X)X j )] = 0, for all i; j so that each component of the quantizer output is orthogonal to each <p> Bennett showed that in the high resolution case, the power spectral density of the quantizer error with uniform quantization is approximately white (and uniformly distributed) provided the assumptions of the high resolution theory are met and the joint density of sample pairs is smooth. (See also Section 5.6 of <ref> [196] </ref>.) Bennett also found exact expressions for the power spectral density of a uniformly quantized Gaussian process. <p> In contrast to code-books to be considered later these will be called unstructured. As a group these techniques use substantial amounts of additional memory in order to significantly reduce arithmetic complexity. A variety of such techniques are mentioned in Section 12.16 of <ref> [196] </ref>. A number of fast search techniques are similar in spirit to the following: the Euclidean distances between all pairs of codevectors are precomputed and stored in a table. Now, given a source vector x to quantize, some initial codevector ~y is chosen. <p> It has been successfully used for video coding [518], [75]. B. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in <ref> [196] </ref>, [458]. Lattice Quantizers Lattice quantization can be viewed as a vector generalization of uniform scalar quantization. <p> The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of <ref> [196] </ref>. A combinatorial optimization method is given in [546]. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k-dimensional quantizer because of the constrained structure (transform and product code). <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQs can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) [342], [94], [439], <ref> [196] </ref> and with larger branching factors than two or even variable branching factors [460]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [53], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm [94], [439], <ref> [196] </ref>. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], [136], [236], [15], [16], [286], <ref> [196] </ref>. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [197] <author> A. Gersho and B. Ramamurthi, </author> <title> "Image coding using vector quantization," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 428-431, </pages> <address> Paris, </address> <month> April </month> <year> 1982. </year>
Reference: [198] <author> J. D. Gibson, </author> <title> "Adaptive prediction in speech differential encoding systems," </title> <journal> Proc. IEEE, </journal> <volume> vol. 68, </volume> <pages> pp. 488-525, </pages> <month> April </month> <year> 1980. </year>
Reference-contexts: Comprehensive discussions may be found in books [265], [374], [196], [424], [50], [458] and survey papers [264], <ref> [198] </ref>. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized.
Reference: [199] <author> J. D. Gibson and K. Sayood, </author> <title> "Lattice quantization," </title> <journal> Advances in Electronics and Electron Phys., </journal> <volume> vol. 72, </volume> <pages> pp. 259-330, </pages> <year> 1988. </year>
Reference-contexts: assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], [104], [105], [459], [106], <ref> [199] </ref>. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [200] <author> N. Gilchrist and C. Grewin, </author> <title> Collected papers on digital audio bit-rate reduction, </title> <booktitle> Audio Engineering Society, </booktitle> <address> New York, </address> <year> 1996. </year>
Reference-contexts: For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], <ref> [200] </ref>.
Reference: [201] <author> B. Girod, </author> <title> "Rate-constrained Motion Estimation", in Visual Commun. and Image Processing VCIP'94, </title> <editor> A.K.Katsaggelos (ed.), </editor> <booktitle> Proc. SPIE vol. </booktitle> <volume> 2308, </volume> <pages> pp. 1026-1034, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Modern video coding schemes often incorporate the La-grangian distortion viewpoint for accomplishing rate control, while using predictive quantization in a general sense through motion compensation and uniform quantizers with optimized lossless coding of transform coefficients for the intraframe coding (cf. <ref> [201] </ref>, [202]). III. Quantization Basics: Encoding, Rate, Distortion, and Optimality This section presents, in a selfcontained manner, the basics of memoryless quantization, that is, vector quantizers which operate independently on successive vectors. For brevity, we omit the "memoryless" qualifier for most of the rest of this section.
Reference: [202] <author> B. Girod, R. M. Gray, J. Kovacevic. and M. Vetterli. </author> <title> "Image and Video Coding," part of "The past, present, and future of image and multidimensional signal processing," edited by R. Chellappa, </title> <editor> B. Girod, D.C. Munson, Jr., A. M. Telkap, and M. Vetterli, </editor> <booktitle> Signal Proc. Magazine, </booktitle> <pages> pp. 40-46, </pages> <month> March </month> <year> 1998. </year>
Reference-contexts: Modern video coding schemes often incorporate the La-grangian distortion viewpoint for accomplishing rate control, while using predictive quantization in a general sense through motion compensation and uniform quantizers with optimized lossless coding of transform coefficients for the intraframe coding (cf. [201], <ref> [202] </ref>). III. Quantization Basics: Encoding, Rate, Distortion, and Optimality This section presents, in a selfcontained manner, the basics of memoryless quantization, that is, vector quantizers which operate independently on successive vectors. For brevity, we omit the "memoryless" qualifier for most of the rest of this section.
Reference: [203] <author> H. </author> <title> Gish "Optimum quantization of random sequences," </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University, </institution> <month> March </month> <year> 1967. </year>
Reference-contexts: Conditions for use in code design resembling the Lloyd optimality conditions have been studied for feedback quantization [161], <ref> [203] </ref>, [41], but the conditions are not optimality conditions in the Lloyd sense, i.e., they are not necessary conditions for a quantizer within a feedback loop to yield the minimum average distortion subject to a rate constraint. We will return to this issue when we consider finite-state vector quantizers.
Reference: [204] <author> H. Gish and J. N. Pierce, </author> <title> "Asymptotically efficient quantizing," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 676-683, </pages> <month> Sep. </month> <year> 1968. </year>
Reference-contexts: The first, by Goblick and Holsinger [205], showed by numerical evaluation that uniform scalar quantization with variable-rate coding attains performance within about 1.5 dB (or 0.25 bit/sample) of the best possible for an i.i.d. Gaussian source. The second, by Gish and Pierce <ref> [204] </ref>, demonstrated analytically what the first paper had found empirically. <p> C. Performance of the Best k-Dimensional, Variable-Rate Quantizers Extensions of high resolution theory to variable-rate quantization can also be based on Bennett's integral, as well as approximations, originally due to Gish and Pierce <ref> [204] </ref>, to the entropy of the output of a quan-tizer. Two such approximations, which can be derived using approximations much like those used to derive Ben-nett's integral, were stated earlier for scalar quantizers in (11) and (13). <p> This can be directly verified using Jensen's inequality [193]. In the case of scalar quantization (k = 1), the optimality of the uniform point density and the operational distortion-rate function ffi 1;L (R) were found by Gish and Pierce (1968) <ref> [204] </ref>. <p> Thus, the loss due to k-dimensional quantization is only the space filling loss M k =M , which explains what Gish and Pierce found for scalar quantizers in 1968 <ref> [204] </ref>. We emphasize that there is no point density, oblongitis or memory loss, even for sources with memory. In effect, the entropy code has eliminated the need to shape the point density, and as a result, there is no need to compromise cell shapes. <p> The most common extension of distortion measures for scalars is the rth power distortion, d (x; y) = jx yj r . For example, Roe [443] generalized Max's formulation to distortion measures of this form. Gish and Pierce <ref> [204] </ref> considered a more general distortion measure of the form d (x; y) = L (x y), where L is a monotone increasing function of the magnitude of its argument and L (0) = 0 with the added property that M (v) j v v=2 has the property that vM 0 <p> Gish and Pierce (1968) <ref> [204] </ref>, who discovered that uniform is the asymptotically best type of scalar quantizer for variable-rate coding, presented both informal and rigorous derivations | the latter being the first to appear in these Transactions. <p> Both theories have been extended to continuous-time random processes. However, the high resolution results are somewhat sketchy [43], [330], <ref> [204] </ref>. Both can be applied to two or higher dimensional sources such as images or video. Both have been developed the most for Gaussian sources in the context of squared-error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.
Reference: [205] <author> T. J. Goblick and J. L. Holsinger, </author> <title> "Analog source digitization: A comparison of theory and practice," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 13, </volume> <pages> pp. 323-326, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Finally, in 1967 and 1968 two papers appeared in the IEEE literature (in fact in these Transactions) on variable-rate quantization, without reference to any of the aforementioned work. The first, by Goblick and Holsinger <ref> [205] </ref>, showed by numerical evaluation that uniform scalar quantization with variable-rate coding attains performance within about 1.5 dB (or 0.25 bit/sample) of the best possible for an i.i.d. Gaussian source. The second, by Gish and Pierce [204], demonstrated analytically what the first paper had found empirically. <p> The first to explicitly apply Shannon's source coding theory to the problem of analog-to-digital conversion combined with digital transmission appear to be Gob-lick and Holsinger <ref> [205] </ref> in 1967, and the first to make explicit comparisons of quantizer performance to Shannon's rate-distortion function was Koshelev [579].
Reference: [206] <author> M. Goldberg and H. Sun, </author> <title> "Image sequence coding using vector quantization," </title> <journal> IEEE Trans. Comm. </journal> <volume> vol. 34, </volume> <pages> pp. 703-710, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: These works all assumed the encoder and decoder to possess copies of the codebooks being used. Zeger, Bist, and Linder [566] considered systems where the codebooks are designed at the encoder, but must be also coded and transmitted to the decoder, as is commonly done in codebook replenishment <ref> [206] </ref>. A good review of the history of universal source coding through the early 1990s may be found in Kieffer (1993) [283].
Reference: [207] <author> A. J. Goldstein, </author> <title> "Quantization noise in P. </title> <type> C.M.," </type> <institution> Bell Telephone Laboratories Technical Memorandum, </institution> <month> 18 October </month> <year> 1957. </year>
Reference-contexts: Later in the same year in another Bell Telephone Laboratories Technical Memorandum, Goldstein <ref> [207] </ref> used variational methods to derive conditions for global optimality of a scalar quantizer in terms of second order partial derivatives with respect to the quantizer levels and thresholds. He also provided a simple counterintuitive example of a symmetric density for which the optimal quantizer was asymmetric.
Reference: [208] <author> R. C. Gonzales and R. C. Woods, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], <ref> [208] </ref>, [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [209] <author> W. M. Goodall, </author> <title> "Telephony by pulse code modulation," </title> <journal> Bell Syst. Tech. Journal, </journal> <volume> vol. 26, </volume> <pages> pp. 395-409, </pages> <month> July </month> <year> 1947. </year>
Reference: [210] <author> D. J. Goodman and T. J. </author> <title> Moulsley "Using simulated annealing to design transmission codes for analogue sources," </title> <journal> Electronics Letters, </journal> <volume> vol. 24, </volume> <pages> pp. 617-618, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: DeMarca and Jayant in 1987 [121] introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho [568], who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include <ref> [210] </ref>, [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], [233], [352].
Reference: [211] <author> V. K. Goyal and J. Kovacevic, </author> <title> "Optimal multiple description transform coding of Gaussian vectors," </title> <booktitle> Proc. Data Compression Conf., </booktitle> <editor> J. A. Storer and M. Cohn, </editor> <booktitle> Eds, </booktitle> <pages> pp. 388-397, </pages> <publisher> Computer Society Press, Los Alamitos, </publisher> <address> Calif., </address> <month> March-April </month> <year> 1998. </year>
Reference-contexts: High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger [324]. An alternative approach to multiple description quantization using transform coding has also been considered, e.g., in [38], <ref> [211] </ref>. I. Other Applications We have not treated many interesting variations and applications of quantization, several of which have been successfully analyzed or designed using the tools described here.
Reference: [212] <author> R. M. Gray, </author> <title> "Information rates of autoregressive processes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 16, </volume> <pages> pp. 516-523, </pages> <month> March </month> <year> 1971. </year>
Reference-contexts: Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], [239], [46], <ref> [212] </ref>, [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [213] <author> R. M. Gray, </author> <title> "A new class of lower bounds to information rates of stationary sources via conditional rate-distortion functions," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-19, </volume> <pages> pp. 480-489, </pages> <month> July </month> <year> 1973. </year>
Reference: [214] <author> R. M. Gray, </author> <title> Vector Quantization, </title> <journal> IEEE ASSP Magazine, </journal> <volume> vol. 1, </volume> <pages> pp. 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [195], <ref> [214] </ref>, [342], [372]. 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 F. The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [215] <author> R. M. Gray, </author> <title> "Oversampled Sigma-Delta Modulation," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 35, </volume> <pages> pp. 481-489, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Transform Code sities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier and Grieg to quantizers inside a feedback loop [260], [71], <ref> [215] </ref>, [216], [72].
Reference: [216] <author> R. M. Gray, </author> <title> "Quantization Noise Spectra," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 1220-1244, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Transform Code sities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier and Grieg to quantizers inside a feedback loop [260], [71], [215], <ref> [216] </ref>, [72]. <p> It follows from the work of Jayant and Ra-biner [266] and Sripad and Snyder [477] (see also <ref> [216] </ref>) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [217] <author> R. M. Gray, </author> <title> Source Coding Theory, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Using stationarity, it can be shown (cf. [562], [577], [221], Lemma 11.2.3 of <ref> [217] </ref>) that the operational distortion-rate function is subadditive in the sense that for any positive integers k and l ffi k+l (R) k + l l ffi l (R); (23) which shows the generally decreasing trend of the ffi k (R)'s as k increases. <p> Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. [46], [494], [560], <ref> [217] </ref>. For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. <p> Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], [239], [46], [212], [550], [559], <ref> [217] </ref>). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [218] <author> R. M. Gray, </author> <title> Entropy and Information Theory, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], <ref> [218] </ref>, [549], [127], [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> x k ); (y 1 ; : : : ; y k )) d kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : :; y k )); (43) and the subadditive ergodic theorem will still lead to positive and negative coding theorems [340], <ref> [218] </ref>. 10 An example of a subadditive distortion measure is the Levenshtein distance [314] which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. <ref> [218] </ref>, p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [221], [469], [126], [138], [479]. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. <p> Applicability: Distortion Measures Shannon rate distortion theory applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = i=1 (or a normalized version), though there are some results for subadditive distortion measures [340], <ref> [218] </ref> and some for distortion measures such as (x y) t B x (x y) [323].
Reference: [219] <author> R. M. Gray, </author> <title> "Combined compression and segmentation of images," </title> <booktitle> Proc. 1997 Int'l Workshop on Mobile Multimedia Com-mun. </booktitle> <address> (MoMuC97), Seoul, Korea, </address> <month> Sept - Oct , </month> <year> 1997. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], [358], [357], [518], [75], <ref> [219] </ref>, do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [88], [39], [402] and the transform subspace domain approach of [78]. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [86], [358]. One technique worth particular mention is called hierarchical table lookup VQ [86], [518], [75], <ref> [219] </ref>. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times.
Reference: [220] <author> R. M. Gray, A. Buzo, Y. Matsuyama, A. H. Gray, Jr., and J. D. Markel, </author> <title> "Source coding and speech compression," </title> <booktitle> Proc. Int'l Telemetering Conf., </booktitle> <volume> vol. XIV, </volume> <pages> pp. 871-878, </pages> <address> Los Angeles, CA, </address> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantiz GRAY AND NEUHOFF: QUANTIZATION 15 ers [87]. In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others <ref> [220] </ref>, [67], [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [221] <author> R. M. Gray and L. D. Davission, </author> <title> "Source coding theorems without the ergodic assumption," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 502-516, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: Moreover, the overall optimal performance for all quantizers of rate less than or equal to R is defined by ffi (R) = inf ffi k (R): (22) Similar definitions hold for the rate vs. distortion and the Lagrangian viewpoints. Using stationarity, it can be shown (cf. [562], [577], <ref> [221] </ref>, Lemma 11.2.3 of [217]) that the operational distortion-rate function is subadditive in the sense that for any positive integers k and l ffi k+l (R) k + l l ffi l (R); (23) which shows the generally decreasing trend of the ffi k (R)'s as k increases. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources <ref> [221] </ref>, [469], [126], [138], [479]. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (33). <p> The classic work on lossy universal source codes was Ziv's 1972 paper [577], which proved the existence of fixed-rate universal lossy codes under certain assumptions on the source statistics and the source and codebook alphabets. The multiple codebook idea was also used in 1974 <ref> [221] </ref> to extend the Shannon source coding theorem to nonergodic stationary sources by using the ergodic decomposition to interpret a nonergodic source as a universal coding problem for a family of ergodic sources. The idea is easily described and provides one means of constructing universal codes.
Reference: [222] <author> R. M. Gray and A. H. Gray, Jr., </author> <title> "Asymptotically optimal quan-tizers," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. IT-23, </volume> <pages> pp. 143-144, </pages> <month> Feb. </month> <year> 1977. </year>
Reference-contexts: The simple derivations combined the vector quantizer point density approximations with the use of Holder's and Jensen's inequalities, generalizing a scalar quantizer technique introduced in 1977 <ref> [222] </ref>. One step of the development rested on a still unproved conjecture regarding the asymptotically optimal quantizer cell shapes and Zador's constants, a conjecture which since has borne Gersho's name and which will be considered at some length in Section IV. <p> In this case, the optimum inertial profile is a constant and Bennett's integral can be minimized by variational techniques or Holder's inequality <ref> [222] </ref>, [193], resulting in the optimal point density fl f k+2 (x) f k+2 (x 0 ) dx 0 and the following approximation to the operational distortion-rate function: for large R ffi k (R) = M k fi k oe 2 2 2R j Z k (R); (30) where M k
Reference: [223] <author> R. M. Gray, A. H. Gray, Jr., and G. Rebolledo, </author> <title> "Optimal speech compression," </title> <booktitle> Proc. 13th Asilomar Conf. on Circuits Systems and Computers, </booktitle> <address> Pacific Grove, CA, </address> <year> 1979. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], [68], <ref> [223] </ref> with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding.
Reference: [224] <author> R. M. Gray and E. Karnin, </author> <title> "Multiple local optima in vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 708-721, </pages> <month> November </month> <year> 1981. </year>
Reference-contexts: a class of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], <ref> [224] </ref>, [387], [386], [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> If the distortion is squared error, the reproduction decoder is simply the conditional expectation of X given it was encoded into i: centroid (S i ) = E [XjX 2 S i ]: If the distortion measure is the input-weighted squared error of (21), then [318], <ref> [224] </ref> centroid (S i ) = E [W X jX 2 S i ] 1 E [W X XjX 2 S i ]: * For a fixed lossy encoder ff , regardless of the reproduction decoder fi, the optimal lossless encoder fl is the optimal lossless code for the discrete source <p> Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion [258], [259], [257], <ref> [224] </ref>, which has the form of an input-weighted quadratic distortion measure of the form of (21).
Reference: [225] <author> R. M. Gray and Y. Linde, </author> <title> "Vector quantizers and predictive quantizers for Gauss-Markov sources," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 30, </volume> <pages> pp. 381-389, </pages> <month> Feb. </month> <year> 1982. </year>
Reference-contexts: This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in [252] The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ [69], <ref> [225] </ref> is to first design the testvectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [226] <author> R. M. Gray, S. J. Park, and B. Andrews, </author> <title> "Tiling Shapes for Image Vector Quantization," </title> <booktitle> Proc. Third Int'l Conf. on Advances in Commun. and Control Systems (COMCON III), </booktitle> <address> Victoria, B.C., Canada, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: the k-dimensional vectors might come from parsing the image into rectangular m-by-n blocks of pixels, where mn = k, or into other tiling polytopes, such as hexagons and other shapes aimed at taking advantage of the eye's insensitivity to noise along diagonals in comparison with along horizontal and vertical lines <ref> [226] </ref>. Or the vectors might come from some less regular parsing. If the image has color, with each pixel value represented by some three-dimensional vector, then k-dimensional vectors can be extracted in even more ways.
Reference: [227] <author> R. M. Gray and T. G. Stockham, Jr. </author> <title> "Dithered quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Volume 39, </volume> <month> May </month> <year> 1993, </year> <pages> pp. 805-812. </pages>
Reference-contexts: The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright [542] in 1979 and Brinton [54] in 1984 and subsequently extended and refined with a variety of proofs [513], [512], [328], <ref> [227] </ref>.
Reference: [228] <author> R. M. Gray and A. D. Wyner, </author> <title> "Source coding over simple networks,"Bell Syst. </title> <journal> Tech. J., </journal> <volume> vol. 53, </volume> <pages> pp. 1681-1721, </pages> <month> Nov. </month> <year> 1974. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis <ref> [228] </ref>, [291], [292], [557], [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [229] <author> L. Guan and M. Kamel, </author> <title> "Equal-average hyperplane partitioning method for vector quantization of image data," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 13, </volume> <pages> pp. 605-609, </pages> <month> Oct. </month> <year> 1992. </year>
Reference: [230] <author> D. J. Hall and G. B. Ball, </author> <title> "ISODATA: A novel method of data analysis and pattern classification," </title> <type> Technical Report, </type> <institution> Stanford Research Institute, </institution> <address> Menlo Park, CA, </address> <year> 1965. </year>
Reference-contexts: This completed what he and Schutzenberger had begun. In the mid-1960's the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey [172], Ball and Hall [29], <ref> [230] </ref>, Jancey [263], and in 1969 by MacQueen [341] (the "k-means" algorithm).
Reference: [231] <author> P. J. Hahn and V. J. Mathews, </author> <title> "Distortion-limited vector quantization," </title> <booktitle> in Proc.Data Compression Conf. - DCC '96, </booktitle> <pages> pp. 340-348, </pages> <address> Snowbird, Utah, </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Quantizer design algorithms exist for this case, but to date no high resolution quantization theory or rate distortion theory has been developed for this distortion measure (cf. [347], <ref> [231] </ref>, [348]). High resolution theory usually considers a fixed dimension k, so neither additivity nor a family of distortion measures is required.
Reference: [232] <author> R. Hagen and P. Hedelin, </author> <title> "Robust vector quantisation by linear mappings of block-codes," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> San Antonio, p. 171, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], <ref> [232] </ref>, [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [233] <author> R. Hagen and P. Hedelin, </author> <title> "Design methods for VQ by linear mappings of block codes," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Trondheim, Norway, p. 241, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], <ref> [233] </ref>, [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [234] <author> H. Hang and B. </author> <title> Haskell, "Interpolative vector quantization of color images," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 36, </volume> <pages> pp. 465-470, </pages> <year> 1987. </year>
Reference-contexts: As an example of other possibilities, one could also extract the first k even samples, followed by the first k odd samples, the next k even samples, and so on. This subsampling could be useful for a multiresolution reconstruction, as in interpolative vector quantization <ref> [234] </ref>, [194]. For other types of data there may be no canonical extraction method. For example, in stereo speech the k-dimensional vectors might consist just of left samples, or just of right samples, or half from each, or k from the left followed by k from the right, etc.
Reference: [235] <author> H.-M. Hang and J. W. Woods, </author> <title> "Predictive vector quantization of images," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 33, </volume> <pages> pp. 1208-1219, </pages> <month> Nov. </month> <year> 1985. </year>
Reference-contexts: Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Figure 3 <ref> [235] </ref>, [116], [85], [417]. Alternatively, the encoder and decoder can share a finite set of states and a quantizer custom designed for each state.
Reference: [236] <author> A. Haoui and D. G. Messerschmitt, </author> <title> "Predictive vector quantization," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. </pages> <address> 10.10.1-10.10.4, San Diego, CA, </address> <month> March </month> <year> 1984. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], [136], <ref> [236] </ref>, [15], [16], [286], [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [237] <author> C. W. Harrison, </author> <title> "Experiments with linear prediction in television," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 31, </volume> <pages> pp. 764-783, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], [300], <ref> [237] </ref>, [511], [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived.
Reference: [238] <author> J. A. Hartigan, </author> <title> Clustering Algorithms, </title> <address> New York:Wiley, </address> <year> 1975. </year>
Reference-contexts: These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represent a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology, cf. Anderberg [9], Harti-gan <ref> [238] </ref>, or Diday and Simon [133]. MacQueen used an incremental incorporation of successive samples of a training set to design the codes, each vector being first mapped into a minimum distortion reproduction level representing a cluster, and then the level for that cluster being replaced by an adjusted centroid.
Reference: [239] <author> B. </author> <title> Haskell, "The computation and bounding of rate-distortion functions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 15, </volume> <pages> pp. 525-531, </pages> <month> September </month> <year> 1969. </year>
Reference-contexts: Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], <ref> [239] </ref>, [46], [212], [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [240] <author> A. Hayashi, </author> <title> "Differential pulse code modulation of the Wiener process," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 26, </volume> <pages> pp. 881-887, </pages> <month> June </month> <year> 1978. </year>
Reference-contexts: with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], <ref> [240] </ref>, and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 X X 2 - . . .
Reference: [241] <author> A. Hayashi, </author> <title> "Differential pulse code modulation of stationary Gaussian inputs," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 26, </volume> <pages> pp. 1137-1147, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], <ref> [241] </ref>, [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8
Reference: [242] <author> E. E. </author> <title> Hilbert, "Cluster compression algorithm: a joint clustering/data compression concept," Publication No. </title> <type> 77-43, </type> <institution> Jet Propulsion Lab, Pasadena, </institution> <address> CA, </address> <month> Dec. </month> <year> 1977. </year> <journal> 56 IEEE TRANSACTIONS ON INFORMATION THEORY, </journal> <volume> VOL. 44, NO. 6, </volume> <month> OCTOBER </month> <year> 1998 </year>
Reference-contexts: In 1974-1975 Chaffee [76] and Chaffee and Omura [77] used clustering ideas to design a vector quantizer for very low rate speech vocoding. In 1977 Hilbert used clustering algorithms for joint image compression and image classification <ref> [242] </ref>. These papers appear to be the first applications of direct vector quantization for speech and image coding applications. Also in 1977, Chen used an algorithm equivalent to a 2-dimensional Lloyd algorithm to design 2-dimensional vector quantiz GRAY AND NEUHOFF: QUANTIZATION 15 ers [87].
Reference: [243] <author> Y.-S. Ho and A. Gersho, </author> <title> "Variable-rate multi-stage vector quantization for image coding," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Processing (ICASSP), </booktitle> <pages> pp. 1156-1159, </pages> <year> 1988. </year>
Reference-contexts: In other words, multistage quantization can be used (and often is) with very different kinds of quantizers in its stages (different dimensions and much different structures, e.g. DPCM or wavelet coding). For example, structuring the stage quantizers leads to good performance and further substantial reductions in complexity, e.g. <ref> [243] </ref>, [79]. Of course, the multistage structuring leads to a suboptimal VQ for its given dimension. <p> And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed <ref> [243] </ref>, [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [244] <author> B. Hochwald and K. Zeger, </author> <title> "Tradeoff between source and channel coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 43, </volume> <pages> pp. 1412-1424, </pages> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: Another issue is the determination of the rate at which overall distortion decreases in an optimal system as the total number of channel uses per source symbol increases. These issues have been addressed in recent papers by Zeger and Manzella [570] and Hochwald and Zeger <ref> [244] </ref>, which use both exponential formulas produced by high resolution quantization theory and exponential bounds to channel coding error probability.
Reference: [245] <author> C. H. Hsieh, P. C. Lu, and J. C. Chang, </author> <title> "Fast codebook generation algorithm for vector quantization of images," </title> <journal> Pattern Recognition Lett., </journal> <volume> vol. 12, </volume> <pages> pp. 605-609, </pages> <year> 1991. </year>
Reference: [246] <author> C. H. Hsieh and J. C. Chang, </author> <title> "Lossless compression of VQ index with search-order coding," </title> <journal> IEEE Trans. Image Proc., </journal> <volume> vol. 5, </volume> <pages> pp. 1579-1582, </pages> <month> Nov. </month> <year> 1996. </year>
Reference: [247] <author> J. Huang, </author> <title> "Quantization of correlated random variables," </title> <type> Ph.D. dissertation, </type> <institution> School of Engineering, Yale University, </institution> <address> New Haven, CT 1962. </address>
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews [299] and analyzed and popularized in 1962-3 by Huang and Schultheiss <ref> [247] </ref>, [248]. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheiss proved that decorrelating does indeed lead to optimal transform code design, at least in the case of Gaussian sources and high resolution. <p> More recently transform coding has also been widely used in high fidelity audio coding [272], [200]. Unlike predictive quantizers, the transform coding approach lent itself quite well to the Bennett high resolution approximations, the classical analysis being that of Huang and Schultheiss <ref> [247] </ref>, [248] of the performance of optimized transform codes for fixed-rate scalar quantizers for Gaussian sources, a result which demonstrated that the Karhunen-Loeve decorrelating transform was optimum for this application for the given assumptions.
Reference: [248] <author> J.-Y. Huang and P. M. Schultheiss, </author> <title> "Block quantization of correlated Gaussian random variables," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 11, </volume> <pages> pp. 289-296, </pages> <month> Sep. </month> <year> 1963. </year>
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews [299] and analyzed and popularized in 1962-3 by Huang and Schultheiss [247], <ref> [248] </ref>. Kramer and Mathews simply assumed that the goal of the transform was to decorrelate the symbols, but Huang and Schultheiss proved that decorrelating does indeed lead to optimal transform code design, at least in the case of Gaussian sources and high resolution. <p> More recently transform coding has also been widely used in high fidelity audio coding [272], [200]. Unlike predictive quantizers, the transform coding approach lent itself quite well to the Bennett high resolution approximations, the classical analysis being that of Huang and Schultheiss [247], <ref> [248] </ref> of the performance of optimized transform codes for fixed-rate scalar quantizers for Gaussian sources, a result which demonstrated that the Karhunen-Loeve decorrelating transform was optimum for this application for the given assumptions.
Reference: [249] <author> S. H. Huang and S. H. Chen, </author> <title> "Fast encoding algorithm for VQ-based encoding," </title> <journal> Electron. Lett., </journal> <volume> vol. 26, </volume> <pages> pp. 1618-1619, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], [476], [363], [426], <ref> [249] </ref>, [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [250] <author> T. S. Huang, </author> <title> "Optimum binary code," </title> <journal> Quarterly Progress Rept. </journal> <volume> 82, </volume> <publisher> M.I.T. Res. Lab Electron., </publisher> <pages> pp. 223-225, </pages> <month> July 15, </month> <year> 1966. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], <ref> [250] </ref>, [501], [112], [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [251] <author> D. A. Huffman, </author> <title> "A method for the construction of minimum redundancy codes," </title> <journal> Proc. IRE, </journal> <volume> vol. 40, </volume> <pages> pp. 1098-1101, </pages> <month> Sept. </month> <year> 1952. </year>
Reference-contexts: Since we have weakened the constraint by expanding the allowed set of quantizers, this operational distortion-rate function will ordinarily be smaller than the fixed-rate optimum. Huffman's algorithm <ref> [251] </ref> provides a systematic method of designing binary codes with the smallest possible average length for a given set of probabilities, such as those of the cells. Codes designed in this way are typically called Huff-man codes.
Reference: [252] <author> D. Hui, D. F. Lyons and D. L. Neuhoff, </author> <title> "Reduced storage VQ via secondary quantization," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 7, </volume> <pages> pp. 477-495, </pages> <month> April </month> <year> 1998. </year>
Reference-contexts: For example, a recent study of VQ codebook storage has shown that in routine cases one needs to store codevector components with only about R+4 bits per component, where R is the rate of the quantizer <ref> [252] </ref>. Though this study did not assess the required arithmetic precision, one would guess that it need not be more than a little larger than that of the storage; e.g. R plus 5 or 6 bit arithmetic should suffice. <p> This reduces the arithmetic complexity and storage roughly in half to approximately kR operations per sample and 2 kR vectors. Further reductions in storage are possible, as described in <ref> [252] </ref> The usual (but not necessarily optimal) greedy method for designing a balanced TSVQ [69], [225] is to first design the testvectors stemming from the root node using the Lloyd algorithm on a training set.
Reference: [253] <author> D. Hui and D. L. Neuhoff, </author> <title> "Asymptotic analysis of optimum uniform scalar quantizers for generalized Gaussian distributions," </title> <booktitle> Proc. 1994 IEEE Int'l. Symp. Inform. Theory, </booktitle> <address> Trond-heim, Norway, p. 461, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Specifically, Hui and Neuhoff <ref> [253] </ref>, [254], [255] have found that for a Gaussian density with variance oe 2 lim N N ln N N!1 4 N 2 ln N This result was independently found by Eriksson and Agrell [149].
Reference: [254] <author> D. Hui and D. L. Neuhoff, </author> <title> "When is overload distortion negligible in uniform scalar quantization," </title> <booktitle> Proc. 1997 IEEE Int'l. Symp. Inform. Theory, </booktitle> <address> Ulm, Germany, p. 517, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Specifically, Hui and Neuhoff [253], <ref> [254] </ref>, [255] have found that for a Gaussian density with variance oe 2 lim N N ln N N!1 4 N 2 ln N This result was independently found by Eriksson and Agrell [149].
Reference: [255] <author> D. Hui and D. L. Neuhoff, </author> <title> "Asymptotic analysis of optimal fixed-rate uniform scalar quantization," </title> <note> Submitted to IEEE Trans. Inform. Theory. </note>
Reference-contexts: Specifically, Hui and Neuhoff [253], [254], <ref> [255] </ref> have found that for a Gaussian density with variance oe 2 lim N N ln N N!1 4 N 2 ln N This result was independently found by Eriksson and Agrell [149].
Reference: [256] <author> D. Hui and D. L. Neuhoff, </author> <title> "On the complexity of scalar quantization," </title> <booktitle> Proc. 1995 IEEE Int'l Symp. on Inform. Thy," </booktitle> <address> Whistler, B.C., p. 372, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: On the other hand, because high resolution theory can analyze the performance of families of quantizers with complexity reducing structure, one can learn much from it about how complexity relates to performance. In recent work, Hui and Neuhoff <ref> [256] </ref> have combined high resolution theory and Turing complexity theory to show that asymptotically optimal quantization can be implemented with complexity increasing at most polynomially with the rate.
Reference: [257] <author> F. Itakura, </author> <title> "Maximum prediction residual principle applied to speech recognition," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 23, </volume> <pages> pp. 67-72, </pages> <month> Feb. </month> <year> 1975. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], [259], <ref> [257] </ref>. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. <p> to a class of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], <ref> [257] </ref>, [224], [387], [386], [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion [258], [259], <ref> [257] </ref>, [224], which has the form of an input-weighted quadratic distortion measure of the form of (21).
Reference: [258] <author> F. Itakura and S. Saito, </author> <title> "Analysis synthesis telephony based on the maximum likelihood method," </title> <booktitle> Proc. 6th Int'l Congress of Acoustics, </booktitle> <pages> pp. </pages> <address> C-17-C-20, Tokyo, Japan, </address> <month> August </month> <year> 1968. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure <ref> [258] </ref>, [259], [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. <p> be generalized to a class of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. <ref> [258] </ref>, [259], [257], [224], [387], [386], [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion <ref> [258] </ref>, [259], [257], [224], which has the form of an input-weighted quadratic distortion measure of the form of (21).
Reference: [259] <author> F. Itakura and S. Saito, </author> <title> "A statistical method for estimation of speech spectral density and formant frequencies," </title> <journal> Electron. Commun. Japan, </journal> <volume> vol. 53-A, </volume> <pages> pp. 36-43, </pages> <year> 1970. </year>
Reference-contexts: In 1978 and 1979 a vector extension of Lloyd's Method I was applied to linear predictive coded (LPC) speech parameters by Buzo and others [220], [67], [68], [223] with a weighted quadratic distortion measure on parameter vectors closely related to the Itakura-Saito spectral distortion measure [258], <ref> [259] </ref>, [257]. Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. <p> generalized to a class of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], <ref> [259] </ref>, [257], [224], [387], [386], [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion [258], <ref> [259] </ref>, [257], [224], which has the form of an input-weighted quadratic distortion measure of the form of (21).
Reference: [260] <author> J. E. Iwersen, </author> <title> "Calculated quantizing noise of single-integration delta-modulation coders," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 48, </volume> <pages> pp. 2359-2389, </pages> <month> Sept. </month> <year> 1969. </year>
Reference-contexts: Transform Code sities, for constant and sinusoidal signals and finite sums of sinusoids using Rice's method, results which extend the work of Panter, Clavier and Grieg to quantizers inside a feedback loop <ref> [260] </ref>, [71], [215], [216], [72].
Reference: [261] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], <ref> [261] </ref>, [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [262] <author> E. Janardhanan, </author> <title> "Differentical PCM systems," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 27, </volume> <pages> pp. 82-93, </pages> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], <ref> [262] </ref>, [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE
Reference: [263] <author> R. C. Jancey, </author> <title> "Multidimensional group analysis," </title> <journal> Austrailian Journal of Botany, </journal> <volume> vol. 14, </volume> <pages> pp. 127-130, </pages> <year> 1966. </year>
Reference-contexts: In the mid-1960's the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey [172], Ball and Hall [29], [230], Jancey <ref> [263] </ref>, and in 1969 by MacQueen [341] (the "k-means" algorithm).
Reference: [264] <author> N. S. Jayant, </author> <title> "Digital coding of speech waveforms: PCM, DPCM and DM quantizers," </title> <journal> Proc. IEEE, </journal> <volume> vol. 62, </volume> <pages> pp. 611-632, </pages> <month> May </month> <year> 1974 </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], [374], [196], [424], [50], [458] and survey papers <ref> [264] </ref>, [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized.
Reference: [265] <author> N. S. Jayant and P. Noll, </author> <title> Digital Coding of Waveforms: Principles and Applications to Speech and Video, </title> <address> Prentice-Hall,Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books <ref> [265] </ref>, [374], [196], [424], [50], [458] and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain [350], [396], [482], [397], <ref> [265] </ref>. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions. <p> Indeed, it is still an open question whether this type of analysis, which typically uses Bennett and Panter-Dite formulas, is asymptotically correct. Nevertheless, the results of such high-resolution approximations are widely accepted and often compare well with experimental results <ref> [265] </ref>, [156]. <p> These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], <ref> [265] </ref>, [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [266] <author> N. S. Jayant and L. R. Rabiner, </author> <title> "The application of dither to the quantization of speech signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 51, </volume> <pages> pp. 1293-1304, </pages> <address> July-Aug. </address> <year> 1972. </year>
Reference-contexts: E. Dithering Dithered quantization was introduced by Roberts [442] in 1962 as a means of randomizing the effects of uniform quantization so as to minimize visual artifacts. It was further developed for images by Limb (1969) [317] and for speech by Jayant and Rabiner (1972) <ref> [266] </ref>. Intuitively, the goal was to cause the reconstruction error to look more like signal-independent additive white noise. It turns out that for one type of dithering, this intuition is true. <p> It follows from the work of Jayant and Ra-biner <ref> [266] </ref> and Sripad and Snyder [477] (see also [216]) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [267] <author> F. Jelinek, </author> <title> "Evaluation of rate distortion functions for low distortions," </title> <journal> Proc. IEEE (letters), </journal> <volume> vol. 55, </volume> <pages> pp. 2067-2068, </pages> <month> Novem-ber </month> <year> 1967. </year>
Reference-contexts: It is well known that D slb (R)=D (R) approaches one as R increases [327], <ref> [267] </ref>, [46], [322], which is entirely consistent with the fact that Z (R)=ffi (R) approaches one as R increases. The relationships among the various distortion-rate functions are summarized below. <p> Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], <ref> [267] </ref>, [239], [46], [212], [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [268] <author> F. Jelinek, </author> <title> "Tree encoding of memoryless time-discrete sources with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 15, </volume> <pages> pp. 584-590, </pages> <month> Sept. </month> <year> 1969. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others <ref> [268] </ref>, [269], [11], [132], [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [269] <author> F. Jelinek and J. B. Anderson, </author> <title> "Instrumentable tree encoding of information sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 17, </volume> <pages> pp. 118-119, </pages> <month> Jan. </month> <year> 1971. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], <ref> [269] </ref>, [11], [132], [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura [519].
Reference: [270] <author> D. G. Jeong and J. D. Gibson, </author> <title> "Uniform and piecewise uniform lattice vector quantization for memoryless Gaussian and Lapla-cian sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 786-804, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Moreover, since the best known lattice tesselations are so close to the best known tesselations, one may use lattice VQ as the second stage, which further reduces complexity. Good schemes of this sort have even been developed for low to moderate rates by Gibson <ref> [270] </ref>, [271] and Pan and Fischer [403], [404]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [302] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders.
Reference: [271] <author> D. G. Jeong and J. D. Gibson, </author> <title> "Image coding with uniform and piecewise-uniform vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.4, </volume> <pages> pp. 140-146, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Moreover, since the best known lattice tesselations are so close to the best known tesselations, one may use lattice VQ as the second stage, which further reduces complexity. Good schemes of this sort have even been developed for low to moderate rates by Gibson [270], <ref> [271] </ref> and Pan and Fischer [403], [404]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [302] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders.
Reference: [272] <author> J. D. Johnston, </author> <title> "Transform Coding of audio signals using perceptual noise criteria," </title> <journal> IEEE J. Selected Areas in Commun., </journal> <volume> vol. 6, </volume> <pages> pp. 314-323, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding <ref> [272] </ref>, [200].
Reference: [273] <author> R. L. Joshi and P. G. Poonacha, </author> <title> "A new MMSE encoding algorithm for vector quantization" Proc. </title> <booktitle> IEEE ICASSP, Toronto, </booktitle> <pages> pp. 645-648, </pages> <year> 1991. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], [476], [363], [426], [249], [399], <ref> [273] </ref>, A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [274] <author> B.-H. Juang and A. H. Gray, Jr., </author> <title> "Multiple stage vector quantization for speech coding," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 597-600, </pages> <address> Paris, </address> <month> April </month> <year> 1982. </year>
Reference-contexts: See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see [393], [439], [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. <ref> [274] </ref> as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [275] <author> D. Kazakos, </author> <title> "New results on robust quantization," </title> <journal> IEEE Trans. Comm, </journal> <pages> pp. 965-974, </pages> <month> Aug. </month> <year> 1983. </year>
Reference-contexts: This can be viewed as a variation on epsilon entropy since the goal is to minimize the maximum distortion. Further results along this line may be found in [37], <ref> [275] </ref>, [491]. Because these are minimax results aimed at scalar quantization, these results apply to any rate or dimension. 48 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 D.
Reference: [276] <author> S.-Z. Kiang, R. L. Baker, G. J. Sullivan, and C.-Y. Chiu, </author> <title> "Recursive optimal pruning with applications to tree structured vector quantizers," </title> <journal> IEEE Trans. Image Proc., </journal> <volume> vol. 1, </volume> <pages> pp. 162-169, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see [393], [439], <ref> [276] </ref>, [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [277] <author> J. C. Kieffer, </author> <title> "A generalization of the Pursley-Davisson-Mackenthun universal variable-rate coding theorem" IEEE Trans. </title> <journal> Inform. Theory, </journal> <volume> vol. 23, </volume> <pages> pp. 694-697, </pages> <month> Nov. </month> <year> 1977. </year>
Reference-contexts: Subsequently a variety of notions of fixed-rate universal codes were considered and compared [382], and fixed-distortion codes with variable-rate were developed by Mackenthun and Pursley [340] and Kieffer <ref> [277] </ref>, [279]. As with the early development of block source codes, universal quantization during its early days in the 1970s was viewed as more of a method for developing the theory than as a practical code design algorithm.
Reference: [278] <author> J. C. Kieffer, </author> <title> "Block coding for an ergodic source relative to a zero-one valued fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 24, </volume> <pages> pp. 422-437, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: As in the Shannon case, all these definitions can be made for k-dimensional vectors X k and the limiting behavior can be studied. Results regarding the convergence of such limits and the equality of the information-theoretic and operational notions of epsilon-entropy can be found, e.g., in [421], [420], <ref> [278] </ref>, [59]. Much of the theory is concerned with approximating epsilon entropy for small *. Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability.
Reference: [279] <author> J. C. Kieffer, </author> <title> "A unified approach to weak universal source coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 24, </volume> <pages> pp. 674-682, </pages> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: Subsequently a variety of notions of fixed-rate universal codes were considered and compared [382], and fixed-distortion codes with variable-rate were developed by Mackenthun and Pursley [340] and Kieffer [277], <ref> [279] </ref>. As with the early development of block source codes, universal quantization during its early days in the 1970s was viewed as more of a method for developing the theory than as a practical code design algorithm.
Reference: [280] <author> J. C. Kieffer, </author> <title> "Exponential rate of convergence for Lloyd's method I," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 205-210, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: theory for vector quantizers and rth power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations [281], Kieffer's study of the convergence rate of Lloyd's algorithm <ref> [280] </ref>, and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard [187].
Reference: [281] <author> J. C. Kieffer, </author> <title> "Stochastic stability for feedback quantization schemes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 248-254, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], <ref> [281] </ref>, [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference <p> Conway and Sloane [103], [104], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers and delta modulators along with adaptive generalizations <ref> [281] </ref>, Kieffer's study of the convergence rate of Lloyd's algorithm [280], and the demonstration by Garey, Johnson, and Witsenhausen that the Lloyd-Max optimization was NP-hard [187].
Reference: [282] <author> J. C. Kieffer, </author> <title> "History of source coding," </title> <journal> Inform. Theory Society Newsletter, </journal> <volume> vol. 43, </volume> <pages> pp. 1-5, </pages> <year> 1993. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], [127], [126], <ref> [282] </ref>, [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions.
Reference: [283] <author> J. C. Kieffer, </author> <title> "A survey of the theory of source coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 1473-1490, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], [127], [126], [282], <ref> [283] </ref>, [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> A good review of the history of universal source coding through the early 1990s may be found in Kieffer (1993) <ref> [283] </ref>.
Reference: [284] <author> J. C. Kieffer and J. G. Dunham, </author> <title> "On a type of stochastic stability for a class of encoding schemes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 29, </volume> <pages> pp. 703-797, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: Because it has not been rigorously shown that one may apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], <ref> [284] </ref>, i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations
Reference: [285] <author> J. C.Kieffer, T. M. Jahns and V. A. Obuljen, </author> <title> "New results on optimal entropy-constrained quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 34, </volume> <pages> pp. 1250-1258, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: In other words, quantizers with variable-rate should use an encoder that minimizes a sum of squared error and weighted bit rate, and not only the squared error. Another approach to entropy-constrained scalar quantization is described in <ref> [285] </ref>. This is a good place to again mention Gish and Pierce's result that if the rate is high, optimal entropy-constrained scalar or vector quantization can provide no more than roughly 1/4 bit improvement over uniform scalar quantization with block entropy coding.
Reference: [286] <author> T. Kim, </author> <title> "Side match and overlap match vector quantizers for images," </title> <journal> IEEE Trans Image Process vol. </journal> <volume> 1, </volume> <pages> pp. 170-185, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Finite-state vector quantizer theory has been developed for finite-state quantizers [161], [178], [179], a variety of design methods exist [174], [175], [136], [236], [15], [16], <ref> [286] </ref>, [196]. Lloyd's optimal decoder extends in a natural way to finite-state vector quantizers, the optimal reproduction decoder is a conditional expectation of the input vector given the binary codeword and the state.
Reference: [287] <author> P. Knagenhjelm and E. Agrell, </author> <title> "The Hadamard transform - a tool for index assignment," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 1139-1151, </pages> <month> Jul. </month> <year> 1996. </year>
Reference-contexts: DeMarca and Jayant in 1987 [121] introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho [568], who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include [210], [543], <ref> [287] </ref>. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], [233], [352].
Reference: [288] <author> A. N. </author> <title> Kolmogorov, "On the Shannon theory of information transmission in the case of continuous signals," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 2, </volume> <pages> pp. 102-108, </pages> <month> Sept. </month> <year> 1956. </year>
Reference-contexts: A distinct variation on the Shannon approach was introduced to the English literature in 1956 by Kol-mogorov <ref> [288] </ref>, who described several results by Russian information theorists inspired by Shannon's 1948 treatment of coding with respect to a fidelity criterion.
Reference: [289] <author> H. Kodama, K. Wakasugi, and M. Kasahara, </author> <title> "A construction of optimum vector quantizers by simulated annealing," </title> <journal> Trans. Inst. Electronics, Inform. and Commun. Engineers B-I, </journal> <volume> vol. J74B-I, </volume> <pages> pp. 58-65, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], <ref> [289] </ref>, deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [290] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory, third edition, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps <ref> [290] </ref>, [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [291] <author> V. Koshelev, </author> <title> "Hierarchical coding of discrete sources," Probl. </title> <journal> Pered. Informat., </journal> <volume> vol. 16, No. 3, </volume> <pages> pp. 31-49, </pages> <month> July-Sept. </month> <year> 1980. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], <ref> [291] </ref>, [292], [557], [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [292] <author> V. Koshelev, </author> <title> "Estimation of mean error for a discrete successive-approximation scheme," Probl. </title> <journal> Pered. Informat., </journal> <volume> vol. 17, No. 3, </volume> <pages> pp. 20-33, </pages> <month> July-Sept. </month> <year> 1981. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], [291], <ref> [292] </ref>, [557], [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [293] <author> T. Koski and S. Cambanis, </author> <title> "On the statistics of the error in predictive coding for stationary Ornstein-Uhlenbeck processes," </title> <journal> IEEE Trans. Inform. Theory, Vol.38, </journal> <volume> pp.1029-40, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], <ref> [293] </ref>, to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO.
Reference: [294] <author> T. Koski and L.-E. Persson, </author> <title> "On quantizer distortion and the upper bound for exponential entropy," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, pp.1168-1172, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: In some cases, simple closed form expressions can be found, e.g. for Gaussian, Laplacian, gamma densities. In other cases numerical integration can be used. Upper bounds to fi 1 are given in <ref> [294] </ref>. To the authors' knowledge, for sources with memory, simple expressions for the Zador factors have been found only for Gaussian sources; they depend on the covariance matrix.
Reference: [295] <author> F. Kossentini, W. C. Chung and M. J. T. Smith, </author> <title> "Subband image coding using entropy-constrained residual vector quanti GRAY AND NEUHOFF: QUANTIZATION 57 zation," </title> <journal> Inform. Processing and Management, </journal> <volume> vol. 30, No. 6, </volume> <pages> pp. 887-896, </pages> <year> 1994. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization <ref> [295] </ref>, and other methods [107]. A major breakthrough in performance and complexity came with the introduction of zerotrees [315], [466], [457], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [296] <author> F. Kossentini, W. C. Chung, and M. J. T. Smith, </author> <title> "Conditional entropy-constrained residual VQ with application to image coding," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> Vol.5, </volume> <pages> pp. 311-320, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], [441], <ref> [296] </ref>. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [297] <author> F. Kossentini, M. J. T. Smith and C. F. Barnes, </author> <title> "Image coding using entropy-constrained residual vector quantization" IEEE Trans. </title> <booktitle> Image Processing, </booktitle> <volume> vol. 4, </volume> <pages> pp. 1349-1357, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], <ref> [297] </ref>, [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [298] <author> F. Kossentini, M. J. T. Smith and C. F. Barnes, </author> <title> "Necessary conditions for the optimality of variable-rate residual vector quan-tizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1903-1914, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], <ref> [298] </ref>, [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [299] <author> H. P. Kramer and M. V. Mathews, </author> <title> "A linear coding for transmitting a set of correlated signals," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. 2, </volume> <pages> pp. 41-46, </pages> <month> Sept. </month> <year> 1956. </year>
Reference-contexts: The operation is depicted in Figure 4. This style of code was introduced in 1956 by Kramer and Mathews <ref> [299] </ref> and analyzed and popularized in 1962-3 by Huang and Schultheiss [247], [248].
Reference: [300] <author> E. R. Kretzmer, </author> <title> "Statistics of television signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 31, </volume> <pages> pp. 751-763, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], <ref> [300] </ref>, [237], [511], [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived. <p> The possibility of applying variable-length coding to quantization may well have occurred to any number of people who were familiar with both quantization and Shan-non's 1948 paper. The earliest references to such that we have found are in the 1952 papers by Oliver [395] and Kret-zmer <ref> [300] </ref>. In 1960, Max [349] had such in mind when he computed the entropy of nonuniform and uniform quan-tizers that had been designed to minimize distortion for a given number of levels. For a Gaussian source his results show that variable-length coding would yield rate reductions of about 0.5 bits/sample.
Reference: [301] <author> A. K. Krishnamurthy, S. C. Ahalt, D. E. Melton, and P. Chen, </author> <title> "Neural networks for vector quantization of speech and images," </title> <journal> IEEE J. Selected Areas Commun., </journal> <volume> vol. 8, </volume> <pages> pp. 1449-1457, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], <ref> [301] </ref>, [492], [337], [65].
Reference: [302] <author> F. Kuhlmann and J. A. Bucklew, </author> <title> "Piecewise uniform vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, Vol.34, </journal> <volume> No. 5, pt.2, </volume> <pages> pp. 1259-1263, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson [270], [271] and Pan and Fischer [403], [404]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew <ref> [302] </ref> as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [487]. Another scheme for adapting each stage to the previous is called codebook sharing, as introduced by Chan and Gersho [80], [82].
Reference: [303] <author> H. Kumazawa, M. Kasahara, and T. Namekawa, </author> <title> "A construction of vector quantizers for noisy channels," </title> <journal> Electronics and Engineering in Japan, </journal> <volume> vol. 67-B, </volume> <pages> pp. 39-47, </pages> <year> 1984. </year> <journal> Translated from Denshi Tsushin Gakkai Ronbunshi, </journal> <volume> vol. 67-B, </volume> <pages> pp. 1-8, </pages> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: A Shannon source coding theorem for trellis encoders using this distortion measure was proved in 1981 [135] and a Lloyd-style design algorithm for such encoders provided in 1987 [19]. A Lloyd algorithm for vector quantizers using the modified distortion measure was introduced in 1984 by Kumazawa, Kasa-hara, and Namekawa <ref> [303] </ref> and further studied in [157], [152], [153]. The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412].
Reference: [304] <author> A. J. Kurtenbach and P. </author> <title> A. </title> <journal> Wintz IEEE Trans. Comm. Tech-nol., </journal> <volume> 17, </volume> <pages> pp. 291-302, </pages> <month> Apr. </month> <year> 1969. </year>
Reference-contexts: This simple modification of the distortion measure allows the channel statistics to be included in an optimal quantizer design formulation. Recently the method has been referred to as "channel-optimized quantization," where the quantization might be scalar, vector, or trellis. This approach was introduced in 1969 by Kurtenbach and Wintz <ref> [304] </ref> for scalar quantizers. A Shannon source coding theorem for trellis encoders using this distortion measure was proved in 1981 [135] and a Lloyd-style design algorithm for such encoders provided in 1987 [19].
Reference: [305] <author> R. Laroia and N. Farvardin, </author> " <title> A structured fixed-rate vector quantizer derived from a variable-length scalar quantizer. I. Memoryless sources, II Vector sources" IEEE Trans. </title> <journal> Inform. Theory, </journal> <volume> Vol.39, </volume> <pages> pp. 851-876, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: While wavelet advocates may credit the decomposition itself for the gains in compression, the theory suggests that rather it is the fact that vector entropy coding for very large vectors is feasible. Scalar-vector Quantization Like permutation vector quantization and Fischer's pyramid vector quantizer, Laroia and Farvardin's <ref> [305] </ref> scalar-vector quantization attempts to match the performance of an optimal entropy constrained scalar quantizer with a low complexity fixed-rate structured vector quantizer. A derivative technique called block constrained quantization [24], [27], [23], [28] is simpler and easier to describe. <p> A high resolution analysis is given in [26], [23]. The scalar-vector method extends to sources with memory by combining it with transform coding using a decorrelating or approximately decorrelating transform <ref> [305] </ref>. Tree-Structured Quantization In its original and simplest form, a k-dimensional tree-structured vector quantizer (TSVQ) [69] is a fixed-rate quantizer with, say, rate R whose encoding is guided by a balanced (fixed-depth) binary tree of depth kR.
Reference: [306] <author> A. Lapidoth, </author> <title> "On the role of mismatch in rate distortion theory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 43, </volume> <pages> pp. 38-47, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Sakrison extended the extremal properties of the rate distortion functions to sources with memory [453], [454], [455] and Lapidoth <ref> [306] </ref> (1997) showed that a code designed for a Gaussian source would yield essentially the same performance when applied to another process with the same covariance structure. These results are essentially Shannon theory and hence should be viewed as primarily of interest for high-dimensional quantizers.
Reference: [307] <author> C.-H. Lee and L.-H. Chen, </author> <title> "Fast closest codeword search algorithm for vector quantisation," </title> <journal> IEE Proc.-Vis. Image Signal Proc.., </journal> <volume> vol. 141, </volume> <pages> pp. 143-148, </pages> <month> June </month> <year> 1994. </year>
Reference: [308] <author> C.-H. Lee and L.-H. Chen, </author> <title> "A fast search algorithm for vector quantization using mean pyramids of codewords," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 43, </volume> <pages> pp. 1697-1702, </pages> <address> Feb./Mar./Apr. </address> <year> 1995. </year>
Reference: [309] <author> D. H. Lee, </author> <title> "Asymptotic quantization error and cell-conditioned two-stage vector quantization," </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: However, two-stage VQ's designed in this way work fairly well. A high resolution analysis of two-stage VQ using Ben-nett's integral on the second stage can be found in [311], <ref> [309] </ref>. In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in [312], [379]. Multistage quantizers have been improved in a number of ways. <p> Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous. One such scheme, introduced by Lee and Neuhoff [310], <ref> [309] </ref>, was motivated by the observation that if the first stage quan-tizer has high rate, say R 1 , then by Gersho's conjecture, the first stage cells all have approximately the shape of T k , the tesselating polytope with least normalized moment of inertia, and the source density is approximately
Reference: [310] <author> D. H. Lee and D. L. Neuhoff, </author> <title> "Conditionally corrected two-stage vector quantization," </title> <journal> Conf. on Inform. Sciences and Systems, </journal> <volume> Princeton, </volume> <pages> pp. 802-806, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Variable-rate multistage quantizers have been developed [243], [297], [298], [441], [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous. One such scheme, introduced by Lee and Neuhoff <ref> [310] </ref>, [309], was motivated by the observation that if the first stage quan-tizer has high rate, say R 1 , then by Gersho's conjecture, the first stage cells all have approximately the shape of T k , the tesselating polytope with least normalized moment of inertia, and the source density is
Reference: [311] <author> D. H. Lee and D. L. Neuhoff, </author> <title> "An asymptotic analysis of two-stage vector quantization," </title> <booktitle> 1991 IEEE Int'l Symp. on Inform. Thy, </booktitle> <address> Budapest, p. 316, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: On the one hand, rate distortion theory analysis [228], [291], [292], [557], [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality. On the other hand, high resolution analyses of TSVQ [383] and two-stage VQ <ref> [311] </ref> have quantified the loss of these particular codes, and in the latter case shown ways of modifying the quantizer to eliminate the loss. Thus, both theories have something to say about successive refinement. V. <p> However, two-stage VQ's designed in this way work fairly well. A high resolution analysis of two-stage VQ using Ben-nett's integral on the second stage can be found in <ref> [311] </ref>, [309]. In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in [312], [379].
Reference: [312] <author> D. H. Lee and D. L. Neuhoff, </author> <title> "Asymptotic Distribution of the Errors in Scalar and Vector Quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 446-460, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Zador [562] found high resolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers. Lee and Neuhoff <ref> [312] </ref>, [379] found high resolution expressions for the density of the error produced by fairly general (deterministic) scalar and vector quantizers in terms of their point density and their shape profile, which is a function that conveys more cell shape information than the inertial profile. <p> In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in <ref> [312] </ref>, [379]. Multistage quantizers have been improved in a number of ways. More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal of) increased complexity.
Reference: [313] <author> D. H. Lee, D. L. Neuhoff and K. K. Paliwal, </author> <title> "Cell-conditioned two-stage vector quantization of speech," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP), </booktitle> <volume> Toronto vol. 4, </volume> <pages> pp. 653-656, </pages> <month> May </month> <year> 1991. </year>
Reference: [314] <author> V. I. Levenshtein, </author> <title> "Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals," Sov. Phys.-Dokl., </journal> <volume> vol. 10, </volume> <pages> pp. 707-710, </pages> <year> 1966. </year>
Reference-contexts: d kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : :; y k )); (43) and the subadditive ergodic theorem will still lead to positive and negative coding theorems [340], [218]. 10 An example of a subadditive distortion measure is the Levenshtein distance <ref> [314] </ref> which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. Originally developed 10 This differs slightly from the previous definition of subadditive because the d k are not assumed to be normalized.
Reference: [315] <author> A. S. Lewis and G. Knowles, </author> <title> "Image compression using the 2-D wavelet transform," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, No. 2, </volume> <pages> pp. 244-250, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees <ref> [315] </ref>, [466], [457], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [316] <author> J. Li, N. Chaddha, and R. M. Gray, </author> <title> "Asymptotic performance of vector quantizers with a perceptual distortion measure," </title> <booktitle> 1997 IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Ulm, Germany, </address> <month> June </month> <year> 1997. </year> <note> (Full paper submitted for possible publication. Preprint available at http://www-isl.stanford.edu/~gray/compression.html.) </note>
Reference-contexts: that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], [386], [150], [186], <ref> [316] </ref>, [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> was introduced by Gardner and Rao [186] to model a perceptual distortion measure for speech, where the matrix B (y) is referred to as the "sensitivity matrix." The requirement for the existence of the derivatives of third order and for the B (y) to be positive definite were added in <ref> [316] </ref> as necessary for the analysis. Examples of distortion measures meeting these conditions are the time-domain form of the Itakura-Saito distortion [258], [259], [257], [224], which has the form of an input-weighted quadratic distortion measure of the form of (21). <p> Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher [150] and Nill [386], [387]. The Bennett integral has been extended to this type of distortion, and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed [186], <ref> [316] </ref>. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently been extended to nondif-ference distortion measures such as (x y) t B x (x y) [186], <ref> [316] </ref>, [325]. In any event both theories are the most fully developed for the squared-error distortion measure, especially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure. For squared-error distortion, it is simply that the variance of the source be finite.
Reference: [317] <author> J. O. </author> <title> Limb, "Design of dithered waveforms for quantized visual signals," </title> <journal> Bell Syst. Tech. Journal, </journal> <volume> vol. 48, </volume> <pages> pp. 2555-2582, </pages> <month> September </month> <year> 1968. </year>
Reference-contexts: E. Dithering Dithered quantization was introduced by Roberts [442] in 1962 as a means of randomizing the effects of uniform quantization so as to minimize visual artifacts. It was further developed for images by Limb (1969) <ref> [317] </ref> and for speech by Jayant and Rabiner (1972) [266]. Intuitively, the goal was to cause the reconstruction error to look more like signal-independent additive white noise. It turns out that for one type of dithering, this intuition is true.
Reference: [318] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 28, </volume> <pages> pp. 84-95, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: In 1980 Linde, Buzo, and Gray explicitly extended Lloyd's algorithm to vector quantizer design <ref> [318] </ref>. <p> If the distortion is squared error, the reproduction decoder is simply the conditional expectation of X given it was encoded into i: centroid (S i ) = E [XjX 2 S i ]: If the distortion measure is the input-weighted squared error of (21), then <ref> [318] </ref>, [224] centroid (S i ) = E [W X jX 2 S i ] 1 E [W X XjX 2 S i ]: * For a fixed lossy encoder ff , regardless of the reproduction decoder fi, the optimal lossless encoder fl is the optimal lossless code for the discrete
Reference: [319] <author> Y. Linde and R. M. Gray, </author> <title> "A fake process approach to data compression," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 26, </volume> <pages> pp. 840-847, </pages> <month> June </month> <year> 1978. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder <ref> [319] </ref>, [531], [481], [18], [40].
Reference: [320] <author> T. Linder, </author> <title> "On asymptotically optimal companding quantization," </title> <journal> Problems of Control and Inform. Theory, </journal> <volume> vol. 20, No. 6, </volume> <pages> pp. 465-484, </pages> <year> 1991. </year>
Reference-contexts: It also contained a generalization to random vectors without probability densities, i.e. with distributions that are not absolutely continuous or even continuous. The paper also gave the first rigorous approach to the derivation of Bennett's integral for scalar quantization via companding. However, as pointed out by Linder (1991) <ref> [320] </ref>, there was "a gap in the proof concerning the convergence of Riemann sums with increasing support to a Riemann integral." Linder fixed this and presented a correct derivation with weaker assumptions.
Reference: [321] <author> T. Linder, T. Lugosi, and K. Zeger, </author> <title> "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1728-1740, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], [91], [101], <ref> [321] </ref>, [335], [131], [36]. <p> GRAY AND NEUHOFF: QUANTIZATION 35 Fig. 7. Signal-to-noise ratios for optimal VQs (dots) and predictions thereof based on the Zador-Gersho formula (straight lines). The convergence rate of ffi k (R) to ffi (R) as k tends to infinity has also been studied [413], [548], <ref> [321] </ref>, [576]. Roughly speaking these results show that for memoryless sources the convergence rate is between q k and log k k . <p> The clustering of codebooks was originally due to Chou [90] in 1991. High resolution quantization theory was used to study rates of convergence with blocklength to the optimal performance, yielding results consistent with earlier convergence results developed by other means, e.g., Linder et al. <ref> [321] </ref>. The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. [558]. A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu [329], [575], [574].
Reference: [322] <author> T. Linder and R. Zamir, </author> <title> "On the asymptotic tightness of the Shannon lower bound," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 2026-2031, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: It is well known that D slb (R)=D (R) approaches one as R increases [327], [267], [46], <ref> [322] </ref>, which is entirely consistent with the fact that Z (R)=ffi (R) approaches one as R increases. The relationships among the various distortion-rate functions are summarized below. Inequalities marked with a "*" become tight as dimension k increases, and those marked with a "+" become tight as R increases.
Reference: [323] <author> T. Linder and R. Zamir, </author> <title> "High-resolution source coding for non-difference distortion measures: The rate distortion function," </title> <booktitle> Proc. 1997 IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Ulm, Ger-many, p. 187, </address> <month> June </month> <year> 1997. </year> <note> Also submitted to IEEE Trans. Inform. Theory. </note>
Reference-contexts: have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], [386], [150], [186], [316], <ref> [323] </ref>, [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Note in particular that the optimal point density for the entropy-constrained case is not in general a uniform density. Parallel results for Shannon lower bounds to the rate-distortion function have been developed for this family of distortion measures by Linder and Zamir <ref> [323] </ref> and results for multidimensional companding with lattice codes for similar distortion measures have been developed by Lin-der, Zamir, and Zeger [325]. H. Rigorous Approaches to High Resolution Theory Over the years, high resolution analyses have been presented in several styles. <p> Shannon rate distortion theory applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = i=1 (or a normalized version), though there are some results for subadditive distortion measures [340], [218] and some for distortion measures such as (x y) t B x (x y) <ref> [323] </ref>. High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently been extended to nondif-ference distortion measures such as (x y) t B x (x y) [186], [316], [325].
Reference: [324] <author> T. Linder, R. Zamir, K. Zeger, </author> <title> "The multiple description rate region for high resolution source coding," </title> <booktitle> Proc. Data Compression Conf., </booktitle> <editor> J. A. Storer and M. Cohn, Eds., </editor> <publisher> Computer Society Press, Los Alamitos, </publisher> <month> March-April </month> <year> 1998. </year>
Reference-contexts: In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained [509] scalar quantizers for the multiple description problem. High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger <ref> [324] </ref>. An alternative approach to multiple description quantization using transform coding has also been considered, e.g., in [38], [211]. I. Other Applications We have not treated many interesting variations and applications of quantization, several of which have been successfully analyzed or designed using the tools described here.
Reference: [325] <author> T. Linder, R. Zamir, K. </author> <title> Zeger "High resolution source coding for non-difference distortion measures: multidimensional com-panding," </title> <note> submitted to IEEE Trans. Inform. Theory. </note>
Reference-contexts: proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], [386], [150], [186], [316], [323], <ref> [325] </ref>. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Parallel results for Shannon lower bounds to the rate-distortion function have been developed for this family of distortion measures by Linder and Zamir [323] and results for multidimensional companding with lattice codes for similar distortion measures have been developed by Lin-der, Zamir, and Zeger <ref> [325] </ref>. H. Rigorous Approaches to High Resolution Theory Over the years, high resolution analyses have been presented in several styles. <p> High resolution theory has the most results for rth power difference distortion measures, and as mentioned previously, some of its results have recently been extended to nondif-ference distortion measures such as (x y) t B x (x y) [186], [316], <ref> [325] </ref>. In any event both theories are the most fully developed for the squared-error distortion measure, especially for Gaussian sources. In addition, both theories require a finite moment condition, specific to the distortion measure. For squared-error distortion, it is simply that the variance of the source be finite.
Reference: [326] <author> T. Linder and K. Zeger, </author> <title> "Asymptotic Entropy-Constrained Performance of Tessellating and Universal Randomized Lattice Quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 575-579, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: On the other hand, it is like Zador's result (or Gersho's generalization of Bennett's integral [193]) in that, in essence, it is assumed that the quantizers have optimal cell shapes. In 1994 Linder and Zeger <ref> [326] </ref> rigorously derived the asymptotic distortion of quantizers generated by tessellations by showing that the quantizer q ff formed by tessellating with some basic cell shape S scaled by a positive number ff has average (narrow sense) rth-power distortion D ff satisfying lim D ff = 1: They then combined the
Reference: [327] <author> Y. N. Linkov, </author> <title> "Evaluation of epsilon entropy of random variables for small epsilon," </title> <journal> Prob. Inform. Transmission, </journal> <volume> vol. 1, </volume> <pages> pp. 12-18, </pages> <year> 1965. </year> <journal> (Translated from Problemy Peredachi Infor-matsii, </journal> <volume> vol. 1, </volume> <pages> 18-26.) </pages>
Reference-contexts: It is well known that D slb (R)=D (R) approaches one as R increases <ref> [327] </ref>, [267], [46], [322], which is entirely consistent with the fact that Z (R)=ffi (R) approaches one as R increases. The relationships among the various distortion-rate functions are summarized below. <p> Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], <ref> [327] </ref>, [267], [239], [46], [212], [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [328] <author> S. P. Lipshitz, R. A. Wannamaker, and J. Vanderkooy, </author> <title> "Quantization and dither: a theoretical survey," </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> vol.40, </volume> <pages> pp. 355-75, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright [542] in 1979 and Brinton [54] in 1984 and subsequently extended and refined with a variety of proofs [513], [512], <ref> [328] </ref>, [227].
Reference: [329] <author> Q. Liu, E. Yang, and Z. Zhang, </author> <title> "A fixed-slope universal sequential algorithm for lossy source coding based on Gold-Washing mechanism," </title> <booktitle> Proc. 33rd Annual Allerton Conf. on Commun., Control, and Computing, </booktitle> <address> Monticello, IL, </address> <pages> pp. 466-474, </pages> <address> Urbana-Champaign, IL., </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. [558]. A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu <ref> [329] </ref>, [575], [574]. Their approach, dubbed "gold washing" did not involve training, but rather created and removed codevectors according to the data received and an auxiliary random process in a way that could be tracked by a decoder without side information. E.
Reference: [330] <author> S. P. Lloyd. </author> <title> Least squares quantization in PCM. Unpublished Bell Laboratories Technical Note. Portions presented at the Institute of Mathematical Statistics Meeting Atlantic City New Jersey September 1957. Published in special issue on quantization, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 129-137, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: In 1957 Smith [474] reexamined companding and PCM. Among other things, he gave somewhat cleaner derivations of Bennett's integral, the optimal compressor function, and the Panter-Dite formula. Also in 1957, Lloyd <ref> [330] </ref> made an important study of quantization with three main contributions. First, he found necessary and sufficient conditions for a fixed-rate quan-tizer to be locally optimal; i.e. conditions that if satisfied implied that small perturbations to the levels or thresholds would increase distortion. <p> One such iteration of T must decrease or leave unchanged the average Lagrangian distortion. Iterate until convergence or the improvement falls beneath some threshold. This algorithm is an extension and variation on the algorithm for optimal scalar quantizer design introduced for fixed-rate scalar quantization by Lloyd <ref> [330] </ref>. The algorithm is a fixed-point algorithm since if it converges to a code, the code must be a fixed point with respect to T . This generalized Lloyd algorithm applies to any distribution, including parametric models and empirical distributions formed from training sets of real data. <p> Though the notion of point density would no doubt have been recognizable to the earliest contributors such as Bennett, Panter and Dite, as mentioned earlier, it was not explicitly introduced until Lloyd's work <ref> [330] </ref>. In nonuniform scalar quantization and vector quantization, there is the additional issue of codevector placement 22 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 within cells and, in the latter case, of cell shape. <p> quantizers (k = 1) with points in the middle of the cells, m (x) = 1 12 and the above reduces to D (q) = 12 N 2 1 f (x) dx (28) which is what Bennett [43] found for companders, as restated in terms of point densities by Lloyd <ref> [330] </ref>. Both (28) and the more general formula (27) are called Bennett's integral. <p> The earliest quantizer distortion analyses to appear in the open literature [43], [405], [474] assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. For example, Lloyd <ref> [330] </ref> used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when multiplied by N 2 . Roe [443], Algazi [8] and Wood [539] also used Taylor series. <p> The paper by Renyi [433] gave, in effect, a rigorous derivation of (11) for a uniform quantizer with infinitely many levels. Specifically, it showed that H (q n (X)) = h (X) + log n + o (1), provided that the source distribution is absolutely continuous 11 Though Lloyd <ref> [330] </ref> gave a fairly rigorous analysis of distortion, we do not include his paper in this category because it ignored overload distortion. and that H (q n (X)) and h (X) are finite, where q n denotes a uniform quantizer with step size 1 n and o (1) denotes a quantity <p> Both theories have been extended to continuous-time random processes. However, the high resolution results are somewhat sketchy [43], <ref> [330] </ref>, [204]. Both can be applied to two or higher dimensional sources such as images or video. Both have been developed the most for Gaussian sources in the context of squared-error distortion, which is not surprising in view of the tractability of squared error and Gaussianity.
Reference: [331] <author> S. P. Lloyd, </author> <title> "Rate versus fidelity for the binary source," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 56, </volume> <pages> pp. 427-437, </pages> <month> Mar. </month> <year> 1977. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization <ref> [331] </ref>, [414], [148], [534], [178], [381], [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [332] <author> K. T. Lo and W. K. Cham, </author> <title> "Subcodebook searching algorithm for efficient VQ encoding of images" IEE Proc.-Vis. Image Signal Process., </title> <journal> vol. </journal> <volume> 140, </volume> <pages> pp. 327-330, </pages> <month> Oct. </month> <year> 1993. </year>
Reference: [333] <author> T. D. Lookabaugh and R. M. Gray, </author> <title> "High-resolution quantization theory and the vector quantizer advantage," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 35, </volume> <pages> pp. 1020-1033, </pages> <month> Sep. </month> <year> 1989. </year>
Reference-contexts: On the other hand, it is also important to understand what specific characteristics of vector quantizers improve with dimension and by how much. Motivated by several prior explanations [342], <ref> [333] </ref>, [365], we offer the following. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. To simplify the discussion, assume k 0 is a multiple of k. <p> 2 sq (x i ) k 0 fi Q k 0 2 sq (x i ) R 1 fl 2 k 0 (x) = L sp fi L ob fi L pt ; (37) where the cell shape loss has been factored into the product of a space filling loss <ref> [333] </ref> 6 , L sp , which is the ratio of the normalized moment of inertia of a cube to that of a high dimensional sphere, and an oblongitis loss, L ob , which is the factor by which the rectangularity of the cells makes the cell shape loss larger than <p> Gaussian source, the optimal choice of scalar quantizer causes the product quantizer to have 0.94 dB oblongitis loss and 1.88 dB point density loss. The sum of these, 2.81 dB, which equals 10 log 10 fi 1 =fi, has been called the "shape loss" <ref> [333] </ref> because it is determined by the shape of the density | the more uniform the density the less need for compromise because the scalar point densities leading to best product cell shapes and best point density are more similar. <p> Specifically, when there is dependence/correlation between source samples, the product point density cannot match the ideal point density, not even approximately. See <ref> [333] </ref>, [365] for a defi nition of memory loss. (One can factor both the point density and oblongitis losses into two terms, one of which is 8 The fact that product quantizers can have the optimal point density is often overlooked. 9 This implies that distortion will not decrease as 2
Reference: [334] <author> A. Lowry, S. Hossain and W. Millar, </author> <title> "Binary search trees for vector quantization," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Dallas, </booktitle> <pages> pp. 2206-2208, </pages> <year> 1987. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], <ref> [334] </ref>, [146], [532], [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [335] <author> G. Lugosi and A. Nobel, </author> <title> "Consistency of data-driven histogram methods for density estimation and classification," </title> <journal> Annals of Statistics, </journal> <volume> vol. 24, </volume> <pages> pp. 687-706, </pages> <year> 1996. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], [418], [108], [91], [101], [321], <ref> [335] </ref>, [131], [36].
Reference: [336] <author> J. Lukaszewicz and H. Steinhaus, </author> <title> "On measuring by comparison," </title> <journal> Zastos. Mat., </journal> <volume> vol. 2, </volume> <pages> pp. 225-231, </pages> <year> 1955. </year> <note> (In Polish.) </note>
Reference-contexts: Specifically, in 1950-1951 Dalenius et al. [118], [119] used variational techniques to consider optimal grouping of Gaussian data with respect to average squared error. Lukaszewicz and H. Steinhaus <ref> [336] </ref> (1955) developed what we now consider to be the Lloyd optimality conditions using variational techniques in a study of optimum go/no-go gauge sets (as acknowledged by Lloyd). Cox in 1957 [111] also derived similar conditions.
Reference: [337] <author> S. P. Luttrell, </author> <title> "Self-supervised training of hierarchical vector quantizers," </title> <booktitle> II Int'l. Conf. Artificial Neural Networks, Conf. Publ. </booktitle> <volume> No. 349, London:IEE, </volume> <pages> pp. 5-9, </pages> <year> 1991. </year>
Reference-contexts: algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], <ref> [337] </ref>, [65].
Reference: [338] <author> D. F. Lyons, </author> <title> "Fundamental limits of low-rate transform codes," </title> <type> Ph.D. Dissertation, </type> <institution> Univ. of Michigan, </institution> <year> 1992. </year>
Reference-contexts: We have already mentioned the traditional high resolution fixed-rate analysis and the more recent high resolution entropy constrained analysis for separate lossless coding of each quantized transform coefficient. An asymptotic low resolution analysis <ref> [338] </ref>, [339] has also been performed. In almost all actual implementations, however, scalar quantizers are combined with a block lossless code, where the lossless code is allowed to effectively operate on an entire block of quantized coefficients at once, usually by combining run-length coding with Huffman or arithmetic coding.
Reference: [339] <author> D. F. Lyons and D. L. Neuhoff, </author> <title> "A coding theorem for low-rate transform codes," </title> <booktitle> Proc. IEEE Int'l Symp. on Inform. Thy, </booktitle> <address> San Antonio, TX, p. 333, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: We have already mentioned the traditional high resolution fixed-rate analysis and the more recent high resolution entropy constrained analysis for separate lossless coding of each quantized transform coefficient. An asymptotic low resolution analysis [338], <ref> [339] </ref> has also been performed. In almost all actual implementations, however, scalar quantizers are combined with a block lossless code, where the lossless code is allowed to effectively operate on an entire block of quantized coefficients at once, usually by combining run-length coding with Huffman or arithmetic coding.
Reference: [340] <author> K. M. Mackenthun and M. B. Pursley, </author> <title> "Strongly and weakly universal source coding," </title> <booktitle> Proc. 1977 Conf. on Inform. Science and Systems, </booktitle> <publisher> Johns Hopkins University, </publisher> <pages> pp. 286-291, </pages> <year> 1977. </year>
Reference-contexts: ; x k ); (y 1 ; : : : ; y k )) d kl ((x l+1 ; : : : ; x k ); (y l+1 ; : : :; y k )); (43) and the subadditive ergodic theorem will still lead to positive and negative coding theorems <ref> [340] </ref>, [218]. 10 An example of a subadditive distortion measure is the Levenshtein distance [314] which counts the number of insertions and deletions along with the number of changes that it takes to convert one sequence into another. <p> Applicability: Distortion Measures Shannon rate distortion theory applies primarily to additive distortion measures; i.e. distortion measures of the form d (x; y) = i=1 (or a normalized version), though there are some results for subadditive distortion measures <ref> [340] </ref>, [218] and some for distortion measures such as (x y) t B x (x y) [323]. <p> Subsequently a variety of notions of fixed-rate universal codes were considered and compared [382], and fixed-distortion codes with variable-rate were developed by Mackenthun and Pursley <ref> [340] </ref> and Kieffer [277], [279]. As with the early development of block source codes, universal quantization during its early days in the 1970s was viewed as more of a method for developing the theory than as a practical code design algorithm.
Reference: [341] <author> J. MacQueen, </author> <title> "Some methods for classification and analysis of multivariate observations," </title> <booktitle> Proc. of the Fifth Berkeley Symp. on Math. Stat. and Prob., </booktitle> <volume> vol. 1, </volume> <pages> pp. 281-296, </pages> <year> 1967. </year>
Reference-contexts: In the mid-1960's the optimality properties described by Steinhaus, Lloyd, and Zador and the design algorithm of Steinhaus and Lloyd were rediscovered in the statistical clustering literature. Similar algorithms were introduced in 1965 by Forgey [172], Ball and Hall [29], [230], Jancey [263], and in 1969 by MacQueen <ref> [341] </ref> (the "k-means" algorithm). These algorithms were developed for statistical clustering applications, the selection of a finite collection of templates that well represent a large collection of data in the MSE sense, i.e., a fixed-rate VQ with an MSE distortion measure in quantization terminology, cf.
Reference: [342] <author> J. Makhoul, S. Roucos, and H. Gish, </author> <title> "Vector quantization in speech coding," </title> <journal> Proc. IEEE, </journal> <volume> vol. 73, </volume> <pages> pp. 1551-1588, </pages> <month> Nov. </month> <year> 1985. </year> <journal> 58 IEEE TRANSACTIONS ON INFORMATION THEORY, </journal> <volume> VOL. 44, NO. 6, </volume> <month> OCTOBER </month> <year> 1998 </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [195], [214], <ref> [342] </ref>, [372]. 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 F. The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. <p> On the other hand, it is also important to understand what specific characteristics of vector quantizers improve with dimension and by how much. Motivated by several prior explanations <ref> [342] </ref>, [333], [365], we offer the following. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. To simplify the discussion, assume k 0 is a multiple of k. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQs can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) <ref> [342] </ref>, [94], [439], [196] and with larger branching factors than two or even variable branching factors [460]. <p> One can grow a balanced tree by splitting all nodes in each level of the tree, or by splitting one node at a time, e.g., by splitting the node with the largest contribution to the distortion <ref> [342] </ref> or in a greedy fashion to maximize the decrease in distortion for the increase in rate [439]. Once grown, the tree can be pruned by removing all descendants of any internal node, thereby making it a leaf. This will increase average distortion, but will also decrease the rate.
Reference: [343] <author> M. W. Marcellin, </author> <title> "On entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 42, </volume> <pages> pp. 14-16, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], [166], [167], [522], <ref> [343] </ref>, [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [344] <author> M. W. Marcellin and T. R. Fischer, </author> <title> "Trellis coded quantization of memoryless and Gauss-Markov sources," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 38, </volume> <pages> pp. 82-93, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], <ref> [344] </ref>, [166], [167], [522], [343], [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [345] <author> M. W. Marcellin, T. R. Fischer, and J. D. Gibson, </author> <title> "Predictive trellis coded quantization of speech," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing vol. </journal> <volume> 38, </volume> <pages> pp. 46-55, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels <ref> [345] </ref>, [344], [166], [167], [522], [343], [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [346] <author> E. Masry and S. Cambanis, </author> <title> "Delta modulation of the Wiener Process," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 23, </volume> <pages> pp. </pages> <month> 1297-1300 Nov. </month> <year> 1975. </year>
Reference-contexts: integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], <ref> [346] </ref>, [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den <p> difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], <ref> [346] </ref>, [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 X X 2 - . . .
Reference: [347] <author> V. J. Mathews, </author> <title> "Vector quantization of images using the L 1 distortion measure," </title> <booktitle> Proc. Int'l Conf. Image Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 109-112, </pages> <address> Washington, DC, </address> <month> Oct. </month> <year> 1995 </year>
Reference-contexts: Quantizer design algorithms exist for this case, but to date no high resolution quantization theory or rate distortion theory has been developed for this distortion measure (cf. <ref> [347] </ref>, [231], [348]). High resolution theory usually considers a fixed dimension k, so neither additivity nor a family of distortion measures is required.
Reference: [348] <author> V. J. Mathews, </author> <title> "Vector quantization using the L 1 distortion measure," </title> <journal> IEEE Signal Processing Letters, </journal> <volume> vol. 4, </volume> <pages> pp. 33-35, </pages> <year> 1997. </year>
Reference-contexts: Quantizer design algorithms exist for this case, but to date no high resolution quantization theory or rate distortion theory has been developed for this distortion measure (cf. [347], [231], <ref> [348] </ref>). High resolution theory usually considers a fixed dimension k, so neither additivity nor a family of distortion measures is required.
Reference: [349] <author> J. Max, </author> <title> "Quantizing for minimum distortion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <pages> pp. 7-12, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: Lloyd provided design examples for uniform, Gaussian and Laplacian random variables and showed that the results were consistent with the high resolution approximations. Although Method II would initially gain more popularity when rediscovered in 1960 by Max <ref> [349] </ref>, it is Method I that easily extends to vector quantizers and many types of quantizers with structural constraints. <p> In 1959 Shtein [471] added terms representing overload distortion to the 2 =12 formula and to Bennett's integral and used them to optimize uniform and nonuniform quantizers. Unaware of prior work except for Bennett's, he rederived the optimal compressor characteristic and the Panter-Dite formula. In 1960 Max <ref> [349] </ref> published a variational proof of the Lloyd optimality properties for rth power distortion measures, rediscovered Lloyd's Method II, and numerically investigated the design of fixed-rate quantizers for a variety of input densities. <p> The earliest references to such that we have found are in the 1952 papers by Oliver [395] and Kret-zmer [300]. In 1960, Max <ref> [349] </ref> had such in mind when he computed the entropy of nonuniform and uniform quan-tizers that had been designed to minimize distortion for a given number of levels. For a Gaussian source his results show that variable-length coding would yield rate reductions of about 0.5 bits/sample.
Reference: [350] <author> R. A. McDonald, </author> <title> "Signal-to-noise and idle channel performance of DPCM systems with particular application to voice signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 45, </volume> <pages> pp. 1123-1151, </pages> <month> Sept. </month> <year> 1966. </year>
Reference-contexts: error is usually sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain <ref> [350] </ref>, [396], [482], [397], [265]. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [351] <author> S. W. McLaughlin, D. L. Neuhoff, and J. K. Ashley, </author> <title> "Optimal binary index assignments for a class of equiprobable scalar and vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 2031-2037, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], <ref> [351] </ref>, [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment. <p> For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment. However, this result remained relatively unknown until rederived and generalized in <ref> [351] </ref>. When source and channel codes are considered together, a key issue is the determination of the quantization rate to be used when the total of number of channel symbols per source symbol is held fixed.
Reference: [352] <author> A. Mehes and K. Zeger, </author> <title> "Binary lattice vector quantization with linear block codes and affine index assignments," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 44, </volume> <pages> pp. 79-94, </pages> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], [233], <ref> [352] </ref>. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment. However, this result remained relatively unknown until rederived and generalized in [351].
Reference: [353] <author> J. Menez, F. Boeri, and D. J. Esteban, </author> <title> "Optimum quantizer algorithm for real-time block quantizing," </title> <booktitle> Proc. of the 1979 IEEE Intl Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 980-984, </pages> <year> 1979. </year>
Reference-contexts: Also in 1978, Adoul, Collin, and Dalle [3] used clustering ideas to design two-dimensional vector quantizers for speech coding. Caprio, Westin, and Esposito in 1978 [74] and Menez, Boeri, and Esteban in 1979 <ref> [353] </ref> also considered clustering algorithms for the design of vector quantizers with squared-error and magnitude-error distortion measures. The most important paper on quantization during the 1970's was without a doubt Gersho's paper on "Asymptotically optimal block quantization" [193].
Reference: [354] <author> D. Miller and K. Rose, </author> <title> "Combined source-channel vector quantization using deterministic annealing," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 42, </volume> <pages> pp. 347-356, </pages> <address> Feb.-Apr. </address> <year> 1994. </year>
Reference-contexts: It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523]. Simulated annealing has also been used to design such quantizers [140], [152], <ref> [354] </ref>.
Reference: [355] <author> N. Moayeri, </author> <title> "Some issues related to fixed-rate pruned tree-structured vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 1523-1531, </pages> <year> 1995 </year>
Reference-contexts: It can be shown that, for quite general measures of distortion, pruning can be done in an optimal fashion and the optimal subtrees of decreasing rate are nested [94]. See also <ref> [355] </ref>. It seems likely that in the moderate to high rate case, pruning removes leaves corresponding to cells that are oblong such as cubes cut in half, leaving mainly cubic cells. <p> See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see [393], [439], [276], [22], <ref> [355] </ref> Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [356] <author> N. Moayeri and D. L. Neuhoff, </author> <title> "Theory of lattice-based fine-coarse vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 1072-1084, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], [358], [357], [518], [75], [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in <ref> [356] </ref> for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [88], [39], [402] and the transform subspace domain approach of [78]. Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be.
Reference: [357] <author> N. Moayeri and D. L. Neuhoff, </author> <title> "Time-memory tradeoffs in vector quantizer codebook searching based on Decision Trees," </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> vol. 2, </volume> <pages> pp. 490-506, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], [358], <ref> [357] </ref>, [518], [75], [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer.
Reference: [358] <author> N. Moayeri, D. L. Neuhoff, and W. E. Stark, </author> <title> "Fine-coarse vector quantization," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 39, </volume> <pages> pp. 1503-1515, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], <ref> [358] </ref>, [357], [518], [75], [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. <p> Thus the question becomes: what is the best partition into N cells, each of which is the union of some number of fine cells. The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [86], <ref> [358] </ref>. One technique worth particular mention is called hierarchical table lookup VQ [86], [518], [75], [219]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on.
Reference: [359] <author> P. W. Moo and D. L. Neuhoff, </author> <title> "An asymptotic analysis of fixed-rate lattice vector quantization," </title> <booktitle> Proc. Intern'l Symp. Inform. Thy. and Its Applications, </booktitle> <address> Victoria, B.C., </address> <pages> pp. 409-412, </pages> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: There are even densities with tails so heavy that o = 2 and the granular distortion becomes negligible in comparison to the overload distortion. In a related result, the asymptotic form of the optimal scaling factor for lattice quantizers has also been found recently for an i.i.d. Gaussian source <ref> [359] </ref>, [149]. We conclude this subsection by mentioning some gaps in rigorous high resolution theory. One, of course, is a proof or counterproof of Gersho's conjecture in dimensions three and higher. Another is the open question of whether the best tessellation in three or more dimensions is a lattice. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the code-book and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. [580], [151], <ref> [359] </ref>, [149], [409]. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [360] <author> P. W. Moo and D. L. Neuhoff, </author> <title> "Uniform polar quantization revisited," </title> <note> to appear in Proc. IEEE Int'l Symp. Inform. </note> <editor> Thy., </editor> <address> Boston, </address> <month> Aug. </month> <year> 1998. </year>
Reference-contexts: Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], [489], [490], [483], [485], [488], <ref> [360] </ref>. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], [483], [485], [488], <ref> [360] </ref>. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [361] <author> J. M. Morris and V. D. Vandelinde, </author> <title> "Robust quantization of discrete-time signals with independent samples," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 22, </volume> <pages> pp. 1897-1901, </pages> <year> 1974. </year>
Reference-contexts: Here a quantizer is considered to be robust if it bounds the maximum distortion for a class of sources. Morris and Vandelinde (1974) <ref> [361] </ref> developed the theory of robust quantization and provide conditions under which the uniform quantizer is optimum in this minimax sense. This can be viewed as a variation on epsilon entropy since the goal is to minimize the maximum distortion.
Reference: [362] <author> K. Motoishi and T. Misumi, </author> <title> "On a fast vector quantization algorithm," </title> <booktitle> Proc. VIIth Symp. on Inform. Theory and Its Applications, </booktitle> <year> 1984. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in <ref> [362] </ref>, [517], [475], [476], [363], [426], [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [363] <author> K. Motoishi and T. Misumi, </author> <title> "Fast vector quantization algorithm by using an adaptive searching technique," </title> <booktitle> Abstracts of the IEEE Int'l Symp. </booktitle> <publisher> Inform. </publisher> <address> Theory San Diego, CA Jan. </address> <year> 1990. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], [476], <ref> [363] </ref>, [426], [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [364] <author> T. Murakami, K. Asai, and E. Yamazaki, </author> <title> "Vector quantizer of video signals," </title> <journal> Electronics Letters, </journal> <volume> vol. 7, </volume> <pages> pp. 1005-1006, </pages> <month> Nov. </month> <year> 1982. </year>
Reference: [365] <author> S. Na and D. L. Neuhoff, </author> " <title> Bennett's integral for vector quan-tizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 886-900, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The extension of Bennett's integral to vector quan-tizers was first made by Gersho (1979) [193] for quantizers with congruent cells for which the concept of inertial profile was not needed, and then to vector quantizers with varying cell shapes (and codevector placements) by Na and Neuhoff (1995) <ref> [365] </ref>. <p> On the other hand, it is also important to understand what specific characteristics of vector quantizers improve with dimension and by how much. Motivated by several prior explanations [342], [333], <ref> [365] </ref>, we offer the following. We wish to compare an optimal quantizer, q k , with dimension k to an optimal k 0 -dimensional quantizer, q k 0 with k 0 &gt;> k. To simplify the discussion, assume k 0 is a multiple of k. <p> Since the widths of the cells are, approximately, determined by sq (x 1 ), the point density and inertial profile of q pr;k 0 are determined by sq . Specifically, from the rectangular nature of the product cells one obtains <ref> [365] </ref>, [378] pr;k 0 (x) = Y sq (x i ) (34) m pr;k 0 (x) = 12 k 0 i=1 2 sq (x i ) i=1 2 sq (x i ) k 0 which derive, respectively, from the facts that the volume of a rectangle is the product of its <p> To quantify the suboptimality of the product quantizer's principal feature, we factor the ratio of the distortions of q pr;k 0 (x) and q k 0 , which is a kind of loss, into terms that reflect the loss due to the inertial profile and point density <ref> [365] </ref>, [379] 5 5 Na and Neuhoff considered the ratio of the product code distortion 26 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 L = ffi k 0 (R) B (k 0 ; m pr;k 0 ; pr;k 0 ; f ) k 0 ; f ) <p> The space-filling loss was called a cubic loss in <ref> [365] </ref> 7 Dimension will be added as a subscript to f in places where the dimension of X needs to be emphasized. quantizer to have, approximately, the optimal point density 8 : pr;k 0 (x) = Q k 0 k 0 (x), where the last step uses the fact that k <p> Specifically, when there is dependence/correlation between source samples, the product point density cannot match the ideal point density, not even approximately. See [333], <ref> [365] </ref> for a defi nition of memory loss. (One can factor both the point density and oblongitis losses into two terms, one of which is 8 The fact that product quantizers can have the optimal point density is often overlooked. 9 This implies that distortion will not decrease as 2 2R <p> The generalization of Bennett's integral to fixed-rate vector quantizers with rather arbitrary cell shapes was accomplished by Na and Neuhoff (1995) <ref> [365] </ref>, who presented both informal and rigorous derivations. <p> Even assuming Gersho's conjecture is correct, there is no rigorous derivation of the Zador-Gersho formulas (30) and (32) along the lines of the informal derivations that start with Bennett's integral. We also mention that the tail conditions given in some of the rigorous results (e.g. [58], <ref> [365] </ref>) are very difficult to check. Simpler ones are needed. Finally, as discussed in Section II there are no convincing (let alone rigorous) asymptotic analyses of the operational distortion-rate function of DPCM. I.
Reference: [366] <author> S. Nanda and W. A. Pearlman, </author> <title> "Tree Coding of Image Sub-bands", </title> <journal> IEEE Trans. Image Proc., </journal> <volume> vol. 1, </volume> <pages> pp. 133-147, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding <ref> [366] </ref>, residual vector quantization [295], and other methods [107]. A major breakthrough in performance and complexity came with the introduction of zerotrees [315], [466], [457], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [367] <author> M. Naraghi-Pour and D. L. Neuhoff, </author> <title> "Mismatched DPCM encoding of autoregressive processes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 296-304, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], <ref> [367] </ref>, [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY,
Reference: [368] <author> M. Naraghi-Pour and D. L. Neuhoff, </author> <title> "On the continuity of the stationary state distribution of DPCM," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 305-311, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], <ref> [368] </ref>, [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL.
Reference: [369] <author> M. Naraghi-Pour and D. L. Neuhoff, </author> <title> "Convergence of the projection method for an autoregressive process and a matched DPCM code," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 1255-1264, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], [473], [17], [346], [241], [262], [156], [189], [190], [367], [368], <ref> [369] </ref>, [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power spectral den 8 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44,
Reference: [370] <author> B. K. Natarajan, </author> <title> "Filtering Random Noise from Deterministic Signals via Data Compression," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 43, no. 11, </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: Related results and approaches can be found in Witsen-hausen's (1980) [535] treatment of rate distortion theory with modified (or "indirect") distortion measures, and in the Occam filters of Natarajan (1995) <ref> [370] </ref>. H. Multiple Description Quantization A topic closely related to quantization for noisy channels is multiple description quantization. The problem is usually formulated as a source coding or quantization problem over a network, but it is most easily described in terms of packet communications.
Reference: [371] <author> N. M. Nasrabadi and Y. Feng, </author> <title> "Image compression using address-vector quantization," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 38, </volume> <pages> pp. </pages> <month> 2166-2173 Dec. </month> <year> 1990. </year>
Reference-contexts: This is a kind of interblock lossless coding [384], [410], [428]. Address-vector quantization, introduced by Nasrabadi and Feng <ref> [371] </ref> (see also [160], [373]), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [372] <author> N. M. Nasrabadi and R. A. King, </author> <title> "Image coding using vector quantization: A review," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 36, </volume> <pages> pp. 957-971, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Towards the middle of the 1980's, several tutorial articles on vector quantization appeared, which greatly increased the accessibility of the subject [195], [214], [342], <ref> [372] </ref>. 16 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 F. The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources.
Reference: [373] <author> N. M. Nasrabadi, J. U. Roy, and C. Y. Choo, </author> <title> "An interframe hierarchical address-vector quantization," </title> <journal> IEEE Trans. Selected Areas Commun., </journal> <volume> vol. 10, </volume> <pages> pp. 960-967, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This is a kind of interblock lossless coding [384], [410], [428]. Address-vector quantization, introduced by Nasrabadi and Feng [371] (see also [160], <ref> [373] </ref>), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [374] <author> A. N. Netravali and B. G. </author> <title> Haskell, Digital Pictures: Representation and Compression, </title> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1988. </year> <note> Second Edition, </note> <year> 1995. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], <ref> [374] </ref>, [196], [424], [50], [458] and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], <ref> [374] </ref>, [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [375] <author> A. N. Netravali and J. O. </author> <title> Limb, "Picture coding: a review," </title> <journal> Proc. IEEE, </journal> <volume> vol. 68, </volume> <pages> pp. 366-406, </pages> <month> March </month> <year> 1980. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], <ref> [375] </ref>, [265], [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [376] <author> A. N. Netravali and R. Saigal, </author> <title> "Optimal quantizer design using a fixed-point algorithm," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 55, </volume> <pages> pp. 1423-1435, </pages> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: In 1976 Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint <ref> [376] </ref>. Yet another approach was taken by Noll and Zelinski (1978) [391]. Berger refined his approach to entropy-constrained quantizer design in [48].
Reference: [377] <author> D. L. Neuhoff, </author> <title> "Source coding strategies: simple quantizers vs. simple noiseless codes," </title> <booktitle> Proc. 1986 Conf. on Inform. Sciences and Systems, </booktitle> <volume> vol. 1, </volume> <pages> pp. 267-271, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Stated another way, the benefits of entropy coding decrease with increasing quantizer dimension, and the benefits of increasing quantizer dimension decrease with increasing entropy coding order. In summary (cf. <ref> [377] </ref>), optimal performance is attainable with and only with a high dimensional lossy encoder, and with or without entropy coding. However, good performance (within 1.53 dB of the best) is attainable with uniform scalar quantizer and high order entropy coding.
Reference: [378] <author> D. L. </author> <title> Neuhoff "Why vector quantizers outperform scalar quan-tizers on stationary memoryless sources," </title> <booktitle> IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Whistler, B.C., p. 438, </address> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Since the widths of the cells are, approximately, determined by sq (x 1 ), the point density and inertial profile of q pr;k 0 are determined by sq . Specifically, from the rectangular nature of the product cells one obtains [365], <ref> [378] </ref> pr;k 0 (x) = Y sq (x i ) (34) m pr;k 0 (x) = 12 k 0 i=1 2 sq (x i ) i=1 2 sq (x i ) k 0 which derive, respectively, from the facts that the volume of a rectangle is the product of its side
Reference: [379] <author> D. L. Neuhoff, </author> <title> "On the asymptotic distribution of the errors in vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 461-468, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: To quantify the suboptimality of the product quantizer's principal feature, we factor the ratio of the distortions of q pr;k 0 (x) and q k 0 , which is a kind of loss, into terms that reflect the loss due to the inertial profile and point density [365], <ref> [379] </ref> 5 5 Na and Neuhoff considered the ratio of the product code distortion 26 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 L = ffi k 0 (R) B (k 0 ; m pr;k 0 ; pr;k 0 ; f ) k 0 ; f ) B <p> Zador [562] found high resolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers. Lee and Neuhoff [312], <ref> [379] </ref> found high resolution expressions for the density of the error produced by fairly general (deterministic) scalar and vector quantizers in terms of their point density and their shape profile, which is a function that conveys more cell shape information than the inertial profile. <p> In order to apply Bennett's integral, it was necessary to find the form of the probability density of the quantization error produced by the first stage. This motivated the asymptotic error density analysis of vector quantization in [312], <ref> [379] </ref>. Multistage quantizers have been improved in a number of ways. More sophisticated (than greedy) encoding algorithms can take advantage of the direct sum nature of the codebook to make optimal or nearly optimal searches, though with some (and sometimes a great deal of) increased complexity.
Reference: [380] <author> D. L. Neuhoff, </author> <title> "Polar quantization revisited," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Ulm, Germany, p. 60, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Originally, methods were developed specifically for polar quantizers. However, recently it has been shown that Bennett's integral can be applied to analyze polar quantization in a straightforward way <ref> [380] </ref>. It turns out that for an i.i.d. Gaussian source, optimized conventional polar quantization gains about .41 42 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 dB over direct scalar quantization, and optimized unrestricted polar quantization gains another .73 dB.
Reference: [381] <author> D. L. Neuhoff and R.K. Gilbert, </author> <title> "Causal source codes," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 701-713, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], [414], [148], [534], [178], <ref> [381] </ref>, [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [382] <author> D. L. Neuhoff, R. M. Gray and L. D. Davisson, </author> <title> "Fixed rate universal block source coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-21, No. 5, </volume> <pages> pp. 511-523, </pages> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: Subsequently a variety of notions of fixed-rate universal codes were considered and compared <ref> [382] </ref>, and fixed-distortion codes with variable-rate were developed by Mackenthun and Pursley [340] and Kieffer [277], [279].
Reference: [383] <author> D. L. Neuhoff and D. H. Lee, </author> <title> "On the performance of tree-structured vector quantization," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP), Toronto, </booktitle> <volume> vol. 4, </volume> <pages> pp. 2277-2280, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: On the one hand, rate distortion theory analysis [228], [291], [292], [557], [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality. On the other hand, high resolution analyses of TSVQ <ref> [383] </ref> and two-stage VQ [311] have quantified the loss of these particular codes, and in the latter case shown ways of modifying the quantizer to eliminate the loss. Thus, both theories have something to say about successive refinement. V. <p> k , the ratio of the normalized moment of inertia of a cube to that of the best k dimensional cell shape, which approaches 1.53 dB for large k, and the remainder, about .5 to .7 dB, is due to the ob-longitis caused by the cubes being cut into pieces <ref> [383] </ref>. A paper investigating the nature of TSVQ cells is [569]. Our experience has been that when taking both performance and complexity into account, TSVQ is a very competitive VQ method.
Reference: [384] <author> D. L. Neuhoff and N. Moayeri, </author> <title> "Tree searched vector quantization with interblock noiseless coding," </title> <booktitle> Proc. Conf. </booktitle> <institution> Inform. Sci. and Syst., Princeton, </institution> <address> NJ, </address> <pages> pp. 781-783, </pages> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar. This is a kind of interblock lossless coding <ref> [384] </ref>, [410], [428]. Address-vector quantization, introduced by Nasrabadi and Feng [371] (see also [160], [373]), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [385] <author> D. J. Newman, </author> <title> "The hexagon theorem," </title> <institution> Bell Laboratories Technical Memorandum, </institution> <year> 1964. </year> <journal> Published in the special issue on quantization of the IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 137-139, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: In addition, Gish and Pierce observed that when coding vectors, performance could be improved by using quan-tizer cells other than the cube implicitly used by uniform scalar quantizers and noted that the hexagonal cell was superior in two dimensions, as originally demonstrated by Fejes Toth [159] and Newman <ref> [385] </ref>. Though uniform quantization is asymptotically best for entropy-constrained quantization, at lower rates nonuniform quantization can do better, and a series of papers explored algorithms for designing them. <p> Using an earlier inequality of his [158], he showed that the optimal two-dimensional quantizer under these assumptions tessellated the support region with hexagons. This was the first evaluation of the performance of a genuinely multidimensional quantizer. It was rederived in a 1964 Bell Laboratories Technical Memorandum by Newman <ref> [385] </ref>; its first appearance in English. <p> The latter statement has been proven for k = 1 (cf. [106], p. 59) and for k = 2 by Fejes Toth (1959) [159]; see also <ref> [385] </ref>. For k = 3, it is known that the best lattice tessellation is the body-centered cubic lattice, which is generated by a truncated octahedron [35]. It has not been proven that this is the best tessellation, though one would suspect that it is. <p> This result was independently rederived in a simpler fashion by Newman (1964) <ref> [385] </ref>. Clearly, the lower bound is asymptotically achievable by a lattice with hexagonal cells. It follows then that the ratio of ffi 2 (R) to M (hexagon)oe 2 2 2R tends to one, and also, that Gersho's conjecture holds for dimension two.
Reference: [386] <author> N. B. Nill, </author> <title> "A Visual Model Weighted Cosine Transform for Image Compression and Quality Assessment," </title> <journal> IEEE Trans. Comm., </journal> <volume> Vol 33, </volume> <pages> pp. 551-557, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], [387], <ref> [386] </ref>, [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher [150] and Nill <ref> [386] </ref>, [387]. The Bennett integral has been extended to this type of distortion, and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed [186], [316].
Reference: [387] <author> N. B. Nill and B. H. Bouxas, </author> <title> "Objective image quality measure derived from digital image power spectra," </title> <journal> Optical Engineering, </journal> <volume> Vol 31, </volume> <pages> pp. 813-825, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: class of distortion measures that have proved useful in perceptual coding, the input weighted quadratic distortion measures of the form d (x; ^x) = (x ^x) t W x (x ^x); (21) where W x is a positive definite matrix that depends on the input, cf. [258], [259], [257], [224], <ref> [387] </ref>, [386], [150], [186], [316], [323], [325]. Most of the theory and design techniques considered here extend to such measures, as will be discussed later. <p> Other distortion measures satisfying the assumptions are the image distortion measures of Eskicioglu and Fisher [150] and Nill [386], <ref> [387] </ref>. The Bennett integral has been extended to this type of distortion, and approximations for both fixed-rate and variable-rate operational distortion-rate functions have been developed [186], [316].
Reference: [388] <author> A. B. Nobel, </author> <title> "Vanishing distortion and shrinking cells," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 1303-5, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], <ref> [388] </ref>, [389]. For other work on tree growing and pruning see [393], [439], [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [389] <author> A. B. Nobel, </author> <title> "Recursive partitioning to reduce distortion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.43, </volume> <pages> pp. 1122-33, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], [388], <ref> [389] </ref>. For other work on tree growing and pruning see [393], [439], [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [390] <author> A. B. Nobel and R.A. Olshen, </author> <title> "Termination and continuity of greedy growing for tree-structured vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 191-205, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen <ref> [390] </ref>, [388], [389]. For other work on tree growing and pruning see [393], [439], [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H.
Reference: [391] <author> P. Noll and R. Zelinski, </author> <title> "Bounds on quantizer performance in the low bit-rate region," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 26, </volume> <pages> pp. 300-305, </pages> <month> Feb. </month> <year> 1978. </year>
Reference-contexts: In 1976 Netravali and Saigal introduced a fixed-point algorithm with the same goal of minimizing average distortion for a scalar quantizer with an entropy constraint [376]. Yet another approach was taken by Noll and Zelinski (1978) <ref> [391] </ref>. Berger refined his approach to entropy-constrained quantizer design in [48]. Variable-rate quantization was also extended to DPCM and transform coding, where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing [398], [154].
Reference: [392] <author> K. L. Oehler and R. M. Gray, </author> <title> "Mean-Gain-Shape Vector Quantization," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 241-244, </pages> <address> Minneapolis, Minnesota, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Similar ideas can be used for mean-removed VQ [20], [21] and mean/gain/shape VQ <ref> [392] </ref>. The most general formulation of product codes has been given by Chan and Gersho [82]. It includes a number of schemes with dependent quantization, even tree-structured and multistage quantization, to be discussed later. Fischer's pyramid VQ [164] is also a kind of shape-gainVQ.
Reference: [393] <author> K. L. Oehler, E. A. Riskin, and R. M.Gray, </author> <title> "Unbalanced tree-growing algorithms for practical image compression," </title> <booktitle> Proc. IEEE ICASSP, Toronto, </booktitle> <pages> pp. 2293-2296, </pages> <year> 1991. </year> <note> GRAY AND NEUHOFF: QUANTIZATION 59 </note>
Reference-contexts: There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see <ref> [393] </ref>, [439], [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [394] <author> B. M. Oliver, J. Pierce, and C. E. Shannon, </author> <title> "The philosophy of PCM," </title> <journal> Proc. IRE, </journal> <volume> vol. 36, </volume> <pages> pp. 1324-1331, </pages> <month> Nov. </month> <year> 1948. </year>
Reference-contexts: A. Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size [468], <ref> [394] </ref>, [43], which we now derive. GRAY AND NEUHOFF: QUANTIZATION 21 Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + .
Reference: [395] <author> B. M. Oliver, </author> <title> "Efficient Coding," </title> <journal> Bell Syst. Tech.J., </journal> <volume> vol. 31, </volume> <pages> pp. 724-750, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include <ref> [395] </ref>, [300], [237], [511], [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived. <p> The possibility of applying variable-length coding to quantization may well have occurred to any number of people who were familiar with both quantization and Shan-non's 1948 paper. The earliest references to such that we have found are in the 1952 papers by Oliver <ref> [395] </ref> and Kret-zmer [300]. In 1960, Max [349] had such in mind when he computed the entropy of nonuniform and uniform quan-tizers that had been designed to minimize distortion for a given number of levels.
Reference: [396] <author> J. B. O'Neal, Jr., </author> <title> "A bound on signal-to-quantizing noise ratios for digital encoding systems," </title> <journal> Proc. IEEE, </journal> <volume> vol. 55, </volume> <pages> pp. 287-292, </pages> <month> Mar. </month> <year> 1967. </year>
Reference-contexts: is usually sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain [350], <ref> [396] </ref>, [482], [397], [265]. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [397] <author> J.B. O'Neal, Jr., </author> <title> "Signal to quantization noise ratio for differential PCM," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 19, pp.568-569, </volume> <month> Aug. </month> <year> 1971. </year>
Reference-contexts: sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain [350], [396], [482], <ref> [397] </ref>, [265]. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [398] <author> J. B. O'Neal, Jr., </author> <title> "Entropy coding in speech and television differential PCM systems," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 17, </volume> <pages> pp. 758-761, </pages> <month> Nov. </month> <year> 1971. </year>
Reference-contexts: Berger refined his approach to entropy-constrained quantizer design in [48]. Variable-rate quantization was also extended to DPCM and transform coding, where high resolution analysis shows that it gains the same relative to fixed-rate quantization as it does when applied to direct scalar quantizing <ref> [398] </ref>, [154]. We note, however, that the variable-rate quantization analysis for DPCM suffers from the same flaws as the fixed-rate quantization analysis for DPCM.
Reference: [399] <author> M. T. Orchard, </author> <title> "A fast nearest neighbor search algorithm," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Toronto, </booktitle> <pages> pp. 2297-2300, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], [476], [363], [426], [249], <ref> [399] </ref>, [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [400] <author> M. T. Orchard and C. A. Bouman, </author> <title> "Color Quantization of Images," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 39, </volume> <pages> pp. 2677-2690, </pages> <month> Dec. </month> <year> 1991. </year>
Reference: [401] <author> L. Ozarow, </author> <title> "On a source-coding problem with two channels and three receivers," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 59, </volume> <pages> pp. 1909-1921, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: This problem was first tackled in the information theory community in 1980 by Wolf, Wyner, and Ziv [536] and Ozarow <ref> [401] </ref> who de GRAY AND NEUHOFF: QUANTIZATION 51 veloped achievable rate regions and lower bounds to performance. The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) [6], and Zhang and Berger (1987) [573].
Reference: [402] <author> K. K. Paliwal and V. Ramasubramanian, </author> <title> "Effect of ordering the codebook on the efficiency of the partial distance search algorithm for vector quantization," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 37, </volume> <pages> 538-540, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. Other fast search methods include the partial distortion method of [88], [39], <ref> [402] </ref> and the transform subspace domain approach of [78]. Consideration of methods based on prequantization leads to the question of how fine the prequantization cells should be.
Reference: [403] <author> J. Pan and T. R. Fischer, </author> <title> "Vector quantization-lattice vector quantization of speech LPC coefficients," </title> <booktitle> Proc. IEEE Intl. Conf. on Acoust., Speech, and Signal Processing (ICASSP), Adelaide, Aust. Part 1, </booktitle> <year> 1994. </year>
Reference-contexts: Good schemes of this sort have even been developed for low to moderate rates by Gibson [270], [271] and Pan and Fischer <ref> [403] </ref>, [404]. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [302] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [487].
Reference: [404] <author> J. Pan and T. R. Fischer, </author> <title> "Two-stage vector quantization-lattice vector quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 155-163, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: For example, one may use this sort of argument to deduce that the quantization error of a good high dimensional quantizer is approximately white and Gaussian when the source is memoryless, the distortion is squared error, and the rate is large, cf. <ref> [404] </ref>, which shows Gaussian-like histograms for the quantization error of VQ's with dimensions 8 to 32. <p> Good schemes of this sort have even been developed for low to moderate rates by Gibson [270], [271] and Pan and Fischer [403], <ref> [404] </ref>. Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [302] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in [487].
Reference: [405] <author> P. F. Panter and W. </author> <title> Dite "Quantizing distortion in pulse-count modulation with nonuniform spacing of levels," </title> <journal> Proc. IRE, </journal> <volume> vol. 39, </volume> <pages> pp. 44-48, </pages> <month> Jan. </month> <year> 1951. </year>
Reference-contexts: In the following we review the development of rigorous theory. Many analyses | informal and rigorous | explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored [43], <ref> [405] </ref>, [474]. In some cases the source really does have finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature [43], <ref> [405] </ref>, [474] assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density. <p> In the scalar case, one can use this philosophy directly to construct a good quantizer, by designing a compander whose nonlinearity c (x) has derivative fl 1 (x), and extracting the resulting reconstruction levels and thresholds to obtain an approximately optimal point quantizer. This was first mentioned in Panter-Dite <ref> [405] </ref> and rediscovered several times. Unfortunately, at higher dimensions, companders cannot implement an optimal point density without creating large oblongitis [193], [56], [57]. So there is no direct way to construct optimal vector quan tizers with the high resolution philosophy.
Reference: [406] <author> W. A. Pearlman, </author> <title> "Polar quantization of a complex Gaussian random variable," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 27, No. 6, </volume> <pages> pp. 892-899, </pages> <year> 1979. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], <ref> [406] </ref>, [61], [62], [530], [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase.
Reference: [407] <author> W. A. Pearlman and R. M. Gray, </author> <title> "Source coding of the discrete Fourier transform," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 24, </volume> <pages> pp. 683-692, </pages> <month> Nov. </month> <year> 1978. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], <ref> [407] </ref>, [406], [61], [62], [530], [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase.
Reference: [408] <author> W. B. Pennebaker and J. L. Mitchell, </author> <title> JPEG Still Image Compression Standard, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], [208], <ref> [408] </ref>, [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [409] <author> C. Pepin, J.-C. Belfiore and J. </author> <title> Boutros "Quantization of both stationary and nonstationary Gaussian sources with Voronoi constellations," </title> <booktitle> Proc. IEEE Int'l Symp. Inform. Theory, </booktitle> <address> Ulm, Germany, p. 59, </address> <year> 1997. </year>
Reference-contexts: Similar results were reported by Pepin et al. <ref> [409] </ref>. On the other hand, as mentioned earlier, a quantizer with dimension 12 can achieve this same distortion. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the code-book and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. [580], [151], [359], [149], <ref> [409] </ref>. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.
Reference: [410] <author> N. Phamdo and N. Farvardin, </author> <title> "Coding of speech LSP parameters using TSVQ with interblock noiseless coding," </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Albuquerque, NM, </address> <pages> pp. 193-196, </pages> <year> 1990. </year>
Reference-contexts: A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar. This is a kind of interblock lossless coding [384], <ref> [410] </ref>, [428]. Address-vector quantization, introduced by Nasrabadi and Feng [371] (see also [160], [373]), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [411] <author> N. Phamdo and N. Farvardin, </author> <title> "Optimal detection of discrete Markov sources over discrete memoryless channels applications to combined source-channel coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 186-193, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412]. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel <ref> [411] </ref>, [523]. Simulated annealing has also been used to design such quantizers [140], [152], [354].
Reference: [412] <author> N. Phamdo, N. Farvardin, and T. Moriya, </author> <title> "A unified approach to tree-structured and multistage vector quantization for noisy channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 835-850, </pages> <month> May, </month> <year> 1993. </year>
Reference-contexts: The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ <ref> [412] </ref>. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], [523]. Simulated annealing has also been used to design such quantizers [140], [152], [354].
Reference: [413] <author> R. Pilc, </author> <title> "The transmission distortion of a source as a function of the encoding block length," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 47, </volume> <pages> pp. 827-885, </pages> <year> 1968. </year>
Reference-contexts: GRAY AND NEUHOFF: QUANTIZATION 35 Fig. 7. Signal-to-noise ratios for optimal VQs (dots) and predictions thereof based on the Zador-Gersho formula (straight lines). The convergence rate of ffi k (R) to ffi (R) as k tends to infinity has also been studied <ref> [413] </ref>, [548], [321], [576]. Roughly speaking these results show that for memoryless sources the convergence rate is between q k and log k k .
Reference: [414] <author> P. Piret, </author> <title> "Causal sliding block encoders with feedback," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 25, </volume> <pages> pp. 237-240, </pages> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], <ref> [414] </ref>, [148], [534], [178], [381], [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [415] <author> G. Poggi, </author> <title> "Fast algorithm for full-search VQ encoding," </title> <journal> Electron. Lett., </journal> <volume> vol. 29, </volume> <pages> pp. 1141-1142, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], [334], [146], [532], [423], <ref> [415] </ref>, [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [416] <author> G. Poggi, </author> <title> "Generalized-cost-measure-based address-predictive vector quantization," </title> <journal> IEEE Trans. Image Proc., </journal> <volume> vol. 5, </volume> <pages> pp. 49-55, </pages> <month> Jan. </month> <year> 1996. </year>
Reference: [417] <author> G. Poggi and R. A. Olshen, </author> <title> "Pruned tree-structured vector quantization of medical images with segmentation and improved prediction," </title> <journal> IEEE Trans. Image Processing vol. </journal> <volume> 4, </volume> <pages> pp. 734-742, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Feedback Vector Quantization Just as with scalar quantizers, a vector quantizer can be predictive; simply replace scalars with vectors in the predictive quantization structure depicted in Figure 3 [235], [116], [85], <ref> [417] </ref>. Alternatively, the encoder and decoder can share a finite set of states and a quantizer custom designed for each state.
Reference: [418] <author> D. Pollard, </author> <title> "Quantization and the method of k-means," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 199-205, </pages> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: rth power distortion measures and his extensive comparison of minimum entropy quantizers and fixed-rate permutation codes [48], generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima [503], results on the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard <ref> [418] </ref>, two seminal papers on lattice quantization by Conway and Sloane [103], [104], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew and Wise [64], Kieffer's demonstration of stochastic stability for a general class of feedback quantizers including the historic class of predictive quantizers <p> We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties [451], <ref> [418] </ref>, [108], [91], [101], [321], [335], [131], [36].
Reference: [419] <author> K. Popat and K. Zeger, </author> <title> "Robust quantization of memory-less sources using dispersive FIR filters," </title> <journal> IEEE Trans. Comm., </journal> <volume> Vol.40, </volume> <pages> pp. 1670-4, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Zeger (1992) took advantage of the central limit theorem and the known structure of an optimal scalar quantizer for a Gaussian random variable to code a general process by first filtering it to produce an approximately Gaussian density, scalar quantizing the result, and then inverse filtering to recover the original <ref> [419] </ref>. C. Robust Quantization The Gaussian quantizers were described as being robust in a minimax average sense: a vector quantizer suitably designed for a Gaussian source will yield no worse average distortion for any source in the class of all sources with the same second order properties.
Reference: [420] <author> E. Posner and E. Rodemich, </author> <title> "Epsilon entropy and data compression," </title> <journal> Ann. Math. Stat., </journal> <volume> vol. 42, </volume> <pages> pp. 2079-2125, </pages> <year> 1971. </year>
Reference-contexts: As in the Shannon case, all these definitions can be made for k-dimensional vectors X k and the limiting behavior can be studied. Results regarding the convergence of such limits and the equality of the information-theoretic and operational notions of epsilon-entropy can be found, e.g., in [421], <ref> [420] </ref>, [278], [59]. Much of the theory is concerned with approximating epsilon entropy for small *. Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability. <p> Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability. Here the epsilon entropy becomes the log of the smallest number of balls of radius * required to cover a compact metric space (e.g., a function space) (see, e.g., [520] <ref> [420] </ref> for a discussion of various notions of epsilon entropy). We mention epsilon entropy because of its close mathematical connection to rate distortion theory. Our emphasis, however, is on codes that minimize average, not maximum, distortion.
Reference: [421] <author> E. Posner, E. Rodemich, and H. Rumsey, Jr., </author> <title> "Epsilon entropy of stochastic processes," </title> <journal> Ann. Math. Stat., </journal> <volume> vol. 38, </volume> <pages> pp. 1000-1020, </pages> <year> 1967. </year>
Reference-contexts: As in the Shannon case, all these definitions can be made for k-dimensional vectors X k and the limiting behavior can be studied. Results regarding the convergence of such limits and the equality of the information-theoretic and operational notions of epsilon-entropy can be found, e.g., in <ref> [421] </ref>, [420], [278], [59]. Much of the theory is concerned with approximating epsilon entropy for small *. Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability.
Reference: [422] <author> W. K. Pratt, </author> <title> Image Transmission Techniques, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], <ref> [422] </ref>, [375], [265], [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [423] <author> S. W. Ra and J. K. Kim, </author> <title> "A fast mean-distance-ordered partial codebook search algorithm for image vector quantization," </title> <journal> IEEE Trans. Circuits Syst. </journal> <volume> II, vol. 40, </volume> <pages> pp. 576-579, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], [334], [146], [532], <ref> [423] </ref>, [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [424] <author> M. Rabbani and P. W. Jones, </author> <title> Digital Image Compression Techniques, vol. TT7 of Tutorial Texts in Optical Engineering, </title> <publisher> SPIE Optical Engineering Press, Bellingham, </publisher> <address> WA, </address> <year> 1991. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], [374], [196], <ref> [424] </ref>, [50], [458] and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], <ref> [424] </ref>, [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [425] <author> V. Ramasubramanian and K. K. Paliwal, </author> <title> "An optimized k-d tree algorithm for fast vector quantization of speech," </title> <booktitle> Proc. European Signal Processing Conf., Grenoble, </booktitle> <pages> pp. 875-878, </pages> <year> 1988. </year>
Reference: [426] <author> V. Ramasubramanian and K. K. Paliwal, </author> <title> "An efficient approximation-elimination algorithm for fast nearest- neighbor search based on a spherical distance coordinate formulation," </title> <booktitle> Proc. European Signal Processing Conf., </booktitle> <address> Barcelona, </address> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], [476], [363], <ref> [426] </ref>, [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [427] <author> K. Ramchandran and M. Vetterli, </author> <title> "Best wavelet packet bases in a rate-distortion sense," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 2, </volume> <pages> pp. 160-176, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The zerotree approach has been extended to vector quantization (e.g., [109]), but the slight improvement comes at a significant cost in added complexity. Rate-distortion ideas have been used to optimize the rate-distortion tradeoffs using wavelet packets by minimizing a Lagrangian distortion over code trees and bit assignments <ref> [427] </ref>. Recently competitive schemes have demonstrated that separate scalar quantization of individual subbands coupled with a sophisticated but low complexity lossless coding algorithm called stack-run coding can provide performance nearly as good as EZW [504].
Reference: [428] <author> X. Ran and N. Farvardin, </author> <title> "Combined VQ-DCT coding of images using interblock noiseless coding," </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Albuquerque, NM, p. 2281-2284, </address> <year> 1990. </year>
Reference-contexts: A simple approach that works for TSVQ is to code the binary path to the codevector for the present source vector relative to the binary path to that of the previous source vector, which is usually very similar. This is a kind of interblock lossless coding [384], [410], <ref> [428] </ref>. Address-vector quantization, introduced by Nasrabadi and Feng [371] (see also [160], [373]), is another way to introduce memory into the lossy encoder of a vector quantizer with the goal of attaining higher dimensional performance with lower dimensional complexity.
Reference: [429] <author> D. R. Rao and P. Yip, </author> <title> Discrete Cosine Transform, </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1990. </year>
Reference-contexts: Transform coding has been extensively developed for coding images and video, where the discrete cosine transform (DCT) [7], <ref> [429] </ref> is most commonly used because of its computational simplicity and its good performance. Indeed DCT coding is the basic approach dominating current image and video coding standards, including H.261, H.263, JPEG, and MPEG.
Reference: [430] <author> C. J. Read, D. M. Chabries, R. W. Christiansen and J. K. Flanagan, </author> <title> "A Method for Computing the DFT of Vector Quantized Data," </title> <journal> pp. </journal> <pages> 1015-1018, </pages> <booktitle> Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), </booktitle> <address> Glasgow, Scotland, </address> <month> May </month> <year> 1989. </year>
Reference: [431] <author> G. Rebolledo and R. M. Gray and J. P. </author> <title> Burg, "A multirate voice digitizer based upon vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 30, </volume> <pages> pp. 721-727, </pages> <month> April </month> <year> 1982. </year>
Reference: [432] <author> A. H. Reeves, </author> <title> French Patent No. </title> <address> 852,183, Oct. 3, </address> <year> 1938. </year>
Reference: [433] <author> A. Renyi, </author> <title> "On the dimension and entropy of probability distributions," </title> <journal> Acta Math. Acad. Sci. Hungar., </journal> <volume> vol. 10, </volume> <pages> pp. 193-215, </pages> <year> 1959. </year>
Reference-contexts: But he did not find the multiplicative factors, nor did he describe the nature of the partitions and codebooks that are best for variable-rate quantization. In 1959, Renyi <ref> [433] </ref> showed that a uniform scalar quan-tizer with infinitely many levels and small cell width has output entropy given approximately by H (q (X)) = h (X) log (11) where h (X) = f (x) log f (x)dx is the differential entropy of the source variable X." 10 IEEE TRANSACTIONS ON <p> Next, two papers appeared in the same issue of Acta Math. Acad. Sci. Hungar. in 1959. The paper by Renyi <ref> [433] </ref> gave, in effect, a rigorous derivation of (11) for a uniform quantizer with infinitely many levels.
Reference: [434] <author> S. O. Rice, </author> <title> "Mathematical analysis of random noise," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 23, </volume> <pages> pp. 282-332, </pages> <year> 1944, </year> <journal> and vol. </journal> <volume> 24, </volume> <pages> pp. 46-156, </pages> <year> 1945. </year> <title> Reprinted in Selected papers on noise and stochastic processes (N. </title> <editor> Wax and N. Wax, </editor> <booktitle> eds.), </booktitle> <pages> pp. 133-294, </pages> <address> New York, NY: </address> <publisher> Dover, </publisher> <year> 1954. </year>
Reference: [435] <author> R. F. Rice and J. R. Plaunt, </author> <title> "The Rice Machine: television data compression," </title> <type> Technical Report 900-408, </type> <institution> Jet Propulsion Laboratory, Pasadena, </institution> <address> CA, </address> <month> September </month> <year> 1970. </year>
Reference-contexts: This is the idea behind universal quantization. Universal quantization or universal source coding had its origins in an approach to universal lossless compression developed by Rice and Plaunt <ref> [435] </ref>, [436] and dubbed the "Rice machine." Their idea was to have a lossless coder that would work well for distinct sources by running multiple lossless codes in parallel and choosing the one producing the fewest bits for a period of time, sending a small amount of overhead to inform the
Reference: [436] <author> R. F. Rice and J. R. Plaunt, </author> <title> "Adaptive variable-length coding for efficient compression of spacecraft television data," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 19, </volume> <pages> pp. 889-897, </pages> <month> December </month> <year> 1971. </year>
Reference-contexts: This is the idea behind universal quantization. Universal quantization or universal source coding had its origins in an approach to universal lossless compression developed by Rice and Plaunt [435], <ref> [436] </ref> and dubbed the "Rice machine." Their idea was to have a lossless coder that would work well for distinct sources by running multiple lossless codes in parallel and choosing the one producing the fewest bits for a period of time, sending a small amount of overhead to inform the decoder
Reference: [437] <author> B. Rimoldi, </author> <title> "Successive refinement of information: characterization of the achievable rates," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 253-259, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], [291], [292], [557], [147], <ref> [437] </ref>, [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [438] <author> E. A. Riskin, </author> <title> "Optimal bit allocation via the generalized BFOS algorithm," vol. 37, </title> <journal> IEEE Trans. Inform. Theory, </journal> <pages> pp. 400-402, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in [526], [470], [94], [439], <ref> [438] </ref>, [463]. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546].
Reference: [439] <author> E. A. Riskin and R. M. Gray, </author> <title> "A Greedy Tree Growing Algorithm for the Design of Variable Rate Vector Quantizers," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 39, </volume> <pages> pp. 2500-2507, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in [526], [470], [94], <ref> [439] </ref>, [438], [463]. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546]. <p> The TSVQ will still be competitive in terms of throughput, however, as the tree-structured search is amenable to pipelining. TSVQs can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) [342], [94], <ref> [439] </ref>, [196] and with larger branching factors than two or even variable branching factors [460]. <p> The most well known is the CART algorithm of Breiman, Friedman, Olshen, and Stone [53], and the variation of CART for designing TSVQs bears their initials: the BFOS algorithm [94], <ref> [439] </ref>, [196]. In this method, a balanced or unbalanced tree with more leaves than needed is first grown and then pruned. <p> a balanced tree by splitting all nodes in each level of the tree, or by splitting one node at a time, e.g., by splitting the node with the largest contribution to the distortion [342] or in a greedy fashion to maximize the decrease in distortion for the increase in rate <ref> [439] </ref>. Once grown, the tree can be pruned by removing all descendants of any internal node, thereby making it a leaf. This will increase average distortion, but will also decrease the rate. <p> There has been a flurry of recent work on the theory of tree growing algorithms for vector quantizers, which are a form of recursive partitioning. See for example the work of Nobel and Olshen [390], [388], [389]. For other work on tree growing and pruning see [393], <ref> [439] </ref>, [276], [22], [355] Multistage Vector Quantization Multistage (or multistep or cascade or residual) vector quantization was introduced by Juang and A.H. Gray, Jr. [274] as a form of tree-structured quantization with much reduced arithmetic complexity and storage.
Reference: [440] <author> E. A. Riskin, R. Ladner, R. Wang, and L. E. </author> <title> Atlas, "Index assignment for progressive transmission of full-search vector quantization," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> Vol.3, </volume> <pages> pp. 307-12, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Tree-structured, multistage and hierarchical quantizers, to be discussed in the next section, are examples of such. Other methods can be used to design progressive indexing into given codebooks, as in Yamada and Tazaki (1991) [553] and Riskin et al. (1994) <ref> [440] </ref> Successive approximation is useful in situations where the decoder needs to produce rough approximations of the data from the first bits it receives and, subsequently, to refine the approximation as more bits are received.
Reference: [441] <author> S. A. Rizvi, N. M. Nasrabadi, and W. L. Cheng, </author> <title> "Entropy-constrained predictive residual vector quantization," Optical Engineering, </title> <journal> pp. </journal> <volume> 187-197 vol. 35, </volume> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: And more sophisticated design algorithms (than the greedy one) can also have benefits [32], [177], [81], [31], [33]. Variable-rate multistage quantizers have been developed [243], [297], [298], <ref> [441] </ref>, [296]. Another way of improving multistage VQ is to adapt each stage to the outcome of the previous.
Reference: [442] <author> L. G. Roberts, </author> <title> "Picture coding using pseudo-random noise," </title> <journal> IRE Trans. Inform. Theory, </journal> <volume> vol. 8, </volume> <pages> pp. 145-154, </pages> <month> Feb. </month> <year> 1962. </year>
Reference-contexts: Their approach, dubbed "gold washing" did not involve training, but rather created and removed codevectors according to the data received and an auxiliary random process in a way that could be tracked by a decoder without side information. E. Dithering Dithered quantization was introduced by Roberts <ref> [442] </ref> in 1962 as a means of randomizing the effects of uniform quantization so as to minimize visual artifacts. It was further developed for images by Limb (1969) [317] and for speech by Jayant and Rabiner (1972) [266].
Reference: [443] <author> G. M. Roe, </author> <title> "Quantizing for minimum distortion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 10, </volume> <pages> pp. 384-385, </pages> <month> Oct. </month> <year> 1964. </year>
Reference-contexts: GRAY AND NEUHOFF: QUANTIZATION 29 Intuitively, the average squared error is the average energy or power in the quantization noise. The most common extension of distortion measures for scalars is the rth power distortion, d (x; y) = jx yj r . For example, Roe <ref> [443] </ref> generalized Max's formulation to distortion measures of this form. <p> For example, Lloyd [330] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when multiplied by N 2 . Roe <ref> [443] </ref>, Algazi [8] and Wood [539] also used Taylor series.
Reference: [444] <author> K. Rose, </author> <title> "Mapping approach to rate-distortion computation and analysis," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1939-1952, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms [168], <ref> [444] </ref>. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources. For other cases, the Blahut algorithm [52] can be used to compute D k (R), though its computational complexity becomes overwhelming unless k is small.
Reference: [445] <author> K. Rose, E. Gurewitz, and G. C. Fox, </author> <title> "A deterministic annealing approach to clustering," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 11, </volume> <pages> pp. 589-594, </pages> <month> September </month> <year> 1990. </year> <journal> 60 IEEE TRANSACTIONS ON INFORMATION THEORY, </journal> <volume> VOL. 44, NO. 6, </volume> <month> OCTOBER </month> <year> 1998 </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing <ref> [445] </ref>, [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [446] <author> K. Rose, E. Gurewitz, and G. C. Fox, </author> <title> "Vector quantization by deterministic annealing," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.38, </volume> <pages> pp. 1249-1257, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], <ref> [446] </ref>, [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [447] <author> K. Rose, E. Gurewitz, and G. C. Fox, </author> <title> "Constrained clustering as an optimization method," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> Vol.15, </volume> <pages> pp. 785-794, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], <ref> [447] </ref>, pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [448] <author> N. Rydbeck and C.-E. W. Sundberg, </author> <title> "Analysis of digital errors in nonlinear PCM systems," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 24, </volume> <pages> pp. 59-65, </pages> <month> Jan. </month> <year> 1976. </year>
Reference-contexts: The codes that do this are often called index assignments. Several specific index assignment methods were considered by Rydbeck and Sundberg <ref> [448] </ref>. DeMarca and Jayant in 1987 [121] introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho [568], who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include [210], [543], [287].
Reference: [449] <author> M. J. Sabin and R. M. Gray, </author> <title> "Product code vector quantizers for speech waveform coding," </title> <booktitle> Conf. Record Globecom, </booktitle> <pages> pp. 1087-1091, </pages> <month> Dec. </month> <year> 1982. </year>
Reference-contexts: As such they are ordinarily much less than the complexities of an unstructured quantizer with the same number of codevectors, whose complexities equal the product of those of the components of a product quantizer. A shape-gain vector quantizer <ref> [449] </ref>, [450] is an example of a product quantizer.
Reference: [450] <author> M. J. Sabin and R. M. Gray, </author> <title> "Product code vector quantizers for waveform and voice coding," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 32, </volume> <pages> pp. 474-488, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: As such they are ordinarily much less than the complexities of an unstructured quantizer with the same number of codevectors, whose complexities equal the product of those of the components of a product quantizer. A shape-gain vector quantizer [449], <ref> [450] </ref> is an example of a product quantizer.
Reference: [451] <author> M. J. Sabin and R. M. Gray, </author> <title> "Global convergence and empirical consistency of the generalized Lloyd algorithm," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 32, </volume> <pages> pp. 148-155, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: We focus on the Lloyd algorithm because of its simplicity, its proven merit at designing codes, and because of the wealth of results regarding its convergence properties <ref> [451] </ref>, [418], [108], [91], [101], [321], [335], [131], [36].
Reference: [452] <author> D. J. Sakrison, </author> <title> "Source encoding in the presence of random disturbance," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 165-167, </pages> <month> January </month> <year> 1968. </year>
Reference-contexts: The usefulness of this modified distortion for source coding noisy sources was first seen by Dobrushin and Tsybakov (1962) [134] and was used by Fine (1965) [162] and Sakrison (1968) <ref> [452] </ref> to obtain information theoretic bounds an quantization and source coding for noisy sources. Berger (1971) [46] explicitly used the modified distortion in his study of Shannon source coding theorems for noise corrupted sources.
Reference: [453] <author> D. J. Sakrison, </author> <title> "The rate distortion function of a Gaussian process with a weighted square error criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 506-508, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: Sakrison extended the extremal properties of the rate distortion functions to sources with memory <ref> [453] </ref>, [454], [455] and Lapidoth [306] (1997) showed that a code designed for a Gaussian source would yield essentially the same performance when applied to another process with the same covariance structure. These results are essentially Shannon theory and hence should be viewed as primarily of interest for high-dimensional quantizers.
Reference: [454] <author> D. J. Sakrison, </author> <title> "The rate distortion function for a class of sources," </title> <journal> Inform. and Control, </journal> <volume> vol. 15, </volume> <pages> pp. 165-195, </pages> <month> August </month> <year> 1969. </year>
Reference-contexts: Sakrison extended the extremal properties of the rate distortion functions to sources with memory [453], <ref> [454] </ref>, [455] and Lapidoth [306] (1997) showed that a code designed for a Gaussian source would yield essentially the same performance when applied to another process with the same covariance structure. These results are essentially Shannon theory and hence should be viewed as primarily of interest for high-dimensional quantizers.
Reference: [455] <author> D. J. Sakrison, </author> <title> "Addendum to `The rate distortion function of a Gaussian process with a weighted-square error criterion,' </title> " <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 15, </volume> <pages> pp. 610-611, </pages> <month> Sept. </month> <year> 1969. </year>
Reference-contexts: Sakrison extended the extremal properties of the rate distortion functions to sources with memory [453], [454], <ref> [455] </ref> and Lapidoth [306] (1997) showed that a code designed for a Gaussian source would yield essentially the same performance when applied to another process with the same covariance structure. These results are essentially Shannon theory and hence should be viewed as primarily of interest for high-dimensional quantizers.
Reference: [456] <author> D. J. Sakrison, </author> <title> "Worst sources and robust codes for difference distortion measures," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 21, </volume> <pages> pp. 301-309, </pages> <month> May </month> <year> 1975. </year>
Reference-contexts: Gaussian Quantizers Shannon [465] showed that a Gaussian i.i.d. source had the worst rate-distortion function of any i.i.d. source with the same variance, thereby showing that the Gaussian source was an extremum in a source coding sense. It was long assumed and eventually proved by Sakrison in 1975 <ref> [456] </ref> that this provided a robust approach to quantization in the sense there exist vector quantizers designed for the i.i.d. Gaussian source with a given average distortion which will provide no worse distortion when applied to any i.i.d. source with the same variance.
Reference: [457] <author> A. Said and W. Pearlman, </author> <title> "A new, fast, and efficient image codec based on set partitioning in hierarchical trees," </title> <journal> IEEE Trans. Circuits and Systems for Video Technology, </journal> <volume> vol. 6, </volume> <pages> pp. 243-50, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees [315], [466], <ref> [457] </ref>, which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [458] <author> K. </author> <title> Sayood Introduction to Data Compression, </title> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> San Francisco, Calif., </address> <year> 1996. </year>
Reference-contexts: In speech coding they form the basis of ITU-G.721, 722, 723, and 726, and in video coding they form the basis of the interframe coding schemes standardized in the MPEG and H.26X series. Comprehensive discussions may be found in books [265], [374], [196], [424], [50], <ref> [458] </ref> and survey papers [264], [198]. Though decorrelation was an early motivation for predictive quantization, the most common view at present is that the primary role of the predictor is to reduce the variance of the variable to be scalar quantized. <p> For discussions of transform coding for images see [533], [422], [375], [265], [98], [374], [261], [424], [196], [208], [408], [50], <ref> [458] </ref>. More recently transform coding has also been widely used in high fidelity audio coding [272], [200]. <p> It has been successfully used for video coding [518], [75]. B. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in [196], <ref> [458] </ref>. Lattice Quantizers Lattice quantization can be viewed as a vector generalization of uniform scalar quantization.
Reference: [459] <author> K. Sayood, J. D. Gibson, and M.C. Rost, </author> <title> "An algorithm for uniform vector quantizer design," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 30, </volume> <pages> pp. 805-814, </pages> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: sources. (These assume that Ger-sho's conjecture holds and that the best lattice quantizer is approximately as good as the best tessellation.) Especially important is the fact that their highly structured nature has lead to algorithms for implementing their lossy encoders with very low arithmetic and stoarge complexities [103], [104], [105], <ref> [459] </ref>, [106], [199]. These find the integers m i associated with the closest lattice point. Con-way and Sloane [104], [106] have reported the best known lattices for several dimensions, as well as fast quantizing and decoding algorithms.
Reference: [460] <author> T. Schmidl, P. C. Cosman, and R. M.Gray, </author> <title> "Unbalanced non-binary tree-structured vector quantization," </title> <booktitle> Proc. Twenty-seventh Asilomar Conf. on Signals, Systems, & Computers, </booktitle> <address> Pacific Grove, CA, </address> <pages> pp. 1519-1523, </pages> <month> October-November </month> <year> 1993. </year>
Reference-contexts: TSVQs can be generalized to unbalanced trees (with variable depth as opposed to the fixed depth discussed above) [342], [94], [439], [196] and with larger branching factors than two or even variable branching factors <ref> [460] </ref>.
Reference: [461] <author> L. Schuchman, </author> <title> "Dither signals and their effects on quantization noise," </title> <journal> IEEE Trans. Comm., </journal> <volume> Vol.12, </volume> <pages> pp. 162-165, </pages> <month> Dec. </month> <year> 1964. </year>
Reference-contexts: An obvious problem is the need for the decoder to possess a copy of the dither signal. Nonsubtractive dithering forms the reproduction as ^ X = q (X n + W n ). The principal theoretical property of nonsubtractive GRAY AND NEUHOFF: QUANTIZATION 49 dithering was developed by Schuchman <ref> [461] </ref>, who showed that the quantizer error e n = X n ~ X n = X n q (X n + W n ) + W n is uniformly distributed on (=2; =2] and is independent of the original input signal X n if and only if the quantizer does
Reference: [462] <author> M. P. Schutzenberger, </author> <title> "On the quantization of finite dimensional messages," </title> <journal> Inform. and Control, </journal> <volume> vol. 1, </volume> <pages> pp. 153-158, </pages> <year> 1958. </year>
Reference-contexts: High-resolution analysis of variable-rate quantization developed in a handful of papers from 1958 to 1968. However, since these papers were widely scattered or unpublished, it was not until 1968 that the situation was well understood in the IEEE community. The first high-resolution analysis was that of Schutzen-berger (1958) <ref> [462] </ref> who showed that the distortion of optimized variable-rate quantization (both scalar and vector) decreases with rate as 2 2R , just as with fixed-rate quantization. <p> The first high resolution approximations for vector quantization were published by Schutzenberger in 1958 <ref> [462] </ref>, who found upper and lower bounds to the least distortion of k-dimensional variable-rate vector quantizers, both of the form K2 2R . Unfortunately, the upper and lower bounds diverge as k increases. <p> The earliest rigorous analysis 11 is contained in Schutzen-berger's 1958 paper <ref> [462] </ref>, which showed that for k-dimensional variable-rate quantization (L = 1), rth power distortion (kx yk r ), and a source with finite differential entropy and E h &lt; 1 for some r 0 &gt; r, there is a K k;r &gt; 0, depending on the source and the dimension, such <p> We will not describe the converse. Zador's 1966 Bell Labs Memorandum [562] reproves these two main results under weaker conditions. The distortion measure is rth power in the general sense, which includes as special cases the narrow sense of the rth power of the Euclidean norm considered by Schutzenberger <ref> [462] </ref>. The requirement on the source density is only that each of its marginals has the property that it is bounded from above by jxj r+* , for some * &gt; 0 and all x of sufficiently large magnitude. <p> Note also that it no longer requires that kf k k k+r be finite. As indicated earlier, Zador's memorandum also derives the asymptotic form of the operational distortion-rate func tion of variable-rate quantization. In other words, it fin ishes what his thesis and Schutzenberger <ref> [462] </ref> started, though he was apparently unaware of the latter.
Reference: [463] <author> T. Senoo and B. Girod, </author> <title> "Vector quantization for entropy coding of image subbands," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 1, </volume> <pages> pp. 526-532, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in [526], [470], [94], [439], [438], <ref> [463] </ref>. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546]. <p> Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], <ref> [463] </ref>, [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [464] <author> C. E. Shannon, </author> <title> "A mathematical theory of communication," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 27, </volume> <pages> pp. 379-423, 623-656, </pages> <year> 1948. </year>
Reference-contexts: Hence, like DPCM, transform coding does a good job of exploiting source memory given that it is a system based on scalar quantization. C. Variable-Rate Quantization Shannon's lossless source coding theory (1948) <ref> [464] </ref> made it clear that assigning equal numbers of bits to all quantization cells is wasteful if the cells have unequal probabilities. Instead, the number of bits produced by the quantizer will, on the average, be reduced if shorter binary codewords are assigned to higher probability cells. <p> In these early decades vector quantization served primarily as a paradigm for exploring fundamental performance limits; it was not yet evident whether it would become a practical coding technique. Shannon's Source Coding Theory In his classic 1948 paper, Shannon <ref> [464] </ref> sketched the idea of the rate of a source as the minimum bit rate required to reconstruct the source to some degree of accuracy as measured by a fidelity criterion such as mean squared error. <p> Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder [565] F. Quantization for Noisy Channels The separation theorem of information theory <ref> [464] </ref>, [180] states that nearly optimal communication of an information source over a noisy channel can be accomplished by separately quantizing or source coding the source and channel coding or error-control coding the resulting encoded source for reliable transmission over a noisy channel.
Reference: [465] <author> C. E. Shannon, </author> <title> "Coding theorems for a discrete source with a fidelity criterion," </title> <booktitle> IRE National Convention Record, </booktitle> <volume> Part 4, </volume> <pages> pages 142-163, </pages> <year> 1959. </year>
Reference-contexts: The sketch was fully developed in his 1959 paper <ref> [465] </ref> for i.i.d. sources, additive measures of distortion, and block source codes, now called vector quantizers. <p> Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. <ref> [465] </ref>, [327], [267], [239], [46], [212], [550], [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function. <p> Recent combinations of TCQ to coding wavelet coefficients [478] have yielded excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard. Gaussian Quantizers Shannon <ref> [465] </ref> showed that a Gaussian i.i.d. source had the worst rate-distortion function of any i.i.d. source with the same variance, thereby showing that the Gaussian source was an extremum in a source coding sense.
Reference: [466] <author> J. Shapiro, </author> <title> "Embedded image coding using zerotrees of wavelet coefficients," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 41, </volume> <pages> pp. 3445-3462, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A major breakthrough in performance and complexity came with the introduction of zerotrees [315], <ref> [466] </ref>, [457], which provided an extremely efficient embedded representation of scalar quantized wavelet coefficients, called embedded ze-rotree wavelet (EZW) coding.
Reference: [467] <author> H. N. Shaver, </author> <title> "Topics in statistical quantization," </title> <type> Technical Report No. 7050-5, </type> <institution> Systems Theory Laboratory, Stanford Electronics Laboratories, Stanford University, Stanford, </institution> <address> CA, </address> <month> May </month> <year> 1965. </year>
Reference-contexts: of the Bennett-style asymptotic GRAY AND NEUHOFF: QUANTIZATION 11 approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., [497], [498], [55], <ref> [467] </ref>, [8]. An excellent summary of the early work is contained in a 1970 paper by Elias [143]. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization.
Reference: [468] <author> W. F. Sheppard, </author> <title> "On the calculation of the most probable values of frequency constants for data arranged according to equidistant divisions of a scale," </title> <journal> Proc. London Math. Soc., </journal> <volume> vol. 24, Pt. 2, </volume> <pages> pp. 353-380, 1898. </pages>
Reference-contexts: A. Asymptotic Distortion As mentioned earlier, the first and most elementary result in high resolution theory is the 2 =12 approximation to the mean squared error of a uniform scalar quantizer with step size <ref> [468] </ref>, [394], [43], which we now derive. GRAY AND NEUHOFF: QUANTIZATION 21 Consider an N -level uniform quantizer q whose levels are y 1 ; : : : ; y N , with y i = y i1 + .
Reference: [469] <author> P. C. Shields, D. L. Neuhoff, L. D. Davisson and F. Ledrappier, </author> <title> "The distortion-rate function for nonergodic sources, </title> " <journal> Annals of Probability, </journal> <volume> vol. 6, No. 1, </volume> <pages> pp. 138-143, </pages> <year> 1978. </year>
Reference-contexts: As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [221], <ref> [469] </ref>, [126], [138], [479]. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (33).
Reference: [470] <author> Y. Shoham and A. Gersho, </author> <title> "Efficient bit allocation for an arbitrary set of quantizers," </title> <journal> IEEE Trans. Acoustics, Speech and Signal Processing vol. </journal> <volume> 36, </volume> <pages> pp. 1445-1453, </pages> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in [526], <ref> [470] </ref>, [94], [439], [438], [463]. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546].
Reference: [471] <author> V. M. Shtein, </author> <title> "On group transmission with frequency division of channels by the pulse-code modulation method," </title> <journal> Telecommunications pp. </journal> <pages> 169-184, </pages> <year> 1959, </year> <title> a translation from Elektrosvyaz, </title> <journal> No. </journal> <volume> 2, </volume> <pages> pp 43-54, </pages> <year> 1959. </year>
Reference-contexts: He also provided a simple counterintuitive example of a symmetric density for which the optimal quantizer was asymmetric. In 1959 Shtein <ref> [471] </ref> added terms representing overload distortion to the 2 =12 formula and to Bennett's integral and used them to optimize uniform and nonuniform quantizers. Unaware of prior work except for Bennett's, he rederived the optimal compressor characteristic and the Panter-Dite formula. <p> Roe [443], Algazi [8] and Wood [539] also used Taylor series. Overload distortion was first explicitly considered in the work of Shtein (1959) <ref> [471] </ref>, who optimized the cell size of uniform scalar quantization using an explicit formula for the overload distortion (as well as 2 =12 for the granular distortion) and while rederiving the Panter-Dite formula, added an overload distortion term.
Reference: [472] <author> D. Slepian, </author> <title> "A class of binary signaling alphabets," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 35, </volume> <pages> pp. 203-234, </pages> <year> 1956. </year>
Reference-contexts: Specifically, we speak of the fixed-rate vector quantizer introduced in 1965 by Dunn [137] for multidimensional i.i.d. Gaussian vectors. He argued that his code was effectively a permutation code as earlier used by Slepian <ref> [472] </ref> for channel coding, in that the reproduction codebook contains only codevectors that are permutations of each other. This leads to a quantizer with reduced (but still fairly large) complexity. Dunn compared numerical computations of the performance of this scheme to the Shannon rate-distortion function.
Reference: [473] <author> D. Slepian, </author> <title> "On delta modulation," </title> <journal> Bell Syst. Tech. J., </journal> <volume> Vol. 51, </volume> <pages> pp. 2101-2136, </pages> <year> 1972. </year>
Reference-contexts: apply Bennett's integral or the Panter-Dite formula directly to the prediction error, the analysis of such feedback quantization systems has proved to be notoriously difficult, with results limited to proofs of stability [191], [281], [284], i.e. asymptotic stationarity, to analyses of distortion via Hermite polynomial expansions for Gaussian processes [124], <ref> [473] </ref>, [17], [346], [241], [262], [156], [189], [190], [367], [368], [369], [293], to analyses of distortion when the source is a Wiener process [163], [346], [240], and to exact solutions of the nonlinear difference equations describing the system and hence to descriptions of the output sequences and their moments, including power
Reference: [474] <author> B. Smith, </author> <title> "Instantaneous companding of quantized signals," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 36, </volume> <pages> pp. 653-709, </pages> <year> 1957. </year>
Reference-contexts: 1 They also indicated that it had been derived earlier by P.R. Aigrain. signal-to-noise ratio is 4.35 dB less than the best possible, or for a fixed distortion D the rate is .72 bits/sample larger than that achievable by the best quantizers. In 1957 Smith <ref> [474] </ref> reexamined companding and PCM. Among other things, he gave somewhat cleaner derivations of Bennett's integral, the optimal compressor function, and the Panter-Dite formula. Also in 1957, Lloyd [330] made an important study of quantization with three main contributions. <p> In the following we review the development of rigorous theory. Many analyses | informal and rigorous | explicitly assume the source has finite range (i.e. a probability distribution with bounded support); so there is no overload distortion to be ignored [43], [405], <ref> [474] </ref>. In some cases the source really does have finite range. In others, for example speech and images, the source samples have infinite range, but the measurement device has finite range. <p> We view that this differs only stylistically from an explicit assumption of finite support, for both approaches ignore overload distortion. However, assuming finite support is, arguably, humbler and mathematically more honest. The earliest quantizer distortion analyses to appear in the open literature [43], [405], <ref> [474] </ref> assumed finite range and used the density-approximately-constant-in-cells assumption. Several papers avoided the latter by using a Taylor series expansion of the source density.
Reference: [475] <author> M. R. Soleymani and S. D. Morgera, </author> <title> "An efficient nearest neighbor search method," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 35, </volume> <pages> pp. 677-679, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], <ref> [475] </ref>, [476], [363], [426], [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [476] <author> M. R. Soleymani and S. D. Morgera, </author> <title> "A fast MMSE encoding algorithm for vector quantization," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 37, </volume> <pages> pp. 656-659, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], [517], [475], <ref> [476] </ref>, [363], [426], [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [477] <author> A. B. Sripad and D. L. Snyder, </author> <title> "A necessary and sufficient condition for quantization errors to be uniform and white," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 25, </volume> <pages> pp. 442-448, </pages> <month> Oct. </month> <year> 1977. </year>
Reference-contexts: Sripad and Snyder <ref> [477] </ref> and Claasen and Jongepier [97] derived conditions under which the quantization error is white in terms of the joint characteristic functions of pairs of samples, two-dimensional analogs of Widrow's [529] condition. <p> It follows from the work of Jayant and Ra-biner [266] and Sripad and Snyder <ref> [477] </ref> (see also [216]) that Schuchman's condition implies that the sequence of quantization errors fe n g is independent. The case of uniform dither remains by far the most widely studied in the literature.
Reference: [478] <author> P. Sriram and M. Marcellin, </author> <title> "Image coding using wavelet transforms and entropy-constrained trellis-coded quantization," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 4, </volume> <pages> pp. 725-733, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], [166], [167], [522], [343], <ref> [478] </ref>, [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance. <p> As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels [505], [506]. Recent combinations of TCQ to coding wavelet coefficients <ref> [478] </ref> have yielded excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [479] <author> Y. Steinberg and S. Verdu, </author> <title> "Simulation of random processes and rate-distortion theory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 42, </volume> <pages> pp. 63-86, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], [549], [127], [126], [282], [283], [138], <ref> [479] </ref>. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions. <p> As such, it applies to sources that are stationary in either the strict sense or some weaker sense, such as asymptotic mean stationarity (cf. [218], p. 16). Though originally derived for ergodic sources, it has been extended to nonergodic sources [221], [469], [126], [138], <ref> [479] </ref>. In contrast, high resolution theory applies, fundamentally, to finite-dimensional random vectors. However, for stationary (or asymptotically stationary) sources, taking limits yields results for random processes. For example, the operational distortion-rate function ffi (R) was found to equal Z (R) in this way; see (33).
Reference: [480] <author> H. Steinhaus, </author> <title> "Sur la division des corp materiels en parties," </title> <journal> Bull. Acad. Polon. Sci., C1. </journal> <volume> III, vol IV, </volume> <pages> 801-804, </pages> <year> 1956. </year>
Reference-contexts: Cox in 1957 [111] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later <ref> [480] </ref>, [159], [561]. B. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable "redundancy" that scalar quantization could not exploit. The term "redundancy" was commonly used in the early days and is still popular in some of the quantization literature. <p> The Earliest Vector Quantization Work Outside of Shannon's sketch of rate distortion theory in 1948, the earliest work with a definite vector quantization flavor appeared in the mathematical and statistical literature. Most important was the remarkable work of Stein-haus in 1956 <ref> [480] </ref>, who considered a problem equivalent to a three-dimensional generalization of scalar quantization with a squared-error distortion measure. Suppose that a mass density m (x) is defined on Euclidean space.
Reference: [481] <author> L. C. Stewart, R. M. Gray, and Y. Linde, </author> <title> "The design of trellis waveform coders," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 30, </volume> <pages> pp. 702-710, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [319], [531], <ref> [481] </ref>, [18], [40].
Reference: [482] <author> R. W. Stroh, </author> <title> "Optimum and adaptive differential pulse code modulation," </title> <type> Ph.D. Dissertation, </type> <institution> Polytechnic Inst., Brooklyn, </institution> <address> N.Y. </address> <year> 1970. </year>
Reference-contexts: usually sufficiently similar in form to that of the source that its operational distortion-rate function is smaller than that of the original source by, approximately, the ratio of the variance of the source to that of the prediction error, a quantity that is often called a prediction gain [350], [396], <ref> [482] </ref>, [397], [265]. Analyses of this form usually claim that under high-resolution conditions the distribution of the prediction error approaches that of the error when predictions are based on past source samples rather than past reproductions.
Reference: [483] <author> P. F. Swaszek, </author> <title> "Uniform spherical coordinate quantization of spherically symmetric sources," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 33, </volume> <pages> pp. 518-521, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], [489], [490], <ref> [483] </ref>, [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], <ref> [483] </ref>, [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [484] <editor> P. F. Swaszek, Ed. Quantization, </editor> <volume> vol. 29, </volume> <booktitle> Benchmark Papers in Electrical Engineering and Computer Science, </booktitle> <publisher> Van Nostrand Reinhold Company,Inc., </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Hierarchical table-lookup vector quantizers provide fixed-rate vector quantizers with minimal computational complexity. Many of the early quantization techniques, results, and applications can be found in original form in Swaszek's 1985 reprint collection on quantization <ref> [484] </ref> and Abut's 1990 IEEE Reprint Collection on Vector Quantization [2]. We close this section with a brief discussion of two specific works which deal with optimizing variable-rate scalar quantizers without additional structure, the problem that leads to the general formulation of optimal quantization in the next section.
Reference: [485] <author> P. Swaszek, </author> <title> "Asymptotic Performance of Dirichlet Rotated Polar Quantizers," </title> <journal> IEEE Trans. Inform. Theory., </journal> <volume> vol. IT-31, </volume> <pages> pp. 537-540, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], [489], [490], [483], <ref> [485] </ref>, [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], [483], <ref> [485] </ref>, [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [486] <author> P. F. Swaszek, </author> <title> "A vector quantizer for the Laplace source," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 37, </volume> <pages> pp. 1355-1365, </pages> <month> Sept. </month> <year> 1991. </year>
Reference: [487] <author> P. F. Swaszek, </author> <title> "Unrestricted multistage vector quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 38, </volume> <pages> pp. 1169-74, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Cell-conditioned two-stage quantizers can be viewed as having a piecewise constant point density of the sort proposed earlier by Kuhlmann and Bucklew [302] as a means of circumventing the fact that optimal vector quantizers cannot be implemented with companders. This approach was further developed by Swaszek in <ref> [487] </ref>. Another scheme for adapting each stage to the previous is called codebook sharing, as introduced by Chan and Gersho [80], [82].
Reference: [488] <author> P .F. Swaszek and T. W. Ku, </author> <title> "Asymptotic performance of unrestricted polar quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 32, </volume> <pages> pp. 330-333, </pages> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], [489], [490], [483], [485], <ref> [488] </ref>, [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Such polar quantizers are called "unrestricted" [530], <ref> [488] </ref>. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], [483], [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. <p> Such polar quantizers are called "unrestricted" [530], <ref> [488] </ref>. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], [483], [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase. Originally, methods were developed specifically for polar quantizers.
Reference: [489] <author> P. F. Swaszek and J. B. Thomas, </author> <title> "Optimal circularly symmetric quantizers," </title> <journal> Franklin Inst. J., </journal> <volume> vol. 313, No. 6, </volume> <pages> pp. 373-384, </pages> <year> 1982. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], <ref> [489] </ref>, [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase.
Reference: [490] <author> P. F. Swaszek and J. B. Thomas, </author> <title> "Multidimensional spherical coordinates quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 29, </volume> <pages> pp. 570-576, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], [530], [489], <ref> [490] </ref>, [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase.
Reference: [491] <author> P. F. Swaszek and J. B. Thomas, </author> <title> "Design of quantizers from histograms," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 32, </volume> <pages> pp. 240-245, </pages> <year> 1984. </year>
Reference-contexts: This can be viewed as a variation on epsilon entropy since the goal is to minimize the maximum distortion. Further results along this line may be found in [37], [275], <ref> [491] </ref>. Because these are minimax results aimed at scalar quantization, these results apply to any rate or dimension. 48 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 D.
Reference: [492] <author> N. Ta, Y. Attikiouzel, and C. Crebbin, </author> <title> "Vector quantization of images using the competitive networks," </title> <booktitle> Proc. Second Austrail-ian Conf. on Neural Networks, </booktitle> <volume> ACNN `91, </volume> <pages> pp. 258-262, </pages> <year> 1991. </year>
Reference-contexts: design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], <ref> [492] </ref>, [337], [65].
Reference: [493] <author> S. C. Tai, C. C. Lai, and Y. C. Lin, </author> <title> "Two fast nearest neighbor searching algorithms for image vector quantization," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 44, </volume> <pages> pp. 1623-1628, </pages> <month> Dec. </month> <year> 1996. </year>
Reference: [494] <author> H .H. Tan and K. Yao, </author> <title> "Evaluation of rate-distortion functions for a class of independent identically distributed sources under an absolute magnitude criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 21,pp. </volume> <pages> 59-64, </pages> <month> January </month> <year> 1975. </year>
Reference-contexts: Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. [46], <ref> [494] </ref>, [560], [217]. For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources.
Reference: [495] <author> D. W. Tank and J. J. </author> <title> Hopfield, "Simple `neural' optimization networks: an A/D converter, signal decision circuit, and a linear programming circuit," </title> <journal> IEEE Trans. Circuits and Systems, </journal> <volume> vol. 33, </volume> <pages> pp. 533-541, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets <ref> [495] </ref>, [301], [492], [337], [65].
Reference: [496] <author> T. Tarpey, L. Li and B. D. Flury, </author> <title> "Principal points and self-consistent points of elliptical distributions," </title> <journal> Annals of Statistics, </journal> <volume> Vol 23, No. 1, </volume> <pages> pp 103-112, </pages> <year> 1995. </year>
Reference-contexts: It is difficult to resist pointing out, however, that in 1990 Lloyd's algorithm was rediscovered in the statistical literature under the name of "principal points," which are distinguished from traditional k-means by the assumption of an absolutely continuous distribution instead of an empirical distribution [171], <ref> [496] </ref>, a formulation included in the VQ formulation for a general distribution. Unfortunately, these works reflect no awareness of the rich quantization literature. Most quantizers today are indeed uniform and scalar, GRAY AND NEUHOFF: QUANTIZATION 17 but are combined with prediction or transforms.
Reference: [497] <author> R. C. Titsworth, </author> <title> "Optimal threshold and level selection for quantizing data," JPL Space Programs Summary 37-23, </title> <booktitle> IV, </booktitle> <pages> pp. 196-200, </pages> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> October </month> <year> 1963. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic GRAY AND NEUHOFF: QUANTIZATION 11 approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., <ref> [497] </ref>, [498], [55], [467], [8]. An excellent summary of the early work is contained in a 1970 paper by Elias [143]. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization.
Reference: [498] <author> R. C. Titsworth, </author> <title> "Asymptotic results for optimum equally spaced quantization of Gaussian data," JPL Space Programs GRAY AND NEUHOFF: QUANTIZATION 61 Summary 37-29, </title> <booktitle> IV, </booktitle> <pages> pp. 242-244, </pages> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> October </month> <year> 1964. </year>
Reference-contexts: Numerous extensions of the Bennett-style asymptotic GRAY AND NEUHOFF: QUANTIZATION 11 approximations and the approximation of r (D) or ffi (R) and the characterizations of properties of optimal high resolution quantization for both fixed- and variable-rate quantization for squared error and other error moments appeared during the 1960's, e.g., [497], <ref> [498] </ref>, [55], [467], [8]. An excellent summary of the early work is contained in a 1970 paper by Elias [143]. We close this section with an important practical observation. The current JPEG and related standards can be viewed as a combination of transform coding and variable-length quantization.
Reference: [499] <author> I. Tokaji and C. W. Barnes, </author> <title> "Roundoff error statistics for a continuous range of multiplier coefficients," </title> <journal> IEEE Trans. Circuits and Systems, </journal> <volume> vol. 34, </volume> <pages> pp. 52-59, </pages> <month> Jan. </month> <year> 1987. </year>
Reference: [500] <author> L. Torres and J. Huhuet, </author> <title> "An improvement on codebook search for vector quantization" IEEE Trans. </title> <journal> Commun., </journal> <volume> vol. 42, </volume> <pages> pp. 208-210, </pages> <address> Feb./Mar./Apr. </address> <year> 1994. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], [334], [146], [532], [423], [415], <ref> [500] </ref>, [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [501] <author> R. E. Totty and G. C. Clark, </author> <title> "Reconstruction error in waveform transmission," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 13, </volume> <pages> pp. 336-338, </pages> <month> Apr. </month> <year> 1967. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], <ref> [501] </ref>, [112], [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [502] <author> A. V. Trushkin, </author> <title> "Optimal bit allocation algorithm for quantizing a random vector," Probl. </title> <journal> Inf. Transmission, </journal> <volume> vol. 17, No. 3, </volume> <pages> pp. 156-161, </pages> <month> July-Sept., </month> <year> 1981. </year> <note> (Translated from Russian.) </note>
Reference: [503] <author> A. V. Trushkin, </author> <title> "Sufficient conditions for uniqueness of a locally optimal quantizer for a class of convex error weighting functions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 187-198, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Bell Laboratories Technical Memos of Lloyd, Newman and Zador along with Berger's extension of the optimality properties of entropy-constrained scalar quantization to rth power distortion measures and his extensive comparison of minimum entropy quantizers and fixed-rate permutation codes [48], generalizations by Trushkin of Fleischer's conditions for uniqueness of local optima <ref> [503] </ref>, results on the asymptotic behavior of Lloyd's algorithm with training sequence size based on the theory of k-means consistency by Pollard [418], two seminal papers on lattice quantization by Conway and Sloane [103], [104], rigorous developments of the Bennett theory for vector quantizers and rth power distortion measures by Bucklew
Reference: [504] <author> M. J. Tsai, J. D. Villasenor, and F. Chen, </author> <title> "Stack-run image coding," </title> <journal> IEEE Trans. Circuits and Systems for Video Technology, </journal> <volume> Vol.6, </volume> <pages> pp. 519-21, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Recently competitive schemes have demonstrated that separate scalar quantization of individual subbands coupled with a sophisticated but low complexity lossless coding algorithm called stack-run coding can provide performance nearly as good as EZW <ref> [504] </ref>. The best wavelet codes tend to use very smart lossless codes, lossless codes which effectively code very large vectors.
Reference: [505] <author> G. Ungerboeck, </author> <title> "Channel coding with multilevel/phase signals," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 55-67, </pages> <month> Jan. </month> <year> 1982. </year>
Reference-contexts: The primary gain resulting is a reduction in encoder complexity for a given level of performance. As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels <ref> [505] </ref>, [506]. Recent combinations of TCQ to coding wavelet coefficients [478] have yielded excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [506] <author> G. Ungerboeck, </author> <title> "Trellis-coded modulation with redundant signal sets," Parts I and II," </title> <journal> IEEE Commun. Magazine, </journal> <volume> vol. 25, </volume> <pages> pp. 5-21, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: As the original trellis encoding systems were motivated by convolutional channel codes with Viterbi decoders, trellis coded quantization was motivated by Ungerboeck's enormously successful coded modulation approach to channel coding for narrowband channels [505], <ref> [506] </ref>. Recent combinations of TCQ to coding wavelet coefficients [478] have yielded excellent performance in image coding applications, winning the JPEG 2000 contest of 1997 and thereby a position as a serious contender for the new standard.
Reference: [507] <editor> J. Vaisey and A. Gersho, </editor> <booktitle> "Simulated annealing and codebook design," </booktitle> <pages> pp. 1176-1179, </pages> <booktitle> Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), </booktitle> <address> New York, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: The Mid 1980's to the Present In the middle to late 1980's a wide variety of vector quantizer design algorithms were developed and tested for speech, images, video, and other signal sources. Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], <ref> [507] </ref>, [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [508] <author> V. A. Vaishampayan, </author> <title> "Design of multiple description scalar quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 39, </volume> <pages> pp. 821-824, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) [6], and Zhang and Berger (1987) [573]. In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate <ref> [508] </ref> and entropy-constrained [509] scalar quantizers for the multiple description problem. High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger [324].
Reference: [509] <author> V. A. Vaishampayan, </author> <title> "Design of entropy-constrained multiple-description scalar quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.40, </volume> <pages> pp. 245-250, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) [6], and Zhang and Berger (1987) [573]. In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained <ref> [509] </ref> scalar quantizers for the multiple description problem. High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger [324]. An alternative approach to multiple description quantization using transform coding has also been considered, e.g., in [38], [211]. I.
Reference: [510] <author> V. A. Vaishampayan and J.-C. </author> <title> Batllo "Asymptotic analysis of multiple description quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 44, </volume> <pages> pp. 278-284, </pages> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained [509] scalar quantizers for the multiple description problem. High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo <ref> [510] </ref> and Linder, Zamir, and Zeger [324]. An alternative approach to multiple description quantization using transform coding has also been considered, e.g., in [38], [211]. I.
Reference: [511] <author> H. Van de Weg, </author> <title> "Quantization noise of a single integration delta modulation system with an N-digit code," </title> <journal> Phillips Res. Rep., </journal> <volume> vol. 8, </volume> <pages> pp. 568-569, </pages> <month> Aug. </month> <year> 1971. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], [300], [237], <ref> [511] </ref>, [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived. <p> In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], [300], [237], <ref> [511] </ref>, [572]. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived.
Reference: [512] <author> J. Vanderkooy and S. P. Lipshitz, </author> <title> "Dither in Digital Audio," </title> <journal> J. Audio Eng. Soc., </journal> <volume> vol. 35, </volume> <pages> pp. 966-975, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright [542] in 1979 and Brinton [54] in 1984 and subsequently extended and refined with a variety of proofs [513], <ref> [512] </ref>, [328], [227].
Reference: [513] <author> J. Vanderkooy and S. P. Lipshitz, </author> <title> "Resolution below the least significant bit in digital systems with dither," </title> <journal> J. Audio Eng. Soc., </journal> <volume> vol. 32, </volume> <pages> pp. 106-113, </pages> <month> Nov. </month> <year> 1984. </year> <title> Correction Ibid., </title> <editor> p. </editor> <volume> 889. </volume>
Reference-contexts: The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright [542] in 1979 and Brinton [54] in 1984 and subsequently extended and refined with a variety of proofs <ref> [513] </ref>, [512], [328], [227].
Reference: [514] <author> R. J. van der Vleuten and J. H. Weber, </author> <title> "Construction and Evaluation of Trellis-Coded Quantizers for Memoryless Sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 41, </volume> <pages> pp. 853-859, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], [166], [167], [522], [343], [478], <ref> [514] </ref>. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [515] <author> M. Vetterli, </author> <title> "Multi-dimensional sub-band coding: some theory and algorithms," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 6, </volume> <pages> pp. 97-112, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [113]. The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli <ref> [515] </ref> and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], [540].
Reference: [516] <author> M. Vetterli and J. Kovacevic, </author> <title> Wavelets and Subband Coding, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: Hence we content ourselves with the mention of a few highlights. The interested reader is referred to the book by Vetterli and Kovacevic on wavelets and subband coding <ref> [516] </ref>. Subband coding was introduced in the context of speech coding in 1976 by Crochiere et al. [113].
Reference: [517] <author> E. Vidal, </author> <title> "An algorithm for finding nearest neighbors in (approximately) constant average time complexity," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 4, </volume> <pages> pp. 145-157, </pages> <year> 1986. </year>
Reference-contexts: In this way the set of potential code-vectors is gradually narrowed. Techniques in this category, with different ways of narrowing the search, may be found in [362], <ref> [517] </ref>, [475], [476], [363], [426], [249], [399], [273], A number of other fast search techniques begin with a "coarse" prequantization with some very low complexity technique. It is called "coarse" because it typically has larger cells than the Voronoi regions of the codebook C that is being searched.
Reference: [518] <author> M. Vishwanath and P. Chou, </author> <title> "Efficient algorithm for hierarchical compression of video," </title> <journal> pp. </journal> <pages> 275-279, </pages> <booktitle> Proc. Int'l Conf. Image Processing, vol. III, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Austin, Texas, </address> <month> November, </month> <year> 1994. </year>
Reference-contexts: Due to the fact that not every bucket contains only one codevector, such techniques, which may be found in [86], [358], [357], <ref> [518] </ref>, [75], [219], do not do a perfect full search. Some quantitative analysis of the increased distortion is given in [356] for a case where the prequantization is a lattice quantizer. <p> The codevectors in C should then be the centroids of these cells. Such techniques have been exploited in [86], [358]. One technique worth particular mention is called hierarchical table lookup VQ [86], <ref> [518] </ref>, [75], [219]. In this case, the prequantizer is itself an unstructured codebook that is searched with a fine prequantizer that is in turn searched with an even finer prequantizer, and so on. Specifically, the first prequantizer uses a high rate scalar quantizer k times. <p> Hence the method is hierarchical. Because each of the quantizers can be implemented entirely with table look up, this method eliminates all arithmetic complexity except memory accesses. It has been successfully used for video coding <ref> [518] </ref>, [75]. B. Structured Quantizers We now turn to quantizers with structured partitions or reproduction codebooks, which in turn lend themselves to fast searching techniques and, in some cases, to greatly reduced storage. Many of these techniques are discussed in [196], [458].
Reference: [519] <author> A. J. Viterbi and J. K. Omura, </author> <title> "Trellis encoding of memory-less discrete-time sources with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 325-332, </pages> <month> May </month> <year> 1974. </year>
Reference-contexts: In the early 1970's the algorithms for tree decoding channel codes were inverted to form tree-encoding algorithms for sources by Jelinek, Anderson, and others [268], [269], [11], [132], [123], [10] Later trellis channel decoding algorithms were modified to trellis-encoding algorithms for sources by Viterbi and Omura <ref> [519] </ref>. While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [319], [531], [481], [18], [40].
Reference: [520] <author> A. G. Vitushkin, </author> <title> Theory of the Transmission and Processing of Information, </title> <publisher> Pergaman Press, </publisher> <address> New York, </address> <year> 1961. </year> <title> Translation by Ruth Feinstein of Otsenka Slozhnosti Zadachi Tabulirovaniya, </title> <address> Fizmatgiz, Moscow, </address> <year> 1959. </year>
Reference-contexts: Epsilon entropy extends to function approximation theory with a slight change by removing the notion of probability. Here the epsilon entropy becomes the log of the smallest number of balls of radius * required to cover a compact metric space (e.g., a function space) (see, e.g., <ref> [520] </ref> [420] for a discussion of various notions of epsilon entropy). We mention epsilon entropy because of its close mathematical connection to rate distortion theory. Our emphasis, however, is on codes that minimize average, not maximum, distortion.
Reference: [521] <author> J. C. Walrand and P. Varaiya, </author> <title> "Optimal causal coding-decoding problems," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 19, </volume> <pages> pp. 814-820, </pages> <month> Nov. </month> <year> 1983. </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], [414], [148], [534], [178], [381], <ref> [521] </ref>. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [522] <author> H. S. Wang and N. Moayeri, </author> <title> "Trellis coded vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 40, </volume> <pages> pp. 1273-1276, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Trellis Coded Quantization Trellis coded quantization, both scalar and vector, improves upon traditional trellis encoded systems by labeling the trellis branches with entire subcodebooks (or "subsets") rather than with individual reproduction levels [345], [344], [166], [167], <ref> [522] </ref>, [343], [478], [514]. The primary gain resulting is a reduction in encoder complexity for a given level of performance.
Reference: [523] <author> R. Wang, E.A. Riskin, and R. Ladner, </author> <title> "Codebook organization to enhance maximum a posteriori detection of progressive transmission of vector quantized images over noisy channels," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> Vol.5, </volume> <pages> pp. 37-48, </pages> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The method has also been applied to tree 50 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 structured VQ [412]. It can be combined with a maximum likelihood detector to further improve performance and permit progressive transmission over a noisy channel [411], <ref> [523] </ref>. Simulated annealing has also been used to design such quantizers [140], [152], [354].
Reference: [524] <author> J. Ward, </author> <title> "Hierarchical grouping to optimize an objective function," </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> vol. 37, </volume> <pages> pp. 236-244, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques <ref> [524] </ref>), stochastic relaxation [567], [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [525] <author> G. S. Watson, </author> <title> Statistics on spheres, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Moreover they showed that it becomes Gaussian as the dimension increases. The basic ideas are that as dimension increases good lattices have nearly spherical cells and that a uniform distribution over a high dimensional sphere is approximately Gaussian, cf. <ref> [525] </ref>.
Reference: [526] <author> P. H. Westerink and J. Biemond and D. E. Boekee, </author> <title> "An optimal bit allocation algorithm for sub-band coding," </title> <booktitle> Proc. Intl. Conf. on Acoust. Speech, and Signal Processing, </booktitle> <pages> pp. 757-760, </pages> <year> 1988. </year>
Reference-contexts: We will not delve into the large literature of transforms, but will observe that bit allocation becomes an important issue, and one can either use the high resolution approximations or a variety of nonasymptotic allocation algorithms such as the "fixed-slope" or Pareto-optimality considered in <ref> [526] </ref>, [470], [94], [439], [438], [463]. The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in [546].
Reference: [527] <author> P. H. Westerink, D. E. Boekee, J. Biemond, and J. W. Woods, </author> <title> "Subband coding of images using vector quantization," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 36, </volume> <pages> pp. 713-719, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], <ref> [527] </ref>, [540]. Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [528] <author> B. Widrow, </author> <title> "A study of rough amplitude quantization by means of Nyquist sampling theory," </title> <journal> IRE Trans. Circuit Theory, </journal> <volume> vol. 3, </volume> <pages> pp. 266-276, </pages> <year> 1956. </year>
Reference: [529] <author> B. Widrow, </author> <title> "Statistical analysis of amplitude quantized sampled data systems," </title> <journal> Trans. Amer. Inst. Elec. Eng., </journal> <volume> Pt. </volume> <booktitle> II: Applications and Industry, </booktitle> <volume> vol. 79, </volume> <pages> pp. 555-568, </pages> <year> 1960. </year>
Reference-contexts: In 1960 Max [349] published a variational proof of the Lloyd optimality properties for rth power distortion measures, rediscovered Lloyd's Method II, and numerically investigated the design of fixed-rate quantizers for a variety of input densities. Also in 1960, Widrow <ref> [529] </ref> derived an exact formula for the characteristic function of a uniformly quantized signal when the quantizer has an infinite number of levels. <p> Sripad and Snyder [477] and Claasen and Jongepier [97] derived conditions under which the quantization error is white in terms of the joint characteristic functions of pairs of samples, two-dimensional analogs of Widrow's <ref> [529] </ref> condition. Zador [562] found high resolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers.
Reference: [530] <author> S. G. Wilson, </author> <title> "Magnitude/phase quantization of independent Gaussian variates," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 28, </volume> <pages> pp. 1924-1929, </pages> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Pyramid VQ's are very well suited to i.i.d. Laplacian sources. An efficient method for indexing the shape codevectors is needed and a suitable method is included in pyramid VQ. Two-dimensional shape-gain product quantizers, usually called polar quantizers, have been extensively developed [182], [183], [407], [406], [61], [62], <ref> [530] </ref>, [489], [490], [483], [485], [488], [360]. Here, a two-dimensional source vector is represented in polar coordinates and, in the basic scheme, the codebook consists of the Cartesian product of a nonuniform scalar codebook for the magnitude and a uniform scalar codebook for the phase. <p> Early versions of polar quantization used independent quantization of the magnitude and phase information, but later versions used the better method described above, and some even allowed the phase quantizers to have a resolution that depends on the outcome of the magnitude quantizer. Such polar quantizers are called "unrestricted" <ref> [530] </ref>, [488]. High resolution analysis can be used to study the rate-distortion performance of these quantizers [61], [62], [483], [485], [488], [360]. Among other things, such analyses find the optimal point density for the magnitude quantizer and the optimal bit allocation between magnitude and phase.
Reference: [531] <author> S. G. Wilson and D. W. Lytle, </author> <title> "Trellis encoding of continuous-amplitude memoryless sources," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 211-226, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: While linear encoders sufficed for channel coding, nonlinear decoders were required for the source coding application, and a variety of design algorithms were developed for designing the decoder to populate the trellis searched by the encoder [319], <ref> [531] </ref>, [481], [18], [40].
Reference: [532] <author> A. P. Wilton and G .F. Carpenter, </author> <title> "Fast search methods for vector lookup in vector quantization," </title> <journal> Electron. Lett., </journal> <volume> vol. 28, </volume> <pages> pp. 2311-2312, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Techniques of this type may be found in [44], [176], [88], [89], [334], [146], <ref> [532] </ref>, [423], [415], [500], [84]. In some of these, the coarse prequantization is one-dimensional; for example, the length of the source vector may be quantized, and then the bucket of all codevectors having similar lengths is searched for the closest codevector.
Reference: [533] <author> P. A. Wintz, </author> <title> "Transform picture coding," </title> <journal> Proc. IEEE, </journal> <volume> vol. 60, </volume> <pages> pp. 809-820, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: These codes combine uniform scalar quantization of the transform coefficients with an efficient lossless coding of the quantizer indices, as will be considered in the next section as a variable-rate quantizer. For discussions of transform coding for images see <ref> [533] </ref>, [422], [375], [265], [98], [374], [261], [424], [196], [208], [408], [50], [458]. More recently transform coding has also been widely used in high fidelity audio coding [272], [200].
Reference: [534] <author> H. S. Witsenhausen, </author> <title> "On the structure of real-time source coders," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 58, </volume> <pages> pp. 1437-1451, </pages> <address> Jul.-Aug. </address> <year> 1979 </year>
Reference-contexts: We will return to this issue when we consider finite-state vector quantizers. There has also been work on the optimality of certain causal coding structures somewhat akin to predictive or feedback quantization [331], [414], [148], <ref> [534] </ref>, [178], [381], [521]. Transform coding is the second approach to exploiting redundancy by using scalar quantization with linear preprocessing.
Reference: [535] <author> H. S. Witsenhausen, </author> <title> "Indirect rate-distortion problems," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-26, </volume> <pages> pp. 518-521, </pages> <month> Sept. </month> <year> 1980. </year>
Reference-contexts: This result was subsequently extended to a more general class of distortion measures include the input-weighted quadratic distortion of Ephraim and Gray [145], where a generalized Lloyd algorithm for design was presented. Related results and approaches can be found in Witsen-hausen's (1980) <ref> [535] </ref> treatment of rate distortion theory with modified (or "indirect") distortion measures, and in the Occam filters of Natarajan (1995) [370]. H. Multiple Description Quantization A topic closely related to quantization for noisy channels is multiple description quantization.
Reference: [536] <author> J. K. Wolf, A. D. Wyner, and J. Ziv, </author> <title> "Source coding for multiple descriptions," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 59, </volume> <pages> pp. 1417-1426, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: This problem was first tackled in the information theory community in 1980 by Wolf, Wyner, and Ziv <ref> [536] </ref> and Ozarow [401] who de GRAY AND NEUHOFF: QUANTIZATION 51 veloped achievable rate regions and lower bounds to performance. The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) [6], and Zhang and Berger (1987) [573].
Reference: [537] <author> J.K. Wolf and J. Ziv, </author> <title> "Transmission of noisy information to a noisy receiver with minimum distortion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 16, </volume> <pages> pp. 406-411, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: Berger (1971) [46] explicitly used the modified distortion in his study of Shannon source coding theorems for noise corrupted sources. In 1970 Wolf and Ziv <ref> [537] </ref> used the modified distortion measure for a squared-error distortion to prove that the optimal quantizer for the modified distortion could be decomposed into the cascade of a minimum mean-squared error estimator followed by an optimal quantizer for the estimated original source.
Reference: [538] <author> D. Wong, B.-H. Juang, A. H. Gray, Jr., </author> <title> "An 800 bit/s vector quantization LPC vocoder," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 30, </volume> <pages> pp. 770-779, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: The result was an 800 bits per second LPC speech coder with intelligible quality comparable to that of scalar quantized LPC speech coders of four times the rate. (See also <ref> [538] </ref>.) In the same year Adoul, Debray, and Dalle [4] also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki [551].
Reference: [539] <author> R. C. </author> <title> Wood "On optimal quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 5, </volume> <pages> pp. 248-252, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: Though uniform quantization is asymptotically best for entropy-constrained quantization, at lower rates nonuniform quantization can do better, and a series of papers explored algorithms for designing them. In 1969 Wood <ref> [539] </ref> provided a numerical descent algorithm for designing an entropy-constrained scalar quantizer, and showed, as predicted by Gish and Pierce, that the performance was only slightly superior to a uniform scalar quantizer followed by a lossless code. <p> For example, Lloyd [330] used this approach to show that, ignoring overload distortion, the approximation error in the Panter-Dite formula is o (1=N 2 ), which means that it tends to zero, even when multiplied by N 2 . Roe [443], Algazi [8] and Wood <ref> [539] </ref> also used Taylor series.
Reference: [540] <author> J. W. Woods, Ed., </author> <title> Subband Image Coding, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. [541], [527], <ref> [540] </ref>. Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [541] <author> J. W. Woods and S. D. O'Neil, </author> <title> "Subband coding of images," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 34, </volume> <pages> pp. 1278-1288, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: The extension of subband filtering from 1-D to 2-D was made by Vet GRAY AND NEUHOFF: QUANTIZATION 43 terli [515] and 2-D subband filtering was first applied to image coding by Woods et al. <ref> [541] </ref>, [527], [540]. Early wavelet coding techniques emphasized scalar or lattice vector quantization [12], [13], [130], [463], [14], [30], [185] and other vector quantization techniques have also been applied to wavelet coefficients, including tree encoding [366], residual vector quantization [295], and other methods [107].
Reference: [542] <author> N. Wright, </author> <title> Unpublished work. </title>
Reference-contexts: For example, it can make the perceived quantization noise energy constant as an input signal fades from high intensity to low intensity, where otherwise it can (and does) exhibit strongly signal-dependent behavior. The properties of nonsubtrac-tive dither were originally developed in unpublished work by Wright <ref> [542] </ref> in 1979 and Brinton [54] in 1984 and subsequently extended and refined with a variety of proofs [513], [512], [328], [227].
Reference: [543] <author> H.-S. Wu and J. Barba, </author> <title> "Index allocation in vector quantisation for noisy channels," </title> <journal> Electronics Letters, </journal> <volume> vol. 29, </volume> <pages> pp. </pages> <address> -1318-1320, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: DeMarca and Jayant in 1987 [121] introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho [568], who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include [210], <ref> [543] </ref>, [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], [233], [352].
Reference: [544] <author> L. Wu and F. Fallside, </author> <title> "On the design of connectionist vector quantizers," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 5, </volume> <pages> pp. 207-229, </pages> <year> 1991. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], <ref> [544] </ref>, [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [545] <author> L. Wu and F. Fallside, </author> <title> "Source coding and vector quantization with codebook-excited neural networks," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 6, </volume> <pages> pp. 43-276, </pages> <year> 1992. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], [571], self organizing feature maps [290], [544], <ref> [545] </ref> and other neural nets [495], [301], [492], [337], [65].
Reference: [546] <author> X. Wu, </author> <title> "Globally optimum bit allocation," </title> <booktitle> Proc. Data Compression Conf., Snowbird, Utah, </booktitle> <pages> pp. 22-31, </pages> <year> 1993. </year>
Reference-contexts: The method involves operating all quantizers at points on their operational distortion-rate curves of equal slopes. For a survey of some of these methods, see [107] or Chapter 10 of [196]. A combinatorial optimization method is given in <ref> [546] </ref>. As a final comment on traditional transform coding, the code can be considered as being suboptimal as a k-dimensional quantizer because of the constrained structure (transform and product code).
Reference: [547] <author> X. Wu and L. Guan, </author> <title> "Acceleration of the LBG algorithm," </title> <journal> IEEE Trans. Commun., </journal> <volume> vol. 42, </volume> <pages> pp. 1518-1523, </pages> <address> Feb./Mar./Apr. </address> <year> 1994. </year>
Reference: [548] <author> A.D. Wyner, </author> <title> "C ommunication of analog data from a Gaussian source over a noisy channel," </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 47, </volume> <pages> pp. 801-812, </pages> <month> May-June </month> <year> 1968. </year>
Reference-contexts: GRAY AND NEUHOFF: QUANTIZATION 35 Fig. 7. Signal-to-noise ratios for optimal VQs (dots) and predictions thereof based on the Zador-Gersho formula (straight lines). The convergence rate of ffi k (R) to ffi (R) as k tends to infinity has also been studied [413], <ref> [548] </ref>, [321], [576]. Roughly speaking these results show that for memoryless sources the convergence rate is between q k and log k k .
Reference: [549] <author> A. D. Wyner, </author> <title> "Recent results in the Shannon theory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 20, </volume> <pages> pp. 2-10, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: This is not usually the case for other sources. Shannon's approach was subsequently generalized to sources with memory, cf. [180], [45], [46], [218], <ref> [549] </ref>, [127], [126], [282], [283], [138], [479]. The general definitions of distortion-rate and rate-distortion functions resemble those for operational distortion-rate and rate-distortion functions in that they are infima of kth-order functions.
Reference: [550] <author> A. D. Wyner and J. Ziv, </author> <title> "Bounds on the rate-distortion function for stationary sources with memory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 17, </volume> <pages> pp. 508-513, </pages> <month> Sept. </month> <year> 1971. </year>
Reference-contexts: Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], [239], [46], [212], <ref> [550] </ref>, [559], [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [551] <author> Y. Yamada and K. Fujita and S. Tazaki, </author> <title> "Vector quantization of video signals," </title> <booktitle> Proc. Annual Conf. of IECE, </booktitle> <pages> pp. 1031, </pages> <year> 1980. </year>
Reference-contexts: quantized LPC speech coders of four times the rate. (See also [538].) In the same year Adoul, Debray, and Dalle [4] also used a spectral distance measure to optimize predictors for DPCM and the first thorough study of vector quantization for image compression was published by Yamada, Fujita, and Tazaki <ref> [551] </ref>. In hindsight, the surprising effectiveness of low dimensional VQ, e.g. k = 2 to 10, can be explained by the fact that in Shannon's theory large dimension is needed to attain performance arbitrarily close to the ideal.
Reference: [552] <author> Y. Yamada and S. Tazaki, </author> <title> "Vector quantizer design for video signals," </title> <journal> IECE Trans., </journal> <volume> vol. J66-B, </volume> <pages> pp. 965-972, </pages> <year> 1983. </year>
Reference: [553] <author> Y. Yamada and S. Tazaki, </author> <title> "Recursive vector quantization 62 IEEE TRANSACTIONS ON INFORMATION THEORY, </title> <journal> VOL. </journal> <volume> 44, NO. </volume> <month> 6, </month> <title> OCTOBER 1998 for monochrome video signals," </title> <journal> IEICE Transactions, </journal> <volume> Vol.E74, </volume> <pages> pp. 399-405, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Tree-structured, multistage and hierarchical quantizers, to be discussed in the next section, are examples of such. Other methods can be used to design progressive indexing into given codebooks, as in Yamada and Tazaki (1991) <ref> [553] </ref> and Riskin et al. (1994) [440] Successive approximation is useful in situations where the decoder needs to produce rough approximations of the data from the first bits it receives and, subsequently, to refine the approximation as more bits are received.
Reference: [554] <author> Y. Yamada and S. Tazaki and R. M. Gray, </author> <title> "Asymptotic performance of block quantizers with a difference distortion measure," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 26, </volume> <pages> pp. 6-14, </pages> <month> Jan. </month> <year> 1980. </year>
Reference-contexts: Portions of this work were extended to nondecreasing functions of norms in <ref> [554] </ref>.
Reference: [555] <author> Y. Yamaguchi and T. S. Huang, </author> <title> "Optimum fixed-length binary code," </title> <journal> Quarterly Progress Rept. </journal> <volume> 78, </volume> <publisher> M.I.T. Res. Lab. Electron., </publisher> <pages> pp. 231-233, </pages> <month> July 15, </month> <year> 1965. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained <ref> [555] </ref>, [556], [250], [501], [112], [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [556] <author> Y. Yamaguchi and T. S. Huang, </author> <title> "Optimum binary code," </title> <journal> Quarterly Progress Rept. </journal> <volume> 78, </volume> <publisher> M.I.T. Res. Lab. Electron., </publisher> <pages> pp. 214-217, </pages> <month> July 25, </month> <year> 1965. </year>
Reference-contexts: Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], <ref> [556] </ref>, [250], [501], [112], [351], [42], [232], [233], [352]. For example, it was shown by Crimmins et al. in 1969 [112] that the index assignment that minimizes mean squared error for a uniform scalar quantizer used on a binary symmetric channel is the natural binary assignment.
Reference: [557] <author> H. Yamamoto, </author> <title> "Source coding theory for cascade and branching communication systems," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 27, </volume> <pages> pp. 299-308, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: An important question is whether the performance of a successive refinement quantizer will be better than one that does quantization in one step. On the one hand, rate distortion theory analysis [228], [291], [292], <ref> [557] </ref>, [147], [437], [96] has shown that there are situations where successive approximation can be done without loss of optimality.
Reference: [558] <author> E. Yang, Z. Zhang, T. Berger, </author> <title> "Fixed-slope universal lossy data compression," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.43, </volume> <pages> pp. 1465-1476, </pages> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. <ref> [558] </ref>. A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu [329], [575], [574].
Reference: [559] <author> K. Yao and H. H. Tan, </author> <title> "Some comments on the generalized Shannon lower bound for stationary finite-alphabet sources with memory," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 19, </volume> <pages> pp. 815-817, </pages> <month> November </month> <year> 1973. </year>
Reference-contexts: Due to the difficulty of computing it, many (mostly lower) bounds to the Shannon distortion-rate function have been developed which for reasonably general cases yield the distortion-rate function exactly for a region of small distortion (cf. [465], [327], [267], [239], [46], [212], [550], <ref> [559] </ref>, [217]). An important upper bound derives from the fact that with respect to squared error, the Gaussian source has the largest Shan-non distortion-rate function (kth-order or in the limit) of any source with the same covariance function.
Reference: [560] <author> K. Yao and H. H. Tan, </author> <title> "Absolute error rate-distortion functions for sources with constrained magnitudes," </title> <journal> IEEE Trans. Inform. Theory vol. </journal> <volume> 24, </volume> <pages> pp. 499-503, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Computability First-order Shannon distortion-rate functions can be computed analytically for squared error and magnitude error and several source densites, such as Gaussian and Laplacian, and for some discrete sources, cf. [46], [494], <ref> [560] </ref>, [217]. For other sources it can be computed with Blahut's algorithm [52]. And in the case of squared error, it can be computed with simpler algorithms [168], [444]. For sources with memory, complete analytical formulas for kth-order distortion-rate functions are known only for Gaussian sources.
Reference: [561] <author> P. L. Zador, </author> <title> "Development and evaluation of procedures for quantizing multivariate distributions," </title> <type> Ph.D. Dissertation, </type> <institution> Stan-ford University, 1963. (Also Stanford University Department of Statistics Technical Report.) </institution>
Reference-contexts: of a sufficient condition (namely, that the log of the source density be concave) in order that the optimal quantizer be the only locally optimal quantizer, and consequently, that Lloyd's Method I yields a globally optimal quantizer. (The condition is satisfied for common densities such as Gaussian and Laplacian.) Zador <ref> [561] </ref> had referred to Lloyd a year earlier in his Ph.D. thesis, to be discussed later. <p> Cox in 1957 [111] also derived similar conditions. Some additional early work, which can now be seen as relating to vector quantization, will be reviewed later [480], [159], <ref> [561] </ref>. B. Scalar Quantization with Memory It was recognized early that common sources such as speech and images had considerable "redundancy" that scalar quantization could not exploit. The term "redundancy" was commonly used in the early days and is still popular in some of the quantization literature. <p> Unfortunately, the upper and lower bounds diverge as k increases. In 1963 Zador <ref> [561] </ref> made a very large advance by using high-resolution methods to show that for large rates, the operational distortion-rate function of fixed-rate quantization has the form ffi k (R) = b k jjfjj k k+2 where b k is a term that is independent of the source, f (x) is the <p> Zador's factor fi k tends to be smaller for source densities that are more "compact" (lighter tails and more uniform) and have more dependence among the source variables. Fortunately, high resolution theory need not rely solely on Gersho's conjecture, because Zador's dissertation <ref> [561] </ref> and subsequent memo [562] showed that for large rate ffi (R) has the form b k fi k oe 2 2 2R , where b k is independent of the source distribution. Thus Gersho's conjecture is really just a conjecture about b k . <p> This operational distortion-rate function was also derived by Zador <ref> [561] </ref>, who showed that his unknown factors b k and c k converged to 1 2e . The derivation given here is due to Gersho [193]. Notice that in this limiting case, there is no doubt about the constant M . <p> Clearly, the lower bound is asymptotically achievable by a lattice with hexagonal cells. It follows then that the ratio of ffi 2 (R) to M (hexagon)oe 2 2 2R tends to one, and also, that Gersho's conjecture holds for dimension two. Zador's thesis (1963) <ref> [561] </ref> was the next rigorous work. As mentioned earlier, it contains two principal results. <p> Finally, it is interesting to note that high resolution the GRAY AND NEUHOFF: QUANTIZATION 37 ory actually contains some analyses of the Shannon random coding approach. For example, Zador's thesis <ref> [561] </ref> gives an upper bound on the distortion of a randomly generated vector quantizer. Nature of the Error Process Both theories have something to say about the distribution of quantization errors.
Reference: [562] <author> P. L. Zador, </author> <title> "Topics in the asymptotic quantization of continuous random variables," </title> <institution> Bell Laboratories Technical Memorandum, </institution> <year> 1966. </year>
Reference-contexts: Unfortunately, Koshelev's paper was published in a journal that was not widely circulated. In an unpublished 1966 Bell Telephone Laboratories Tehnical Memo <ref> [562] </ref>, Zador also studied variable-rate (as well as fixed-rate) quantization. As his focus was on vector quantization, his work will be described later. <p> Zador's dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo <ref> [562] </ref> to derive the correct formula. (Curiously, his 1982 paper [563] reports the formula from the thesis rather than the memo.) Again using high-resolution methods, he showed that for large rates, the operational distortion-rate function of variable-rate vector quantization has the form ffi k (R) = c k 2 2h k <p> Moreover, the overall optimal performance for all quantizers of rate less than or equal to R is defined by ffi (R) = inf ffi k (R): (22) Similar definitions hold for the rate vs. distortion and the Lagrangian viewpoints. Using stationarity, it can be shown (cf. <ref> [562] </ref>, [577], [221], Lemma 11.2.3 of [217]) that the operational distortion-rate function is subadditive in the sense that for any positive integers k and l ffi k+l (R) k + l l ffi l (R); (23) which shows the generally decreasing trend of the ffi k (R)'s as k increases. <p> Zador's factor fi k tends to be smaller for source densities that are more "compact" (lighter tails and more uniform) and have more dependence among the source variables. Fortunately, high resolution theory need not rely solely on Gersho's conjecture, because Zador's dissertation [561] and subsequent memo <ref> [562] </ref> showed that for large rate ffi (R) has the form b k fi k oe 2 2 2R , where b k is independent of the source distribution. Thus Gersho's conjecture is really just a conjecture about b k . <p> This can be directly verified using Jensen's inequality [193]. In the case of scalar quantization (k = 1), the optimality of the uniform point density and the operational distortion-rate function ffi 1;L (R) were found by Gish and Pierce (1968) [204]. Zador (1966) <ref> [562] </ref> considered the L = 1 case and showed that ffi k;1 (R) has the form c k fl k oe 2 2 2R when R is large, where c k is a constant that is independent of the source density and no larger than the constant b k that he <p> Zador <ref> [562] </ref> defined a very general rth power distortion measure as any distortion measure of the form d (x; y) = ae (x y) where for any a &gt; 0, ae (ax) = a r ae (jx 1 j; : : : ; jx k j), for some r &gt; 0. <p> The best codes are at least this good and it follows that lim sup k!1 N r k+r easily see how this construction creates codes with essentially optimal point density and cell shape. We will not describe the converse. Zador's 1966 Bell Labs Memorandum <ref> [562] </ref> reproves these two main results under weaker conditions. The distortion measure is rth power in the general sense, which includes as special cases the narrow sense of the rth power of the Euclidean norm considered by Schutzenberger [462]. <p> Sripad and Snyder [477] and Claasen and Jongepier [97] derived conditions under which the quantization error is white in terms of the joint characteristic functions of pairs of samples, two-dimensional analogs of Widrow's [529] condition. Zador <ref> [562] </ref> found high resolution expressions for the characteristic function of the error produced by randomly chosen vector quantizers.
Reference: [563] <author> P. L. Zador, </author> <title> "Asymptotic quantization error of continuous signals and the quantization dimension," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 139-148, </pages> <month> March </month> <year> 1982. </year> <note> (Revised version of [562].) </note>
Reference-contexts: Unfortunately, the results of Zador's thesis were not published until 1982 <ref> [563] </ref> and were little known outside of Bell Laboratories until 14 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 Gersho's important paper of 1979 [193], to be described later. <p> Zador's dissertation also dealt with the analysis of variable-rate vector quantization, but the asymptotic formula given there is not the correct one. Rather it was left to his subsequent unpublished 1966 memo [562] to derive the correct formula. (Curiously, his 1982 paper <ref> [563] </ref> reports the formula from the thesis rather than the memo.) Again using high-resolution methods, he showed that for large rates, the operational distortion-rate function of variable-rate vector quantization has the form ffi k (R) = c k 2 2h k (X) 2 2R ; (20) where c k is a
Reference: [564] <author> R. Zamir and M. Feder, </author> <title> "On lattice quantization noise," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 1152-9, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Though it has long been presumed, only recently has it been directly shown that the M k 's tend to 1 2e as k increases (Zamir and Feder <ref> [564] </ref>), which is the limit of the normalized moment of inertia of k-dimensional spheres as k tends to infinity. Previously, 4 A cell T "tessellates" if there exists a partition of &lt; k whose cells are, entirely, translations and rotations of T . <p> As a side benefit, these expressions indicate that much can be deduced about the point density and cell shapes of a quantizer from a histogram of the lengths of the errors. Zamir and Feder <ref> [564] </ref> showed that the error produced by an optimal lattice quantizer with infinitely many small cells is asymptotically white in the sense that its components are uncorrelated with zero means and identical variances. Moreover they showed that it becomes Gaussian as the dimension increases.
Reference: [565] <author> R. Zamir and M. Feder, </author> <title> "Information rates of pre/post-filtered dithered quantizers," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 1340-53, </pages> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: For example, Ziv [578] showed that even without high resolution theory, uniform scalar quantization combined with dithering and vector lossless coding could yield performance within .75 bits/symbol of the rate-distortion function. Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder <ref> [565] </ref> F.
Reference: [566] <author> K. Zeger, A. Bist, and T. Linder, </author> <title> "Universal source coding with codebook transmission," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 42, </volume> <pages> pp. 336-346, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The Rice machine, however, proved the practicality and importance of a simple multiple codebook scheme for handling composite sources. These works all assumed the encoder and decoder to possess copies of the codebooks being used. Zeger, Bist, and Linder <ref> [566] </ref> considered systems where the codebooks are designed at the encoder, but must be also coded and transmitted to the decoder, as is commonly done in codebook replenishment [206]. A good review of the history of universal source coding through the early 1990s may be found in Kieffer (1993) [283].
Reference: [567] <author> K. Zeger and A. Gersho, </author> <title> "A stochastic relaxation algorithm for improved vector quantiser design," </title> <journal> Electronics Letters, </journal> <volume> Vol 25, </volume> <pages> pp. 896-898, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation <ref> [567] </ref>, [571], self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [568] <author> K. Zeger and A. Gersho, </author> <title> "Pseudo-Gray coding," </title> <journal> IEEE Trans. Comm., </journal> <volume> vol. 38, </volume> <pages> pp. 2147-2156, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several specific index assignment methods were considered by Rydbeck and Sundberg [448]. DeMarca and Jayant in 1987 [121] introduced an iterative search algorithm for designing index assignments for scalar quantizers, which was extended to vector quantization by Zeger and Gersho <ref> [568] </ref>, who dubbed the approach "pseudo-Gray" coding. Other index assignment algorithms include [210], [543], [287]. For binary symmetric channels and certain special sources and quantizers, analytical results have been obtained [555], [556], [250], [501], [112], [351], [42], [232], [233], [352].
Reference: [569] <author> K. Zeger and M. R. Kantorovitz, </author> <title> "Average number of facets per cell in tree-structured vector quantizer partitions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.39, </volume> <pages> pp. 1053-1055, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: A paper investigating the nature of TSVQ cells is <ref> [569] </ref>. Our experience has been that when taking both performance and complexity into account, TSVQ is a very competitive VQ method.
Reference: [570] <author> K. Zeger and V. Manzella, </author> <title> "Asymptotic bounds on optimal noisy channel quantization via Random Coding," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1926-1938, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Clearly, there is an optimal choice of quantization rate. Another issue is the determination of the rate at which overall distortion decreases in an optimal system as the total number of channel uses per source symbol increases. These issues have been addressed in recent papers by Zeger and Manzella <ref> [570] </ref> and Hochwald and Zeger [244], which use both exponential formulas produced by high resolution quantization theory and exponential bounds to channel coding error probability.
Reference: [571] <author> K. Zeger, J. Vaisey and A. Gersho, </author> <title> "Globally optimal vector quantizer design by stochastic relaxation," </title> <journal> IEEE Trans. Signal Process., </journal> <volume> vol. 40, </volume> <pages> pp. 310-322, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Some of the quantizer design algorithms developed as alternatives to Lloyd's algorithm include simulated annealing [140], [507], [169], [289], deterministic annealing [445], [446], [447], pairwise nearest neighbor [146] (which had its origins in earlier clustering techniques [524]), stochastic relaxation [567], <ref> [571] </ref>, self organizing feature maps [290], [544], [545] and other neural nets [495], [301], [492], [337], [65].
Reference: [572] <author> L. H. Zetterberg, </author> <title> "A comparison between delta and pulse code modulation," </title> <journal> Ericsson Technics, </journal> <volume> vol. 11, No. 1, </volume> <pages> pp. 95-154, </pages> <year> 1955. </year>
Reference-contexts: In 1950 Elias [141] provided an information theoretic development of the benefits of predictive coding, but the work was not published until 1955 [142]. Other early references include [395], [300], [237], [511], <ref> [572] </ref>. In particular, [511] claims Bennett-style asymptotics for high resolution quantization error, but as will be discussed later such approximations have yet to be rigorously derived.
Reference: [573] <author> Z. Zhang and T. Berger, </author> <title> "New results in binary multiple descriptions," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 33, </volume> <pages> pp. 502-521, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The results were extended by El Gamal and Cover (1982) [139], Ahlswede (1985) [6], and Zhang and Berger (1987) <ref> [573] </ref>. In 1993 Vaishampayan et al. used a Lloyd algorithm to actually design fixed rate [508] and entropy-constrained [509] scalar quantizers for the multiple description problem. High resolution quantization ideas were used to evaluate achievable performance in 1998 by Vaisham-payan and Batllo [510] and Linder, Zamir, and Zeger [324].
Reference: [574] <author> Z. Zhang and V. K. Wei, </author> <title> "An on-line universal lossy data compression algorithm via continuous codebook refinement. I. Basic results," </title> <journal> IEEE Trans.Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 803-821, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. [558]. A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu [329], [575], <ref> [574] </ref>. Their approach, dubbed "gold washing" did not involve training, but rather created and removed codevectors according to the data received and an auxiliary random process in a way that could be tracked by a decoder without side information. E.
Reference: [575] <author> Z. Zhang and E. Yang, </author> <title> "An on-line universal lossy data compression algorithm via continuous codebook refinement. II. Optimality for phi-mixing source models," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol.42, </volume> <pages> pp. 822-836, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The fixed-slope universal quantizer approach was further developed with other code structures and design algorithms by Yang et al. [558]. A different approach which more closely resembles traditional adaptive and codebook replenishment was developed by Zhang, Yang, Wei, and Liu [329], <ref> [575] </ref>, [574]. Their approach, dubbed "gold washing" did not involve training, but rather created and removed codevectors according to the data received and an auxiliary random process in a way that could be tracked by a decoder without side information. E.
Reference: [576] <author> Z. Zhang, E.-H. Yang and V. K. Wei, </author> <title> "The redundancy of source coding with a fidelity criterion | Part One: </title> <journal> Known Statistics," IEEE Trans. Inform. Theory, </journal> <volume> vol. 43, </volume> <pages> pp. 71-91, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: GRAY AND NEUHOFF: QUANTIZATION 35 Fig. 7. Signal-to-noise ratios for optimal VQs (dots) and predictions thereof based on the Zador-Gersho formula (straight lines). The convergence rate of ffi k (R) to ffi (R) as k tends to infinity has also been studied [413], [548], [321], <ref> [576] </ref>. Roughly speaking these results show that for memoryless sources the convergence rate is between q k and log k k .
Reference: [577] <author> J. Ziv, </author> <title> "Coding sources with unknown statistics- Part II: Distortion relative to a fidelity criterion," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 18, </volume> <pages> pp. 389-394, </pages> <month> May </month> <year> 1972. </year>
Reference-contexts: Moreover, the overall optimal performance for all quantizers of rate less than or equal to R is defined by ffi (R) = inf ffi k (R): (22) Similar definitions hold for the rate vs. distortion and the Lagrangian viewpoints. Using stationarity, it can be shown (cf. [562], <ref> [577] </ref>, [221], Lemma 11.2.3 of [217]) that the operational distortion-rate function is subadditive in the sense that for any positive integers k and l ffi k+l (R) k + l l ffi l (R); (23) which shows the generally decreasing trend of the ffi k (R)'s as k increases. <p> The classic work on lossy universal source codes was Ziv's 1972 paper <ref> [577] </ref>, which proved the existence of fixed-rate universal lossy codes under certain assumptions on the source statistics and the source and codebook alphabets.
Reference: [578] <author> J. Ziv, </author> <title> "Universal Quantization," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. 31, </volume> <pages> pp. 344-347, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: quantization with variable-length coding of k successive quantizer outputs (block entropy coding ) achieves performance that is 1.53 dB (0.255 bits/sample) from ffi (R), even for sources with memory. (They accomplished this by comparing to Shannon lower bounds.) This important result was not widely appreciated until rediscovered by Ziv (1985) <ref> [578] </ref>, who also showed that a similar result holds for small rates. Note that although uniform scalar quantizers are quite simple, the lossless code capable of approaching the kth-order entropy of the quantized source can be quite complicated. <p> Berger [47] showed that permutation codes achieved roughly the same performance with a fixed-rate vector quantizer. Ziv <ref> [578] </ref> showed in 1985 that if subtractive dithering is allowed, dithered uniform quantization followed by block lossless encoding will be at most .754 bits worse than the optimal entropy constrained vector quantizer with the same block size, even if the rate is not high. (Subtractive dithering, as will be discussed later, <p> In addition to its role in whitening quantization noise and making the noise or its moments independent of the input, dithering has played a role in proofs of "universal quantization" results in information theory. For example, Ziv <ref> [578] </ref> showed that even without high resolution theory, uniform scalar quantization combined with dithering and vector lossless coding could yield performance within .75 bits/symbol of the rate-distortion function. Extensions to lattice quantization and variations of this result have been developed by Zamir and Feder [565] F.
Reference: [579] <author> V. N. Koshelev, </author> <title> "Quantization with minimal entropy," Probl. </title> <journal> Pered. Inform., </journal> <volume> no. 14, </volume> <pages> pp. 151-156, </pages> <year> 1963. </year>
Reference-contexts: small cell width has output entropy given approximately by H (q (X)) = h (X) log (11) where h (X) = f (x) log f (x)dx is the differential entropy of the source variable X." 10 IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 44, NO. 6, OCTOBER 1998 In 1963, Koshelev <ref> [579] </ref> discovered the very interesting fact that in the high-resolution case, the mean-squared error of uniform scalar quantization exceeds that of the least distortion achievable by any quantization scheme whatsoever, i.e. ffi (R), by a factor of only e=6 = 1:42. <p> Next, by comparing to what is called the Shannon lower bound to ffi (R), they showed that for i.i.d. sources, the latter is only 1.53 dB (0.255 bits/sample) from the best possible performance ffi (R) of any quantization system whatsoever, which is what Koshelev <ref> [579] </ref> found earlier. Their results showed that such good performance was attainable for any source distribution, not just the Gaussian case checked by Goblick and Holsinger. They also generalized the results from squared-error distortion to nondecreasing functions of magnitude error. <p> The first to explicitly apply Shannon's source coding theory to the problem of analog-to-digital conversion combined with digital transmission appear to be Gob-lick and Holsinger [205] in 1967, and the first to make explicit comparisons of quantizer performance to Shannon's rate-distortion function was Koshelev <ref> [579] </ref>. A distinct variation on the Shannon approach was introduced to the English literature in 1956 by Kol-mogorov [288], who described several results by Russian information theorists inspired by Shannon's 1948 treatment of coding with respect to a fidelity criterion.
Reference: [580] <author> V. F. Babkin, M. M. Lange, and Yu. M. Shtarkov, </author> <title> "About fixed rate lattice coding of sources with difference fidelity criterion," </title> <booktitle> Voprosi Kibernetikia, Problems of Redundancy in Information Systems, </booktitle> <volume> vol. 34, </volume> <pages> pp. 10-30, </pages> <year> 1977. </year>
Reference-contexts: It also applies to quantizers with finitely many cells for sources with compact support. But it does not apply to quantizers with finitely many cells and sources with infinite support, because it does not deal with the overload region of such quantizers. In 1977 Babkin et al. <ref> [580] </ref> obtained results indicating how rapidly the distortion of fixed-rate lattice quan-tizers approaches ffi (R) as rate R and dimension k increase, for difference distortion measures. In 1978 these same authors [581] studied uniform scalar quantization with variable-rate coding, and extended Koshelev's result to r-th power distortion measures. <p> The theory becomes more difficult if, as is usually the case, only a bounded portion of the lattice is used as the code-book and one must separately consider granular and overload distortion. There are a variety of ways of considering the tradeoffs involved, cf. <ref> [580] </ref>, [151], [359], [149], [409]. In any case, the essence of a lattice code is its uniform point density and nicely shaped cells with low normalized moment of inertia. For fixed-rate coding, they work well for uniform sources or other sources with bounded support.

References-found: 580

