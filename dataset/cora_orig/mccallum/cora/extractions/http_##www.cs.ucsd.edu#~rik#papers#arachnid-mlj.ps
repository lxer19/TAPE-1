URL: http://www.cs.ucsd.edu/~rik/papers/arachnid-mlj.ps
Refering-URL: http://www.cs.ucsd.edu/~rik/
Root-URL: http://www.cs.ucsd.edu
Email: ffil,rikg@cs.ucsd.edu  
Title: Adaptive Retrieval Agents: Internalizing Local Context and Scaling up to the Web  
Author: FILIPPO MENCZER AND RICHARD K. BELEW Editor: Jaime Carbonell, Yiming Yang, William Cohen 
Keyword: InfoSpiders, distributed information retrieval, World Wide Web, evolutionary algorithms, local selection, internalization, reinforcement learning, Q-learning, neural networks, relevance feedback, linkage topology, relevance autocorrelation, scalability, context, personalization, localization, selective query expansion, dimensionality reduction, graph search, browsing, on-line, mobile, situated, adaptive agents  
Address: La Jolla, CA 92093-0114 USA  
Affiliation: Computer Science and Engineering Department University of California San Diego  
Note: Machine Learning, 1-45 (1999) c 1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: This paper focuses on two machine learning abstractions springing from ecological models: (i) evolutionary adaptation by local selection, and (ii) selective query expansion by internalization of environmental signals. We first outline a number of experiments pointing to the feasibility and performance of these methods on a general class of graph environments. We then describe how these methods have been applied to the intelligent retrieval of information distributed across networked environments. In particular, the paper discusses a novel distributed evolutionary algorithm and representation used to construct populations of adaptive Web agents. These InfoSpiders search on-line for information relevant to the user, by traversing hyperlinks in an autonomous and intelligent fashion. They can adapt to the spatial and temporal regularities of their local context. Our results suggest that InfoSpiders could complement current search engine technology by starting up where search engines stop. Engines provide global starting points, based on statistical features of the search space (words); InfoSpiders can use topological features (links) to guide their subsequent search on-line. We show how this approach can help overcoming some of the limitations of the current state of the art, dealing with the problems of scalability, personalization, and localization. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R Armstrong, D Freitag, T Joachims, and T Mitchell. Webwatcher: </author> <title> A learning apprentice for the world wide web. </title> <booktitle> In AAAI Spring Symposium Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <year> 1995. </year>
Reference-contexts: We want to allow the inputs and activation values of the network to take negative values, corresponding to the possibly negative correlations perceived between terms and relevance. For this reason the network uses the hyperbolic tangent as its squashing function, with inputs and activation values in <ref> [1; +1] </ref>. Let us now see how the different parts of the system are implemented, based on the this representation. 5.3.1. Action Selection An agent performs action selection by first computing the relevance estimates for each link from the current document. <p> Second, it is computed on-line and therefore uses document frequencies based on the contents of the cache rather than the entire collection. The hyperbolic tangent is used to normalize energy intakes into the appropriate range <ref> [1; +1] </ref> | the same range as the corresponding neural nets prediction. 5.3.3. Q-learning The agent then compares the relevance (assessed or estimated) of the current document with the estimate of the link that led to it. <p> Then the offspring is mutated to provide the evolutionary algorithm with the necessary power of exploration. If a 0 is an offspring of a: fi a 0 U [fi a (1 fi ); fi a (1 + fi )] where fi 2 <ref> [0; 1] </ref> is a parameter and U is the uniform distribution. The values of fi are clipped to fi max to maintain some exploratory behavior. The neural net is mutated by adding random noise to a fraction w of the weights. <p> This was due to a fixed, nonadaptive strategy: a mixture of depth-first-, breadth-first-, and best-first-search, with user-determined depth and breadth cutoff levels. One difficulty of the Fish Search approach was in determining appropriate cutoff levels a priori, possibly resulting in load-unfriendly search behaviors. WebWatcher <ref> [1] </ref> is an agent that learns to mimic the user by looking over his/her shoulder while browsing. Then it performs look-ahead searches and provides the browsing user with suggestions about what links to follow next.
Reference: 2. <author> GO Arocena, AO Mendelzon, and GA Mihaila. </author> <title> Applications of a web query language. </title> <booktitle> In Proc. 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments [39, 54] and incorporated into query formulation to improve searching <ref> [2, 47] </ref>. Fish Search [8] was a search system proposed at the same time as InfoSpiders [35] and inspired by some of the same ideas from artificial life.
Reference: 3. <author> M Balabanovic. </author> <title> An adaptive web page recommendation service. </title> <booktitle> In Proc. 1st International Conference on Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: As with InfoSpiders, these agents learn to predict an objective function on-line; they can also track time-varying user preferences. Unlike InfoSpiders, however, WebWatcher 40 and Letizia are single agents, and more importantly they need supervision from the user in order to work; no autonomous search is possible. Fab <ref> [3] </ref> and Amalthaea [37] are multi-agent adaptive filtering systems inspired by genetic algorithms, artificial life, and market models. Term weighting and relevance feedback are used to adapt a matching between a set of discovery agents (typically search engine parasites) and a set of user profiles (corresponding to single-or multiple-user interests).
Reference: 4. <editor> RK Belew and M Mitchell, editors. </editor> <title> Adaptive Individuals in Evolving Populations: Models and Algorithms. Santa Fe Institute Studies in the Sciences of Complexity. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: The neural net's weights are then updated by back-propagation of error [41]. Learned changes to the weights are "Lamarckian" in that they are inherited by offspring at reproduction (see <ref> [4] </ref> for a treatment of this issue). In the absence of relevance assessments, this reinforcement learning algorithm is unsupervised because it is the environment that provides the reinforcement signal e (D).
Reference: 5. <author> CM Bowman, PB Danzig, U Manber, and MF Schwartz. </author> <title> Scalable internet resource discovery: Research problems and approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <year> 1994. </year>
Reference-contexts: Each experiment, consisting of 10 runs, used a different value of A between 0.1 (very noisy predictions) and 1.0 (perfectly accurate predictions). In each of these runs, fi was initialized with uniform distribution in 17 over agents in the population. Linear regression is also shown. the range <ref> [0; 5] </ref> and measured after 750 node accesses. As Figure 7 shows, the fi values evolved by the population are indeed well correlated with the accuracy of the environmental cues. <p> Other related work The idea of decentralizing the index-building process is not new. Dividing the task into localized indexing, performed by a set of gatherers, and centralized searching, performed by a set of brokers, has been suggested since the early days of the Web by the Harvest project <ref> [5] </ref>. Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments [39, 54] and incorporated into query formulation to improve searching [2, 47].
Reference: 6. <author> S Chakrabarti, B Dom, P Raghavan, S Rajagopalan, D Gibson, and J Kleinberg. </author> <title> Automatic resource compilation by analyzing hyperlink structure and associated text. </title> <booktitle> In Proc. 7th International World Wide Web Conference, </booktitle> <year> 1998. </year>
Reference-contexts: Yet, all fi values are significantly positive. Independent evidence for the value of linkage topology can be found in bibliometric studies of the Web [20] as well as link-based approaches to Web page categorization or discovery <ref> [6] </ref>. If links constitute useful cues for navigation, they can be exploited by autonomous browsing agents just as they are by browsing users | indeed, even the dumbest of agents (random walkers) can exploit linkage information.
Reference: 7. <author> WS Cooper. </author> <title> Expected search length: A single measure of retrieval effectiveness based on weak ordering action of retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 19 </volume> <pages> 30-41, </pages> <year> 1968. </year>
Reference-contexts: Another metric lends itself better to assess the performance of on-line retrieval systems. Search length is normally defined as the number of irrelevant documents that appear, in some ordered list of retrieved documents, in front of some fraction of the relevant set <ref> [7] </ref>. We can easily extend this method by imagining that only visited documents appear in the list of retrieved documents, and that their ordering is given by visit time rather than rank | length then refers to waiting time.
Reference: 8. <author> PME De Bra and RDJ Post. </author> <title> Information retrieval in the world wide web: Making client-based searching feasible. </title> <booktitle> In Proc. 1st International World Wide Web Conference, </booktitle> <address> Geneva, </address> <year> 1994. </year>
Reference-contexts: Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments [39, 54] and incorporated into query formulation to improve searching [2, 47]. Fish Search <ref> [8] </ref> was a search system proposed at the same time as InfoSpiders [35] and inspired by some of the same ideas from artificial life. Fish Search was based on a population of search agents who browsed the Web autonomously, driven by an internally generated energy measure based on relevance estimations.
Reference: 9. <author> KA De Jong. </author> <title> An analysis of the behavior of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year>
Reference-contexts: In the evolutionary algorithm community this behavior is often referred to as niche selection [16, 10, 29]. The most notable selection variations explicitly aimed at niching are crowding <ref> [9] </ref> and fitness sharing [15]. In both of these methods selection is altered to take into account some measure of similarity among individuals, leading to inefficiency; if p is the population size, selection has time complexity O (p) rather than O (1) per individual.
Reference: 10. <author> KA De Jong and J Sarma. </author> <title> On decentralizing selection algorithms. </title> <booktitle> In Proc. 6th International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: In the evolutionary algorithm community this behavior is often referred to as niche selection <ref> [16, 10, 29] </ref>. The most notable selection variations explicitly aimed at niching are crowding [9] and fitness sharing [15].
Reference: 11. <institution> Digital Equipment Corporation. </institution> <note> http://altavista.digital.com. </note>
Reference-contexts: This conjecture is equivalent to the cluster hypothesis [52] under a hypertext derived definition of association. 5. But a retrieved set can be viewed as the relevant set for some query. 6. For Alta Vista, at the time of this writing we estimate n=qt 5 <ref> [11] </ref>; even at a conservative growth rate of a doubling per year, the condition would be met within about 5 years. 7. This list would typically be obtained by consulting a search engine. 8.
Reference: 12. <editor> Encyclopaedia Britannica, </editor> <publisher> Inc. </publisher> <address> http://www.eb.com. </address>
Reference-contexts: Measures of fi for ten queries submitted to Lycos [27] and Britannica Online <ref> [12] </ref>. The minimum score parameter used for Lycos was 0.1. Only full (absolute) HTTP references were considered for Lycos, and only Micropaedia and Macropaedia article references for EB. Multiple-term queries represent AND Boolean searches. <p> A more general difficulty is the lack of queries with available well-defined relevant sets. To overcome this problem, a special chunk of the Web has been selected as a test environment [50, 49]: the Encyclopaedia Britannica (EB) <ref> [12] </ref>. The advantage is that we can make use of readily available relevant sets of articles associated with a large number of queries. 27 Here we use a subset of the EB corpus, corresponding to the "Human Society" topic | roughly one tenth of the whole collection.
Reference: 13. <author> C Fox. </author> <title> Lexical analysis and stop lists. In Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: There is no direct interaction between the user and the agents. The InfoSpiders prototype is written in C and runs on UNIX and MacOS platforms. The Web interface is based on the W3C library [53]. Agents employ standard information retrieval tools such as a filter for noise words <ref> [13] </ref> and a stemmer based 22 on Porter's algorithm [14]. Finally, agents store an efficient representation of visited documents in the shared cache on the client machine. Each document is represented by a list of links and stemmed keywords.
Reference: 14. <author> WB Frakes. </author> <title> Stemming algorithms. In Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: The InfoSpiders prototype is written in C and runs on UNIX and MacOS platforms. The Web interface is based on the W3C library [53]. Agents employ standard information retrieval tools such as a filter for noise words [13] and a stemmer based 22 on Porter's algorithm <ref> [14] </ref>. Finally, agents store an efficient representation of visited documents in the shared cache on the client machine. Each document is represented by a list of links and stemmed keywords. If the cache reaches its size limit, the LRU (least recently used) replacement strategy is used. 5.3.
Reference: 15. <author> DE Goldberg and J Richardson. </author> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <booktitle> In Proc. 2nd International Conference on Genetic Algorithms, </booktitle> <year> 1987. </year>
Reference-contexts: In the evolutionary algorithm community this behavior is often referred to as niche selection [16, 10, 29]. The most notable selection variations explicitly aimed at niching are crowding [9] and fitness sharing <ref> [15] </ref>. In both of these methods selection is altered to take into account some measure of similarity among individuals, leading to inefficiency; if p is the population size, selection has time complexity O (p) rather than O (1) per individual.
Reference: 16. <author> G Harik. </author> <title> Finding multimodal solutions using restricted tournament selection. </title> <booktitle> In Proc. 6th International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: In the evolutionary algorithm community this behavior is often referred to as niche selection <ref> [16, 10, 29] </ref>. The most notable selection variations explicitly aimed at niching are crowding [9] and fitness sharing [15].
Reference: 17. <author> D Harman. </author> <title> Relevance feedback and other query modification techniques. In Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: Feature selection is normally used in information retrieval for query expansion <ref> [43, 17] </ref>. This is also the case for our system, where environmental word features are internalized into agent representations by adaptive mutations. But when space, time, and user needs constitute highly dynamic and heterogeneous contexts, no 39 query expansion is necessarily appropriate everywhere or forever.
Reference: 18. <author> WE Hart and RK Belew. </author> <title> Optimization with genetic algorithm hybrids that use local search. In Adaptive Individuals in Evolving Populations: Models and Algorithms. </title> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Genetically evolved solutions might also reflect an inappropriate coarseness of scale, due to individual agents' incapability to learn during their life. These are the same reasons that have motivated the hybridization of genetic algorithms with local search <ref> [18] </ref>, and reflect the general problem of machine learning techniques in environments with very large feature space dimensionalities. 3.1. <p> Therefore it affords efficiency both in centralized and, especially, distributed tasks. Local selection and reinforcement learning both allow agents to internalize environmental features in an unsupervised fashion [38]. InfoSpiders integrate population-based adaptation and individual-based learning in a seamless way, not to accelerate 38 global optimization <ref> [18] </ref> but to take advantage of environmental signals at different spatial and temporal scales.
Reference: 19. <author> E Koutsoupias, C Papadimitriou, and M Yannakakis. </author> <title> Searching a fixed graph. </title> <booktitle> In Proc. 23rd International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 280-289, </pages> <address> Berlin, Germany, 1996. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The framework is well studied theoretically in the case of random-walkers. The problem of finding the optimal path to an unknown node in a weighted graph is NP-complete <ref> [19] </ref>. Agents can do no better than heuristically searching on-line through the graph. The problem is interesting because typically the graph is distributed, so that agents are charged costs for using its resources, e.g., traversing edges and evaluating nodes' payoff.
Reference: 20. <author> RR Larson. </author> <title> Bibliometrics of the world wide web: An exploratory analysis of the intellectual structure of cyberspace. </title> <booktitle> In Proc. 1996 Annual ASIS Meeting, </booktitle> <year> 1996. </year>
Reference-contexts: Yet, all fi values are significantly positive. Independent evidence for the value of linkage topology can be found in bibliometric studies of the Web <ref> [20] </ref> as well as link-based approaches to Web page categorization or discovery [6]. If links constitute useful cues for navigation, they can be exploited by autonomous browsing agents just as they are by browsing users | indeed, even the dumbest of agents (random walkers) can exploit linkage information.
Reference: 21. <author> DD Lewis. </author> <title> Active by accident: Relevance feedback in information retrieval. </title> <booktitle> In AAAI Fall Symposium on Active Learning, </booktitle> <year> 1995. </year>
Reference-contexts: If relevance assessments from the user are available, active learning should take advantage of them because it has been shown that they can considerably improve the performance of retrieval systems <ref> [21] </ref>. This is why, in a user-oriented system such as InfoSpiders, we have integrated unsupervised adaptation with "adaptation by examples," driven by relevance feedback. The environmental model behind local selection allows relevance feedback to interact asynchronously with the on-line agents.
Reference: 22. <author> DD Lewis. </author> <title> Information retrieval and the statistics of large data sets. </title> <booktitle> In Proc. NRC Massive Data Sets Workshop, </booktitle> <address> Washington, DC, </address> <year> 1996. </year>
Reference-contexts: Large, distributed text collections are a typical example of massive data sets that challenge machine learning techniques due to their huge feature space dimensionality <ref> [22] </ref>. InfoSpiders deal with dimensionality reduction in a localized, situated way. Agents internalize those words that appear maximally correlated (or anticorrelated) with their objective function, in their (temporally and spatially) local context. <p> More in general, we believe that the ideas incorporated into the InfoSpiders framework are a first step towards addressing some of the new challenges posed by text classification to machine learning, especially the need to extend information retrieval to deal with time-varying documents and user needs <ref> [22] </ref> and with large, dynamic, and heterogeneous collections such as the Web [23].
Reference: 23. <author> DD Lewis. </author> <title> Challenges in machine learning for text classification. </title> <booktitle> In Proc. 9th Annual Conference on Computational Learning Theory, </booktitle> <address> New York, NY, </address> <year> 1997. </year> <month> 44 </month>
Reference-contexts: incorporated into the InfoSpiders framework are a first step towards addressing some of the new challenges posed by text classification to machine learning, especially the need to extend information retrieval to deal with time-varying documents and user needs [22] and with large, dynamic, and heterogeneous collections such as the Web <ref> [23] </ref>.
Reference: 24. <author> H Lieberman. </author> <title> Autonomous interface agents. </title> <booktitle> In Proc. ACM Conference on Computers and Human Interface, </booktitle> <address> Atlanta, GA, </address> <year> 1997. </year>
Reference-contexts: WebWatcher [1] is an agent that learns to mimic the user by looking over his/her shoulder while browsing. Then it performs look-ahead searches and provides the browsing user with suggestions about what links to follow next. Similarly, Letizia <ref> [24] </ref> is an autonomous interface agent that assists the user in browsing the Web by making real-time suggestions for pages that might interest the user. As with InfoSpiders, these agents learn to predict an objective function on-line; they can also track time-varying user preferences.
Reference: 25. <author> L-J Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: To this end, we have endowed agents with the capability to adjust their neural nets by Q-learning (step (7) of the algorithm in Figure 1). This algorithm was chosen because it is model-free and easy to implement within the connectionist framework of the agent representation <ref> [25] </ref>; agents' neural nets are naturally used as Q-value function approximators. An agent compares the payoff of the current node with the prediction based on the features of the link that was followed to visit the node. <p> Q-learning The agent then compares the relevance (assessed or estimated) of the current document with the estimate of the link that led to it. By using the connectionist version of Q-learning <ref> [25] </ref>, the neural net can be trained on-line to predict values of links based on local context.
Reference: 26. <author> RM Lukose and BA Huberman. </author> <title> Surfing as a real option. </title> <booktitle> In Proc. Computational Economics Symposium, </booktitle> <address> Cambridge, England, </address> <year> 1998. </year>
Reference-contexts: The idea is to weigh the diminishing returns of continuing to browse against the accumulated value of visited nodes, much like future options are evaluated in financial markets <ref> [26] </ref>. 8. Conclusion 8.1. Summary Our results suggest that distributed, adaptive, on-line information browsing agents could complement current indexing technology by starting up where search engines stop.
Reference: 27. <author> Lycos. </author> <note> http://www.lycos.com. </note>
Reference-contexts: Measures of fi for ten queries submitted to Lycos <ref> [27] </ref> and Britannica Online [12]. The minimum score parameter used for Lycos was 0.1. Only full (absolute) HTTP references were considered for Lycos, and only Micropaedia and Macropaedia article references for EB. Multiple-term queries represent AND Boolean searches.
Reference: 28. <author> SW Mahfoud. </author> <title> Population sizing for sharing methods. </title> <booktitle> In Foundations of Genetic Algorithms 3, </booktitle> <year> 1994. </year>
Reference-contexts: Moreover, the population size required to maintain a cover across niches grows rapidly with the number of niches <ref> [28] </ref>. Local selection naturally enforces the maintenance of population diversity and is implicitly niched without any communication overhead. Therefore it affords efficiency both in centralized and, especially, distributed tasks. Local selection and reinforcement learning both allow agents to internalize environmental features in an unsupervised fashion [38].
Reference: 29. <author> SW Mahfoud. </author> <title> A comparison of parallel and sequential niching methods. </title> <booktitle> In Proc. 6th International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year>
Reference-contexts: In the evolutionary algorithm community this behavior is often referred to as niche selection <ref> [16, 10, 29] </ref>. The most notable selection variations explicitly aimed at niching are crowding [9] and fitness sharing [15].
Reference: 30. <author> F Menczer. ARACHNID: </author> <title> Adaptive retrieval agents choosing heuristic neighborhoods for information discovery. </title> <booktitle> In Proc. 14th International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: As a reality check, we have measured fi for a few queries from a couple of search engines <ref> [30] </ref>. Relevance autocorrelation statistics were collected by counting the 5 fraction of links, from documents in each relevant set, pointing back to documents in the set. Generality statistics were collected by normalizing the size of the relevant sets by the size of the collections. <p> Internalization of global cues In a second set of experiments, the goal was to test the capability of agents evolving by the local selection algorithm to internalize global environmental cues <ref> [30] </ref>. The signal considered was the accuracy of payoff predictions based on link cues, i.e., the potential accuracy of optimally evolved agents. For high A, the optimal agent strategy is best-first-search; for low A, it is random-walk. <p> Our evaluation of the InfoSpiders collective performance provides us with encouraging support for the approach: the population can locate relevant documents in a large distributed corpora faster than best-first-search, taking advantage of its distributed model and implementation. We have shown elsewhere <ref> [30, 33] </ref> that In-foSpiders outperform exhaustive (breadth-first) search in this domain by an order of magnitude, and that their performance receives a boost from the synergy between individual learning and relevance feedback.
Reference: 31. <author> F Menczer and RK Belew. </author> <title> From complex environments to complex behaviors. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 4 </volume> <pages> 317-363, </pages> <year> 1996. </year>
Reference-contexts: 5. produce motor action 6. update energy and environmental state 7. optionally learn by reinforcement 8. if (E ) 9. reproduce 10. split energy with offspring 11. elsif (E 0) 12. die 13. replenish environmental resources ing in natural environments provide us with ways to remain agnostic about these questions <ref> [32, 31] </ref>. Such an algorithm is illustrated in Figure 1.
Reference: 32. <author> F Menczer and RK Belew. </author> <title> Latent energy environments. In Adaptive Individuals in Evolving Populations: Models and Algorithms. </title> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: 5. produce motor action 6. update energy and environmental state 7. optionally learn by reinforcement 8. if (E ) 9. reproduce 10. split energy with offspring 11. elsif (E 0) 12. die 13. replenish environmental resources ing in natural environments provide us with ways to remain agnostic about these questions <ref> [32, 31] </ref>. Such an algorithm is illustrated in Figure 1.
Reference: 33. <author> F Menczer and RK Belew. </author> <title> Adaptive information agents in distributed textual environments. </title> <booktitle> In Proc. 2nd International Conference on Autonomous Agents, </booktitle> <address> Minneapolis, MN, </address> <year> 1998. </year>
Reference-contexts: InfoSpiders evaluation In this section we report on results of experiments and analysis aimed at evaluating the performance of InfoSpiders in responding to queries by searching the Web online. In so doing we extend previous, preliminary results <ref> [33] </ref>. 26 Table 3. InfoSpiders parameter descriptions and values for the experiments reported in this paper. <p> Our evaluation of the InfoSpiders collective performance provides us with encouraging support for the approach: the population can locate relevant documents in a large distributed corpora faster than best-first-search, taking advantage of its distributed model and implementation. We have shown elsewhere <ref> [30, 33] </ref> that In-foSpiders outperform exhaustive (breadth-first) search in this domain by an order of magnitude, and that their performance receives a boost from the synergy between individual learning and relevance feedback.
Reference: 34. <author> F Menczer and RK Belew. </author> <title> Local selection. </title> <booktitle> In Proc. 7th Annual Conference on Evolutionary Programming, </booktitle> <address> San Diego, CA, </address> <year> 1998. </year>
Reference-contexts: Local versus global selection We have first used the graph environments to compare local and global selection <ref> [34] </ref>. Binary deterministic tournament selection was chosen as the global scheme for the comparison because of its steady-state nature. <p> Machine learning contributions Evolutionary algorithms using local selection are a general adaptive paradigm for distributed agents. While we have shown elsewhere that the approach is not suitable in every domain (e.g., combinatorial optimization <ref> [34] </ref>), local selection has proven successful in multimodal optimization problems requiring a heterogeneous cover of the search space rather than a convergence to the perceived global optimum. In the evolutionary algorithm community this behavior is often referred to as niche selection [16, 10, 29].
Reference: 35. <author> F Menczer, W Willuhn, and RK Belew. </author> <title> An endogenous fitness paradigm for adaptive information agents. </title> <booktitle> In CIKM Workshop on Intelligent Information Agents, </booktitle> <year> 1994. </year>
Reference-contexts: Links have been used for enhancing relevance judgments [39, 54] and incorporated into query formulation to improve searching [2, 47]. Fish Search [8] was a search system proposed at the same time as InfoSpiders <ref> [35] </ref> and inspired by some of the same ideas from artificial life. Fish Search was based on a population of search agents who browsed the Web autonomously, driven by an internally generated energy measure based on relevance estimations. The population was client-based, and used a centralized cache for efficiency.
Reference: 36. <author> T Mitchell. </author> <title> Machine Learning, chapter 4. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1997. </year>
Reference-contexts: The connectionist model in which InfoSpiders learn to estimate link relevance is also reminiscent of the use of neural networks to learn probability distributions for text retrieval by logistic regression <ref> [36] </ref>. If relevance assessments from the user are available, active learning should take advantage of them because it has been shown that they can considerably improve the performance of retrieval systems [21].
Reference: 37. <author> A Moukas and G Zacharia. </author> <title> Evolving a multi-agent information filtering solution in amalthaea. </title> <booktitle> In Proc. 1st International Conference on Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: Unlike InfoSpiders, however, WebWatcher 40 and Letizia are single agents, and more importantly they need supervision from the user in order to work; no autonomous search is possible. Fab [3] and Amalthaea <ref> [37] </ref> are multi-agent adaptive filtering systems inspired by genetic algorithms, artificial life, and market models. Term weighting and relevance feedback are used to adapt a matching between a set of discovery agents (typically search engine parasites) and a set of user profiles (corresponding to single-or multiple-user interests).
Reference: 38. <author> L Pack Kaelbling, ML Littman, and AW Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: If the agent has access to reinforcement signals from the environment that somehow assess the appropriateness of its actions, then such signals can be used as reward or penalty signals to adjust the agent's behavior during its life. This is the basis of the reinforcement learning framework <ref> [38] </ref>. On-line agents performing in complex environment, such as networked information environment, have access to many signals. <p> Local selection naturally enforces the maintenance of population diversity and is implicitly niched without any communication overhead. Therefore it affords efficiency both in centralized and, especially, distributed tasks. Local selection and reinforcement learning both allow agents to internalize environmental features in an unsupervised fashion <ref> [38] </ref>. InfoSpiders integrate population-based adaptation and individual-based learning in a seamless way, not to accelerate 38 global optimization [18] but to take advantage of environmental signals at different spatial and temporal scales.
Reference: 39. <author> E Rivlin, R Botafogo, and B Shneiderman. </author> <title> Navigating in hyperspace: designing a structure-based toolbox. </title> <journal> Communications of the ACM, </journal> <volume> 37(2) </volume> <pages> 87-96, </pages> <year> 1994. </year>
Reference-contexts: Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments <ref> [39, 54] </ref> and incorporated into query formulation to improve searching [2, 47]. Fish Search [8] was a search system proposed at the same time as InfoSpiders [35] and inspired by some of the same ideas from artificial life.
Reference: 40. <author> SE Robertson and K Spark Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> pages 129-146, </pages> <year> 1976. </year>
Reference-contexts: Local adaptation is therefore "hardwired" into the system. Our use of relevance feedback also follows in the mainstream of text retrieval research, perhaps closest to the way in which supervised learning is used to estimate word probability distributions <ref> [40, 46] </ref>. Again, the main difference is that InfoSpi-ders use cues provided by user assessments in conjunction with local context; an agent will not waste its limited resources paying attention to a word that never appears in the current search area, even if the user likes that word a lot.
Reference: 41. <author> DE Rumelhart, GE Hinton, and RJ Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In DE Rumelhart and JL McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> Bradford Books (MIT Press), </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The neural net's weights are then updated by back-propagation of error <ref> [41] </ref>. Learned changes to the weights are "Lamarckian" in that they are inherited by offspring at reproduction (see [4] for a treatment of this issue). In the absence of relevance assessments, this reinforcement learning algorithm is unsupervised because it is the environment that provides the reinforcement signal e (D).
Reference: 42. <author> D Rus, R Gray, and D Kotz. </author> <title> Transportable information agents. </title> <booktitle> In Proc. 1st International Conference on Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: Secure languages and protocols are surely needed before trusted autonomous agents will become a welcome reality; agent research is providing systems 42 technology with the thrust that may very soon make such mobile agents possible <ref> [42] </ref>. Finally, the feasibility of integrating agent-based on-line search with index-based search engines must be put to the test. Hybrid systems can be constructed in which search engines provide agents with good staring points, based on the statistical (word-based) topology of the search space.
Reference: 43. <author> G Salton and C Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41 </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: Feature selection is normally used in information retrieval for query expansion <ref> [43, 17] </ref>. This is also the case for our system, where environmental word features are internalized into agent representations by adaptive mutations. But when space, time, and user needs constitute highly dynamic and heterogeneous contexts, no 39 query expansion is necessarily appropriate everywhere or forever.
Reference: 44. <author> G Salton and MJ McGill. </author> <title> An Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference-contexts: In the following subsections we discuss three ways in which agents can improve on the current state of the art. 2.1. Linkage topology Indexing can be described as the process of building a statistical topology over a document space. In the vector representation <ref> [44] </ref>, documents and queries are viewed as vectors in very large feature spaces where each word corresponds to a dimension. Two documents are similar, or a document is relevant with respect to a query, if the angle between their respective vectors is small. <p> Information retrieval contributions Unsupervised learning has been applied extensively in information retrieval, especially for automatic classification and clustering <ref> [52, 44] </ref>. The difference in the InfoSpiders approach is that adaptation is driven by local interactions, and no single agent is intended to learn a globally optimal document or link relevance estimation strategy.
Reference: 45. <author> K Sparck Jones. </author> <title> A statistical interpretation of term specificity and its application in retrieval. </title> <journal> Journal of Documentation, </journal> <volume> 28 </volume> <pages> 111-121, </pages> <year> 1972. </year>
Reference-contexts: Such a weighting formula differs from more traditional T F IDF schemes <ref> [45] </ref> in at least two respects. First, it is not aimed at weighting terms based on how well they describe documents, but rather on how well they correlate with relevance.
Reference: 46. <author> K Sparck Jones. </author> <title> Experiments in relevance weighting of search terms. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 15 </volume> <pages> 133-144, </pages> <year> 1979. </year>
Reference-contexts: Local adaptation is therefore "hardwired" into the system. Our use of relevance feedback also follows in the mainstream of text retrieval research, perhaps closest to the way in which supervised learning is used to estimate word probability distributions <ref> [40, 46] </ref>. Again, the main difference is that InfoSpi-ders use cues provided by user assessments in conjunction with local context; an agent will not waste its limited resources paying attention to a word that never appears in the current search area, even if the user likes that word a lot.
Reference: 47. <author> E Spertus. Parasite: </author> <title> Mining structural information on the web. </title> <booktitle> In Proc. 6th International World Wide Web Conference, </booktitle> <year> 1997. </year>
Reference-contexts: Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments [39, 54] and incorporated into query formulation to improve searching <ref> [2, 47] </ref>. Fish Search [8] was a search system proposed at the same time as InfoSpiders [35] and inspired by some of the same ideas from artificial life.
Reference: 48. <author> P Srinivasan, </author> <year> 1998. </year> <type> Personal communication. </type>
Reference-contexts: Our weighting scheme may also be improved; for example, it has been suggested that the use of IDF in the local relevance estimation mechanism may be inappropriately biased toward global features <ref> [48] </ref>.
Reference: 49. <author> AM Steier. </author> <title> Statistical semantics of phrases in hierarchical contexts. </title> <type> PhD thesis, </type> <institution> Computer Science and Engineering Department, </institution> <address> U. C. San Diego, </address> <year> 1994. </year>
Reference-contexts: The utility of an index term, as a discriminator of relevant from irrelevant items, can become a muddy average of its application across multiple, distinct sub-corpora within which these words have more focused meaning <ref> [50, 49] </ref>. Situated agents, on the other hand, can rely on local coherence in keyword distributions by exploiting their structural (link) proximity. Over time, agents may come to internalize the features that best describe the current documents and discriminate between relevant and other pages. <p> A more general difficulty is the lack of queries with available well-defined relevant sets. To overcome this problem, a special chunk of the Web has been selected as a test environment <ref> [50, 49] </ref>: the Encyclopaedia Britannica (EB) [12].
Reference: 50. <author> AM Steier and RK Belew. </author> <title> Exporting phrases: A statistical analysis of topical language. </title> <editor> In R Casey and B Croft, editors, </editor> <booktitle> 2nd Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1994. </year>
Reference-contexts: The utility of an index term, as a discriminator of relevant from irrelevant items, can become a muddy average of its application across multiple, distinct sub-corpora within which these words have more focused meaning <ref> [50, 49] </ref>. Situated agents, on the other hand, can rely on local coherence in keyword distributions by exploiting their structural (link) proximity. Over time, agents may come to internalize the features that best describe the current documents and discriminate between relevant and other pages. <p> A more general difficulty is the lack of queries with available well-defined relevant sets. To overcome this problem, a special chunk of the Web has been selected as a test environment <ref> [50, 49] </ref>: the Encyclopaedia Britannica (EB) [12].
Reference: 51. <author> R Sutton. </author> <title> Reinforcement learning and information access. </title> <booktitle> In AAAI Spring Symposium on Machine Learning and Information Access, </booktitle> <year> 1996. </year> <month> 45 </month>
Reference-contexts: There is plenty of data available on-line, and although it may be noisy and inconsistent compared with manually constructed relevance assessments, adaptive algorithms must take advantage of what is cheap and realistic in the actual search environment <ref> [51] </ref>. The connectionist model in which InfoSpiders learn to estimate link relevance is also reminiscent of the use of neural networks to learn probability distributions for text retrieval by logistic regression [36]. <p> This is why, in a user-oriented system such as InfoSpiders, we have integrated unsupervised adaptation with "adaptation by examples," driven by relevance feedback. The environmental model behind local selection allows relevance feedback to interact asynchronously with the on-line agents. Relevance feedback is a selfish process from the user's standpoint <ref> [51] </ref>, but it provides agents with modified rewards that improve on their models of relevance and therefore on their performance. Large, distributed text collections are a typical example of massive data sets that challenge machine learning techniques due to their huge feature space dimensionality [22].
Reference: 52. <author> CJ van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year> <note> Second edition. </note>
Reference-contexts: If we use nodes to model hypertext documents, edges for hyperlinks, and payoff for some measure of relevance, then the problem is that of networked information retrieval; we can explore different search strategies in simulated information environments, given a model of relevance <ref> [52] </ref>. Alternatively, the graph could be used to model a 2-dimensional environment in 12 which agents have to sense their position and move to reach some goal. This would be a typical task for situated robots. <p> This makes it impossible to use standard information retrieval performance analysis methods such as precision-recall curves, which require rank or similar measures as a control parameter <ref> [52] </ref>. Another metric lends itself better to assess the performance of on-line retrieval systems. Search length is normally defined as the number of irrelevant documents that appear, in some ordered list of retrieved documents, in front of some fraction of the relevant set [7]. <p> Information retrieval contributions Unsupervised learning has been applied extensively in information retrieval, especially for automatic classification and clustering <ref> [52, 44] </ref>. The difference in the InfoSpiders approach is that adaptation is driven by local interactions, and no single agent is intended to learn a globally optimal document or link relevance estimation strategy. <p> Several search engines now allow such queries for k = 1. 3. We make this conservative assumption to obtain a lower bound for the value added of linkage topology. 4. This conjecture is equivalent to the cluster hypothesis <ref> [52] </ref> under a hypertext derived definition of association. 5. But a retrieved set can be viewed as the relevant set for some query. 6.
Reference: 53. <author> W3C Reference Library. </author> <note> Libwww version 5.1. http://www.w3.org/pub/WWW/Library, 1997. </note>
Reference-contexts: There is no direct interaction between the user and the agents. The InfoSpiders prototype is written in C and runs on UNIX and MacOS platforms. The Web interface is based on the W3C library <ref> [53] </ref>. Agents employ standard information retrieval tools such as a filter for noise words [13] and a stemmer based 22 on Porter's algorithm [14]. Finally, agents store an efficient representation of visited documents in the shared cache on the client machine.
Reference: 54. <author> R Weiss, B Velez, M Sheldon, C Nemprempre, P Szilagyi, and DK Giffor. Hypursuit: </author> <title> A hierarchical network search engine that exploits content-link hypertext clustering. </title> <booktitle> In Proc. Seventh ACM Conference on Hypertext, </booktitle> <year> 1996. </year>
Reference-contexts: Linkage topology also has been considered by others in the context of the Web, with different motivations. Links have been used for enhancing relevance judgments <ref> [39, 54] </ref> and incorporated into query formulation to improve searching [2, 47]. Fish Search [8] was a search system proposed at the same time as InfoSpiders [35] and inspired by some of the same ideas from artificial life.
References-found: 54

