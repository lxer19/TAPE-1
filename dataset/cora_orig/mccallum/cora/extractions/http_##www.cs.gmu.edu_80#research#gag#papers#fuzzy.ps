URL: http://www.cs.gmu.edu:80/research/gag/papers/fuzzy.ps
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Title: Feature Selection Methods: Genetic Algorithms vs. Greedy-like Search  
Author: Haleh Vafaie and Ibrahim F. Imam 
Address: Fairfax, VA, 22030  
Affiliation: George Mason University,  
Abstract: This paper presents a comparison between two feature selection methods, the Importance Score (IS) which is based on a greedy-like search and a genetic algorithm-based (GA) method, in order to better understand their strengths and limitations and their area of application. The results of our experiments show a very strong relation between the nature of the data and the behavior of both systems. The Importance Score method is more efficient when dealing with little noise and small number of interacting features, while the genetic algorithms can provide a more robust solution at the expense of increased computational effort. Keywords. feature selection, machine learning, genetic algorithms, search. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brodatz, P., </author> <title> A Photographic Album for Arts and Design, </title> <publisher> Dover Publishing Co., </publisher> <address> Toronto, Canada, </address> <year> 1966. </year>
Reference-contexts: There is a very small amount of noise in the data. In this experiment both methods find an optimal subset, but the IS method requires fewer iterations. The second experiment is based on texture images which were randomly selected from Brodatz <ref> (Brodatz, 1966) </ref> album of textures as depicted in Figure 5.
Reference: <author> Chan, K. C., and Wong, A. K., </author> <title> A Statistical Technique for Extracting Classificatory Knowledge from Databases, Knowledge Discovery In Databases, </title> <editor> Piatetsky-Shapiro, G., Frawley, W., (Eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference: <author> De Jong, K., </author> <title> Analysis of the behavior of a class of genetic adaptive systems, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor, MI., </address> <year> 1975. </year>
Reference-contexts: It acts as a population perturbation operator and is a means for inserting new information into the population. This operator prevents any stagnation that might occur during the search process. Genetic algorithms have demonstrated substantial improvement over a variety of random and local search methods <ref> (De Jong, 1975) </ref>. This is accomplished by their ability to exploit accumulating information about an initially unknown search space in order to bias subsequent search into promising subspaces. <p> In order to keep as many things as possible constant, the parameters of the AQ14 system were held constant for both methods. For the GA approach, GENESIS (Grefenstette, 1984), a general purpose genetic algorithm program, was used with the standard parameter settings recommended in <ref> (De Jong, 1975) </ref>: a population size=50, a mutation rate= 0.001, and a crossover rate=0.6. Table 1 summarizes the results of our experiments comparing the best performance of the IS method to that of the GAbased strategy.
Reference: <author> De Jong, K., </author> <title> Learning with Genetic Algorithms : An overview, </title> <journal> Machine Learning Vol. </journal> <volume> 3, </volume> <publisher> Kluwer Academic publishers, </publisher> <year> 1988. </year>
Reference-contexts: Since GAs are basically a domain independent search technique, they are ideal for applications where domain knowledge and theory is difficult or impossible to provide <ref> (De Jong, 1988) </ref>. The main issues in applying GAs to any problem are selecting an appropriate representation and an adequate evaluation function. For detailed description of both of these issues for the problem of feature selection see (Vafaie, 1991).
Reference: <author> Dom, B., Niblack, W., and Sheinvald, J., </author> <title> Feature selection with stochastic complexity, </title> <booktitle> Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Rosemont, IL., </address> <year> 1989. </year>
Reference-contexts: These feature selection techniques fall into two main categories. In the first approach problem specific strategies are developed based on the domain knowledge in order to reduce the number of features used to a manageable size <ref> (Dom 89) </ref>. The second approach is used when the domain knowledge is unavailable or expensive to exploit. In this case, generic heuristics, essentially greedy algorithms, are applied to select a subset d of the available m features (Kittler 78).
Reference: <author> Grefenstette, John J., </author> <type> Technical Report CS-83-11, </type> <institution> Computer Science Dept., Vanderbilt Univ., </institution> <year> 1984. </year>
Reference-contexts: EXPERIMENTAL RESULTS In performing the experiments reported here, AQ14 was used for learning decision rules. In order to keep as many things as possible constant, the parameters of the AQ14 system were held constant for both methods. For the GA approach, GENESIS <ref> (Grefenstette, 1984) </ref>, a general purpose genetic algorithm program, was used with the standard parameter settings recommended in (De Jong, 1975): a population size=50, a mutation rate= 0.001, and a crossover rate=0.6.
Reference: <author> Holland, J. H.,. </author> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI., </address> <year> 1975. </year>
Reference-contexts: For a detailed description of the fitness function see (Vafaie, 1993). 2.2 Genetic algorithm-Based Method The presented method uses a genetic algorithm for feature selection. Genetic algorithms (GAs), a form of inductive learning strategy, are adaptive search techniques initially introduced by Holland <ref> (Holland, 1975) </ref>. Genetic algorithms derive their name from the fact that their operations are similar to the mechanics of genetic models of natural systems. Genetic algorithms typically maintain a constantsized population of individuals which represent samples of the space to be searched.
Reference: <editor> Imam, I.F., Michalski, R.S., and Kerschberg, L. </editor> <title> Discovering Attribute Dependence in Databases by Integrating Symbolic Learning and Statistical Analysis Techniques, </title> <booktitle> Proceeding of the AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Washington D.C., </address> <month> July 11-12, </month> <year> 1993. </year>
Reference-contexts: section the selected methods and the major components involved in their design are described in more detail. 2.1 The importance score method The importance score method is based on combining AQ, a symbolic learning system, and statistical methods for determining degree of dependence between the decision classes and other features <ref> (Imam, 1993) </ref>. The dependence between the features and the decision classes are determined in a form of score (number between zero and one) for each feature.
Reference: <author> Kittler, J., </author> <title> Feature set search algorithms, in Pattern Recognition and Signal Processing, </title> <editor> C.H. Chen, Ed., Sijthoff and Noordhoff, </editor> <address> The Netherlands, </address> <year> 1978. </year>
Reference-contexts: The second approach is used when the domain knowledge is unavailable or expensive to exploit. In this case, generic heuristics, essentially greedy algorithms, are applied to select a subset d of the available m features <ref> (Kittler 78) </ref>. The experiments reported in this paper compare two techniques which belong to the second category, in order to better understand their strengths and limitations and their area of application.
Reference: <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 20, </volume> <pages> pp. 111-116, </pages> <year> 1983. </year>
Reference: <author> Michalski, </author> <title> R.S., </title> <booktitle> AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition in Proceeding of the First International Joint Conference on Pattern Recognition, </booktitle> <address> Washington, DC, </address> <pages> pp. 3-17, </pages> <month> October 30-November, </month> <year> 1973. </year>
Reference: <author> Michalski, R.S., Mozetic, I., Hong, J., and Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> pp. 1041-1045, </pages> <address> Philadelphia, PA: </address> , <year> 1986. </year>
Reference-contexts: Table 1 summarizes the results of our experiments comparing the best performance of the IS method to that of the GAbased strategy. In the first experiment, a natural database designed for diagnosing breast cancer <ref> (Michalski, 1986) </ref> was used. This data was originally extracted from the MLI database of George Mason University and each case was described using nine features. There is a very small amount of noise in the data.
Reference: <author> Reinke, R.E., </author> <title> Knowledge Acquisition and Refinement Tools for the ADVISE Meta-Expert System, </title> <type> Master thesis, </type> <institution> ISG 84-4, Urbana, Illinois, </institution> <month> July, </month> <year> 1984. </year>
Reference: <author> Vafaie, H. and De Jong, K., </author> <title> Genetic Algorithms as a Tool for Feature Selection in Machine Learning, </title> <booktitle> Proceeding of the 4th International Conference on Tools with Artificial Intelligence, </booktitle> <address> Arlington, VA, </address> <month> November, </month> <year> 1992. </year>
Reference: <author> Vafaie, H. and De Jong, K., </author> <title> Imporving the Performance of a Rule Induction System Using Genetic Algorithms, in Machine Learning: A Multistrategy Approach, Vol. IV, R.S. </title> <editor> Michalski and G.Tecuci (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The experiments reported in this paper compare two techniques which belong to the second category, in order to better understand their strengths and limitations and their area of application. The selected feature selection algorithms (Important Score method and GAbased technique) contain the basic components as shown in Figure 1 <ref> (Vafaie 93) </ref>. The search procedures used by the Importance Score (IS) technique and the genetic algorithm-based (GA) method require no domain knowledge to assist the search process. <p> For a detailed description of the fitness function see <ref> (Vafaie, 1993) </ref>. 2.2 Genetic algorithm-Based Method The presented method uses a genetic algorithm for feature selection. Genetic algorithms (GAs), a form of inductive learning strategy, are adaptive search techniques initially introduced by Holland (Holland, 1975).
Reference: <author> Vafaie, H., and De Jong, K.A., </author> <title> Improving the performance of a Rule Induction System Using Genetic Algorithms, </title> <booktitle> Proceedings of the First International Workshop on MULTISTRATEGY LEARNING, </booktitle> <address> Harpers Ferry, </address> <publisher> W. </publisher> <address> Virginia, USA, </address> <year> 1991. </year>
Reference-contexts: The fitness function then evaluates the AQ generated rules on the testing examples as follows. For every testing example a match score <ref> (Vafaie 91) </ref> is evaluated for each of the classification rules generated by the AQ algorithm, in order to find the rule (s) with the highest or best match. <p> The main issues in applying GAs to any problem are selecting an appropriate representation and an adequate evaluation function. For detailed description of both of these issues for the problem of feature selection see <ref> (Vafaie, 1991) </ref>. Representation The natural representation for the feature selection problem is precisely the one described earlier, namely a binary string of length N representing the presence or absence of each of the N possible features. <p> Evaluation functions provide GAs with the feedback about the fitness of each individual in the population. GAs then use this feedback to bias the search process so as to provide an improvement in the populations average fitness. The process of evaluation involves the steps presented in Figure 4 <ref> (Vafaie & De Jong, 1991) </ref>. The evaluation function is solely based on the performance of the classification process used, in order to select the appropriate feature set, without attempting to bias the search toward small feature subsets.
References-found: 16

