URL: http://robotics.stanford.edu/~ronnyk/roc.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: foster@basit.com  fawcett@basit.com  ronnyk@sgi.com  
Title: The Case Against Accuracy Estimation for Comparing Induction Algorithms  
Author: Foster Provost Tom Fawcett Ron Kohavi 
Address: 400 Westchester Avenue White Plains, NY 10604  400 Westchester Avenue White Plains, NY 10604  M/S 8U-876 2011 N. Shoreline Blvd. Mountain View, CA 94043  
Affiliation: Bell Atlantic Science and Tech  Bell Atlantic Science and Tech  Silicon Graphics Inc.  
Abstract: We analyze critically the use of classification accuracy to compare classifiers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions.
Abstract-found: 1
Intro-found: 1
Reference: <author> J. Bradford, C. Kunz, R. Kohavi, C. Brunk, and C. Brodley. </author> <title> (1998) Pruning decision trees with misclassification costs. </title> <booktitle> In Proceedings of ECML-98, </booktitle> <pages> pages 131-136. </pages>
Reference-contexts: Naive Bayes is robust with respect to changes in costs|it will produce the same ROC curve regardless of the target costs and class distribution. Furthermore, it has been shown that decision trees are surprisingly robust if the probability estimates are generated with the Laplace estimate <ref> ( Bradford et al., 1998 ) </ref> . If this result holds generally, the results in the previous section would disconfirm the present hypothesis as well. Second, Bradley's ( 1997 ) results provide disconfirming evidence.
Reference: <author> A. P. Bradley. </author> <title> (1997) The use of the area under the ROC curve in the evaluation of machine learning al-gorithms. </title> <journal> Pattern Recognition, </journal> <volume> 30(7) </volume> <pages> 1145-1159. </pages>
Reference: <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. </author> <note> J. </note>
Reference: <author> Stone. </author> <title> (1984) Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> L. Breiman. </author> <title> (1996) Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 123-140. </pages>
Reference-contexts: We selected several inducers from MLC ++ ( Kohavi et al., 1997 ) : a decision tree learner (MC4), Naive Bayes with discretization (NB), k-nearest neighbor for several k values (IBk), and Bagged-MC4 <ref> ( Breiman, 1996 ) </ref> . MC4 is similar to C4.5 ( Quinlan, 1993 ) ; probabilistic predictions are made by using a Laplace correction at the leaves.
Reference: <author> J. Catlett. </author> <title> (1995) Tailoring rulesets to misclassificatioin costs. </title> <booktitle> In Proceedings of the 1995 Conference on AI and Statistics, </booktitle> <pages> pages 88-94. </pages>
Reference: <author> T. G. Dietterich. </author> <title> (1998) Approximate statistical tests for comparing supervised classification learning algo-rithms. Neural Computation. </title> <note> To appear. </note>
Reference: <author> P. Domingos and M. Pazzani. </author> <title> (1997) Beyond inde-pendence: Conditions for the optimality of the simple Bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130. </pages>
Reference-contexts: MC4 is similar to C4.5 ( Quinlan, 1993 ) ; probabilistic predictions are made by using a Laplace correction at the leaves. NB discretizes the data based on entropy minimization ( Dougherty et al., 1995 ) and then builds the Naive-Bayes model <ref> ( Domingos and Pazzani, 1997 ) </ref> . IBk votes the closest k neighbors; each neighbor votes with a weight equal to one over its distance from the test instance. The averaged ROC curves are shown in Figures 2 and 3.
Reference: <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> (1995) Supervised and unsupervised discretization of continuous features. </title> <editor> In A. Prieditis and S. Russell, (eds.), </editor> <booktitle> Proceedings of ICML-95, </booktitle> <pages> pages 194-202. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: MC4 is similar to C4.5 ( Quinlan, 1993 ) ; probabilistic predictions are made by using a Laplace correction at the leaves. NB discretizes the data based on entropy minimization <ref> ( Dougherty et al., 1995 ) </ref> and then builds the Naive-Bayes model ( Domingos and Pazzani, 1997 ) . IBk votes the closest k neighbors; each neighbor votes with a weight equal to one over its distance from the test instance.
Reference: <author> J. P. Egan. </author> <title> (1975) Signal Detection Theory and ROC Analysis. Series in Cognitition and Perception. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: However, since we are also recommending an analytical framework, we note that extending our work to multiple dimensions is an interesting open problem. Finally, we are not completely satisfied with our method of generating confidence intervals. The present intervals are appropriate for the Neyman- Pearson observer <ref> ( Egan, 1975 ) </ref> , which wants to maximize TP for a given FP. However, their appropriateness is questionable for evaluating minimum expected cost, for which a given set of costs contours ROC space with lines of a particular slope.
Reference: <author> T. Fawcett and F. Provost. </author> <title> (1997) Adaptive fraud detection. Data Mining and Knowledge Discovery, </title> <type> 1(3). </type> <note> Available: http://www.croftj.net/~fawcett/ DMKD-97.ps.gz. </note>
Reference-contexts: This fact is well documented, primarily in other fields (statistics, medical diagnosis, pattern recognition and decision theory). As an example, consider machine learning for fraud detection, where the cost of missing a case of fraud is quite different from the cost of a false alarm <ref> ( Fawcett and Provost, 1997 ) </ref> . Accuracy maximization also assumes that the class distribution (class priors) is known for the target environment. Unfortunately, for our benchmark data sets, we often do not know whether the existing distribution is the natural distribution, or whether it has been stratified. <p> Moreover, in many cases specifying target conditions is not just virtually impossible, it is actually impossible. Of- ten in real-world domains there are no "true" target costs and class distribution. These change from time to time, place to place, and situation to situation <ref> ( Fawcett and Provost, 1997 ) </ref> . Therefore the ability to transform cost minimization into accuracy maximization does not, by itself, justify limiting our comparisons to classification accuracy on the given class distribution.
Reference: <author> R. Kohavi, D. Sommerfield, and J. Dougherty. </author> <booktitle> (1997) Data mining using MLC ++ : A machine learning li-brary in C ++ . International Journal on Artificial Intelligence Tools, </booktitle> <volume> 6(4) </volume> <pages> 537-566. </pages> <note> Available: http: //www.sgi.com/Technology/mlc. </note>
Reference-contexts: For each domain, we induced classifiers for the minority class (for Road we chose the class Grass). We selected several inducers from MLC ++ <ref> ( Kohavi et al., 1997 ) </ref> : a decision tree learner (MC4), Naive Bayes with discretization (NB), k-nearest neighbor for several k values (IBk), and Bagged-MC4 ( Breiman, 1996 ) .
Reference: <author> R. Kohavi. </author> <title> (1995) A study of cross-validation and bootstrap for accuracy estimation and model selec-tion. </title> <editor> In C. S. Mellish, (ed.), </editor> <booktitle> Proceedings of IJCAI95, </booktitle> <pages> pages 1137-1143. </pages> <publisher> Morgan Kaufmann. </publisher> <address> Available: http://robotics.stanford.edu/~ronnyk. </address>
Reference: <author> C. Merz and P. Murphy. </author> <note> (1998) UCI repository of machine learning databases. Available: http://www. ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: To this end, the field has amassed an admirable collection of data sets from a wide variety of classifier applications <ref> ( Merz and Murphy, 1998 ) </ref> . Countless research results have been published based on comparisons of classifier accuracy over these benchmark data sets. We argue that comparing accuracies on our benchmark data sets says little, if anything, about classifier performance on real-world tasks.
Reference: <author> F. Provost and T. Fawcett. </author> <title> (1997) Analysis and visu-alization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proceedings of KDD-97, </booktitle> <pages> pages 43-48. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: This fact is well documented, primarily in other fields (statistics, medical diagnosis, pattern recognition and decision theory). As an example, consider machine learning for fraud detection, where the cost of missing a case of fraud is quite different from the cost of a false alarm <ref> ( Fawcett and Provost, 1997 ) </ref> . Accuracy maximization also assumes that the class distribution (class priors) is known for the target environment. Unfortunately, for our benchmark data sets, we often do not know whether the existing distribution is the natural distribution, or whether it has been stratified. <p> Moreover, in many cases specifying target conditions is not just virtually impossible, it is actually impossible. Of- ten in real-world domains there are no "true" target costs and class distribution. These change from time to time, place to place, and situation to situation <ref> ( Fawcett and Provost, 1997 ) </ref> . Therefore the ability to transform cost minimization into accuracy maximization does not, by itself, justify limiting our comparisons to classification accuracy on the given class distribution.
Reference: <author> F. Provost and T. Fawcett. </author> <title> (1998) Robust clas-sification systems for imprecise environments. </title> <note> In Proceedings of AAAI-98. AAAI Press. To ap-pear. Available: http://www.croftj.net/~fawcett/ papers/aaai98-dist.ps.gz. </note>
Reference: <author> J. R. Quinlan. </author> <year> (1993) </year> <month> C4.5: </month> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: We selected several inducers from MLC ++ ( Kohavi et al., 1997 ) : a decision tree learner (MC4), Naive Bayes with discretization (NB), k-nearest neighbor for several k values (IBk), and Bagged-MC4 ( Breiman, 1996 ) . MC4 is similar to C4.5 <ref> ( Quinlan, 1993 ) </ref> ; probabilistic predictions are made by using a Laplace correction at the leaves. NB discretizes the data based on entropy minimization ( Dougherty et al., 1995 ) and then builds the Naive-Bayes model ( Domingos and Pazzani, 1997 ) .
Reference: <author> L. Saitta and F. Neri. </author> <title> (1998) Learning in the "Real World". </title> <journal> Machine Learning, </journal> <volume> 30 </volume> <pages> 133-163. </pages>
Reference-contexts: The iris data set has exactly 50 instances of each class. The splice junction data set (DNA) has 50% donor sites, 25% acceptor sites and 25% non- boundary sites, even though the natural class distribution is very skewed: no more than 6% of DNA actually codes for human genes <ref> ( Saitta and Neri, 1998 ) </ref> . Without knowledge of the target class distribution we cannot even claim that we are indeed maximizing accuracy for the problem from which the data set was drawn.
Reference: <author> S. L. Salzberg. </author> <title> (1997) On comparing classifiers: Pitfalls to avoid and a recommended approach. Data Mining and Knowledge Discovery, </title> <booktitle> 1 </booktitle> <pages> 317-328. </pages>
Reference: <author> C. Schaffer. </author> <title> (1994) A conservation law for generaliza-tion performance. </title> <booktitle> In ICML-94, </booktitle> <pages> pages 259-265. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> J. A. Swets and R. M. Pickett. </author> <title> (1982) Evaluation of Diagnostic Systems: Methods from Signal Detection Theory. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> J. Swets. </author> <title> (1988) Measuring the accuracy of diagnostic systems. </title> <journal> Science, </journal> <volume> 240 </volume> <pages> 1285-1293. </pages>
Reference: <author> P. Turney. </author> <title> (1996) Cost sensitive learning bib-liography. </title> <note> Available: http://ai.iit.nrc.ca/ bibliographies/cost-sensitive.html. </note>
Reference: <author> D. H. Wolpert. </author> <title> (1994) The relationship between PAC, the statistical physics framework, the Bayesian frame-work, and the VC framework. </title> <editor> In D. H. Wolpert, (ed.), </editor> <title> The Mathematics of Generalization. Addison Wesley. (a) Breast cancer (b) CRX (c) German (d) Pima (e) RoadGrass (f) Satimage </title>
References-found: 24

