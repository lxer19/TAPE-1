URL: http://www.cs.colostate.edu/~vision/ps/tr-m-sens-map.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors  
Author: J. Ross Beveridge, Zhongfei Zhang, Mike Goss, Mark R. Stevens, A. Schwickerath 
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Date: April 16, 1996  
Pubnum: Technical Report CS-96-112  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [Ant96] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coregistration of Range and Optical Images Using Coplanarity and Orientation Constraints. </title> <booktitle> In 1996 Conference on Computer Vision and Patter Recognition, page (to appear), </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Those with such backgrounds either do not need this background or should read quickly. Several highly practical considerations have brought this paper into existence. First, much of the work at Colorado State on multi-sensor fusion <ref> [SB94, BHP95, J. 96, Ant96] </ref> has been implicitly using assumptions fleshed out and tested in this paper.
Reference: [Bel93] <author> Mark Bellrichard. </author> <title> Alliant techsystems LADAR field calibration. </title> <type> Personal Correspondence, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: The maximum range measured by the LADAR is 1074 feet, and hence multiplying a raw pixel value by the ratio 1074=4095 yields a range measurement in feet. The on the range meaurement is approximately 1 foot <ref> [Bel93] </ref>. More will be said about the geometry of the LADAR in the following section. In going back and forth between alternative ways of describing a sensor, a minor point where confusion can arise concerns the exact 'position' of a pixel.
Reference: [BHP95] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Model-based Fusion of FLIR, </title> <editor> Color and LADAR. In Paul S. Schenker and Gerard T. McKee, editors, </editor> <booktitle> Proceedings: Sensor Fusion and Networked Robotics VIII, Proc. SPIE 2589, </booktitle> <pages> pages 2 - 11, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Those with such backgrounds either do not need this background or should read quickly. Several highly practical considerations have brought this paper into existence. First, much of the work at Colorado State on multi-sensor fusion <ref> [SB94, BHP95, J. 96, Ant96] </ref> has been implicitly using assumptions fleshed out and tested in this paper.
Reference: [BPY94] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: One way to view this technical report is as a long tutorial working up to Section 6 which answers this question. Another motivation for this technical report is to better record and understand the characteristics of the sensors used in the Fort Carson data collection <ref> [BPY94] </ref>. In November of 1993 Colorado State University, Al-liant Techsystems and Martin Marrietta jointly collected a set of range, IR and color data at the Colorado National Guard Facility at Fort Carson, Colorado. Over 400 range images were collected in such a manner as to approximate 3 boresighted sensors.
Reference: [CB93] <author> James L. Crowley and Philippe Bobet. </author> <title> Maintaining stereo calibration by tracking image p oints. </title> <booktitle> In CVPR-93 Proceedings, </booktitle> <pages> pages 483 - 488, </pages> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: The internal parameters estimated for the Fort Carson image data using this method are recorded in Table 1. To have a comparison between the performance of this algorithm and those of the others, we use the same data points for Ganapathy's [Gan84] and Crowley et al's <ref> [CB93] </ref> algorithm. The following table lists the total error projected back to the image plane for each algorithm, respectively. The total error is the sum over all the points used in the calibration procedure (n = 18), of the error in each point.
Reference: [CCHR94] <author> Y-Q Cheng, R. Collins, A.R. Hanson, </author> <title> and E.M. Riseman. Robust camera calibration for a moving camera on a mobile robot. </title> <type> Master Thesis, </type> <institution> Computer Science, University of Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: recovering these intrinsic parameters from known calibration targets is taken up in in the following section. 3.1 Calibration from Calibration Targets The calibration work for the color imagery was performed by Zhongfei Zhang at the University of Mas-sachusetts using a method developed earlier at UMass by Yong-Qing Cheng et al <ref> [CCHR94] </ref>. The resulting intrinsic parameters are presented in Table 1. There is a minor inconsistency between the imagery used for calibration and that typically distributed with the Fort Carson dataset: the latter has been cropped to the center 720x480 pixels.
Reference: [Fau93] <author> O.D. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: There is a marvelous trick implicit in this technique which makes the non-linear perspective mapping amenable to a simple linear algebraic form. This trick is actually quite proper and rigorous in terms of projective geometry and is nicely explained in <ref> [Fau93] </ref>. From a mechanical standpoint, simply observe that expanding out the matrix multiplication yields: I = s v Y + t v Z (2) The 2D point I is represented in projective coordinates in which there are an infinite number of ways to express a single point. <p> It is equally correct to interpret these parameters as the size of the horizontal and vertical focal lengths measured in pixel units <ref> [Fau93] </ref>. The terms t u and t v represent the coordinates of the point where the optical axis pierces the image plane as measured in pixel units.
Reference: [FD82] <author> J. D. Foley and A. Van Dam. </author> <title> Fundamentals of Interactive Computer Graphics. </title> <booktitle> The Systems Programming Series. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1982. </year>
Reference-contexts: The key mapping is between 3D points and their projection on the 2D image plane. Many texts treat this topic <ref> [FD82] </ref>. One of the most compact and simplest ways of expressing the 3D to 2D relationship closely follows concepts developed in projective geometry. The following is a general equation for projection.
Reference: [Gan84] <author> S. Ganapathy. </author> <title> Decomposition of Transformation Matrices for Robot Vision. </title> <booktitle> In Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <pages> pages 130-139, </pages> <year> 1984. </year>
Reference-contexts: Sensor calibration is a rather complicated topic and this section will not attempt the same level of tutorial presentation used elsewhere in this report. Readers unfamiliar with calibration are encouraged to see <ref> [Gan84, LT86, STH80] </ref>. As laid out above, the camera model used in this work is assumed to be pinhole and the underlying mathematical model is a perspective transformation. <p> The internal parameters estimated for the Fort Carson image data using this method are recorded in Table 1. To have a comparison between the performance of this algorithm and those of the others, we use the same data points for Ganapathy's <ref> [Gan84] </ref> and Crowley et al's [CB93] algorithm. The following table lists the total error projected back to the image plane for each algorithm, respectively. The total error is the sum over all the points used in the calibration procedure (n = 18), of the error in each point.
Reference: [GBSF94] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491 - 494, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The second incorporates a correction generated by hand based upon visual appearance of modeled 3D objects in both range and IR. This was done interactively using our own multi-sensor visualization software <ref> [GBSF95, GBSF94] </ref>. With this software, it is possible to first align a 3D object model with range data and also with IR using the manufacturers specifications for the IR sensor.
Reference: [GBSF95] <author> M. E. Goss, J. R. Beveridge, M. Stevens, and A. Fuegi. </author> <title> Three-dimensional visualization environment for multisensor data analysis, interpretation, and model-based object recognition. </title> <booktitle> In IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <pages> pages 283 - 291, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The second incorporates a correction generated by hand based upon visual appearance of modeled 3D objects in both range and IR. This was done interactively using our own multi-sensor visualization software <ref> [GBSF95, GBSF94] </ref>. With this software, it is possible to first align a 3D object model with range data and also with IR using the manufacturers specifications for the IR sensor.
Reference: [J. 96] <author> J. Ross Beveridge and Bruce A. Draper and Kris Siejko. </author> <title> Progress on Target and Terrain Recognition Research at Colorado State University. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Those with such backgrounds either do not need this background or should read quickly. Several highly practical considerations have brought this paper into existence. First, much of the work at Colorado State on multi-sensor fusion <ref> [SB94, BHP95, J. 96, Ant96] </ref> has been implicitly using assumptions fleshed out and tested in this paper.
Reference: [Kum92] <author> Rakesh Kumar. </author> <title> Model Dependent Inference of 3D Information From a Sequence of 2D Images. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, COINS TR92-04, Amherst, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: See Appendix A of <ref> [Kum92] </ref> for the details of those constraints.
Reference: [LT86] <author> R.K. Lenz and R.Y Tsai. </author> <title> Techniques for calibration of the scale factor and image center for high accuracy 3-d machine vision metrology. </title> <journal> Trans. PAMI, </journal> <volume> 10(5), </volume> <year> 1986. </year>
Reference-contexts: Sensor calibration is a rather complicated topic and this section will not attempt the same level of tutorial presentation used elsewhere in this report. Readers unfamiliar with calibration are encouraged to see <ref> [Gan84, LT86, STH80] </ref>. As laid out above, the camera model used in this work is assumed to be pinhole and the underlying mathematical model is a perspective transformation.
Reference: [Mar63] <author> D. W. Marquardt. </author> <title> An algorithm for least squares estimation of nonli near parameters. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 11 </volume> <pages> 431-441, </pages> <year> 1963. </year>
Reference-contexts: Here, the Levenberg-Marquardt algorithm which is a robust algorithm to solve nonlinear systems developed by Levenberg and Marquardt <ref> [Mar63, PFTV88] </ref>, is used to compute the camera parameters ~ff. In the Levenberg-Marquardt method, we have ~ff = (H + I) 1 ~! (10) where is a conditioning factor and I is an identity matrix.
Reference: [PFTV88] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Here, the Levenberg-Marquardt algorithm which is a robust algorithm to solve nonlinear systems developed by Levenberg and Marquardt <ref> [Mar63, PFTV88] </ref>, is used to compute the camera parameters ~ff. In the Levenberg-Marquardt method, we have ~ff = (H + I) 1 ~! (10) where is a conditioning factor and I is an identity matrix.
Reference: [RL87] <author> P. J. Rousseeuw and A. M. Leroy. </author> <title> Robust regression & outlier detection. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Hence, gross errors or outliers may occur. In order to deal with gross errors or outliers in the 2D and 3D data, the following least median of squares (LMS) estimator is used. It has been proved that the following minimization always leads to a solution <ref> [RL87] </ref> Minimize m (i) = median i 8 : u i ^u i i + v i ^v i i 9 ; Since the median is not differentiable, (i) m must be minimized using combinatorial methods such as subsampling.
Reference: [SB94] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Model to Multisensor Coregistration with Eight Degrees of Freedom. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 481 - 490, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Those with such backgrounds either do not need this background or should read quickly. Several highly practical considerations have brought this paper into existence. First, much of the work at Colorado State on multi-sensor fusion <ref> [SB94, BHP95, J. 96, Ant96] </ref> has been implicitly using assumptions fleshed out and tested in this paper.
Reference: [STH80] <author> C. Slama, C. Theurer, and S. Henriksen. </author> <title> Manual of Photogrammetry, Fourth Edition. </title> <journal> American Society of Photogram-metry, </journal> <year> 1980. </year> <month> 21 </month>
Reference-contexts: Sensor calibration is a rather complicated topic and this section will not attempt the same level of tutorial presentation used elsewhere in this report. Readers unfamiliar with calibration are encouraged to see <ref> [Gan84, LT86, STH80] </ref>. As laid out above, the camera model used in this work is assumed to be pinhole and the underlying mathematical model is a perspective transformation.
References-found: 19

