URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3798.ps
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3798.html
Root-URL: 
Email: fdbader, josephg@umiacs.umd.edu  
Title: SIMPLE A Methodology for Programming High Performance Algorithms on Clusters of Symmetric Multiprocessors (SMPs) (Preliminary Version)  
Author: David A. Bader Joseph JaJa 
Keyword: Cluster Computing, Symmetric Multiprocessors (SMP), ATM Networks, Parallel Algorithms, Shared Memory, message passing (MPI), Experimental Parallel Algorithms, Parallel Performance.  
Date: May 16, 1997  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies University of Maryland,  
Abstract: We describe a methodology for developing high performance programs running on clusters of SMP nodes. Our methodology is based on a small kernel (SIMPLE ) of collective communication primitives that make efficient use of the hybrid shared and message passing environment. We illustrate the power of our methodology by presenting experimental results for sorting integers, two-dimensional fast Fourier transforms (FFT), and constraint-satisfied searching. Our testbed is a cluster of DEC AlphaServer 2100 4/275 nodes interconnected by an ATM switch. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alasdair, A. Bruce, J.G. Mills, and A.G. Smith. </author> <title> CHIMP/MPI User Guide. </title> <institution> Edin-burgh Parallel Computing Centre, The University of Edinburgh, </institution> <address> 1.2 edition, </address> <month> June </month> <year> 1994. </year> <note> http://www.epcc.ed.ac.uk/epcc-projects/CHIMP/. </note>
Reference-contexts: Experimental results are provided from implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 [22], MPICH 1.0.13 [12], or CHIMP 2.1.1c <ref> [1] </ref>) and POSIX threads (DECthreads [9] or freely available pthreads implementations [25, 20]) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [2] <author> C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <year> 1996. </year>
Reference-contexts: We call this model SIMPLE , referring to the joining of the SMP and MPI-like message passing paradigms and the simple programming approach (see Figures 1 and 2). Programming methodologies for COSMOS fall into two categories. The first, distributed shared memory (DSM) systems (for example, TreadMarks <ref> [2] </ref> from Rice University, Multigrain Shared Memory (MGS) [30] from MIT and Coherent Virtual Machine (CVM) [17] from University of Maryland), provides a software layer which simulates coherent shared memory between nodes by internally using messaging to move around specific data or referenced memory pages. <p> For instance, we ported an efficient SMP radix sort code into a software distributed shared memory package called Coherent Virtual Machine (CVM, version 0.1) [17] which is an extension of the commercial TreadMarks <ref> [2] </ref> DSM implementation. The performance of this DSM radix sort is given in Figure 14.
Reference: [3] <author> D.A. Bader. </author> <title> On the Design and Analysis of Practical Parallel Algorithms for Combinatorial Problems with Applications to Image Processing. </title> <type> PhD thesis, </type> <institution> University of Maryland, College Park, Department of Electrical Engineering, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: Thus, the most costly operation in a SIMPLE algorithm is internode communication, and algorithmic design must attempt to minimize the communication costs between the nodes. Since this is a similar optimization criterion used when designing efficient message passing algorithms <ref> [3] </ref>, it is beneficial to first design an efficient message passing algorithm on a COSMOS , and then adapt the algorithm for the SIMPLE paradigm. Given an efficient message passing algorithm, an incremental process can be used to design an efficient SIMPLE algorithm.
Reference: [4] <author> D.A. Bader, D.R. Helman, and J. JaJa. </author> <title> Practical Parallel Algorithms for Personalized Communication and Integer Sorting. </title> <institution> CS-TR-3548 and UMIACS-TR-95-101 Technical Report, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Once the rank of each key is known, we can move each key into its correct position using a permutation ( n p -relation) routing <ref> [4, 5] </ref>, whereby no node is the source or destination of more than n p keys. Counting Sort is a stable sorting routine, that is, if two keys are identical, their relative order in the final sort remains the same as their initial order.
Reference: [5] <author> D.A. Bader, D.R. Helman, and J. JaJa. </author> <title> Practical Parallel Algorithms for Personalized Communication and Integer Sorting. </title> <journal> ACM Journal of Experimental Algorithmics, </journal> <volume> 1(3) </volume> <pages> 1-42, </pages> <year> 1996. </year> <note> http://www.jea.acm.org/1996/BaderPersonalized/. </note>
Reference-contexts: Suppose instead that each node contains a set of messages, each message holding a destination tag, such that no node sends or receives more than h messages [28]. The resulting h-relation personalized communication <ref> [5] </ref> is a useful communication routine used in a variety of parallel algorithms. <p> However, if significant imbalance exists, an alternative algorithm might replace the one-phase data routing in Step (4) with a two-phase routing approach using balanced Alltoall primitives in each phase (see <ref> [5] </ref>). Similarly, other complex communication algorithms can be developed using the SIMPLE methodology. The above permutation algorithm minimizes the number of communication steps, which is optimal on our COSMOS testbed where communication is expensive compared with local computation. <p> We chose the technique of radix sort since it is well known for sequential programming, but efficient methods for solving this problem on clusters of SMPs are not. The SIMPLE approach for radix sort is similar to our efficient message passing algorithm <ref> [5] </ref>, except when applicable, shared memory computation replaces sequential node work, and communication uses the improved SIMPLE communication library. Consider the problem of sorting n integer keys in the range [0; M 1] that are distributed equally in the shared memories of a p-node cluster of r-way SMPs. <p> Once the rank of each key is known, we can move each key into its correct position using a permutation ( n p -relation) routing <ref> [4, 5] </ref>, whereby no node is the source or destination of more than n p keys. Counting Sort is a stable sorting routine, that is, if two keys are identical, their relative order in the final sort remains the same as their initial order. <p> The performance of this DSM radix sort is given in Figure 14. In addition, we took an efficient message passing code for radix sort (the reader is referred to <ref> [5] </ref> for a complete analysis of the algorithm and its performance) whose performance on an IBM SP-2 is shown in Figure 12. The IBM SP-2 contains uniprocessor nodes interconnected by a fast switch. On this platform, the message passing algorithm 20 performs very well.
Reference: [6] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> Numerical Aerodynamic Simulation Facility, NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Fast integer sorting is crucial for solving problems in many domains, and as such, is used as a kernel in several parallel benchmarks such as NAS 3 <ref> [6] </ref> and SPLASH [29]. We present an efficient sorting algorithm based on our SIMPLE methodology. We chose the technique of radix sort since it is well known for sequential programming, but efficient methods for solving this problem on clusters of SMPs are not.
Reference: [7] <author> W.P. Brown. </author> <title> Parallel Computation of Atmospheric Propagation. </title> <type> Technical report, </type> <institution> Maui High Performance Computing Center and Phillips Laboratory, </institution> <address> Kihei, Maui, HI, </address> <year> 1995. </year>
Reference-contexts: A 2D-FFT computation can be reduced to 1D-FFT's by first performing 1D-FFT's across the rows, followed by 1D-FFT's down the columns, similar to the FFT algorithms in <ref> [7, 8] </ref> which performs an all-to-all transpose of the data between two phases of local computation. In fact, a k-dimensional transform can be formed by performing k (k 1)-dimensional FFTs along each axis. In Figure 15, we illustrate the major steps of the two-dimensional FFT algorithm.
Reference: [8] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: A 2D-FFT computation can be reduced to 1D-FFT's by first performing 1D-FFT's across the rows, followed by 1D-FFT's down the columns, similar to the FFT algorithms in <ref> [7, 8] </ref> which performs an all-to-all transpose of the data between two phases of local computation. In fact, a k-dimensional transform can be formed by performing k (k 1)-dimensional FFTs along each axis. In Figure 15, we illustrate the major steps of the two-dimensional FFT algorithm.
Reference: [9] <institution> Digital Equipment Corp. </institution> <note> Guide to DECthreads. </note> <institution> Maynard, </institution> <address> MA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Experimental results are provided from implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 [22], MPICH 1.0.13 [12], or CHIMP 2.1.1c [1]) and POSIX threads (DECthreads <ref> [9] </ref> or freely available pthreads implementations [25, 20]) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [10] <institution> Digital Equipment Corporation, Maynard, MA. Digital UNIX (formerly OSF/1), </institution> <address> v3.2c edition, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Our model is simply implemented using a portable thread package called POSIX threads (pthreads), which is a standard (IEEE Std. 1003.1c), supplied with POSIX 1.c ([24, 27]). Note that pthreads are also available in the "standard" Distributed Computing Environment (DCE) used in operating systems such as OSF <ref> [10] </ref> and AIX [15]. A Possible Approach The latency for message passing is an order of magnitude higher than accessing local memory. Thus, the most costly operation in a SIMPLE algorithm is internode communication, and algorithmic design must attempt to minimize the communication costs between the nodes.
Reference: [11] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical report, </type> <institution> Silicon Graphics Computer Systems, Mountain View, </institution> <address> CA, </address> <month> May </month> <year> 1994. </year> <pages> 10 pp. </pages> <note> Available from ftp://ftp.sgi.com/sgi/whitepaper/challenge paper.ps.Z. </note>
Reference-contexts: It can be argued that 1) many future workstations will be SMPs with more than one processor, and 2) SMP nodes will be the basis of workstation clusters. There are already several examples of clusters of SMPs, such as clusters of DEC AlphaServer [14], SGI Challenge/PowerChallenge <ref> [11] </ref>, or Sun Ultra HPC machines, and the IBM SP system with SMP "High" nodes [16, 13]. With the acceptance of message passing standards such as MPI [19], it has become easier to design portable parallel algorithms making use of these primitives.
Reference: [12] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A High-Performance, Portable Implementation of the MPI Message Passing Interface Standard. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1996. </year> <note> http://www.mcs.anl.gov/mpi/mpich/. </note>
Reference-contexts: Experimental results are provided from implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 [22], MPICH 1.0.13 <ref> [12] </ref>, or CHIMP 2.1.1c [1]) and POSIX threads (DECthreads [9] or freely available pthreads implementations [25, 20]) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [13] <author> C. Harris. </author> <title> Node Selection for the IBM RS/6000 SP System. </title> <type> Version 2.1. </type> <institution> IBM RS/6000 Division, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: There are already several examples of clusters of SMPs, such as clusters of DEC AlphaServer [14], SGI Challenge/PowerChallenge [11], or Sun Ultra HPC machines, and the IBM SP system with SMP "High" nodes <ref> [16, 13] </ref>. With the acceptance of message passing standards such as MPI [19], it has become easier to design portable parallel algorithms making use of these primitives.
Reference: [14] <author> F.M. Hayes. </author> <title> Design of the AlphaServer Multiprocessor Server Systems. </title> <journal> Digital Technical Journal, </journal> <volume> 6(3) </volume> <pages> 8-19, </pages> <month> Summer </month> <year> 1994. </year>
Reference-contexts: It can be argued that 1) many future workstations will be SMPs with more than one processor, and 2) SMP nodes will be the basis of workstation clusters. There are already several examples of clusters of SMPs, such as clusters of DEC AlphaServer <ref> [14] </ref>, SGI Challenge/PowerChallenge [11], or Sun Ultra HPC machines, and the IBM SP system with SMP "High" nodes [16, 13]. With the acceptance of message passing standards such as MPI [19], it has become easier to design portable parallel algorithms making use of these primitives. <p> Each Alpha chip has two separate data and instruction on-chip caches. Both on-chip caches are 16 KB, but the instruction cache is direct mapped, while the data cache is two-way set-associative. In addition, each CPU has a 4 MB backup (L2) cache. <ref> [14] </ref> All CPUs communicate via a 128-bit system bus which connects the four CPU modules to a shared memory up to 2 GB in size. 9 Future Work The future research directions of the SIMPLE project can be categorized into two areas: methodology and algorithmics.
Reference: [15] <institution> IBM Corporation. </institution> <note> AIX DCE Base Services/6000 Version 1.2, 7 edition, </note> <month> October </month> <year> 1993. </year> <month> 34 </month>
Reference-contexts: Note that pthreads are also available in the "standard" Distributed Computing Environment (DCE) used in operating systems such as OSF [10] and AIX <ref> [15] </ref>. A Possible Approach The latency for message passing is an order of magnitude higher than accessing local memory. Thus, the most costly operation in a SIMPLE algorithm is internode communication, and algorithmic design must attempt to minimize the communication costs between the nodes.
Reference: [16] <institution> IBM Corporation. RS/6000 SP System. RS/6000 Division, </institution> <year> 1997. </year>
Reference-contexts: There are already several examples of clusters of SMPs, such as clusters of DEC AlphaServer [14], SGI Challenge/PowerChallenge [11], or Sun Ultra HPC machines, and the IBM SP system with SMP "High" nodes <ref> [16, 13] </ref>. With the acceptance of message passing standards such as MPI [19], it has become easier to design portable parallel algorithms making use of these primitives.
Reference: [17] <author> P. Keleher. CVM: </author> <title> The Coherent Virtual Machine. </title> <institution> University of Maryland, </institution> <address> 0.1 edition, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Programming methodologies for COSMOS fall into two categories. The first, distributed shared memory (DSM) systems (for example, TreadMarks [2] from Rice University, Multigrain Shared Memory (MGS) [30] from MIT and Coherent Virtual Machine (CVM) <ref> [17] </ref> from University of Maryland), provides a software layer which simulates coherent shared memory between nodes by internally using messaging to move around specific data or referenced memory pages. <p> As we claim in the introduction, software distributed shared memory and message passing algorithms are not optimal for COSMOS platforms. For instance, we ported an efficient SMP radix sort code into a software distributed shared memory package called Coherent Virtual Machine (CVM, version 0.1) <ref> [17] </ref> which is an extension of the commercial TreadMarks [2] DSM implementation. The performance of this DSM radix sort is given in Figure 14.
Reference: [18] <author> D.E. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching, volume 3. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Radix sort decomposes each key into groups of -bit digits, for a suitably chosen , and sorts the keys by applying a counting sort routine on each of the -bit digits beginning with the digit containing the least significant bit positions <ref> [18] </ref>. Let R = 2 p.
Reference: [19] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: There are already several examples of clusters of SMPs, such as clusters of DEC AlphaServer [14], SGI Challenge/PowerChallenge [11], or Sun Ultra HPC machines, and the IBM SP system with SMP "High" nodes [16, 13]. With the acceptance of message passing standards such as MPI <ref> [19] </ref>, it has become easier to design portable parallel algorithms making use of these primitives. <p> The second, based on message passing primitives (for example, MPI <ref> [19] </ref>), enforces a shared-nothing paradigm between tasks, and all communication and coordination between tasks are performed through the exchange of explicit messages, even between tasks on a node with physically shared memory.
Reference: [20] <author> F. Muller. </author> <title> A Library Implementation of POSIX Threads under UNIX. </title> <booktitle> In Proceedings of the 1993 Winter USENIX Conference, </booktitle> <pages> pages 29-41, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year> <note> http://www.informatik.hu-berlin.de/~mueller/projects.html. </note>
Reference-contexts: implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 [22], MPICH 1.0.13 [12], or CHIMP 2.1.1c [1]) and POSIX threads (DECthreads [9] or freely available pthreads implementations <ref> [25, 20] </ref>) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [21] <institution> Netlib Repository for mathematical software, </institution> <note> papers, and databases. </note> <institution> University of Tennessee and Oak Ridge National Laboratory. </institution> <note> http://www.netlib.org/. </note>
Reference-contexts: We begin with an efficient message passing algorithm for the FFT. The one-dimensional FFT used in the first and last steps is a benchmark kernel from netlib <ref> [21] </ref>. As shown in Figure 16, the message passing implementation performs very well on the IBM SP-2. When we fix a problem size and double the number of processors, the execution time scales appropriately.
Reference: [22] <institution> Ohio Supercomputer Center. LAM / MPI Parallel Computing. The Ohio State University, Columbus, OH, </institution> <year> 1995. </year> <note> http://www.osc.edu/lam.html. </note>
Reference-contexts: Experimental results are provided from implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 <ref> [22] </ref>, MPICH 1.0.13 [12], or CHIMP 2.1.1c [1]) and POSIX threads (DECthreads [9] or freely available pthreads implementations [25, 20]) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [23] <author> G.F. Pfister. </author> <title> In Search of Clusters. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: 1 Problem Overview With the cost of commercial off-the-shelf (COTS) high performance interconnects falling and the respective performance of microprocessors increasing, workstation clusters have become an attractive computing platform offering potentially a superior cost effective performance <ref> [23] </ref>. Indeed, this trend highly leverages both workstation-focused technologies including systems software and networking infrastructure, for example, COTS networks (e.g. Ethernet, Myrinet, FDDI, or ATM).
Reference: [24] <author> Portable Applications Standards Committee of the IEEE. </author> <title> Information technology Portable Operating System Interface (POSIX) Part 1: System Application Program Interface (API), </title> <address> 1996-07-12 edition, 1996. ISO/IEC 9945-1, ANSI/IEEE Std. 1003.1. </address>
Reference: [25] <author> C. Provenzano. Proven Pthreads. </author> <note> WWW page., 1995. http://www.mit.edu/people/proven/pthreads.html. </note>
Reference-contexts: implementations on a cluster of DEC AlphaServer 2100 4/275 nodes each with a DEC (OC-3c) 155.52 Mbps PCI card connected to a DEC Gigaswitch/ATM switch, and using the MPI (e.g., LAM 6.1 [22], MPICH 1.0.13 [12], or CHIMP 2.1.1c [1]) and POSIX threads (DECthreads [9] or freely available pthreads implementations <ref> [25, 20] </ref>) packages. Finally, Section 9 presents a direction for future work. 2 The SIMPLE Parallel Computation Methodology We use a simple paradigm for designing efficient and portable parallel algorithms.
Reference: [26] <author> W. Saphir, A. Woo, and M. Yarrow. </author> <title> The NAS Parallel Benchmarks 2.1 Results. Report NAS-96-010, Numerical Aerodynamic Simulation Facility, </title> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: The second, based on message passing primitives (for example, MPI [19]), enforces a shared-nothing paradigm between tasks, and all communication and coordination between tasks are performed through the exchange of explicit messages, even between tasks on a node with physically shared memory. For example, the model assumed in <ref> [26] </ref> is that each processor in the cluster will be assigned a message passing (MPI-level) 1 cosmos ('kaz-mos) noun Greek kosmos c. 1650 1: an orderly harmonious systematic universe 2: a complex orderly self-inclusive system 3: Cluster Of Shared Memory Nodes 2 Message Passing Algorithm SIMPLE Algorithm computation phases.
Reference: [27] <author> Sun Microsystems, Inc. </author> <title> POSIX Threads. </title> <note> WWW page., 1995. http://www.sun.com/developer-products/sig/threads/posix.html. </note>
Reference: [28] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: Suppose instead that each node contains a set of messages, each message holding a destination tag, such that no node sends or receives more than h messages <ref> [28] </ref>. The resulting h-relation personalized communication [5] is a useful communication routine used in a variety of parallel algorithms.
Reference: [29] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Fast integer sorting is crucial for solving problems in many domains, and as such, is used as a kernel in several parallel benchmarks such as NAS 3 [6] and SPLASH <ref> [29] </ref>. We present an efficient sorting algorithm based on our SIMPLE methodology. We chose the technique of radix sort since it is well known for sequential programming, but efficient methods for solving this problem on clusters of SMPs are not.
Reference: [30] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> A Multigrain Shared Memory System. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <month> 35 </month>
Reference-contexts: Programming methodologies for COSMOS fall into two categories. The first, distributed shared memory (DSM) systems (for example, TreadMarks [2] from Rice University, Multigrain Shared Memory (MGS) <ref> [30] </ref> from MIT and Coherent Virtual Machine (CVM) [17] from University of Maryland), provides a software layer which simulates coherent shared memory between nodes by internally using messaging to move around specific data or referenced memory pages.
References-found: 30

