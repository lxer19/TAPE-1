URL: http://www.cs.iastate.edu/~yang/Papers/bookfinal.ps
Refering-URL: http://www.cs.iastate.edu/~yang/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: yangjhonavar-@cs.iastate.edu  
Title: 1 FEATURE SUBSET SELECTION USING A GENETIC ALGORITHM time needed for learning a sufficiently accurate
Author: Jihoon Yang and Vasant Honavar 
Note: 1.1 INTRODUCTION The  
Address: 226 Atanasoff Hall  Ames, IA 50011 U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University  
Abstract: Practical pattern classification and knowledge discovery problems require selection of a subset of attributes or features (from a much larger set) to represent the patterns to be classified. This is due to the fact that the performance of the classifier (usually induced by some learning algorithm) and the cost of classification are sensitive to the choice of the features used to construct the classifier. Exhaustive evaluation of possible feature subsets is usually infeasible in practice because of the large amount of computational effort required. Genetic algorithms, which belong to a class of randomized heuristic search techniques, offer an attractive approach to find near-optimal solutions to such optimization problems. This paper presents an approach to feature subset selection using a genetic algorithm. Some advantages of this approach include the ability to accommodate multiple criteria such as accuracy and cost of classification into the feature selection process and to find feature subsets that perform well for particular choices of the inductive learning algorithm used to construct the pattern classifier. Our experiments with several benchmark real-world pattern classification problems demonstrate the feasibility of this approach to feature subset selection in the automated Many practical pattern classification tasks (e.g., medical diagnosis) require learning of an appropriate classification function that assigns a given input pattern (typically represented using a vector of attribute or feature values) to one of a finite set of classes. The choice of features, attributes, or measurements used to represent patterns that are presented to a classifier affect (among other things): The accuracy of the classification function that can be learned using an inductive learning algorithm (e.g., a decision tree induction algorithm or a neural network learning algorithm): The features used to describe the patterns implicitly define a pattern language. If the language is not expressive enough, it would fail to capture the information that is necessary for classification and hence regardless of the learning algorithm used, the accuracy of the classification function learned would be limited by this lack of information. design of neural networks for pattern classification and knowledge discovery.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H. and Dietterich, T. </author> <year> (1994). </year> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, 69(1-2):279-305. </journal>
Reference: <author> Balakrishnan, K. and Honavar, V. </author> <year> (1995). </year> <title> Properties of genetic representations of neural architectures. </title> <booktitle> In Proceedings of WCNN'95 July 17-21 Washington D.C., </booktitle> <volume> volume 1, </volume> <pages> pages 807-813. </pages>
Reference: <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996a). </year> <title> Analysis of neurocontrollers designed by simulated evolution. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <address> Washington, D.C. </address>
Reference: <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996b). </year> <title> On sensor evolution in robotics. </title> <editor> In Koza, Goldberg, Fogel, and Riolo, editors, </editor> <booktitle> Proceedings of the 1996 Genetic Programming Conference - GP-96, </booktitle> <pages> pages 455-460. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Balakrishnan, K. and Honavar, V. </author> <year> (1996c). </year> <title> Some experiments in the evolutionary synthesis of robotic neurocontrollers. </title> <booktitle> In Proceedings of the World Congress on Neural Networks (WCNN'96), </booktitle> <pages> pages 1035-1040, </pages> <address> San Diego, CA. </address>
Reference: <author> Banzaf, W., Nordin, P., Keller, R., and Francone, F. </author> <year> (1997). </year> <title> Genetic Programming An Introduction. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Brill, F., Brown, D., and Martin, W. </author> <year> (1992). </year> <title> Fast genetic selection of features for neural network classifiers. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 324-328. </pages>
Reference: <author> Caruana, R. and Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 28-36, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cormen, T., Leiserson, C., and Rivest, R. </author> <year> (1990). </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78. </pages> <note> 16 Cover, </note> <author> T. and Hart, P. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27. </pages>
Reference: <author> Dasarathy, B. </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techiniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference: <author> Dash, M. and Liu, H. </author> <year> (1997). </year> <title> Feature selection for classification. </title> <journal> Intelligent Data Analysis, </journal> <volume> 1(3). </volume>
Reference: <author> Devijver, P. </author> <year> (1982). </year> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Diday, E. </author> <year> (1974). </year> <title> Recent progress in distance and similarity measures in pattern recognition. </title> <booktitle> In Proceedings of the Second International Joint Conference on Pattern Recognition, </booktitle> <pages> pages 534-539. </pages>
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security. </title> <type> Technical Report CSE-92-18, </type> <institution> Department of Computer Science, University of California, Davis, </institution> <address> CA. </address>
Reference: <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Fogel, D. </author> <year> (1995). </year> <title> Evolutionary Computation: Toward a New Philosophy of Machine Intelligence. </title> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Foroutan, I. and Sklansky, J. </author> <year> (1987). </year> <title> Feature selection for automatic classification of non-gaussian data. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 17 </volume> <pages> 187-198. </pages>
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Gallant, S. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> New York. </address>
Reference: <author> Guo, Z. </author> <year> (1992). </year> <title> Nuclear Power Plant Fault Diagnostics and Thermal Performance Studies Using Neural Networks and Genetic Algorithms. </title> <type> PhD thesis, </type> <institution> University of Tennessee, Knoxville, TN. </institution>
Reference: <author> Guo, Z. and Uhrig, R. </author> <year> (1992). </year> <title> Using genetic algorithms to select inputs for neural networks. </title> <booktitle> In Proceedings of COGANN'92, </booktitle> <pages> pages 223-234. </pages>
Reference: <author> Hassoun, M. </author> <year> (1995). </year> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Boston, MA. </address>
Reference: <author> Holland, J. </author> <year> (1992). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Honavar, V. </author> <year> (1994). </year> <title> Toward learning systems that integrate multiple strategies and representations. </title> <editor> In Honavar, V. and Uhr, L., editors, </editor> <booktitle> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, </booktitle> <pages> pages 615-644. </pages> <publisher> Academic Press: </publisher> <address> New York. </address>
Reference: <author> Honavar, V. </author> <year> (1998a). </year> <title> Machine learning: Principles and applications. </title> <editor> In Webster, J., editor, </editor> <booktitle> Encyclopedia of Electrical and Electronics Engineering. </booktitle> <publisher> Wiley, </publisher> <address> New York. </address> <note> To appear. </note>
Reference: <author> Honavar, V. </author> <year> (1998b). </year> <title> Structural learning. </title> <editor> In Webster, J., editor, </editor> <booktitle> Encyclopedia of Electrical and Electronics Engineering. </booktitle> <publisher> Wiley, </publisher> <address> New York. </address> <note> To appear. </note>
Reference: <author> Honavar, V. and Uhr, L. </author> <year> (1993). </year> <title> Generative learning structures for generalized connectionist networks. </title> <journal> Information Sciences, 70(1-2):75-108. </journal>
Reference: <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 121-129, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Keeney, R. and Raiffa, H. </author> <year> (1976). </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Kira, K. and Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 249-256. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1994). </year> <title> Feature subset selection as search with probabilistic estimates. </title> <booktitle> In AAAI Fall Symposium on Relevance. </booktitle>
Reference: <author> Kohavi, R. and Frasca, B. </author> <year> (1994). </year> <title> Useful feature subsets and rough set reducts. </title> <booktitle> In Third International Workshop on Rough Sets and Soft Computing. </booktitle>
Reference: <author> Koller, D. and Sahami, M. </author> <year> (1996). </year> <title> Toward optimal feature selection. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Koller, D. and Sahami, M. </author> <year> (1997). </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 170-178. </pages>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extension of relief. </title> <booktitle> In Proceedings of European Conference on Mahcine Learning, </booktitle> <pages> pages 171-182. </pages>
Reference: <author> Kothari, R. and Agyepong, K. </author> <year> (1996). </year> <title> On lateral connections in feed-forward neural networks. </title> <booktitle> In Proceedings of the International Conference on Neural Networks, </booktitle> <month> pages 13-18. </month> <title> FEATURE SUBSET SELECTION USING A GENETIC ALGORITHM 17 Koza, </title> <editor> J. </editor> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Langley, P. </author> <year> (1994). </year> <title> Selection of relevant features in machine learning. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 1-5, </pages> <address> New Orleans, LA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Langley, P. </author> <year> (1995). </year> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Liu, H. and Setiono, R. </author> <year> (1996a). </year> <title> Feature selection and classification a probabilistic wrapper approach. </title> <booktitle> In Proceedings of the Ninth International Conference on Industrial and Engineering Applications of AI and ES. </booktitle>
Reference: <author> Liu, H. and Setiono, R. </author> <year> (1996b). </year> <title> A probabilistic approach to feature selection a filter solution. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Michalewicz, Z. </author> <year> (1996). </year> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer-Verlag, </publisher> <address> New York, third edition. </address>
Reference: <author> Mitchell, M. </author> <year> (1996). </year> <title> An Introduction to Genetic algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <address> New York. </address>
Reference: <author> Modrzejewski, M. </author> <year> (1993). </year> <title> Feature selection using rough sets theory. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 213-226. </pages> <publisher> Springer. </publisher>
Reference: <author> Motwani, R. and Raghavan, P. </author> <year> (1996). </year> <title> Randomized algorithms. </title> <journal> ACM Computing Surveys, </journal> <volume> 28(1) </volume> <pages> 33-37. </pages>
Reference: <author> Mucciardi, A. and Gose, E. </author> <year> (1971). </year> <title> A comparison of seven techniques for choosing subsets of pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 20 </volume> <pages> 1023-1031. </pages>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> Repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. </address>
Reference: <author> Narendra, P. and Fukunaga, K. </author> <year> (1977). </year> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 26 </volume> <pages> 917-922. </pages>
Reference: <author> Parekh, R., Yang, J., and Honavar, V. </author> <year> (1997a). </year> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical Report ISU-CS-TR97-06, </type> <institution> Department of Computer Science, Iowa State University. </institution>
Reference: <author> Parekh, R., Yang, J., and Honavar, V. </author> <year> (1997b). </year> <title> MUpstart a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the IEEE/INNS International Conference on Neural Networks, ICNN'97, </booktitle> <pages> pages 1924-1929. </pages>
Reference: <author> Pawlak, Z. </author> <year> (1991). </year> <title> Rough Sets, Theoretical Aspects of Reasoning about Data. </title> <publisher> Kluwer Academic. </publisher>
Reference: <author> Punch, W., Goodman, E., Pei, M., Chia-Shun, L., Hovland, P., and Enbody, R. </author> <year> (1993). </year> <title> Further research on feature selection and classification using genetic algorithms. </title> <booktitle> In Proceedings of the International Conference on Genetic Algorithms, </booktitle> <pages> pages 557-564. </pages> <publisher> Springer. </publisher>
Reference: <author> Quinlan, R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Richeldi, M. and Lanzi, P. </author> <year> (1996). </year> <title> Performing effective feature selection by investigating the deep structure of the data. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 379-383. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Ripley, B. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modelling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Russell, S. and Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> En-glewood Cliffs, NJ. </address>
Reference: <author> Salton, G. and McGill, M. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill, </publisher> <address> New York. </address>
Reference: <author> Schlimmer, J. </author> <year> (1993). </year> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optmal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 284-290, </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sheinvald, J., Dom, B., and Niblack, W. </author> <year> (1990). </year> <title> A modelling approach to feature selection. </title> <booktitle> In Proceedings of the Tenth International Conference on Pattern Recognition, </booktitle> <pages> pages 535-539. </pages>
Reference: <author> Siedlecki, W. and Sklansky, J. </author> <year> (1988). </year> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition, </journal> <volume> 2 </volume> <pages> 197-220. </pages>
Reference: <author> Siedlecki, W. and Sklansky, J. </author> <year> (1989). </year> <title> A note on genetic algorithms for large-scale feature selection. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 10 </volume> <pages> 335-347. </pages> <address> 18 Skalak, D. </address> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill-climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 293-301, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vafaie, H. and De Jong, K. </author> <year> (1993). </year> <title> Robust feature selection algorithms. </title> <booktitle> In Proceedings of the IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 356-363. </pages>
Reference: <author> Wettschereck, D., Aha, D., and Mohri, T. </author> <year> (1995). </year> <title> A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. </title> <type> Technical Report AIC95-012, </type> <institution> Naval Research Laboratory, Navy Center for Applied Research in Artificial Intelligence, </institution> <address> Washington, D.C. </address>
Reference: <author> Yang, J., Pai, P., Honavar, V., and Miller, L. </author> <year> (1998a). </year> <title> Mobile intelligent agents for document classification and retrieval: A machine learning approach. </title> <booktitle> In 14th European Meeting on Cybernetics and Systems Research. Symposium on Agent Theory to Agent Implementation, </booktitle> <address> Vienna, Austria. </address>
Reference: <author> Yang, J., Parekh, R., and Honavar, V. </author> <year> (1996). </year> <title> MTiling a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the World Congress on Neural Networks'96, </booktitle> <pages> pages 182-187, </pages> <address> San Diego. </address>
Reference: <author> Yang, J., Parekh, R., and Honavar, V. </author> <year> (1998b). </year> <title> DistAl: An inter-pattern distance-based constructive learning algorithm. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Anchorage, Alaska. </address> <note> To appear. </note>

References-found: 69

