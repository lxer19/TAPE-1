URL: ftp://ftp.cs.unc.edu/pub/users/manocha/PAPERS/EQUATIONS/survey.ps.gz
Refering-URL: http://www.cs.unc.edu/~geom/chron.html
Root-URL: http://www.cs.unc.edu
Email: fmanocha,krishnasg@cs.unc.edu  
Title: Solving Algebraic Systems using Matrix Computations  
Author: Dinesh Manocha Shankar Krishnan 
Address: Chapel Hill, NC 27599  
Affiliation: Department of Computer Science University of North Carolina  
Abstract: Finding zeros of algebraic sets is a fundamental problem in scientific and geometric computation. It arises in symbolic and numeric techniques used to manipulate sets of polynomial equations. In this paper, we outline algorithms and applications for solving zero and one dimensional algebraic sets using matrix computations. These algorithms make use of techniques from elimination theory and reduce the problem to finding singular sets of matrix polynomials. We make use of algorithms for eigendecomposition, singular value decomposition and Gaussian elimination to compute the singular sets. These algorithms have been implemented and perform very well in practice. We describe their application to computing conformations of molecular chains, inverse kinematics of serial robots, solid modeling and manufacturing. 
Abstract-found: 1
Intro-found: 1
Reference: [AB88] <author> S.S. Abhyankar and C. Bajaj. </author> <title> Computations with algebraic curves. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> volume 358, </volume> <pages> pages 279-284. </pages> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Based on a classic theorem in algebraic geometry, an algebraic plane curve birationally equivalent to the space curve is computed using elimination techniques. Given an algebraic plane curve, techniques for desingularisation based on quadratic transformations are given in <ref> [Wal50, Abh90, AB88, Joh87] </ref>. These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves [Arn83, SS83].
Reference: [ABB + 92] <author> E. Anderson, Z. Bai, C. Bischof, J. Dem-mel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, and D. Sorensen. </author> <note> LAPACK User's Guide, Release 1.0. SIAM, Philadel-phia, </note> <year> 1992. </year>
Reference-contexts: If the system corresponds to a set of linear equations, good algorithms and their implementations, in the form of linear algebra packages like LINPACK [BDMS79] and LAPACK <ref> [ABB + 92] </ref>, are known and widely used. At the same time, a great deal of work has been done in the symbolic and numeric literature for finding roots of a univariate polynomial and good algorithms are known based on Sturm sequences, Uspensky's methods, Jenkins-Traub algorithm etc. <p> As a result, the eigenvalues of a diagonal matrix, upper triangular matrix or a lower triangular matrix correspond to the elements on its diagonal. Efficient algorithms for computing eigenvalues and eigenvectors are well known, [GL89], and their implementations are available as part of packages EISPACK, [GBDM77], and LAPACK <ref> [ABB + 92] </ref>. 2.2.2 Generalized Eigenvalue Problem Given n fi n matrices, A and B, the generalized eigenvalue problem corresponds to solving Ax = sBx: We represent this problem as eigenvalues of A sB. The vectors x 6= 0 correspond to the eigenvectors of this equation.
Reference: [Abh90] <author> S. S. Abhyankar. </author> <title> Algebraic Geometry for Scientists and Engineers. </title> <publisher> American Mathematical Society, Providence, </publisher> <editor> R. I., </editor> <year> 1990. </year>
Reference-contexts: Based on a classic theorem in algebraic geometry, an algebraic plane curve birationally equivalent to the space curve is computed using elimination techniques. Given an algebraic plane curve, techniques for desingularisation based on quadratic transformations are given in <ref> [Wal50, Abh90, AB88, Joh87] </ref>. These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves [Arn83, SS83].
Reference: [AF88] <author> S. Arnborg and H. Feng. </author> <title> Algebraic decomposition of regular curves. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 5 </volume> <pages> 131-140, </pages> <year> 1988. </year>
Reference-contexts: Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves [Arn83, SS83]. Its worst case complexity is doubly exponential in the number of variables. For plane curves, improved polynomial time algorithms based on CAD have been presented in <ref> [AF88] </ref>. However the exponent in terms of N (the degree of the curve) is rather high. Furthermore, these algorithms are implemented using exact arithmetic, which makes them slow and memory intensive in practice.
Reference: [Arn83] <author> D. S. Arnon. </author> <title> Topologically reliable display of algebraic curves. </title> <journal> Computer Graphics, </journal> <volume> 17 </volume> <pages> 219-227, </pages> <year> 1983. </year>
Reference-contexts: These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves <ref> [Arn83, SS83] </ref>. Its worst case complexity is doubly exponential in the number of variables. For plane curves, improved polynomial time algorithms based on CAD have been presented in [AF88]. However the exponent in terms of N (the degree of the curve) is rather high.
Reference: [AS86] <author> W. Auzinger and H.J. Stetter. </author> <title> An elimination algorithm for the computation of all zeros of a system of multivariate polynomial equations. </title> <booktitle> In International Series of Numerical Mathematics, </booktitle> <volume> volume 86, </volume> <pages> pages 11-30, </pages> <year> 1986. </year>
Reference: [BA82] <author> Ulrich Burkert and Norman Allinger. </author> <title> Molecular Mechanics. ACS Monographs. </title> <publisher> Americal Chemical Society, </publisher> <address> Washington, D.C., </address> <year> 1982. </year>
Reference-contexts: These three dimensional configurations are called conformers. Finding conformers corresponding to minimal energy configuration involves three dimensional manipulation, given the position and orientation of the molecular chain [GoS70]. This is also useful for protein folding applications. We model the molecular chain as a rigid structure <ref> [BA82] </ref>. Robot Kinematics: We initially use the terminology from the robotics and mechanics literature to formulate the problem. The geometric problem in molecular modeling is similar, except it is described using chemical bonds, bond angles and dihedral angles (as opposed to links, length of the links, joint angles etc.). <p> As a result we are interested in calculation of changes in dihedral angles, which cause local deformations of conformations in long polymer chains <ref> [GoS70, BA82] </ref>. We use the Denavit-Hartenberg (DH) notation to model a molecular chain. The algorithm proceeds by assigning coordinate systems and computing the DH parameters of a local chain. In Fig. 5.1, we highlight the DH formulation for a peptide unit and the corresponding parameters are shown in Fig. 6.
Reference: [BDM89] <author> Z. Bai, J. Demmel, and A. McKenney. </author> <title> On the conditioning of the nonsymmetric eigen-problem: Theory and software. </title> <institution> Computer Science Dept. </institution> <type> Technical Report 469, </type> <institution> Courant Institute, </institution> <address> New York, NY, </address> <month> October </month> <year> 1989. </year> <note> (LA-PACK Working Note #13). </note>
Reference-contexts: However, in many cases it is possible to identify higher multiplicity eigenvalues of a matrix by identifying clusters of eigenvalues and using the knowledge of the condition number of the clusters <ref> [BDM89] </ref>. More details of its application to finding solutions of polynomial equations are given in [Man92]. Such analysis is well developed for eigenvalues of a matrix and no equivalent analysis is known for higher multiplicity roots of a polynomial.
Reference: [BDMS79] <author> J. Bunch, J. Dongarra, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: If the system corresponds to a set of linear equations, good algorithms and their implementations, in the form of linear algebra packages like LINPACK <ref> [BDMS79] </ref> and LAPACK [ABB + 92], are known and widely used. At the same time, a great deal of work has been done in the symbolic and numeric literature for finding roots of a univariate polynomial and good algorithms are known based on Sturm sequences, Uspensky's methods, Jenkins-Traub algorithm etc.
Reference: [Ber75] <author> D. N. Bernshtein. </author> <title> The number of roots of a system of equations. </title> <journal> Funktsional'nyi Analiz i Ego Prilozheniya, </journal> <volume> 9(3) </volume> <pages> 1-4, </pages> <year> 1975. </year>
Reference-contexts: A bound derived from the set of non-zero coefficients is called a Bernstein bound. This bound is defined in terms of the mixed volumes of the newton polytopes corresponding to each equation. Bernstein showed that his bound is exact if all the coefficients of the polynomial system are generic <ref> [Ber75] </ref>. Resultant formulations in terms of matrices and determinants based on Bernstein bound have appeared in the literature as well [Stu91, SZ94, CE93]. Given a system of polynomial equations, sparse or dense, it is possible to express their resultant in terms of matrices and determinants.
Reference: [BGW88] <author> C. Bajaj, T. Garrity, and J. Warren. </author> <title> On the applications of multi-equational resultants. </title> <type> Technical Report CSD-TR-826, </type> <institution> Department of Computer Science, Purdue University, </institution> <year> 1988. </year>
Reference: [Can88] <author> J.F. Canny. </author> <title> The Complexity of Robot Motion Planning. ACM Doctoral Dissertation Award. </title> <publisher> MIT Press, </publisher> <year> 1988. </year> <month> 16 </month>
Reference-contexts: Many a time both determinants evaluate to zero. To compute the resultant we need to perturb the equations and use limiting arguments. This corresponds to computing the characteristic polynomials of both the determinants <ref> [Can88] </ref>. Many special cases, corresponding to n = 2; 3; 4; 5; 6 when the resultant can be expressed as the determinant of a matrix, are given in [Dix08, Jou91, Mor25, MC27]. Most of the formulation presented in the classical literature correspond to computing the resultants of dense polynomial systems.
Reference: [CE93] <author> J. Canny and I. Emiris. </author> <title> An efficient algo-rithm for the sparse mixed resultant. </title> <booktitle> In Proceedings of AAECC, </booktitle> <pages> pages 89-104. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Bernstein showed that his bound is exact if all the coefficients of the polynomial system are generic [Ber75]. Resultant formulations in terms of matrices and determinants based on Bernstein bound have appeared in the literature as well <ref> [Stu91, SZ94, CE93] </ref>. Given a system of polynomial equations, sparse or dense, it is possible to express their resultant in terms of matrices and determinants.
Reference: [Col75] <author> G.E. Collins. </author> <title> Quantifier elimination for real closed fields by cylindrical algebraic decomposition. </title> <booktitle> In Lecture Notes in Computer Science, number 33, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1975. </year>
Reference-contexts: These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), <ref> [Col75] </ref>, have been used for evaluating all components of algebraic curves [Arn83, SS83]. Its worst case complexity is doubly exponential in the number of variables. For plane curves, improved polynomial time algorithms based on CAD have been presented in [AF88].
Reference: [Dix08] <author> A.L. Dixon. </author> <title> The eliminant of three quantics in two independent variables. </title> <journal> Proceedings of Lon-don Mathematical Society, </journal> <volume> 6 </volume> <pages> 49-69, 209-236, </pages> <year> 1908. </year>
Reference-contexts: This corresponds to computing the characteristic polynomials of both the determinants [Can88]. Many special cases, corresponding to n = 2; 3; 4; 5; 6 when the resultant can be expressed as the determinant of a matrix, are given in <ref> [Dix08, Jou91, Mor25, MC27] </ref>. Most of the formulation presented in the classical literature correspond to computing the resultants of dense polynomial systems. More recently, resultants of sparse polynomial systems have received a lot attention in a newly developed area sparse elimination theory. <p> Almost all the projections are one-to-one and result in a birationally equivalent curve. In our case, we perform a generic linear transformation and eliminate w 1 ; : : : ; w n1 from the resulting set. The resultant can be expressed as the determinant of a matrix polynomial <ref> [Dix08] </ref>, M (u; v) say (u and v are the two remaining variables). The algorithm evaluates the resulting algebraic set and substitutes the values back into the original equations to discard the extraneous solutions.
Reference: [FM90] <author> O.D. Faugeras and S. Maybank. </author> <title> Motion from point matches: Multiplicity of solutions. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4 </volume> <pages> 225-246, </pages> <year> 1990. </year>
Reference-contexts: Typical examples of sparse systems are those that describe the inverse kinematics for a 6R robot [Man92], forward kinematics for the Stewart platform [Mer92], camera motion from point matches <ref> [FM90] </ref>, and geometric constraints describing two-or three-dimensional objects. As the dimension of the problem increases, the difference between the sparse and non-sparse bounds increases dramatically. A large gap between the Bezout bound and the actual number of solutions is not unusual for geometric problems.
Reference: [GBDM77] <author> B.S. Garbow, J.M. Boyle, J. Dongarra, and C.B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, volume 51. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: As a result, the eigenvalues of a diagonal matrix, upper triangular matrix or a lower triangular matrix correspond to the elements on its diagonal. Efficient algorithms for computing eigenvalues and eigenvectors are well known, [GL89], and their implementations are available as part of packages EISPACK, <ref> [GBDM77] </ref>, and LAPACK [ABB + 92]. 2.2.2 Generalized Eigenvalue Problem Given n fi n matrices, A and B, the generalized eigenvalue problem corresponds to solving Ax = sBx: We represent this problem as eigenvalues of A sB. The vectors x 6= 0 correspond to the eigenvectors of this equation. <p> A better algorithm, called the QZ algorithm [GL89], applies orthogonal transformation to A and B to reduce A to Hessenberg form, to reduce B to upper triangular form, and then implicitly perform the QR algorithm on B 1 A without ever forming it. This algorithm is in EISPACK <ref> [GBDM77] </ref> and in the most recent release of LAPACK. 2.2.3 Singular Value Decomposition The singular value decomposition (SVD) is a powerful tool which gives us accurate information about matrix rank in the presence of round off errors. The rank of a matrix can also be computed by Gauss elimination.
Reference: [GKZ88] <author> I.M. Gelfand, </author> <title> M.M. Kapranov, and A.V. Zelevinsky. Equations of hypergeometric type and newton polyhedra. </title> <journal> Doklady AN SSSR, </journal> <volume> 300 </volume> <pages> 529-534, </pages> <year> 1988. </year>
Reference-contexts: More recently, resultants of sparse polynomial systems have received a lot attention in a newly developed area sparse elimination theory. In the late 1980's, Gel'fand and his colleagues began the study of discriminants and resultants of sparse polynomial systems <ref> [GKZ88] </ref>. Sparseness leads to a lowering of effective degree, and the sparse theory provides a simple direct method for proving bounds on the number of solutions. The main idea is to make use of sparseness to speedup equation solving and elimination of variables.
Reference: [GL89] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: We use this linear algebra formulation in the algorithms presented in the following sections. 2.2 Matrix Computations In this section we briefly review some techniques from linear algebra and numerical analysis. More details can be found in <ref> [GL89, Wil65] </ref>. 2.2.1 Eigenvalues and Eigenvectors Given an n fi n matrix A, its eigenvalues and eigenvectors are the solutions to the equation Ax = sx; where s is the eigenvalue and x 6= 0 is the eigenvector. <p> As a result, the eigenvalues of a diagonal matrix, upper triangular matrix or a lower triangular matrix correspond to the elements on its diagonal. Efficient algorithms for computing eigenvalues and eigenvectors are well known, <ref> [GL89] </ref>, and their implementations are available as part of packages EISPACK, [GBDM77], and LAPACK [ABB + 92]. 2.2.2 Generalized Eigenvalue Problem Given n fi n matrices, A and B, the generalized eigenvalue problem corresponds to solving Ax = sBx: We represent this problem as eigenvalues of A sB. <p> A better algorithm, called the QZ algorithm <ref> [GL89] </ref>, applies orthogonal transformation to A and B to reduce A to Hessenberg form, to reduce B to upper triangular form, and then implicitly perform the QR algorithm on B 1 A without ever forming it. <p> The oe i 's are called the singular values and columns of U and V, denoted as u i 's and v j 's, are known as the left and right singular vectors, respectively <ref> [GL89] </ref>. 2.2.4 Power Iterations The largest or the smallest eigenvalue of a matrix (and the corresponding eigenvector) can be computed using the Power method [GL89]. Power method involves multiplication of a matrix by a vector and after a few steps it converges to the largest eigenvalue of a method. <p> values and columns of U and V, denoted as u i 's and v j 's, are known as the left and right singular vectors, respectively <ref> [GL89] </ref>. 2.2.4 Power Iterations The largest or the smallest eigenvalue of a matrix (and the corresponding eigenvector) can be computed using the Power method [GL89]. Power method involves multiplication of a matrix by a vector and after a few steps it converges to the largest eigenvalue of a method. <p> In many applications we are only interested in the real solutions or solutions lying in a particular domain. The QR or QZ algorithm for eigenvalue computation returns all the eigenvalues of a given matrix and it is difficult to restrict it to eigenvalues in a particular domain <ref> [GL89] </ref>. Algorithms to compute selected eigenvalues of the these matrices based on power iterations and their structure are given in [Man94a]. Let us assume that ff 1 is a simple eigenvalue of C. <p> Therefore, we need to compute the smallest eigenvalue of the matrix C sI. The smallest eigenvalue of C sI corresponds to the largest eigenvalue of (C sI) 1 . Instead of computing the inverse explicitly (which is numerically unstable), we use inverse power iterations <ref> [GL89] </ref>. To solve the matrix system efficiently, we use LU decomposition of the matrix (C sI) using Gaussian elimination. We also make use of the structure of the matrix to reduce its complexity. Given s, let B = C sI.
Reference: [GoS70] <author> N. </author> <title> Go and H.A. Scherga. Ring closure and local conformational deformations of chain molecules. </title> <journal> Macromolecules, </journal> <volume> 3(2) </volume> <pages> 178-187, </pages> <year> 1970. </year>
Reference-contexts: These three dimensional configurations are called conformers. Finding conformers corresponding to minimal energy configuration involves three dimensional manipulation, given the position and orientation of the molecular chain <ref> [GoS70] </ref>. This is also useful for protein folding applications. We model the molecular chain as a rigid structure [BA82]. Robot Kinematics: We initially use the terminology from the robotics and mechanics literature to formulate the problem. <p> We initially model them as rigid chains using fixed bond lengths and bond angles. It has been shown that the minimal energy conformations computed by fixing these parameters is a good guess to the actual minima and can be used along with a perturbation treatment for the energy minimization <ref> [GoS70] </ref>. Given fixed bond-lengths and bond-angles, we present algorithms for two fundamental problems in conformational energy calculations: ring closure and local conformational deformations [GoS70]. These problems reduce to solving a system of algebraic equations. <p> conformations computed by fixing these parameters is a good guess to the actual minima and can be used along with a perturbation treatment for the energy minimization <ref> [GoS70] </ref>. Given fixed bond-lengths and bond-angles, we present algorithms for two fundamental problems in conformational energy calculations: ring closure and local conformational deformations [GoS70]. These problems reduce to solving a system of algebraic equations. The problem of ring closure arises when we deal with cyclic molecules, i.e. the calculation of dihedral angles which correspond to exact ring closure. <p> As a result we are interested in calculation of changes in dihedral angles, which cause local deformations of conformations in long polymer chains <ref> [GoS70, BA82] </ref>. We use the Denavit-Hartenberg (DH) notation to model a molecular chain. The algorithm proceeds by assigning coordinate systems and computing the DH parameters of a local chain. In Fig. 5.1, we highlight the DH formulation for a peptide unit and the corresponding parameters are shown in Fig. 6.
Reference: [GoS73] <author> N. </author> <title> Go and H.A. Scherga. Ring closure in chain molecules with c n , i or s 2n symmetry. </title> <journal> Macromolecules, </journal> <volume> 6(2) </volume> <pages> 273-281, </pages> <year> 1973. </year>
Reference-contexts: Since we are only concerned with main chain conformation, the bond lengths and bond angles of C H are irrelevant to our analysis. It was proved in <ref> [GoS73] </ref> that cyclohexane has infinite geometrically possible conformation due to its structural symmetry. In the following example, we slightly increase the length of the last bond. Since the symmetry no longer holds, we obtain a finite set of solutions.
Reference: [GZ79] <author> C.B. Garcia and W.I. Zangwill. </author> <title> Finding all solutions to polynomial systems and other systems of equations. </title> <journal> Math. Prog., </journal> <volume> 16 </volume> <pages> 159-176, </pages> <year> 1979. </year>
Reference-contexts: Iterative techniques, like the New-ton's method, are good for local analysis only and work well if we are given good initial guesses to the solutions. This is rather difficult for most applications. Homotopy methods have a good theoretical background and proceed by following paths in the complex space <ref> [GZ79] </ref>. In theory, each path converges to a geometrically isolated solution. They have been implemented and tried on a variety of applications [Mor87, Mor92]. Some public domain software packages, like HOMPACK, are available as well. However, in practice current implementations and algorithms for ho-motopy methods suffer from many problems. <p> Many applications in solid modeling and manufacturing need to evaluate very high degree algebraic curves (a few hundred) and these techniques are not suitable for such curves. Numerical and finite precision algorithms based on interval arithmetic [Moo79] and homotopy methods <ref> [GZ79, Mor92] </ref> have also been used for evaluating algebraic sets. While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping.
Reference: [Hof89] <author> C.M. Hoffmann. </author> <title> Geometric and Solid Modeling. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Califor-nia, </address> <year> 1989. </year>
Reference-contexts: While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping. There is a considerable amount of emphasis in the solid modeling literature to evaluate surface intersections and offset curves <ref> [Hof89, SN91, Man92] </ref> and in vision literature to compute aspect graphs [PK92]. This includes algorithms for computing all components including the closed loops. However, these algorithms are somewhat restrictive and cannot be used for evaluating general algebraic curves. <p> A number of algorithms for tracing based on local iterative methods have been used in ho-motopy methods, surface interrogations and solutions of differential equations <ref> [Hof89, Mor92] </ref>. Given a point on the curve, an approximate value of the next point is obtained by taking a small step size in a direction determined by the local geometry of the curve. <p> The primitive solids may correspond to polyhedra, quadrics or solids whose surfaces are represented using piecewise rational parametric or piecewise algebraic surfaces <ref> [Hof89] </ref>. Fig. 8 demonstrates two difference operation and the final solid obtained after the difference operations. Most modelers use the CSG representation for model construction and compute the B-rep's for geometric operations.
Reference: [Hof90] <author> C.M. Hoffmann. </author> <title> Algebraic and numeric techniques for offsets and blends. </title> <editor> In W. Dahmen, M. Gasca, and C. Micchelli, editors, </editor> <booktitle> Computations of Curves and Surfaces, </booktitle> <pages> pages 499-528. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Many a times the computation goes on until the machine runs out of all the virtual space (which is of the order of few gigabytes). This fact was highlighted in the context of geometric modeling applications by Hoffmann <ref> [Hof90] </ref>. Furthermore, this approach based on symbolic elimination and finding roots of a single polynomial needs to be implemented in exact 1 arithmetic as opposed to finite precision arithmetic (like IEEE floating point arithmetic available on current hardware).
Reference: [Hor91] <author> B.K.P. Horn. </author> <title> Relative orientation revisited. </title> <journal> Journal of Optical Society of America, </journal> <volume> 8(10) </volume> <pages> 1630-1638, </pages> <year> 1991. </year>
Reference-contexts: Some public domain software packages, like HOMPACK, are available as well. However, in practice current implementations and algorithms for ho-motopy methods suffer from many problems. The different paths being followed may not be geometrically isolated. In particular, this problem has been summarized by Horn in <ref> [Hor91] </ref>, who applied continuation methods for the structure from motion problem in computer vision, as: "one problem with continuation methods is that, while in theory paths of roots should never cross, in practice they often come close enough to permit path jumping, unless the path is followed with impractically tight tolerances".
Reference: [Joh87] <author> J.K. Johnstone. </author> <title> The Sorting of points along an algebraic curve. </title> <type> PhD thesis, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Based on a classic theorem in algebraic geometry, an algebraic plane curve birationally equivalent to the space curve is computed using elimination techniques. Given an algebraic plane curve, techniques for desingularisation based on quadratic transformations are given in <ref> [Wal50, Abh90, AB88, Joh87] </ref>. These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves [Arn83, SS83].
Reference: [Jou91] <author> Jean-Pierre Jouanolou. </author> <title> Le Formalisme du Resultant, </title> <booktitle> volume 90 of Advances in Mathematics. </booktitle> <year> 1991. </year>
Reference-contexts: This corresponds to computing the characteristic polynomials of both the determinants [Can88]. Many special cases, corresponding to n = 2; 3; 4; 5; 6 when the resultant can be expressed as the determinant of a matrix, are given in <ref> [Dix08, Jou91, Mor25, MC27] </ref>. Most of the formulation presented in the classical literature correspond to computing the resultants of dense polynomial systems. More recently, resultants of sparse polynomial systems have received a lot attention in a newly developed area sparse elimination theory.
Reference: [KM95] <author> S. Kr-ishnan and D. Manocha. </author> <title> Numeric-symbolic algorithms for evaluating one-dimensional algebraic sets. </title> <booktitle> In Proceedings of International Symposium on Symbolic and Algebraic Computation, </booktitle> <year> 1995. </year>
Reference-contexts: This method of identifying singularities is susceptible to numerical errors especially if they lie very close to each other. We, however, believe that such pathological cases are rare in practice. 8 u 0 1 P More details of the component splitting algorithm and singularity detection are presented in <ref> [KM95] </ref>. After performing component splitting, we have regions with at most one curve component and its starting points. Starting from these points, the tracing algorithm computes successive curve points using the local geometry of the curve. Let the component be C (see Fig.2 (d)).
Reference: [LL88] <author> H.Y. Lee and C.G. Liang. </author> <title> A new vector theory for the analysis of spatial mechanisms. </title> <journal> Mechanisms and Machine Theory, </journal> <volume> 23(3) </volume> <pages> 209-217, </pages> <year> 1988. </year>
Reference-contexts: Thus, the problem of inverse kinematics of general manipulators with 6 joints corresponds to solving 6 equations for 6 unknowns. These problem has been extensively studied in the robotics literature. In particular, a sparse resultant for these particular equations was derived by <ref> [LL88, RR89] </ref> and it was shown there can be at most 16 different configurations of the robot chain for a given pose of the end effector. We made use of the sparse resultant formulation and combined it with matrix computations to reduce to an eigenvalue problem.
Reference: [Mac02] <author> F.S. </author> <title> Macaulay. On some formula in elimination. </title> <journal> Proceedings of London Mathematical Society, </journal> <volume> 1(33) </volume> <pages> 3-27, </pages> <month> May </month> <year> 1902. </year>
Reference-contexts: In this case, the resultant can always be expressed as determinant of a matrix. However, a single determinant formulation may not exist for any arbitrary n and the most general formulation of resultant (to the best of our knowledge) expresses it as a ratio of two determinants <ref> [Mac02] </ref>. Many a time both determinants evaluate to zero. To compute the resultant we need to perturb the equations and use limiting arguments. This corresponds to computing the characteristic polynomials of both the determinants [Can88]. <p> computation of the form z i = Aq i1 ; i = q T After a few iterations, k corresponds to the eigenvalue of maximum magnitude and q k is the corresponding eigenvector. 2.2.5 Sparse Matrix Computations The general formulation of resultants corresponding to Macaulay formulation results in sparse matrices <ref> [Mac02] </ref>. In such cases we want to make use of the sparsity of the matrix in computing its eigendecomposition. The order of Macaulay matrix is a function of the number of polynomials and the degrees of the polynomial.
Reference: [Man92] <author> D. Manocha. </author> <title> Algebraic and Numeric Techniques for Modeling and Robotics. </title> <type> PhD thesis, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping. There is a considerable amount of emphasis in the solid modeling literature to evaluate surface intersections and offset curves <ref> [Hof89, SN91, Man92] </ref> and in vision literature to compute aspect graphs [PK92]. This includes algorithms for computing all components including the closed loops. However, these algorithms are somewhat restrictive and cannot be used for evaluating general algebraic curves. <p> There is an elegant relationship between the kernel of these matrix polynomials and the variables being eliminated, which is being used for computing the rest of the variables and the birational map. We survey algorithms for zero-dimensional algebraic sets <ref> [Man92, Man94b, Man94a] </ref>, highlights their application to inverse kinematics of serial robot manipulators [Man92] and conformational analysis of molecular chains [MZW95]. We evaluate one-dimensional algebraic sets up to a desired precision using tracing methods. <p> We survey algorithms for zero-dimensional algebraic sets [Man92, Man94b, Man94a], highlights their application to inverse kinematics of serial robot manipulators <ref> [Man92] </ref> and conformational analysis of molecular chains [MZW95]. We evaluate one-dimensional algebraic sets up to a desired precision using tracing methods. We present algorithms to find start points on all components of the curves, prevent component jumping and handling singularities. <p> We call this resultant the Multipolynomial Resultant of the given system of polynomial equations <ref> [Man92, MC93] </ref>. 2.1 MultiPolynomial Resultants Given a system of polynomial equations, the resultant is obtained by eliminating a set of variables and thereby computing a projection of the algebraic set in the lower dimension. The resultant is defined as a polynomial in the coefficients of the original system of equations. <p> The major benefit comes from the fact that the total number of solutions in the affine domain is much lower than the Bezout bound. Typical examples of sparse systems are those that describe the inverse kinematics for a 6R robot <ref> [Man92] </ref>, forward kinematics for the Stewart platform [Mer92], camera motion from point matches [FM90], and geometric constraints describing two-or three-dimensional objects. As the dimension of the problem increases, the difference between the sparse and non-sparse bounds increases dramatically. <p> In particular, resultants are being used to linearize the problem in terms of matrices and determinants. Initially we consider zero-dimensional algebraic sets and the same formulation is extendible to higher dimensional algebraic sets. More details are given in <ref> [Man92, Man94b] </ref>. Given a system of n equations in n unknowns, F 1 (x 1 ; x 2 ; : : : ; x n ) = 0 . . . <p> Let us denote that m fi 1 vector as v. That is M (ff 1 )v = 0; (3.4) where 0 is a mfi1 null vector. The roots of the determinant of M (x 1 ) correspond to the eigenvalues of C highlighted in the following theorem <ref> [Man92] </ref>: Theorem 3.1 Given the matrix polynomial, M (x 1 ) the roots of the polynomial corresponding to its determinant are the eigenvalues of the matrix C = B B @ 0 0 I m : : : 0 . . . : : : . . . <p> Many a times the leading matrix M l is singular or close to being singular (due to high condition number). Some techniques based on linear transformations are highlighted in <ref> [Man92] </ref>, such that the problem of finding roots of the determinant of a matrix polynomial can be reduced to an eigenvalue problem. However there are cases where they may not work. For example, when the matrices have singular pencils. <p> However there are cases where they may not work. For example, when the matrices have singular pencils. In such cases, we reduce the intersection problem to a generalized eigenvalue problem using the following theorem <ref> [Man92] </ref>: Theorem 3.2 Given the matrix polynomial, M (x 1 ) the roots of the polynomial corresponding to its determinant are the eigenvalues of the generalized system C 1 x 1 C 2 , where C 1 = B B B I m 0 0 : : : 0 . . <p> However, in many cases it is possible to identify higher multiplicity eigenvalues of a matrix by identifying clusters of eigenvalues and using the knowledge of the condition number of the clusters [BDM89]. More details of its application to finding solutions of polynomial equations are given in <ref> [Man92] </ref>. Such analysis is well developed for eigenvalues of a matrix and no equivalent analysis is known for higher multiplicity roots of a polynomial. Given a higher multiplicity eigenvalue, ff 1 , we compute its geometric multiplicity by computing the SVD of C ff 1 I. <p> Given a point on the plane curve, (u 0 ; v 0 ), the corresponding point on the space curve, (w 1 0 ; w 2 0 ; : : :; w n1 0 ), is computed using the kernel of M (u 0 ; v 0 ) <ref> [Man92] </ref>. In most applications, the one-dimensional sets encountered are of high degree, and are usually characterized by the presence of a number of curve components inside the domain of interest. The algorithm we propose uses tracing to evaluate the algebraic curves. <p> We made use of the sparse resultant formulation and combined it with matrix computations to reduce to an eigenvalue problem. In particular, we obtain a 24 fi 24 matrix and applied different eigenvalue algorithms to compute the solutions <ref> [Man92] </ref>. The performance of different algorithms for inverse kinematics, along with the machine platform are highlighted in Table 1. Molecule Conformations: A molecular chain is classified using bond lengths, bond angles and dihedral angles. We initially model them as rigid chains using fixed bond lengths and bond angles. <p> It is therefore, desired to 11 Algorithm Reference Machine Average Time Continuation (Wampler and Morgan 1991 [WM91]) IBM 370-3090 10 sec. Resultant - (Manocha 1992 <ref> [Man92] </ref>) IBM RS/6000 0.011 sec. QR algorithm Resultant - (Manocha 1994 [Man94a]) IBM RS/6000 0.0043 sec.
Reference: [Man94a] <author> D. Manocha. </author> <title> Computing selected solutions of polynomial equations. </title> <booktitle> In Proceedings of International Symposium on Symbolic and Algebraic Computation, </booktitle> <pages> pages 1-8, </pages> <address> Oxford, Eng-land, 1994. </address> <publisher> ACM Press. </publisher>
Reference-contexts: There is an elegant relationship between the kernel of these matrix polynomials and the variables being eliminated, which is being used for computing the rest of the variables and the birational map. We survey algorithms for zero-dimensional algebraic sets <ref> [Man92, Man94b, Man94a] </ref>, highlights their application to inverse kinematics of serial robot manipulators [Man92] and conformational analysis of molecular chains [MZW95]. We evaluate one-dimensional algebraic sets up to a desired precision using tracing methods. <p> The QR or QZ algorithm for eigenvalue computation returns all the eigenvalues of a given matrix and it is difficult to restrict it to eigenvalues in a particular domain [GL89]. Algorithms to compute selected eigenvalues of the these matrices based on power iterations and their structure are given in <ref> [Man94a] </ref>. Let us assume that ff 1 is a simple eigenvalue of C. In the rest of the paper, we carry out the analysis on the eigenvalues of C and the resulting algorithm is similar for the finite eigenvalues of the pencil C 1 x 1 C 2 . <p> It is therefore, desired to 11 Algorithm Reference Machine Average Time Continuation (Wampler and Morgan 1991 [WM91]) IBM 370-3090 10 sec. Resultant - (Manocha 1992 [Man92]) IBM RS/6000 0.011 sec. QR algorithm Resultant - (Manocha 1994 <ref> [Man94a] </ref>) IBM RS/6000 0.0043 sec. Selected Solutions Table 1: Relative performance of various algorithms for inverse kinematics C C a O N Z P X X on DH formalism have a cooperative variation of angles which confines the conformational changes to a local section of the chain.
Reference: [Man94b] <author> D. Manocha. </author> <title> Solving systems of polynomial equations. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 46-55, </pages> <month> March </month> <year> 1994. </year> <note> Special Issue on Solid Modeling. </note>
Reference-contexts: There is an elegant relationship between the kernel of these matrix polynomials and the variables being eliminated, which is being used for computing the rest of the variables and the birational map. We survey algorithms for zero-dimensional algebraic sets <ref> [Man92, Man94b, Man94a] </ref>, highlights their application to inverse kinematics of serial robot manipulators [Man92] and conformational analysis of molecular chains [MZW95]. We evaluate one-dimensional algebraic sets up to a desired precision using tracing methods. <p> In particular, resultants are being used to linearize the problem in terms of matrices and determinants. Initially we consider zero-dimensional algebraic sets and the same formulation is extendible to higher dimensional algebraic sets. More details are given in <ref> [Man92, Man94b] </ref>. Given a system of n equations in n unknowns, F 1 (x 1 ; x 2 ; : : : ; x n ) = 0 . . .
Reference: [MC27] <author> F. Morley and A.B. Coble. </author> <title> New results in elimination. </title> <journal> American Journal of Mathematics, </journal> <volume> 49 </volume> <pages> 463-488, </pages> <year> 1927. </year>
Reference-contexts: This corresponds to computing the characteristic polynomials of both the determinants [Can88]. Many special cases, corresponding to n = 2; 3; 4; 5; 6 when the resultant can be expressed as the determinant of a matrix, are given in <ref> [Dix08, Jou91, Mor25, MC27] </ref>. Most of the formulation presented in the classical literature correspond to computing the resultants of dense polynomial systems. More recently, resultants of sparse polynomial systems have received a lot attention in a newly developed area sparse elimination theory.
Reference: [MC91] <author> D. Manocha and J.F. Canny. </author> <title> A new approach for surface intersection. </title> <journal> International Journal of Computational Geometry and Applications, </journal> <volume> 1(4) </volume> <pages> 491-516, </pages> <year> 1991. </year> <journal> Special issue on Solid Modeling. </journal> <volume> 17 </volume>
Reference-contexts: any point on the piecewise representation of the curve is not more than * apart from the curve. * Compute D u (u 1 ; v 1 ) and D v (u 1 ; v 1 ), the partial derivatives of the curve with respect to u and v, respectively <ref> [MC91] </ref>. This is the vector (on the plane) normal to the plane curve. * Given the normal vector, find the unit vector corre sponding to the tangent.
Reference: [MC93] <author> D. Manocha and J.F. Canny. </author> <title> Multipolyno--mial resultant algorithms. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 15(2) </volume> <pages> 99-122, </pages> <year> 1993. </year>
Reference-contexts: We call this resultant the Multipolynomial Resultant of the given system of polynomial equations <ref> [Man92, MC93] </ref>. 2.1 MultiPolynomial Resultants Given a system of polynomial equations, the resultant is obtained by eliminating a set of variables and thereby computing a projection of the algebraic set in the lower dimension. The resultant is defined as a polynomial in the coefficients of the original system of equations.
Reference: [Mer92] <author> J-P. Merlet. </author> <title> Direct kinematics and assembly modes of parallel manipulators. </title> <journal> International Journal of Robotics Research, </journal> <volume> 11(2) </volume> <pages> 150-162, </pages> <year> 1992. </year>
Reference-contexts: The major benefit comes from the fact that the total number of solutions in the affine domain is much lower than the Bezout bound. Typical examples of sparse systems are those that describe the inverse kinematics for a 6R robot [Man92], forward kinematics for the Stewart platform <ref> [Mer92] </ref>, camera motion from point matches [FM90], and geometric constraints describing two-or three-dimensional objects. As the dimension of the problem increases, the difference between the sparse and non-sparse bounds increases dramatically. A large gap between the Bezout bound and the actual number of solutions is not unusual for geometric problems.
Reference: [Moo79] <author> R.E. Moore. </author> <title> Methods and applications of interval analysis. </title> <note> SIAM studies in applied mathematics. Siam, </note> <year> 1979. </year>
Reference-contexts: Many applications in solid modeling and manufacturing need to evaluate very high degree algebraic curves (a few hundred) and these techniques are not suitable for such curves. Numerical and finite precision algorithms based on interval arithmetic <ref> [Moo79] </ref> and homotopy methods [GZ79, Mor92] have also been used for evaluating algebraic sets. While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping.
Reference: [Mor25] <author> F. Morley. </author> <title> The eliminant of a net of curves. </title> <journal> American Journal of Mathematics, </journal> <volume> 47 </volume> <pages> 91-97, </pages> <year> 1925. </year>
Reference-contexts: This corresponds to computing the characteristic polynomials of both the determinants [Can88]. Many special cases, corresponding to n = 2; 3; 4; 5; 6 when the resultant can be expressed as the determinant of a matrix, are given in <ref> [Dix08, Jou91, Mor25, MC27] </ref>. Most of the formulation presented in the classical literature correspond to computing the resultants of dense polynomial systems. More recently, resultants of sparse polynomial systems have received a lot attention in a newly developed area sparse elimination theory.
Reference: [Mor87] <author> A.P. Morgan. </author> <title> Solving Polynomial Systems Using Continuation for Scientific and Engineering Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference-contexts: This is rather difficult for most applications. Homotopy methods have a good theoretical background and proceed by following paths in the complex space [GZ79]. In theory, each path converges to a geometrically isolated solution. They have been implemented and tried on a variety of applications <ref> [Mor87, Mor92] </ref>. Some public domain software packages, like HOMPACK, are available as well. However, in practice current implementations and algorithms for ho-motopy methods suffer from many problems. The different paths being followed may not be geometrically isolated.
Reference: [Mor92] <author> A. P. Morgan. </author> <title> Polynomial continuation and its relationship to the symbolic reduction of polynomial systems. </title> <booktitle> In Symbolic and Numerical Computation for Artificial Intelligence, </booktitle> <pages> pages 23-45, </pages> <year> 1992. </year>
Reference-contexts: This is rather difficult for most applications. Homotopy methods have a good theoretical background and proceed by following paths in the complex space [GZ79]. In theory, each path converges to a geometrically isolated solution. They have been implemented and tried on a variety of applications <ref> [Mor87, Mor92] </ref>. Some public domain software packages, like HOMPACK, are available as well. However, in practice current implementations and algorithms for ho-motopy methods suffer from many problems. The different paths being followed may not be geometrically isolated. <p> Many applications in solid modeling and manufacturing need to evaluate very high degree algebraic curves (a few hundred) and these techniques are not suitable for such curves. Numerical and finite precision algorithms based on interval arithmetic [Moo79] and homotopy methods <ref> [GZ79, Mor92] </ref> have also been used for evaluating algebraic sets. While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping. <p> A number of algorithms for tracing based on local iterative methods have been used in ho-motopy methods, surface interrogations and solutions of differential equations <ref> [Hof89, Mor92] </ref>. Given a point on the curve, an approximate value of the next point is obtained by taking a small step size in a direction determined by the local geometry of the curve.
Reference: [MZW95] <author> D. Manocha, Y. Zhu, and W. Wright. </author> <title> Con-formational analysis of molecular chains using nano-kinematics. </title> <booktitle> Computer Application of Biological Sciences (CABIOS), </booktitle> <pages> pages 71-86, </pages> <year> 1995. </year>
Reference-contexts: We survey algorithms for zero-dimensional algebraic sets [Man92, Man94b, Man94a], highlights their application to inverse kinematics of serial robot manipulators [Man92] and conformational analysis of molecular chains <ref> [MZW95] </ref>. We evaluate one-dimensional algebraic sets up to a desired precision using tracing methods. We present algorithms to find start points on all components of the curves, prevent component jumping and handling singularities. <p> In Fig. 5.1, we highlight the DH formulation for a peptide unit and the corresponding parameters are shown in Fig. 6. Given this formulation, the problems of ring closure for cyclic chains and local deformations reduce to inverse kinematics 5.7 <ref> [MZW95] </ref>. Depending on the geometry of the molecular chain, we reduce it to a 24 fi 24 or 32 fi 32 eigenvalue problem. The resulting algorithm takes anywhere from 1020 milliseconds to compute all the solu d1 a1 Ca Ca Ca C O tions.
Reference: [PFTV90] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference-contexts: is that there are no good, general methods for solving systems of more than one nonlinear equations, as highlighted fl Supported in part by Sloan Research Foundation, ARO Contract P-34982-MA, NSF Grant CCR-9319957, NSF Grant CCR-9625217 and ONR Contract N00014-94-1-0738 in the book "Numerical Recipes: The Art of Scientific Computing" <ref> [PFTV90] </ref>. If the system corresponds to a set of linear equations, good algorithms and their implementations, in the form of linear algebra packages like LINPACK [BDMS79] and LAPACK [ABB + 92], are known and widely used.
Reference: [PK92] <author> J. Ponce and D.J. Kriegman. </author> <title> Elimination theory and computer vision: Recognition and positioning of curved 3d objects from range, intensity, or contours. </title> <booktitle> In Symbolic and Numerical Computation for Artificial Intelligence, </booktitle> <pages> pages 123-146, </pages> <year> 1992. </year>
Reference-contexts: There is a considerable amount of emphasis in the solid modeling literature to evaluate surface intersections and offset curves [Hof89, SN91, Man92] and in vision literature to compute aspect graphs <ref> [PK92] </ref>. This includes algorithms for computing all components including the closed loops. However, these algorithms are somewhat restrictive and cannot be used for evaluating general algebraic curves. In this paper we outline algorithms for solving algebraic systems using multipolynomial resultants and matrix computations.
Reference: [RR89] <author> M. Raghavan and B. Roth. </author> <title> Kinematic analysis of the 6r manipulator of general geometry. </title> <booktitle> In International Symposium on Robotics Research, </booktitle> <pages> pages 314-320, </pages> <address> Tokyo, </address> <year> 1989. </year>
Reference-contexts: Thus, the problem of inverse kinematics of general manipulators with 6 joints corresponds to solving 6 equations for 6 unknowns. These problem has been extensively studied in the robotics literature. In particular, a sparse resultant for these particular equations was derived by <ref> [LL88, RR89] </ref> and it was shown there can be at most 16 different configurations of the robot chain for a given pose of the end effector. We made use of the sparse resultant formulation and combined it with matrix computations to reduce to an eigenvalue problem.
Reference: [Sal85] <author> G. Salmon. </author> <title> Lessons Introductory to the Modern Higher Algebra. G.E. </title> <publisher> Stechert & Co., </publisher> <address> New York, </address> <month> 1885. </month>
Reference-contexts: This problem has been studied extensively in the classic as well as modern literature. In particular, elimination theory, a branch of classical algebraic geometry investigates the condition under which sets of polynomials have common roots. Some of its results were known at least a century ago <ref> [Sal85, Wae50] </ref> and still appear in modern treatments of algebraic geometry, at times in non-constructive form.
Reference: [SN91] <author> T.W. Sederberg and T. Nishita. </author> <title> Geometric hermite approximation of surface patch intersection curves. </title> <booktitle> Computer Aided Geometric Design, </booktitle> <volume> 8 </volume> <pages> 97-114, </pages> <year> 1991. </year>
Reference-contexts: While the former are slow in practice, the latter need points all the components and suffer from problems like component jumping. There is a considerable amount of emphasis in the solid modeling literature to evaluate surface intersections and offset curves <ref> [Hof89, SN91, Man92] </ref> and in vision literature to compute aspect graphs [PK92]. This includes algorithms for computing all components including the closed loops. However, these algorithms are somewhat restrictive and cannot be used for evaluating general algebraic curves.
Reference: [SS83] <author> J. T. Schwartz and M. Sharir. </author> <title> On the piano movers probelem ii, general techniques for computing topological properties of real algebraic manifolds. </title> <journal> Advances of Applied Maths, </journal> <volume> 4 </volume> <pages> 298-351, </pages> <year> 1983. </year>
Reference-contexts: These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves <ref> [Arn83, SS83] </ref>. Its worst case complexity is doubly exponential in the number of variables. For plane curves, improved polynomial time algorithms based on CAD have been presented in [AF88]. However the exponent in terms of N (the degree of the curve) is rather high.
Reference: [Ste76] <author> G.W. Stewart. </author> <title> Simultaneous iteration for computing invariant subspaces of non-hermitian matrices. </title> <journal> Numerische Mathematik, </journal> <volume> 25 </volume> <pages> 123-136, </pages> <year> 1976. </year>
Reference-contexts: The sparsity of the matrix increases with the degrees of the polynomials or the number of equations. Algorithms for sparse matrix computations are based on matrix vector as highlighted in the Power method and inverse iteration. For our applications, we use the algorithm highlighted in <ref> [Ste76] </ref> for computing the invariant subspaces and thereby the eigendecomposition of a sparse matrix. 3 Resultants and Matrix Polynomials In this section, we show how the resultant of a system of polynomial equations can be expressed in terms of matrix polynomials. common solutions).
Reference: [Stu91] <author> B. Sturmfels. </author> <title> Sparse elimination theory. </title> <editor> In D. Eisenbud and L. Robbiano, editors, </editor> <title> Computational Algebraic Geometry and Commutative Algebra. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: Bernstein showed that his bound is exact if all the coefficients of the polynomial system are generic [Ber75]. Resultant formulations in terms of matrices and determinants based on Bernstein bound have appeared in the literature as well <ref> [Stu91, SZ94, CE93] </ref>. Given a system of polynomial equations, sparse or dense, it is possible to express their resultant in terms of matrices and determinants.
Reference: [SV89] <author> M.W. Spong and M. Vidyasagar. </author> <title> Robot Dynamics and Control. </title> <publisher> John Wiley and Sons, </publisher> <year> 1989. </year>
Reference-contexts: In most robotics applications, we are given the pose of the end effector and the problem of inverse kinematics corresponds to computing the joint displacements for that pose. Inverse kinematics has been a fundamental problem in robotics literature for more than three decades <ref> [SV89] </ref>. Over the last few decades, the use of computer tools has gained a lot of importance in molecular modeling and drug design. One of the fundamental problem in drug design is that of finding the three dimensional structures of complex molecules. <p> A coordinate system is attached to each link for describing the relative arrangements among the various links. The coordinate system attached to the ith link is numbered i. The 4 fi 4 transformation matrix relating i + 1 coordinate system to i coordinate system is <ref> [SV89] </ref>: A i = B @ s i c i i c i i a i s i 0 0 0 1 C A ; (5.6) where s i = sin i ; c i = cos i ; i is the ith joint rotation angle; i = sinff i ;
Reference: [SZ94] <author> B. Sturmfels and A. Zelevinsky. </author> <title> Multigraded resultants of sylvester type. </title> <journal> Journal of Algebra, </journal> <note> 1994. To appear. </note>
Reference-contexts: Bernstein showed that his bound is exact if all the coefficients of the polynomial system are generic [Ber75]. Resultant formulations in terms of matrices and determinants based on Bernstein bound have appeared in the literature as well <ref> [Stu91, SZ94, CE93] </ref>. Given a system of polynomial equations, sparse or dense, it is possible to express their resultant in terms of matrices and determinants.
Reference: [Wae50] <author> B.L. Van Der Waerden. </author> <title> Modern Algebra (third edition). </title> <editor> F. </editor> <publisher> Ungar Publishing Co., </publisher> <address> New York, </address> <year> 1950. </year>
Reference-contexts: This problem has been studied extensively in the classic as well as modern literature. In particular, elimination theory, a branch of classical algebraic geometry investigates the condition under which sets of polynomials have common roots. Some of its results were known at least a century ago <ref> [Sal85, Wae50] </ref> and still appear in modern treatments of algebraic geometry, at times in non-constructive form. <p> In case, D is singular, we replace M by its largest non-vanishing minor. Given M, whose entries are polynomials in the u i 's, the resultant corresponding to its determinant can be factored into linear factors of the form <ref> [Wae50] </ref>: Det (M) = i=1 where k is the total number of non-trivial solutions and (ff i0 ; ff i1 ; ff i2 ; : : : ; ff in ) are the projective coordinates of a solution of the given system of equations.
Reference: [Wal50] <author> R.J. Walker. </author> <title> Algebraic Curves. </title> <publisher> Princeton University Press, </publisher> <address> New Jersey, </address> <year> 1950. </year>
Reference-contexts: Based on a classic theorem in algebraic geometry, an algebraic plane curve birationally equivalent to the space curve is computed using elimination techniques. Given an algebraic plane curve, techniques for desingularisation based on quadratic transformations are given in <ref> [Wal50, Abh90, AB88, Joh87] </ref>. These are useful for computing all the branches of the curve. However, the resulting algorithm can potentially be exponential in the degree of the curve. Algorithms based on Collins' cylindrical algebraic decomposition (CAD), [Col75], have been used for evaluating all components of algebraic curves [Arn83, SS83].
Reference: [Wil59] <author> J.H. Wilkinson. </author> <title> The evaluation of the zeros of ill-conditioned polynomials. parts i and ii. </title> <journal> Numer. Math., </journal> <volume> 1 </volume> <pages> 150-166 and 167-180, </pages> <year> 1959. </year>
Reference-contexts: This is due to the fact that the problem of finding roots of a univariate polynomial may be ill-conditioned for high degree polynomials, as shown by Wilkinson <ref> [Wil59] </ref>. That slows down the overall computation. In the context of floating point arithmetic, the two main approaches for zero-dimensional sets are iterative methods and homotopy methods.
Reference: [Wil65] <author> J.H. Wilkinson. </author> <title> The algebraic eigenvalue problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: We use this linear algebra formulation in the algorithms presented in the following sections. 2.2 Matrix Computations In this section we briefly review some techniques from linear algebra and numerical analysis. More details can be found in <ref> [GL89, Wil65] </ref>. 2.2.1 Eigenvalues and Eigenvectors Given an n fi n matrix A, its eigenvalues and eigenvectors are the solutions to the equation Ax = sx; where s is the eigenvalue and x 6= 0 is the eigenvector.
Reference: [WM91] <author> C. Wampler and A.P. Morgan. </author> <title> Solving the 6r inverse position problem using a generic-case solution methodology. </title> <journal> Mechanisms and Machine Theory, </journal> <volume> 26(1) </volume> <pages> 91-106, </pages> <year> 1991. </year> <month> 18 </month>
Reference-contexts: example, to solve the problem of inverse kinematics of 6 revolute joints (which can be reduced to solving 6 polynomial equations in 6 unknowns), where a performance of the order of milliseconds is desired, the best implementation of continuation methods takes about 10 seconds on an IBM 370 3090 mainframe <ref> [WM91] </ref>. Many of the symbolic and numeric techniques highlighted above have been generalized to one-dimensional algebraic sets. Based on a classic theorem in algebraic geometry, an algebraic plane curve birationally equivalent to the space curve is computed using elimination techniques. <p> However, a small change in a dihedral angle located near the middle of the long chain causes a drastic change in the overall conformation of the molecule. It is therefore, desired to 11 Algorithm Reference Machine Average Time Continuation (Wampler and Morgan 1991 <ref> [WM91] </ref>) IBM 370-3090 10 sec. Resultant - (Manocha 1992 [Man92]) IBM RS/6000 0.011 sec. QR algorithm Resultant - (Manocha 1994 [Man94a]) IBM RS/6000 0.0043 sec.
References-found: 57

