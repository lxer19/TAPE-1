URL: http://www-white.media.mit.edu:80/vismod/publications/techdir/TR-316.ps.Z
Refering-URL: http://www-white.media.mit.edu/~testarne/asl/index.html
Root-URL: http://www.media.mit.edu
Title: Visual Recognition of American Sign Language Using Hidden Markov Models  Architecture and Planning,  
Author: by Thad Eugene Starner Alex Pentland Stephen A. Benton 
Note: Submitted to the Program in  c Massachusetts Institute of Technology,  Program in  
Address: Cambridge MA  
Affiliation: S.B., Computer Science S.B., Brain and Cognitive Science Massachusetts Institute of Technology,  Media Arts and Sciences, School of  Massachusetts Institute of Technology  Program in Media  Media Arts and Sciences  
Pubnum: Head, Perceptual Computing Section  
Degree: in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE IN MEDIA ARTS AND SCIENCES at the  All Rights Reserved Signature of Author Program in Media Arts and Sciences  Certified by  Arts and Sciences Thesis Supervisor Accepted by  Chairperson Departmental Committee on Graduate Students  
Date: June 1991  February 1995  1995  20 January 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adaptive Optics Associates. </author> <title> Multi-Trax User Manual. Adaptive Optics Associates, </title> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems [27, 14, 32, 42], ballet step recognition [5], body capture <ref> [1] </ref>, real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9].
Reference: [2] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: In the following sections the vision methods used to track the hands are described, and the basics of HMM's are applied to ASL recognition. 4.1 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [43, 16, 30, 2, 20] </ref>, this section modifies a traditional discussion of the algorithms so as to provide the perspective used for recognizing sign language. <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [16, 2, 43] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem. <p> It is easy to see how, instead of quantizing, the actual probability density for y motion might be used. However, the above algorithms must be modified to accept continuous densities. The efforts of Baum, Petrie, Liporace, and Juang <ref> [3, 2, 22, 18] </ref> showed how to generalize the Baum-Welch, Viterbi, and forward-backward algorithms to handle a variety of characteristic densities. In this context, however, the densities will be assumed to be Gaussian.
Reference: [3] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. </title> <journal> Ann. Math. Stat., </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: It is easy to see how, instead of quantizing, the actual probability density for y motion might be used. However, the above algorithms must be modified to accept continuous densities. The efforts of Baum, Petrie, Liporace, and Juang <ref> [3, 2, 22, 18] </ref> showed how to generalize the Baum-Welch, Viterbi, and forward-backward algorithms to handle a variety of characteristic densities. In this context, however, the densities will be assumed to be Gaussian.
Reference: [4] <author> Richard A. Bolt and Edward Herranz. </author> <title> Two-handed gesture in multi-modal natural dialog. </title> <booktitle> In Proceedings of UIST '92, Fifth Annual Symposium on User Interface Software and Technology, </booktitle> <address> Monterey, CA, </address> <year> 1992. </year>
Reference-contexts: This, combined with Kalman filtering, simplifies occlusion problems and allows recovery of a detailed hand model through a wide range of motion. Datagloves by VPL are often used for sensing as well <ref> [24, 39, 4] </ref>.
Reference: [5] <author> L. Campbell. </author> <title> Recognizing classical ballet setps using phase space constraints. </title> <type> Master's thesis, </type> <institution> Masschusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems [27, 14, 32, 42], ballet step recognition <ref> [5] </ref>, body capture [1], real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9]. <p> A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions <ref> [19, 23, 5] </ref>, expression tracking [11], conducting music [24], and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems.
Reference: [6] <author> J. Cassell, C. Pelachaud, N. Badler, M. Steedman, B. Achorn, T. Becket, B. Dou-ville, S. Prevost, and M. Stone. </author> <title> Animated conversation: Rule-based generation of facial expression, gesture, and spoken intonation for multiple conversational agents. </title> <booktitle> In Computer Graphics (SIGGRAPH '94 Proceedings), </booktitle> <pages> pages 413-420, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Gestures are often made at points of stress in a conversation, when illustrating a motion, or when describing an object. In fact, research has been done on the language of these gestures <ref> [6] </ref>. By recognizing gestures made in conjunction with spoken language, a computer may be able to better understand the wishes of the user. If an ASL recognizer can be created, then similar technology may be applied to these conversational gestures as well.
Reference: [7] <author> C. Charayaphan and A.E. Marble. </author> <title> Image processing system for interpreting motion in American Sign Language. </title> <journal> Journal of Biomedical Engineering, </journal> <volume> 14 </volume> <pages> 419-425, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Tamura and Kawasaki demonstrated an early image processing system which could recognize 20 Japanese signs based on matching cheremes [40]. A chereme is composed of the tab, dez, and sig as discussed earlier. Charayaphan and Marble <ref> [7] </ref> demonstrated a feature set that could distinguish between the 31 isolated ASL signs in their training set. Takahashi and Kishino in [39] discuss a Dataglove-based system that could recognize 34 of the 46 Japanese kana alphabet gestures (user dependent) using a joint angle and hand orientation coding technique.
Reference: [8] <author> T.J. Darrell and A.P. Pentland. </author> <title> Space-time gestures. </title> <booktitle> IEEE Conf. on Computer Vision and Pattern Rec., </booktitle> <pages> pages 335-340, </pages> <year> 1993. </year> <month> 48 </month>
Reference-contexts: However, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences. Several vision systems have been developed with technology closely related to HMM methodology. Darrell <ref> [8] </ref> uses the dynamic time warping method to recognize gestures ("hello" and "good-bye") through time. Siskind and Morris [34] argue that event perception requires less information and may be an easier problem than object recognition.
Reference: [9] <author> Trevor Darrell and Alex Pentland. </author> <title> Attention-driven expression and gesture analysis in an interactive environment. MIT Media Lab Perceptual Computing Group Technical Report No. </title> <type> 312, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic <ref> [9] </ref>. In the case of ASL, the hands and head are of the most interest. 9 2.2 American Sign Language American Sign Language is the language of choice for most deaf people in the United States. <p> Another solution is to monitor an entire room with a single fixed camera and use narrow field of view cameras mounted on servo platforms to direct attention to specific areas of interest <ref> [9] </ref>. This technique allows high detail and a wide range of motion, but suffers from the coupled motion problems of an active camera. Tracking can often be simplified through using calibrated gloves or wired sensors.
Reference: [10] <author> B. Dorner. </author> <title> Hand shape identification and tracking for sign language interpretation. </title> <booktitle> In IJCAI Workshop on Looking at People, </booktitle> <year> 1993. </year>
Reference-contexts: Most work on sign language recognition employs expensive wired "datagloves" that the user must wear [39]. In addition, these systems mostly concentrate on finger signing, where the user spells each word with hand signs corresponding to the letters in the alphabet <ref> [10] </ref>. However, most sign does not involve finger spelling but signs that represent whole words. This allows signed conversations to proceed at about the pace of spoken conversation. <p> This technique allows high detail and a wide range of motion, but suffers from the coupled motion problems of an active camera. Tracking can often be simplified through using calibrated gloves or wired sensors. Dorner <ref> [10] </ref> uses a specially calibrated glove with different colors for each finger and the wrist and markers at each finger joint and tip. This, combined with Kalman filtering, simplifies occlusion problems and allows recovery of a detailed hand model through a wide range of motion.
Reference: [11] <author> Irfan Essa, Trevor Darrell, and Alex Pentland. </author> <title> Tracking facial motion. </title> <type> Technical Report 272, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <address> 20 Ames St, Cambridge MA, </address> <year> 1994. </year> <note> To appear in IEEE Workshop on Nonrigid and articulated Motion, </note> <institution> Austin TX, </institution> <month> Nov 94. </month>
Reference-contexts: ASL uses facial expressions to distinguish between statements, questions, and directives. The eyebrows are raised for a question, held normal for a statement, and furrowed for a directive. While there has been work in recognizing facial gestures <ref> [11] </ref>, facial features will not be used to aid recognition in the task addressed. <p> A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions [19, 23, 5], expression tracking <ref> [11] </ref>, conducting music [24], and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. Tracking the natural hand in real time using camera imagery is difficult, but successful systems have been demonstrated in controlled settings. <p> Since recognition time increases with the log of the size of the lexicon, considerable expansion should be possible while still providing immediacy. Currently the system ignores semantic context given by facial expressions while signing. By adding expression tracking techniques demonstrated by Essa and Darrell <ref> [11] </ref>, this information might be recovered in the current system. An active camera to provide higher resolution "focus of attention" images might also be added to the current apparatus. This would help alleviate the constraint of constant position when signing as well as provide better data for the tracking software. 47
Reference: [12] <author> W. T. Freeman and M. Roth. </author> <title> Orientation histograms for hand gesture recognition. </title> <type> Technical Report 94-03, </type> <institution> Mitsubishi Electric Research Labs., </institution> <address> 201 Broadway, Cambridge, MA 02139, </address> <year> 1994. </year>
Reference-contexts: A large variety of interfaces have been proposed, using video driven gestures for mouse control <ref> [12] </ref>, full body interactions [19, 23, 5], expression tracking [11], conducting music [24], and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. <p> Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. Tracking the natural hand in real time using camera imagery is difficult, but successful systems have been demonstrated in controlled settings. Freeman <ref> [12] </ref> has shown a hand tracker that can be used for navigating 3D worlds. A greyscale camera tracks the hand in a small area on a table and uses hand and finger position to control the direction of a virtual graphics camera.
Reference: [13] <author> Yang He and Amlan Kundu. </author> <title> Planar shape classification using hidden markov models. </title> <booktitle> In Proc. 1991 IEEE Conf. on Computer Vision and Pattern Rec., </booktitle> <pages> pages 10-15. </pages> <publisher> IEEE Press, </publisher> <year> 1991. </year>
Reference-contexts: Most early work was limited to handwriting recognition [21, 26]. More recently, He and Kundu <ref> [13] </ref> report using continuous density HMM's to classify planar shapes. Their method segmented closed shapes and exploited characteristic relations between consecutive segments for classification. The algorithm was reported to tolerate shape contour perturbation and some occlusion.
Reference: [14] <author> David Hogg. </author> <title> Model-based vision: a program to see a walking person. </title> <journal> Image and Vision Computing, </journal> <volume> 1(1) </volume> <pages> 5-20, </pages> <month> Feb </month> <year> 1983. </year>
Reference-contexts: Therefore, both natural constraints of the human body and simplifying assumptions are often used to curtail the amount of data needed for analysis. Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems <ref> [27, 14, 32, 42] </ref>, ballet step recognition [5], body capture [1], real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9].
Reference: [15] <author> Berthold Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: I 0 b = I 0 c = I 0 (x 0 and y 0 are the x and y coordinates normalized to the centroid) The axis of least inertia is then determined by the major axis of the bounding ellipse, which corresponds to the primary eigenvector of the matrix <ref> [15] </ref>. Figure 4-4 demonstrates bounding ellipses fitted to the images of the hands. Note the 180 degree ambiguity in the angle of the ellipses.
Reference: [16] <author> X. D. Huang, Y. Ariki, and M. A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1990. </year>
Reference-contexts: In the following sections the vision methods used to track the hands are described, and the basics of HMM's are applied to ASL recognition. 4.1 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [43, 16, 30, 2, 20] </ref>, this section modifies a traditional discussion of the algorithms so as to provide the perspective used for recognizing sign language. <p> A simplistic example develops the fundamental theory in training and testing of a discrete HMM which is then generalized to the continuous density case used in the experiments. For broader discussion of the topic, <ref> [16] </ref> is recommended. 23 sequence can be divided into the states which produced each section. sequence can no longer be precisely recovered. 24 A time domain process demonstrates a Markov property if the conditional probability density of the current event, given all present and past events, depends only on the jth <p> Summing this probability for all possible state sequences S produces P r (Oj). However, this method is exponential in time, so the more efficient forward-backward algorithm is used in practice. The forward variable has already been defined above. Here its inductive calculation, called the forward algorithm, is shown (from <ref> [16] </ref>). * ff 1 (i) = i b i (O 1 ), for all states i (if i*S I ; i = 1 n I ;otherwise i = 0) * Calculating ff () along the time axis, for t = 2; :::; T , and all states j, compute ff t <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [16, 2, 43] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem. <p> While this is not strictly proper, the values are approximately equal in contiguous iterations <ref> [16] </ref> and seem not to make an empirical difference [43].
Reference: [17] <author> Tom Humphries, Carol Padden, and Terrence J. O'Rourke. </author> <title> A Basic Course in American Sign Language. </title> <editor> T. J. </editor> <publisher> Publishers, Inc., </publisher> <address> Silver Spring, MD, </address> <year> 1980. </year>
Reference-contexts: At first a naive eye was used to avoid ambiguities in 11 the selected signs, but this was shortly subsumed by the coherency constraint. Figures 2-1 to 2-5 illustrate the signs selected (from <ref> [17] </ref>). The process of creating a new recognition system often limits the amount of initial training that can be collected. False starts and complications sometimes require discarding of data, making the initial process extremely frustrating for the subject. Therefore, the author learned the necessary signs to provide the database.
Reference: [18] <author> B. H. Juang. </author> <title> Maximum likelihood estimation for mixture multivariate observations of markov chains. </title> <journal> AT&T Technical Journal, </journal> <volume> 64 </volume> <pages> 1235-1249, </pages> <year> 1985. </year>
Reference-contexts: It is easy to see how, instead of quantizing, the actual probability density for y motion might be used. However, the above algorithms must be modified to accept continuous densities. The efforts of Baum, Petrie, Liporace, and Juang <ref> [3, 2, 22, 18] </ref> showed how to generalize the Baum-Welch, Viterbi, and forward-backward algorithms to handle a variety of characteristic densities. In this context, however, the densities will be assumed to be Gaussian.
Reference: [19] <author> Myron W. Krueger. </author> <title> Artificial Reality II. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1991. </year> <month> 49 </month>
Reference-contexts: A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions <ref> [19, 23, 5] </ref>, expression tracking [11], conducting music [24], and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. <p> While successfully demonstrating tracking with limited motion, 17 occlusion was not addressed. Furthermore, a simple background was required, which is often impossible when observing natural hand gestures. Simpler, less constrained hand tracking systems have been created in a variety of environments. Krueger <ref> [19] </ref> has shown light table systems that tracked hands using single and multiple cameras aimed at the entire body. Maes et al [23] showed a similar system with one camera which allowed more arbitrary backgrounds to be used.
Reference: [20] <author> F. Kubala, A. Anastasakos, J. Makhoul, L. Nguyen, R. Schwartz, and G. Zavaliagkos. </author> <title> Comparative experiments on large vocabulary speech recognition. </title> <booktitle> In ICASSP 94, </booktitle> <year> 1994. </year>
Reference-contexts: In the following sections the vision methods used to track the hands are described, and the basics of HMM's are applied to ASL recognition. 4.1 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [43, 16, 30, 2, 20] </ref>, this section modifies a traditional discussion of the algorithms so as to provide the perspective used for recognizing sign language.
Reference: [21] <author> A. Kundu, Y. He, and P. Bahl. </author> <title> Handwritten word recognition: a hidden markov model based approach. </title> <booktitle> volume 22, </booktitle> <pages> pages 283-297, </pages> <year> 1989. </year>
Reference-contexts: Most early work was limited to handwriting recognition <ref> [21, 26] </ref>. More recently, He and Kundu [13] report using continuous density HMM's to classify planar shapes. Their method segmented closed shapes and exploited characteristic relations between consecutive segments for classification. The algorithm was reported to tolerate shape contour perturbation and some occlusion.
Reference: [22] <author> L. R. Liporace. </author> <title> Maximum likelihood estimation for multivariate observations of markov sources. </title> <journal> IEEE Trans. Information Theory, </journal> <volume> IT-28:729-734, </volume> <year> 1982. </year>
Reference-contexts: It is easy to see how, instead of quantizing, the actual probability density for y motion might be used. However, the above algorithms must be modified to accept continuous densities. The efforts of Baum, Petrie, Liporace, and Juang <ref> [3, 2, 22, 18] </ref> showed how to generalize the Baum-Welch, Viterbi, and forward-backward algorithms to handle a variety of characteristic densities. In this context, however, the densities will be assumed to be Gaussian.
Reference: [23] <author> Pattie Maes, Trevor Darrell, Bruce Blumberg, and Alex Pentland. </author> <title> The ALIVE system: full-body interaction with animated autonomous agents. MIT Media Lab Perceptual Computing Group Technical Report No. </title> <type> 257, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems [27, 14, 32, 42], ballet step recognition [5], body capture [1], real-time interface systems <ref> [38, 23] </ref>, and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9]. <p> A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions <ref> [19, 23, 5] </ref>, expression tracking [11], conducting music [24], and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. <p> Simpler, less constrained hand tracking systems have been created in a variety of environments. Krueger [19] has shown light table systems that tracked hands using single and multiple cameras aimed at the entire body. Maes et al <ref> [23] </ref> showed a similar system with one camera which allowed more arbitrary backgrounds to be used. Unfortunately, camera resolution often limits whole body systems to recovering just the position of the hands. When cameras are dedicated to the hands, more detail can be obtained. <p> Since finger spelling is not being allowed and there are few ambiguities in the test vocabulary based on individual finger motion, a relatively coarse tracking system may be used. Based on previous work <ref> [23] </ref>, it was assumed that a system could be designed to separate the hands from the rest of the scene (explained in the next section). Traditional vision algorithms could then be applied to the binarized result.
Reference: [24] <author> H. Morita, S. Hashimoto, and S. Ohteru. </author> <title> A computer music system that follows a human conductor. </title> <journal> Computer, </journal> <volume> 24(7) </volume> <pages> 44-53, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions [19, 23, 5], expression tracking [11], conducting music <ref> [24] </ref>, and electronic presentation [38]. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. Tracking the natural hand in real time using camera imagery is difficult, but successful systems have been demonstrated in controlled settings. <p> This, combined with Kalman filtering, simplifies occlusion problems and allows recovery of a detailed hand model through a wide range of motion. Datagloves by VPL are often used for sensing as well <ref> [24, 39, 4] </ref>.
Reference: [25] <author> Eadweard Muybridge. </author> <title> Human and Animal Locomotion, volume 1-2. </title> <publisher> Dover Publications, Inc., </publisher> <address> Mineola, N.Y., </address> <year> 1979. </year>
Reference-contexts: Techniques from cognition, psychophysics, dynamics, photography, and athletics can all be applied to help constrain problem domains. 2.1 Analyzing Human Body Motion Photography has been used to help understand human body motion for over a century <ref> [25] </ref>. More recently, computers have been added to perform more complex analysis. Athletic programs may use computer tracking systems and dynamics to help maximize the amount of effort their athletes can produce. Many of these systems use hand labeled data or wired sensor systems to produce the data.
Reference: [26] <author> R. Nag, K. H. Wong, and F. Fallside. </author> <title> Script recognition using hidden markov models. </title> <booktitle> In ICASSP 86, </booktitle> <year> 1986. </year>
Reference-contexts: Most early work was limited to handwriting recognition <ref> [21, 26] </ref>. More recently, He and Kundu [13] report using continuous density HMM's to classify planar shapes. Their method segmented closed shapes and exploited characteristic relations between consecutive segments for classification. The algorithm was reported to tolerate shape contour perturbation and some occlusion.
Reference: [27] <author> S. Niyogi and E. Adelson. </author> <title> Analyzing gait with spatiotemporal surfaces. </title> <type> Technical Report 290, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <address> 20 Ames St, Cambridge MA, </address> <year> 1994. </year> <note> To appear in IEEE Workshop on Nonrigid and articulated Motion, </note> <institution> Austin TX, </institution> <month> Nov 94. </month>
Reference-contexts: Therefore, both natural constraints of the human body and simplifying assumptions are often used to curtail the amount of data needed for analysis. Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems <ref> [27, 14, 32, 42] </ref>, ballet step recognition [5], body capture [1], real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9].
Reference: [28] <author> D. H. Parish, G. Sperling, </author> <title> and M.S. Landy. Intelligent temporal subsampling of american sign language using event boundaries. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 16(2) </volume> <pages> 282-294, </pages> <year> 1990. </year>
Reference-contexts: The hand tracking stage of the system does not attempt to produce a fine-grain description of hand shape; studies have shown that such detailed information may be unnecessary for humans to interpret sign language <ref> [28] </ref>. Instead, the 6 tracking process produces only a coarse description of hand shape, orientation, and trajec-tory. The user is required to wear inexpensive colored gloves to facilitate the hand tracking frame rate and stability. <p> Sperling and Parish <ref> [35, 28] </ref> have done careful studies on the bandwidth necessary for a sign conversation using spatially and temporally subsampled images.
Reference: [29] <author> H. Poizner, U. Bellugi, and V. Lutes-Driscoll. </author> <title> Perception of american sign language in dynamic point-light displays. </title> <booktitle> volume 7, </booktitle> <pages> pages 430-440, </pages> <year> 1981. </year>
Reference-contexts: Point light experiments (where "lights" are attached to significant locations on the body and just these points are used for 18 recognition), have been carried out by Poizner et al <ref> [29] </ref> and Tartter and Knowlton [41]. Tartter's 27 light experiment (13 lights per hand plus one on the nose) showed that a brief conversation in ASL was possible using only these stimuli.
Reference: [30] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1996. </year> <month> 50 </month>
Reference-contexts: In the following sections the vision methods used to track the hands are described, and the basics of HMM's are applied to ASL recognition. 4.1 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [43, 16, 30, 2, 20] </ref>, this section modifies a traditional discussion of the algorithms so as to provide the perspective used for recognizing sign language. <p> For example, if there are two mostly disjoint state sequences through one model with medium probability and one state sequence through a second model with high probability, the Viterbi algorithm would 30 favor the second HMM over the first. However, <ref> [30] </ref> shows that the probabilities obtained from both methods may be typically very close. In practice, the Viterbi algorithm may be modified with a limit on the lowest numerical value of the probability of the state sequence, which in effect causes a beam search of the space.
Reference: [31] <author> J. M. Rehg and T. Kanade. DigitEyes: </author> <title> vision-based human hand tracking. </title> <institution> School of Computer Science Technical Report CMU-CS-93-220, Carnegie Mellon University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: A greyscale camera tracks the hand in a small area on a table and uses hand and finger position to control the direction of a virtual graphics camera. Rehg and Kanade have shown a two camera system that can recover the full 27 degrees of freedom in a hand <ref> [31] </ref>. While successfully demonstrating tracking with limited motion, 17 occlusion was not addressed. Furthermore, a simple background was required, which is often impossible when observing natural hand gestures. Simpler, less constrained hand tracking systems have been created in a variety of environments.
Reference: [32] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 94-115, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Therefore, both natural constraints of the human body and simplifying assumptions are often used to curtail the amount of data needed for analysis. Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems <ref> [27, 14, 32, 42] </ref>, ballet step recognition [5], body capture [1], real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9].
Reference: [33] <author> Jennifer Schlenzig, Edd Hunter, and Ramesh Jain. </author> <title> Recursive identification of gesture inputers using hidden markov models. </title> <booktitle> In Proc. of the Second Annual Conference on Applications of Computer Vision, </booktitle> <pages> pages 187-194, </pages> <year> 1994. </year>
Reference-contexts: This experiment is significant because it used a 25x25 pixel quantized subsampled camera image as a feature vector. Even with such low-level information, the model could learn the set of motions to perform respectable recognition rates. Schlenzig et al <ref> [33] </ref> also use hidden Markov models for visual gesture recognition. The gestures are limited to "hello," "good-bye," and "rotate". The authors report "intuitively" defining the HMM associated with each gesture and imply that the normal Baum-Welch re-estimation method was not implemented.
Reference: [34] <author> Jeffrey Mark Siskind and Quaid Morris. </author> <title> A maximum-likelihood approach to visual event perception. </title> <type> manuscript, </type> <year> 1995. </year>
Reference-contexts: However, this study shows the continuous gesture recognition capabilities of HMM's by recognizing gesture sequences. Several vision systems have been developed with technology closely related to HMM methodology. Darrell [8] uses the dynamic time warping method to recognize gestures ("hello" and "good-bye") through time. Siskind and Morris <ref> [34] </ref> argue that event perception requires less information and may be an easier problem than object recognition.
Reference: [35] <author> G. Sperling, M. Landy, Y. Cohen, and M. Pavel. </author> <title> Intelligible encoding of ASL image sequences at extremely low information rates. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 31 </volume> <pages> 335-391, </pages> <year> 1985. </year>
Reference-contexts: Conversants in ASL will often describe a person, place, or thing and then point to a place in space to temporarily store that object for later reference <ref> [35] </ref>. For example, "the man with the green sweater," "the old grocery store," and "the red garage" might be signed and put into various positions in space. <p> Sperling and Parish <ref> [35, 28] </ref> have done careful studies on the bandwidth necessary for a sign conversation using spatially and temporally subsampled images.
Reference: [36] <author> Thad Starner, John Makhoul, Richard Schwartz, and George Chou. </author> <title> On-line cursive handwriting recognition using speech recognition methods. </title> <booktitle> In ICASSP 94, </booktitle> <year> 1994. </year>
Reference-contexts: The training process can automatically align the components of the transcription to the data. Thus, no special effort is needed to label training data. The segmentation problem, as often seen in handwriting research, can be avoided altogether <ref> [36] </ref>. Recognition is also performed on a continuous data stream. Again, no explicit segmentation is necessary. The segmentation of sentences into words occurs naturally by incorporating the use of a lexicon and a language model into the recognition process. <p> Feature vectors will simply result in multi-dimensional Gaussian densities. Given this freedom, what features should be used to recognize sign language? Previous experience has shown that starting simple and evolving the feature set is often best <ref> [36] </ref>. Since finger spelling is not being allowed and there are few ambiguities in the test vocabulary based on individual finger motion, a relatively coarse tracking system may be used. <p> As such, context modeling would tend to suppress this sentence in recognition unless strong evidence was given for it. In speech and handwriting, a factor of 2 and factor of 4 cut in error rate can be expected for application of contexts and grammars respectively <ref> [36] </ref>. While ASL does not have the same hierarchy of components as speech and handwriting (letters to words to sentences), the factor of 3 decrease in error when the grammar was applied hints at similar performance increases at the sentence level.
Reference: [37] <author> W. C. Stokoe, D. C. Casterline, and C. G. Groneberg. </author> <title> A Dictionary of American Sign Language on Linguistic Principles. </title> <publisher> Linstok Press, </publisher> <address> London, </address> <year> 1976. </year>
Reference-contexts: In the Stokoe ASL dictionary <ref> [37] </ref>, 12 tabulars, 19 designators, and 24 signations are 10 distinguished. Even though both hands are used in ASL, only seven of the signations use two hands.
Reference: [38] <author> Y. Suenaga, K. Mase, M. Fukumoto, and Y. Watanabe. </author> <title> Human reader: an advanced man-machine interface based on human images and speech. </title> <journal> Systems and Computers in Japan, </journal> <volume> 24(2) </volume> <pages> 88-101, </pages> <year> 1993. </year>
Reference-contexts: Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems [27, 14, 32, 42], ballet step recognition [5], body capture [1], real-time interface systems <ref> [38, 23] </ref>, and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9]. <p> A large variety of interfaces have been proposed, using video driven gestures for mouse control [12], full body interactions [19, 23, 5], expression tracking [11], conducting music [24], and electronic presentation <ref> [38] </ref>. Due to their expressiveness, the hands have been a point of focus for many gesture recognition systems. Tracking the natural hand in real time using camera imagery is difficult, but successful systems have been demonstrated in controlled settings. <p> Unfortunately, camera resolution often limits whole body systems to recovering just the position of the hands. When cameras are dedicated to the hands, more detail can be obtained. The "Hand Reader" by Suenaga et al <ref> [38] </ref> recovers 3D pointing information by dedicating two cameras, at close range, to the task. A limitation of such systems is that the working volume tends to be small. Also, the cameras for these systems tend to be obtrusive in that they are placed near the user.
Reference: [39] <author> T. Takahashi and F. Kishino. </author> <title> Hand gesture coding based on experiments using a hand gesture interface device. </title> <journal> SIGCHI Bulletin, </journal> <volume> 23(2) </volume> <pages> 67-73, </pages> <year> 1991. </year>
Reference-contexts: In sign language, each gesture already has an assigned meaning (or meanings) and strong rules of context and grammar may be applied to make recognition tractable. Most work on sign language recognition employs expensive wired "datagloves" that the user must wear <ref> [39] </ref>. In addition, these systems mostly concentrate on finger signing, where the user spells each word with hand signs corresponding to the letters in the alphabet [10]. However, most sign does not involve finger spelling but signs that represent whole words. <p> This, combined with Kalman filtering, simplifies occlusion problems and allows recovery of a detailed hand model through a wide range of motion. Datagloves by VPL are often used for sensing as well <ref> [24, 39, 4] </ref>. <p> A chereme is composed of the tab, dez, and sig as discussed earlier. Charayaphan and Marble [7] demonstrated a feature set that could distinguish between the 31 isolated ASL signs in their training set. Takahashi and Kishino in <ref> [39] </ref> discuss a Dataglove-based system that could recognize 34 of the 46 Japanese kana alphabet gestures (user dependent) using a joint angle and hand orientation coding technique.
Reference: [40] <author> Shinichi Tamura and Shingo Kawasaki. </author> <title> Recognition of sign language motion images. </title> <booktitle> volume 21, </booktitle> <pages> pages 343-353, </pages> <year> 1988. </year>
Reference-contexts: However, most machine recognition systems to date have studied isolated and/or static gestures. In many cases these gestures are finger spelling signs, whereas everyday ASL uses word signs for speed. Tamura and Kawasaki demonstrated an early image processing system which could recognize 20 Japanese signs based on matching cheremes <ref> [40] </ref>. A chereme is composed of the tab, dez, and sig as discussed earlier. Charayaphan and Marble [7] demonstrated a feature set that could distinguish between the 31 isolated ASL signs in their training set.
Reference: [41] <author> V. C. Tartter and K. C. Knowlton. </author> <title> Perceiving sign language from an array of 27 moving spots. </title> <booktitle> volume 289, </booktitle> <pages> pages 676-678, </pages> <year> 1981. </year> <month> 51 </month>
Reference-contexts: Point light experiments (where "lights" are attached to significant locations on the body and just these points are used for 18 recognition), have been carried out by Poizner et al [29] and Tartter and Knowlton <ref> [41] </ref>. Tartter's 27 light experiment (13 lights per hand plus one on the nose) showed that a brief conversation in ASL was possible using only these stimuli.
Reference: [42] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time-sequetial images using hidden markov model. </title> <booktitle> In Proc. 1992 IEEE Conf. on Computer Vision and Pattern Rec., </booktitle> <pages> pages 379-385. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, both natural constraints of the human body and simplifying assumptions are often used to curtail the amount of data needed for analysis. Several systems that address whole body systems have been developed in the past. These include gait recognition and analysis systems <ref> [27, 14, 32, 42] </ref>, ballet step recognition [5], body capture [1], real-time interface systems [38, 23], and numerous others. Greater accuracy and detail can be gained by focusing attention on the body part of interest. Recent experimentation with active "focus of attention" systems is attracting interest to this topic [9].
Reference: [43] <author> S. J. Young. </author> <title> HTK: Hidden Markov Model Toolkit V1.5. </title> <institution> Cambridge University Engineering Department Speech Group and Entropic Research Laboratories Inc., </institution> <address> Washing-ton DC, </address> <month> December </month> <year> 1993. </year> <month> 52 </month>
Reference-contexts: In the following sections the vision methods used to track the hands are described, and the basics of HMM's are applied to ASL recognition. 4.1 Hidden Markov Modeling While a substantial body of literature exists on HMM technology <ref> [43, 16, 30, 2, 20] </ref>, this section modifies a traditional discussion of the algorithms so as to provide the perspective used for recognizing sign language. <p> While the technique described only handles a single observation sequence, it is easy to extend to a set of observation sequences. A more formal discussion can be found in <ref> [16, 2, 43] </ref>. While the estimation and evaluation processes described above are sufficient for the development of an HMM system, the Viterbi algorithm provides a quick means of evaluating a set of HMM's in practice as well as providing a solution for the decoding problem. <p> While this is not strictly proper, the values are approximately equal in contiguous iterations [16] and seem not to make an empirical difference <ref> [43] </ref>. <p> While initial training of the models might rely on manual segmentation or dividing the evidence evenly among the models, embedded training trains the models in situ and allows these boundaries to shift through a probabilistic entry into the initial states of each model <ref> [43] </ref>. Context training uses the co-occurrence of two or more fundamental units to allow recognition of blocks of units, which have more evidence than single units alone. In speech recognition, two and three phoneme blocks (biphones and triphones) are generally used.
References-found: 43

