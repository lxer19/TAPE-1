URL: http://www.pmg.lcs.mit.edu/papers/dgc.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/areas/gc.html
Root-URL: 
Title: Fault-Tolerant Distributed Garbage Collection in a Client-Server Object-Oriented Database  
Author: Umesh Maheshwari Barbara H. Liskov 
Address: Cambridge, MA 02139  
Affiliation: M.I.T. Laboratory for Computer Science  
Abstract: We present a scalable garbage collection scheme for systems that store objects at multiple servers while clients run transactions on locally cached copies of objects. It is the first scheme that provides fault tolerance for such a system: Servers recover from failures and retrieve information needed for safe garbage collection; clients do not recover from failures, yet the scheme is able to reclaim objects referenced only from failed clients. The scheme is optimized to reduce overhead on common client operations, and it provides fault tolerance by doing work in the background and during client operations that are infrequent. 
Abstract-found: 1
Intro-found: 1
Reference: [Ady94] <author> A. Adya. </author> <title> A Distributed Commit Protocol for Optimistic ConcurrencyControl. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: To commit the current transaction, the client sends the request to a server selected from the owners of objects used in the transaction. Copies of objects modified during the transaction are sent along with the request. A 2-phase commit is needed for transactions that use objects at multiple servers <ref> [Gra78, Ady94] </ref>. We outline the commit protocol since it must be augmented to make garbage collection run properly. The selected server acts as the coordinator; it sends prepare messages to the owners of other used ob jects, called the participants. <p> When a transaction commits, any volatile objects that have become reachable from the persistent root are sent to the coordinator; the objects then become persistent at some preferred server. Transactions are serialized using optimistic concurrency control <ref> [KR81, Ady94] </ref>. Objects are not locked when fetched by a client, so that other clients are free to fetch and modify them. Modifications committed by one client may cause objects cached by another to become invalid.
Reference: [Ali84] <author> K. A. M. Ali. </author> <title> Garbage Collection Schemes for Distributed Storage Systems. </title> <booktitle> Proceedings of Workshop on Implementation of Functional Languages, </booktitle> <pages> pages 422-428, </pages> <address> As-penas, Sweden, </address> <month> February </month> <year> 1985. </year>
Reference-contexts: Most scalable systems therefore use some variant of distributed reference counting [Bis77]. The variants differ in the information kept for incoming remote references. Some schemes only record a flag for remotely referenced local objects <ref> [Ali84, JJ92] </ref>. Although this approach minimizes the reference information, it cannot detect locally when an object ceases to be referenced remotely. Other schemes record a count of how many external nodes have references to an object [Ves87].
Reference: [BENOW93] <author> A. Birrell, D. Evers, G. Nelson, S. Owicki, and E. Wobber. </author> <title> Distributed Garbage Collection for Network Objects. </title> <note> Systems Research Center Technical Report 116, Digital, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: We call the list the inlist for N 2 at N 1 , denoted as IN (N 2 )@N 1 . In our system, clients do not have inlists, while servers keep inlists for clients and other servers. Similar schemes have been used before for non-client-server systems <ref> [SDP92, BENOW93] </ref>; we have adapted them to handle fetches and commits and to provide the fault tolerance needed in our environment. <p> Unlike inserting entries into the inlists, deletion can be done lazily. Synchronous insertion and lazy deletion guarantee the safety invariant. There are two ways of removing entries from inlists. N 2 may send a delete message to N 1 for z <ref> [BENOW93] </ref> or N 2 may periodically send the complete list of references that it holds for objects in N 1 [SDP92]. We call the list the outlist for N 1 at N 2 , denoted as OUT (N 1 )@N 2 . <p> In this case it may hold references to deallocated objects, and these references must be prevented from corrupting persistent objects at other servers. One solution is for servers to never reuse references assigned to deleted objects, as in <ref> [BENOW93] </ref>. This would allow the owner of a reference to detect whether the reference is invalid by checking if the referenced object exists. Owners need to perform this check on receiving insert messages to prevent dangling references from entering server objects. <p> The fixes suggested involve either a global mechanism or indefinite retention of garbage. The model does not consider nodes that recover from crashes. <ref> [BENOW93] </ref> uses reference listing for a model similar to that of [SDP92], but takes the opposite approach. It sends synchronous insert messages rather than have temporary nodes provide indirect protection. When a node crashes, its inlists at other nodes are discarded.
Reference: [Bev87] <author> D. I. Bevan. </author> <title> Distributed Garbage Collection Using Reference Counting. </title> <booktitle> Lecture Notes in Computer Science 259, </booktitle> <pages> pages 176-187, </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: The increment and decrement messages must be sent reliably without duplication, loss, or reordering. Most of the research in the area has focused on how to avoid the extra messages <ref> [Bev87, Piq91] </ref>; these schemes do not address node failures. [MS91] uses a combination of a reference count and a bit per client for remotely referenced objects to handle crashes. [SDP92] uses reference listing and outlist messages. Nodes use a form of indirect protection to suppress insert messages.
Reference: [Bis77] <author> P. B. Bishop. </author> <title> Computer Systems with a Very Large Address Space, and Garbage Collection. </title> <type> Technical Report MIT/LCS/TR-178, </type> <institution> MIT Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <month> May </month> <year> 1977. </year>
Reference-contexts: Nevertheless, we cannot discard y because C may use it later. In fact, C may make it reachable from the persistent root again. 3 The basic scheme: reference listing Scalable distributed garbage collection schemes use variants of reference counting between separately traced regions as pioneered by <ref> [Bis77] </ref>. Each node does local garbage collections independently of other nodes. <p> Such schemes do not tolerate node crashes; further a global sweep over large numbers of nodes each with large storage does not allow timely collection of garbage. Most scalable systems therefore use some variant of distributed reference counting <ref> [Bis77] </ref>. The variants differ in the information kept for incoming remote references. Some schemes only record a flag for remotely referenced local objects [Ali84, JJ92]. Although this approach minimizes the reference information, it cannot detect locally when an object ceases to be referenced remotely. <p> The second approach is to migrate objects unreachable from local roots to the node from which they are referenced. The scheme meshes well with reference listing because that provides information about which remote nodes reference a local object. This approach was originally proposed by <ref> [Bis77] </ref> and is used in [SGP90]. 9 Conclusion This paper has described a distributed garbage collection scheme for a client-server object-oriented database. Like other scalable schemes, our scheme is a variant of reference counting, with the difference that it is integrated with client caching and distributed transactions.
Reference: [DLMM93] <author> M. Day, B. Liskov, U. Maheshwari, and A. Myers. </author> <title> References to Remote Mobile Objects in Thor. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <year> 1994. </year>
Reference-contexts: Objects are clustered so that objects that refer to one another are likely to be in the same segment. A reference is a triple howner-id, segment-id, indexi, which allows objects to be located efficiently <ref> [DLMM93] </ref>. References are recycled for pragmatic reasons: after an object is reclaimed, a new object may be given the same reference. User applications run on client machines and interact with Thor by invoking object methods. <p> Owners need to perform this check on receiving insert messages to prevent dangling references from entering server objects. We rejected this scheme because of its expense: in addition to the cost of the check, it precludes the use of small and efficient references <ref> [DLMM93] </ref>. Reuse of references allows us to use small references that contain information to locate objects efficiently, and to avoid maintaining information about deleted objects; note that in a long-lived system like ours, there can be a large number of deleted objects.
Reference: [Gra78] <author> J. N. Gray. </author> <booktitle> Notes on Database Operating Systems. Operating Systems: An Advanced Course (Lecture Notes in Computer Science 60), </booktitle> <pages> pages 393-481, </pages> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: To commit the current transaction, the client sends the request to a server selected from the owners of objects used in the transaction. Copies of objects modified during the transaction are sent along with the request. A 2-phase commit is needed for transactions that use objects at multiple servers <ref> [Gra78, Ady94] </ref>. We outline the commit protocol since it must be augmented to make garbage collection run properly. The selected server acts as the coordinator; it sends prepare messages to the owners of other used ob jects, called the participants.
Reference: [HK82] <author> P. Hudak, and R. Keller. </author> <title> Garbage Collection and Task Deletion in Distributed Applicative Processing Systems. </title> <booktitle> ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 168-178, </pages> <month> August </month> <year> 1982. </year>
Reference-contexts: Global-marking traverses the entire object graph from the roots, sending marking messages to span remote references <ref> [HK82] </ref>. Such schemes do not tolerate node crashes; further a global sweep over large numbers of nodes each with large storage does not allow timely collection of garbage. Most scalable systems therefore use some variant of distributed reference counting [Bis77].
Reference: [Hug85] <author> J. Hughes. </author> <title> A Distributed Garbage Collection Algorithm. </title> <booktitle> Functional Programming and Computer Architecture (Lecture Notes in Computer Science 201), </booktitle> <pages> pages 256-272, </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1985. </year>
Reference-contexts: Variants of distributed reference counting do not collect distributed cyclic garbage. There are two approaches to handling this problem. One is to use complementary global marking [JJ92]. <ref> [Hug85] </ref> propagates timestamps instead of marks so that multiple rounds of marking and collection proceed simultaneously. [LQP92] uses marking within groups of nodes so that a cycle of garbage can be collected by a group that includes the nodes on which the cycle resides.
Reference: [JJ92] <author> N. C. Juul, E. </author> <month> Jul. </month> <title> Comprehensive and Robust Garbage Collection in a Distributed System. </title> <booktitle> 1992 International Workshop on Memory Management, (Lecture Notes in Computer Science 637), </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Most scalable systems therefore use some variant of distributed reference counting [Bis77]. The variants differ in the information kept for incoming remote references. Some schemes only record a flag for remotely referenced local objects <ref> [Ali84, JJ92] </ref>. Although this approach minimizes the reference information, it cannot detect locally when an object ceases to be referenced remotely. Other schemes record a count of how many external nodes have references to an object [Ves87]. <p> The model involves multiple clients and a single server, and the paper focuses on the various alternatives for local collection at the server. Variants of distributed reference counting do not collect distributed cyclic garbage. There are two approaches to handling this problem. One is to use complementary global marking <ref> [JJ92] </ref>. [Hug85] propagates timestamps instead of marks so that multiple rounds of marking and collection proceed simultaneously. [LQP92] uses marking within groups of nodes so that a cycle of garbage can be collected by a group that includes the nodes on which the cycle resides.
Reference: [KR81] <author> H. T. Kung, J. T. Robinson. </author> <title> On Optimistic Methods for Concurrency Control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2), </volume> <pages> pages 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: When a transaction commits, any volatile objects that have become reachable from the persistent root are sent to the coordinator; the objects then become persistent at some preferred server. Transactions are serialized using optimistic concurrency control <ref> [KR81, Ady94] </ref>. Objects are not locked when fetched by a client, so that other clients are free to fetch and modify them. Modifications committed by one client may cause objects cached by another to become invalid.
Reference: [LDS92] <author> B. Liskov, M. Day, and L. Shrira. </author> <title> Distributed Object Management in Thor. Distributed Object Management, </title> <editor> ed. M. T. Ozsu, U. Dayal, and P. Valduriez, </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Section 6 describes how server and client failures are handled. Section 7 summarizes the space and time overheads of the scheme. We discuss related work in Section 8 and conclude in Section 9. 2 The environment Our algorithm is designed for use in the Thor object-oriented database system <ref> [LDS92] </ref>. Thor provides a universe of persistent objects stored at geographically distributed servers. The server where an object resides is referred to as its owner. Objects contain references to other 1 objects, which may reside at any server.
Reference: [LGGJSW91] <author> B. Liskov, S. Ghemawat, R. Gruber, P. Johnson, L. Shrira, and M. Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 226-238, </pages> <year> 1991. </year>
Reference-contexts: Clients and servers have different fault-tolerance characteristics. Servers are persistent and eventually recover from crashes; they are replicated for high availability and reliability <ref> [LGGJSW91] </ref>. Clients are temporary: they may terminate either normally or due to a crash. Further, servers cannot differentiate between client crashes and long term communication failures like network partitions.
Reference: [LL92] <author> R. Ladin, and B. Liskov. </author> <title> Garbage Collection of a Distributed Heap. </title> <booktitle> Int. Conference on Distributed Computing Systems, </booktitle> <pages> pages 708-715, </pages> <address> Yokohoma, Japan, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: When a node crashes, its inlists at other nodes are discarded. As in our scheme, the inlist for a live but uncommunicative node might be discarded; the use of stale references by such nodes is detected by not reusing object references. The model does not consider persistent nodes. <ref> [LL92] </ref> uses a logically centralized service that tracks all inter-node references. Nodes inform the service of their outgoing references and references in transit to other nodes. They also query the service about the reachability of their remotely referenced objects.
Reference: [LQP92] <author> B. Lang, C. Queinnec, and J. Piquer. </author> <title> Garbage Collecting the World. </title> <booktitle> Proceedings of the 19th Annual ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, </booktitle> <pages> pages 39-50, </pages> <address> Albuquerque, </address> <month> Jan </month> <year> 1992. </year>
Reference-contexts: Variants of distributed reference counting do not collect distributed cyclic garbage. There are two approaches to handling this problem. One is to use complementary global marking [JJ92]. [Hug85] propagates timestamps instead of marks so that multiple rounds of marking and collection proceed simultaneously. <ref> [LQP92] </ref> uses marking within groups of nodes so that a cycle of garbage can be collected by a group that includes the nodes on which the cycle resides. The second approach is to migrate objects unreachable from local roots to the node from which they are referenced.
Reference: [Mah93] <author> U. Maheshwari. </author> <title> Distributed Garbage Collection in a Client-Server, Transactional, Persistent Object System. </title> <type> Technical Report MIT/LCS/TR-574, </type> <institution> Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Commits that involve insert messages thus have an added latency of one additional message. <ref> [Mah93] </ref> describes a scheme that hides this latency by having the coordinator send the insert messages on behalf of the participants in parallel with the prepare messages.
Reference: [MS91] <author> L. Manchini, and S. K. Shrivastava. </author> <title> Fault-Tolerant Reference Counting for Garbage Collection in Distributed Systems. </title> <journal> The Computer Journal, </journal> <volume> 34(6), </volume> <pages> pages 503-513, </pages> <month> Decem-ber </month> <year> 1991. </year>
Reference-contexts: The increment and decrement messages must be sent reliably without duplication, loss, or reordering. Most of the research in the area has focused on how to avoid the extra messages [Bev87, Piq91]; these schemes do not address node failures. <ref> [MS91] </ref> uses a combination of a reference count and a bit per client for remotely referenced objects to handle crashes. [SDP92] uses reference listing and outlist messages. Nodes use a form of indirect protection to suppress insert messages.
Reference: [Piq91] <author> J. M. Piquer. </author> <title> Indirect Reference Counting: A Distributed Garbage Collection Algorithm. </title> <booktitle> PARLE '91 Parallel Architecture and Languages (Lecture Notes in Computer Science 505), </booktitle> <pages> pages 150-165, </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: The situation is shown in Figure 5. In effect, the sender secures the reference to the object at the owner on behalf of the receiver (the client). We refer to this scheme as indirect protection. The schemes used in <ref> [Piq91, SDP92] </ref> are similar, but implemented differently. <p> The increment and decrement messages must be sent reliably without duplication, loss, or reordering. Most of the research in the area has focused on how to avoid the extra messages <ref> [Bev87, Piq91] </ref>; these schemes do not address node failures. [MS91] uses a combination of a reference count and a bit per client for remotely referenced objects to handle crashes. [SDP92] uses reference listing and outlist messages. Nodes use a form of indirect protection to suppress insert messages.
Reference: [SDP92] <author> M. Shapiro, P. Dickman, and D. Plainfosse. </author> <title> Robust, Distributed References and Acyclic garbage Collection. </title> <booktitle> Symposium on Principles of Distributed Computing, </booktitle> <address> Vancouver, Canada, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: We call the list the inlist for N 2 at N 1 , denoted as IN (N 2 )@N 1 . In our system, clients do not have inlists, while servers keep inlists for clients and other servers. Similar schemes have been used before for non-client-server systems <ref> [SDP92, BENOW93] </ref>; we have adapted them to handle fetches and commits and to provide the fault tolerance needed in our environment. <p> There are two ways of removing entries from inlists. N 2 may send a delete message to N 1 for z [BENOW93] or N 2 may periodically send the complete list of references that it holds for objects in N 1 <ref> [SDP92] </ref>. We call the list the outlist for N 1 at N 2 , denoted as OUT (N 1 )@N 2 . Upon receiving the outlist, N 1 uses it to replace IN (N 2 )@N 1 . <p> The situation is shown in Figure 5. In effect, the sender secures the reference to the object at the owner on behalf of the receiver (the client). We refer to this scheme as indirect protection. The schemes used in <ref> [Piq91, SDP92] </ref> are similar, but implemented differently. <p> Most of the research in the area has focused on how to avoid the extra messages [Bev87, Piq91]; these schemes do not address node failures. [MS91] uses a combination of a reference count and a bit per client for remotely referenced objects to handle crashes. <ref> [SDP92] </ref> uses reference listing and outlist messages. Nodes use a form of indirect protection to suppress insert messages. When a node terminates abnormally, other nodes cannot discard their inlists for it because that might cause indirectly protected objects reachable from live nodes to be reclaimed. <p> The fixes suggested involve either a global mechanism or indefinite retention of garbage. The model does not consider nodes that recover from crashes. [BENOW93] uses reference listing for a model similar to that of <ref> [SDP92] </ref>, but takes the opposite approach. It sends synchronous insert messages rather than have temporary nodes provide indirect protection. When a node crashes, its inlists at other nodes are discarded.
Reference: [SGP90] <author> M. Shapiro, O. Gruber, and D. Plainfosse. </author> <title> A Garbage Detection Protocol for a Realistic Distributed Object-Support System. </title> <type> Research Report 1320, </type> <institution> INRIA-Rocquencourt, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The second approach is to migrate objects unreachable from local roots to the node from which they are referenced. The scheme meshes well with reference listing because that provides information about which remote nodes reference a local object. This approach was originally proposed by [Bis77] and is used in <ref> [SGP90] </ref>. 9 Conclusion This paper has described a distributed garbage collection scheme for a client-server object-oriented database. Like other scalable schemes, our scheme is a variant of reference counting, with the difference that it is integrated with client caching and distributed transactions.
Reference: [Ves87] <author> S. C. Vestal. </author> <title> Garbage Collection: An Exercise in Distributed, Fault-Tolerant Programming. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: Some schemes only record a flag for remotely referenced local objects [Ali84, JJ92]. Although this approach minimizes the reference information, it cannot detect locally when an object ceases to be referenced remotely. Other schemes record a count of how many external nodes have references to an object <ref> [Ves87] </ref>. These schemes can detect when an object is no longer remotely referenced, since the count is incremented or decremented as nodes acquire or drop the reference. The increment and decrement messages must be sent reliably without duplication, loss, or reordering.
Reference: [YNY94] <author> V. Yong, J. F. Naughton, and J. Yu. </author> <title> Storage Reclamation and Reorganization in Client Server Persistent Object Stores. </title> <booktitle> Data Engineering, </booktitle> <year> 1994. </year> <month> 10 </month>
Reference-contexts: They also query the service about the reachability of their remotely referenced objects. One drawback of this scheme is that the service may become a bottleneck in a scalable system. The only other scheme we know of that caters to client-server database systems is <ref> [YNY94] </ref>. The model involves multiple clients and a single server, and the paper focuses on the various alternatives for local collection at the server. Variants of distributed reference counting do not collect distributed cyclic garbage. There are two approaches to handling this problem.
References-found: 22

