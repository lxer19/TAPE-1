URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/projects96/brumfield.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/fall1996.html
Root-URL: http://www.cs.colorado.edu
Email: robert.brumfield@colorado.edu  
Title: 1 WWW Caching Simulation  
Author: Robert Brumfield 
Address: Boulder  
Affiliation: University of Colorado,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Spero, S. </author> <title> Analysis of HTTP Performance Problems, </title> <address> (URL) http://sunsite.unc.edu/mdma release/http-prob.html. </address>
Reference-contexts: 1. Introduction The explosion of users of the WorldWide Web (WWW) has lead to a number of concerns regarding bandwidth and latency <ref> [1, 2] </ref>. Solutions to the bandwidth problem can mainly be solved with faster networks. Problems with latency can be addressed via methods ranging from protocol changes to prefetching. Caching is one method reducing such perceived latency.
Reference: [2] <author> Touche, J., J. Heideman, and K. Obraczka. </author> <title> Analysis of HTTP Performance, </title> <note> August 16, 1996. Version 1.2. Submitted to IEEE Communications Magazine. </note>
Reference-contexts: 1. Introduction The explosion of users of the WorldWide Web (WWW) has lead to a number of concerns regarding bandwidth and latency <ref> [1, 2] </ref>. Solutions to the bandwidth problem can mainly be solved with faster networks. Problems with latency can be addressed via methods ranging from protocol changes to prefetching. Caching is one method reducing such perceived latency. <p> Next, the methodology of the comparisons is discussed and results are presented. Then, suggested directions of future work is considered . Last, conclusions summarize the paper. 2. Motivation There are a number of areas where latency can occur during WWW communications <ref> [2] </ref>. There are latencies introduced due to the network: inherent latency occurs from transmission distances between clients and servers; the TCP protocol introduces latencies through startup overhead, anticongestion techniques, and maintaining closed connections; and HTTP protocol latencies occur by transferring a single object per connection. <p> These suggestions also make provisions for methods to retrieve multiple objects with one request to reduce the overhead latency from request and acknowledgment signals. Some, however, believe these improvements would not effect a large number of the Internet users until bandwidth increases significantly <ref> [2] </ref>. Caching objects can reduce latency and bandwidth by reducing server load through replication. Clientside caching maintains objects on the client machine. Clientside caching is particularly good for maintaining objects that do not change significantly over time, such as background bitmaps or graphics.
Reference: [3] <author> Berners-Lee, Tim, R. Fielding, and H. Frystyk. </author> <title> Hypertext Transfer Protocol - HTTP/1.0, RFC 1945, </title> <month> May, </month> <year> 1996. </year>
Reference-contexts: Other latencies can occur if servers become overloaded and cannot respond to client requests in a timely fashion or if clients are unable to parse and display incoming documents quickly enough. There have been a number of suggestions to reduce these latency problems. The current HTTP specification <ref> [3] </ref> incurs connection startup cost for every object retrieved by allowing only one object to be retrieved per connection. Suggested HTTP protocol changes, such as PHTTP, reduce this connection overhead by maintaining longer-term connections for each session [4].
Reference: [4] <author> Padmanabhan, Venkata N. and Jeffrey C. Mogul. </author> <title> Improving HTTP Latency, Computer Networks and ISDN Systems, </title> <address> v.28, nos.1&2, </address> <month> December </month> <year> 1995, </year> <pages> pp. 25-35. </pages>
Reference-contexts: The current HTTP specification [3] incurs connection startup cost for every object retrieved by allowing only one object to be retrieved per connection. Suggested HTTP protocol changes, such as PHTTP, reduce this connection overhead by maintaining longer-term connections for each session <ref> [4] </ref>. These suggestions also make provisions for methods to retrieve multiple objects with one request to reduce the overhead latency from request and acknowledgment signals. Some, however, believe these improvements would not effect a large number of the Internet users until bandwidth increases significantly [2].
Reference: [5] <author> Glassman, Steven. </author> <title> A Caching Relay for the World Wide Web, </title> <institution> System Research Center, - 9 - Digital Equipment Corporation, </institution> <address> 130 Lytton Ave., Palo Alto, CA, </address> <month> 94301. </month>
Reference-contexts: The client, then, can determine which objects, if any, should be retrieved [8] 3. Related Work Caching strategies for the WWW has been researched in <ref> [5, 6] </ref>. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in [10, 11]. In [10] the authors suggest a replacement policy based on Lowest Relative Value (LRV).
Reference: [6] <author> Chankhunthod, Anawat, P. Danzig, C. Neerdaels, M. Schwartz, K. Worrell. </author> <title> A Hierarchical Internet Object Cache, </title> <booktitle> Proceedings of the 1994 Sigmetrics Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> 150-160. </pages>
Reference-contexts: The client, then, can determine which objects, if any, should be retrieved [8] 3. Related Work Caching strategies for the WWW has been researched in <ref> [5, 6] </ref>. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in [10, 11]. In [10] the authors suggest a replacement policy based on Lowest Relative Value (LRV).
Reference: [7] <author> Bestavros, Azer. </author> <title> Using Speculation to Reduce Server Load and Service Time on WWW, Technical Report TR-95-006, </title> <institution> Department of Computer Science, Boston University, </institution> <address> Boston MA, </address> <month> 02215. </month>
Reference-contexts: Examples of proxy server caches are available in [5,6] as well as others. Prefetching allows the client, server, or cache to determine what the next most-likely references will be and retrieve these references before the client actually makes the request <ref> [7, 8, 9] </ref>. Some prefetching schemes require clients and servers to cooperate. In these scenarios, the proxy-servers actually predict the next most likely objects for the client and returns a list of these objects to the client. <p> One study indicates that, on average, round-trip times can be reduced to 0.6 per file [16]. There have been a number of predictive approaches suggested for WWW <ref> [7, 8, 9] </ref>. None of these approaches have applied the Prediction by Partial Match technique to the World-Wide Web. 4. Methodology The traces used for the simulation were obtained from Digital Equipment Corporation. All client machines at Digital access the WWW through one of two proxy server caches.
Reference: [8] <author> Padmanabhan, Venkata N. and Jeffrey C. Mogul. </author> <title> Using Predictive Prefetching to Improve World Wide Web Latency, </title> <journal> ACM SIGCOMM Computer Communication Review, </journal> <month> July </month> <year> 1996. </year>
Reference-contexts: Examples of proxy server caches are available in [5,6] as well as others. Prefetching allows the client, server, or cache to determine what the next most-likely references will be and retrieve these references before the client actually makes the request <ref> [7, 8, 9] </ref>. Some prefetching schemes require clients and servers to cooperate. In these scenarios, the proxy-servers actually predict the next most likely objects for the client and returns a list of these objects to the client. <p> Some prefetching schemes require clients and servers to cooperate. In these scenarios, the proxy-servers actually predict the next most likely objects for the client and returns a list of these objects to the client. The client, then, can determine which objects, if any, should be retrieved <ref> [8] </ref> 3. Related Work Caching strategies for the WWW has been researched in [5, 6]. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in [10, 11]. <p> Utilizing data compression techniques to file system prefetching has been studied in [12, 13]. The Prediction by Partial Match [14] data-compression technique was adapted and applied to file system prefetching in [15]. Applying file system prediction techniques to the Web has been done in <ref> [8] </ref>. FTP servers and caches also mirror our goals of Web prefetching: there is a lot of time to predict the next reference, gather statistics, and a need to overcome network latency. One study indicates that, on average, round-trip times can be reduced to 0.6 per file [16]. <p> One study indicates that, on average, round-trip times can be reduced to 0.6 per file [16]. There have been a number of predictive approaches suggested for WWW <ref> [7, 8, 9] </ref>. None of these approaches have applied the Prediction by Partial Match technique to the World-Wide Web. 4. Methodology The traces used for the simulation were obtained from Digital Equipment Corporation. All client machines at Digital access the WWW through one of two proxy server caches.
Reference: [9] <author> Bestavros, Azer and Carlos Cunha. </author> <title> A prefetching protocol using client speculation for the WWW, Technical report TR-95-011, </title> <institution> Boston University, Computer Science Department, </institution> <address> Boston, MA 02215. </address>
Reference-contexts: Examples of proxy server caches are available in [5,6] as well as others. Prefetching allows the client, server, or cache to determine what the next most-likely references will be and retrieve these references before the client actually makes the request <ref> [7, 8, 9] </ref>. Some prefetching schemes require clients and servers to cooperate. In these scenarios, the proxy-servers actually predict the next most likely objects for the client and returns a list of these objects to the client. <p> One study indicates that, on average, round-trip times can be reduced to 0.6 per file [16]. There have been a number of predictive approaches suggested for WWW <ref> [7, 8, 9] </ref>. None of these approaches have applied the Prediction by Partial Match technique to the World-Wide Web. 4. Methodology The traces used for the simulation were obtained from Digital Equipment Corporation. All client machines at Digital access the WWW through one of two proxy server caches.
Reference: [10] <author> Lorenzitti, Paolo, L. Rizzo, and L. Vicisano. </author> <title> Replacement policies for a proxy cache, </title> <type> Draft of technical report, </type> <institution> Dipartimento di Ingegneria dellInformazione, Universita di Pisa. </institution> <note> (http://www.iet.unipi.it/~luigi/caching.ps.gz). </note>
Reference-contexts: The client, then, can determine which objects, if any, should be retrieved [8] 3. Related Work Caching strategies for the WWW has been researched in [5, 6]. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in <ref> [10, 11] </ref>. In [10] the authors suggest a replacement policy based on Lowest Relative Value (LRV). LRV is a function of time from the last access, number of previous accesses, and document size. They dont feel that the document type (e.g. GIF) has a significant impact on their LRV calculation. <p> Related Work Caching strategies for the WWW has been researched in [5, 6]. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in [10, 11]. In <ref> [10] </ref> the authors suggest a replacement policy based on Lowest Relative Value (LRV). LRV is a function of time from the last access, number of previous accesses, and document size. They dont feel that the document type (e.g. GIF) has a significant impact on their LRV calculation. <p> This gives a theoretical maximum cache hit ratio of 18% for a non-predictive cache. And, combined with the high number of bytes between re-references, could explain the low cache-hit ratios. The 82% unique references is slightly higher than seen in <ref> [10] </ref>, where the authors observed that 67% of the objects were only referenced once. This, however, contradicts other analysis of similar DEC traces for logs outside our selected date range indicating that, on average, 33% of the references are unique. <p> Examining trace logs indicates tendencies that may be utilized to increase caching performance. A number of studies have demonstrated that roughly half of all references are GIF images <ref> [10, 18, 19] </ref>.
Reference: [11] <author> Williams, S., M. Abrams, C.R. Standridge, G. Abdulla, and E.A. Fox. </author> <title> Removal Policies in network Caches for WorldWide Web Docuements, </title> <booktitle> Proceeding of ACM SIGCOMM, </booktitle> <month> August </month> <year> 1996, </year> <institution> Standford University, </institution> <address> CA, USA. </address>
Reference-contexts: The client, then, can determine which objects, if any, should be retrieved [8] 3. Related Work Caching strategies for the WWW has been researched in [5, 6]. Replacement policies for these systems appears to be a standard LRU. Other replacement policies for WWW caches are discussed in <ref> [10, 11] </ref>. In [10] the authors suggest a replacement policy based on Lowest Relative Value (LRV). LRV is a function of time from the last access, number of previous accesses, and document size. They dont feel that the document type (e.g. GIF) has a significant impact on their LRV calculation. <p> This should work fine as long as the majority of the multiply-referenced objects fall under the maximum object size threshold. The effectiveness of this on the WWW was demonstrated in <ref> [11] </ref>, and seems to hold for these results sets as well (See Figure 4 to Figure 7). We see a significant increase in hit ratios for small object size limits. This increase tails off quickly after the one-kilobyte limit, however.
Reference: [12] <author> Vitter, J. S. and P. Krishnan. </author> <title> Optimal Prefetching via Data Compression, </title> <booktitle> Proceedings 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 121-130. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: File system caching, however, matches much closer to the goals of caching on the Web. In particular, there is more time to determine the next file accessed and we can keep statistics on access patterns. Utilizing data compression techniques to file system prefetching has been studied in <ref> [12, 13] </ref>. The Prediction by Partial Match [14] data-compression technique was adapted and applied to file system prefetching in [15]. Applying file system prediction techniques to the Web has been done in [8].
Reference: [13] <author> Curewitz, K. M., P. Krishnan, and J. S. Vitter. </author> <title> Practical Prefetching via Data Compression, </title> <journal> SIG-MOD Record, </journal> <volume> 22(2) </volume> <pages> 257-266. </pages> <publisher> ACM, </publisher> <month> Jun. </month> <year> 1993. </year>
Reference-contexts: File system caching, however, matches much closer to the goals of caching on the Web. In particular, there is more time to determine the next file accessed and we can keep statistics on access patterns. Utilizing data compression techniques to file system prefetching has been studied in <ref> [12, 13] </ref>. The Prediction by Partial Match [14] data-compression technique was adapted and applied to file system prefetching in [15]. Applying file system prediction techniques to the Web has been done in [8].
Reference: [14] <author> Bell, T.C., J.G. Cleary, and I.H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: In particular, there is more time to determine the next file accessed and we can keep statistics on access patterns. Utilizing data compression techniques to file system prefetching has been studied in [12, 13]. The Prediction by Partial Match <ref> [14] </ref> data-compression technique was adapted and applied to file system prefetching in [15]. Applying file system prediction techniques to the Web has been done in [8].
Reference: [15] <author> Kroeger, T. M. and D. D. E. </author> <title> Long, Predicting File System Actions from Prior Events, </title> <booktitle> Proceedings of the USENIX 1996 Annual Technical Conference, p352, </booktitle> <pages> 319-28. </pages>
Reference-contexts: Utilizing data compression techniques to file system prefetching has been studied in [12, 13]. The Prediction by Partial Match [14] data-compression technique was adapted and applied to file system prefetching in <ref> [15] </ref>. Applying file system prediction techniques to the Web has been done in [8]. FTP servers and caches also mirror our goals of Web prefetching: there is a lot of time to predict the next reference, gather statistics, and a need to overcome network latency. <p> The probability the next character will be an e or an i is higher than the probability the next character is a z. This model makes the trie [17] a suitable data structure for storing the context information and statistics. This is also what was utilized in <ref> [15] </ref>. It may not be the most efficient storage technique, but should suit our purposes here. Each clients context is tracked throughout the simulation. <p> This is consistent with results seen for the file system Prediction by Partial Match algorithm <ref> [15] </ref>. If we compare the prediction thresholds of a order 1 context model for our traces, we see thresholds between 5% and 15% perform about the same with regards to cache hits. A slight decrease occurs after 15% (Figure 8).
Reference: [16] <author> Touche, J. D. and D. J. Farber. </author> <title> An Experiment in Latency Reduction, </title> <booktitle> IEEE Infocom 1994. </booktitle> <pages> pp. 175-181. </pages> <month> June, </month> <year> 1994. </year>
Reference-contexts: FTP servers and caches also mirror our goals of Web prefetching: there is a lot of time to predict the next reference, gather statistics, and a need to overcome network latency. One study indicates that, on average, round-trip times can be reduced to 0.6 per file <ref> [16] </ref>. There have been a number of predictive approaches suggested for WWW [7, 8, 9]. None of these approaches have applied the Prediction by Partial Match technique to the World-Wide Web. 4. Methodology The traces used for the simulation were obtained from Digital Equipment Corporation.
Reference: [17] <author> Knuth, </author> <title> The Art of Computer Programming: Sorting and Searching, </title> <publisher> v.3. Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: The probability the next character will be an e or an i is higher than the probability the next character is a z. This model makes the trie <ref> [17] </ref> a suitable data structure for storing the context information and statistics. This is also what was utilized in [15]. It may not be the most efficient storage technique, but should suit our purposes here. Each clients context is tracked throughout the simulation.
Reference: [18] <author> Woodruff, Allison, et.al. </author> <title> An Investigation of Documents from the World Wide Web, </title> <institution> Computer Science Division, University of California at Berkeley. </institution> <note> http://www.cs.berkeley.edu/~woodruff/inktomi/ </note>
Reference-contexts: Examining trace logs indicates tendencies that may be utilized to increase caching performance. A number of studies have demonstrated that roughly half of all references are GIF images <ref> [10, 18, 19] </ref>.
Reference: [19] <author> Gwertzman, James and Margo Seltzer. </author> <title> WorldWide Web Cache Consistency, </title> <booktitle> Proceedings of the USENIX 1996 Annual Technical Conference, p352, </booktitle> <pages> 141-51. </pages>
Reference-contexts: Examining trace logs indicates tendencies that may be utilized to increase caching performance. A number of studies have demonstrated that roughly half of all references are GIF images <ref> [10, 18, 19] </ref>.
References-found: 19

