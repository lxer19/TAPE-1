URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1996/147.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1996/SCSE_publications.html
Root-URL: 
Email: Email: fbjtong,jas,anneg@cse.unsw.edu.au  
Phone: Telephone: +61 2 9385 3980 Fax: +61 2 9385 1813  
Title: Query Size Estimation using Systematic Sampling  
Author: Banchong Harangsri John Shepherd Anne Ngu 
Address: Sydney 2052, AUSTRALIA.  
Affiliation: School of Computer Science and Engineering, The University of New South Wales,  
Abstract: In this paper, we propose a new approach to the estimation of query size for select and join operations. The technique, which we have called "systematic sampling", is a novel variant of the sampling approach, which sorts the relation before sampling, and which maintains a summary relation to improve run-time performance. We compare the method to a number of existing methods to tackle the problem of query size estimation, and demonstrate, with extensive experimental results, that it performs better than existing approaches over a wide range of data sets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha. </author> <title> A Study of Instance-Based Algorithms for Su pervised Learning Tasks: Mathematical, Empirical, and Psychological Evaluations. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Science, University of Califor-nia, </institution> <address> Irvine, CA 92717, </address> <month> Nov 27 </month> <year> 1990. </year>
Reference-contexts: Curve-Fitting The methods [23, 5] in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning <ref> [14, 1, 2] </ref>. Using feedback from user queries (instances), a regression tree [4] is created whose leaf nodes consist of linear regression functions.
Reference: [2] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Curve-Fitting The methods [23, 5] in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning <ref> [14, 1, 2] </ref>. Using feedback from user queries (instances), a regression tree [4] is created whose leaf nodes consist of linear regression functions.
Reference: [3] <author> J. L. Bentley. </author> <title> Multidimensional Binary Search Trees Used for Associative Searching. </title> <journal> Comm. ACM, </journal> <volume> 18(9) </volume> <pages> 507-517, </pages> <year> 1975. </year>
Reference-contexts: Moreover, we can also attach an index (in our implementation a multiple attribute index kd-tree <ref> [3] </ref> is used) to the summary relation to speed up retrieval. Given a query, to estimate its result size, scan the summary relation to find all the tuples which satisfy the query. Let S be the total number of such tuples.
Reference: [4] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Tress. </title> <publisher> Chapman & Hall, Inc., </publisher> <year> 1984. </year>
Reference-contexts: Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning [14, 1, 2]. Using feedback from user queries (instances), a regression tree <ref> [4] </ref> is created whose leaf nodes consist of linear regression functions.
Reference: [5] <author> C. M. Chen and N. Roussopoulos. </author> <title> Adaptive Selectivity Estimation using Query Feedback. </title> <booktitle> In Proceedings of 1994 ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1994. </year>
Reference-contexts: There has been a considerable amount of work on the issue of selectivity estimation over one and a half decades [22, 6, 7, 19, 13, 11, 17, 18, 16, 8, 23, 5]. This work can be classified into four categories <ref> [23, 5] </ref>, namely parametric, histogram, curve fitting and sampling. Let us briefly describe each of them; the reader can find more details in the references given above. <p> However, with our sampling method, we use a summary relation that better summarises (represents) the source relation. The frequency distribution of each attribute of the summary relation "follows" much closer to the actual frequency distributions in the source relation. Curve-Fitting The methods <ref> [23, 5] </ref> in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning [14, 1, 2]. <p> In [9], we compared the performance of M5 and a curve-fitting method called ASE (Adaptive Selectivity Estimation) <ref> [5] </ref>. It appeared that M5 significantly outperformed ASE. These curve-fitting methods can deal very well with queries with simple selections (i.e., whose selection predicates specify on a single attribute) and their performance was even better than the sampling methods.
Reference: [6] <author> S. Christodoulakis. </author> <title> Estimating Block Transfers and Join Sizes. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 40-54, </pages> <year> 1983. </year>
Reference-contexts: This work can be classified into four categories [23, 5], namely parametric, histogram, curve fitting and sampling. Let us briefly describe each of them; the reader can find more details in the references given above. Parametric The parametric methods <ref> [22, 6, 7] </ref> are ones which depend upon underlying assumptions about the data distribution such as uniform, normal, poisson, Zipf distributions and so on. The methods will approximate query result sizes effectively if the actual data distribution follows the a priori assumption.
Reference: [7] <author> S. Christodoulakis. </author> <title> Estimating Record Selectivities. </title> <journal> Infor mation System, </journal> <volume> 8(2) </volume> <pages> 105-115, </pages> <year> 1983. </year>
Reference-contexts: This work can be classified into four categories [23, 5], namely parametric, histogram, curve fitting and sampling. Let us briefly describe each of them; the reader can find more details in the references given above. Parametric The parametric methods <ref> [22, 6, 7] </ref> are ones which depend upon underlying assumptions about the data distribution such as uniform, normal, poisson, Zipf distributions and so on. The methods will approximate query result sizes effectively if the actual data distribution follows the a priori assumption.
Reference: [8] <author> P. Haas and A. Swami. </author> <title> Sequential Sampling Procedures for Query Size Estimation. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 341-350, </pages> <year> 1992. </year>
Reference: [9] <author> B. Harangsri, J. Shepherd, and A. Ngu. </author> <title> Query Size Estimation using Machine Learning. </title> <booktitle> In 1996 International Computer Symposium (ICS '96), </booktitle> <month> December 19-21, </month> <institution> 1996 National Sun Yat-Sen University Kaohsiung, Taiwan, R.O.C. </institution> <year> 1996. </year> <note> To be published. </note>
Reference-contexts: Curve-Fitting The methods [23, 5] in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently <ref> [9] </ref>, we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning [14, 1, 2]. Using feedback from user queries (instances), a regression tree [4] is created whose leaf nodes consist of linear regression functions. <p> In <ref> [9] </ref>, we compared the performance of M5 and a curve-fitting method called ASE (Adaptive Selectivity Estimation) [5]. It appeared that M5 significantly outperformed ASE.
Reference: [10] <author> B. Harangsri, J. Shepherd, and A. Ngu. </author> <title> Query Size Estimation using Systematic Sampling. </title> <type> Technical report, </type> <institution> The University of New South Wales, School of Computer Science and Engineering, </institution> <address> Sydney 2052, AUSTRALIA, </address> <year> 1996. </year>
Reference-contexts: Due to the space limit, we only describe the core concept of the systematic sampling in Section 2. Sections 3-7 are not given here; we refer the reader to the complete version of this paper in <ref> [10] </ref>. 2 Basic Idea of Systematic Sampling In this section, we describe the basic idea of the systematic sampling, which is essentially the same for selections and joins. <p> The main difference between the systematic sampling methods for selections and joins is that the systematic sampling for the former builds a summary relation while the systematic sampling for the latter does not (see Section 4 in <ref> [10] </ref> for query result size estimation for joins). The idea to be explained next, although more for selections, is applicable for joins too. Previous work has proposed runtime sampling to estimate query result sizes. <p> index to the source rela-tion since the source relation must be sorted on several biased attributes, not a single attribute. * It's significantly faster to access a summary relation created separately and estimate query result sizes based on it (see the experiments in the complete version of the paper in <ref> [10] </ref>).
Reference: [11] <author> W. Hou, G. Ozsoyoglu, and B. K. Taneja. </author> <title> Statistical Estimators for Relational Algebra Expressions. </title> <booktitle> In Proceedings of the ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 276-287, </pages> <year> 1988. </year>
Reference: [12] <author> Y. E. Ioannidis and S. Christodoulakis. </author> <title> On the Propagation of Errors in the Size of Join Results. </title> <booktitle> In Proceedings of the ACM-SIGMOD Intl. Conf. on Management of Data, </booktitle> <pages> pages 268-277, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Query optimisers for database systems aim to determine the most efficient query execution plan to be executed by the database system. Choosing an efficient plan relies on cost estimates derived from the statistics maintained by the underlying database system. Work by <ref> [12] </ref> pointed out that inaccurate estimates derived from such statistics may cause the opti-miser to choose a very poor plan.
Reference: [13] <author> N. Kamel and R. King. </author> <title> A Method of Data Distribution Based on Texture Analysis. </title> <booktitle> In Proceedings of the ACM SIGMOD Intl. Conf. on Management of Data, </booktitle> <pages> pages 319-325, </pages> <year> 1985. </year>
Reference: [14] <author> D. Kibler, D. W. Aha, and M. K. Albert. </author> <title> Instance-Based Prediction of Real-Valued Attributes. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 51-57, </pages> <year> 1989. </year>
Reference-contexts: Curve-Fitting The methods [23, 5] in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning <ref> [14, 1, 2] </ref>. Using feedback from user queries (instances), a regression tree [4] is created whose leaf nodes consist of linear regression functions.
Reference: [15] <author> Y. Ling and W. Sun. </author> <title> An Evaluation of Sampling-Based Size Estimation Methods for Selections in Database Systems. </title> <booktitle> In The International Confererence on Data Engineering, </booktitle> <pages> pages 532-539, </pages> <year> 1995. </year>
Reference-contexts: The method we propose in this paper is called systematic sampling [21]. There are two separate algorithms to implement systematic sampling: one for selections and the other for joins (most of the previous work on query size estimation has concentrated only on these two operations). Ling and Sun <ref> [15] </ref> have made a comparison of the three sampling methods: adaptive, double and sequential sampling. They pointed out that, in fact, all the three methods are instances of sequential random sampling 2 , differing only in their stopping conditions. <p> The value of n is basically involved with a stopping condition of a sampling method. In the literature of sampling, there has been a fair amount of work attempting to address the stopping condition. In <ref> [15] </ref>, the authors discussed several stopping conditions. <p> our work is controlled by value of n which is defined as follows: ( p e Y where 1 ff is the confidence level, t 1ff the abscissa of the standard normal curve, e the required relative error, Y total mean of population and V total variance of population (see <ref> [15] </ref> for more de tails).
Reference: [16] <author> R. J. Lipton, J. F. Naughton, and D. A. Schneider. </author> <title> Practical Selectivity Estimation through Adaptive Sampling. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 1-12, </pages> <year> 1990. </year>
Reference: [17] <author> M. Mannino, P. Chu, and T. Sager. </author> <title> Statistical Profile Estimation in Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 20(3) </volume> <pages> 191-221, </pages> <month> september </month> <year> 1988. </year>
Reference: [18] <author> M. Muralikrishma and D. DeWitt. </author> <title> Equi-depth Histograms for Estimating Selectivity Factors for Multi-Dimensional Queries. </title> <booktitle> In Proceedings of the ACM SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 28-36, </pages> <year> 1988. </year>
Reference-contexts: The methods will approximate query result sizes effectively if the actual data distribution follows the a priori assumption. However, in reality, data distributions in real database systems may not fit well with the assumed distribution and, consequently, the quality of the size estimates could be unpredictable. Histogram <ref> [19, 18] </ref> A histogram is built by dividing an attribute domain into buckets and counting the number of tuples which fall into the ranges of the buckets.
Reference: [19] <author> G. Piatetsky-Shapiro and C. Connell. </author> <title> Accurate Estimation of the Number of Tuples Satisfying a Condition. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <pages> pages 256-276, </pages> <address> 1984. Boston, Mass, June, </address> <publisher> ACM, </publisher> <address> New York. </address>
Reference-contexts: The methods will approximate query result sizes effectively if the actual data distribution follows the a priori assumption. However, in reality, data distributions in real database systems may not fit well with the assumed distribution and, consequently, the quality of the size estimates could be unpredictable. Histogram <ref> [19, 18] </ref> A histogram is built by dividing an attribute domain into buckets and counting the number of tuples which fall into the ranges of the buckets.
Reference: [20] <author> J. R. Quinlan. </author> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> In Proceedings of Machine Learning. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Curve-Fitting The methods [23, 5] in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 <ref> [20] </ref> which combines model-based learning and instance-based learning [14, 1, 2]. Using feedback from user queries (instances), a regression tree [4] is created whose leaf nodes consist of linear regression functions.
Reference: [21] <author> R. L. Scheaffer, W. Mendenhall, and L. Ott. </author> <title> Elementary Survey Sampling. </title> <publisher> PWS-KENT Publishing Company, </publisher> <address> fourth edition, </address> <year> 1990. </year>
Reference-contexts: The method we propose in this paper is called systematic sampling <ref> [21] </ref>. There are two separate algorithms to implement systematic sampling: one for selections and the other for joins (most of the previous work on query size estimation has concentrated only on these two operations).
Reference: [22] <editor> P.G. Selinger, M.M. Astrahan, D.D. Chamberlin, R.A. Lo-rie, and T.G. Price. </editor> <title> Access Path Selection in a Relational Database Management System. </title> <booktitle> In ACM SIGMOD, </booktitle> <pages> pages 23-34, </pages> <address> 1979. Boston, MA, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: This work can be classified into four categories [23, 5], namely parametric, histogram, curve fitting and sampling. Let us briefly describe each of them; the reader can find more details in the references given above. Parametric The parametric methods <ref> [22, 6, 7] </ref> are ones which depend upon underlying assumptions about the data distribution such as uniform, normal, poisson, Zipf distributions and so on. The methods will approximate query result sizes effectively if the actual data distribution follows the a priori assumption.
Reference: [23] <author> W. Sun, Y. Ling, N. Rishe, and Y. Deng. </author> <title> An Instant and Accurate Size Estimation Method for Joins and Selection in a Retrieval-Intensive Environment. </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 79-88, </pages> <year> 1993. </year>
Reference-contexts: There has been a considerable amount of work on the issue of selectivity estimation over one and a half decades [22, 6, 7, 19, 13, 11, 17, 18, 16, 8, 23, 5]. This work can be classified into four categories <ref> [23, 5] </ref>, namely parametric, histogram, curve fitting and sampling. Let us briefly describe each of them; the reader can find more details in the references given above. <p> However, with our sampling method, we use a summary relation that better summarises (represents) the source relation. The frequency distribution of each attribute of the summary relation "follows" much closer to the actual frequency distributions in the source relation. Curve-Fitting The methods <ref> [23, 5] </ref> in this class are based on using polynomial regression to find the best-fit set of coefficients to minimise the criterion of least-squared error. Recently [9], we have proposed the use of a learning machine called M5 [20] which combines model-based learning and instance-based learning [14, 1, 2]. <p> It appeared that M5 significantly outperformed ASE. These curve-fitting methods can deal very well with queries with simple selections (i.e., whose selection predicates specify on a single attribute) and their performance was even better than the sampling methods. Both our experiments and the experiments reported in <ref> [23] </ref> for the queries with simple selections confirmed this observation in the same direction. However, the methods do not handle well queries with complex selections whose selection predi-cates specify on multiple attributes. In these cases they actually performed worse than sampling methods. <p> However, the methods do not handle well queries with complex selections whose selection predi-cates specify on multiple attributes. In these cases they actually performed worse than sampling methods. Again both our experiments and the experiments reported in <ref> [23] </ref> confirmed this observation.
References-found: 23

