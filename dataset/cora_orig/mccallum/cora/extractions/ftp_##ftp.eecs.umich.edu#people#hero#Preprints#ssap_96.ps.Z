URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/ssap_96.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/det_est.html
Root-URL: http://www.cs.umich.edu
Email: baygun@ridgefield.sdr.slb.com  hero@eecs.umich.edu  
Title: An Iterative Solution to the Min-Max Simultaneous Detection and Estimation Problem  
Author: Bulent Baygun Alfred O. Hero 
Address: Quarry Road Ridgefield, CT 06812  Ann Arbor, MI 48109  
Affiliation: Schlumberger-Doll Research Old  University of Michigan Department of EECS  
Abstract: Min-max simultaneous signal detection and parameter estimation requires the solution to a nonlinear optimization problem. Under certain conditions, the solution can be obtained by equalizing the probabilities of correctly estimating the signal parameter over the parameter range. We present an iterative algorithm based on Newton's root finding method to solve the nonlinear min-max optimization problem through explicit use of the equalization criterion. The proposed iterative algorithm does not require prior proof of whether an equalizer rule exists: convergence of the algorithm implies existence. A theoretical study of algorithm convergence is followed by an example which has applications in simultaneous detection and power estimation of a signal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. C. Preisig. </author> <title> A minmax approach to adaptive matched field processing in an uncertain propagation environment. </title> <journal> IEEE Trans. on Signal Proc., </journal> <volume> 42(6) </volume> <pages> 1305-1316, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The constrained min-max classifier is characterized by a set of optimal weights. In Bayesian terminology, the optimal weights represent a least favorable distribution on the unknown parameter values. Numerical solutions to min-max detection or estimation problems involve nonlinear optimization to obtain the least favorable distribution <ref> [3, 1] </ref>. On the other hand, under certain assumptions, it is possible to formulate a min-max solution by making explicit use of a simplifying sufficient condition for min-max optimality. In the case of the constrained min-max classifier, this sufficient condition is the equalization of the correct classification probabilities.
Reference: [2] <author> B. Baygun and A. O. Hero. </author> <title> Optimal simultaneous detection and estimation under a false alarm constraint. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 41(3) </volume> <pages> 688-703, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: A min-max solution to simultaneous detection and estimation was recently given in <ref> [2] </ref>. The problem considered in [2] is estimation of a discrete parameter under a false alarm constraint. The statistical decision procedure which solves the problem is called the constrained min-max classifier. The constrained min-max classifier is characterized by a set of optimal weights. <p> A min-max solution to simultaneous detection and estimation was recently given in <ref> [2] </ref>. The problem considered in [2] is estimation of a discrete parameter under a false alarm constraint. The statistical decision procedure which solves the problem is called the constrained min-max classifier. The constrained min-max classifier is characterized by a set of optimal weights. <p> A decision rule which maximizes the worst case correct classification probability under a false alarm constraint is called a constrained min-max classifier. In <ref> [2] </ref> it was shown that the constrained min-max classifier is a weighted likelihood ratio test: max f i (x) H i max &lt; fl ; (3) i.e. if the maximum weighted likelihood ratio exceeds the threshold fl, then decide H i max , where i max = arg max i&gt;0 fc <p> A sufficient condition for min-max optimality is the equalization of the correct classification probabilities P i (decide H i ) for i = 1; : : : ; n <ref> [2, Corollary 2] </ref>. The equalization condition is represented by the set of equations P i (decide H i ) = p; i = 1; : : : ; n (7) where p 2 (0; 1) is the unknown common value of the correct classification probabilities.
Reference: [3] <author> C.-I. Chang and L. D. Davisson. </author> <title> Two iterative algorithms for finding minimax solutions. </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 36(1) </volume> <pages> 126-140, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The constrained min-max classifier is characterized by a set of optimal weights. In Bayesian terminology, the optimal weights represent a least favorable distribution on the unknown parameter values. Numerical solutions to min-max detection or estimation problems involve nonlinear optimization to obtain the least favorable distribution <ref> [3, 1] </ref>. On the other hand, under certain assumptions, it is possible to formulate a min-max solution by making explicit use of a simplifying sufficient condition for min-max optimality. In the case of the constrained min-max classifier, this sufficient condition is the equalization of the correct classification probabilities.
Reference: [4] <author> E. L. Lehmann. </author> <title> Testing Statistical Hypotheses. 2nd ed., </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, </address> <year> 1991. </year>
Reference-contexts: An important class of probability densities that satisfies the monotone likelihood property is the single parameter exponential family. Furthermore, a sufficient condition for f (x ) to have a monotone likelihood ratio is for the function log f (x) to be convex in x <ref> [4, page 509] </ref>. The normal, the double exponential and the logistic distributions all satisfy the convexity condition.
Reference: [5] <author> J. E. Dennis, Jr. and R. B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1983. </year>
Reference-contexts: is Lipschitz continuous); and 3. jjJ 1 (y fl )jj fi for some fi &gt; 0 (the norm of the Jacobian inverse is bounded from above); then the sequence y (k) generated through (10) is well-defined, converges to y fl and has a quadratic rate of convergence with coefficient flfi <ref> [5, Theorem 5.2.1] </ref>. Next we provide a sketch of the proof that the three conditions are satisfied in the present problem. Condition 1: Since f (x) &gt; 0 for all x, the columns of J are linearly independent. <p> It can then be shown that the Frobenius norm of ffiJ , denoted by jjffiJ jj F is bounded above by a multiple of the l 2 norm of the vector ffiy. Since the l 2 -induced norm of ffiJ is smaller than the Frobenius norm of ffiJ <ref> [5, Theorem 3.1.3] </ref>, Lipschitz continuity is satisfied.
References-found: 5

