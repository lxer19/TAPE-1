URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR402.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Analyzing Data-structure Movements in Message-Passing Programs  
Author: Sekhar R. Sarukkai Jacob K. Gotwals 
Address: MS  Moffett Field, CA 94035-1000  Bloomington, IN 47405  
Affiliation: INDIANA UNIVERSITY COMPUTER SCIENCE DEPARTMENT  Recom Technologies,  NASA Ames Research Center,  Dept. of Computer Science, Indiana University,  
Pubnum: TECHNICAL REPORT NO.  
Email: email: sekhar@kronos.arc.nasa.gov  email: jgotwals@moose.cs.indiana.edu  
Phone: 269-3,  phone: (415) 604 4242  phone: (812) 855 9761  
Date: 402  March 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson and Edward D. Lazowska, " Quartz: </author> <title> A Tool for Tunning Paralel Program Performance," </title> <booktitle> Proceedings of the 1990 Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as [15, 13, 16, 7]. * Performance tools centered on metrics, such as <ref> [11, 6, 1] </ref>. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics. <p> Then with the help of a number of examples, we show how we can complement traditional performance indices centered on functions and processes, with a set of data structure indices that can be used to systematically guide the programmer towards performance bottlenecks. 1.1 Related Work Quartz <ref> [1] </ref> is a tool for tuning parallel program performance on shared memory multiprocessors. The principle metric of Quartz is the normalized processor time.
Reference: [2] <author> David Bailey, John Barton, Thomas lasinski and Horst Simon, </author> " <title> The NAS Parallel Benchmarks," </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In this section we consider an example: a tridiagonal solver, which implements several methods for solving systems of tridiagonal equations. This kernel operation is used commonly in a number of fluid dynamics computations such as a block tridiagonal solver (one of the NAS parallel benchmarks <ref> [2] </ref>) 13 which solves multiple, independent systems of block tridiagonal equations. Our example program is executed on an Intel iPSC/860 hypercube with 16 nodes. All the statistics shown in this section are derived by the data movement analyzer, from trace files generated during execution of the program on the hypercube.
Reference: [3] <author> S. H. Bokhari, </author> " <title> Communication Overhead on the Intel iPSC/860 Hypercube," </title> <type> ICASE Interim Report 10, </type> <month> May </month> <year> 1990. </year>
Reference: [4] <author> H.Davis, S.R. Goldschidt and J. Hennessy, </author> <title> "Tango: A Multiprocessor Simulation and Tracing System," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The statistics were primarily geared to determine the hit ratio of data structure references in cache and to provide a possible explanation for the same. Since this tool needs information regarding cache hits and misses, it was built on top of the Tango-lite simulator <ref> [4] </ref>. Our work differs from this work in a number of significant ways. Firstly, we do not track every data use. Instead we track those which are potentially the most expensive: interprocessor data references, involving movement of data between processors.
Reference: [5] <author> D. Gannon, J.K.Lee, B. Shei, S.R.Sarukkai, S.Narayana, N.Sundaresan, D.Atapattu, F.Bodin, " SigmaII: </author> <title> A toolkit for Building Parallelizing Compilers and Performance Analysis Systems ," , Proceedings of the Programming Environments for Parallel Computing Conference, </title> <publisher> Edinburgh, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: goal is to automatically determine the identity of a pair of arrays for each communication, the sending and using arrays, and to make that array pair information available to postmortem performance analysis tools. 5 components. 2.1 Version 1: No Static Analysis Our instrumenter implementation is built using the Sigma system <ref> [5] </ref>. Sigma parses the user's source code, building an internal representation and performing data flow analysis. We make calls to the Sigma library routines, to analyze and instrument the user's code, and to output the instrumented version of the program.
Reference: [6] <author> S. L. Graham, P.B. Kessler and M. K. McKusick, </author> " <title> An Execution Profiler for Modular Programs," </title> <journal> Software Practice and Experience, </journal> <month> August </month> <year> 1983. </year>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as [15, 13, 16, 7]. * Performance tools centered on metrics, such as <ref> [11, 6, 1] </ref>. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics. <p> IPS-2 [11] is a parallel performance tool which provides a set of system and application based metrics, by which program performance is debugged. Profile tables similar to the type of information presented by the standard UNIX profiling tool Gprof <ref> [6] </ref> are provided. Critical path analysis is based on identifying the path through the program's execution that consumed the most time. The profile table lists actual amounts of time spent in selected phases of the computation and communication as well.
Reference: [7] <author> Michael T. Heath, Jennifer A. Etheridge, </author> <title> "Visualizing the Performance of Parallel Programs," </title> <journal> IEEE Software, </journal> <month> September </month> <year> 1991. </year>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as <ref> [15, 13, 16, 7] </ref>. * Performance tools centered on metrics, such as [11, 6, 1]. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics.
Reference: [8] <author> High Performacne Fortran Forum, </author> " <title> High Performance Language Specification," </title> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: Recently there have been significant efforts toward developing parallel programming paradigms supporting higher level abstractions of parallelism, such as in HPF <ref> [8] </ref> and pC++ [9]. These languages provide means for explicit specification of data alignments and distributions. Though this approach largely hides explicit message communication from the programmer, the programmer still makes the important decisions on how to distribute and align the various data structures to be operated on.
Reference: [9] <author> Jenq Kuen Lee and Dennis Gannon, </author> <title> "Object Oriented Parallel Programming Experiments and Results," </title> <booktitle> Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: Recently there have been significant efforts toward developing parallel programming paradigms supporting higher level abstractions of parallelism, such as in HPF [8] and pC++ <ref> [9] </ref>. These languages provide means for explicit specification of data alignments and distributions. Though this approach largely hides explicit message communication from the programmer, the programmer still makes the important decisions on how to distribute and align the various data structures to be operated on.
Reference: [10] <author> Margaret Martonosi and Anoop Gupta, " MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs," </title> <booktitle> Proceedings of the 1992 ACM International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: We highlight performance problems in terms of these data structure pairs, with the help of a comprehensive set of performance indices. The concept of using data structure oriented views was incorporated into the MemSpy system <ref> [10] </ref>. This tool provided a means of studying performance with respect to individual data structures, based on tracking individual data structure references for sequential and shared memory systems. <p> Tracking local data structure references (such as in Mtool <ref> [10] </ref>) is another possible approach, which can yield information on local data structure performance (e.g. cache interactions between data structures), at the expense of significant increases in execution time (for simulation based analyzers), or trace file size (for postmortem analyzers).
Reference: [11] <author> Barton P. Miller, Morgan Clark, Jeff Hollingsworth, Steven Kierstead, Sek-See Lim and Timothy Torzewski, "IPS-2: </author> <title> The second Generation of a Parallel Program Measurement System," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1990. </year>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as [15, 13, 16, 7]. * Performance tools centered on metrics, such as <ref> [11, 6, 1] </ref>. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics. <p> The principle metric of Quartz is the normalized processor time. This is the total processor time spent in each section of the code divided by the number of other processors which are concurrently busy when that section of the code is being executed. IPS-2 <ref> [11] </ref> is a parallel performance tool which provides a set of system and application based metrics, by which program performance is debugged. Profile tables similar to the type of information presented by the standard UNIX profiling tool Gprof [6] are provided.
Reference: [12] <author> Sekhar R. Sarukkai and Allen Malony, </author> " <title> Perturbation Analysis of High Level Instrumentation of SPMD Programs, </title> <booktitle> Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: As the number of events collected increases with increasing run time, run-time perturbation may become more significant. However, recently techniques have been established to eliminate or reduce the effect of flushes and monitor overheads (and hence perturbations) for SP M D programs, as discussed in <ref> [12, 14] </ref>.
Reference: [13] <author> Sekhar R. Sarukkai and Dennis Gannon, </author> <title> "SIEVE: A Performance Debugging Environment for Parallel Programs," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1993. </year>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as <ref> [15, 13, 16, 7] </ref>. * Performance tools centered on metrics, such as [11, 6, 1]. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics.
Reference: [14] <author> Sekhar R. Sarukkai and Jerry Yan, </author> <title> "Integration of Perturbation Compensation and Application Monitoring Tools for Message Passing Parallel Porgrams," </title> <note> Submitted to IEEE Transactions on Parallel and Distributed Systems 24 </note>
Reference-contexts: As the number of events collected increases with increasing run time, run-time perturbation may become more significant. However, recently techniques have been established to eliminate or reduce the effect of flushes and monitor overheads (and hence perturbations) for SP M D programs, as discussed in <ref> [12, 14] </ref>.
Reference: [15] <author> Eileen Kraemer and John T. Stasko, </author> " <title> The Visualization of Parallel Systems: An Overview," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1993. </year>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as <ref> [15, 13, 16, 7] </ref>. * Performance tools centered on metrics, such as [11, 6, 1]. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics.
Reference: [16] <author> Jerry Yan, Charles Fineman, Phil Hontalas, Melisa Schmidt, Sherry Listgarten, Pankaj Mehra, Sekhar Sarukkai and Cathy Schulbach, </author> " <title> The Automated Instrumentation and Monitoring System (AIMS) Reference MAnual," </title> <institution> NASA Ames Research Center, </institution> <month> June </month> <year> 1993. </year> <title> [17] "UNICOS Performance Utilities Reference Manual," </title> <institution> Cray Research Inc., </institution> <year> 1991. </year> <month> 25 </month>
Reference-contexts: In recent years there have been a number of efforts in providing software tools for debugging the performance of programs. These tools can broadly be classified into: 2 * Performance visualization tools such as <ref> [15, 13, 16, 7] </ref>. * Performance tools centered on metrics, such as [11, 6, 1]. * Expert systems for performance debugging [17]. In the next section we discuss some tools which are centered on metrics. <p> Existing postmortem performance analyzers such as AIMS <ref> [16] </ref> instrument the source code of the program being analyzed, replacing communication system calls with calls to a monitor library. The monitor library subroutines perform the intended communication calls, appending timing and other performance information onto a trace file. <p> To determine how we can improve the performance of these array interactions, we need to study various performance indices described in the next subsection. 3 Finer level instrumentation can be automatically enabled by using suitable selections in the AIMS instrumenter <ref> [16] </ref>. User defined blocks can be introduced and statistics of data-structure interaction in these blocks will also be reported. Further, linking the code segment to the data-structure interactions is possible, using the standard clickback facility in AIMS [16] 16 Given the communication index values for array-pairs, tabulated over the whole program <p> instrumentation can be automatically enabled by using suitable selections in the AIMS instrumenter <ref> [16] </ref>. User defined blocks can be introduced and statistics of data-structure interaction in these blocks will also be reported. Further, linking the code segment to the data-structure interactions is possible, using the standard clickback facility in AIMS [16] 16 Given the communication index values for array-pairs, tabulated over the whole program and over each function/subroutine, we can quickly determine the communications which should be studied, and the significance of these communications with respect to the overall execution of the program.
References-found: 16

