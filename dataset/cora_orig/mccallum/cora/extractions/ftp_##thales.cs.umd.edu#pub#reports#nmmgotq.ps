URL: ftp://thales.cs.umd.edu/pub/reports/nmmgotq.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/biographical/xapp.html
Root-URL: 
Note: EDITED BOOK TITLE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Grassmann and D. P. Heyman. </author> <title> Equilibrium distribution of block-structured Markov chains with repeating rows. </title> <journal> J. Appl. Prob., </journal> <volume> 27 </volume> <pages> 557-576, </pages> <year> 1990. </year>
Reference-contexts: Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. These Markov chains have been studied by many authors; we only cite here Neuts [7, 8], Ramaswami [9], Grassmann and Heyman <ref> [1] </ref>, Latouche [5]. A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type.
Reference: [2] <author> G. Latouche. </author> <title> A note on two matrices occurring in the solution of quasi-birth-and-death processes. </title> <journal> Commun. Statist. | Stochastic Models, </journal> <volume> 3 </volume> <pages> 251-257, </pages> <year> 1987. </year>
Reference-contexts: For if the ith row sum of G is less than one, there is a positive probability that starting from state (k; i) the chain will never arrive at level k 1. It is shown in <ref> [2] </ref> that G and U are related as follows: 1: U = A 1 + A 2 G + A 3 G 2 + ; (1.3) Hence G satisfies the equation G = A 0 + A 1 G + A 2 G 2 + ; (1.4) and indeed G is
Reference: [3] <author> G. Latouche. </author> <title> Algorithms for evaluating the matrix G in Markov chains of PH/G/1 type. </title> <type> Technical report, </type> <institution> Bellcore, </institution> <year> 1992. </year>
Reference-contexts: These equations indicate one reason why it has long been of interest to de termine the matrix G, which was seen as the key to the whole computational procedure. Several algorithms have been proposed, among which the following requires the least number of iterations <ref> [3] </ref>: 1: G 0 = 0; - + ; - = 0; 1 : : :; (1.5) The two matrix sequences monotonically converge respectively to G and U . Turning to practical matters, we first note that we cannot form the infinite sums required by the algorithm.
Reference: [4] <author> G. Latouche. </author> <title> A simple proof for the matrix-geometric theorem. Applied Stochastic Models and Data Analysis, </title> <booktitle> 8 </booktitle> <pages> 25-29, </pages> <year> 1992. </year>
Reference-contexts: C C ; (1.12) where Q ** = I P ** . In fact each row of (1.12) is just (1.4) multiplied by a power of G. In order to prove that the only solution consists of powers of G, we use an argument similar to that of <ref> [4] </ref>, Equation (13). 3 BLOCK HESSENBERG SYSTEMS The approach taken in this section is based on the following observation. Let P ** and Q ** be defined as in the last section. Equation (1.12) implies that if we can solve block Hessenberg systems, we can compute G. <p> to (1.5) is given below; the two sequences converge monotonically respectively to R and U . 1: R 0 = 0; A 3 + ; - = 0; 1 : : : ; (1.22) Of particular interest to us is the fact that the matrix R satisfies the following equation <ref> [4] </ref>: (R R 2 R 3 : : :)Q ** = (A 0 0 0 : : :); (1.23) where Q ** = (I P ** ). This clearly indicates that the matrix R may be determined by applying, mutatis mutandis, the recursive descent described in the preceding section.
Reference: [5] <author> G. Latouche. </author> <title> Algorithms for infinite Markov chains with repeating columns. </title> <editor> In C. D. Meyer and R. J. Plemmons, editors, </editor> <title> Linear Algebra, Markov Chains, </title> <booktitle> and Queueing Models, </booktitle> <pages> pages 231-266, </pages> <address> New York, </address> <year> 1993. </year> <title> Springer. </title> <journal> The IMA Volumes in Mathematics and Its Applications, </journal> <volume> Volume 48. </volume>
Reference-contexts: Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. These Markov chains have been studied by many authors; we only cite here Neuts [7, 8], Ramaswami [9], Grassmann and Heyman [1], Latouche <ref> [5] </ref>. A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type. <p> The matrix R is equal to A 0 (I U ) 1 , where U is defined as in Section 2. As in the case of the matrix G, several algorithms have been proposed, in order to compute R, most of which are described and compared in <ref> [5] </ref>.
Reference: [6] <author> D. M. Lucantoni. </author> <title> New results on the single server queue with a batch Markovian arrival process. </title> <journal> Commun. Statist.|Stochastic Models, </journal> <volume> 7 </volume> <pages> 1-46, </pages> <year> 1991. </year>
Reference-contexts: On the other hand, with additional calculations it is possible to normalize T 0 at once (see <ref> [6] </ref> for details). This algorithm is, in essence, Gaussian elimination; computing G corresponds to the forward elimination phase, computing T as outlined above corresponds to the backward substitution phase.
Reference: [7] <author> M. Neuts. </author> <title> Matrix-Geometric Solutions in Stochastic Models. </title> <publisher> Johns Hop-kins University Press, </publisher> <address> Baltimore, </address> <year> 1981. </year>
Reference-contexts: C C C C A is also of interest. These are called Markov chains of GI/M/1 type (see Neuts <ref> [7] </ref> for background). Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. <p> Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. These Markov chains have been studied by many authors; we only cite here Neuts <ref> [7, 8] </ref>, Ramaswami [9], Grassmann and Heyman [1], Latouche [5]. A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type. <p> C C C : (1.20) In this case, one proves <ref> [7] </ref> that there exists a matrix R of order m such that the stationary probability vector is given by T 0 R k ; k 0; (1.21) 0 is a left eigenvector of the matrix B [R] = P n1 B n R n1 , normalized so that T 0 (I
Reference: [8] <author> M. F. Neuts. </author> <title> Structured Stochastic Matrices of M/G/1 Type and Their Applications. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION Queues of M/G/1 type (for background see Neuts <ref> [8] </ref>) give rise to embedded Markov chains whose transition matrices have the form P = B B B B @ A 0 A 1 A 2 A 3 0 A 0 A 1 A 2 0 0 A 0 A 1 . . . . . . . . . <p> Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. These Markov chains have been studied by many authors; we only cite here Neuts <ref> [7, 8] </ref>, Ramaswami [9], Grassmann and Heyman [1], Latouche [5]. A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type.
Reference: [9] <author> V. Ramaswami. </author> <title> Stable recursion for the steady state vector for Markov chains of M/G/1 type. </title> <journal> Commun. Statist.|Stochastic Models, </journal> <volume> 4 </volume> <pages> 183-188, </pages> <year> 1988. </year>
Reference-contexts: Such chains are derived by looking at a queue immediately before each arrival. Between two arrival epochs, many customers may depart; hence P is lower Hessenberg. These Markov chains have been studied by many authors; we only cite here Neuts [7, 8], Ramaswami <ref> [9] </ref>, Grassmann and Heyman [1], Latouche [5]. A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type. <p> It is further shown in <ref> [9] </ref> that if we partition T = ( T 1 T then T 0 satisfies T ^ B 0 = T 6 Chapter 1 and given T 0 we can compute T k recursively in the form T 0 ^ B k + i=1 i ^ A ki+1 (I U )
Reference: [10] <author> G. W. Stewart. </author> <title> On the solution of block hessenberg systems. </title> <type> Technical Report CS-TR-2973, </type> <institution> Department of Computer Science, University of Maryland, College Park, </institution> <year> 1992. </year> <note> To appear in Numerical Linear Algebra and Applications.. </note>
Reference-contexts: A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type. In contrast to these procedures, Stewart <ref> [10, 11] </ref> proposes to directly determine the vector T , by an approach based on a recursive descent method for solving block Hessenberg systems. It turns out that this method also produces the matrix G or the matrix R as a byproduct.
Reference: [11] <author> G. W. Stewart. </author> <title> Implementing an algorithm for solving block Hessenberg systems. </title> <type> Technical Report CS-TR-3295, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1994. </year>
Reference-contexts: A number of iterative procedures have been proposed, usually requiring the de termination of an auxiliary m fi m matrix, called G for queues of M/G/1 type and R for queues of GI/M/1 type. In contrast to these procedures, Stewart <ref> [10, 11] </ref> proposes to directly determine the vector T , by an approach based on a recursive descent method for solving block Hessenberg systems. It turns out that this method also produces the matrix G or the matrix R as a byproduct. <p> = F ^ Q 1 ; 3: Y = ^ Q 1 E: It can be shown that if we partition ^x T = (^x T t ^x T N ) then x = ^x Y S 1 ^x t : An implementation of this method has been described in <ref> [11] </ref>, and we will only point out some salient features. The matrices X and Y are called patch 10 Chapter 1 matrices. As we have seen, Y is used to solve systems of the form Q ** x = b. The patch matrix X is used to solve transposed systems.
References-found: 11

