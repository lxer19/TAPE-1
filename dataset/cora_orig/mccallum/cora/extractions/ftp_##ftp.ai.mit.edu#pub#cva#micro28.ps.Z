URL: ftp://ftp.ai.mit.edu/pub/cva/micro28.ps.Z
Refering-URL: http://www.ai.mit.edu/people/npcarter/npcarter.html
Root-URL: 
Email: fillo@ai.mit.edu skeckler@ai.mit.edu billd@ai.mit.edu  npcarter@ai.mit.edu achang@ai.mit.edu yev@ai.mit.edu wslee@ai.mit.edu  
Title: The M-Machine Multicomputer  
Author: Marco Fillo Stephen W. Keckler William J. Dally Nicholas P. Carter Andrew Chang Yevgeny Gurevich Whay S. Lee 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: Artificial Intelligence Laboratory Laboratory for Computer Science Massachusetts Institute of Technology  
Note: Appears in the Proceedings of MICRO-28.  
Abstract: The M-Machine is an experimental multicomputer being developed to test architectural concepts motivated by the constraints of modern semiconductor technology and the demands of programming systems. The M-Machine computing nodes are connected with a 3-D mesh network; each node is a multithreaded processor incorporating 12 function units, on-chip cache, and local memory. The multiple function units are used to exploit both instruction-level and thread-level parallelism. A user accessible message passing system yields fast communication and synchronization between nodes. Rapid access to remote memory is provided transparently to the user with a combination of hardware and software mechanisms. This paper presents the architecture of the M-Machine and describes how its mechanisms attempt to maximize both single thread performance and overall system throughput. The architecture is complete and the MAP chip, which will serve as the M-Machine processing node, is currently being implemented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AGARWAL, A., ET AL. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. In Scalable Shared Memory Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: This code is easily incorporated within the remote read and write handlers described in Section 4.2. Using local memory as a repository will allow more remote data to be cached locally than could fit in the on-chip cache alone. Discussion: Directory-based, cache coherent multiprocessors such as Alewife <ref> [1] </ref> and DASH [20] implement coherence policies in hardware. This improves performance at the cost of flexibility. Like the M-Machine, FLASH [19] implements remote memory access and cache coherence in software, but uses a coprocessor.
Reference: [2] <author> AGRAWAL, P., DALLY, W., FISCHER, W., JAGADISH, H., KRISHNAKUMAR, A., AND TUTUNDJIAN, R. </author> <title> MARS: A multiprocessor-based programmable accelerator. </title> <booktitle> IEEE Design Test 4 (October 1987), </booktitle> <pages> 28-36. </pages>
Reference-contexts: Register-mapped network interfaces have been used previously in the Mars Machine <ref> [2] </ref>, J-Machine, and iWarp [4], and have been described by *T [26] as well as Henry and Joerg [15]. However, none of these systems provide protection for user-level messages.
Reference: [3] <editor> ALVERSON, R., ET AL. </editor> <booktitle> The Tera computer system. In Proceedings of the 1990 International Conference on Supercomputing (Sept. 1990), ACM SIGPLAN Computer Architecture News, </booktitle> <pages> pp. 1-6. </pages>
Reference-contexts: Gupta and Weber explore the use of multiple hardware contexts in multiprocessors [10], but the context switch overheads they used are too large to mask pipeline latencies. MASA [13] as well as HEP [29] and TERA <ref> [3] </ref> use fine grain multithread-ing to issue an instruction from a different context on every cycle in order to mask pipeline latencies. However, with the required round-robin scheduling, single thread performance is degraded by the number of pipeline stages.
Reference: [4] <author> BORKAR, S., ET AL. </author> <title> Supporting systolic and memory communicationin iWarp. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture (May 1990), </booktitle> <pages> pp. 70-81. </pages>
Reference-contexts: Register-mapped network interfaces have been used previously in the Mars Machine [2], J-Machine, and iWarp <ref> [4] </ref>, and have been described by *T [26] as well as Henry and Joerg [15]. However, none of these systems provide protection for user-level messages.
Reference: [5] <author> CARTER, N. P., KECKLER, S. W., AND DALLY, W. J. </author> <title> Hardware support for fast capability-based addressing. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI) (Oct. </booktitle> <publisher> 1994),Association for Computing Machinery Press, </publisher> <pages> pp. 319-327. </pages>
Reference-contexts: Previous publications have introduced some of the mechanisms used in the M-Machine. The first description of Processor Coupling, a method for exploiting instruction level parallelism, appeared in [18]. The novel capability-based memory protection system of the M-Machine was described in <ref> [5] </ref>. This paper describes the M-Machine's other features which include an improved form of Processor Coupling as well as communication and global addressing mechanisms. The M-Machine architectural design is complete and the MAP chip, which will serve as the M-Machine processing node, is currently being implemented. <p> A pair of load and store operations specify a precondition and a postcondition on the synchronization bit and are used as atomic read-modify-write memory operations. The M-Machine supports a single global virtual address space. A light-weight capability system implements protection through guarded pointers <ref> [5] </ref>, while paging is used to manage the relocation of data in physical memory within the virtual address space. The segmentation and paging mechanisms are independent so that protection may be preserved on variable-size segments of memory.
Reference: [6] <author> COLWELL, R. P., HALL, W. E., JOSHI, C. S., PAPWORTH, D. B., RODMAN, P. K., AND TORNES, J. E. </author> <title> Architecture and implementationof a VLIW supercomputer. </title> <booktitle> In Proceedings of Supercomputing '90 (November 1990), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 910-919. </pages>
Reference-contexts: In addition, superscalars attempt to schedule instructions at runtime (much of which could be done at compile time), but they can only examine a small subsequence of the instruction stream. Alternatively, Very Long Instruction Word (VLIW) processors such as the Multiflow Trace series <ref> [6] </ref> use only compile time scheduling to manage instruction-level parallelism, resource usage, and communication among a partitioned register file. However, the strict lock-step execution is unable to tolerate the dynamic latencies found in multiprocessors. Processor Coupling, originally introduced in [18], used implicit synchronization between the clusters on every wide instruction.
Reference: [7] <author> CRAY RESEARCH, INC. </author> <title> Cray T3D System Architecture Overview. </title> <institution> Chippewa Falls, WI, </institution> <year> 1993. </year>
Reference-contexts: Most messages fit easily in this size and larger messages can be packetized and reassembled with very low overhead. Automatic translation of virtual processor numbers to physical processor identifiers is used in the Cray T3D <ref> [7] </ref>. The use of virtual addresses as message destinations in the M-Machine has two advantages.
Reference: [8] <author> DALLY, W. J., ET AL. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <booktitle> In Proceedings of the IFIP Congress (Aug. </booktitle> <year> 1989), </year> <editor> G. Ritter, Ed., </editor> <publisher> North-Holland, </publisher> <pages> pp. 1147-1153. </pages>
Reference-contexts: the contents of the original message which are copied into the buffer and sent again later. 7 Discussion: The M-Machineprovides direct register-to-register communication, avoiding the overhead of memory copying at both the sender and the receiver, and eliminating the dedicated memory for message arrival, as is found on the J-Machine <ref> [8] </ref>. Register-mapped network interfaces have been used previously in the Mars Machine [2], J-Machine, and iWarp [4], and have been described by *T [26] as well as Henry and Joerg [15]. However, none of these systems provide protection for user-level messages.
Reference: [9] <author> FRANK, S. J., ET AL. </author> <title> Multiprocessor digital data processing system. United States Patent No. </title> <address> 5,055,999, </address> <month> October 8 </month> <year> 1991. </year>
Reference-contexts: Like the M-Machine, FLASH [19] implements remote memory access and cache coherence in software, but uses a coprocessor. However, this system does not provide block status bits in the TLB to support caching remote data in DRAM. The subpage status bits of the KSR-1 <ref> [9] </ref> perform a function similar to that of the block status bits of the M-Machine. Implementing remote memory access and coherence completely in software on a conventional processor would involve delays much greater than those shown in Table 1, as evidenced by experience with the Ivy system [21].
Reference: [10] <author> GUPTA, A., AND WEBER, W.-D. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessorarchitecture: </title> <booktitle> Preliminaryresults. In Proceedings of 16th Annual Symposium on Computer Architecture (May 1989), IEEE, </booktitle> <pages> pp. 273-280. </pages>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been examined in several different studies and machines. Gupta and Weber explore the use of multiple hardware contexts in multiprocessors <ref> [10] </ref>, but the context switch overheads they used are too large to mask pipeline latencies. MASA [13] as well as HEP [29] and TERA [3] use fine grain multithread-ing to issue an instruction from a different context on every cycle in order to mask pipeline latencies.
Reference: [11] <author> GUREVICH, Y. </author> <title> The M-Machineoperatingsystem. </title> <institution> Master of EngineeringThesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The runtime system consists of independent modules which manage virtual memory allocation, physical memory allocation, memory coherence between nodes, and multiple threads on a single node and across nodes. The implementation of the runtime system is described more fully in <ref> [11] </ref>. 6 Conclusion In this paper we have described the architecture of the M-Machine with an emphasis on its novel features. The M-Machine, currently under development, is a 3-D mesh, each node of which contains a multi-ALU processor (MAP) and 8 MBytes of synchronous DRAM.
Reference: [12] <author> GWENNAP, L. </author> <title> New MIPS chip targets windows NT boxes. </title> <type> Microprocessor Report (November 18, </type> <year> 1992). </year>
Reference-contexts: The M-Machine also addresses the demand for easier programmability by providing an incremental path for increasing parallelism and performance. An unmodified sequential program can 2 Area was determined by measuring the processing components of various chips, in particular the R4600 described in <ref> [12] </ref>. 1 run on a single M-Machine node, accessing both local and remote memory. This code can be incrementally parallelized by identifying tasks, such as loop iterations, that can be distributed both across nodes and within each node to run in parallel.
Reference: [13] <author> HALSTEAD, R. H., AND FUJITA, T. MASA: </author> <title> a multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In 15th Annual Symposium on Computer Architecture (May 1988), IEEE Computer Society, </booktitle> <pages> pp. 443-451. </pages>
Reference-contexts: Using multiple threads to hide memory latencies and pipeline delays has been examined in several different studies and machines. Gupta and Weber explore the use of multiple hardware contexts in multiprocessors [10], but the context switch overheads they used are too large to mask pipeline latencies. MASA <ref> [13] </ref> as well as HEP [29] and TERA [3] use fine grain multithread-ing to issue an instruction from a different context on every cycle in order to mask pipeline latencies. However, with the required round-robin scheduling, single thread performance is degraded by the number of pipeline stages.
Reference: [14] <author> HENNESSY, J. L., AND JOUPPI, N. P. </author> <title> Computer technology and architecture: An evolving interaction. </title> <booktitle> Computer (Sept. </booktitle> <year> 1991), </year> <pages> 18-29. </pages>
Reference-contexts: Advances in VLSI technology have resulted in computers with chip area dominated by memory and not by processing resources. The normalized area (in 2 ) of a VLSI chip 1 is increasing by 50% per year, while gate speed and communication bandwidth are increasing by 20% per year <ref> [14] </ref>.
Reference: [15] <author> HENRY, D. S., AND JOERG, C. F. </author> <title> A tightly-coupled processor-networkinterface. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V) (Oct. 1992), ACM, </booktitle> <pages> pp. 111-122. </pages>
Reference-contexts: Register-mapped network interfaces have been used previously in the Mars Machine [2], J-Machine, and iWarp [4], and have been described by *T [26] as well as Henry and Joerg <ref> [15] </ref>. However, none of these systems provide protection for user-level messages. Systems, like the J-Machine, that provide user access to the network interface without atomicity must temporarily disable interrupts to allow the sending process to complete the message.
Reference: [16] <author> HUM, H. H., ET AL. </author> <title> A design study of the EARTH multiprocessor. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques (1995), </booktitle> <pages> pp. 59-68. </pages>
Reference-contexts: Furthermore, none of the multithreaded machines have multiple clusters for exploiting wide instruction level parallelism. Various machines optimized for dataflow languages <ref> [24, 16, 28] </ref> provide hardware support for fine grained synchronization between threads (usually via memory synchronization bits), but they do not exploit instruction level parallelism, nor do they provide low cost register-based synchronization between threads.
Reference: [17] <author> JOHNSON, W. M. </author> <title> Superscalar Microprocessor Design. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: User H-Threads executing on neighboring clusters are unaffected. 3.4 Discussion There are two major methods of exploiting instruction level parallelism. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies <ref> [31, 17] </ref>. However, they do not scale well with increasing number of function units because a greater number of register file ports and connections to the function units are required.
Reference: [18] <author> KECKLER, S. W., AND DALLY, W. J. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture (Queensland, </booktitle> <address> Australia, </address> <month> May </month> <year> 1992), </year> <booktitle> ACM, </booktitle> <pages> pp. 202-213. </pages>
Reference-contexts: To do this, nodes are designed to manage parallelism at a variety of granularities, from the instruction level to the process level. The 12 function units in a single M-Machine node are controlled using a form of Processor Coupling <ref> [18] </ref> to exploit instruction level parallelism by executing 12 operations from the same thread, or to exploit thread-level parallelism by executing operations from up to six different threads. The fast internode communication allows collaborating threads to reside on different nodes. <p> The caching of remote data in local DRAM automatically migrates a task's data to exploit locality. Previous publications have introduced some of the mechanisms used in the M-Machine. The first description of Processor Coupling, a method for exploiting instruction level parallelism, appeared in <ref> [18] </ref>. The novel capability-based memory protection system of the M-Machine was described in [5]. This paper describes the M-Machine's other features which include an improved form of Processor Coupling as well as communication and global addressing mechanisms. <p> A synchronization pipeline stage holds the next instruction to be issued from each of the six V-Threads until all of its operands are present and all of the required resources are available, similar to the architecture described in <ref> [18] </ref>. At every cycle this stage decides which instruction to issue from those which are ready to run. An H-Thread that is stalled waiting for data or resource availability consumes no resources other than the thread slot that holds its state. <p> However, the strict lock-step execution is unable to tolerate the dynamic latencies found in multiprocessors. Processor Coupling, originally introduced in <ref> [18] </ref>, used implicit synchronization between the clusters on every wide instruction. Relaxing the lock-step synchronization, as described in this section, has several advantages. First, it is easier to implement because control is localized completely within the clusters.
Reference: [19] <author> KUSKIN, J., OFELT, D., HEINRICH, M., HEINLEIN, J., SIMONI, R., ET AL. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proc. 21st International Symposium on Computer Architecture (Apr. 1994), IEEE, </booktitle> <pages> pp. 302-313. </pages>
Reference-contexts: The interleaving performed by the GTLB, although not as versatile as the CRAY T3D address centrifuge or the interleaving of the RP3 [27], provides a means of distributing ranges of the address space across a region of nodes. In contrast to both *T and FLASH <ref> [19] </ref> which use a separate communication coprocessor for receiving incoming messages, the M-Machine incorporates that function on its already existing execution resources, an H-Thread in the event V-Thread. This avoids idling a dedicated processor when it is not in use. <p> Discussion: Directory-based, cache coherent multiprocessors such as Alewife [1] and DASH [20] implement coherence policies in hardware. This improves performance at the cost of flexibility. Like the M-Machine, FLASH <ref> [19] </ref> implements remote memory access and cache coherence in software, but uses a coprocessor. However, this system does not provide block status bits in the TLB to support caching remote data in DRAM.
Reference: [20] <author> LENOSKI, D., LAUDON, J., JOE, T., NAKAHIRA, D., STEVENS, L., GUPTA, A., AND HENNESSY, J. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of 19th Annual International Symposium on Computer Architecture (1992), IEEE, </booktitle> <pages> pp. 92-103. </pages>
Reference-contexts: Using local memory as a repository will allow more remote data to be cached locally than could fit in the on-chip cache alone. Discussion: Directory-based, cache coherent multiprocessors such as Alewife [1] and DASH <ref> [20] </ref> implement coherence policies in hardware. This improves performance at the cost of flexibility. Like the M-Machine, FLASH [19] implements remote memory access and cache coherence in software, but uses a coprocessor.
Reference: [21] <author> LI, K. Ivy: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In International Conference on Parallel Processing (1988), </booktitle> <pages> pp. 94-101. </pages>
Reference-contexts: This fine-grain control over data is similar to that provided in hardware based cache coherent multiprocessors, and alleviates the false sharing that exists in other software data coherence systems <ref> [21] </ref>. <p> Implementing remote memory access and coherence completely in software on a conventional processor would involve delays much greater than those shown in Table 1, as evidenced by experience with the Ivy system <ref> [21] </ref>. The M-Machine's fast exception handling in a dedicated H-Thread avoids the delay associated with context switching and allows the user thread to execute in parallel with the exception handler. The GTLB avoids the overhead of manual translation and the cost of a system call to access the network.
Reference: [22] <author> LOWNEY, P. G., FREUDENBERGER, S. G., KARZES, T. J., LICHTENSTEIN, W. D., NIX, R. P., O'DONNELL, J. S., AND RUTTENBERG, J. C. </author> <title> The multiflow trace scheduling compiler. </title> <booktitle> The Journal of Supercomputing 7, </booktitle> <month> 1-2 (May </month> <year> 1993), </year> <pages> 51-142. </pages>
Reference-contexts: For the cases where a programmer wants to extract the maximum performance, fast, protected, user-level messages may be employed. The M-Machine software is being designed and implemented jointly with the Scalable Concurrent Programming Laboratory at Caltech. The Multiflow compiler <ref> [22] </ref> is being ported to the M-Machine to generate long instructions spanning multiple clusters. The Multiflow compiler is designed to generate VLIW instructions from a sequential source program using Trace Scheduling.
Reference: [23] <author> MEAD, C. A., AND CONWAY, L. A. </author> <title> Introduction to VLSI Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass, </address> <year> 1980. </year>
Reference-contexts: a result, a 64-bit proces fl The research described in this paper was supported by the Advanced Research Projects Agency and monitored by the Air Force Electronic Systems Division under contract F19628-92-C-0045. 1 The parameter is a normalized, process independentunit of distance equivalent to one half of the gate length <ref> [23] </ref>. For a 0:5m process, is 0:25m. sor with a pipelined FPU (400M 2 ) 2 is only 8% of a 5G 2 1996 0.35m chip. In a system with 256 MBytes of DRAM, the processor accounts for 0.13% of the silicon area in the system.
Reference: [24] <author> NIKHIL, R. S., PAPADOPOULOS, G. M., AND ARVIND. </author> <title> *T: A multithreaded massively parallel architecture. Computation Structures Group Memo 325-1, </title> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Furthermore, none of the multithreaded machines have multiple clusters for exploiting wide instruction level parallelism. Various machines optimized for dataflow languages <ref> [24, 16, 28] </ref> provide hardware support for fine grained synchronization between threads (usually via memory synchronization bits), but they do not exploit instruction level parallelism, nor do they provide low cost register-based synchronization between threads.
Reference: [25] <author> NOAKES, M. D., WALLACH, D. A., AND DALLY, W. J. </author> <title> The J-Machine mul-ticomputer: An architectural evaluation. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture (San Diego, </booktitle> <address> California, </address> <month> May </month> <year> 1993), </year> <journal> IEEE, </journal> <pages> pp. 224-235. </pages>
Reference-contexts: Two message priorities are provided: user messages are sent at priority zero, while priority 1 is reserved for system level message reply, thus avoiding deadlock. Message Address Translation: As described in <ref> [25] </ref>, the explicit management of processor identifiers by application programs is cumbersome and slow. To eliminate this overhead, the MAP implements a Global Translation Lookaside Buffer (GTLB), backed by a software Global Destination Table (GDT), to hold mappings of virtual address regions to node numbers.
Reference: [26] <author> PAPADOPOULOS, G. M., BOUGHTON, G. A., GRAINER, R., AND BECKERLE, M. J. </author> <title> *T: Integrated building blocks for parallel computing. </title> <booktitle> In Proc. Supercomputing 1993 (1993), IEEE, </booktitle> <pages> pp. 624-635. </pages>
Reference-contexts: Register-mapped network interfaces have been used previously in the Mars Machine [2], J-Machine, and iWarp [4], and have been described by *T <ref> [26] </ref> as well as Henry and Joerg [15]. However, none of these systems provide protection for user-level messages. Systems, like the J-Machine, that provide user access to the network interface without atomicity must temporarily disable interrupts to allow the sending process to complete the message.
Reference: [27] <author> PFISTER, G., ET AL. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proc. International Conference on Parallel Processing (1985), </booktitle> <pages> pp. 764-771. </pages>
Reference-contexts: It also facilitates the implementation of global shared memory. The interleaving performed by the GTLB, although not as versatile as the CRAY T3D address centrifuge or the interleaving of the RP3 <ref> [27] </ref>, provides a means of distributing ranges of the address space across a region of nodes.
Reference: [28] <author> SAKAI, S., KODAMA, Y., AND YAMAGUCHI, Y. </author> <title> Prototype implementation of a highly parallel dataflow machine em-4. </title> <booktitle> In Proceedings of the Fifth International Parallel Processing Symposium (May 1991), ieee Computer Society, ieee, </booktitle> <pages> pp. 278-286. </pages>
Reference-contexts: Furthermore, none of the multithreaded machines have multiple clusters for exploiting wide instruction level parallelism. Various machines optimized for dataflow languages <ref> [24, 16, 28] </ref> provide hardware support for fine grained synchronization between threads (usually via memory synchronization bits), but they do not exploit instruction level parallelism, nor do they provide low cost register-based synchronization between threads.
Reference: [29] <author> SMITH, B. J. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE Vol. 298 Real-Time Signal Processing IV (1981), Denelcor, </booktitle> <publisher> Inc., Aurora, CO, </publisher> <pages> pp. 241-248. </pages>
Reference-contexts: Gupta and Weber explore the use of multiple hardware contexts in multiprocessors [10], but the context switch overheads they used are too large to mask pipeline latencies. MASA [13] as well as HEP <ref> [29] </ref> and TERA [3] use fine grain multithread-ing to issue an instruction from a different context on every cycle in order to mask pipeline latencies. However, with the required round-robin scheduling, single thread performance is degraded by the number of pipeline stages.
Reference: [30] <author> SOHI, G. S., BREACH, S. E., AND VIJAYKUMAR, T. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd International Symposium On Computer Architecture (May 1995), </booktitle> <pages> pp. 414-425. </pages>
Reference-contexts: However, it uses a single global register file and does not interleave multiple threads over the same execution units. Two approaches that do exploit instruction level parallelism using multiple threads and multiple ALUs include <ref> [30] </ref> and [32]. 4 Inter-node Concurrency Mechanisms The M-Machine provides a fast, protected, user-level message passing substrate. A user program may communicate and synchronize by directly sending messages or by reading and writing remote memory using a coherent shared memory system layered on the message-passing substrate.
Reference: [31] <author> TOMASULO, R. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal 11 (January 1967), </journal> <pages> 25-33. </pages>
Reference-contexts: User H-Threads executing on neighboring clusters are unaffected. 3.4 Discussion There are two major methods of exploiting instruction level parallelism. Superscalar processors execute multiple instructions simultaneously by relying upon runtime scheduling mechanisms to determine data dependencies <ref> [31, 17] </ref>. However, they do not scale well with increasing number of function units because a greater number of register file ports and connections to the function units are required.
Reference: [32] <author> TULLSEN, D. M., EGGERS, S. J., AND LEVY, H. M. Simultaneousmultithreading: </author> <title> Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd International Symposium On Computer Architecture (May 1995), </booktitle> <pages> pp. 392-403. </pages>
Reference-contexts: However, it uses a single global register file and does not interleave multiple threads over the same execution units. Two approaches that do exploit instruction level parallelism using multiple threads and multiple ALUs include [30] and <ref> [32] </ref>. 4 Inter-node Concurrency Mechanisms The M-Machine provides a fast, protected, user-level message passing substrate. A user program may communicate and synchronize by directly sending messages or by reading and writing remote memory using a coherent shared memory system layered on the message-passing substrate.
Reference: [33] <author> WOLFE, A., AND SHEN, J. P. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (April 1991), </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 2-14. </pages>
Reference-contexts: Various machines optimized for dataflow languages [24, 16, 28] provide hardware support for fine grained synchronization between threads (usually via memory synchronization bits), but they do not exploit instruction level parallelism, nor do they provide low cost register-based synchronization between threads. The XIMD architecture <ref> [33] </ref> uses multiple ALUs to exploit instruction level parallelism as well as thread level parallelism. However, it uses a single global register file and does not interleave multiple threads over the same execution units.
Reference: [34] <author> ZAAFRANI, A., DIETZ, H. G., AND O'KEEFE, MATTHEW, T. </author> <title> Static scheduling for barrier MIMD architectures. </title> <booktitle> In 1990 International Conference on Parallel Processing (1990). </booktitle> <pages> 11 </pages>
Reference-contexts: Communication is implemented by writing to remote registers, while the global condition registers are used to implement explicit barrier synchronization. An algorithm that might be used to discover the synchronization points is described in <ref> [34] </ref>. The compiler currently generates code for a single cluster, but adding the partitioning and synchronization as well as integrating the standard optimizations is underway. A prototype runtime system consisting of primitive message and event handlers has also been implemented.
References-found: 34

