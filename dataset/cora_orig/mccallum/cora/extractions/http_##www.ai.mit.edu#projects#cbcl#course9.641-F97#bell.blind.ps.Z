URL: http://www.ai.mit.edu/projects/cbcl/course9.641-F97/bell.blind.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/course9.641-F97/
Root-URL: 
Title: An information-maximisation approach to blind separation and blind deconvolution  
Author: Anthony J. Bell and Terrence J. Sejnowski 
Address: 10010 N. Torrey Pines Road La Jolla, California 92037  
Affiliation: Computational Neurobiology Laboratory The Salk Institute  
Abstract: We derive a new self-organising learning algorithm which maximises the information transferred in a network of non-linear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximisation has extra properties not found in the linear case (Linsker 1989). The non-linearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalisation of Principal Components Analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to ten speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information max-imisation provides a unifying framework for problems in `blind' signal processing. fl Please send comments to tony@salk.edu. This paper will appear as Neural Computation, 7, 6, 1004-1034 (1995). The reference for this version is: Technical Report no. INC-9501, February 1995, Institute for Neural Computation, UCSD, San Diego, CA 92093-0523. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Atick J.J. </author> <year> 1992. </year> <title> Could information theory provide an ecological theory of sensory processing? Network 3, </title> <type> 213-251 </type>
Reference: [2] <author> Atick J.J. </author> & <title> Redlich A.N. 1993. Convergent algorithm for sensory receptive field development, </title> <booktitle> Neural Computation 5, </booktitle> <pages> 45-60 </pages>
Reference: [3] <author> Baram Y. & Roth Z. </author> <year> 1994. </year> <title> Multi-dimensional density shaping by sig-moidal networks with application to classification, estimation and forecasting, </title> <type> CIS report no. 9420, </type> <month> October </month> <year> 1994, </year> <institution> Centre for Intelligent systems, Dept. of Computer Science, Technion, Israel Inst. of Technology, Haifa, </institution> <note> submitted for publication </note>
Reference: [4] <author> Barlow H.B. </author> <year> 1961. </year> <title> Possible principles underlying the transformation of sensory messages, in Sensory Communication, </title> <address> Rosenblith W.A. </address> <publisher> (ed), MIT press </publisher>
Reference: [5] <author> Barlow H.B. </author> <year> 1989. </year> <title> Unsupervised learning, </title> <booktitle> Neural Computation 1, </booktitle> <pages> 295-311 </pages>
Reference: [6] <author> Barlow H.B. & Foldiak P. </author> <year> 1989. </year> <title> Adaptation and decorrelation in the cortex, </title> <editor> in Durbin R. et al (eds) The Computing Neuron, p.54-72, </editor> <publisher> Addison-Wesley </publisher>
Reference: [7] <author> Battiti R. </author> <year> 1992. </year> <title> First- and second-order methods for learning: between steepest descent and Newton's method, </title> <journal> Neural Computation, </journal> <volume> 4, 2, </volume> <pages> 141-166 </pages>
Reference: [8] <author> Becker S. & Hinton G.E. </author> <year> 1992. </year> <title> A self-organising neural network that discovers surfaces in random-dot stereograms, </title> <booktitle> Nature 355: </booktitle> <pages> 161-163 34 </pages>
Reference: [9] <author> Bell A.J. & Sejnowski T.J. </author> <year> 1995. </year> <title> A non-linear information maximisation algorithm that performs blind separation. </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro et al (eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge </address>
Reference: [10] <author> Bialek W. Ruderman D.L. & Zee A. </author> <year> 1991. </year> <title> Optimal sampling of natural images: a design principle for the visual system? in Advances in Neural Information Processing Systems 3, </title> <editor> R.P.Lippmann et al. (eds.), </editor> <publisher> Morgan-Kaufmann </publisher>
Reference: [11] <author> Bellini S. </author> <year> 1994. </year> <title> Bussgang techniques for blind deconvolution and equali-sation, </title> <booktitle> in [23] </booktitle>
Reference: [12] <author> Burel G. </author> <year> 1992. </year> <title> Blind separation of sources: a non-linear neural algorithm, </title> <booktitle> Neural Networks 5, </booktitle> <pages> 937-947 </pages>
Reference: [13] <author> Cohen M.H. & Andreou A.G. </author> <year> 1992. </year> <title> Current-Mode Subthreshold MOS Implementation of the Herault-Jutten Autoadaptive Network, </title> <journal> IEEE J. Solid-State Circuits vol. </journal> <volume> 27, 5, </volume> <pages> 714-727 </pages>
Reference: [14] <author> Cohen M.H. & Andreou A.G. </author> <year> 1995. </year> <title> Analog CMOS integration and experimentation with an autoadaptive independent component analyzer, </title> <journal> IEEE Transactions on Circuits and Systems-II: analog and digital signal processing, </journal> <volume> vol. 42, no. 2, </volume> <pages> 65-77 </pages>
Reference: [15] <author> Comon P., Jutten C. & Herault J. </author> <year> 1991. </year> <title> Blind separation of sources, part II: problems statement, </title> <booktitle> Signal processing 24, </booktitle> <pages> 11-21 </pages>
Reference: [16] <author> Comon P. </author> <year> 1994. </year> <title> Independent component analysis, a new concept? Signal processing 36, </title> <type> 287-314 </type>
Reference: [17] <author> Cover T.M. & Thomas J.A. </author> <year> 1991. </year> <title> Elements of information theory, </title> <publisher> John Wiley. </publisher>
Reference: [18] <author> Deco G. & Brauer W. </author> <year> 1995. </year> <title> Non-linear higher-order statistical decorrela-tion by volume-conserving neural architectures, Neural Networks, </title> <publisher> in press </publisher>
Reference: [19] <author> Field D.J. </author> <year> 1994. </year> <title> What is the goal of sensory coding? Neural Computation 6, </title> <type> 559-601 </type>
Reference: [20] <author> Hatzinakos D. & Nikias C.L. </author> <year> 1994. </year> <title> Blind equalisation based on higher-order statistics, </title> <booktitle> in [23], </booktitle> <pages> 181-258 35 </pages>
Reference: [21] <author> Haykin S. </author> <year> 1991. </year> <title> Adaptive filter theory, 2nd ed., </title> <publisher> Prentice-Hall </publisher>
Reference: [22] <author> Haykin S. </author> <year> 1992. </year> <title> Blind equalisation formulated as a self-organized learning process, </title> <booktitle> Proceedings of the 26th Asilomar conference on signals, systems and computers, </booktitle> <address> Pacific Grove, CA </address>
Reference: [23] <author> Haykin S. (ed.) </author> <year> 1994a. </year> <title> Blind Deconvolution, </title> <publisher> Prentice-Hall, </publisher> <address> New Jersey. </address>
Reference: [24] <author> Haykin S. (ed.) </author> <year> 1994b. </year> <title> Neural networks: a comprehensive foundation, </title> <publisher> MacMillan, </publisher> <address> New York </address>
Reference: [25] <author> Herault J. & Jutten C. </author> <year> 1986. </year> <title> Space or time adaptive signal processing by neural network models, </title> <editor> in Denker J.S. (ed), </editor> <booktitle> Neural networks for computing: AIP conference proceedings 151, American Institute for physics, </booktitle> <address> New York </address>
Reference: [26] <author> Hopfield J.J. </author> <year> 1991. </year> <title> Olfactory computation and object perception, </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <note> vol. 88, pp.6462-6466 </note>
Reference: [27] <author> Jutten C. & Herault J. </author> <year> 1991. </year> <title> Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture, </title> <booktitle> Signal processing 24, </booktitle> <pages> 1-10 </pages>
Reference: [28] <author> Karhunen J. & Joutsensalo J. </author> <year> 1994. </year> <title> Representation and separation of signals using non-linear PCA type learning, </title> <booktitle> Neural networks 7, </booktitle> <volume> 1, </volume> <pages> 113-127 </pages>
Reference: [29] <author> Laughlin S. </author> <year> 1981. </year> <title> A simple coding procedure enhances a neuron's information capacity, </title> <journal> Z. Naturforsch. </journal> <volume> 36, </volume> <pages> 910-912 </pages>
Reference: [30] <author> Linsker R. </author> <year> 1989. </year> <title> An application of the principle of maximum information preservation to linear systems, </title> <booktitle> in Advances in Neural Information Processing Systems 1, </booktitle> <editor> Touretzky D.S. (ed), </editor> <publisher> Morgan-Kauffman </publisher>
Reference: [31] <author> Li S. & Sejnowski T.J. </author> <year> 1994. </year> <title> Adaptive separation of mixed broadband sound sources with delays by a beamforming Herault-Jutten network, </title> <journal> IEEE Journal of Oceanic Engineering, </journal> <volume> 20, 1, </volume> <pages> 73-79 </pages>
Reference: [32] <author> Linsker R. </author> <year> 1992. </year> <title> Local synaptic learning rules suffice to maximise mutual information in a linear network, </title> <booktitle> Neural Computation 4, </booktitle> <pages> 691-702 36 </pages>
Reference: [33] <author> Molgedey L. & Schuster H.G. </author> <year> 1994. </year> <title> Separation of independent signals using time-delayed correlations, </title> <journal> Phys. Rev. Letts. </journal> <volume> 72, 23, </volume> <pages> 3634-3637 </pages>
Reference: [34] <author> Nadal J-P. & Parga N. </author> <year> 1994. </year> <title> Non-linear neurons in the low noise limit: a factorial code maximises information transfer. </title> <journal> Network, </journal> <volume> 5, </volume> <pages> 565-581. </pages>
Reference: [35] <author> Papoulis A. </author> <year> 1984. </year> <title> Probability, random variables and stochastic processes, 2nd edition, </title> <publisher> McGraw-Hill, </publisher> <address> New York </address>
Reference: [36] <author> Parra L., Deco G. & Miesbach S. </author> <year> 1995. </year> <title> Statistical independence and novelty detection with information preserving nonlinear maps, </title> <publisher> Neral Com-put., in press </publisher>
Reference: [37] <author> Platt J.C. & Faggin F. </author> <year> 1992. </year> <title> Networks for the separation of sources that are superimposed and delayed, </title> <booktitle> in Moody J.E et al (eds) Advances in Neural Information Processing Systems 4, </booktitle> <address> p.730-737, </address> <publisher> Morgan-Kaufmann </publisher>
Reference: [38] <author> Plumbley M.D. & Fallside F. </author> <year> 1988. </year> <title> An information-theoretic approach to unsupervised connectionist models, </title> <editor> in Touretzky D, </editor> <booktitle> Hinton G & Se-jnowski T (eds) Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> p.239-245, </address> <publisher> Morgan-Kaufmann 1988 </publisher>
Reference: [39] <author> Schmidhuber J. </author> <year> 1992. </year> <title> Learning factorial codes by predictability minimi-sation, </title> <booktitle> Neural Computation 4, </booktitle> <volume> 6, </volume> <pages> 863-87 </pages>
Reference: [40] <author> Schraudolph N.N., Hart W.E. & Belew R.K. </author> <year> 1991. </year> <title> Optimal information flow in sigmoidal neurons, </title> <type> unpublished manuscript </type>
Reference: [41] <author> Schraudolph N.N. & Sejnowski T.J. </author> <year> 1992. </year> <title> Competitive anti-Hebbian learning of invariants, </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <editor> Moody J.E. et al (eds), </editor> <publisher> Morgan-Kauffman </publisher>
Reference: [42] <author> Schuster H.G. </author> <year> 1992. </year> <title> Learning by maximizing the information transfer through nonlinera noisy neurons and "noise breakdown", </title> <journal> Phys. Rev. A, </journal> <volume> 46,4, </volume> <pages> 2131-2138 </pages>
Reference: [43] <author> Sorouchyari E. </author> <year> 1991. </year> <title> Blind separation of sources, part III: stability analysis, </title> <booktitle> Signal processing 24, </booktitle> <volume> 1, </volume> <pages> 11-20 37 </pages>
Reference: [44] <author> Vittoz E.A. & Arreguit X. </author> <year> 1989. </year> <title> CMOS integration of Herault-Jutten cells for separation of sources, </title> <editor> in Mead C. & Ismail M. </editor> <title> (eds) Analog VLSI implementation of neural systems, </title> <publisher> p.57-84, Kluwer, </publisher> <address> Boston </address>
Reference: [45] <author> Yellin D. & Weinstein E. </author> <year> 1994. </year> <title> Criteria for multichannel signal separation, </title> <journal> IEEE transactions on signal processing, </journal> <volume> vol. 42, no. 8, </volume> <pages> 2158-2168 38 </pages>
References-found: 45

