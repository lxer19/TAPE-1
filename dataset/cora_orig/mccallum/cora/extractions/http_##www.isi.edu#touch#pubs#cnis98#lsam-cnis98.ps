URL: http://www.isi.edu/touch/pubs/cnis98/lsam-cnis98.ps
Refering-URL: http://www.isi.edu/touch/pubs/cnis98/
Root-URL: http://www.isi.edu
Title: The LSAM Proxy Cache (LPC) is web proxy cache designed to support multicast web push
Keyword: multicast, web, proxy, push, cache  
Note: 1: Introduction  
Abstract: 1 , 2 The LSAM Proxy Cache (LPC) is a multicast distributed web cache that provides automated multicast push of web pages, based on self-configuring interest groups. The LPC is designed to reduce network and server load, and to provide increased client performance for associated groups of web pages, called affinity groups. These affinity groups track the shifting popularity of web sites, such as for the Superbowl, the Olympics, and the Academy Awards. The LPCs multicast hierarchy is self-configuring, such that these popular affinity groups are automatically cached at natural network aggregation points. This document describes the LPC architecture and the properties of a prototype implementation. Shared proxy caches allow pages to be shared among a group of clients because they aggregate the responses; even first-time clients can hit in the cache, if a sibling client has recently requested a page. Even when only a small set of clients can fill the cache with new information, the rest of the clients benefit. However, shared proxy caches work only where requests can already be aggregated, e.g., near the border router of a domain. These border proxies are not sufficient for some types of traffic, notably which would aggregate across different borders. There are several examples of groups of related web pages that become popular over time, such as those of the Superbowl, the Olympic games, and the Academy Awards. The content of these pages often shifts over time, as new events occur in the Olympics, or during the football playoffs. Even though these page groups become popular, there is no one place a proxy can be placed to avoid a hot-spot at the server. We call such groups of popular pages affinity groups. As a specific example, the winter Olympics web pages are an affinity group, whose content evolves as the games proceed. Pages for events, such as ice skating, ski jumping, and bobsled, become popular as each event occurs, and as new pages appear with the results of the competition. In current web cache systems, such pages always generate hot-spots, because they are globally interesting. The pages are also part of popular groups, but the page popularity is not known in advance, so client subscriptions would not exist, defeating page-cast systems. In LSAM, a proxy tunes to the servers Olympic channel if its downstream clients are sufficiently interested in the general topic. When a new page appears, e.g., for downhill skiing, its results are multicast to the entire set of tuned-in proxies the first time it is requested by any client. A client near any of these proxies can retrieve the page from the proxy cache, benefiting as there were one global 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Apache HTTP Server Project web page, </institution> <month> March </month> <year> 1998. </year> <note> http://www.apache.org </note>
Reference-contexts: In heavily-loaded systems, the latter may be preferable. The LPC can be parameterized to balance these two competing incentives, by adjusting its threshold for light vs. heavy traffic. 4: Implementation Issues The LPC is implemented as an extension of the Apache proxy cache <ref> [1] </ref>. There are implementation issues in providing multicast proxy caching in the Apache context. There are also development issues that include multicast channel management, intelligent request routing, object scheduling, dynamic caching algorithms, and support for mobility. FIGURE 7. <p> CANES also uses modulo caching to cache pages at every Nth proxy on the unicast return path; the LPC uses announcements and affinity group channels to similarly limit caching to a subset of proxies. 7: Current status The LPC is implemented as a modified version of the Apache proxy cache <ref> [1] </ref>, with additional control scripts and daemons written in Perl. It can be instantiated as a pump or as a filter via command-line arguments, and currently runs on FreeBSD 2.2.x.
Reference: [2] <author> A. Bestavros, </author> <title> Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic, and Service Time, </title> <booktitle> in: Proc. International Conference on Data Engineering, </booktitle> <address> New Orleans, LA, </address> <month> March </month> <year> 1996. </year> <note> http://cs-www.bu.edu/faculty/best/res/papers/icde96.ps </note>
Reference-contexts: It anticipates requests of individual clients by multi-casting pages to the channel. Client-side prefetching, using server-provided hints, has also been examined in the unicast domain in the Boston Univ. Oceans group <ref> [2] </ref>. Other hints have also been used to direct unicast push, such as geographical hints [7]. Other hierarchical cache systems have been developed, including Harvest [4], and its follow-on Squid [15]. Harvest uses a directory to locate entries in a distributed hierarchy.
Reference: [3] <author> S. Bhattacharjee, K. Calvert, E. Zegura, </author> <title> Self-Organizing Wide-Area Network Caches, </title> <booktitle> in: Proc. Infocom 1998, IEEE, </booktitle> <address> San Francisco, CA, </address> <month> April </month> <year> 1998. </year> <note> http://www.cc.gatech.edu/fac/Ellen.Zegura/papers/ git-cc-97-31.ps.gz </note>
Reference-contexts: Unicast diffusion push was examined in another tack of the Oceans group [8]. Other large distributed cache systems have been developed; AT&Ts Crisp uses a central server to distribute requests to a set of local caches, and both Georgia Techs CANES <ref> [3] </ref> and UCLs Cachemesh [14] route requests on their way to the cache. Georgia Techs CANES also targets one of the LPCs goals, to provide caching at optimal network aggregation points.
Reference: [4] <author> A. Chankunthod, P. Danzig, C. Neerdaels, M. Schwartz, K. Worrell, </author> <title> A Hierarchical Internet Object Cache, </title> <booktitle> in: Proc. of USENIX 1996, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year> <note> http://catarina.usc.edu/danzig/cache.ps 7 </note>
Reference-contexts: Client-side prefetching, using server-provided hints, has also been examined in the unicast domain in the Boston Univ. Oceans group [2]. Other hints have also been used to direct unicast push, such as geographical hints [7]. Other hierarchical cache systems have been developed, including Harvest <ref> [4] </ref>, and its follow-on Squid [15]. Harvest uses a directory to locate entries in a distributed hierarchy. Squid uses multicast to find cache entries in hierarchical clusters of caches, sending messages via the ICP protocol.
Reference: [5] <author> J. Cooperstock,., S. Kotsopoulos, </author> <title> Why use a fishing line when you have a net? an Adaptive Multicast data Distribution protocol, </title> <booktitle> in: Proc. of USENIX 1996. </booktitle> <address> http://www.ecf.utoronto.ca/afdp/ </address>
Reference-contexts: The pump monitors web accesses at a server and creates multicast channels for affinity groups. Requests for web pages in the affinity group are reliably multicast to the associated multicast channel using AFDP <ref> [5] </ref>. The pump creates and destroys multicast channels as different affinity groups become more or less popular; these groups are announced on a single, global multicast channel, similar to the teleconference announcement channel in the sd teleconference management tool [12] (Figure 2). FIGURE 1. <p> The LPC uses AFDP to push the file reliably over the multicast channel, directly into the file system of the filter <ref> [5] </ref>. A redirect response is returned to the filter (step 6), and the file is found in the local file system (steps 7-8). As a result of these steps, web pages that are members of the affinity group are multicast to the filters.
Reference: [6] <author> S. Gadde, M. Rabinovich, J. Chase, </author> <title> Reduce, Reuse, Recycle: An Approach to Building Large Internet Caches, </title> <booktitle> in: Proc. of Workshop on Hot Topics in Operating Systems (HotOS), </booktitle> <month> May </month> <year> 1997. </year> <note> http://www.research.att.com/~misha/crisp/distrProxy/ hotos.ps </note>
Reference: [7] <author> J. Gwertzman, M. Seltzer, </author> <title> The Case for Geographical Push-Caching, </title> <booktitle> in: Proc. Fifth Annual Workshop on Hot Operating Systems, </booktitle> <address> Orcas Island, WA, </address> <month> May </month> <year> 1995. </year> <note> http://www.eecs.harvard.edu/~vino/web/hotos.ps </note>
Reference-contexts: It anticipates requests of individual clients by multi-casting pages to the channel. Client-side prefetching, using server-provided hints, has also been examined in the unicast domain in the Boston Univ. Oceans group [2]. Other hints have also been used to direct unicast push, such as geographical hints <ref> [7] </ref>. Other hierarchical cache systems have been developed, including Harvest [4], and its follow-on Squid [15]. Harvest uses a directory to locate entries in a distributed hierarchy. Squid uses multicast to find cache entries in hierarchical clusters of caches, sending messages via the ICP protocol.
Reference: [8] <author> A. Heddaya, S. Mirdad, D. Yates, </author> <title> Diffusion Based Caching along Routing Paths, </title> <booktitle> in: Proc. of the 2nd NLANR Web Cache Workshop, </booktitle> <address> Boulder, CO, </address> <month> June </month> <year> 1997. </year> <note> http://www.cs.bu.edu/faculty/heddaya/ Papers-NonTR/webcache-wkp.ps.Z </note>
Reference-contexts: Many other web cache systems use multicast to distributed pages, supporting explicit subscriptions in NCSAs Webcast [10], and diffusion-based push to move pages closer to their locus of interest in LBNL/UCLAs Adaptive Web Caching [16]. Unicast diffusion push was examined in another tack of the Oceans group <ref> [8] </ref>. Other large distributed cache systems have been developed; AT&Ts Crisp uses a central server to distribute requests to a set of local caches, and both Georgia Techs CANES [3] and UCLs Cachemesh [14] route requests on their way to the cache.
Reference: [9] <author> LSAM web pages, </author> <month> March </month> <year> 1998. </year> <note> http://www.isi.edu/lsam/ </note>
Reference-contexts: Six different cache replacement algorithms have been implemented, selected in the configuration file at proxy boot time. Several different object scheduling mechanisms have also been implemented, and compared in network-limited and processor-limited environments. This release, v0.8, is currently available on the LSAM web pages <ref> [9] </ref>. The current system has been demonstrated in a lab, using artificial bandwidth limiters and delay inducers. A demonstration is also available, implemented in the ns network simulation tool. In both cases, client access is equivalent to a local cache hit, even for pages not yet accessed.
Reference: [10] <author> NCSA, </author> <title> Overview of Webcast, web pages, </title> <month> March </month> <year> 1998. </year> <note> http://wwwdoctest.ncsa.uiuc.edu/SDG/Software/ XMosaic/CCI/webcast-doc.html </note>
Reference-contexts: Harvest uses a directory to locate entries in a distributed hierarchy. Squid uses multicast to find cache entries in hierarchical clusters of caches, sending messages via the ICP protocol. Many other web cache systems use multicast to distributed pages, supporting explicit subscriptions in NCSAs Webcast <ref> [10] </ref>, and diffusion-based push to move pages closer to their locus of interest in LBNL/UCLAs Adaptive Web Caching [16]. Unicast diffusion push was examined in another tack of the Oceans group [8].
Reference: [11] <institution> NLANR global internet cache web pages, </institution> <month> March </month> <year> 1998. </year> <title> http://ircache.nlanr.net/ [12] sd tool web pages, </title> <month> March </month> <year> 1998. </year> <month> ftp://ftp.ee.lbl.gov/conferencing/sd// </month>
Reference-contexts: The LSAM project is currently developing three algorithms for syntactic approximation of semantic affinity groups: on-line algorithms for the pump and filter, and an off-line algorithm for performance analysis. The latter, used to analyze regional Squid caches <ref> [11] </ref>, will help determine the current potential for performance enhancement by the LPC.
Reference: [13] <author> J. </author> <title> Touch, Defining `High Speed' Protocols: Five Challenges & an Example That Survives the Challenges, </title> <journal> IEEE Journal on Selected Areas of Communications v13 (June 1995) 828-835. </journal> <note> http://www.isi.edu/touch/pubs/jsac95.html </note>
Reference-contexts: The LPCs main distinction is its use of multicast push to reduce the first-hit cost of retrieval throughout the system. The LPC is based on source preloading of a receiver cache, a multicast version of an earlier unicast scheme <ref> [13] </ref>. It anticipates requests of individual clients by multi-casting pages to the channel. Client-side prefetching, using server-provided hints, has also been examined in the unicast domain in the Boston Univ. Oceans group [2]. Other hints have also been used to direct unicast push, such as geographical hints [7].
Reference: [14] <author> Z. Wang, Cachemesh: </author> <title> A Distributed Cache System for the World Wide Web, </title> <booktitle> in: Proc. of the 2nd NLANR Web Caching Workshop, </booktitle> <address> Boulder, CO, </address> <month> June </month> <year> 1997. </year> <note> http://www.bell-labs.com/user/zhwang/papers/ cache.html </note>
Reference-contexts: Unicast diffusion push was examined in another tack of the Oceans group [8]. Other large distributed cache systems have been developed; AT&Ts Crisp uses a central server to distribute requests to a set of local caches, and both Georgia Techs CANES [3] and UCLs Cachemesh <ref> [14] </ref> route requests on their way to the cache. Georgia Techs CANES also targets one of the LPCs goals, to provide caching at optimal network aggregation points. CANES deploys caches at routers using Active Networks technology; The LPC relies on multicast to 6 achieve similar benefits using only application-layer code.
Reference: [15] <author> D. Wessels, K. Claffy, </author> <title> ICP and the Squid Web Cache, </title> <address> NLANR, </address> <month> August </month> <year> 1997. </year> <note> http://www.nlanr.net/%7ewessels/Papers/icp-squid.ps </note>
Reference-contexts: One result of deep proxy chains is poor response for missed data, because requests do not cut-through the chain; the check-and-forward style cache checking at each level imposes unacceptable delays for more than 2-3 levels <ref> [15] </ref>. The solution is to allow requests to split at the first proxy, into a foreground request that walks the chain, and a background request that cuts-through to the root server (Figure 8). Object scheduling supports the variety of request priorities inherent in the LPC. <p> Client-side prefetching, using server-provided hints, has also been examined in the unicast domain in the Boston Univ. Oceans group [2]. Other hints have also been used to direct unicast push, such as geographical hints [7]. Other hierarchical cache systems have been developed, including Harvest [4], and its follow-on Squid <ref> [15] </ref>. Harvest uses a directory to locate entries in a distributed hierarchy. Squid uses multicast to find cache entries in hierarchical clusters of caches, sending messages via the ICP protocol.

References-found: 14

