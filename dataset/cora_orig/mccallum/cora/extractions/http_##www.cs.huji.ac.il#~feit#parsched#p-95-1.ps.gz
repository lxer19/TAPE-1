URL: http://www.cs.huji.ac.il/~feit/parsched/p-95-1.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~feit/parsched/parsched95.html
Root-URL: http://www.cs.huji.ac.il
Email: feit@watson.ibm.com  rudolph@theory.lcs.mit.edu  
Phone: 2  
Title: Parallel Job Scheduling: Issues and Approaches  
Author: Dror G. Feitelson and Larry Rudolph ? P. O. 
Address: Box 218, Yorktown Heights, NY 10598  91904 Jerusalem, Israel  
Affiliation: 1 IBM T. J. Watson Research Center  Institute of Computer Science The Hebrew University,  
Abstract: Parallel job scheduling is beginning to gain recognition as an important topic that is distinct from the scheduling of tasks within a parallel job by the programmer or runtime system. The main issue is how to share the resources of the parallel machine among a number of competing jobs, giving each the required level of service. This level of scheduling is done by the operating system. The four most commonly used or advocated techniques are to use a global queue, use variable partitioning, use dynamic partitioning, and use gang scheduling. These techniques are surveyed, and the benefits and shortcomings of each are identified. Then additional requirements that are not addressed by current systems are outlined, followed by considerations for evaluating various scheduling schemes.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Alverson, S. Kahan, R. Korry, C. McCann, and B. Smith, </author> <title> "Scheduling on the Tera MTA". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The challenge is to design the system so that it can take advantage of such resources if they are available, and to quantify the effect of doing without them if they are not, so users can make an informed choice <ref> [1] </ref>. Ignoring the issue is not a viable alternative. Finally, one should remember that the operating system is there to serve applications, and through them, to serve users, and the client is always right. Thus the operating system should provide opportunities, not impose restrictions. <p> Of course, there is also ample place for more work on scheduling schemes for both common parallel systems and emerging new types of systems, such as mul-tithreaded architectures <ref> [1] </ref> and NOWs [36]. The future will tell which of these survive the ultimate test, that of satisfying real users.
Reference: 2. <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> "Scheduler ac-tivations: effective kernel support for the user-level management of parallelism". </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 10(1), </volume> <pages> pp. 53-79, </pages> <month> Feb </month> <year> 1992. </year>
Reference-contexts: The spectrum of opinions range from those who claim that most scheduling should be done at the application level, keeping the operating system out of the loop <ref> [2] </ref>, to those that prefer to let the operating system do all the work [4]. Interrupts, context switches, and memory swapping are overheads that can be very costly on a parallel supercomputer. <p> And the overheads for repartitioning, which include switching processors from one protection domain to another, may negate the benefits that were expected [44]. In addition, dynamic partitioning requires extensive coordination between the operating system and the application's internal scheduler, so as to handle the changes in processor allocation efficiently <ref> [2] </ref>. For example, if a processor is taken away from an application, the whole application might deadlock if the thread running on that processor happened to be holding a lock. To prevent such scenarios, the application's runtime system should be notified so it can take appropriate action.
Reference: 3. <author> J. M. Barton and N. Bitar, </author> <title> "A scalable multi-discipline, multiple-processor scheduling framework for IRIX ". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Processors pick the first thread from the queue, execute it for a certain time quantum, and then return it to the queue. This approach is especially common on small-scale bus-based UMA shared memory machines, such as Sequent multiprocessors [46] and SGI multiprocessor workstations <ref> [3] </ref>, and is also used in the Mach operating system [4]. The main merit of a global queue is that it provides automatic load sharing. No processor is idle if there is any waiting thread in the system. However, this comes at a price. <p> This solution is completely general, and works for any programming model. In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors <ref> [3] </ref>, the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes [32, 13, 19, 22]. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16].
Reference: 4. <author> D. L. Black, </author> <title> "Scheduling support for concurrency and parallelism in the Mach operating system". </title> <booktitle> Computer 23(5), </booktitle> <pages> pp. 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The spectrum of opinions range from those who claim that most scheduling should be done at the application level, keeping the operating system out of the loop [2], to those that prefer to let the operating system do all the work <ref> [4] </ref>. Interrupts, context switches, and memory swapping are overheads that can be very costly on a parallel supercomputer. However, as parallel systems become more commonly used, there is growing recognition of the need for resource management at the operating system level. <p> This approach is especially common on small-scale bus-based UMA shared memory machines, such as Sequent multiprocessors [46] and SGI multiprocessor workstations [3], and is also used in the Mach operating system <ref> [4] </ref>. The main merit of a global queue is that it provides automatic load sharing. No processor is idle if there is any waiting thread in the system. However, this comes at a price. One problem is contention for the global queue, which grows with the number of processors.
Reference: 5. <author> S. H. Bokhari, </author> <title> "On the mapping problem". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-30(3), </volume> <pages> pp. 207-214, </pages> <month> Mar </month> <year> 1981. </year>
Reference-contexts: This is a compromise between two conflicting forces: keeping nodes separate increases parallelism at the cost of communication, whereas clustering them causes serialization but saves on communication [29, 34]. If the interconnection network has some specific topology, this can also be taken into account <ref> [5] </ref>. 2.2 Scheduling in the Runtime System While DAG scheduling has generated a large body of research, much of it cannot be applied to programming environments that use other representations.
Reference: 6. <author> F. P. Brooks, Jr., </author> <title> The Mythical Man-Month: </title> <booktitle> Essays on Software Engineering. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1975. </year>
Reference-contexts: This has two aspects. The first is functionality (e.g. interactive 3 Brooks estimates a factor of three for each of the two main hurdles, regardless of the order in which they are undertaken; these are (1) system integration and (2) creating product-quality, tested, documented, and maintainable software <ref> [6] </ref>. response time and virtual memory). The second is performance (e.g. low average response time and high throughput) 4 . The most precise form of evaluation is to implement a full system and measure its performance when used in a production environment.
Reference: 7. <author> R. M. Bryant, H-Y. Chang, and B. S. Rosenburg, </author> <title> "Operating system support for parallel programming on RP3". </title> <journal> IBM J. Res. Dev. </journal> <volume> 35(5/6), </volume> <pages> pp. 617-634, </pages> <month> Sep/Nov </month> <year> 1991. </year>
Reference-contexts: Other variants include coscheduling, which attempts to schedule a large subset of the gang if it is impossible to schedule all the threads at once [32], and family scheduling, which allows more threads than processors and uses a second level of internal time slicing <ref> [7] </ref>. The price of gang scheduling is that the overall system performance is not always optimal (although it does favor individual jobs). There is some interference in the cache, and overhead for the context switching. In addition, there may be some processor fragmentation.
Reference: 8. <author> N. Carriero, E. Freedman, D. Gelernter, and D. Kaminsky, </author> <title> "Adaptive parallelism and Piranha". </title> <booktitle> Computer 28(1), </booktitle> <pages> pp. 40-49, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: As a result of the above shortcomings, dynamic partitioning has so far been used only to a very limited degree, mainly in the context of running parallel jobs on networks of workstations, where the workstation must be returned to its owner when required <ref> [8, 36] </ref>. 3.4 Gang Scheduling The only way to guarantee interactive response times is via time slicing. However, if done in an uncoordinated manner, as with a global queue, this can lead to large inefficiencies. Rather, the context switching should be coordinated across the processors.
Reference: 9. <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum, </author> <title> "Scheduling and page migration for multiprocessor compute servers". </title> <booktitle> In 6th Intl. Conf. Architect. Support for Prog. Lang. & Operating Syst., </booktitle> <pages> pp. 12-24, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: In addition, there may be some processor fragmentation. However, due to the time slicing, its effect is less severe than in variable partitioning [14, 15]. Indeed, most studies find that gang scheduling is nearly as efficient as dynamic partitioning <ref> [9] </ref>. 4 The Requirements Literally hundreds of papers have been written about job scheduling in parallel systems (see [11]). However, in many respects, we are at the beginning of the beginning.
Reference: 10. <author> C. Connelly and C. S. Ellis, </author> <title> "Scheduling to reduce memory coherence overhead on coarse-grain multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The system can use a page fault by one process to trigger a prefetch for the same page in other processes. Swapping can be done by collective I/O, where the program images on all processors are downloaded to multiple disks in parallel. Other ideas have also been proposed <ref> [10, 41] </ref>. Regrettably, many systems do not take such a comprehensive approach. The reason is that creating a useful cohesive system is an order of magnitude 3 harder than creating a working prototype, and that is already hard enough.
Reference: 11. <author> D. G. Feitelson, </author> <title> A Survey of Scheduling in Multiprogrammed Parallel Systems. </title> <type> Research Report RC 19790 (87657), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> Oct </month> <year> 1994. </year>
Reference-contexts: An interesting observation is that the two sharing schemes are largely orthogonal, so various combinations can be tried. Indeed, a remarkable variety of approaches have been devised over the years (see Fig. 3) <ref> [11] </ref>. The following subsections survey the four most popular approaches: global queue, variable partitioning, dynamic partitioning with two-level scheduling, and gang scheduling. Of these, two emphasize the use of space slicing (variable and dynamic partitioning), and two the use of time slicing (global queue and gang scheduling). <p> Examples of systems that use different combinations of space slicing and time slicing (updated from <ref> [11] </ref>). placed in this queue. Processors pick the first thread from the queue, execute it for a certain time quantum, and then return it to the queue. <p> There are different approaches to partitioning, and the following taxonomy is common (see Fig. 4) <ref> [11] </ref>: Fixed partitioning is when the partition sizes are set in advance by the system administrator. Repartitioning requires a reboot. Variable is when the set of nodes are partitioned according to user requests when jobs are submitted. <p> However, due to the time slicing, its effect is less severe than in variable partitioning [14, 15]. Indeed, most studies find that gang scheduling is nearly as efficient as dynamic partitioning [9]. 4 The Requirements Literally hundreds of papers have been written about job scheduling in parallel systems (see <ref> [11] </ref>). However, in many respects, we are at the beginning of the beginning. The split between the academic focus on dynamic partitioning and the practical use of variable partitioning and gang scheduling is a symptom of disagreement even about the fundamental questions.
Reference: 12. <author> D. G. Feitelson and B. Nitzberg, </author> <title> "Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: There is a very diverse set of requirements, spanning a spectrum from rather small interactive jobs, through large but time-sensitive jobs, to very large and not-time-sensitive jobs that can be satisfied by a batch system <ref> [12] </ref>. The problem is that these different classes may be present on the same system at the same time, and each must be serviced according to its unique requirements. This wide distribution of requirements implies that some sort of time slicing has to be used [35]. <p> This again points out the utility of measurements of real systems, albeit not measurements of the system performance per se but rather measurements of user behavior <ref> [12] </ref>. The usefulness of the results obtained from simulation or analysis depends on the metrics that are used. Amazingly little work has gone into defining and evaluating metrics. Even the straightforward metrics such as average response time and throughput have problems.
Reference: 13. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Distributed hierarchical control for parallel processing". </title> <booktitle> Computer 23(5), </booktitle> <pages> pp. 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors [3], the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes <ref> [32, 13, 19, 22] </ref>. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43].
Reference: 14. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Mapping and scheduling in a shared parallel environment using distributed hierarchical control". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 1-8, </pages> <month> Aug </month> <year> 1990. </year>
Reference-contexts: There is some interference in the cache, and overhead for the context switching. In addition, there may be some processor fragmentation. However, due to the time slicing, its effect is less severe than in variable partitioning <ref> [14, 15] </ref>. Indeed, most studies find that gang scheduling is nearly as efficient as dynamic partitioning [9]. 4 The Requirements Literally hundreds of papers have been written about job scheduling in parallel systems (see [11]). However, in many respects, we are at the beginning of the beginning.
Reference: 15. <author> D. G. Feitelson and L. Rudolph, </author> <booktitle> "Wasted resources in gang scheduling ". In 5th Jerusalem Conf. Information Technology, </booktitle> <pages> pp. 127-136, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Oct </month> <year> 1990. </year>
Reference-contexts: There is some interference in the cache, and overhead for the context switching. In addition, there may be some processor fragmentation. However, due to the time slicing, its effect is less severe than in variable partitioning <ref> [14, 15] </ref>. Indeed, most studies find that gang scheduling is nearly as efficient as dynamic partitioning [9]. 4 The Requirements Literally hundreds of papers have been written about job scheduling in parallel systems (see [11]). However, in many respects, we are at the beginning of the beginning.
Reference: 16. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Gang scheduling performance benefits for fine-grain synchronization". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 16(4), </volume> <pages> pp. 306-318, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: If the interaction is at a high rate (i.e. a fine granularity), the lack of coordination in the scheduling implies that often the interacting partners will not be executing at the same time. Therefore the interactions will not be able to proceed, inducing extra context switches and overhead <ref> [16] </ref>. Finally, scheduling from a global queue has the interesting property that the service a job receives is proportional to the number of threads that it spawns. It is subject to debate whether this is good or bad. <p> An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently <ref> [16] </ref>. Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43].
Reference: 17. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Coscheduling based on runtime identification of activity working sets". </title> <booktitle> Intl. J. Parallel Programming 23(2), </booktitle> <pages> pp. 135-160, </pages> <month> Apr </month> <year> 1995. </year>
Reference-contexts: An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs <ref> [17, 43] </ref>. Other variants include coscheduling, which attempts to schedule a large subset of the gang if it is impossible to schedule all the threads at once [32], and family scheduling, which allows more threads than processors and uses a second level of internal time slicing [7].
Reference: 18. <author> M. J. Gonzalez, Jr., </author> <title> "Deterministic processor scheduling". </title> <journal> ACM Comput. Surv. </journal> <volume> 9(3), </volume> <pages> pp. 173-204, </pages> <month> Sep </month> <year> 1977. </year>
Reference-contexts: Many restricted cases of DAG scheduling have been shown to be NP-complete, meaning that it is impractical to look for an optimal schedule. This is true even for very restricted special cases, e.g. when communication is free and all tasks have unit execution times <ref> [48, 18] </ref>. Many parallel programming styles are close to the DAG model, although with substantially less information. Dataflow, functional programming, and other side-effect free programming models can be viewed as DAGs. Consequently, various heuristics have been developed to approximate good schedules.
Reference: 19. <author> B. C. Gorda and E. D. Brooks III, </author> <title> Gang Scheduling a Parallel Machine. </title> <type> Technical Report UCRL-JC-107020, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors [3], the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes <ref> [32, 13, 19, 22] </ref>. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43].
Reference: 20. <author> A. Gupta, A. Tucker, and S. Urushibara, </author> <title> "The impact of operating system schedul-ing policies and synchronization methods on the performance of parallel applications". </title> <booktitle> In SIGMETRICS Conf. Measurement & Modeling of Comput. Syst., </booktitle> <pages> pp. 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Dynamic partitioning with two-level scheduling is probably the most studied parallel scheduling scheme, and has repeatedly been shown to be superior to other schemes <ref> [30, 20] </ref> either by analytic means or through simulation with synthetic workloads. This is due to a number of factors: There is no loss of resources to fragmentation. There is no overhead for context switching, except that for redistributing the processors when the load changes.
Reference: 21. <author> R. L. Henderson, </author> <title> "Job scheduling under the portable batch system". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Therefore variable partitioning is more suitable for batch processing than for interactive work. Indeed, the inability to run a program when desired sometimes causes significant user frustration, especially during the program development phase. To reduce this effect, sophisticated batch systems are designed <ref> [24, 21, 27] </ref>. 3.3 Dynamic Partitioning with Two-Level Scheduling One way to reduce the waiting time of queued jobs is to prevent jobs from monopolizing too many processors when the system is heavily loaded.
Reference: 22. <author> A. Hori et al., </author> <title> "Time space sharing scheduling and architectural support". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors [3], the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes <ref> [32, 13, 19, 22] </ref>. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43].
Reference: 23. <author> Intel Supercomputer Systems Division, </author> <title> Paragon User's Guide. Order number 312489-003, </title> <month> Jun </month> <year> 1994. </year>
Reference-contexts: This solution is completely general, and works for any programming model. In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors [3], the Intel Paragon <ref> [23] </ref>, and the Meiko CS-2, and has also been studied in academia and research prototypes [32, 13, 19, 22]. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16].
Reference: 24. <author> O. Kipersztok and J. C. Patterson, </author> <title> "Intelligent fuzzy control to augment the scheduling capabilities of network queueing systems". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Therefore variable partitioning is more suitable for batch processing than for interactive work. Indeed, the inability to run a program when desired sometimes causes significant user frustration, especially during the program development phase. To reduce this effect, sophisticated batch systems are designed <ref> [24, 21, 27] </ref>. 3.3 Dynamic Partitioning with Two-Level Scheduling One way to reduce the waiting time of queued jobs is to prevent jobs from monopolizing too many processors when the system is heavily loaded.
Reference: 25. <author> L. Kleinrock and J-H. Huang, </author> <title> "On parallel processing systems: Amdahl's law generalized and some results on optimal design". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 18(5), </volume> <pages> pp. 434-447, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Instead of using multiple independent metrics, it has sometimes been proposed to combine them into a derived metric. For example, "power" is loosely defined as the throughput divided by the response time, so it goes up when throughput goes up or when response time goes down <ref> [25] </ref>. However, it is not clear that this simple equation captures the relative importance of the two orig 4 Note that we make a distinction between the qualitative requirement of interactive response time and the quantitative measure of low average response time. inal metrics.
Reference: 26. <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Gan-mukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5". </title> <booktitle> In 4th Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 272-285, </pages> <month> Jun </month> <year> 1992. </year>
Reference-contexts: This solution is completely general, and works for any programming model. In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines <ref> [26] </ref>, the IRIX system on SGI multiprocessors [3], the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes [32, 13, 19, 22].
Reference: 27. <author> D. Lifka, </author> <title> "The ANL/IBM SP scheduling system". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Therefore variable partitioning is more suitable for batch processing than for interactive work. Indeed, the inability to run a program when desired sometimes causes significant user frustration, especially during the program development phase. To reduce this effect, sophisticated batch systems are designed <ref> [24, 21, 27] </ref>. 3.3 Dynamic Partitioning with Two-Level Scheduling One way to reduce the waiting time of queued jobs is to prevent jobs from monopolizing too many processors when the system is heavily loaded.
Reference: 28. <author> D. J. Lilja, </author> <title> "Exploiting the parallelism available in loops". </title> <booktitle> Computer 27(2), </booktitle> <pages> pp. 13-26, </pages> <month> Feb </month> <year> 1994. </year>
Reference-contexts: The runtime system is responsible for implementing these functions when they are called. Another example is the use of parallelizing compilers. Parallelizing compilers usually extract parallelism from the loops of sequential programs. At runtime, the different loop iterations are scheduled for execution on distinct processors <ref> [28] </ref>. A typical approach is to assign decreasing chunks of iterations to the different processors, so as to balance the load on one hand while avoiding extra coordination on the other.
Reference: 29. <author> V. M. Lo, </author> <title> "Heuristic algorithms for task assignment in distributed systems". </title> <journal> IEEE Trans. Comput. </journal> <volume> 37(11), </volume> <pages> pp. 1384-1397, </pages> <month> Nov </month> <year> 1988. </year>
Reference-contexts: This is a compromise between two conflicting forces: keeping nodes separate increases parallelism at the cost of communication, whereas clustering them causes serialization but saves on communication <ref> [29, 34] </ref>. If the interconnection network has some specific topology, this can also be taken into account [5]. 2.2 Scheduling in the Runtime System While DAG scheduling has generated a large body of research, much of it cannot be applied to programming environments that use other representations.
Reference: 30. <author> C. McCann, R. Vaswani, and J. Zahorjan, </author> <title> "A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors". </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 11(2), </volume> <pages> pp. 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Dynamic partitioning with two-level scheduling is probably the most studied parallel scheduling scheme, and has repeatedly been shown to be superior to other schemes <ref> [30, 20] </ref> either by analytic means or through simulation with synthetic workloads. This is due to a number of factors: There is no loss of resources to fragmentation. There is no overhead for context switching, except that for redistributing the processors when the load changes.
Reference: 31. <author> M. G. Norman and P. Thanisch, </author> <title> "Models of machines and computation for mapping in multicomputers". </title> <journal> ACM Comput. Surv. </journal> <volume> 25(3), </volume> <pages> pp. 263-302, </pages> <month> Sep </month> <year> 1993. </year>
Reference-contexts: When the system is modeled as a set of homogeneous processors, that can each execute one task at a time, the objective is usually to minimize the makespan <ref> [31] </ref>. In some cases, communication costs are also explicitly modeled.
Reference: 32. <author> J. K. Ousterhout, </author> <title> "Scheduling techniques for concurrent systems". </title> <booktitle> In 3rd Intl. Conf. Distributed Comput. Syst., </booktitle> <pages> pp. 22-30, </pages> <month> Oct </month> <year> 1982. </year>
Reference-contexts: In fact, it decouples the application from the operating system. It is used by some vendors, e.g. the CM-5 from Thinking Machines [26], the IRIX system on SGI multiprocessors [3], the Intel Paragon [23], and the Meiko CS-2, and has also been studied in academia and research prototypes <ref> [32, 13, 19, 22] </ref>. An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43]. <p> Therefore the rate of interaction can be used to drive the grouping of threads into gangs [17, 43]. Other variants include coscheduling, which attempts to schedule a large subset of the gang if it is impossible to schedule all the threads at once <ref> [32] </ref>, and family scheduling, which allows more threads than processors and uses a second level of internal time slicing [7]. The price of gang scheduling is that the overall system performance is not always optimal (although it does favor individual jobs).
Reference: 33. <author> C. M. Pancake, </author> <title> "Multithreaded languages for scientific and technical computing". </title> <booktitle> Proc. IEEE 81(2), </booktitle> <pages> pp. 288-304, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: In many such environments, the programmer is actually shielded from the details of how the program is mapped onto the machine. The details are then handled by the environment's runtime system. One example of this approach is the use of thread packages <ref> [33] </ref>. The application is structured as a set of interacting threads. The environment supplies functions for thread creation, synchronization, and termination. The runtime system is responsible for implementing these functions when they are called. Another example is the use of parallelizing compilers.
Reference: 34. <author> C. H. Papadimitriou and M. Yannakakis, </author> <title> "Towards an architecture-independent analysis of parallel algorithms". </title> <journal> SIAM J. Comput. </journal> <volume> 19(2), </volume> <pages> pp. 322-328, </pages> <month> Apr </month> <year> 1990. </year>
Reference-contexts: This is a compromise between two conflicting forces: keeping nodes separate increases parallelism at the cost of communication, whereas clustering them causes serialization but saves on communication <ref> [29, 34] </ref>. If the interconnection network has some specific topology, this can also be taken into account [5]. 2.2 Scheduling in the Runtime System While DAG scheduling has generated a large body of research, much of it cannot be applied to programming environments that use other representations.
Reference: 35. <author> E. W. Parsons and K. C. Sevcik, </author> <title> "Multiprocessor scheduling for high-variability service time distributions". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The problem is that these different classes may be present on the same system at the same time, and each must be serviced according to its unique requirements. This wide distribution of requirements implies that some sort of time slicing has to be used <ref> [35] </ref>. An important aspect of workload requirements that is often overlooked is memory and secondary storage. In fact, many respectable commercial parallel systems (e.g. the CM-5) do not support memory paging: applications are required to fit into physical memory.
Reference: 36. <author> J. Pruyne and M. Livny, </author> <title> "Parallel processing on dynamic resources with CARMI". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: As a result of the above shortcomings, dynamic partitioning has so far been used only to a very limited degree, mainly in the context of running parallel jobs on networks of workstations, where the workstation must be returned to its owner when required <ref> [8, 36] </ref>. 3.4 Gang Scheduling The only way to guarantee interactive response times is via time slicing. However, if done in an uncoordinated manner, as with a global queue, this can lead to large inefficiencies. Rather, the context switching should be coordinated across the processors. <p> Of course, there is also ample place for more work on scheduling schemes for both common parallel systems and emerging new types of systems, such as mul-tithreaded architectures [1] and NOWs <ref> [36] </ref>. The future will tell which of these survive the ultimate test, that of satisfying real users.
Reference: 37. <author> M. E. Rosenkrantz, D. J. Schneider, R. Leibensperger, M. shore, and J. Zollweg, </author> <title> "Requirements of the Cornell Theory Center for resource management and process scheduling ". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: be isolated from each other, that an accounting service be provided, that the scheduler be aware of the fact that multiple processes belong to the same job (and all should be killed if one terminates abnormally), and that systems where processors have somewhat different configurations have to be supported efficiently <ref> [40, 37] </ref>. In addition, it is important to understand the workload that must be supported. Obviously, the workload is related to the ultimate use of the parallel machine. It seems that parallel supercomputers are used for three main reasons: Short response time.
Reference: 38. <author> E. Rosti, E. Smirni, G. Serazzi, and L. W. Dowdy, </author> <title> "Analysis of non-work-conserving processor partitioning policies". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: This is done by adaptive partitioning schemes, and several ideas about how to allocate the processors have been proposed <ref> [38] </ref>. The problem with adaptive partitioning, however, is that once a number of processors are allocated to a job this number is fixed until the job terminates.
Reference: 39. <author> L. Rudolph, M. Slivkin-Allalouf, and E. Upfal, </author> <title> "A simple load balancing scheme for task allocation in parallel machines". </title> <booktitle> In 3rd Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 237-245, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: This effect is countered to a certain degree by affinity scheduling, which attempts to re-schedule threads on the same processors they used before [47]. Alternatively, it is possible to provide global load balance with local queues and occasional migration to overcome any imbalance <ref> [39] </ref>. A third problem with a global queue is that the threads in a single application are scheduled in an uncoordinated manner. This may be fine if the threads do not interact with each other. But in many parallel programs the threads do interact and synchronize with each other.
Reference: 40. <author> W. Saphir, L. A. Tanner, and B. Traversat, </author> <title> "Job management requirements for NAS parallel systems and clusters". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: be isolated from each other, that an accounting service be provided, that the scheduler be aware of the fact that multiple processes belong to the same job (and all should be killed if one terminates abnormally), and that systems where processors have somewhat different configurations have to be supported efficiently <ref> [40, 37] </ref>. In addition, it is important to understand the workload that must be supported. Obviously, the workload is related to the ultimate use of the parallel machine. It seems that parallel supercomputers are used for three main reasons: Short response time.
Reference: 41. <author> S. Setia, </author> <title> "The interaction between memory allocation and adaptive partitioning in message-passing multicomputers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: The system can use a page fault by one process to trigger a prefetch for the same page in other processes. Swapping can be done by collective I/O, where the program images on all processors are downloaded to multiple disks in parallel. Other ideas have also been proposed <ref> [10, 41] </ref>. Regrettably, many systems do not take such a comprehensive approach. The reason is that creating a useful cohesive system is an order of magnitude 3 harder than creating a working prototype, and that is already hard enough.
Reference: 42. <author> J. E. Smith, </author> <title> "Characterizing computer performance with a single number". </title> <journal> Comm. ACM 31(10), </journal> <pages> pp. 1202-1206, </pages> <month> Oct </month> <year> 1988. </year>
Reference-contexts: On the other hand, makespan can be used to gauge the effectiveness of a batch system (which often operates in an off-line mode), but is useless for interactive systems. Finally, when combining the results of multiple experiments, care must be taken to use the correct method for averaging <ref> [42] </ref>. Instead of using multiple independent metrics, it has sometimes been proposed to combine them into a derived metric. For example, "power" is loosely defined as the throughput divided by the response time, so it goes up when throughput goes up or when response time goes down [25].
Reference: 43. <author> P. G. Sobalvarro and W. E. Weihl, </author> <title> "Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: An interesting variant of gang scheduling is based on the observation that coordinated scheduling is only needed if the job's threads interact frequently [16]. Therefore the rate of interaction can be used to drive the grouping of threads into gangs <ref> [17, 43] </ref>. Other variants include coscheduling, which attempts to schedule a large subset of the gang if it is impossible to schedule all the threads at once [32], and family scheduling, which allows more threads than processors and uses a second level of internal time slicing [7].
Reference: 44. <author> M. S. Squillante, </author> <title> "On the benefits and limitations of dynamic partitioning in parallel computer systems". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: Moreover, it could lead to extensive queueing if sufficient processors are not available. And the overheads for repartitioning, which include switching processors from one protection domain to another, may negate the benefits that were expected <ref> [44] </ref>. In addition, dynamic partitioning requires extensive coordination between the operating system and the application's internal scheduler, so as to handle the changes in processor allocation efficiently [2].
Reference: 45. <author> I. Stoica, H. Abdel-Wahab, and A. Pothen, </author> <title> "A microeconomic scheduler for parallel computers". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <note> Lecture Notes in Computer Science Vol. 949. </note>
Reference-contexts: It is hard enough to program parallel machines effectively without restricting the available tools and idioms. In addition, it is important to provide consistent and predictable service, including provisions for fairness and control <ref> [45] </ref>. 5 Evaluation Designs must be evaluated carefully to gauge their effectiveness. For evaluation to be meaningful it should measure something meaningful. In system design, the meaningful metric is adherence to the requirements, i.e. support for the expected workload. This has two aspects.
Reference: 46. <author> S. Thakkar, P. Gifford, and G. Fielland, </author> <title> "Balance: a shared memory multiprocessor system". </title> <booktitle> In 2nd Intl. Conf. Supercomputing, </booktitle> <volume> vol. I, </volume> <pages> pp. 93-101, </pages> <year> 1987. </year>
Reference-contexts: Processors pick the first thread from the queue, execute it for a certain time quantum, and then return it to the queue. This approach is especially common on small-scale bus-based UMA shared memory machines, such as Sequent multiprocessors <ref> [46] </ref> and SGI multiprocessor workstations [3], and is also used in the Mach operating system [4]. The main merit of a global queue is that it provides automatic load sharing. No processor is idle if there is any waiting thread in the system. However, this comes at a price.
Reference: 47. <author> J. Torrellas, A. Tucker, and A. Gupta, </author> <title> "Evaluating the performance of cache-affinity scheduling in shared-memory multiprocessors". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 24(2), </volume> <pages> pp. 139-151, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: As a result, threads cannot stash data in local memory, and their cache state is wiped out with each rescheduling. This effect is countered to a certain degree by affinity scheduling, which attempts to re-schedule threads on the same processors they used before <ref> [47] </ref>. Alternatively, it is possible to provide global load balance with local queues and occasional migration to overcome any imbalance [39]. A third problem with a global queue is that the threads in a single application are scheduled in an uncoordinated manner.
Reference: 48. <author> J. D. Ullman, </author> <title> "Complexity of sequencing problems". In Computer and Job-Shop Scheduling Theory, </title> <editor> E. G. Coffman, Jr. (ed.), chap. </editor> <volume> 4, </volume> <publisher> John Wiley & Sons, </publisher> <year> 1976. </year>
Reference-contexts: Many restricted cases of DAG scheduling have been shown to be NP-complete, meaning that it is impractical to look for an optimal schedule. This is true even for very restricted special cases, e.g. when communication is free and all tasks have unit execution times <ref> [48, 18] </ref>. Many parallel programming styles are close to the DAG model, although with substantially less information. Dataflow, functional programming, and other side-effect free programming models can be viewed as DAGs. Consequently, various heuristics have been developed to approximate good schedules.
Reference: 49. <author> K. Y. Wang and D. C. Marinescu, </author> <title> "Correlation of the paging activity of individual node programs in the SPMD execution model". </title> <booktitle> In 28th Hawaii Intl. Conf. System Sciences, </booktitle> <volume> vol. I, </volume> <pages> pp. 61-71, </pages> <month> Jan </month> <year> 1995. </year>
Reference-contexts: The simple solution adopted by some recent systems, such as the IBM SP2 and Meiko CS-2, is to have independent paging on each node. This has the undesirable side effect that processes can be blocked asynchronously for relatively long periods, preventing fine-grain synchronization and communication from taking place <ref> [49] </ref>. To quote a somewhat overused phrase, "there must be a better way". The main thing to remember is that scheduling is not an isolated issue. Scheduling is but one service provided by the operating system.
Reference: 50. <author> K. K. Yue and D. J. Lilja, </author> <title> "Loop-level process control: an effective processor allocation policy for multiprogrammed shared-memory multiprocessors". In Job Scheduling Strategies for Parallel Processing, </title> <editor> D. G. Feitelson and L. Rudolph (eds.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <month> 949. </month> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: A possible solution is to only change the processor allocation at certain points in the program, e.g. at the beginning of a new parallel loop <ref> [50] </ref>.
References-found: 50

