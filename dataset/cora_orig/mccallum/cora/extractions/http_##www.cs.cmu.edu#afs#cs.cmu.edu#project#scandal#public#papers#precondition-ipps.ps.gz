URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/precondition-ipps.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/precondition-ipps.html
Root-URL: 
Title: Abstract  
Abstract: The linear systems associated with large, sparse, symmetric, positive definite matrices are often solved iteratively using the preconditioned conjugate gradient method. We have developed a new class of preconditioners, support tree preconditioners, that are based on the connectivity of the graphs corresponding to the matrices and are well-structured for parallel implementation. In this paper, we evaluate the performance of support tree precondi-tioners by comparing them against two common types of precon-ditioners: diagonal scaling, and incomplete Cholesky. Support tree preconditioners require less overall storage and less work per iteration than incomplete Cholesky preconditioners. In terms of total execution time, support tree preconditioners outperform both diagonal scaling and incomplete Cholesky preconditioners. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Arioli, I. Duff, D. Ruiz, </author> <title> Stopping criteria for iterative solvers. </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 13(1) </volume> <pages> 138-144, </pages> <year> 1992. </year>
Reference-contexts: For our initial experiments, we used the same forcing function as Greenbaum et al.: We used as our stopping criterion the condition reported to be superior by Arioli et al. <ref> [1] </ref>: (3) We halted when w 2 1.0 x 10 -10 . The starting vector was the zero vector. The results are reported in Figure 5.
Reference: [2] <author> G. E. Blelloch, M. A. Heroux, and M. Zagha, </author> <title> Segmented operations for sparse matrix computation on vector multiprocessors. </title> <institution> CMU-CS-93-173, School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: Thus, the bulk of the computation in STCG can be implemented with a single general-purpose sparse matrix multiplication subroutine. On the Cray C-90, we use an algorithm called SEGMV, which accomodates arbitrary row sizes using segmented scan operations <ref> [2] </ref>. Compared to other methods (such as Ell-pack/Itpack and Jagged Diagonal), SEGMV performance is comparable for structured matrices, and superior for most irregular matrices. Thus our STCG implementation per forms well on both regular and irregular meshes.
Reference: [3] <author> J. J. Dongarra, I. S. Duff, D. C. Sorensen, and H. A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: We made no attempt to go beyond the obvious optimizations of ICCG. Numerous other authors have reported on optimizations of ICCG (see, for example <ref> [3] </ref>, [4], [11]). We studied their results and concluded that a general, optimal implementation of ICCG would require twice the time per iteration of DSCG. Accordingly, we report these extrapolated optimal values as ICCG-OPT.
Reference: [4] <author> I. S. Duff, and G. A. Meurant, </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT 29 </journal> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: We made no attempt to go beyond the obvious optimizations of ICCG. Numerous other authors have reported on optimizations of ICCG (see, for example [3], <ref> [4] </ref>, [11]). We studied their results and concluded that a general, optimal implementation of ICCG would require twice the time per iteration of DSCG. Accordingly, we report these extrapolated optimal values as ICCG-OPT.
Reference: [5] <author> A. Greenbaum, C. Li, and H. Z. Chao, </author> <title> Comparison of linear system solvers applied to diffusion-type finite element equations. </title> <journal> Numer. Math. </journal> <volume> 56 </volume> <pages> 529-546, </pages> <year> 1989. </year>
Reference: [6] <author> K. D. Gremban, and G. L. Miller, </author> <title> Towards the Application of Graph Theory to Finding Parallel Precondi-tioners for Sparse Symmetric Linear Systems. </title> <type> Technical Report, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <note> in preparation. </note>
Reference-contexts: Let H be the support tree for G. Let B be the Laplacian matrix corresponding to H. We would like to use B as a pre-conditioner for A, but B is of order 2n-1, and A is of order n. In another paper <ref> [6] </ref>, we describe the theory proving that B can be used as a preconditioner for A. Here, we present an overview of the theory in order to gain some intuition. Suppose that H is a binary tree with n leaves and n-1 internal nodes. <p> In <ref> [6] </ref>, we show the following: K is an effective preconditioner for A; If , then we also have .
Reference: [7] <author> K. D. Gremban, G. L. Miller, and M. Zagha, </author> <title> Performance Evaluation of a New Parallel Preconditioner. </title> <institution> CMU-CS-94-205, School of Computer Science, Car-negie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: At present, support tree preconditioners can only be applied to Laplacian matrices. One of our goals is to extend the support tree methodology to larger classes of matrices. An expanded version of this paper is available as a technical report <ref> [7] </ref>. a) crack00 with 10 nodes. b) crack08 with 2560 nodes b) a) a) number of iterations for convergence. b) total time for iterative process on a Cray C-90. a) 6 Acknowledgments The authors would like to thank Guy Blelloch, Omar Ghat-tas, and Mike Heroux for many useful conversations.
Reference: [8] <author> X. -Z. Guo, </author> <title> Multilevel Preconditioners: Analysis, performance enhancements, and parallel algorithms. </title> <institution> CS-TR-2903, Department of Mathematics, University of Maryland, </institution> <year> 1992. </year>
Reference: [9] <author> M. A. Heroux, P. Vu, and C. Yang, </author> <title> A parallel preconditioned conjugate gradient package for solving sparse linear systems on a Cray Y-MP. </title> <journal> Appl. Num. Math. </journal> <volume> 8 </volume> <pages> 93-115, </pages> <year> 1991. </year>
Reference: [10] <author> J. A. Meijerink, and H. A. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math. Comp. </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: The best performance is achieved by multilevel precondi-tioners; some can achieve nearly optimal convergence rates and can be effectively parallelized [5][8]. However, they require a priori knowledge about the differential equation or the discretization process, which is often unavailable. Diagonal scaling and incomplete Cholesky <ref> [10] </ref> precondi-tioners depend only on the coefficient matrix; we term these a posteriori preconditioners, since they can be constructed after the linear system is formulated. A posteriori precondi-tioners are the most general. Diagonal scaling can be effectively parallelized, but yields little improvement in the convergence rate. <p> Let A be a Laplacian matrix of order n, and let G = G (A) be the corresponding graph. Let S 0 be an edge separator of G; that partitions G into two disconnected subgraphs G 0 and 1. is an M-matrix if for , is nonsingular, and <ref> [10] </ref> A A ij 0 i j A A a ij i j Performance Evaluation of a New Parallel Preconditioner School of Computer Science Carnegie Mellon University 5000 Forbes Avenue Pittsburgh PA 15213 This research was sponsored in part by the Avionics Laboratory, Wright Research and Development Center, Aeronautical Systems Division
Reference: [11] <author> H. A. van der Vorst, </author> <title> ICCG and related methods for 3D problems on vector computers. </title> <journal> Comp. Physics Comm. </journal> <volume> 53 </volume> <pages> 223-235, </pages> <year> 1989. </year>
Reference-contexts: We made no attempt to go beyond the obvious optimizations of ICCG. Numerous other authors have reported on optimizations of ICCG (see, for example [3], [4], <ref> [11] </ref>). We studied their results and concluded that a general, optimal implementation of ICCG would require twice the time per iteration of DSCG. Accordingly, we report these extrapolated optimal values as ICCG-OPT.
References-found: 11

