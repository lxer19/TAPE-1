URL: http://www.cs.rpi.edu/tr/92-18.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Phone: 950  
Title: Finding Optimum Wavefront of Parallel Computation  
Author: Balaram Sinharoy Boleslaw Szymanski 
Keyword: parallel programming, data dependence, algorithm transformation, uniform de pendence algorithm, compile-time scheduling, hyperplane. Computer Review Categories: D.1.3. parallel programming, D.3.4. compilers, D.4.1. scheduling.  
Address: P.O. Box  Poughkeepsie, NY 12602 Troy, NY 12180-3590  
Affiliation: Enterprise Systems Division Department of Computer Science IBM Corporation,  Rensselaer Polytechnic Institute  
Note: appeared in Journal of Parallel Algorithms and Applications, vol. 2, no. 1, 1994, pp. 5-26  
Abstract: Data parallelism, in which the same operation is performed on many elements of an n-dimensional array, is one of the most powerful methods of extracting parallelism in scientific computation. One form of data parallelism involves defining a sequence of parallel wavefronts of a computation. Each wavefront consists of an (n 1)-dimensional subarray of the evaluated array and all wavefront elements are evaluated simultaneously. Different wavefronts result in different performance, so the question arises how to determine the wavefronts that result in the minimum computation time. Wavefront determination should define also allocation of wavefront elements to processors. In this paper we present efficient algorithms for determining the optimum wavefront and for partitioning it into sections assigned to individual processors. Presented algorithms are applicable to computations that are defined over two or higher dimensional arrays and are executed on distributed memory machines interconnected into a one or two-dimensional processor array. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Most methods described in the literature for mapping a computation structure onto an array of processors (or a VLSI array) reduce the problem to an instance of the integer programming optimization. However, the integer programming optimization problem is NP-hard <ref> [1] </ref>. In this paper, we assume the spacetime representation of an algorithm to be a continuous domain and determine the time hyperplane h, termed wavefront, that results in the minimum execution time. <p> beyond the array boundary are considered to be zero) 8 (x 1 ; x 2 ) 2 X : E [x 1 ; x 2 ] = f (E [x 1 2; x 2 + 2]; E [x 1 4; x 2 2]) (2) where the index domain X is <ref> [1; n] </ref> fi [1; n] 1 . There are two dependence vectors in this computation, OA ([4,2]) and OB ([2,-2]). Since all dependence vectors are on one side of the lines EH, E 0 H 0 and E 00 H 00 , all of them are potential wavefronts. <p> boundary are considered to be zero) 8 (x 1 ; x 2 ) 2 X : E [x 1 ; x 2 ] = f (E [x 1 2; x 2 + 2]; E [x 1 4; x 2 2]) (2) where the index domain X is <ref> [1; n] </ref> fi [1; n] 1 . There are two dependence vectors in this computation, OA ([4,2]) and OB ([2,-2]). Since all dependence vectors are on one side of the lines EH, E 0 H 0 and E 00 H 00 , all of them are potential wavefronts.
Reference: [2] <author> L. Lamport. </author> <title> "The parallel execution of do loops", </title> <journal> CACM, </journal> <volume> 17, </volume> <year> 1974. </year>
Reference-contexts: Variables can be evaluated simultaneously at i 1 and i 2 , if and only if they are independent. If the data dependence vectors in an algorithm are all constants, then an n-deep loop nest has at least n 1 degrees of parallelism. In the classic paper <ref> [2] </ref>, Lamport proposed two methods, the hyperplane and the coordinate method, for finding the set of index points at which evaluation can proceed simultaneously. In the hyperplane method, Lamport considered partitioning the set of index points into a set of parallel hyperplanes.
Reference: [3] <author> P.-Z. Lee and Z. M. Kedem, </author> <title> "Synthesizing Linear Array Algorithms from Nested For Loop Algorithms", </title> <journal> IEEE trans. on Computers, </journal> <volume> Vol. 37, No. 12, </volume> <month> December </month> <year> 1988. </year>
Reference-contexts: In a program written in a single assignment language [9] (or a conventional language, where each variable occurrence is labeled with the appropriate indexes of the surrounding loops <ref> [3] </ref>), a data dependence vector can be viewed as the difference of indexes where a variable is used and where that variable is generated. The set of index points along with the set of dependence vectors define a computational structure [11]. <p> Lamport's original results have been extended by many researchers over the years <ref> [7, 11, 3, 4] </ref>. Moldovan et al [7] considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array. <p> To apply the analysis to an algorithm specified using a procedural language, appropriate indexes of the loops surrounding each variable occurrence in the algorithm are needed (a process labeling variables with such indexes is described in <ref> [3] </ref>). Wavefront scheduling is appropriate for Single Program Multiple Data (SPMD) [10] implementation on distributed memory architecture or for data parallelism on SIMD architectures. <p> The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see [12, 5]). Many methods described in the literature <ref> [11, 3, 4, 7] </ref> determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [4] <author> P.-Z. Lee and Z. M. Kedem, </author> <title> "Mapping Nested Loop Algorithms into Multidimensional Systolic Arrays," </title> <journal> in IEEE Transactions on Parallel and Distributed Processing, </journal> <volume> vol 1, no 1, </volume> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: Lamport's original results have been extended by many researchers over the years <ref> [7, 11, 3, 4] </ref>. Moldovan et al [7] considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array. <p> Again for correct execution ordering, we should have h d &gt; 0 8 d 2 D (1) and for the minimum execution time, mapping h should minimize t = max h ( i 1 i 2 ) + 1 ' Lee et al <ref> [4] </ref>, stated the necessary and sufficient conditions for such a transformation T to correctly map a p-nested loop algorithm onto a q-dimensional systolic array, where 1 q p 1. They have also described exhaustive and heuristic search methods to determine, respectively, the optimal and suboptimal mappings h and S. <p> The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see [12, 5]). Many methods described in the literature <ref> [11, 3, 4, 7] </ref> determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [5] <author> J. Li and M. Chen, </author> <title> Index Domain Alignment: Minimizing Cost of Cross-Referencing Between Distributed Arrays, </title> <type> Tech. Rep., </type> <institution> Dept. of Comp. Sc., Yale, Univ., YALEU/DCS/TR-725, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see <ref> [12, 5] </ref>). Many methods described in the literature [11, 3, 4, 7] determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [6] <author> W. L. Miranker and A. Winkler, </author> <title> "Spacetime Representations of Computational Structures," </title> <booktitle> in Computing Vol 32, </booktitle> <volume> No 2, </volume> <pages> pp. 93-114, </pages> <year> 1984. </year>
Reference-contexts: Algorithms that employ such iterations are referred to here as iterative algorithms. In the spacetime representation of a computation described by an iterative algorithm, the values of different variables are to be evaluated at different index points <ref> [6] </ref>. For correct execution of the algorithm, the evaluation order must be in accordance with the various dependence relationships among the index points. In most scientific computations, variables can be evaluated at multiple index points simultaneously, resulting in a very high speedup of the computation.
Reference: [7] <author> D. I. Moldovan, </author> <title> "Partitioning and Mapping Algorithms into Fixed Size Systolic Arrays," </title> <journal> in IEEE Transactions on Computers, </journal> <volume> Vol C-35, No 1, </volume> <pages> pp. 1-12, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Lamport's original results have been extended by many researchers over the years <ref> [7, 11, 3, 4] </ref>. Moldovan et al [7] considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array. <p> Lamport's original results have been extended by many researchers over the years [7, 11, 3, 4]. Moldovan et al <ref> [7] </ref> considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array. <p> The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see [12, 5]). Many methods described in the literature <ref> [11, 3, 4, 7] </ref> determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [8] <author> C. D. Polychronopoulos, D. J. Kuck and D. A. Padua, </author> <title> "Utilizing Multidimensional loop Parallelism on Large-Scale Parallel Processor Systems," </title> <journal> in IEEE Transactions on Computers, </journal> <volume> Vol 38, No 9, </volume> <pages> pp. 1285-1296, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Given a nested loop, algorithms for partitioning the index domain among a number of processors to minimize the total computation time involve solving an integer optimization problem that is NP-hard <ref> [8, 14] </ref>. In this paper, we discuss polynomial-time algorithms for such partitioning under the simplifying assumption about positions of the index points versus wavefronts. The paper is organized as follows.
Reference: [9] <author> S. K. Rao, </author> <title> Regular Iterative Algorithms and their Implementations on a Processor Arrays, </title> <type> Ph. D. Thesis, </type> <institution> Dept. of Electrical Engineering, Stanford University, California, </institution> <year> 1985. </year> <month> 20 </month>
Reference-contexts: In most scientific computations, variables can be evaluated at multiple index points simultaneously, resulting in a very high speedup of the computation. In a program written in a single assignment language <ref> [9] </ref> (or a conventional language, where each variable occurrence is labeled with the appropriate indexes of the surrounding loops [3]), a data dependence vector can be viewed as the difference of indexes where a variable is used and where that variable is generated.
Reference: [10] <author> M. Rosing, R. B. Schnabel and R. P. Weaver, </author> <title> Scientific Programming Languages for Dis--tributed Memory Multiprocessors: Paradigms and Research Issues, in Languages, Compilers and Run-Time Environments for Distributed Memory Machines by J. </title> <editor> Saltz and P. Mehrotra (eds), </editor> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: To apply the analysis to an algorithm specified using a procedural language, appropriate indexes of the loops surrounding each variable occurrence in the algorithm are needed (a process labeling variables with such indexes is described in [3]). Wavefront scheduling is appropriate for Single Program Multiple Data (SPMD) <ref> [10] </ref> implementation on distributed memory architecture or for data parallelism on SIMD architectures. SPMD implementation, in general, requires larger parallel granules than SIMD implementation, therefore it is more efficient provided that the computations at each index point are fairly complex (i.e., involve computationally intensive function evaluation). an algorithm. <p> In this model, off-processor values required to compute a designated block of parallel code are obtained immediately before the beginning of the block, and all off-processor values generated within the block are communicated immediately after the end of the block <ref> [10] </ref>. Typically, packets of values are formed for communication and transferred between non-neighboring processors by means of hopping. Under these assumptions, the minimum width of the wavefront partition assigned to a processor can be less than max (z l ; z r ).
Reference: [11] <author> J.-P. Sheu and T.-H. Tai, </author> <title> "Partitioning and Mapping nested Loops on Multiprocessor Systems," </title> <journal> in IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 2, No. 4, </volume> <pages> pp. 430-439, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The set of index points along with the set of dependence vectors define a computational structure <ref> [11] </ref>. <p> Lamport's original results have been extended by many researchers over the years <ref> [7, 11, 3, 4] </ref>. Moldovan et al [7] considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array. <p> They have also described exhaustive and heuristic search methods to determine, respectively, the optimal and suboptimal mappings h and S. Sheu et al <ref> [11] </ref> describe an algorithm to partition the index points found on a time hyperplane onto the processors in a message passing system. Their method first projects all index points and dependence vectors onto hyperplane hx = 0. <p> The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see [12, 5]). Many methods described in the literature <ref> [11, 3, 4, 7] </ref> determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [12] <author> B. Sinharoy and B. K. Szymanski, </author> <title> "Complexity Issues of the Alignment Problem and the Closest Vectors in a Lattice", </title> <type> Technical Report 91-10, </type> <institution> Department of Computer Science, Rens-selaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <month> May </month> <year> 1991. </year> <note> Submitted to IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: The methods described here can be applied to any set of uncoupled recurrence equations. To decrease the communication cost a good alignment of all arrays in the program should be determined first (see <ref> [12, 5] </ref>). Many methods described in the literature [11, 3, 4, 7] determine the actual mapping of the computation onto the processors, once the wavefront is determined by solving an integer programming optimization problem (see section 1). These algorithms can be used for the wavefronts obtained by our method.
Reference: [13] <editor> Szymanski, B.K. (edt): </editor> <booktitle> Parallel Functional Languages and Environments ACM Press: </booktitle> <address> New York, NY, </address> <year> 1991 </year>
Reference-contexts: = min fmax fj h ( i 1 i 2 )j j i 1 ; i 2 2 Igg where I is the index domain, i.e. the set of all index points in the computation (the index domain is usually defined by the loop control variables or by separate statements <ref> [13] </ref>). Lamport's original results have been extended by many researchers over the years [7, 11, 3, 4]. Moldovan et al [7] considered a linear transformation, T, of the algorithm to map it efficiently on a VLSI processor array.
Reference: [14] <author> C.-M. Wang and S.D. Wang, </author> <title> "Efficient Processor Assignment Algorithms and Loop Transformations for Executing Nested Parallel Loops on Multiprocessors," </title> <journal> in IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol 3, No 1, </volume> <pages> pp. 71-82, </pages> <month> January </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: Given a nested loop, algorithms for partitioning the index domain among a number of processors to minimize the total computation time involve solving an integer optimization problem that is NP-hard <ref> [8, 14] </ref>. In this paper, we discuss polynomial-time algorithms for such partitioning under the simplifying assumption about positions of the index points versus wavefronts. The paper is organized as follows.
References-found: 14

