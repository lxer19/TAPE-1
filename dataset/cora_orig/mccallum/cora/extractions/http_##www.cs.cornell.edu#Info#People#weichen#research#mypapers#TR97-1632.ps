URL: http://www.cs.cornell.edu/Info/People/weichen/research/mypapers/TR97-1632.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/weichen/research.htm
Root-URL: 
Email: aguilera,weichen,sam@cs.cornell.edu  
Title: Quiescent Reliable Communication and Quiescent Consensus in Partitionable Networks  
Author: Marcos Kawazoe Aguilera Wei Chen Sam Toueg 
Date: June 8, 1997  
Address: Upson Hall,  Ithaca, NY 14853-7501, USA.  
Affiliation: Department of Computer Science  Cornell University  
Abstract: We consider partitionable networks with process crashes and lossy links, and focus on the problems of reliable communication and consensus for such networks. For both problems we seek algorithms that are quiescent, i.e., algorithms that eventually stop sending messages. We first tackle the problem of reliable communication for partitionable networks by extending the results of [ACT97a]. In particular, we generalize the specification of the heartbeat failure detector HB, show how to implement it, and show how to use it to achieve quiescent reliable communication. We then turn our attention to the problem of consensus for partitionable networks. We first show that, even though this problem can be solved using a natural extension of 3S, such solutions are not quiescent in other words, 3S alone is not sufficient to achieve quiescent consensus in partitionable networks. We then solve this problem using 3S and the quiescent reliable communication primitives that we developed in the first part of the paper. Our model of failure detectors for partitionable networks, a natural extension of the model in [CT96], is also a contribution of this paper. 
Abstract-found: 1
Intro-found: 1
Reference: [ACT97a] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. Heartbeat: </author> <title> a timeout-free failure detector for quiescent reliable communication. </title> <type> Technical Report 97-1631, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: For both problems we seek algorithms that are quiescent, i.e., algorithms that eventually stop sending messages. This paper consists of two parts. In the first part, we show how to achieve quiescent reliable communication over partitionable networks by extending the results for non-partitionable networks described in <ref> [ACT97a] </ref>. In the second part, we show how to achieve quiescent consensus for partitionable networks by using the results of the first part. <p> None of the partitions are isolated. For example, processes in D may receive messages from processes in C and are able to send messages to processes in B. There is no fair path from C to A, or from D to C, etc. Quiescent Reliable Communication <ref> [ACT97a] </ref> shows that without the help of failure detectors it is impossible to achieve quiescent reliable communication in the presence of process crashes and lossy links even if one assumes that the network never partitions. In order to overcome this problem, [ACT97a] introduces the heartbeat failure detector (denoted HB), and shows <p> Quiescent Reliable Communication <ref> [ACT97a] </ref> shows that without the help of failure detectors it is impossible to achieve quiescent reliable communication in the presence of process crashes and lossy links even if one assumes that the network never partitions. In order to overcome this problem, [ACT97a] introduces the heartbeat failure detector (denoted HB), and shows how it can be implemented, and how it can be used to achieve quiescent reliable communication. All these results are for networks that do not partition. In this paper, we extend the above results to partitionable networks. <p> The system can experience both process failures and link failures. Processes can fail by crashing, and links can fail by crashing, or by intermittently dropping messages (while remaining fair). Failures may cause permanent network partitions. The detailed model, based on those in <ref> [CHT96b, ACT97a] </ref>, is described next. A network is a directed graph G = (P; L) where P = f1; : : : ; ng is the set of processes, and L P fi P is the set of links. <p> If p ! q 62 crashed (F L ), we say that p ! q is fair in F L . A failure pattern F = (F P ; F L ) combines a process failure pattern and a link failure pattern. 2.2 Connectivity In contrast to <ref> [ACT97a] </ref>, the network is partitionable: there may be two correct processes p and q such that q is not reachable from p (Fig. 1). Intuitively, a partition is a maximal set of processes that are mutually reachable from each other. <p> Finite Receipt implies that if a link crashes then it eventually stops transporting messages. 3 The Heartbeat Failure Detector HB for Partitionable Networks One of our goals is to achieve quiescent reliable communication in partitionable networks with process crashes and message losses. In <ref> [ACT97a] </ref> it is shown that without failure detectors this is impossible, even if one assumes that the network does not partition. In order to circumvent this impossibility result, [ACT97a] introduces the heartbeat failure detector, denoted HB, for non-partitionable networks. <p> In <ref> [ACT97a] </ref> it is shown that without failure detectors this is impossible, even if one assumes that the network does not partition. In order to circumvent this impossibility result, [ACT97a] introduces the heartbeat failure detector, denoted HB, for non-partitionable networks. In this section, we generalize the definition of HB to partitionable networks. We then show how to implement it in Section 6. <p> This stronger property is not necessary in this paper. 4 In <ref> [ACT97a] </ref>, the output of D at p is an array with one nonnegative integer for each neighbor of p. 6 * HB-Accuracy: At each process p, the heartbeat sequence of every process is nondecreasing. <p> We first define reliable versions of these primitives, and then give quiescent implementations that use HB, for partitionable networks. Our definitions generalize those for non-partitionable networks given in <ref> [ACT97a] </ref>. 4.1 Quasi Reliable Send and Receive for Partitionable Networks Consider any two distinct processes s and r. We define quasi reliable send and receive from s to r (for partitionable networks) in terms of two primitives: qr-send s;r and qr-receive r;s . <p> The implementation of reliable broadcast is identical to the one given in <ref> [ACT97a] </ref> for non-partitionable networks. However, the network assumptions, the reliable broadcast requirements, and the failure detector properties are different, and so its proof of correctness and quiescence changes. <p> Moreover, if the largest partition contains a majority of processes, then it also satisfies Termination. 6 Implementation of HB for Partitionable Networks We now show how to implement HB for partitionable networks. Our implementation (Fig. 4) is a minor modification of the one given in <ref> [ACT97a] </ref> for non-partitionable networks. Every process p executes two concurrent tasks. In the first task, p periodically increments its own heartbeat value, and sends the message (HEARTBEAT; p) to all its neighbors. The second task handles the receipt of messages of the form (HEARTBEAT; path). <p> Therefore p and q are in the same partition a contradiction. 2 By Corollary 27 and the above lemma, we have: Theorem 31 Figure 4 implements HB for partitionable networks. 7 Related Work Regarding reliable communication, the works that are closest to ours are <ref> [BCBT96, ACT97a] </ref>. Both of these works, however, consider only non-partitionable networks. <p> However, the communication algorithms that they give are not quiescent (and do not use failure detectors). <ref> [ACT97a] </ref> was the first paper to study the problem of achieving quiescent reliable communication by using failure detectors in a system with process crashes and lossy links. Regarding consensus, the works that are closest to ours are [FKM + 95, CHT96a, DFKM96, GS96].
Reference: [ACT97b] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. </author> <title> On the weakest failure detector to achieve quiescence. </title> <type> Manuscript, </type> <month> April </month> <year> 1997. </year>
Reference-contexts: In contrast to existing failure detectors, HB is implementable without the use of timeouts (see Section 6). Moreover, as explained below, HB outputs a vector of counters rather than a list of suspected processes. In <ref> [ACT97b] </ref> we show that this is a fundamental difference. A heartbeat failure detector D (for partitionable networks) has the following features.
Reference: [BCBT96] <author> Anindya Basu, Bernadette Charron-Bost, and Sam Toueg. </author> <title> Simulating reliable links with unreliable links in the presence of process crashes. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 105-122. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Therefore p and q are in the same partition a contradiction. 2 By Corollary 27 and the above lemma, we have: Theorem 31 Figure 4 implements HB for partitionable networks. 7 Related Work Regarding reliable communication, the works that are closest to ours are <ref> [BCBT96, ACT97a] </ref>. Both of these works, however, consider only non-partitionable networks. <p> Both of these works, however, consider only non-partitionable networks. In <ref> [BCBT96] </ref>, Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted [BN92, Gop92] 12
Reference: [BDM97] <author> Ozalp Babao glu, Renzo Davoli, and Alberto Montresor. </author> <title> Partitionable group membership: specification and algorithms. </title> <type> Technical Report UBLCS-97-1, </type> <institution> Dept. of Computer Science, University of Bologna, Bologna, Italy, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in <ref> [BDM97] </ref>. <p> The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in [BDM97]. We compare models in the next section. 8 Comparison with other Models In <ref> [DFKM96, BDM97] </ref>, network connectivity is defined in terms of the messages exchanged in a run in particular, it depends on whether the algorithm being executed sends a message or not, on the times these messages are sent, and on whether these messages are received. <p> Clearly, network connectivity depends on the messages of the run. In <ref> [BDM97] </ref>, process q is partitioned from p at time t if the last message that p sent to q by time t 0 t is never received by q. This particular way of defining network connectivity in terms of messages is problematic for our purposes, as the following example shows. <p> Suppose that at time t, p sends m to q, and this message is lost (it is never received by q). By the definition in <ref> [BDM97] </ref>, q is partitioned from p at time t. Suppose that the failure detector module at p now tells p (correctly) that q is partitioned from p. At this point, p stops sending messages to q until the failure detector says that q has become reachable again. <p> The proof of correctness of an algorithm (such as the one in the simple example above) should refer only to the abstract properties of the failure detector that it uses, and not to any aspects of its implementation. As a final remark, the model of <ref> [BDM97] </ref> is not suitable for our results because of the following. Consider a completely connected network in which all links are bidirectional and fair. Let R be any run in which every link p ! q loses messages from time to time (but every message repeatedly sent is eventually received). <p> Consider a completely connected network in which all links are bidirectional and fair. Let R be any run in which every link p ! q loses messages from time to time (but every message repeatedly sent is eventually received). In run R, by the definitions in <ref> [BDM97] </ref>: (a) neither q remains partitioned from p, nor q remains reachable from p, and (b) an Eventually Perfect failure detector 3P is allowed to behave arbitrarily. Therefore, with the definitions in [BDM97], 3P cannot be used to solve consensus in such a network. <p> In run R, by the definitions in <ref> [BDM97] </ref>: (a) neither q remains partitioned from p, nor q remains reachable from p, and (b) an Eventually Perfect failure detector 3P is allowed to behave arbitrarily. Therefore, with the definitions in [BDM97], 3P cannot be used to solve consensus in such a network. Our model was designed to deal with fair links explicitly 14 , and consensus can be solved even with 3S.
Reference: [BN92] <author> Rida Bazzi and Gil Neiger. </author> <title> Simulating crash failures with many faulty processors. </title> <booktitle> In Proceedings of the 6th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 166-184. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In [BCBT96], Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted <ref> [BN92, Gop92] </ref> 12 or if more than half of the processes do not crash.
Reference: [Cha97] <author> Tushar Deepak Chandra, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title>
Reference-contexts: Our heartbeat failure detector HB is different from the ones defined in [CT96], or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations <ref> [vR97, Cha97] </ref>). In contrast to existing failure detectors, HB is implementable without the use of timeouts (see Section 6). Moreover, as explained below, HB outputs a vector of counters rather than a list of suspected processes. In [ACT97b] we show that this is a fundamental difference. <p> Thus, HB should not be confused with existing implementations of failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>). Even though existing failure detectors are also based on the repeated sending of a heartbeat, they use timeouts on heartbeats in order to derive lists of processes considered to be up or down; applications can only see these lists.
Reference: [CHT96a] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg, </author> <month> March </month> <year> 1996. </year> <title> Private Communication to the authors of [FKM + 95]. </title>
Reference-contexts: The first paper to consider the consensus problem for partitionable networks is [FKM + 95]. Algorithms for this problem are given in <ref> [CHT96a, DFKM96] </ref>. <p> of eventual failure detectors introduced in [CT96] can be generalized in a similar way. 13 5.3 Quiescent Consensus for Partitionable Networks Cannot be Achieved using 3S Although consensus for partitionable networks can be solved using 3S, we now show that any such solution is not quiescent (the consensus algorithms in <ref> [CHT96a, DFKM96] </ref> do not contradict this result because they are not quiescent). Theorem 14 In partitionable networks with 5 or more processes, consensus has no quiescent implementation using 3S. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. <p> However, their model assumes that the network is completely connected and links between correct processes do not lose messages thus, no permanent partition is possible. The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors <ref> [CHT96a] </ref>. Correct algorithms can be found in [CHT96a, DFKM96]. 13 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors [CHT96a]. Correct algorithms can be found in <ref> [CHT96a, DFKM96] </ref>. 13 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> Second, it raises the following issue: are the messages defining network connectivity, those of the applications, those of the failure detection mechanism, or both? In our model, network connectivity does not depend on messages sent by the algorithm, and so we avoid 13 Actually, the specification of consensus considered in <ref> [FKM + 95, CHT96a] </ref> only requires that one correct process in the largest partition eventually decides. Ensuring that all correct processes in the largest partition decide can be subsequently achieved by a (quiescent) reliable broadcast of the decision value. 22 the above drawbacks.
Reference: [CHT96b] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <journal> Journal of the ACM, </journal> <volume> 43(4) </volume> <pages> 685-722, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: In order to do so, we first generalize the traditional definition of consensus to partitionable networks. We also generalize the definition of 3S the weakest failure detector for solving consensus in networks that do not partition <ref> [CHT96b] </ref>. We first show that, although 3S can be used to solve consensus for partitionable networks, any such solution is not quiescent: Thus, 3S alone is not sufficient to solve quiescent consensus for partitionable networks. We then show that this problem can be solved using 3S together with HB. <p> The system can experience both process failures and link failures. Processes can fail by crashing, and links can fail by crashing, or by intermittently dropping messages (while remaining fair). Failures may cause permanent network partitions. The detailed model, based on those in <ref> [CHT96b, ACT97a] </ref>, is described next. A network is a directed graph G = (P; L) where P = f1; : : : ; ng is the set of processes, and L P fi P is the set of links. <p> To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in [CT96]. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks <ref> [CHT96b] </ref>), and extend it to partitionable networks. 8 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes that p suspects to have crashed. In our case, these are the processes that p suspects to be outside its partition.
Reference: [CT96] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 225-267, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: We then show that this problem can be solved using 3S together with HB. In fact, our quiescent consensus algorithm for partitionable networks is identical to the one given in <ref> [CT96] </ref> for non-partitionable networks with reliable links: we simply replace the communication primitives used by the algorithm in [CT96] with the quiescent reliable communication primitives that we derive in the first part of this paper (the proof of correctness, however, is different). <p> We then show that this problem can be solved using 3S together with HB. In fact, our quiescent consensus algorithm for partitionable networks is identical to the one given in <ref> [CT96] </ref> for non-partitionable networks with reliable links: we simply replace the communication primitives used by the algorithm in [CT96] with the quiescent reliable communication primitives that we derive in the first part of this paper (the proof of correctness, however, is different). The first paper to consider the consensus problem for partitionable networks is [FKM + 95]. Algorithms for this problem are given in [CHT96a, DFKM96]. <p> In this section, we generalize the definition of HB to partitionable networks. We then show how to implement it in Section 6. Our heartbeat failure detector HB is different from the ones defined in <ref> [CT96] </ref>, or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations [vR97, Cha97]). In contrast to existing failure detectors, HB is implementable without the use of timeouts (see Section 6). <p> To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in <ref> [CT96] </ref>. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks [CHT96b]), and extend it to partitionable networks. 8 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes <p> unreliable failure detectors in <ref> [CT96] </ref>. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks [CHT96b]), and extend it to partitionable networks. 8 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes that p suspects to have crashed. In our case, these are the processes that p suspects to be outside its partition. <p> By a slight abuse of notation, we sometimes use 3S and 3S LP to refer to an arbitrary member of the respective class. 8 The other classes of eventual failure detectors introduced in <ref> [CT96] </ref> can be generalized in a similar way. 13 5.3 Quiescent Consensus for Partitionable Networks Cannot be Achieved using 3S Although consensus for partitionable networks can be solved using 3S, we now show that any such solution is not quiescent (the consensus algorithms in [CHT96a, DFKM96] do not contradict this result <p> Note that the behavior of the failure detector in each of the above three runs is compatible with 3S. 2 5.4 Quiescent Consensus for Partitionable Networks using 3S LP and HB To solve consensus using 3S LP and HB in partitionable networks, we take the rotating coordinator consensus algorithm of <ref> [CT96] </ref>, we replace its communication primitives with the corresponding ones defined in Sections 4.1 and 4.2, namely, qr-send, qr-receive, broadcast and deliver, and then we plug in the quiescent implementations of these primitives given in Section 4.3 (these implementations use HB). <p> Although this algorithm is almost identical to the one given in <ref> [CT96] </ref> for non-partitionable networks, the network assumptions, the consensus requirements, and the failure detector properties are different, and so its proof of correctness and quiescence changes. The rotating coordinator algorithm is shown in Fig. 3 (the code consisting of lines 39-41 is executed atomically). Processes proceed in asynchronous rounds. <p> Consequently, c reliably broadcasts a request to decide estimate c . At any time, if a process delivers such a request, it decides accordingly. We next prove that the algorithm is correct and quiescent. Our proof is similar to the one in <ref> [CT96] </ref>, except for the proofs of Termination and Quiescence. The main difficulty in these proofs stems from the fact that we do not assume that partitions are eventually isolated: it is possible for processes in one partition to receive messages from outside this partition, forever. <p> If p is correct then the result follows from the Agreement property of reliable broadcast. 2 We omit the proof of the next lemma because it is almost identical to the one of Lemma 6.2.1 in <ref> [CT96] </ref>. Lemma 18 (Uniform Agreement) No two processes (whether in the same partition or not) decide differently. Lemma 19 Every process p invokes a finite number of broadcasts. Proof (Sketch). If p crashes, the result is obvious.
Reference: [DFKM96] <author> Danny Dolev, Roy Friedman, Idit Keidar, and Dahlia Malkhi. </author> <title> Failure detectors in omission failure environments. </title> <type> Technical Report TR96-1608, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: The first paper to consider the consensus problem for partitionable networks is [FKM + 95]. Algorithms for this problem are given in <ref> [CHT96a, DFKM96] </ref>. <p> A weaker class of failure detectors, denoted 3S LP , is obtained by defining the largest partition as in Section 5.1, and replacing For every partition P with For the largest partition P in the two properties above (this definition is similar to one given in <ref> [DFKM96] </ref>). <p> of eventual failure detectors introduced in [CT96] can be generalized in a similar way. 13 5.3 Quiescent Consensus for Partitionable Networks Cannot be Achieved using 3S Although consensus for partitionable networks can be solved using 3S, we now show that any such solution is not quiescent (the consensus algorithms in <ref> [CHT96a, DFKM96] </ref> do not contradict this result because they are not quiescent). Theorem 14 In partitionable networks with 5 or more processes, consensus has no quiescent implementation using 3S. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. <p> The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors [CHT96a]. Correct algorithms can be found in <ref> [CHT96a, DFKM96] </ref>. 13 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in [BDM97]. We compare models in the next section. 8 Comparison with other Models In <ref> [DFKM96, BDM97] </ref>, network connectivity is defined in terms of the messages exchanged in a run in particular, it depends on whether the algorithm being executed sends a message or not, on the times these messages are sent, and on whether these messages are received. <p> In particular, the link failure pattern is intended to model the physical condition of each link independent of the particular messages sent by the algorithm being executed. In <ref> [DFKM96] </ref>, two processes p and q are permanently connected in a given run if they do not crash and there is a time after which every message that p sends to q is received by q, and vice-versa. Clearly, network connectivity depends on the messages of the run.
Reference: [FKM + 95] <author> Roy Friedman, Idit Keidar, Dahlia Malkhi, Ken Birman, and Danny Dolev. </author> <title> Deciding in partitionable networks. </title> <type> Technical Report TR95-1554, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: The first paper to consider the consensus problem for partitionable networks is <ref> [FKM + 95] </ref>. Algorithms for this problem are given in [CHT96a, DFKM96]. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. <p> However, their model assumes that the network is completely connected and links between correct processes do not lose messages thus, no permanent partition is possible. The first paper to consider the consensus problem in partitionable networks is <ref> [FKM + 95] </ref>, but the algorithms described in that paper had errors [CHT96a]. <p> Second, it raises the following issue: are the messages defining network connectivity, those of the applications, those of the failure detection mechanism, or both? In our model, network connectivity does not depend on messages sent by the algorithm, and so we avoid 13 Actually, the specification of consensus considered in <ref> [FKM + 95, CHT96a] </ref> only requires that one correct process in the largest partition eventually decides. Ensuring that all correct processes in the largest partition decide can be subsequently achieved by a (quiescent) reliable broadcast of the decision value. 22 the above drawbacks.
Reference: [FLP85] <author> Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. </author> <title> Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: D (F ) denotes the set of possible failure detector histories permitted by D for the failure pattern F . 2.4 Algorithms and Runs An algorithm A is a collection of n deterministic automata, one for each process in the system. As in <ref> [FLP85] </ref>, computation proceeds in atomic steps of A. In each step, a process (1) attempts to receive a message from some process, (2) queries its failure detector module, (3) undergoes a state transition according to A, and (4) may send a message to a neighbor. <p> Roughly speaking, some processes propose a value and must decide on one of the proposed values <ref> [FLP85] </ref>. More precisely, consensus is defined in terms of two primitives, propose (v) and decide (v), where v is a value drawn from a set of possible proposed values. When a process invokes propose (v), we say that it proposes v. <p> of the correct processes actually propose a value (the others may not wish to run consensus). 5.2 3S for Partitionable Networks It is well known that consensus cannot be solved in asynchronous systems, even if at most one process may crash and the network is completely connected with reliable links <ref> [FLP85] </ref>. To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in [CT96].
Reference: [Gop92] <author> Ajei Gopal. </author> <title> Fault-Tolerant Broadcasts and Multicasts: The Problem of Inconsistency and Contamination. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In [BCBT96], Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted <ref> [BN92, Gop92] </ref> 12 or if more than half of the processes do not crash.
Reference: [GS96] <author> Rachid Guerraoui and Andr e Schiper. </author> <title> Gamma-Accurate failure detectors. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 269-286. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. <p> Regarding consensus, the works that are closest to ours are [FKM + 95, CHT96a, DFKM96, GS96]. In <ref> [GS96] </ref>, as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy 12 I.e., its specification refers only to the behavior of non-faulty processes. 21 property.
Reference: [vR97] <author> Robbert van Renesse, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title> <type> 24 </type>
Reference-contexts: Our heartbeat failure detector HB is different from the ones defined in [CT96], or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations <ref> [vR97, Cha97] </ref>). In contrast to existing failure detectors, HB is implementable without the use of timeouts (see Section 6). Moreover, as explained below, HB outputs a vector of counters rather than a list of suspected processes. In [ACT97b] we show that this is a fundamental difference. <p> Thus, HB should not be confused with existing implementations of failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>). Even though existing failure detectors are also based on the repeated sending of a heartbeat, they use timeouts on heartbeats in order to derive lists of processes considered to be up or down; applications can only see these lists.
References-found: 15

