URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/97/tr1343.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/97/
Root-URL: http://www.cs.wisc.edu
Email: cao@cs.wisc.edu irani@ics.uci.edu  
Title: Cost-Aware WWW Proxy Caching Algorithms  
Author: Pei Cao Sandy Irani 
Address: California-Irvine.  
Affiliation: Department of Computer Science, Information and Computer Science Department, University of Wisconsin-Madison. University of  
Abstract: Web caches can not only reduce network traffic and downloading latency, but can also affect the distribution of web traffic over the network through cost-aware caching. This paper introduces GreedyDual-Size, which incorporates locality with cost and size concerns in a simple and non-parameterized fashion for high performance. Trace-driven simulations show that with appropriate cost definition, GreedyDual-Size outperforms various existing web cache replacement algorithms in many aspects, including hit ratios, latency reduction and network cost reduction. In addition, GreedyDual-Size can potentially improve the performance of main-memory caching of Web documents. 
Abstract-found: 1
Intro-found: 1
Reference: [ASAWF95] <author> M. Abrams, C.R. Standbridge, G.Abdulla, S. Williams and E.A. Fox. </author> <title> Caching Proxies: Limitations and Potentials. </title> <address> WWW-4, Boston Conference, </address> <month> December, </month> <year> 1995. </year>
Reference-contexts: which means it replaces no other cached documents in the proxy. * Least-Recently-Used (LRU) evicts the doc ument which was requested the least recently. * Least-Frequently-Used (LFU) evicts the doc ument which has the least number of times be ing accessed. * Size [WASAF96] evicts the largest document. * LRU-Threshold <ref> [ASAWF95] </ref> is the same as LRU, except documents larger than a certain threshold size are never cached; Page 2 * Log (Size)+LRU [ASAWF95] evicts the doc-ument who has the largest log (size) and is the least recently used document among all docu ments with the same log (size). * Hyper-G [WASAF96] <p> least recently. * Least-Frequently-Used (LFU) evicts the doc ument which has the least number of times be ing accessed. * Size [WASAF96] evicts the largest document. * LRU-Threshold <ref> [ASAWF95] </ref> is the same as LRU, except documents larger than a certain threshold size are never cached; Page 2 * Log (Size)+LRU [ASAWF95] evicts the doc-ument who has the largest log (size) and is the least recently used document among all docu ments with the same log (size). * Hyper-G [WASAF96] is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the ear liest document, except if <p> Thus, LRV takes into account locality, cost and size of a document. Existing studies narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in <ref> [WASAF96, ASAWF95] </ref> show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate.
Reference: [Bel66] <author> L.A. Belady. </author> <title> A study of replacement algorithms for virtual storage computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5 </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: If one is given a sequence of requests to uniform size blocks of memory, it is well known that the simple rule of evicting the block whose next request is farthest in the future will yield the optimal performance <ref> [Bel66] </ref>. In the variable-size case, no such o*ine algorithm is known.
Reference: [CD73] <author> G. Coffman, Jr., Edward and Peter J. Den-ning, </author> <title> Operating Systems Theory, </title> <publisher> Prentice-Hall, Inc. </publisher> <year> 1973. </year>
Reference-contexts: The time is in minutes. Both the x-axis and the y-axis are in log scale. In the absence of cost and size concerns, LRU is the optimal online algorithm for reference streams exhibiting good locality <ref> [CD73] </ref> (strictly speaking, those conforming to the LRU-stack model). However, in the Web context, replacing a more recently used but large file can yield a higher hit ratio than replacing a less recently used but small file.
Reference: [DEC96] <institution> Digital Equipment Cooperation, </institution> <note> Digital's Web Proxy Traces ftp://ftp.digital.com/pub/DEC/traces/proxy /webtraces.html. </note>
Reference: [FKIP96] <author> A. Feldman, A. Karlin, S. Irani, S. Phillips. </author> <title> Private Communication. </title>
Reference-contexts: Interestingly, it is also known that LRU has an optimal competitive ratio when the page size can vary and the cost of bringing in a document is the same for all documents or proportional to the size of a document <ref> [FKIP96] </ref>. 2.2 Existing Document Replacement Al gorithms In existing web caching studies, at least nine algorithms have been proposed, to minimize miss ratio, byte miss ratio, average latency, total cost, etc. Below we give a brief description of all of them.
Reference: [Ho97] <author> Hosseini, Saied, </author> <title> Private Communication. </title>
Reference-contexts: In the variable-size case, no such o*ine algorithm is known. In fact, it is known that determining the optimal performance is NP-hard <ref> [Ho97] </ref>, although there is an algorithm which can approximate the optimal to within a logarithmic factor.The approximation factor is logarithmic in the maximum number of bytes that can fit in the cache, which we will call k.
Reference: [LRV97] <author> P. Lorenzetti, L. Rizzo and L. Vi-cisano. </author> <title> Replacement Policies for a Proxy Cache. </title> <type> Manuscript. </type>
Reference-contexts: Estimates for c s and b s are based on the the times to fetch documents from server s in the recent past. * Lowest Relative Value (LRV), introduced in <ref> [LRV97] </ref>, includes the cost and size of a document in the calculation of a value that estimates the utility of keeping a document in the cache. The algorithm evicts the document with the lowest value. The calculation of the value is based on extensive empirical analysis of trace data. <p> Existing studies narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in [WASAF96, ASAWF95] show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study <ref> [LRV97] </ref> shows that LRU outperforms SIZE in terms of byte hit rate. Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. <p> Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. In terms of minimizing latency, [WA97] show that Hybrid performs better than Lowest-Latency-First. Finally, <ref> [LRV97] </ref> shows that LRV outperforms both LRU and SIZE in terms of hit ratio and byte hit ratio. One disadvantage of both Hybrid and LRV is their heavy parameterization, which leaves one uncertain about their performance across access streams. <p> Clearly, the probablity of reference drops significantly as the time since last reference increases (note that in the figure, the y-axis is in log scale.) A different study <ref> [LRV97] </ref> reaches similar conclusions on a different set of traces. Page 4 time since last access to the same document. The time is in minutes. Note that both the x-axis and the y-axis are in log scale. <p> Though one might expect that browsers caches absorb the locality among the same user's accesses seen by the proxy, the results seems to indicate that this is not necessarily the case, and users are using proxy caches as an extension to the browser cache. <ref> [LRV97] </ref> observes the same pheomonen. The other reason is that users' interests overlap in time | comparing figures 2 and 1, we can see that for the same t, the probability in figure 1 is higher than that in figure 2. <p> Size, Hybrid, LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> The relative comparison of LRU and Size differs from the results in [WASAF96], but agrees with those in <ref> [LRV97] </ref>. In summary, for proxy designers that seek to maximize hit ratio, GD-Size (1) is the appropriate algorithm. If both high hit ratio and high byte hit ratio are desired, GD-Size (packets) is the appropriate algorithm.
Reference: [CBC95] <author> Carlos R. Cunba, Azer Bestavros, Mark E. </author> <title> Crovella Characteristics of WWW Client-based Traces BU-CS-96-010, </title> <address> Boston University. </address>
Reference-contexts: For each trace, we first calculate the benefit obtained if the cache size is infinite. The values for all traces are shown in Table 1. In the table, BU-272 and BU-B19 are two sets of traces from Boston University <ref> [CBC95] </ref>, VT-BL, VT-C, VT-G, VT-U are four sets of traces from Virginia Tech [WASAF96], DEC-U1:8/29-9/4 through DEC-U1:9/19-9/22 are the requests made by users 0-512 (user group 1) for each week in the three and half week period, and DEC-U2:8/29-9/4 through DEC-U2:9/19-9/22 are the traces for users 1024-2048 (user group 2).
Reference: [LM96] <author> Paul Leach and Jeff Mogul. </author> <title> The Hit Metering Protocol. </title> <type> Manuscript. </type>
Reference: [HT97] <institution> IETF The HTTP 1.1 Protocol Draft. </institution> <note> http://www.ietf.org. </note>
Reference-contexts: We perform some necessary pre-processing over the traces. For the DEC traces, we simulated only those requests whose replies are cacheable as specified in HTTP 1.1 <ref> [HT97] </ref> (i.e. GET or HEAD requests with status 200, 203, 206, 300, or 301, and not a "cgi bin" request).
Reference: [ST85] <author> D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communications of the ACM, </journal> <volume> 28 </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: The competitive ratio is essentially the maximum ratio of the algorithms cost to the optimal o*ine algorithm's cost over all possible request sequences. (For an introduction to competitive analysis, see <ref> [ST85] </ref>). We have generalized the result in [You91b] to show that our algorithm GreedyDual-Size, which handles documents of differing sizes and differing cost (described in Section 4), also has an optimal competitive ratio.
Reference: [W3C] <institution> The Notification Protocol. </institution> <note> http://www.w3c.org. </note>
Reference: [WASAF96] <author> S. Williams, M. Abrams, C.R. Stand-bridge, G.Abdulla and E.A. Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proceedings of the ACM Sigcomm96, </booktitle> <month> August, </month> <year> 1996, </year> <institution> Stanford University. </institution>
Reference-contexts: Since documents are stored at the proxy cache, many HTTP requests can be satisfied directly from the cache instead of generating traffic to and from the Web 1 Navigator is a trademark of Netscape Inc. server. Numerous studies <ref> [WASAF96] </ref> have shown that the hit ratio for Web proxy caches can be as high as over 50%. This means that if proxy caching is utilized extensively, the network traffic can potentially be reduced by as much as 20%. <p> evicted upon its arrival into the cache, which means it replaces no other cached documents in the proxy. * Least-Recently-Used (LRU) evicts the doc ument which was requested the least recently. * Least-Frequently-Used (LFU) evicts the doc ument which has the least number of times be ing accessed. * Size <ref> [WASAF96] </ref> evicts the largest document. * LRU-Threshold [ASAWF95] is the same as LRU, except documents larger than a certain threshold size are never cached; Page 2 * Log (Size)+LRU [ASAWF95] evicts the doc-ument who has the largest log (size) and is the least recently used document among all docu ments with <p> [ASAWF95] is the same as LRU, except documents larger than a certain threshold size are never cached; Page 2 * Log (Size)+LRU [ASAWF95] evicts the doc-ument who has the largest log (size) and is the least recently used document among all docu ments with the same log (size). * Hyper-G <ref> [WASAF96] </ref> is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the ear liest document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First [WA97] tries to minimize average latency by removing the document with the lowest <p> never cached; Page 2 * Log (Size)+LRU [ASAWF95] evicts the doc-ument who has the largest log (size) and is the least recently used document among all docu ments with the same log (size). * Hyper-G <ref> [WASAF96] </ref> is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the ear liest document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First [WA97] tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. <p> Thus, LRV takes into account locality, cost and size of a document. Existing studies narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in <ref> [WASAF96, ASAWF95] </ref> show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in [WASAF96] also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate. <p> Thus, LRV takes into account locality, cost and size of a document. Existing studies narrowed down the choice for proxy replacement algorithms to LRU, SIZE, Hybrid and LRV. Results in [WASAF96, ASAWF95] show that SIZE performs better than LFU, LRU-threshold, Log (size)+LRU, Hyper-G and Pitkow/Recker. Results in <ref> [WASAF96] </ref> also show that SIZE outperforms LRU in most situations. However, a different study [LRV97] shows that LRU outperforms SIZE in terms of byte hit rate. <p> were successful in obtaining the following traces of HTTP requests going through Web proxies: * Digital Equipment Cooperation Web Proxy server traces [DEC96](Aug-Sep 1996), servicing about 17,000 workstations, for a period of 25 days, containing a total of about 24,000,000 accesses; * University of Virginia proxy server and client traces <ref> [WASAF96] </ref> (Feb-Oct 1995), containing four sets of traces, each servicing from 25 to 61 workstations, containing from 13,127 to 227,210 accesses; * Boston University client traces [CBC95](Nov 1994 May 1995), containing two sets of traces, one servicing 5 workstations (17,008 accesses), the other 32 workstations (118,105 accesses); We are in the <p> For Virginia Tech traces, we simulated only the "GET" requests with reply status 200 and a known reply size. Thus, our numbers differ from what are reported in <ref> [WASAF96] </ref>. The Virginia Tech traces unfortunately do not come with latency information. <p> Size, Hybrid, LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> The values for all traces are shown in Table 1. In the table, BU-272 and BU-B19 are two sets of traces from Boston University [CBC95], VT-BL, VT-C, VT-G, VT-U are four sets of traces from Virginia Tech <ref> [WASAF96] </ref>, DEC-U1:8/29-9/4 through DEC-U1:9/19-9/22 are the requests made by users 0-512 (user group 1) for each week in the three and half week period, and DEC-U2:8/29-9/4 through DEC-U2:9/19-9/22 are the traces for users 1024-2048 (user group 2). <p> LRU performs better than SIZE in terms of hit ratio when the cache size is small (less or equal than 5% of the total date set size), but performs slight worse when the cache size is large. The relative comparison of LRU and Size differs from the results in <ref> [WASAF96] </ref>, but agrees with those in [LRV97]. In summary, for proxy designers that seek to maximize hit ratio, GD-Size (1) is the appropriate algorithm. If both high hit ratio and high byte hit ratio are desired, GD-Size (packets) is the appropriate algorithm.
Reference: [WA97] <author> R. Wooster and M. Abrams. </author> <title> Proxy Caching the Estimates Page Load Delays. </title> <booktitle> To appear in the 6th International World Wide Web Conference. </booktitle>
Reference-contexts: document among all docu ments with the same log (size). * Hyper-G [WASAF96] is a refinement of LFU with last access time and size considerations; * Pitkow/Recker [WASAF96] removes the ear liest document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First <ref> [WA97] </ref> tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. A function is computed for each document which is designed to capture the utility of retaining a given document in the cache. <p> access time and size considerations; * Pitkow/Recker [WASAF96] removes the ear liest document, except if all documents are accessed today, in which case the largest one is removed; * Lowest-Latency-First <ref> [WA97] </ref> tries to minimize average latency by removing the document with the lowest download latency first; * Hybrid, introduced in [WA97], is aimed at reducing the total latency. A function is computed for each document which is designed to capture the utility of retaining a given document in the cache. The document with the smallest function value is then evicted. <p> Comparing LFU and LRU, our experiments show that though LFU can outperform LRU slightly when the cache size is very small, in most cases LFU performs worse than LRU. In terms of minimizing latency, <ref> [WA97] </ref> show that Hybrid performs better than Lowest-Latency-First. Finally, [LRV97] shows that LRV outperforms both LRU and SIZE in terms of hit ratio and byte hit ratio. One disadvantage of both Hybrid and LRV is their heavy parameterization, which leaves one uncertain about their performance across access streams. <p> Another concern about both Hybrid and LRV is that they employ constants which might have to be tuned to the patterns in the request stream. For Hybrid, we use the values which were used in <ref> [WA97] </ref> in our simulations. We did not experiment with tuning those constants to improve the performance of Hybrid. Though LRV can incorporate arbitrary network costs associated with documents, the O (k) computational complexity of finding a replacement can be prohibitively expensive. <p> Size, Hybrid, LRV are all "champion" algorithms from previously published studies <ref> [WASAF96, LRV97, WA97] </ref>. In addition, for LRV, we first go through the whole trace to obtain the necessary parameters, thus giving it the advantage of perfect statistical information. <p> Page 8 5.3 Reduced Latency Another major concern for proxies is to reduce the latency of HTTP requests through caching, as numerous studies have shown that the waiting time has become the number one concern of Web users. One study <ref> [WA97] </ref> introduced a proxy replacement algorithm called Hybrid, which takes into account the different latencies incurred to load different web pages, and attempts to minimize the average latency. The study [WA97] further showed that in general the algorithm has a lower average latency than LRU, LFU and SIZE. <p> One study <ref> [WA97] </ref> introduced a proxy replacement algorithm called Hybrid, which takes into account the different latencies incurred to load different web pages, and attempts to minimize the average latency. The study [WA97] further showed that in general the algorithm has a lower average latency than LRU, LFU and SIZE. We also designed two versions of GreedyDual-Size that take latency into account. One, called GD-Size (latency), sets the cost of a document to the latency it took to download. <p> One, called GD-Size (latency), sets the cost of a document to the latency it took to download. The other, called GD-Size (avg latency), sets the cost to the estimated download latency of a document, using the same method of estimating latency as in Hybrid <ref> [WA97] </ref>. LRU, Hybrid, GD-Size (1), GD-Size (latency) and GD-Size (avg latency). The figure unfortunately does not include Virginia Tech traces because those traces do not come with latency information for each HTTP request. Clearly, GD-Size (1) performs the best, yielding the highest reduced latency. <p> For all DEC traces, Hybrid's hit ratio is much lower than LRU's, under all cache sizes. The reason for Hybrid's low hit ratio is because it does not consider how recently a document has been accessed in choosing replacements. Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. <p> Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. It is a surprise to us that GD-Size (1), which does not take latency into account, performs better than GD-Size (latency) and GD-Size (avg latency). <p> Since <ref> [WA97] </ref> reports that Hybrid performs well, our results here seem to suggest that Hybrid's performance is perhaps trace-dependent. In our simulation of Hybrid we used the same constants in [WA97], without tuning them to our traces. Unfortunately we were not able to obtain the traces used in [WA97]. It is a surprise to us that GD-Size (1), which does not take latency into account, performs better than GD-Size (latency) and GD-Size (avg latency). Detailed examination of the traces shows that the latency of loading the same document varies significantly.
Reference: [You91b] <author> N. Young. </author> <title> The k-server dual and loose competitiveness for paging. </title> <journal> Algorithmica, </journal> <year> 1991. </year> <title> To appear. Rewritten version of "On-line caching as cache size varies", </title> <booktitle> in The 2nd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> 241-250, </pages> <year> 1991. </year> <pages> Page 12 </pages>
Reference-contexts: In this paper, we introduce a new algorithm, called GreedyDual-Size, which combines locality, size and latency/cost concerns effectively to achieve the best overall performance. GreedyDual-Size is a variation on a simple and elegant algorithm called GreedyDual (designed by Neal Young <ref> [You91b] </ref>), which handles uniform-size variable-cost cache replacement. Using trace-driven simulation, we show that GreedyDual-Size with appropriate cost definitions outperforms the various "champion" web caching algorithms in existing studies on a number of performance issues, including hit ratios, latency reduction, and network cost reduction. <p> For the cost consideration, there have been several algorithms developed for the uniform-size variable-cost paging problem. GreedyDual, developed by Neal Young <ref> [You91b] </ref>, is actually a range of algorithms which include a generalization of LRU and a generalization of FIFO. The name GreedyDual comes from the technique used to prove that this entire range of algorithms is optimal according to its competitive ratio. <p> The competitive ratio is essentially the maximum ratio of the algorithms cost to the optimal o*ine algorithm's cost over all possible request sequences. (For an introduction to competitive analysis, see [ST85]). We have generalized the result in <ref> [You91b] </ref> to show that our algorithm GreedyDual-Size, which handles documents of differing sizes and differing cost (described in Section 4), also has an optimal competitive ratio. <p> Thus, we need an algorithm that combines locality, size and cost considerations in a simple, no parameter, online way that maximize the overall performance. Below we introduce one such algorithm, GreedyDual. 4 Algorithm GreedyDual-Size The original GreedyDual algorithm is proposed by Young and Tarjan <ref> [You91b] </ref>. It is concerned with the case when pages in a cache have the same size, but incur different costs to fetch from a secondary storage. The algorithm associates a value, H, with each cached page p.
References-found: 15

