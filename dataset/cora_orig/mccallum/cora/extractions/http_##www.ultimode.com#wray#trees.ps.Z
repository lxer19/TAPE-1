URL: http://www.ultimode.com/wray/trees.ps.Z
Refering-URL: http://WWW.Ultimode.com/~wray/refs.html
Root-URL: 
Title: Learning Classification Trees  
Author: Wray Buntine 
Keyword: Classification trees, Bayesian statistics, CART, ID3  
Note: Date: November 10, 1991  
Address: Mail Stop 269-2, Moffett Field, CA 94035, USA  
Affiliation: RIACS NASA Ames Research Center  
Email: wray@kronos.arc.nasa.gov  
Date: November 20, 1991  
Abstract: Algorithms for learning classification trees have had successes in artificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statistics. This introduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to Quinlan's information gain, while smoothing and averaging replace pruning. Comparative experiments with reimplementations of a minimum encoding approach, Quinlan's C4 (Quinlan et al., 1987) and Breiman et al.'s CART (Breiman et al., 1984) show the full Bayesian algorithm can produce Publication: This paper is a final draft submitted for publication to the Statistics and Computing journal; a version with some minor changes appeared in Volume 2, 1992, pages 63-73. more accurate predictions than versions of these other approaches, though pay a computational price.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bahl, L., Brown, P., de Souza, P., and Mercer, R. </author> <year> (1989). </year> <title> A tree-based langauge model for natural language speech recognition. </title> <journal> IEEE Trans. on AS and SP, </journal> <volume> 37(7) </volume> <pages> 1001-1008. </pages>
Reference-contexts: Perhaps the major classical statistics text in this area is Breiman et al. (Breiman et al., 1984), and a wide variety of methods and comparative studies exist in other areas. For instance, see Quinlan (Quinlan, 1988), Mingers (Mingers, 1989b), Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer <ref> (Bahl et al., 1989) </ref>, Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 (Crawford, 1989) and Chou (Chou, 1991). The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging. <p> The pruned tree may still have observed class probabilities at the leaves with zeroes for some classes, an unrealistic situation when noisy data is being used. So smoothing techniques described by Bahl et al. <ref> (Bahl et al., 1989) </ref> and Chou (Chou, 1991), explained later, are sometimes employed to make better class probability estimates. A final technique from Kwok and Carter (Kwok and Carter, 1990) is to build multiple trees and use the benefits of averaging to arrive at possibly more accurate class probability estimates. <p> These two encoding approaches are based on the idea of the "most probable model", or its logarithmic counterpart, the minimum encoding. But the approaches here average over the best few models using two techniques, the simplest is a Bayesian variant of the smoothing technique of Bahl et al. <ref> (Bahl et al., 1989) </ref>. A fuller discussion of the Bayesian methods presented here, including the treatment of real values and extensive comparative experiments, is given by Buntine (Buntine, 1991b). The current experiments are described in more detail by Buntine (Buntine, 1991a). <p> In the smoothing approach, we also average all the class probability vectors encountered at interior nodes along the way, see Bahl et al. <ref> (Bahl et al., 1989, p1005) </ref>. Given a particular tree structure T 0 , presumably grown using the algorithm described in Section 3.1, consider the space of tree structures, pruned (T 0 ), obtained by pruning the tree structure T 0 in all possible ways.
Reference: <author> Berger, J. O. </author> <year> (1985). </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: We therefore might use a hierarchical prior that relates class probabilities across leaves. Rather than these, I use priors that have a simple conjugate (that is, the prior is the same functional form as the likelihood function, see Berger <ref> (Berger, 1985) </ref>) and multiplicative form. Generic representation-independent techniques for forming priors, such as Rodriguez's entropic priors and smoothing priors described by Buntine and Weigend (Buntine and Weigend, 1991), yield priors that lack the simple form I use. <p> Let ff 0 = P C The Dirichlet terms ensure that the class probability vectors at each leaf l center about a common mean ff 0 ff C with standard deviation proportional to 1 p ff 0 +1 . I take an empirical Bayes approach (see Berger <ref> (Berger, 1985) </ref>) and set the common mean to the observed common mean in the training sample. Because of the sample size, this should be close to the population base-rate.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont. </address>
Reference-contexts: This prediction problem is often handled with partitioning classifiers. These classifiers split the example space into partitions, for instance, Quinlan's ID3 (Quinlan, 1986) and Breiman, Friedland, Olshen and Stone's CART <ref> (Breiman et al., 1984) </ref> use classification trees to recursively partition the example space and Clark and Niblett's CN2 (Clark and Niblett, 1989) and Weiss et al.'s PVM (Weiss et al., 1990) use disjunctive rules (disjunctive rules also partition a space, but not recursively in the manner of trees). <p> The tree-based approaches have been pursued in many areas such as applied statistics, character recognition and information theory for well over two decades. Perhaps the major classical statistics text in this area is Breiman et al. <ref> (Breiman et al., 1984) </ref>, and a wide variety of methods and comparative studies exist in other areas. <p> To help overcome this problem, a second process is subsequently employed to prune back the tree, for instance using resampling or hold-out methods, such as Breiman et al. <ref> (Breiman et al., 1984) </ref> or Crawford (Crawford, 1989), approximate significance tests such as Quinlan (Quinlan, 1988) or Mingers (Mingers, 1989a), or minimum encoding such as Quinlan and Rivest (Quinlan and Rivest, 1989). <p> In medical data, for instance, where many of the attributes supplied for each example may well be noise, we would expect the stronger 6 priors to be more reasonable. In problems like the classic faulty LED problem of Breiman et al. <ref> (Breiman et al., 1984) </ref>, we would expect all faulty LED indicators to be somewhat informative about the actual digit being represented, so large trees seem reasonable and the type I prior should be used. <p> I use the cut that maximizes the quality test Quality 1 (R &lt; cut). Experiments show the quality heuristic of Equation (6) is very similar in performance to the information gain measure of Quinlan (Quinlan, 1986) and to the GINI measure of Breiman et al.'s CART <ref> (Breiman et al., 1984) </ref>. The Approximation (4) explains why. The quality measure has the distinct advantage, however, of returning a measure of quality that is a probability. <p> To reduce computation for very large N , we could evaluate the tests on a subset of the sample (i.e. reduce N in the computation), as suggested by Breiman et al. <ref> (Breiman et al., 1984) </ref> and Catlett (Catlett, 1991). Because the measure of quality is in units of probability, one can readily determine if one test is significantly better than another according to the measure simply by taking their ratio. <p> The parameter setting allows improved predictive accuracy at computational expense. 5 Conclusion Bayesian algorithms for learning class probability trees were presented and compared empirically with reim-plementations of existing approaches like Breiman et al.'s CART <ref> (Breiman et al., 1984) </ref>, Quinlan's C4 (Quinlan, 1988), and minimum encoding approaches. The Bayesian option trees and averaging algorithm gave significantly better accuracy and half-Brier score on predictions for a set of learning problems, but this was at the expense of computational cost.
Reference: <author> Buntine, W. </author> <year> (1991a). </year> <title> Some experiments with learning classification trees. </title> <type> Technical report, </type> <institution> NASA Ames Research Center. </institution> <note> In preparation. </note>
Reference-contexts: A fuller discussion of the Bayesian methods presented here, including the treatment of real values and extensive comparative experiments, is given by Buntine (Buntine, 1991b). The current experiments are described in more detail by Buntine <ref> (Buntine, 1991a) </ref>. Source code and manual for the implementations and reimplementations are available in some cases as the IND Tree Package by Buntine and Caruana (Buntine and Caruana, 1991). 2 Theory This section introduces the notation, the priors and the posteriors for the prediction task just described. <p> Algorithms are part of the IND Tree Package and options used on Version 1.1, more details of the data sets, and acknowledgements to the sources are given by Buntine <ref> (Buntine, 1991a) </ref>. 13 4.2 Results A somewhat random selection of the results are presented in Table 1. The MSE column refers to half-Brier score.
Reference: <author> Buntine, W. </author> <year> (1991b). </year> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, Sydney. </institution>
Reference-contexts: Perhaps the major classical statistics text in this area is Breiman et al. (Breiman et al., 1984), and a wide variety of methods and comparative studies exist in other areas. For instance, see Quinlan (Quinlan, 1988), Mingers (Mingers, 1989b), Buntine <ref> (Buntine, 1991b) </ref>, Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 (Crawford, 1989) and Chou (Chou, 1991). The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging. <p> A fuller discussion of the Bayesian methods presented here, including the treatment of real values and extensive comparative experiments, is given by Buntine <ref> (Buntine, 1991b) </ref>. The current experiments are described in more detail by Buntine (Buntine, 1991a). <p> Our claim, however, is that the priors described above are an adequate family of mildly-informative tree priors for the purposes of demonstrating a Bayesian approach to learning tree classifiers. 2.3 Posteriors Using standard properties of the Dirichlet distributions, given by Buntine <ref> (Buntine, 1991b, Sec.2.5) </ref>, posteriors conditioned on the training sample can now be computed as follows. <p> This section introduces methods closer in spirit to Henrion (Henrion, 1990) who collects the dominant terms in the posterior sum. Fuller details of the methods described below are given by Buntine <ref> (Buntine, 1991b) </ref>. Intuitively, this full Bayesian approach involves averaging all those trees that seem a reasonable explanation of the classifications in the training sample, and then predicting the class of a new example based on the weighted predictions of the reasonable trees.
Reference: <author> Buntine, W. and Caruana, R. </author> <year> (1991). </year> <title> Introduction to ind and recursive partitioning. </title> <type> Technical Report FIA-91-28, </type> <institution> RIACS and NASA Ames Research Center, Moffett Field, </institution> <address> CA. </address>
Reference-contexts: The current experiments are described in more detail by Buntine (Buntine, 1991a). Source code and manual for the implementations and reimplementations are available in some cases as the IND Tree Package by Buntine and Caruana <ref> (Buntine and Caruana, 1991) </ref>. 2 Theory This section introduces the notation, the priors and the posteriors for the prediction task just described. This develops the theoretical tools on which the Bayesian tree learning methods are based. <p> Generic representation-independent techniques for forming priors, such as Rodriguez's entropic priors and smoothing priors described by Buntine and Weigend <ref> (Buntine and Weigend, 1991) </ref>, yield priors that lack the simple form I use. But 5 although our motivation for choice of priors is expediency, I believe they are reasonable "generic" priors for many circumstances.
Reference: <author> Buntine, W. and Weigend, A. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5(1) </volume> <pages> 603-643. </pages>
Reference-contexts: The current experiments are described in more detail by Buntine (Buntine, 1991a). Source code and manual for the implementations and reimplementations are available in some cases as the IND Tree Package by Buntine and Caruana <ref> (Buntine and Caruana, 1991) </ref>. 2 Theory This section introduces the notation, the priors and the posteriors for the prediction task just described. This develops the theoretical tools on which the Bayesian tree learning methods are based. <p> Generic representation-independent techniques for forming priors, such as Rodriguez's entropic priors and smoothing priors described by Buntine and Weigend <ref> (Buntine and Weigend, 1991) </ref>, yield priors that lack the simple form I use. But 5 although our motivation for choice of priors is expediency, I believe they are reasonable "generic" priors for many circumstances.
Reference: <author> Carter, C. and Catlett, J. </author> <year> (1987). </year> <title> Assessing credit card applications using machine learning. </title> <journal> IEEE Expert, </journal> <volume> 2(3) </volume> <pages> 71-79. </pages>
Reference-contexts: 1 Introduction A common inference task consists of making a discrete prediction about some object given other details about the object. For instance, in financial credit assessment as discussed by Carter and Catlett <ref> (Carter and Catlett, 1987) </ref> we wish to decide whether to accept or reject a customer's application for a loan given particular personal information.
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: machine learning on very large databases. </title> <type> PhD thesis, </type> <institution> University of Sydney. </institution>
Reference-contexts: To reduce computation for very large N , we could evaluate the tests on a subset of the sample (i.e. reduce N in the computation), as suggested by Breiman et al. (Breiman et al., 1984) and Catlett <ref> (Catlett, 1991) </ref>. Because the measure of quality is in units of probability, one can readily determine if one test is significantly better than another according to the measure simply by taking their ratio.
Reference: <author> Cestnik, B., Kononenko, I., and Bratko, I. </author> <year> (1987). </year> <title> Assistant86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In Bratko, I. and Lavrac, N., editors, </editor> <booktitle> Progress in Machine Learning: Proceedings of EWSL-87, </booktitle> <pages> pages 31-45, </pages> <address> Bled, Yugoslavia. </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Data sets came from different real and simulated domains, and have a variety of characteristics. They include Quinlan's hypothyroid and XD6 data (Quinlan, 1988), the CART digital LED problem, medical domains reported by Cestnik, Kononenko and Bratko <ref> (Cestnik et al., 1987) </ref>, pole balancing data from human subjects collected by Michie, Bain and Hayes-Michie (Michie et al., 1990), and a variety of other data sets from the Irvine Machine Learning Database such as "glass", "voting records", "hepatitis" and "mushrooms".
Reference: <author> Chou, P. </author> <year> (1991). </year> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4). </volume>
Reference-contexts: For instance, see Quinlan (Quinlan, 1988), Mingers (Mingers, 1989b), Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 (Crawford, 1989) and Chou <ref> (Chou, 1991) </ref>. The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging. A tree is first grown to completion so that the tree partitions the training sample into terminal regions of all one class. <p> The pruned tree may still have observed class probabilities at the leaves with zeroes for some classes, an unrealistic situation when noisy data is being used. So smoothing techniques described by Bahl et al. (Bahl et al., 1989) and Chou <ref> (Chou, 1991) </ref>, explained later, are sometimes employed to make better class probability estimates. A final technique from Kwok and Carter (Kwok and Carter, 1990) is to build multiple trees and use the benefits of averaging to arrive at possibly more accurate class probability estimates. <p> Standard computational techniques for performing averaging such as importance sampling and Gibbs sampling or therefore avoided. More sophisticated priors could help here, but it is surely just as important to consider more versatile classification models such as the decision trellises suggested by Chou <ref> (Chou, 1991) </ref>. Second, the Bayesian methods derived here corresponded to a variety of subtasks previously done by a collection of disparate ad hoc approaches honed through experience.
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283. </pages>
Reference-contexts: This prediction problem is often handled with partitioning classifiers. These classifiers split the example space into partitions, for instance, Quinlan's ID3 (Quinlan, 1986) and Breiman, Friedland, Olshen and Stone's CART (Breiman et al., 1984) use classification trees to recursively partition the example space and Clark and Niblett's CN2 <ref> (Clark and Niblett, 1989) </ref> and Weiss et al.'s PVM (Weiss et al., 1990) use disjunctive rules (disjunctive rules also partition a space, but not recursively in the manner of trees). Tree algorithms build trees such as the ones shown in Figure 1.
Reference: <author> Crawford, S. </author> <year> (1989). </year> <title> Extensions to the CART algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31(2) </volume> <pages> 197-217. </pages>
Reference-contexts: For instance, see Quinlan (Quinlan, 1988), Mingers (Mingers, 1989b), Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 <ref> (Crawford, 1989) </ref> and Chou (Chou, 1991). The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging. A tree is first grown to completion so that the tree partitions the training sample into terminal regions of all one class. <p> To help overcome this problem, a second process is subsequently employed to prune back the tree, for instance using resampling or hold-out methods, such as Breiman et al. (Breiman et al., 1984) or Crawford <ref> (Crawford, 1989) </ref>, approximate significance tests such as Quinlan (Quinlan, 1988) or Mingers (Mingers, 1989a), or minimum encoding such as Quinlan and Rivest (Quinlan and Rivest, 1989). <p> This can be used to determine if evaluation on the current subsample is sufficient, or if we need to view a larger subsample. A related problem occurs when growing trees incrementally. In this regime, a tree needs to be updated given some additions to the training sample. Crawford <ref> (Crawford, 1989) </ref> shows that if we take the naive approach as done by Utgoff (Utgoff, 1989) and attempt to update the current tree so the "best" split according to the updated sample is taken at each node, the algorithm suffers from repeated restructuring.
Reference: <author> Henrion, M. </author> <year> (1990). </year> <title> Towards efficient inference in multiply connected belief networks. </title> <editor> In Oliver, R. and Smith, J., editors, </editor> <title> Influence Diagrams, </title> <booktitle> Belief Nets and Decision Analysis, </booktitle> <pages> pages 385-407. </pages> <note> Wiley. 16 Kwok, </note> <author> S. and Carter, C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <editor> In Shachter, R., Levitt, T., Kanal, L., and Lemmer, J., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <publisher> North-Holland. </publisher>
Reference-contexts: This section introduces methods closer in spirit to Henrion <ref> (Henrion, 1990) </ref> who collects the dominant terms in the posterior sum. Fuller details of the methods described below are given by Buntine (Buntine, 1991b).
Reference: <author> Lee, P. </author> <year> (1989). </year> <title> Bayesian Statistics: An Introduction. </title> <publisher> Oxford University Press, </publisher> <address> New York. </address>
Reference-contexts: This develops the theoretical tools on which the Bayesian tree learning methods are based. The section largely assumes the reader is familiar with the basics of Bayesian analysis, that is, the application of Bayes theorem, the notion of subjective priors, etc. See for instance, Press (Press, 1989) or Lee <ref> (Lee, 1989) </ref>. In what follows, the term "prior" is an abbreviation for "prior probability distribution". Likewise for posterior. The prior and the posterior are both subjective Bayesian probabilities or measures of belief, and so do not correspond to measurable frequencies.
Reference: <author> Michie, D., Bain, M., and Hayes-Michie, J. </author> <year> (1990). </year> <title> Cognitive models from subcognitive skills. </title> <editor> In McGhee, J., Grimble, M., and Mowforth, P., editors, </editor> <title> Knowledge-based Systems for Industrial Control. </title> <publisher> Stevenage: Peter Peregrinus. </publisher>
Reference-contexts: They include Quinlan's hypothyroid and XD6 data (Quinlan, 1988), the CART digital LED problem, medical domains reported by Cestnik, Kononenko and Bratko (Cestnik et al., 1987), pole balancing data from human subjects collected by Michie, Bain and Hayes-Michie <ref> (Michie et al., 1990) </ref>, and a variety of other data sets from the Irvine Machine Learning Database such as "glass", "voting records", "hepatitis" and "mushrooms". The "voting" data has had the attribute "physician-fee-freeze" deleted, as recommended by Michie. These are available via ftp at ics.uci.edu in ``/pub''.
Reference: <author> Mingers, J. </author> <year> (1989a). </year> <title> An empirical comparison of pruning methods for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243. </pages>
Reference-contexts: To help overcome this problem, a second process is subsequently employed to prune back the tree, for instance using resampling or hold-out methods, such as Breiman et al. (Breiman et al., 1984) or Crawford (Crawford, 1989), approximate significance tests such as Quinlan (Quinlan, 1988) or Mingers <ref> (Mingers, 1989a) </ref>, or minimum encoding such as Quinlan and Rivest (Quinlan and Rivest, 1989). The pruned tree may still have observed class probabilities at the leaves with zeroes for some classes, an unrealistic situation when noisy data is being used.
Reference: <author> Mingers, J. </author> <year> (1989b). </year> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 319-342. </pages>
Reference-contexts: Perhaps the major classical statistics text in this area is Breiman et al. (Breiman et al., 1984), and a wide variety of methods and comparative studies exist in other areas. For instance, see Quinlan (Quinlan, 1988), Mingers <ref> (Mingers, 1989b) </ref>, Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 (Crawford, 1989) and Chou (Chou, 1991). The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging.
Reference: <author> Pagallo, G. and Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99. </pages>
Reference-contexts: Trees tend to fracture the example space unnecessarily, referred to by Pagallo and Haussler <ref> (Pagallo and Haussler, 1990) </ref> as the replication problem, because some tests higher in the tree are unnecessary for some (but not all) of the lower leaves. For this reason, we might expect class probabilities to be similar across different leaf nodes.
Reference: <author> Press, S. </author> <year> (1989). </year> <title> Bayesian Statistics. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: This develops the theoretical tools on which the Bayesian tree learning methods are based. The section largely assumes the reader is familiar with the basics of Bayesian analysis, that is, the application of Bayes theorem, the notion of subjective priors, etc. See for instance, Press <ref> (Press, 1989) </ref> or Lee (Lee, 1989). In what follows, the term "prior" is an abbreviation for "prior probability distribution". Likewise for posterior. The prior and the posterior are both subjective Bayesian probabilities or measures of belief, and so do not correspond to measurable frequencies.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: A training sample in this case would be historical records of previous loan applications together with whether the loan went bad. This generic learning task is referred to as supervised learning in pattern recognition, and induction or empirical learning in machine learning, see for instance Quinlan <ref> (Quinlan, 1986) </ref>. The statistical community uses techniques such as discriminant analysis and nearest neighbor methods, for instance described by Ripley (Ripley, 1987). This prediction problem is often handled with partitioning classifiers. These classifiers split the example space into partitions, for instance, Quinlan's ID3 (Quinlan, 1986) and Breiman, Friedland, Olshen and Stone's <p> in machine learning, see for instance Quinlan <ref> (Quinlan, 1986) </ref>. The statistical community uses techniques such as discriminant analysis and nearest neighbor methods, for instance described by Ripley (Ripley, 1987). This prediction problem is often handled with partitioning classifiers. These classifiers split the example space into partitions, for instance, Quinlan's ID3 (Quinlan, 1986) and Breiman, Friedland, Olshen and Stone's CART (Breiman et al., 1984) use classification trees to recursively partition the example space and Clark and Niblett's CN2 (Clark and Niblett, 1989) and Weiss et al.'s PVM (Weiss et al., 1990) use disjunctive rules (disjunctive rules also partition a space, but not <p> Experience has shown that a tree so grown will suffer from over-fitting, in the sense that nodes near the bottom of the tree represent noise in the sample, and their removal can often increase predictive accuracy, see Quinlan <ref> (Quinlan, 1986) </ref> for an introduction. <p> The information measure I (C j T ) when applied to a tree of depth one is used by Quinlan <ref> (Quinlan, 1986) </ref> as a splitting rule when growing trees. If some leaves have small numbers of examples, then Approxi mation (4) is poor, and the beta function really needs to be used. <p> I use the cut that maximizes the quality test Quality 1 (R &lt; cut). Experiments show the quality heuristic of Equation (6) is very similar in performance to the information gain measure of Quinlan <ref> (Quinlan, 1986) </ref> and to the GINI measure of Breiman et al.'s CART (Breiman et al., 1984). The Approximation (4) explains why. The quality measure has the distinct advantage, however, of returning a measure of quality that is a probability.
Reference: <author> Quinlan, J. </author> <year> (1988). </year> <title> Simplifying decision trees. </title> <editor> In Gaines, B. and Boose, J., editors, </editor> <booktitle> Knowledge Acquisition for Knowledge-Based Systems, </booktitle> <pages> pages 239-252. </pages> <publisher> Academic Press, London. </publisher>
Reference-contexts: Perhaps the major classical statistics text in this area is Breiman et al. (Breiman et al., 1984), and a wide variety of methods and comparative studies exist in other areas. For instance, see Quinlan <ref> (Quinlan, 1988) </ref>, Mingers (Mingers, 1989b), Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest (Quinlan and Rivest, 1989), Crawford 3 (Crawford, 1989) and Chou (Chou, 1991). <p> To help overcome this problem, a second process is subsequently employed to prune back the tree, for instance using resampling or hold-out methods, such as Breiman et al. (Breiman et al., 1984) or Crawford (Crawford, 1989), approximate significance tests such as Quinlan <ref> (Quinlan, 1988) </ref> or Mingers (Mingers, 1989a), or minimum encoding such as Quinlan and Rivest (Quinlan and Rivest, 1989). The pruned tree may still have observed class probabilities at the leaves with zeroes for some classes, an unrealistic situation when noisy data is being used. <p> This behaves similarly to the standard GINI and information gain criteria on binary splits. Data sets came from different real and simulated domains, and have a variety of characteristics. They include Quinlan's hypothyroid and XD6 data <ref> (Quinlan, 1988) </ref>, the CART digital LED problem, medical domains reported by Cestnik, Kononenko and Bratko (Cestnik et al., 1987), pole balancing data from human subjects collected by Michie, Bain and Hayes-Michie (Michie et al., 1990), and a variety of other data sets from the Irvine Machine Learning Database such as "glass", <p> The parameter setting allows improved predictive accuracy at computational expense. 5 Conclusion Bayesian algorithms for learning class probability trees were presented and compared empirically with reim-plementations of existing approaches like Breiman et al.'s CART (Breiman et al., 1984), Quinlan's C4 <ref> (Quinlan, 1988) </ref>, and minimum encoding approaches. The Bayesian option trees and averaging algorithm gave significantly better accuracy and half-Brier score on predictions for a set of learning problems, but this was at the expense of computational cost.
Reference: <author> Quinlan, J., Compton, P., Horn, K., and Lazarus, L. </author> <year> (1987). </year> <title> Inductive knowledge acquisition: A case study. </title> <editor> In Quinlan, J., editor, </editor> <booktitle> Applications of Expert Systems. </booktitle> <publisher> Addison Wesley, London. </publisher>
Reference: <author> Quinlan, J. and Rivest, R. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248. </pages>
Reference-contexts: For instance, see Quinlan (Quinlan, 1988), Mingers (Mingers, 1989b), Buntine (Buntine, 1991b), Bahl, Brown, De Souza and Mercer (Bahl et al., 1989), Quinlan and Rivest <ref> (Quinlan and Rivest, 1989) </ref>, Crawford 3 (Crawford, 1989) and Chou (Chou, 1991). The standard approach to building a class probability tree consists of several stages: growing, pruning, and sometimes smoothing or averaging. <p> second process is subsequently employed to prune back the tree, for instance using resampling or hold-out methods, such as Breiman et al. (Breiman et al., 1984) or Crawford (Crawford, 1989), approximate significance tests such as Quinlan (Quinlan, 1988) or Mingers (Mingers, 1989a), or minimum encoding such as Quinlan and Rivest <ref> (Quinlan and Rivest, 1989) </ref>. The pruned tree may still have observed class probabilities at the leaves with zeroes for some classes, an unrealistic situation when noisy data is being used. <p> This penalty has the same effect as the "cost of encoding the cut-point" added by Quinlan and Rivest in their minimum encoding approach <ref> (Quinlan and Rivest, 1989) </ref>. While this evaluates the average quality of the cut-point test, we also need to select a cut point. I use the cut that maximizes the quality test Quality 1 (R &lt; cut). <p> The number of partitions to use in cross-validation cost complexity pruning can also be changed, and in general more partitions increases accuracy slightly. The minimum encoding approaches are (according to the purist) free of parameters. However, these approaches often strongly overprune, so Quinlan and Rivest <ref> (Quinlan and Rivest, 1989) </ref> introduce a parameter that allows lighter pruning. So all approaches, Bayesian and non-Bayesian, have parameters that allow more or less pruning that can be chosen depending on the amount of structure believed to exist in the data.
Reference: <author> Ripley, B. </author> <year> (1987). </year> <title> An introduction to statistical pattern recognition. </title> <booktitle> In Interactions in Artificial Intelligence and Statistical Methods, </booktitle> <pages> pages 176-187, </pages> <address> Aldershot, UK. </address> <publisher> Unicom, Gower Technical Press. </publisher>
Reference-contexts: This generic learning task is referred to as supervised learning in pattern recognition, and induction or empirical learning in machine learning, see for instance Quinlan (Quinlan, 1986). The statistical community uses techniques such as discriminant analysis and nearest neighbor methods, for instance described by Ripley <ref> (Ripley, 1987) </ref>. This prediction problem is often handled with partitioning classifiers.
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Enquiry. </title> <publisher> World Scientific. Section 7.2. </publisher>
Reference-contexts: This paper outlines a Bayesian approach to the problem of building trees that is related to the minimum encoding techniques of Wallace and Patrick (Wallace and Patrick, 1991) and Rissanen <ref> (Rissanen, 1989) </ref>. These two encoding approaches are based on the idea of the "most probable model", or its logarithmic counterpart, the minimum encoding. <p> IV : Tree structures are coded using bits for "node", "leaf", and "choice of test". This is a combination of type II and III priors together with "encoding" of cut-points. See Rissanen <ref> (Rissanen, 1989, p.165) </ref> or Wallace and Patrick (Wallace and Patrick, 1991) for details. The priors have been given in increasing strength, in the sense that type IV priors represent an extreme preference for simpler trees, but type I priors represent no such preference.
Reference: <author> Rodriguez, C. </author> <year> (1990). </year> <title> Objective bayesianism and geometry. </title> <editor> In Fougere, P., editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer. </publisher>
Reference-contexts: Entropic arguments like Rodriguez <ref> (Rodriguez, 1990) </ref> suggest that leaves should have more extreme probabilities (closer to zero and one) when the leaf is more infrequent (fewer examples occur at the leaves), for instance, lower down the tree.
Reference: <author> Stewart, L. </author> <year> (1987). </year> <title> Hierarchical Bayesian analysis using Monte Carlo integration: computing posterior distributions when there are many possible models. </title> <journal> The Statistician, </journal> <volume> 36 </volume> <pages> 211-219. </pages>
Reference-contexts: The beta function then has the effect of discounting leaves with small example numbers. 7 3 Methods To implement a full Bayesian approach, we need to consider averaging over possible models, as for instance approximated by Kwok and Carter (Kwok and Carter, 1990), or done by Stewart <ref> (Stewart, 1987) </ref> using Monte Carlo methods. This section introduces methods closer in spirit to Henrion (Henrion, 1990) who collects the dominant terms in the posterior sum. Fuller details of the methods described below are given by Buntine (Buntine, 1991b).
Reference: <author> Utgoff, P. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 161-186. </pages>
Reference-contexts: A related problem occurs when growing trees incrementally. In this regime, a tree needs to be updated given some additions to the training sample. Crawford (Crawford, 1989) shows that if we take the naive approach as done by Utgoff <ref> (Utgoff, 1989) </ref> and attempt to update the current tree so the "best" split according to the updated sample is taken at each node, the algorithm suffers from repeated restructuring. This occurs because the best split at a node vacillates while the sample at the node is still small.
Reference: <author> Wallace, C. and Patrick, J. </author> <year> (1991). </year> <title> Coding decision trees. </title> <type> Technical Report 151, </type> <institution> Monash University, Melbourne. </institution> <note> To appear, Machine Learning. </note>
Reference-contexts: This paper outlines a Bayesian approach to the problem of building trees that is related to the minimum encoding techniques of Wallace and Patrick <ref> (Wallace and Patrick, 1991) </ref> and Rissanen (Rissanen, 1989). These two encoding approaches are based on the idea of the "most probable model", or its logarithmic counterpart, the minimum encoding. <p> IV : Tree structures are coded using bits for "node", "leaf", and "choice of test". This is a combination of type II and III priors together with "encoding" of cut-points. See Rissanen (Rissanen, 1989, p.165) or Wallace and Patrick <ref> (Wallace and Patrick, 1991) </ref> for details. The priors have been given in increasing strength, in the sense that type IV priors represent an extreme preference for simpler trees, but type I priors represent no such preference.
Reference: <author> Weiss, S., Galen, R., and Tadepalli, P. </author> <year> (1990). </year> <title> Maximizing the predictive value of production rules. </title> <journal> Artificial Intelligence, </journal> <volume> 45 </volume> <pages> 47-71. 17 18 </pages>
Reference-contexts: These classifiers split the example space into partitions, for instance, Quinlan's ID3 (Quinlan, 1986) and Breiman, Friedland, Olshen and Stone's CART (Breiman et al., 1984) use classification trees to recursively partition the example space and Clark and Niblett's CN2 (Clark and Niblett, 1989) and Weiss et al.'s PVM <ref> (Weiss et al., 1990) </ref> use disjunctive rules (disjunctive rules also partition a space, but not recursively in the manner of trees). Tree algorithms build trees such as the ones shown in Figure 1.
References-found: 31

