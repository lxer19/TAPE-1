URL: ftp://ftp.cs.wisc.edu/sohi/papers/1995/ieee-proc.superscalar.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Email: email: jes@ece.wisc.edu  email: sohi@cs.wisc.edu  
Title: The Microarchitecture of Superscalar Processors  
Author: James E. Smith ph: ()-- Gurindar S. Sohi ph: ()-- 
Date: Aug. 20, 1995  
Address: 1415 Johnson Drive Madison, WI 53706  1210 W. Dayton Madison, WI 53706  
Affiliation: Department of Electrical and Computer Engineering  Computer Sciences Department  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Control dependences due to an incrementing program counter are the simplest, and we deal with them first. One can view the static program as a collection of basic blocks, where a basic block is a contiguous block of instructions, with a single entry point and a single exit point <ref> [1] </ref>. In the assembly code of Fig. 1, there are three basic blocks.
Reference: [2] <author> G. Amdahl, et al, </author> <title> ``Architecture of the IBM System/360,'' </title> <journal> IBM J. Res. Devel., </journal> <volume> vol. 8, </volume> <pages> pp. 87-101, </pages> <month> April </month> <year> 1964. </year>
Reference-contexts: Then, software was developed for each instruction set. It did not take long, however, until it became apparent that there were significant advantages to designing instruction sets that were compatible with previous generations and with different models of the same generation <ref> [2] </ref>.
Reference: [3] <author> D. W. Anderson, F. J. Sparacio, and R. M. Tomasulo, </author> <title> ``The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling,'' IBM Journal of Research and Development, </journal> <volume> vol. 11, </volume> <pages> pp. 8-24, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Although it was capable of sustained execution of only a single instruction per cycle, the 6600's instruction set, parallel processing units, and dynamic instruction scheduling are similar to the superscalar microprocessors of today. Another remarkable processor of the 1960s was the IBM 360/91 <ref> [3] </ref>. The 360/91 was heavily pipelined, and provided a dynamic instruction issuing mechanism, known as Tomasulo's algorithm [63] after its inventor. <p> with information for each memory location, as we did in the case of registers, the general approach is to keep information only for a currently active subset of the memory locations, i.e., memory locations with currently pending memory operations, and search this subset associatively when memory needs to be accessed <ref> [3, 7, 19, 46] </ref>. Some superscalar processors only allow single memory operations per cycle, but this is rapidly becoming a performance bottleneck. To allow multiple memory requests to be serviced simultaneously, the memory hierarchy has to be multiported.
Reference: [4] <author> T. Asprey, et al, </author> <title> ``Performance Features of the PA7100 Microprocessor,'' </title> <journal> IEEE Micro, </journal> <volume> vol. 13, </volume> <pages> pp. 22-35, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Most microprocessors integrate the primary cache on the same chip as the processor; notable exceptions are some of processors developed by HP <ref> [4] </ref> and the high-end IBM POWER series [65].
Reference: [5] <author> M. C. August, G. M. Brost, C. C. Hsiung, and A. J. Schiffleger, </author> <title> ``Cray X-MP: The Birth of a Supercomputer,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 45-54, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: The pipeline initiation rate remained at one instruction per cycle for many years and was often perceived to be a serious practical bottleneck. Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing [28, 49] and multiprocessing <ref> [5, 6] </ref>. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54]. <p> One such method is a variation of the time-tested multiprocessing method. Some supercomputer and mini-supercomputer vendors have already implemented approaches for automatic parallelization of high level language programs <ref> [5, 33] </ref>. With chip resources being sufficient to allow multiple processors to be integrated on a chip, multiprocessing, once the domain of supercomputers and mainframes, is being considered for high-end microprocessor design [25, 42].
Reference: [6] <author> C. G. Bell, ``Multis: </author> <title> a New Class of Multiprocessor Computers,'' </title> <journal> Science, </journal> <volume> vol. 228, </volume> <pages> pp. 462-467, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: The pipeline initiation rate remained at one instruction per cycle for many years and was often perceived to be a serious practical bottleneck. Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing [28, 49] and multiprocessing <ref> [5, 6] </ref>. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54].
Reference: [7] <author> L. J. Boland, G. D. Granito, A. U. Marcotte, B. U. Messina, and J. W. Smith, </author> <title> ``The IBM System/360 Model 91: Storage System,'' </title> <journal> IBM Journal, </journal> <volume> vol. 11, </volume> <pages> pp. 54-68, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: with information for each memory location, as we did in the case of registers, the general approach is to keep information only for a currently active subset of the memory locations, i.e., memory locations with currently pending memory operations, and search this subset associatively when memory needs to be accessed <ref> [3, 7, 19, 46] </ref>. Some superscalar processors only allow single memory operations per cycle, but this is rapidly becoming a performance bottleneck. To allow multiple memory requests to be serviced simultaneously, the memory hierarchy has to be multiported.
Reference: [8] <editor> Werner Bucholz, ed., </editor> <title> Planning a Computer System. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1962. </year>
Reference-contexts: With simple pipelining, only one instruction at a time is initiated into the pipeline, but multiple instructions may be in some phase of execution concurrently. Pipelining was initially developed in the late 1950s <ref> [8] </ref> and became a mainstay of large scale computers during the 1960s. The CDC 6600 [61] used a degree of pipelining, but achieved most of its ILP through parallel functional units.
Reference: [9] <author> B. </author> <title> Case, ``Intel Reveals Pentium Implementation Details,'' </title> <type> Microprocessor Report, </type> <pages> pp. 9-13, </pages> <month> Mar. 29, </month> <year> 1993. </year>
Reference-contexts: Furthermore, transfers from the higher levels to the primary cache are in the form of lines, containing multiple consecutive words of data. Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory <ref> [9, 29, 58] </ref>, or by making multiple serial requests during the same cycle [65]. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking [37, 58].
Reference: [10] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman, </author> <title> ``A VLIW Architecture for a Trace Scheduling Compiler,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 967-979, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Where do we go from that point? One school of thought believes that the Very Large Instruction Word (VLIW) model <ref> [10, 17, 48] </ref> will become the paradigm of choice. The VLIW model is derived from the sequential execution model: control sequences from instruction to instruction, just like in the sequential model discussed in this paper.
Reference: [11] <author> T. M. Conte, P. M. Mills, K. N. Menezes, and B. A. Patel, </author> <title> ``Optimization of Instruction Fetch Mechanisms for High Issue Rates,'' </title> <booktitle> Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 333-344, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Some of the superscalar processors have taken special steps to allow wider fetches in this case, for example by fetching from two adjacent cache lines simultaneously [20, 65]. A discussion of a number of alternatives for high bandwidth instruction fetching appears in <ref> [11] </ref>. To help smooth out the instruction fetch irregularities caused by cache misses and branches, there is often an instruction buffer (shown in Fig. 4) to hold a number of fetched instructions.
Reference: [12] <author> K. Diefendorff and M. Allen, </author> <title> ``Organization of the Motorola 88110 Superscalar RISC Microprocessor,'' </title> <journal> IEEE Micro, </journal> <volume> vol. 12, </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: If there is an interrupt, the control information used to adjust the logical-to-physical mapping table so that the mapping reflects the correct precise state. Although the checkpoint/history buffer method has been used in some superscalar processors <ref> [12] </ref>, the reorder buffer technique is by far the more popular technique because, in addition to providing a precise state, the reorder buffer method also helps implement the register renaming function [57]. 3.6.
Reference: [13] <author> P. K. Dubey and M. J. Flynn, </author> <title> ``Optimal Pipelining,'' </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 8, </volume> <pages> pp. 10-19, </pages> <year> 1990. </year>
Reference-contexts: This will affect the clock period and causes some very important design tradeoffs regarding the degree of pipelining and the width of parallel instruction issue. However, we will not discuss these issues in detail here; discussions of these types of tradeoffs can be found in <ref> [13, 34, 38] </ref>. 5 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh instr. fetch & branch predict. instr. dispatch instr. issue execution instr. commit & reorder instr. static program window of execution Fig. 3. A conceptual figure of superscalar execution.
Reference: [14] <author> P. K. Dubey, K. O'brien, K. O'brien, and C. Barton, </author> <title> ``Single-Program Speculative Multithreading (SPSM) Architecture: Compiler-assisted Fine-Grained Multithreading,'' </title> <booktitle> Proc. Int. Conf. on Parallel Architectures and Compilation Techniques (PACT '95), </booktitle> <pages> pp. 109-121, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: As with traditional multiprocessing, single-chip multiprocessors will require enormous amounts compiler support to create a parallel version of a program statically. Looking even farther ahead, more radical approaches that support multiple flows of control are being studied <ref> [14, 59] </ref>. In these approaches, individual processing units are much more highly integrated than in a traditional multiprocessor, and the flow control mechanism is built into the hardware, as well.
Reference: [15] <author> M. Dubois, C. Scheurich, and F. A. Briggs, </author> <title> ``Synchronization, Coherence, and Event Ordering in Multiprocessors,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 21, </volume> <pages> pp. 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: With this technique, the speculative state is maintained in a reorder buffer; to commit an instruction, its result has to hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3 Some complex memory ordering issues can arise in shared memory multiprocessor implementations <ref> [15] </ref>, but these are beyond the scope of this paper. 14 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh to memory hazard control load address compare address issue instruction store address buffers buffers loads stores & translation add address Fig. 11. Memory hazard detection logic. Store addresses are buffered in a queue (FIFO).
Reference: [16] <author> K. I. Farkas and N. P. Jouppi, </author> <title> ``Complexity/Performance Tradeoffs with Non-Blocking Loads,'' </title> <booktitle> Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 211-222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Further accesses to the missing line must be made to wait, but other accesses may proceed. Miss Handling Status Registers (MHSRs) are used to track the status of outstanding cache misses, and allow multiple requests to the memory hierarchy to be overlapped <ref> [16, 37, 58] </ref>. 3.5. Committing State The final phase of the lifetime of an instruction is the commit or the retire phase, where the effects of the instruction are allowed to modify the logical process state.
Reference: [17] <author> J. A. Fisher and B. R. Rau, </author> <title> ``Instruction-Level Parallel Processing,'' </title> <booktitle> Science, </booktitle> <pages> pp. 1233-1241, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Where do we go from that point? One school of thought believes that the Very Large Instruction Word (VLIW) model <ref> [10, 17, 48] </ref> will become the paradigm of choice. The VLIW model is derived from the sequential execution model: control sequences from instruction to instruction, just like in the sequential model discussed in this paper.
Reference: [18] <author> J. A. Fisher and S. M. Freudenberger, </author> <title> ``Predicting Conditional Branch Directions from Previous Runs of a Program,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <year> 1992. </year>
Reference-contexts: Profiling information -- program execution statistics collected during a previous run of the program -- can also be used by the compiler as an aid for static branch prediction <ref> [18] </ref>. Other predictors use dynamic information, i.e., information that becomes available as the program executes. This is usually information regarding the past history of branch outcomes -- either the specific branch being predicted, other branches leading up to it, or both.
Reference: [19] <author> M. Franklin and G. S. Sohi, ``ARB: </author> <title> A Hardware Mechanism for Dynamic Memory Disambiguation,'' </title> <note> to appear in IEEE Transactions on Computers. </note>
Reference-contexts: with information for each memory location, as we did in the case of registers, the general approach is to keep information only for a currently active subset of the memory locations, i.e., memory locations with currently pending memory operations, and search this subset associatively when memory needs to be accessed <ref> [3, 7, 19, 46] </ref>. Some superscalar processors only allow single memory operations per cycle, but this is rapidly becoming a performance bottleneck. To allow multiple memory requests to be serviced simultaneously, the memory hierarchy has to be multiported.
Reference: [20] <author> G. F. Grohoski, J. A. Kahle, L. E. Thatcher, and C. R. Moore, </author> <title> ``Branch and Fixed Point Instruction Execution Units,'' in IBM RISC System/6000 Technology, </title> <address> Austin, TX. </address>
Reference-contexts: Some of the superscalar processors have taken special steps to allow wider fetches in this case, for example by fetching from two adjacent cache lines simultaneously <ref> [20, 65] </ref>. A discussion of a number of alternatives for high bandwidth instruction fetching appears in [11]. To help smooth out the instruction fetch irregularities caused by cache misses and branches, there is often an instruction buffer (shown in Fig. 4) to hold a number of fetched instructions.
Reference: [21] <author> G. F. Grohoski, </author> <title> ``Machine Organization of the IBM RISC System/6000 processor,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 37-58, </pages> <month> January </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear <ref> [21, 43, 54] </ref>. By initiating more than one instruction at a time into multiple pipelines, superscalar processors break the single-instruction-per-cycle bottleneck. In the years since its introduction, the superscalar approach has become the standard method for implementing high performance microprocessors. 1.2. <p> There are two register renaming methods in common usage. In the first, there is a physical register file larger than the logical register file. A mapping table is used to associate a physical register with the current value of a logical register <ref> [21, 30, 45, 46] </ref>. Instructions are decoded and register renaming is performed in sequential program 8 order. <p> For example, only the registers loaded from memory may be renamed. This allows the load/store queue to "slip" ahead of the other instruction queues, fetching memory data in advance of when it is needed. Some earlier superscalar implementations used this method <ref> [21, 43, 54] </ref>. Reservation Stations With reservation stations (see Fig. 10), which were first proposed as a part of Tomasulo's algorithm [63], instructions may issue out of order; there is no strict FIFO ordering. Consequently, all of the reservation stations simultaneously monitor their source operands for data availability.
Reference: [22] <author> L. Gwennap, </author> <title> ``PPC 604 Powers Past Pentium,'' </title> <type> Microprocessor Report, </type> <pages> pp. 5-8, </pages> <month> Apr. 18, </month> <year> 1994. </year>
Reference-contexts: Finding the target address can be sped up by having a branch target buffer that holds the target address that was used the last time the branch was executed. An example is the Branch Target Address Cache used in the PowerPC 604 <ref> [22] </ref>. Transferring control When there is a taken (or predicted taken) branch there is often at least a clock cycle delay in recognizing the branch, modifying the program counter, and fetching instructions from the target address. This delay can result in pipeline bubbles unless some steps are taken.
Reference: [23] <author> L. Gwennap, </author> <title> ``MIPS R10000 Uses Decoupled Architecture,'' </title> <type> Microprocessor Report, </type> <pages> pp. 18-22, </pages> <month> Oct. 24, </month> <year> 1994. </year>
Reference-contexts: Three Superscalar Microprocessors In this section we discuss three current superscalar microprocessors in light of the above framework. The three were chosen to cover the spectrum of superscalar implementations as much as possible. They are the MIPS R10000 <ref> [23] </ref>, which fits the "typical" framework described above, the DEC Alpha 21164 [24] which strives for greater simplicity, and the AMD K5 [51] which implements a more complex and older instruction set than the other two -- the Intel x86. 4.1.
Reference: [24] <author> L. Gwennap, </author> <title> ``Digital Leads the Pack with the 21164,'' </title> <type> Microprocessor Report, </type> <pages> pp. </pages> <address> 1,6-10, Sept. 12, </address> <year> 1994. </year>
Reference-contexts: The three were chosen to cover the spectrum of superscalar implementations as much as possible. They are the MIPS R10000 [23], which fits the "typical" framework described above, the DEC Alpha 21164 <ref> [24] </ref> which strives for greater simplicity, and the AMD K5 [51] which implements a more complex and older instruction set than the other two -- the Intel x86. 4.1. MIPS R10000 The MIPS R10000 does extensive dynamic scheduling and is very similar to our "typical" superscalar processor described above.
Reference: [25] <author> L. Gwennap, </author> <title> ``Architects Debate VLIW, Single-Chip MP,'' </title> <type> Microprocessor Report, </type> <pages> pp. 20-21, </pages> <month> December 5, </month> <year> 1994. </year>
Reference-contexts: Some supercomputer and mini-supercomputer vendors have already implemented approaches for automatic parallelization of high level language programs [5, 33]. With chip resources being sufficient to allow multiple processors to be integrated on a chip, multiprocessing, once the domain of supercomputers and mainframes, is being considered for high-end microprocessor design <ref> [25, 42] </ref>. If multiprocessing is to exploit instruction level parallelism at the very small grain size that is implied by "instruction level", suitable support has to be provided to keep the inter-processor synchronization and communication overhead extremely low.
Reference: [26] <author> E. Hao, P.-Y. Chang, and Y. N. Patt, </author> <title> ``The Effect of Speculatively Updating Branch History on Branch Prediction Accuracy, Revisited,'' </title> <booktitle> Proc. 27th Int. Symp. on Microarchitecture, </booktitle> <pages> pp. 228-232, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Consequently, a counter's value summarizes the dominant outcome of recent branch executions. At some time after a prediction has been made, the actual branch outcome is evaluated. Dynamic branch history information can then be updated. (It could also be updated speculatively, at the time the prediction was made <ref> [26, 60] </ref> ). If the prediction was incorrect, instruction fetching must be re-directed to the correct path. Furthermore, if instructions were processed speculatively based on the prediction, they must be purged and their results must be nullified. The process of speculatively executing instruction is described in more detail later.
Reference: [27] <author> J. Hennessy, N. Jouppi, F. Baskett, T. Gross, and J. Gill, </author> <title> ``Hardware/Software Tradeoffs for Increased Performance,'' </title> <booktitle> Proc. Int. Symp. on Arch. Support for Prog. Lang. and Operating Sys., </booktitle> <pages> pp. 2-11, </pages> <month> March </month> <year> 1982. </year>
Reference-contexts: Perhaps the most common solution is to use the instruction buffer with its stockpiled instructions to mask the delay; more complex buffers may contain instructions from both the taken and the not taken paths of the branch. Some of the earlier RISC instruction sets used delayed branches <ref> [27, 47] </ref>. That is, a branch did not take effect until the instruction after the branch. With this method, the fetch of target instructions could be overlapped with the execution of the instruction following the branch.
Reference: [28] <author> R. G. Hintz and B. P. Tate, </author> <title> ``Control Data STAR-100 Processor Design,'' COMPCON, </title> <editor> p. </editor> <volume> 396, </volume> <month> Sept. </month> <year> 1972. </year>
Reference-contexts: The pipeline initiation rate remained at one instruction per cycle for many years and was often perceived to be a serious practical bottleneck. Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing <ref> [28, 49] </ref> and multiprocessing [5, 6]. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54].
Reference: [29] <author> P. Y. T. Hsu, </author> <title> ``Design of the R8000 Microprocessor,'' </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Furthermore, transfers from the higher levels to the primary cache are in the form of lines, containing multiple consecutive words of data. Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory <ref> [9, 29, 58] </ref>, or by making multiple serial requests during the same cycle [65]. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking [37, 58].
Reference: [30] <author> W. W. Hwu and Y. N Patt, ``HPSm, </author> <title> a High Performance Restricted Data Flow Architecture Having Minimal Functionality,'' </title> <booktitle> Proc. 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 297-307, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: There are two register renaming methods in common usage. In the first, there is a physical register file larger than the logical register file. A mapping table is used to associate a physical register with the current value of a logical register <ref> [21, 30, 45, 46] </ref>. Instructions are decoded and register renaming is performed in sequential program 8 order.
Reference: [31] <author> W. W. Hwu and Y. N. Patt, </author> <title> ``Checkpoint Repair for High-Performance Out-of-Order Execution Machines,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 1496-1514, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: In the first technique, the state of the machine at certain points is saved, or checkpointed, in either a history buffer or a checkpoint <ref> [31, 55] </ref>. Instructions update the state of the machine as they execute and when a precise state is needed, it is recovered from the history buffer.
Reference: [32] <author> M. Johnson, </author> <title> Superscalar Design. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: In addition, there is a buffer with one entry per active instruction (i.e., an instruction that been dispatched for execution but which has not yet committed.) This buffer is typically referred to as a reorder buffer <ref> [32, 55, 57] </ref> because it is also used to maintain proper instruction ordering for precise interrupts. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh add r3,r3,4 r1 r3 R8 R9 mapping table: free list: after: R6 R13 before: r0 r2 r4 R5 mapping table: free list: R7 R7 R2 R6 R13 Fig. 5.
Reference: [33] <author> T. Jones, </author> <title> ``Engineering Design of the Convex C2,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 36-44, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: One such method is a variation of the time-tested multiprocessing method. Some supercomputer and mini-supercomputer vendors have already implemented approaches for automatic parallelization of high level language programs <ref> [5, 33] </ref>. With chip resources being sufficient to allow multiple processors to be integrated on a chip, multiprocessing, once the domain of supercomputers and mainframes, is being considered for high-end microprocessor design [25, 42].
Reference: [34] <author> N. P. Jouppi and D. W. Wall, </author> <title> ``Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines,'' </title> <booktitle> in Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: This will affect the clock period and causes some very important design tradeoffs regarding the degree of pipelining and the width of parallel instruction issue. However, we will not discuss these issues in detail here; discussions of these types of tradeoffs can be found in <ref> [13, 34, 38] </ref>. 5 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh instr. fetch & branch predict. instr. dispatch instr. issue execution instr. commit & reorder instr. static program window of execution Fig. 3. A conceptual figure of superscalar execution.
Reference: [35] <author> D. R. Kaeli and P. G. Emma, </author> <title> ``Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns,'' </title> <booktitle> Proc. 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 34-42, </pages> <month> May, </month> <year> 1991. </year>
Reference-contexts: That is, there is a dependence between the branch instruction and a preceding, uncompleted instruction. Rather than wait, the outcome of a conditional branch can be predicted using one of several types of branch prediction methods <ref> [35, 40, 44, 53, 66] </ref>.
Reference: [36] <author> R. M. Keller, </author> <title> ``Look-Ahead Processors,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 7, </volume> <pages> pp. 66-72, </pages> <month> December </month> <year> 1975. </year>
Reference-contexts: This is accomplished in the decode/rename/dispatch phase (see Fig. 4) by replacing the logical register name in an instruction's execution tuple (i.e., the register designator) with the new name of the physical storage location; the register is therefore said to be renamed <ref> [36] </ref>. There are two register renaming methods in common usage. In the first, there is a physical register file larger than the logical register file. A mapping table is used to associate a physical register with the current value of a logical register [21, 30, 45, 46].
Reference: [37] <author> D. Kroft, </author> <title> ``Lockup-Free Instruction Fetch/Prefetch Cache Organization,'' </title> <booktitle> Proc. 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory [9, 29, 58], or by making multiple serial requests during the same cycle [65]. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking <ref> [37, 58] </ref>. That is, if a memory request misses in the data cache, other processing operations, including further memory requests, should be allowed to proceed. <p> Further accesses to the missing line must be made to wait, but other accesses may proceed. Miss Handling Status Registers (MHSRs) are used to track the status of outstanding cache misses, and allow multiple requests to the memory hierarchy to be overlapped <ref> [16, 37, 58] </ref>. 3.5. Committing State The final phase of the lifetime of an instruction is the commit or the retire phase, where the effects of the instruction are allowed to modify the logical process state.
Reference: [38] <author> S. R. Kunkel and J. E. Smith, </author> <title> ``Optimal Pipelining in Supercomputers,'' </title> <booktitle> Proc. 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 404-413, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: This will affect the clock period and causes some very important design tradeoffs regarding the degree of pipelining and the width of parallel instruction issue. However, we will not discuss these issues in detail here; discussions of these types of tradeoffs can be found in <ref> [13, 34, 38] </ref>. 5 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh instr. fetch & branch predict. instr. dispatch instr. issue execution instr. commit & reorder instr. static program window of execution Fig. 3. A conceptual figure of superscalar execution.
Reference: [39] <author> M. S. Lam and R. Wilson, </author> <title> ``Limits of Control Flow on Parallelism,'' </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 46-57, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Rather than sequence through the program instruction-by-instruction, in an attempt to create the dynamic instruction sequence from the static representation, more parallel forms of sequencing may be used. In <ref> [39] </ref> it is shown that allowing multiple control flows provides the potential for substantial performance gains. That is, if multiple program counters can be used for fetching instructions, then the effective window of execution from which independent instructions can be chosen for parallel execution can be much wider.
Reference: [40] <author> J. K. F. Lee and A. J. Smith, </author> <title> ``Branch Prediction Strategies and Branch Target Buffer Design,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 17, </volume> <pages> pp. 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: That is, there is a dependence between the branch instruction and a preceding, uncompleted instruction. Rather than wait, the outcome of a conditional branch can be predicted using one of several types of branch prediction methods <ref> [35, 40, 44, 53, 66] </ref>. <p> This is usually information regarding the past history of branch outcomes -- either the specific branch being predicted, other branches leading up to it, or both. This branch history is saved in a branch history table or branch prediction table <ref> [40, 53] </ref>, or may be appended to the instruction cache line that contains the branch.
Reference: [41] <author> C. R. Moore, </author> <title> ``The PowerPC 601 Microprocessor,'' </title> <booktitle> Proc. Compcon 1993, </booktitle> <pages> pp. 109-116, </pages> <month> Februrary </month> <year> 1993. </year>
Reference-contexts: To support this high instruction fetch bandwidth, it has become almost mandatory to separate the instruction cache from the data cache (although the PowerPC 601 <ref> [41] </ref> provides an interesting counter example). The number of instructions fetched per cycle should at least match the peak instruction decode and execution rate and is usually somewhat higher.
Reference: [42] <author> B. A. Nayfeh and K. Olukotun, </author> <title> ``Exploring the Design Space for a Shared-Cache Multiprocessor,'' </title> <booktitle> Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 166-175, </pages> <month> April, </month> <year> 1994. </year>
Reference-contexts: Some supercomputer and mini-supercomputer vendors have already implemented approaches for automatic parallelization of high level language programs [5, 33]. With chip resources being sufficient to allow multiple processors to be integrated on a chip, multiprocessing, once the domain of supercomputers and mainframes, is being considered for high-end microprocessor design <ref> [25, 42] </ref>. If multiprocessing is to exploit instruction level parallelism at the very small grain size that is implied by "instruction level", suitable support has to be provided to keep the inter-processor synchronization and communication overhead extremely low.
Reference: [43] <author> R. R. Oehler and R. D. Groves, </author> <title> ``IBM RISC System/6000 Processor Architecture,'' </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 34, </volume> <pages> pp. 23-36, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear <ref> [21, 43, 54] </ref>. By initiating more than one instruction at a time into multiple pipelines, superscalar processors break the single-instruction-per-cycle bottleneck. In the years since its introduction, the superscalar approach has become the standard method for implementing high performance microprocessors. 1.2. <p> For example, only the registers loaded from memory may be renamed. This allows the load/store queue to "slip" ahead of the other instruction queues, fetching memory data in advance of when it is needed. Some earlier superscalar implementations used this method <ref> [21, 43, 54] </ref>. Reservation Stations With reservation stations (see Fig. 10), which were first proposed as a part of Tomasulo's algorithm [63], instructions may issue out of order; there is no strict FIFO ordering. Consequently, all of the reservation stations simultaneously monitor their source operands for data availability.
Reference: [44] <author> S.-T. Pan, K. So, and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pp. 76-84, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: That is, there is a dependence between the branch instruction and a preceding, uncompleted instruction. Rather than wait, the outcome of a conditional branch can be predicted using one of several types of branch prediction methods <ref> [35, 40, 44, 53, 66] </ref>.
Reference: [45] <author> Y. N. Patt, W. W. Hwu, and M. Shebanow, ``HPS, </author> <title> A New Microarchitecture: Rationale and Introduction,'' </title> <booktitle> Proc. 18th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 103-108, </pages> <month> December </month> <year> 1985. </year> <month> 23 </month>
Reference-contexts: There are two register renaming methods in common usage. In the first, there is a physical register file larger than the logical register file. A mapping table is used to associate a physical register with the current value of a logical register <ref> [21, 30, 45, 46] </ref>. Instructions are decoded and register renaming is performed in sequential program 8 order.
Reference: [46] <author> Y. N. Patt, S. W. Melvin, W. W. Hwu, and M. Shebanow, </author> <title> ``Critical Issues Regarding HPS, A High Perfor--mance Microarchitecture,'' </title> <booktitle> Proc. 18th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 109-116, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: There are two register renaming methods in common usage. In the first, there is a physical register file larger than the logical register file. A mapping table is used to associate a physical register with the current value of a logical register <ref> [21, 30, 45, 46] </ref>. Instructions are decoded and register renaming is performed in sequential program 8 order. <p> with information for each memory location, as we did in the case of registers, the general approach is to keep information only for a currently active subset of the memory locations, i.e., memory locations with currently pending memory operations, and search this subset associatively when memory needs to be accessed <ref> [3, 7, 19, 46] </ref>. Some superscalar processors only allow single memory operations per cycle, but this is rapidly becoming a performance bottleneck. To allow multiple memory requests to be serviced simultaneously, the memory hierarchy has to be multiported.
Reference: [47] <author> D. A. Patterson and C. H. Sequin, </author> <title> ``RISC 1: A Reduced Instruction Set VLSI Computer,'' </title> <booktitle> Proc. 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 443-459, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Perhaps the most common solution is to use the instruction buffer with its stockpiled instructions to mask the delay; more complex buffers may contain instructions from both the taken and the not taken paths of the branch. Some of the earlier RISC instruction sets used delayed branches <ref> [27, 47] </ref>. That is, a branch did not take effect until the instruction after the branch. With this method, the fetch of target instructions could be overlapped with the execution of the instruction following the branch.
Reference: [48] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. Towle, </author> <title> ``The Cydra 5 Departmental Supercomputer: Design Philosophies, Decisions, and Tradeoffs,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 12-35, </pages> <month> January, </month> <year> 1989. </year>
Reference-contexts: Where do we go from that point? One school of thought believes that the Very Large Instruction Word (VLIW) model <ref> [10, 17, 48] </ref> will become the paradigm of choice. The VLIW model is derived from the sequential execution model: control sequences from instruction to instruction, just like in the sequential model discussed in this paper.
Reference: [49] <author> R. M. Russel, </author> <title> ``The CRAY-1 Computer System,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, </volume> <pages> pp. 63-72, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: The pipeline initiation rate remained at one instruction per cycle for many years and was often perceived to be a serious practical bottleneck. Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing <ref> [28, 49] </ref> and multiprocessing [5, 6]. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54].
Reference: [50] <author> H. Schorr, </author> <title> ``Design Principles for a High Performance System,'' </title> <booktitle> in Proc. of the Symposium on Computers and Automata, </booktitle> <address> New York, NY, </address> <pages> pp. 165-192, </pages> <month> April </month> <year> 1971. </year>
Reference-contexts: Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing [28, 49] and multiprocessing [5, 6]. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s <ref> [50, 62] </ref>, none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54]. By initiating more than one instruction at a time into multiple pipelines, superscalar processors break the single-instruction-per-cycle bottleneck.
Reference: [51] <author> M. Slater, </author> <title> ``AMD's K5 Designed to Outrun Pentium,'' </title> <type> Microprocessor Report, </type> <pages> pp. </pages> <address> 1,6-11, Oct. 24, </address> <year> 1994. </year>
Reference-contexts: The three were chosen to cover the spectrum of superscalar implementations as much as possible. They are the MIPS R10000 [23], which fits the "typical" framework described above, the DEC Alpha 21164 [24] which strives for greater simplicity, and the AMD K5 <ref> [51] </ref> which implements a more complex and older instruction set than the other two -- the Intel x86. 4.1. MIPS R10000 The MIPS R10000 does extensive dynamic scheduling and is very similar to our "typical" superscalar processor described above. In fact, Fig. 4 is modeled after the R10000 design.
Reference: [52] <author> A. J. Smith, </author> <title> ``Cache Memories,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473-530, </pages> <month> Sept. </month> <year> 1982. </year>
Reference-contexts: Instruction Fetching and Branch Prediction The instruction fetch phase of superscalar processing supplies instructions to the rest of the processing pipeline. An instruction cache, which is a small memory containing recently-used instructions <ref> [52] </ref>, is used in almost all current processors, superscalar or not, to reduce the latency and increase the bandwidth of the instruction fetch process. <p> After address calculation, an address translation may be required to generate a physical address. A cache of translation descriptors of recently accessed pages, the translation lookaside buffer (TLB), is used to speed up this address translation process <ref> [52] </ref>. Once a valid memory address has been obtained, the load or store operation can be submitted to the memory.
Reference: [53] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies,'' </title> <booktitle> Proc. 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: That is, there is a dependence between the branch instruction and a preceding, uncompleted instruction. Rather than wait, the outcome of a conditional branch can be predicted using one of several types of branch prediction methods <ref> [35, 40, 44, 53, 66] </ref>. <p> This is usually information regarding the past history of branch outcomes -- either the specific branch being predicted, other branches leading up to it, or both. This branch history is saved in a branch history table or branch prediction table <ref> [40, 53] </ref>, or may be appended to the instruction cache line that contains the branch. <p> Within the table previous branch history is often recorded by using multiple bits, typi-cally implemented as small counters, so that the results of several past branch executions can be summarized <ref> [53] </ref>. The counters are incremented on a taken branch (stopping at a maximum value) and are decremented on a not-taken branch (stopping at a minimum value). Consequently, a counter's value summarizes the dominant outcome of recent branch executions.
Reference: [54] <author> J. E. Smith, et al, </author> <title> ``The ZS-1 Central Processor,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), </booktitle> <pages> pp. 199-204, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Although some processors capable of multiple instruction initiation were considered during the '60s and '70s [50, 62], none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear <ref> [21, 43, 54] </ref>. By initiating more than one instruction at a time into multiple pipelines, superscalar processors break the single-instruction-per-cycle bottleneck. In the years since its introduction, the superscalar approach has become the standard method for implementing high performance microprocessors. 1.2. <p> For example, only the registers loaded from memory may be renamed. This allows the load/store queue to "slip" ahead of the other instruction queues, fetching memory data in advance of when it is needed. Some earlier superscalar implementations used this method <ref> [21, 43, 54] </ref>. Reservation Stations With reservation stations (see Fig. 10), which were first proposed as a part of Tomasulo's algorithm [63], instructions may issue out of order; there is no strict FIFO ordering. Consequently, all of the reservation stations simultaneously monitor their source operands for data availability.
Reference: [55] <author> J. E. Smith and A. R. Pleszkun, </author> <title> ``Implementing Precise Interrupts in Pipelined Processors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 562-573, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: In addition, there is a buffer with one entry per active instruction (i.e., an instruction that been dispatched for execution but which has not yet committed.) This buffer is typically referred to as a reorder buffer <ref> [32, 55, 57] </ref> because it is also used to maintain proper instruction ordering for precise interrupts. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh add r3,r3,4 r1 r3 R8 R9 mapping table: free list: after: R6 R13 before: r0 r2 r4 R5 mapping table: free list: R7 R7 R2 R6 R13 Fig. 5. <p> In the first technique, the state of the machine at certain points is saved, or checkpointed, in either a history buffer or a checkpoint <ref> [31, 55] </ref>. Instructions update the state of the machine as they execute and when a precise state is needed, it is recovered from the history buffer.
Reference: [56] <author> M. D. Smith, M. Johnson, and M. A. Horowitz, </author> <title> ``Limits on Multiple Instruction Issue,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pp. 290-302, </pages> <year> 1989. </year>
Reference-contexts: Meanwhile enough information concerning the original sequential order is retained so that the instruction stream can conceptually be squeezed back together again should there be need for a precise interrupt. A number of studies have been done to determine the performance of superscalar methods, for example <ref> [56, 64] </ref>. Because the hardware and software assumptions and benchmarks vary widely, so do the potential speedups -- ranging from close to 1 for some program/hardware combinations to many 100s for others.
Reference: [57] <author> G. S. Sohi, </author> <title> ``Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In addition, there is a buffer with one entry per active instruction (i.e., an instruction that been dispatched for execution but which has not yet committed.) This buffer is typically referred to as a reorder buffer <ref> [32, 55, 57] </ref> because it is also used to maintain proper instruction ordering for precise interrupts. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh add r3,r3,4 r1 r3 R8 R9 mapping table: free list: after: R6 R13 before: r0 r2 r4 R5 mapping table: free list: R7 R7 R2 R6 R13 Fig. 5. <p> When all the operands are ready in the reservation station, the instruction may issue (subject to hardware resource availability). Reservations may be partitioned according to instruction type (to reduce data paths)[45, 63] or may be pooled into a single large block <ref> [57] </ref>. Finally, some recent reservation station implementations do not hold the actual source data, but instead hold pointers to where the data can be found, e.g. in the register file or a reorder buffer entry. 3.4. Handling Memory Operations Memory operations need special treatment in superscalar processors. <p> Although the checkpoint/history buffer method has been used in some superscalar processors [12], the reorder buffer technique is by far the more popular technique because, in addition to providing a precise state, the reorder buffer method also helps implement the register renaming function <ref> [57] </ref>. 3.6. The Role of Software Though a major selling point of superscalar processors is to speed up the execution of existing program binaries, their performance can be enhanced if new, optimized binaries can be created 4 .
Reference: [58] <author> G. S. Sohi and M. Franklin, </author> <title> ``High-Bandwidth Data Memory Systems for Superscalar Processors,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 53-62, </pages> <month> April, </month> <year> 1991. </year>
Reference-contexts: Furthermore, transfers from the higher levels to the primary cache are in the form of lines, containing multiple consecutive words of data. Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory <ref> [9, 29, 58] </ref>, or by making multiple serial requests during the same cycle [65]. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking [37, 58]. <p> Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory [9, 29, 58], or by making multiple serial requests during the same cycle [65]. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking <ref> [37, 58] </ref>. That is, if a memory request misses in the data cache, other processing operations, including further memory requests, should be allowed to proceed. <p> Further accesses to the missing line must be made to wait, but other accesses may proceed. Miss Handling Status Registers (MHSRs) are used to track the status of outstanding cache misses, and allow multiple requests to the memory hierarchy to be overlapped <ref> [16, 37, 58] </ref>. 3.5. Committing State The final phase of the lifetime of an instruction is the commit or the retire phase, where the effects of the instruction are allowed to modify the logical process state.
Reference: [59] <author> G. S. Sohi, S. E. Breach, and T. N. Vijaykumar, </author> <title> ``Multiscalar Processors,'' </title> <booktitle> Proc. 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 414-425, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: As with traditional multiprocessing, single-chip multiprocessors will require enormous amounts compiler support to create a parallel version of a program statically. Looking even farther ahead, more radical approaches that support multiple flows of control are being studied <ref> [14, 59] </ref>. In these approaches, individual processing units are much more highly integrated than in a traditional multiprocessor, and the flow control mechanism is built into the hardware, as well.
Reference: [60] <author> A. R. Talcott, W. Yamamoto, M. J. Serrano, R. C. Wood, and M. Nemirovsky, </author> <title> ``The Impact of Unresolved Branches on Branch Prediction Performance,'' </title> <booktitle> in Proc. 21st Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago, </address> <publisher> Illinois, </publisher> <pages> pp. 12-21, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Consequently, a counter's value summarizes the dominant outcome of recent branch executions. At some time after a prediction has been made, the actual branch outcome is evaluated. Dynamic branch history information can then be updated. (It could also be updated speculatively, at the time the prediction was made <ref> [26, 60] </ref> ). If the prediction was incorrect, instruction fetching must be re-directed to the correct path. Furthermore, if instructions were processed speculatively based on the prediction, they must be purged and their results must be nullified. The process of speculatively executing instruction is described in more detail later.
Reference: [61] <author> J. E. Thornton, </author> <title> ``Parallel Operation in the Control Data 6600,'' </title> <booktitle> Fall Joint Computers Conference, </booktitle> <volume> vol. 26, </volume> <pages> pp. 33-40, </pages> <year> 1961. </year>
Reference-contexts: With simple pipelining, only one instruction at a time is initiated into the pipeline, but multiple instructions may be in some phase of execution concurrently. Pipelining was initially developed in the late 1950s [8] and became a mainstay of large scale computers during the 1960s. The CDC 6600 <ref> [61] </ref> used a degree of pipelining, but achieved most of its ILP through parallel functional units. Although it was capable of sustained execution of only a single instruction per cycle, the 6600's instruction set, parallel processing units, and dynamic instruction scheduling are similar to the superscalar microprocessors of today.
Reference: [62] <author> G. S. Tjaden and M. J. Flynn, </author> <title> ``Detection and Parallel Execution of Independent Instructions,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-19, </volume> <pages> pp. 889-895, </pages> <month> October </month> <year> 1970. </year>
Reference-contexts: Meanwhile other avenues for improving performance via parallelism were developed, such as vector processing [28, 49] and multiprocessing [5, 6]. Although some processors capable of multiple instruction initiation were considered during the '60s and '70s <ref> [50, 62] </ref>, none were delivered to the market. Then, in the mid-to-late 1980s, superscalar processors began to appear [21, 43, 54]. By initiating more than one instruction at a time into multiple pipelines, superscalar processors break the single-instruction-per-cycle bottleneck.
Reference: [63] <author> R. M. Tomasulo, </author> <title> ``An Efficient Algorithm for Exploiting Multiple Arithmetic Units,'' </title> <journal> IBM Journal of Research and Development, </journal> <pages> pp. 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Another remarkable processor of the 1960s was the IBM 360/91 [3]. The 360/91 was heavily pipelined, and provided a dynamic instruction issuing mechanism, known as Tomasulo's algorithm <ref> [63] </ref> after its inventor. As with the CDC 6600, the IBM 360/91 could sustain only one instruction per cycle and was not superscalar, but the strong influence of Tomasulo's algorithm is evident in many of today's superscalar processors. <p> Some earlier superscalar implementations used this method [21, 43, 54]. Reservation Stations With reservation stations (see Fig. 10), which were first proposed as a part of Tomasulo's algorithm <ref> [63] </ref>, instructions may issue out of order; there is no strict FIFO ordering. Consequently, all of the reservation stations simultaneously monitor their source operands for data availability. The traditional way of doing this is to hold operand data in the reservation station.
Reference: [64] <author> D. W. Wall, </author> <title> ``Limits of Instruction-Level Parallelism,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 176-188, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Meanwhile enough information concerning the original sequential order is retained so that the instruction stream can conceptually be squeezed back together again should there be need for a precise interrupt. A number of studies have been done to determine the performance of superscalar methods, for example <ref> [56, 64] </ref>. Because the hardware and software assumptions and benchmarks vary widely, so do the potential speedups -- ranging from close to 1 for some program/hardware combinations to many 100s for others. <p> There are a number of reasons for thinking this, we give two of the more important. First, there may be limits to the instruction level parallelism that can be exposed and exploited by currently-known superscalar methods, even if very aggressive methods are used <ref> [64] </ref>. Perhaps the most important of these limits results from conditional branches. Studies that compare 19 performance using real branch prediction with theoretically perfect branch prediction note a significant performance decline when real prediction is used. Second, superscalar implementations grow in complexity as the number of simultaneously issued instructions increases.
Reference: [65] <author> S. Weiss and J. E. Smith, </author> <title> Power and PowerPC: </title> <booktitle> Principles, Architecture, Implementation. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Some of the superscalar processors have taken special steps to allow wider fetches in this case, for example by fetching from two adjacent cache lines simultaneously <ref> [20, 65] </ref>. A discussion of a number of alternatives for high bandwidth instruction fetching appears in [11]. To help smooth out the instruction fetch irregularities caused by cache misses and branches, there is often an instruction buffer (shown in Fig. 4) to hold a number of fetched instructions. <p> Most microprocessors integrate the primary cache on the same chip as the processor; notable exceptions are some of processors developed by HP [4] and the high-end IBM POWER series <ref> [65] </ref>. Unlike ALU instructions, for which it is possible to identify during the decode phase the register operands that will be accessed, it is not possible to identify the memory locations that will be accessed by load and store instructions until after the issue phase. <p> Mul-tiporting can be achieved by having multiported storage cells, by having multiple banks of memory [9, 29, 58], or by making multiple serial requests during the same cycle <ref> [65] </ref>. To allow memory operations to be overlapped with other operations (both memory and non-memory), the memory hierarchy must be non-blocking [37, 58]. That is, if a memory request misses in the data cache, other processing operations, including further memory requests, should be allowed to proceed.
Reference: [66] <author> T. Y. Yeh and Y. N. Patt, </author> <title> ``Alternative Implementations of Two-Level Adaptive Training Branch Prediction,'' </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-134, </pages> <month> May, </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: That is, there is a dependence between the branch instruction and a preceding, uncompleted instruction. Rather than wait, the outcome of a conditional branch can be predicted using one of several types of branch prediction methods <ref> [35, 40, 44, 53, 66] </ref>.
References-found: 66

