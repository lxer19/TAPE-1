URL: ftp://ftp.idiap.ch/pub/papers/neural/moerland.LCLV.ps.Z
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: Incorporating LCLV Non-Linearities in Optical Multilayer Neural Networks  
Author: P. Moerland, E. Fiesler, and I. Saxena 
Keyword: (artificial) neural network, optical multilayer neural network, hardware implementation, liquid crystal light valve (LCLV), activation function, curve fit, gain.  
Date: September 1996.  
Note: Preprint of an article published in Applied Optics, vol.35, no.26,  
Abstract: Sigmoid-like activation functions as available in analog hardware differ in various ways from the standard sigmoidal function as they are usually asymmetric, truncated, and have a non-standard gain. We present an adaptation of the backpropagation learning rule to compensate for these non-standard sigmoids. This method is applied to multilayer neural networks with all-optical forward propagation and liquid crystal light valves (LCLV) as optical thresholding devices. In this paper, the results of simulations of a backpropagation neural network with five different LCLV response curves as activation functions are presented. While performing poorly with the standard backpropagation algorithm, it is shown that our adapted learning rule performs well with these LCLV curves. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Yu, T. Lu, X. Yang, and D. Gregory, </author> <title> "Optical Neural Network with Pocket-Sized Liquid-Crystal Televisions," </title> <journal> Optics Letters, </journal> <volume> volume 15, number 15, </volume> <pages> pages 863-865, </pages> <year> (1990). </year>
Reference-contexts: This means that the large number of interconnections of a NN can be optically implemented in a compact way, paving the way for truly parallel implementations of large NNs. Most optical implementations of multilayer NNs perform non-linear thresholding, which is an essential constituent of all NN models, electronically <ref> [1] </ref> [2], and hence involve conversion of optical signals to electronic ones and vice versa. In order to avoid this conversion and to progress to all-optical forward propagation in multilayer NNs, the use of optical activation functions is essential. <p> Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. Each pattern is a set of 60 numbers in the range <ref> [0; 1] </ref>. The corresponding output patterns are the two unit vectors. Wine is the result of a chemical analysis of wines grown in a region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. <p> The analysis determined the quantities of 13 constituents found in each of the three types of wines. A wine has to be classified using these 13 values, which have been scaled to the interval <ref> [0; 1] </ref>. The target patterns are the three unit vectors. Table 2 shows the simulation parameters for the various benchmarks. Training is continued until the convergence criterion is satisfied or the maximal number of iterations has been performed. <p> These results are the mean over the converged experiments out of 20 (for XOR) and 5 (for Sonar) different random weight initializations in the interval <ref> [1; 1] </ref>.
Reference: [2] <author> J.-S. Jang, S.-G. Shin, S.-W Yuk, S.-Y Shin, and S.-Y. Lee, </author> <title> "Dynamic Optical Interconnections Using Holographic Lenslet Arrays for Adaptive Neural Networks," </title> <journal> Optical Engineering, </journal> <volume> volume 32, number 1, </volume> <pages> pages 80-87, </pages> <address> SPIE, Bellingham, Washington, ISSN 0091-3286, </address> <year> (1993). </year>
Reference-contexts: This means that the large number of interconnections of a NN can be optically implemented in a compact way, paving the way for truly parallel implementations of large NNs. Most optical implementations of multilayer NNs perform non-linear thresholding, which is an essential constituent of all NN models, electronically [1] <ref> [2] </ref>, and hence involve conversion of optical signals to electronic ones and vice versa. In order to avoid this conversion and to progress to all-optical forward propagation in multilayer NNs, the use of optical activation functions is essential.
Reference: [3] <author> I. Shariv and A. A. Friesem, </author> <title> "All-Optical Neural Network with Inhibitory Neurons," </title> <journal> Optics Letters, </journal> <volume> volume 13, </volume> <pages> pages 485-487, </pages> <year> (1989). </year> <month> 8 </month>
Reference-contexts: In order to avoid this conversion and to progress to all-optical forward propagation in multilayer NNs, the use of optical activation functions is essential. Foremost is the use of liquid crystal light valves as non-linear optical activation functions <ref> [3] </ref> [4] [5], since their response curves are sigmoid-like. Common to most of these approaches is the fact that a thorough mathematical description of the optical activation functions and an analysis of their differences with ideal thresholding functions is lacking.
Reference: [4] <author> D. Psaltis and Y. Qiao, </author> <title> "Adaptive Multilayer Optical Networks," In Progress in Optics, </title> <editor> E. Wolf (editor), </editor> <volume> volume 31, chapter 4, </volume> <pages> pages 227-261, </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, ISBN 0-444-89836-0, </address> <year> (1993). </year> <note> See also: </note> <author> D. Psaltis and Y. Qiao, </author> <title> "Optical Neural Networks," </title> <journal> Optics & Photonics News, </journal> <pages> pages 17-21, </pages> <year> (1990). </year>
Reference-contexts: In order to avoid this conversion and to progress to all-optical forward propagation in multilayer NNs, the use of optical activation functions is essential. Foremost is the use of liquid crystal light valves as non-linear optical activation functions [3] <ref> [4] </ref> [5], since their response curves are sigmoid-like. Common to most of these approaches is the fact that a thorough mathematical description of the optical activation functions and an analysis of their differences with ideal thresholding functions is lacking.
Reference: [5] <author> W. Xue, </author> <title> "Characterization of Liquid Crystal Light Valves for Neural Network Applications," </title> <type> PhD thesis, </type> <institution> IMT, University of Neuch^atel, Neuch^atel, Switzerland, </institution> <year> (1994). </year>
Reference-contexts: In order to avoid this conversion and to progress to all-optical forward propagation in multilayer NNs, the use of optical activation functions is essential. Foremost is the use of liquid crystal light valves as non-linear optical activation functions [3] [4] <ref> [5] </ref>, since their response curves are sigmoid-like. Common to most of these approaches is the fact that a thorough mathematical description of the optical activation functions and an analysis of their differences with ideal thresholding functions is lacking. <p> If required, an additional mask could also be inserted into the optical system having a spatially varying transmission which complements (and thereby compensates for) existing spatial non-uniformities <ref> [5] </ref>. The lack of a mechanism for intensity subtraction is a limiting factor for the use of optical neural networks. Using polarization modulating devices like LCLVs, image subtraction can be performed [7] independently in a stage previous to that of thresholding. <p> To be able to describe various properties (translation, range, gain) of the sigmoid-like LCLV response data and the differences with the standard sigmoid, a close approximation by a generic sigmoid curve fit is used, based on <ref> [5] </ref>. <p> The normalized curve fit parameters of the five LCLV activation functions are given in table 1. Response curves and curve fits for the first three LCLVs were obtained by Xue <ref> [5] </ref> [10]. The two LCLV4 curves are obtained from the same light valve [11], but by applying a voltage of approximately 10 volts for LCLV4a and 15 volts for LCLV4b [8].
Reference: [6] <author> M. G. Robinson and K. M. Johnson, </author> <title> "Noise Analysis of Polarization-Based Optoelectronic Connectionist Machines," </title> <journal> Applied Optics, </journal> <volume> volume 31, number 2, </volume> <pages> pages 263-272, </pages> <year> (1992). </year>
Reference-contexts: In addition, the fan-out optics and other optical elements may have non-ideal behaviour. These non-uniformities are expected to be compensated for to a considerable extent in an adaptive optical neural network <ref> [6] </ref>, the weights of which are updated during the training on the actual optical system. If required, an additional mask could also be inserted into the optical system having a spatially varying transmission which complements (and thereby compensates for) existing spatial non-uniformities [5].
Reference: [7] <author> W.P. Bleha, L.T. Lipton, E. Wiener-Avnear, J. Grinberg, P.G. Reif, David Casasent, H.B. Brown, and B.V. Markevitch, </author> <title> "Application of the Liquid Crystal Light Valve to Real-Time Optical Data Processing," </title> <journal> Optical Engineering, </journal> <volume> volume 17, number 4, </volume> <pages> pages 371-384, </pages> <year> (1978). </year>
Reference-contexts: The lack of a mechanism for intensity subtraction is a limiting factor for the use of optical neural networks. Using polarization modulating devices like LCLVs, image subtraction can be performed <ref> [7] </ref> independently in a stage previous to that of thresholding. It would, however, be very desirable to be able to do this in the same processing plane (of the LCLV) as for thresholding.
Reference: [8] <author> I. Saxena and E. Fiesler, </author> <title> "Adaptive Multilayer Optical Neural Network with Optical Thresholding," Optical Engineering (ISSN 0091-3286), special on Optics in Switzerland (P. Rastogi, </title> <editor> editor), </editor> <volume> volume 34, number 8, </volume> <pages> pages 2435-2440, </pages> <year> (1995). </year>
Reference-contexts: Light information flow Electronic information flow Input Layer Hidden Layer LCTV1 LCLV MVM1 MVM2 LCLV PC Laser Output Layer Photodiode Array Photodiode Array PBS 2 Adaptive Optical Multilayer Neural Network Description Saxena and Fiesler <ref> [8] </ref> have described an adaptive multilayer optical NN with a large number of interconnections and all-optical forward propagation. <p> Response curves and curve fits for the first three LCLVs were obtained by Xue [5] [10]. The two LCLV4 curves are obtained from the same light valve [11], but by applying a voltage of approximately 10 volts for LCLV4a and 15 volts for LCLV4b <ref> [8] </ref>. <p> In general the response curves are truncated on x 0 and y 0, and these truncated parts are closely approximated by their curve fits (see <ref> [8] </ref> for a more detailed description). Since the LCLV activation functions are only defined on non-negative values, the neuron inputs of the activation function have to be non-negative. In our adapted learning rule, negative inputs are therefore considered to be equal to zero in the forward propagation step. <p> As can be seen in table 1, the gain fi of the LCLV curve fits (and hence the response data) has values different from one, especially for LCLV1, LCLV2, and LCLV3. Saxena and Fiesler <ref> [8] </ref> suggest to divide the initial weights and the learning rate by the gain to obtain better results with activation functions with a non-standard gain.
Reference: [9] <author> E. Fiesler, </author> " <title> Neural Network Classification and Formalization," Computer Standards & Interfaces, special issue on Neural Network Standards, </title> <editor> John Fulcher (editor), </editor> <volume> volume 16, number 3, </volume> <pages> pages 231-239, </pages> <publisher> North-Holland / Elsevier Science Publishers, </publisher> <address> Amsterdam, The Netherlands, ISSN 0920-5489, </address> <year> (1994). </year>
Reference-contexts: This three layer neural network (a layer of a neural network is defined as a layer of neurons, see <ref> [9] </ref>) will be realized as a neurocomputer consisting 2 of optical hardware, a computer, and an interface between them (figure 1). The optical system uses liquid crystal televisions (LCTVs) to implement adaptive interconnection weight matrices permitting learning and LCLVs to implement non-linear thresholding.
Reference: [10] <author> N. Collings and W. Xue, </author> <title> "Liquid Crystal Light Valves as Thresholding Elements in Neural Networks: Basic Device Requirements," </title> <journal> Applied Optics, </journal> <volume> volume 33, number 14, </volume> <pages> pages 2829-2833, </pages> <year> (1994). </year>
Reference-contexts: The normalized curve fit parameters of the five LCLV activation functions are given in table 1. Response curves and curve fits for the first three LCLVs were obtained by Xue [5] <ref> [10] </ref>. The two LCLV4 curves are obtained from the same light valve [11], but by applying a voltage of approximately 10 volts for LCLV4a and 15 volts for LCLV4b [8].
Reference: [11] <institution> Micro-Optics Technologies, </institution> <address> 8608 University Green # 5, Middleton, WI 53562, U.S.A. </address>
Reference-contexts: The normalized curve fit parameters of the five LCLV activation functions are given in table 1. Response curves and curve fits for the first three LCLVs were obtained by Xue [5] [10]. The two LCLV4 curves are obtained from the same light valve <ref> [11] </ref>, but by applying a voltage of approximately 10 volts for LCLV4a and 15 volts for LCLV4b [8].
Reference: [12] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> "Learning Internal Representations by Error Propagation," Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <booktitle> volume 1: Foundations, </booktitle> <pages> pages 318-362, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, ISBN: 0-262-18210-7, </address> <year> (1986). </year>
Reference-contexts: The derivative of the response data curve, which is needed in the backward pass of the backpropagation learning algorithm, is defined to be the derivative of this linear interpolation. 4 Benchmarks and Simulation Parameters As a test of the capability of the backpropagation algorithm <ref> [12] </ref> to train a three-layer network using activation functions realized by LCLVs, five different benchmarks have been used in the simulations.
Reference: [13] <author> R. P. Gorman and T. J. Sejnowski, </author> <title> "Analysis of hidden units in a layered network trained to classify sonar targets," </title> <booktitle> Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 75-89, </pages> <year> (1988). </year>
Reference-contexts: Furthermore, two real-world data sets have been used, namely the sonar benchmark <ref> [13] </ref> and the wine data set [14]: Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network.
Reference: [14] <author> P. M. Murphy and D. W. Aha (Librarians), </author> <title> "UCI Repository of Machine Learning Databases," </title> <note> Machine-readable data repository accessible via anonymous ftp ics.uci.edu: pub/machine-learning-databases, </note> <year> (1994). </year>
Reference-contexts: Furthermore, two real-world data sets have been used, namely the sonar benchmark [13] and the wine data set <ref> [14] </ref>: Sonar This data set was originally used by R. Gorman and T. Sejnowski in their study of the classification of sonar signals using a neural network. The task is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.
Reference: [15] <author> G. Thimm and E. Fiesler, </author> <title> "Weight Initialization in Higher Order and Multi-Layer Perceptrons," </title> <journal> IEEE Transactions on Neural Networks, </journal> <note> conditionally accepted for publication. See also: </note> <author> G. Thimm and E. Fiesler, </author> <title> "Weight Initialization in Higher Order and Multi-Layer Perceptrons," </title> <booktitle> Proceedings of the '94 SIPAR-Workshop on Parallel and Distributed Computing, M. Aguilar (editor), </booktitle> <pages> pages 87-90, </pages> <institution> Institute of Informatics, University of Fribourg, Fribourg, Switzerland, </institution> <year> (1994). </year>
Reference-contexts: First, it will be shown how to handle activation functions which are located in the non-negative quadrant. 5.1 Adaptation of Weight Initialization The initial weights for a multilayer neural network are usually uniformly chosen in an interval symmetric around zero <ref> [15] </ref>. However, this initialization method leads to non-convergence results when using a sigmoidal activation function which has been translated along the x-axis.
Reference: [16] <author> J. M. C. Oosse, H. C. A. M. Withagen, and J. A. Hegt, </author> <title> "Analog VLSI Implementation of a Feed-Forward Neural Network," </title> <booktitle> Proceedings of the First International Conference on Electronics, Circuits, and Systems (ICECS'94), </booktitle> <address> Cairo, Egypt, </address> <year> (1994). </year>
Reference-contexts: An example of such functions are the LCLV activation functions described in section 3, but also in analog electronic implementations the sigmoid non-linearity can be translated along the x-axis <ref> [16] </ref>.
Reference: [17] <author> G. Thimm, P. Moerland, and E. Fiesler, </author> <title> "The Interchangeability of Learning Rate and Gain in Backpropagation Neural Networks," </title> <booktitle> to appear in volume 8, number 2 of Neural Computation, </booktitle> <year> (1996). </year>
Reference-contexts: This influence can be eliminated by applying a recently proven simple and precise relationship that enables compensating for the non-standard gain in backpropagation neural networks by changing the learning rate and the initial weights <ref> [17] </ref>: Theorem 1 Two neural networks M and N of identical topology whose activation function ', gain fi, learning rate j, and initial weights w are related to each other as given in table 4, are equivalent under the on-line backpropagation algorithm; that is, when presented the same pattern set in <p> An increase of the gain with a factor fi can therefore be compensated for by dividing the initial weights by fi and the learning rate by fi 2 . Some extensions of the above theorem for variations of the standard backpropagation algorithm are also given in <ref> [17] </ref>. In the experiments described in the next section, one of these variants has been used: flat spot elimination [18]. This technique adds a constant to the derivative of the activation function in the backward pass.
Reference: [18] <author> S. E. Fahlman, </author> <title> "An Empirical Study of Learning Speed in Backpropagation Networks," </title> <type> Tech--nical Report CMU-CS-88-162, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA, </address> <year> (1988). </year>
Reference-contexts: Some extensions of the above theorem for variations of the standard backpropagation algorithm are also given in [17]. In the experiments described in the next section, one of these variants has been used: flat spot elimination <ref> [18] </ref>. This technique adds a constant to the derivative of the activation function in the backward pass.
Reference: [19] <author> D. B. Mundie and L. W. Massengill, </author> <title> "Threshold Non-Linearity Effects on Weight-Decay Tolerance in Analog Neural Networks," </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (IJCNN'92), </booktitle> <volume> volume 2, </volume> <pages> pages 583-587, </pages> <address> Baltimore, USA, </address> <year> (1992). </year> <month> 10 </month>
Reference-contexts: From an engineering point of view this is pertinent since it opens up new device possibilities for non-linearities in neural networks. For example, the problem of training neural networks with high gain thresholds, which are efficient to implement in analog electronic hardware and use minimal power <ref> [19] </ref>, has been eliminated.
References-found: 19

