URL: http://www.cs.colorado.edu/homes/rupa/public_html/mypapers/p93.1.ps
Refering-URL: http://www.cs.colorado.edu/~rupa/mypapers.html
Root-URL: http://www.cs.colorado.edu
Title: Using Prior Knowledge in an NNPDA to Learn Context-Free Languages  
Author: Sreerupa Das C. Lee Giles* Guo-Zheng Sun 
Address: Boulder, CO 80309  4 Independence Way Princeton, NJ 08540  College Park, MD 20742  
Affiliation: Dept. of Comp. Sc. Inst. of Cognitive Sc. University of Colorado  NEC Research Inst.  Inst. for Adv. Comp. Studies University of Maryland  
Abstract: Although considerable interest has been shown in language inference and automata induction using recurrent neural networks, success of these models has mostly been limited to regular languages. We have previously demonstrated that Neural Network Pushdown Automaton (NNPDA) model is capable of learning deterministic context-free languages (e.g., a n b n and parenthesis languages) from examples. However, the learning task is computationally intensive. In this paper we discuss some ways in which a priori knowledge about the task and data could be used for efficient learning. We also observe that such knowledge is often an experimental prerequisite for learning nontrivial languages (eg. a n b n cb m a m ).
Abstract-found: 1
Intro-found: 1
Reference: <author> Y.S. Abu-Mostafa. </author> <title> (1990) Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6 </volume> <pages> 192-198. </pages>
Reference: <author> K.A. Al-Mashouq and I.S. Reed. </author> <title> (1991) Including hints in training neural networks. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 418-427. </pages>
Reference: <author> S. Das and R. Das. </author> <title> (1992) Induction of discrete state-machine by stabilizing a continuous recurrent network using clustering. </title> <note> To appear in CSI Journal of Computer Science and Informatics. Special Issue on Neural Computing. </note>
Reference-contexts: A continuous stack is essential in order to permit the use of a continuous optimization method during learning. The stack is manipulated by the continuous valued action neuron. A detailed discussion on the operations may be found in <ref> (Das, Giles, and Sun, 1992) </ref>. 2.2 LEARNABLE CLASS OF LANGUAGES The class of language learnable by the NNPDA is a proper subset of deterministic context-free languages.
Reference: <author> S. Das, C.L. Giles, and G.Z. Sun. </author> <title> (1992) Learning context free grammars: capabilities and limitations of neural network with an external stack memory. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Indiana University, Bloomington, </address> <pages> pages 791-795. </pages> <publisher> Morgan Kauffmann Publishers. </publisher>
Reference-contexts: A continuous stack is essential in order to permit the use of a continuous optimization method during learning. The stack is manipulated by the continuous valued action neuron. A detailed discussion on the operations may be found in <ref> (Das, Giles, and Sun, 1992) </ref>. 2.2 LEARNABLE CLASS OF LANGUAGES The class of language learnable by the NNPDA is a proper subset of deterministic context-free languages.
Reference: <author> J.L. Elman. </author> <title> (1991) Incremental learning, or the importance of starting small. </title> <type> Technical Report CRL Tech Report 9101, </type> <institution> Center for Research in Language, University of California at San Diego, La Jolla, </institution> <address> CA. </address>
Reference: <author> C.L. Giles, C. B. Miller, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> (1992) Learning and extracting finite state automata with second-order recurrent neural network. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405. </pages>
Reference-contexts: A continuous stack is essential in order to permit the use of a continuous optimization method during learning. The stack is manipulated by the continuous valued action neuron. A detailed discussion on the operations may be found in <ref> (Das, Giles, and Sun, 1992) </ref>. 2.2 LEARNABLE CLASS OF LANGUAGES The class of language learnable by the NNPDA is a proper subset of deterministic context-free languages.
Reference: <author> J.E. Hopfcroft and J.D. Ullman. </author> <title> (1979) Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> C.W. Omlin and C.L. Giles. </author> <title> (1992) Training second-order recurrent neural networks using hints. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 363-368. </pages> <editor> D. Sleeman and P. Edwards (eds). </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Ca. </address>
Reference: <author> G.Z. Sun, H.H. Chen, C.L. Giles, Y.C. Lee and D. Chen. </author> <title> (1991) Neural networks with external memory stack that learn context-free grammars from examples. </title> <booktitle> In Proceedings of the Conference on Information Science and Systems, Princeton University, </booktitle> <volume> Vol. II, </volume> <pages> pages 649-653. </pages>
Reference: <author> G.G. Towell, J.W. Shavlik and M.O Noordewier. </author> <title> (1990) Refinement of approximately correct domain theories by knowledge-based neural-networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA. </address> <pages> pages 861. </pages>
Reference: <author> R.J. Williams and D. Zipser. </author> <title> (1989) A learning algorithm for continually running fully recurrent neural networks. </title> <booktitle> Neural Computation 1(2) </booktitle> <pages> 270-280. </pages>
Reference-contexts: Once the end is reached the activation of the output neuron is matched with the target (which is 1:0 for positive string and 0:0 for a negative string) The learning rule used in the NNPDA is a significantly enhanced extension to Real Time Recurrent Learning <ref> (Williams and Zipser, 1989) </ref>. 2.4 OBJECTIVE FUNCTION The objective function used to train the network consists of two error terms: one for positive strings and the other for negative strings. For positive strings we require (a) the NNPDA must reach a final state and (b) the stack must be empty.
References-found: 11

