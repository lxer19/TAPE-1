URL: http://www.sls.lcs.mit.edu/hazen/publications/ICASSP98.ps.gz
Refering-URL: http://www.sls.lcs.mit.edu/hazen/publications.html
Root-URL: 
Title: USING AGGREGATION TO IMPROVE THE PERFORMANCE OF MIXTURE GAUSSIAN ACOUSTIC MODELS 1  
Author: Timothy J. Hazen and Andrew K. Halberstadt 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper investigates the use of aggregation as a means of improving the performance and robustness of mixture Gaussian models. This technique produces models that are more accurate and more robust to different test sets than traditional cross-validation using a development set. A theoretical justification for this technique is presented along with experimental results in phonetic classification, phonetic recognition, and word recognition tasks on the TIMIT and Resource Management corpora. In speech classification and recognition tasks error rate reductions of up to 12% were observed using this technique. A method for utilizing tree-structured density functions for the purpose of pruning the aggregated models is also presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bishop, </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: A second disadvantage of simple cross-validation is that computation is wasted, since the results of only one training trial are kept, while the models from the other trials are thrown away <ref> [1] </ref>. To counter the problems discussed above, an algorithm is needed which produces a mixture density function which can be proven to yield better classification accuracy, on average, than any randomly initialized density function trained using standard techniques. <p> At the very least, it is desirable to show the new algorithm can reduce the value of some error function which is strongly correlated with classification accuracy. Aggregation is a technique which meets this criterion <ref> [1] </ref>. Aggregation improves the performance of classifiers which exhibit uncertainty or instability during their training phase. Aggregation has been applied to a variety of types of predictors and classifiers.
Reference: [2] <author> L. Breiman, </author> <title> Bagging predictors, </title> <journal> Machine Learning, </journal> <volume> vol. 24, </volume> <pages> pp. 123140, </pages> <year> 1996. </year>
Reference-contexts: Aggregation has been applied to a variety of types of predictors and classifiers. For example, Breiman has shown the effectiveness of a specific type of aggregation known as bagging (or bootstrap aggregating) on linear regression predictors and on classification trees <ref> [2] </ref>. In [2], Breiman indicates aggregation could also be used on probabilistic classifiers. In this paper, Breiman's work is extended by proving, both theoretically and empirically, how aggregation can be used to improve the performance and robustness of probabilistic classifiers. <p> Aggregation has been applied to a variety of types of predictors and classifiers. For example, Breiman has shown the effectiveness of a specific type of aggregation known as bagging (or bootstrap aggregating) on linear regression predictors and on classification trees <ref> [2] </ref>. In [2], Breiman indicates aggregation could also be used on probabilistic classifiers. In this paper, Breiman's work is extended by proving, both theoretically and empirically, how aggregation can be used to improve the performance and robustness of probabilistic classifiers.
Reference: [3] <author> J. Glass, J. Chang, and M. McCandless, </author> <title> A probabilistic framework for feature-based speech recognition, </title> <booktitle> in ICSLP96 (Philadelphia), </booktitle> <pages> pp. 22772280, </pages> <year> 1996. </year>
Reference-contexts: EXPERIMENTS Overview: The effectiveness of aggregation will be demonstrated with results obtained on three different tasks: phonetic classification, phonetic recognition, and word recognition. Each experiment utilizes the SUMMIT <ref> [3] </ref> recognition system. This system uses mixture Gaussian acoustic models to score segment-based phonetic units. For each experiment, 24 individual sets of acoustic models were independently trained. These 24 individual trials were then used to create sets of independent models for each aggregation trial. <p> All models were trained on a 462 speaker training set. Phonetic recognition accuracies were computed on both a 50 speaker development test set (400 utts) and on the 24 speaker core test (192 utts). The recognizer used for these experiments is described in detail in <ref> [3] </ref>. Word Recognition: For the word recognition task, SUMMIT was tested on the DARPA Resource Management corpus. The recog-nizer used the same measurement set as the TIMIT phonetic rec-ognizer used above.
Reference: [4] <author> A. Halberstadt and J. Glass, </author> <title> Heterogeneous measurements for phonetic classification, </title> <booktitle> in EUROSPEECH97 (Rhodes, </booktitle> <address> Greece), </address> <pages> pp. 401404, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: Phonetic Classification: For the task of phonetic classification, the SUMMIT system was tested on the TIMIT corpus using context-independent phonetic segment models. The classifier and measurements used for these experiments are similar, but not identical, to those described in <ref> [4] </ref>. The classifier used a 71 dimensional feature vector to model the segments of 61 different phones. The feature vector contains MFCC and energy averages and derivatives, duration, zero-crossing rate and fundamental frequency. The classifier uses mixtures of full-covariance Gaussian density functions.
Reference: [5] <author> R. Jacobs, </author> <title> Methods for combining experts' probability assessments, </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 867888, </pages> <year> 1995. </year>
Reference-contexts: Another set of techniques emerges from perturbing the learning set through weighting, resampling, or generating new learning sets through alternative preprocessing of the same underlying input data. In addition to these extensions, more sophisticated schemes for combining classifiers have been developed <ref> [5] </ref>. Aggregation remains attractive because of its simplicity, and because the empirical evidence indicates its effectiveness in reducing the error rate in typical speech classification and recognition tasks.
Reference: [6] <author> T. Watanabe, K. Shinoda, K. Takagi, and E. Yamada, </author> <title> Speech recognition using tree-structured probability density function, </title> <booktitle> in ICSLP94 (Yokohama), </booktitle> <pages> pp. 223226, </pages> <year> 1994. </year> <month> 4 </month>
Reference-contexts: Similar performance curves were observed for both phonetic classification and word recognition. 4. MODEL PRUNING We have investigated the possibility of reducing the variance of the performance of a randomly trained N -fold model through the use of pruned tree-structured probability density functions <ref> [6] </ref>. We have constructed a hierarchical tree of the Gaussian kernels in the 24-fold models used for phonetic recognition. Pruning is performed by utilizing the model of a branch node as a replacement for the mixture of models of the leaf nodes which emanate from it. <p> In the future, we hope to further utilize the tree structure to reduce computation by performing the fast likelihood approximation technique suggested by Watanabe, et al. <ref> [6] </ref>. In this approach, pruning of low scoring branches is done dynamically during testing. Aggregated models may be well suited to this type of approach since many of the Gaussian kernels from different training trials may be similar and exhibit large overlap with each other.
References-found: 6

