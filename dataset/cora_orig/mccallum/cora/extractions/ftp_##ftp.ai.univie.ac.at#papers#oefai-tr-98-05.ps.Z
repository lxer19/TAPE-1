URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-98-05.ps.Z
Refering-URL: http://www.ai.univie.ac.at/oefai/ml/toxicology/project.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: stefan@ai.univie.ac.at  
Title: Generating Declarative Language Bias for Top-Down ILP Algorithms  
Author: Stefan Kramer 
Address: Schottengasse 3 A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Many of today's algorithms for Inductive Logic Programming (ILP) put a heavy burden and responsibility on the user, because their declarative bias have to be defined in a rather low-level fashion. To address this issue, we developed a method for generating declarative language bias for top-down ILP systems from high-level declarations. The key feature of our approach is the distinction between a user level and an expert level of language bias declarations. The expert provides abstract meta-declarations, and the user declares the relationship between the meta-level and the given database to obtain a low-level declarative language bias. The suggested languages allow for compact and abstract specifications of the declarative language bias for top-down ILP systems using schemata. We verified several properties of the translation algorithm that generates schemata, and applied it successfully to a few chemical domains. As a consequence, we propose to use a two-level approach to generate declarative language bias.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bergadano and D. Gunetti. </author> <title> An interactive system to learn functional logic programs. </title> <editor> In R. Bajcsy, editor, </editor> <booktitle> Proc. Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1044-1049, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction Many of today's Inductive Logic Programming (ILP) algorithms put a heavy burden and responsibility on the user, because their declarative language biases have to be defined in a rather low-level fashion. While all sorts of declarative biases have been proposed in the past (e.g., <ref> [21, 1] </ref>, or [17] for an excellent overview) and the things that could be specified are relatively clear, the level of abstraction of current formal languages leaves much to be desired.
Reference: [2] <author> H. Blockeel and L. De Raedt. </author> <title> Top-down induction of logical decision trees. </title> <type> Technical Report Report CW 247, </type> <institution> Katholieke Universiteit Leuven, Belgium, </institution> <year> 1997. </year>
Reference-contexts: In fact, this has been hinted at in [4] and in [14]. In this paper we propose a method for generating the declarative language bias of top-down ILP algorithms based on schemata. 2 Schemata as used in several ILP algorithms (e.g., FOCL [19], FOSSIL [12], SRT [14], and TILDE <ref> [2] </ref>) are a comparably useful and practical form of declarative bias for ILP, but their definition is still a hard and error-prone task. <p> Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE <ref> [2] </ref> The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level. Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches.
Reference: [3] <author> W.W. Cohen. </author> <title> Rapid prototyping of ilp systems using explicit bias. </title> <editor> In F. Bergadano, editor, </editor> <booktitle> Proceedings of the IJCAI-93 Workshop on Inductive Logic Programming, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Given this specification, clauses like this will be generated: h (X) :- p (X Y), q (X, Z). ... Since binary literals need to be variabilized, and the variablizations depend on the variables "introduced" so far, ADGs cannot express this in a compact form. Augmented ADGs <ref> [3] </ref>, however, provide a mechanism for reusing variables in a flexible way. Among other things, Augmented ADGs include meta-variables, which can be used to denote sets of variables. So, meta-variables can serve the same purpose as in our approach.
Reference: [4] <author> W.W. Cohen. </author> <title> Grammatically biased learning: Learning logic programs using an explicit antecedent description language. </title> <journal> Artificial Intelligence, </journal> <volume> 68(2), </volume> <year> 1994. </year>
Reference-contexts: In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option. In fact, this has been hinted at in <ref> [4] </ref> and in [14]. <p> Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches. Subsequently, we will compare it with two other well-known techniques for specifying declarative language bias, ADGs <ref> [4] </ref> and Dlab [7]. 5.1 Relational Cliches In [19], a meta-level approach to look-ahead for top-down ILP systems is presented. The look-ahead is based on so-called relational cliches, which are second-order expressions defining admissible refinements of a clause. <p> Besides, the representation developed in [18] (see the example above) is not as expressive as the language proposed in this paper. The representation proposed there is especially restricted with respect to variables. 5.2 ADGs and Augmented ADGs ADGs <ref> [4] </ref> are grammars for the declaration of antecedents of rules. In ADGs, variables are logical variables. Variables in a right-hand side of a grammar rule that are not in the left-hand side are distinct for each application of the rule. So, generating new variables is easy with ADGs. <p> So, meta-variables can serve the same purpose as in our approach. One advantage of schemata over ADGs is that one does not have to specify a "global" grammar, but only "local" schemata used for refinement. Admissible variabilizations might be easier to declare using types and modes than using ADGs <ref> [4] </ref>. 5.3 Dlab A Dlab [7] grammar is a declaration of all literals that can be used in a clause using sets of literals, including some cardinality constraints on the number of literals to be selected from these sets of literals.
Reference: [5] <author> W.W. Cohen, </author> <year> 1998. </year> <type> Personal Communication. </type>
Reference-contexts: Variables in a right-hand side of a grammar rule that are not in the left-hand side are distinct for each application of the rule. So, generating new variables is easy with ADGs. The hard task is to keep track of them so that they can be reused <ref> [5] </ref>. To illustrate the reuse of variables with ADGs, we present an example that is first formulated in terms of meta-schemata and schemata, and then in terms of ADGs.
Reference: [6] <author> L. De Raedt and M. Bruynooghe. </author> <title> Interactive concept-learning and constructive induction by analogy. </title> <journal> Machine Learning, </journal> <volume> 8(2), </volume> <year> 1992. </year>
Reference-contexts: Secondly, these declarations can be at the meta-level or at the object-level. Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata <ref> [6] </ref> 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level.
Reference: [7] <author> L. De Raedt and L. Dehaspe. </author> <title> Clausal discovery. </title> <booktitle> Machine Learning, </booktitle> <address> 26(2-3):99-146, </address> <year> 1997. </year>
Reference-contexts: Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches. Subsequently, we will compare it with two other well-known techniques for specifying declarative language bias, ADGs [4] and Dlab <ref> [7] </ref>. 5.1 Relational Cliches In [19], a meta-level approach to look-ahead for top-down ILP systems is presented. The look-ahead is based on so-called relational cliches, which are second-order expressions defining admissible refinements of a clause. If there is a match with predicates at the object-level, the cliche is applied. <p> One advantage of schemata over ADGs is that one does not have to specify a "global" grammar, but only "local" schemata used for refinement. Admissible variabilizations might be easier to declare using types and modes than using ADGs [4]. 5.3 Dlab A Dlab <ref> [7] </ref> grammar is a declaration of all literals that can be used in a clause using sets of literals, including some cardinality constraints on the number of literals to be selected from these sets of literals. The number of variables in a Dlab grammar is restricted.
Reference: [8] <author> L. Dehaspe, </author> <year> 1998. </year> <type> Personal Communication. </type>
Reference-contexts: In this sense, they are not embedded in Dlab itself, but act as a filter on Dlab's output. For efficiency, this kind of information should be encoded in the language templates whenever possible, because then unwanted clauses are never generated <ref> [8] </ref>. One of the main differences between Dlab and schemata is that schemata are usually a lot easier to declare, but they also produce a lot more unwanted clauses. <p> This does not pose a serious problem to greedy algorithms such as SRT, but probably to algorithms inducing interesting patterns (e.g., Claudien). In the latter case, the algorithms have to minimize the number of queries for efficiency reasons <ref> [8] </ref>.
Reference: [9] <author> S. Dzeroski and B. Kompare, </author> <year> 1995. </year> <type> Personal Communication. </type>
Reference: [10] <author> S. Dzeroski, S. Schulze-Kremer, K. Heidtke, K. Siems, and D. Wettschereck. </author> <title> Applying ilp to diterpene structure elucidation from 13 c nmr spectra. </title> <editor> In S. Mug-gleton, editor, </editor> <booktitle> Proceedings of the 6th International Workshop on Inductive Logic Programming (ILP-96), </booktitle> <address> Berlin Heidelberg New York, 1996. </address> <publisher> Springer. </publisher> <pages> 13 </pages>
Reference-contexts: We thus think that one way to improve the acceptance 1 There have been, nevertheless, several successful applications of ILP algorithms to real-world problems, e.g., <ref> [20, 10, 13, 15] </ref>. 1 could be to devise easy-to-use methods for the specification of declarative lan-guage bias for relational application domains. In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option.
Reference: [11] <author> W. Emde, C.U. Habel, and C.-R. Rollinger. </author> <title> The discovery of the equator or concept-driven learning. </title> <booktitle> In Proc. International Joint Conference on Artificial Intelligence 1983, </booktitle> <year> 1983. </year>
Reference-contexts: Secondly, these declarations can be at the meta-level or at the object-level. Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata <ref> [11] </ref>, rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level.
Reference: [12] <author> J. Furnkranz. </author> <title> Efficient Pruning Methods for Relational Learning. </title> <type> PhD thesis, </type> <institution> Vienna University of Technology, Vienna, Austria, </institution> <year> 1994. </year>
Reference-contexts: In fact, this has been hinted at in [4] and in [14]. In this paper we propose a method for generating the declarative language bias of top-down ILP algorithms based on schemata. 2 Schemata as used in several ILP algorithms (e.g., FOCL [19], FOSSIL <ref> [12] </ref>, SRT [14], and TILDE [2]) are a comparably useful and practical form of declarative bias for ILP, but their definition is still a hard and error-prone task. <p> Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL <ref> [12] </ref>, SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level. Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches.
Reference: [13] <author> R.D. King and A. Srinivasan. </author> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives, </title> <year> 1997. </year>
Reference-contexts: We thus think that one way to improve the acceptance 1 There have been, nevertheless, several successful applications of ILP algorithms to real-world problems, e.g., <ref> [20, 10, 13, 15] </ref>. 1 could be to devise easy-to-use methods for the specification of declarative lan-guage bias for relational application domains. In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option. <p> Secondly, the algorithm, by design, always fulfills the type constraints for the assignment of variables. This also holds with respect to meta-types. We also did some initial validation of the approach: the algorithm has been applied to several chemical domains ([9], [20], <ref> [13] </ref>) using all graph-based meta-schemata (see appendix D). Subsequently, we compared the results generated 6 by the algorithm with the manually engineered declarative language bias. Not surprisingly, there were only a few observed differences.
Reference: [14] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option. In fact, this has been hinted at in [4] and in <ref> [14] </ref>. In this paper we propose a method for generating the declarative language bias of top-down ILP algorithms based on schemata. 2 Schemata as used in several ILP algorithms (e.g., FOCL [19], FOSSIL [12], SRT [14], and TILDE [2]) are a comparably useful and practical form of declarative bias for ILP, <p> In fact, this has been hinted at in [4] and in <ref> [14] </ref>. In this paper we propose a method for generating the declarative language bias of top-down ILP algorithms based on schemata. 2 Schemata as used in several ILP algorithms (e.g., FOCL [19], FOSSIL [12], SRT [14], and TILDE [2]) are a comparably useful and practical form of declarative bias for ILP, but their definition is still a hard and error-prone task. <p> Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL [12], SRT <ref> [14] </ref>, TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level. Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches.
Reference: [15] <author> S. Kramer, B. Pfahringer, and C. Helma. </author> <title> Mining for causes of cancer: Machine learning experiments at various levels of detail. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97), </booktitle> <address> Menlo Park, CA, 1997. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We thus think that one way to improve the acceptance 1 There have been, nevertheless, several successful applications of ILP algorithms to real-world problems, e.g., <ref> [20, 10, 13, 15] </ref>. 1 could be to devise easy-to-use methods for the specification of declarative lan-guage bias for relational application domains. In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option.
Reference: [16] <author> K. Morik. </author> <title> Balanced cooperative modeling. </title> <booktitle> Machine Learning, </booktitle> <pages> 11(2-3), </pages> <year> 1993. </year>
Reference-contexts: The first subexpression can also consist of a single condition only. The refinement is constrained by the specifications in the second subexpression, a 2 We employ the term "schemata" for specifications of admissible refinements of clauses at the first-order level. In contrast to this, rule models <ref> [16] </ref>, also sometimes referred to as rule schemata, are second-order declarations of complete rules. 2 list containing type and mode declarations of the variables. The variables in the list refer to argument positions in the conjunction of literals. <p> Secondly, these declarations can be at the meta-level or at the object-level. Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models <ref> [16] </ref>, second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches [19, 18] 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level.
Reference: [17] <author> C. Nedellec, C. Rouveirol, F. Bergadano, H. Ade, and B. Tausend. </author> <title> Declarative bias in ilp. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, </booktitle> <pages> pages 82-103. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Many of today's Inductive Logic Programming (ILP) algorithms put a heavy burden and responsibility on the user, because their declarative language biases have to be defined in a rather low-level fashion. While all sorts of declarative biases have been proposed in the past (e.g., [21, 1], or <ref> [17] </ref> for an excellent overview) and the things that could be specified are relatively clear, the level of abstraction of current formal languages leaves much to be desired.
Reference: [18] <author> G. Silverstein and M. Pazzani. </author> <title> Learning relational cliches. </title> <editor> In F. Bergadano, editor, </editor> <booktitle> Proceedings of the IJCAI-93 Workshop on Inductive Logic Programming, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches <ref> [19, 18] </ref> 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level. <p> The look-ahead is based on so-called relational cliches, which are second-order expressions defining admissible refinements of a clause. If there is a match with predicates at the object-level, the cliche is applied. The subsequent expressions are examples of relational cliches as defined in <ref> [18] </ref>: Pattern: part-of (A,B) & ext-pred (...,B,...) var restr: -[introduces-new-var non-numeric 2], [include-old-variable non-numeric 1]- & -[include-new-var non-numeric *ANY*]- pred restr: -[pred-type = ext-pred, include-pred (part-of)]- & 3 This list contains just a small fraction of the approaches described in the literature. 7 -[pred-type = ext-pred]- Pattern: ext-pred (...,A,...) & thresh-comp <p> Secondly, it is more transparent in that the users cannot mix up the levels. Thirdly, the same schemata are repeatedly used during learning. Thus, it is not necessary to derive them anew every time they are applied. Besides, the representation developed in <ref> [18] </ref> (see the example above) is not as expressive as the language proposed in this paper. The representation proposed there is especially restricted with respect to variables. 5.2 ADGs and Augmented ADGs ADGs [4] are grammars for the declaration of antecedents of rules. In ADGs, variables are logical variables.
Reference: [19] <author> G. Silverstein and M.J. Pazzani. </author> <title> Relational cliches: Constraining constructive induction during relational learning. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 203-207, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In fact, this has been hinted at in [4] and in [14]. In this paper we propose a method for generating the declarative language bias of top-down ILP algorithms based on schemata. 2 Schemata as used in several ILP algorithms (e.g., FOCL <ref> [19] </ref>, FOSSIL [12], SRT [14], and TILDE [2]) are a comparably useful and practical form of declarative bias for ILP, but their definition is still a hard and error-prone task. <p> Using these two dimensions, various existing systems and languages can be categorized as follows 3 : 1. Specification of complete clauses, meta-level: Rule schemata [11], rule models [16], second-order schemata [6] 2. Specification of refinement, meta-level: Relational cliches <ref> [19, 18] </ref> 3. Specification of refinement, object-level: FOSSIL [12], SRT [14], TILDE [2] The approach presented here belongs to the second group of methods, as it allows for the specification of refinements at the meta-level. <p> Next, we will compare it with probably the closest work in the literature, the approach based on relational cliches. Subsequently, we will compare it with two other well-known techniques for specifying declarative language bias, ADGs [4] and Dlab [7]. 5.1 Relational Cliches In <ref> [19] </ref>, a meta-level approach to look-ahead for top-down ILP systems is presented. The look-ahead is based on so-called relational cliches, which are second-order expressions defining admissible refinements of a clause. If there is a match with predicates at the object-level, the cliche is applied. <p> The following meta-schemata express exactly the same thing: meta_schema ((part_of (A,B), ext_pred (B,C)), [A:non_numeric:'1-1':'+', B:non_numeric:'1-1':'-', C:other:'0-n':'-']). meta_schema ((ext_pred (A, B), thresh_comp (A, Thresh)), [A:numeric:'1-1':'+', B:other:'0-n':'+', Thresh:numeric:'1-1':'=']). The main difference between our approach and <ref> [19] </ref> is that we propose a two-level, generative approach, and that in [19] cliches are applied "on the fly". We think there are several advantages of our approach over [19]. Firstly, it allows for a degree of user-control that cannot be accomplished with a "dynamic" approach. <p> The following meta-schemata express exactly the same thing: meta_schema ((part_of (A,B), ext_pred (B,C)), [A:non_numeric:'1-1':'+', B:non_numeric:'1-1':'-', C:other:'0-n':'-']). meta_schema ((ext_pred (A, B), thresh_comp (A, Thresh)), [A:numeric:'1-1':'+', B:other:'0-n':'+', Thresh:numeric:'1-1':'=']). The main difference between our approach and <ref> [19] </ref> is that we propose a two-level, generative approach, and that in [19] cliches are applied "on the fly". We think there are several advantages of our approach over [19]. Firstly, it allows for a degree of user-control that cannot be accomplished with a "dynamic" approach. <p> The main difference between our approach and <ref> [19] </ref> is that we propose a two-level, generative approach, and that in [19] cliches are applied "on the fly". We think there are several advantages of our approach over [19]. Firstly, it allows for a degree of user-control that cannot be accomplished with a "dynamic" approach. The user can inspect the generated schemata, and delete some of them, if needed, to restrict the search space. Secondly, it is more transparent in that the users cannot mix up the levels.
Reference: [20] <author> A. Srinivasan, S. Muggleton, R.D. King, and M. Sternberg. </author> <title> Theories for muta-genicity: a study of first-order and feature based induction. </title> <journal> Artificial Intelligence, </journal> <volume> 85(1-2):277-299, </volume> <year> 1996. </year>
Reference-contexts: We thus think that one way to improve the acceptance 1 There have been, nevertheless, several successful applications of ILP algorithms to real-world problems, e.g., <ref> [20, 10, 13, 15] </ref>. 1 could be to devise easy-to-use methods for the specification of declarative lan-guage bias for relational application domains. In particular, generating or compiling a low-level specification from an abstract high-level specification appears to be an option. <p> Secondly, the algorithm, by design, always fulfills the type constraints for the assignment of variables. This also holds with respect to meta-types. We also did some initial validation of the approach: the algorithm has been applied to several chemical domains ([9], <ref> [20] </ref>, [13]) using all graph-based meta-schemata (see appendix D). Subsequently, we compared the results generated 6 by the algorithm with the manually engineered declarative language bias. Not surprisingly, there were only a few observed differences.
Reference: [21] <author> B. Tausend. </author> <title> Representing biases for inductive logic programming. </title> <editor> In F. Bergadano and L. De Raedt, editors, </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, Berlin Heidelberg New York, 1994. </address> <publisher> Springer. </publisher>
Reference-contexts: 1 Introduction Many of today's Inductive Logic Programming (ILP) algorithms put a heavy burden and responsibility on the user, because their declarative language biases have to be defined in a rather low-level fashion. While all sorts of declarative biases have been proposed in the past (e.g., <ref> [21, 1] </ref>, or [17] for an excellent overview) and the things that could be specified are relatively clear, the level of abstraction of current formal languages leaves much to be desired.
References-found: 21

