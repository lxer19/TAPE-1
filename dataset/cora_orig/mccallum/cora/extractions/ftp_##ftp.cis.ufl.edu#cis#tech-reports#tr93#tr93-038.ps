URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr93/tr93-038.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr93-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: Sensitivity Analysis of Frequency Counting  
Author: Theodore Johnson 
Note: N ad(1 a)=(b a) 2 e references.  
Affiliation: Dept. of Computer and Information Science University of Florida  
Abstract: Many database optimization activities, such as prefetching, data clustering and partitioning, and buffer allocation, depend on the detection of hot spots in access patterns. While a database designer can in some cases use special knowledge about the data and the users to predict hot spots, in general one must use information about past activity to predict future activity. However, algorithms that make use of hot spots pay little attention to the way in which hot spot information is gathered, or to the quality of this information. In this paper, we present a model for analyzing hot spot estimates based on frequency counting. We present a numerical method for estimating the quality of the data, and a rule-of-thumb. We find that if b of the references are made to the hottest a of the N data items, then one should process 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.I. Aven, E.G. Coffman, and Y.A. Kogan. </author> <title> Stochastic Analysis of Computer Storage. </title> <address> D. </address> <publisher> Reidel Publishing, </publisher> <year> 1987. </year>
Reference-contexts: We are given a reference string of length M , R = (r 1 ; r 2 ; : : : ; r M ). The reference string is generated by the independent reference model <ref> [1] </ref>, so that Pr [r i = d j ] = p j for all r i in R. We denote the distribution fp j g by P , and we assume that p i p i+1 for i = 1 : : : ; M 1. <p> Fortunately there is a simple first-order approximation. We will approximate N (0; 1; x) by a linear approximation N fl (0; 1; x), which is non-constant only in <ref> [1; 1] </ref>. As a result, we find that: P r [ref (j) &gt; K] &gt; &gt; &gt; &lt; 0 Mp j C1 + C2 Mp j Mp j K p 1 Mp j K p (1) where C1 = :5 and C2 = 1=( p 2) :3989. <p> Every data item in partition i is equally likely to be referenced. Because the partitioned distribution has an inherently discrete description, we take a different approach to calculating E [K] and V [K]. 1. set <ref> [1] </ref> = hits [1] = hits [fl] = 0. 2. For every K, (a) For every partition i, 10 i. Calculate the expected number of data items in partition i that will receive K references using the Poisson approximation to the binomial distribution Citation?. ii. <p> Every data item in partition i is equally likely to be referenced. Because the partitioned distribution has an inherently discrete description, we take a different approach to calculating E [K] and V [K]. 1. set <ref> [1] </ref> = hits [1] = hits [fl] = 0. 2. For every K, (a) For every partition i, 10 i. Calculate the expected number of data items in partition i that will receive K references using the Poisson approximation to the binomial distribution Citation?. ii.
Reference: [2] <author> R.E. Bechhofer. </author> <title> A single-sample multiple decision procedure for ranking means of normal populations with known variances. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 25 </volume> <pages> 16-39, </pages> <year> 1954. </year>
Reference-contexts: A good survey of the area is contained in the book by Gibbons, Olken, and Sobel [12]. 14 Bechhofer <ref> [2] </ref> worked on the original problems in this area. Given k populations, each with (unknown) mean X i and (known) variance 2 i , find the t populations with the largest (best) means. The best populations can be ranked or unranked. <p> Sobel and Huyett calculated the number of samples to collect by approximating the binomial distributions (the sum of Bernoulli samples) as a Normal distribution and applying the methods described by Bechhofer in <ref> [2] </ref>. An extention to selecting the t best Bernoulli populations out of K is analogous to the work in [24]. However, we are more concerned with buffer value than with finding the t best populations. <p> However, we are more concerned with buffer value than with finding the t best populations. Though a stopping rule with a similar form to our rule of thumb can be found in <ref> [2] </ref>, it depends on k and t (or N and b) in a complicated way, and the expansion constant has been computed only for small k and t.
Reference: [3] <author> R.E. Bechhofer, J. Kiefer, and M. Sobel. </author> <title> Sequential Identification and Ranking. </title> <publisher> University of Chicago Press, </publisher> <year> 1968. </year>
Reference-contexts: Later works in the area have considered the problem of finding the t best of k populations when s &gt; t populations are selected [10]. A large set of selection and ranking problems involving a variety of distributions and both known and unknown variances is discussed in <ref> [3] </ref>. In the problem addressed in this paper, the populations have a Bernoulli distribution and the same number of samples is drawn from each population.
Reference: [4] <author> R.E. Bechhofer and R.V. Kulkarni. </author> <title> Statistical Decision Theory and Related Topics III, </title> <journal> Vol. </journal> <volume> 1, </volume> <pages> pages 61-108. </pages> <publisher> Academic Press, </publisher> <year> 1982. </year> <month> 17 </month>
Reference-contexts: The Sobel and Huyett procedure was shown to be the optimal single-stage procedure [14]. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in <ref> [15, 5, 4, 20] </ref>. In [4], the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure). <p> The Sobel and Huyett procedure was shown to be the optimal single-stage procedure [14]. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in [15, 5, 4, 20]. In <ref> [4] </ref>, the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure). Jennison and Kulkarni [18] improve the work of [4] to minimize the number of samples that must be <p> In <ref> [4] </ref>, the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure). Jennison and Kulkarni [18] improve the work of [4] to minimize the number of samples that must be collected. The current work is most closely related to that of Sobel and Huyett [24], since every population (data item) has the same number of samples taken (M ).
Reference: [5] <author> H. Buringer, S.M. Johnson, and K-H Schriever. </author> <title> Nonparametric Sequential Selection Procedures. </title> <publisher> Birkhauser, </publisher> <year> 1980. </year>
Reference-contexts: The Sobel and Huyett procedure was shown to be the optimal single-stage procedure [14]. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in <ref> [15, 5, 4, 20] </ref>. In [4], the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure).
Reference: [6] <author> E.G. Coffman and P.J. Denning. </author> <title> Operating System Theory. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: We present a brief survey here. Suppose that references to database objects in a reference stream are independent and chosen from a stationary distribution. In this case, the optimal page replacement algorithm is the A 0 algorithm, which locks the most frequently accessed data objects into memory <ref> [6] </ref>. The layout of objects on a disk drive that minimizes head travel time is the "organ pipe arrangement" (place the most frequently accessed object in the middle, and more frequently accessed objects are placed closer to the center than less frequently accessed objects) [28].
Reference: [7] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in Bubba. </title> <booktitle> In ACM SIGMOD Conf., </booktitle> <year> 1988. </year>
Reference-contexts: Shared-nothing parallel database systems need to decluster the data stored in a relation, and scatter it across the processors so that the proper number of processors is involved in an average query. The declustering algorithms make use of statistics on average queries as part of their input <ref> [11, 7] </ref>. An object-oriented database typically consists of a number of objects which can have links to one another. Queries on the object base typically follow a path through the object links.
Reference: [8] <author> K.M. Curewitz, P. Krishnan, and J.S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In ACM SIGMOD Conf., </booktitle> <pages> pages 257-266, </pages> <year> 1993. </year>
Reference-contexts: The reference stream for accessing blocks in a file system often shows a great deal of regularity. Several authors have proposed algorithms that observe patterns in the reference stream, and pre-fetch blocks that were often requested after the previous request <ref> [8, 19, 22, 21] </ref>. For prefetching and object clustering, one does not count object accesses, rather one counts pairs of references. Hence, the set of objects is correspondingly larger and the available data correspondingly scarcer.
Reference: [9] <author> H.A. David. </author> <title> Order Statistics. </title> <publisher> John Wiley, </publisher> <year> 1981. </year>
Reference-contexts: Let R (s; i) be the probability that object d i is ranked s. Then <ref> [9] </ref> R (s; i) = 0 P (s1;i) j2A (1 F j (x)) j2B F j (x) f i (x)dx This integral is extremely difficult to evaluate for a large or an arbitrary number of objects in the system.
Reference: [10] <author> M.M. Desu and M. Sobel. </author> <title> A fixed subset-size approach to the selection problem. </title> <journal> Biometrika, </journal> <volume> 55(2) </volume> <pages> 401-410, </pages> <year> 1968. </year>
Reference-contexts: Later works in the area have considered the problem of finding the t best of k populations when s &gt; t populations are selected <ref> [10] </ref>. A large set of selection and ranking problems involving a variety of distributions and both known and unknown variances is discussed in [3]. In the problem addressed in this paper, the populations have a Bernoulli distribution and the same number of samples is drawn from each population.
Reference: [11] <author> S. Ghandeharizadeh, D.J. DeWitt, and W. Qureshi. </author> <title> A performance analysis of alternative multi-attribute declustering strategies. </title> <booktitle> In ACM SIGMOD Conf., </booktitle> <pages> pages 29-38, </pages> <year> 1992. </year>
Reference-contexts: Shared-nothing parallel database systems need to decluster the data stored in a relation, and scatter it across the processors so that the proper number of processors is involved in an average query. The declustering algorithms make use of statistics on average queries as part of their input <ref> [11, 7] </ref>. An object-oriented database typically consists of a number of objects which can have links to one another. Queries on the object base typically follow a path through the object links.
Reference: [12] <author> J.D. Gibbons, I. Olkin, and M. Sobel. </author> <title> Selecting and Ordering Populations: A New Statistical Methodology. </title> <publisher> John Wiley and Sons, </publisher> <year> 1977. </year>
Reference-contexts: A good survey of the area is contained in the book by Gibbons, Olken, and Sobel <ref> [12] </ref>. 14 Bechhofer [2] worked on the original problems in this area. Given k populations, each with (unknown) mean X i and (known) variance 2 i , find the t populations with the largest (best) means. The best populations can be ranked or unranked. <p> populations that I have selected are the t best with probability P ? For the case when all variances are equal and the populations have a Normal distribution, Bechhofer gives a formula for determining the number of samples to collect that looks similar to our rule of thumb (see also <ref> [12] </ref>, page 275). However, this formula has a different interpretation, and depends on an expansion factor that is a function of k, k, and P in a complicated way.
Reference: [13] <author> R.L. Graham, D.E. Knuth, and O. Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Since object d j is accessed independently on every reference with probability p j , the density function of the number of accesses over M references is binomial (M; p). The distribution is therefore an incomplete binomial sum, and does not have a closed form <ref> [13] </ref>. We instead use the normal approximation to the binomial distribution: binomial (M; p) N (M p; M p (1 p)) The factor of (1 p) in the variance of the normal distribution creates difficulties in the subsequent calculations.
Reference: [14] <author> W.J. Hall. </author> <title> The most economical character of bechhofer and sobel decision rules. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 30 </volume> <pages> 964-969, </pages> <year> 1959. </year>
Reference-contexts: The Sobel and Huyett procedure was shown to be the optimal single-stage procedure <ref> [14] </ref>. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in [15, 5, 4, 20].
Reference: [15] <author> D.G. Hoel, M. Sobel, and G.H. Weiss. </author> <booktitle> Perspectives in Biometry, </booktitle> <pages> pages 29-61. </pages> <publisher> Academic Press, </publisher> <year> 1975. </year>
Reference-contexts: The Sobel and Huyett procedure was shown to be the optimal single-stage procedure [14]. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in <ref> [15, 5, 4, 20] </ref>. In [4], the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure).
Reference: [16] <author> M.F. Hornick and S.B. Zdonick. </author> <title> A shared, segmented memory system for an object-oriented database. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 5(1), </volume> <year> 1987. </year>
Reference-contexts: As a result there has been much interest in placing objects that are `close' on the same page, to reduce the page miss rate. Many researchers have observed that some links are more often traversed than others, and have proposed algorithms that use this kind of information <ref> [26, 17, 16, 25] </ref>. Tsangaris and Naughton [27] have found that stochastic clustering out-performs other methods. The reference stream for accessing blocks in a file system often shows a great deal of regularity.
Reference: [17] <author> S.E. Hudson and R. King. Cactis: </author> <title> A self-adaptive, concurrent implementation of an object-oriented database management system. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 14(3) </volume> <pages> 291-321, </pages> <year> 1989. </year>
Reference-contexts: As a result there has been much interest in placing objects that are `close' on the same page, to reduce the page miss rate. Many researchers have observed that some links are more often traversed than others, and have proposed algorithms that use this kind of information <ref> [26, 17, 16, 25] </ref>. Tsangaris and Naughton [27] have found that stochastic clustering out-performs other methods. The reference stream for accessing blocks in a file system often shows a great deal of regularity.
Reference: [18] <author> C. Jennison and R.V. Kulkarni. </author> <title> Design of Experiments: </title> <booktitle> Ranking and Selection, </booktitle> <pages> pages 113-125. </pages> <publisher> Dekker, </publisher> <year> 1984. </year>
Reference-contexts: A survey of this work is found in [15, 5, 4, 20]. In [4], the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure). Jennison and Kulkarni <ref> [18] </ref> improve the work of [4] to minimize the number of samples that must be collected. The current work is most closely related to that of Sobel and Huyett [24], since every population (data item) has the same number of samples taken (M ).
Reference: [19] <author> D. Kotz and C.S. Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <year> 1990. </year> <month> 18 </month>
Reference-contexts: The reference stream for accessing blocks in a file system often shows a great deal of regularity. Several authors have proposed algorithms that observe patterns in the reference stream, and pre-fetch blocks that were often requested after the previous request <ref> [8, 19, 22, 21] </ref>. For prefetching and object clustering, one does not count object accesses, rather one counts pairs of references. Hence, the set of objects is correspondingly larger and the available data correspondingly scarcer.
Reference: [20] <author> R.V. Kulkarni and C. Jennison. </author> <title> Optimal properties of the bechhofer-kulkarni bernoulli selection proce-dure. </title> <journal> Annals of Statistics, </journal> <volume> 14(1) </volume> <pages> 298-314, </pages> <year> 1986. </year>
Reference-contexts: The Sobel and Huyett procedure was shown to be the optimal single-stage procedure [14]. The number of required samples can be reduced by adaptive sampling. A survey of this work is found in <ref> [15, 5, 4, 20] </ref>. In [4], the theory is extended to selecting the best 15 t out of k Bernoulli populations (this work shows that choosing the populations with the largest number of success is the best procedure).
Reference: [21] <author> M. Palmer and S.B. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proc. 17th Int'l Conf. of Very Large Databases, </booktitle> <pages> pages 255-264, </pages> <year> 1991. </year>
Reference-contexts: The reference stream for accessing blocks in a file system often shows a great deal of regularity. Several authors have proposed algorithms that observe patterns in the reference stream, and pre-fetch blocks that were often requested after the previous request <ref> [8, 19, 22, 21] </ref>. For prefetching and object clustering, one does not count object accesses, rather one counts pairs of references. Hence, the set of objects is correspondingly larger and the available data correspondingly scarcer.
Reference: [22] <author> K. Salem. </author> <title> Adaptive prefetching for disk buffers. </title> <type> Technical Report TR-91-64, </type> <institution> CESDIS, NASA Goddard Space Flight Center, </institution> <year> 1991. </year>
Reference-contexts: The reference stream for accessing blocks in a file system often shows a great deal of regularity. Several authors have proposed algorithms that observe patterns in the reference stream, and pre-fetch blocks that were often requested after the previous request <ref> [8, 19, 22, 21] </ref>. For prefetching and object clustering, one does not count object accesses, rather one counts pairs of references. Hence, the set of objects is correspondingly larger and the available data correspondingly scarcer.
Reference: [23] <author> K. Salem, D. Barbara, and R.J. Lipton. </author> <title> Probabilistic diagnosis of hot spots. </title> <booktitle> In IEEE Data Engineering Conf., </booktitle> <pages> pages 30-39, </pages> <year> 1992. </year>
Reference-contexts: In this work we make a contribution to determining when one has collected enough access frequency data to make 2 clustering, declustering, migration, and prefetching decisions. We would like to make note of a related work by Salem, Barbara, and Lipton <ref> [23] </ref>. These authors consider the problem of identifying the most frequently accessed objects in a very large reference string while using a limited amount of work space.
Reference: [24] <author> M. Sobel and M.J. Huyett. </author> <title> Selecting the one best of several binomial populations. </title> <journal> The Bell Systems Technical Journal, </journal> <volume> 36 </volume> <pages> 537-576, </pages> <year> 1957. </year>
Reference-contexts: In the problem addressed in this paper, the populations have a Bernoulli distribution and the same number of samples is drawn from each population. Sobel and Huyett <ref> [24] </ref> calculate the number of samples to collect when selecting the best one of k Bernoulli populations with confidence P , and the best population has a chance of success d larger than the second best population. <p> Jennison and Kulkarni [18] improve the work of [4] to minimize the number of samples that must be collected. The current work is most closely related to that of Sobel and Huyett <ref> [24] </ref>, since every population (data item) has the same number of samples taken (M ). Sobel and Huyett calculated the number of samples to collect by approximating the binomial distributions (the sum of Bernoulli samples) as a Normal distribution and applying the methods described by Bechhofer in [2]. <p> An extention to selecting the t best Bernoulli populations out of K is analogous to the work in <ref> [24] </ref>. However, we are more concerned with buffer value than with finding the t best populations.
Reference: [25] <author> J.W. Stamos. </author> <title> Static grouping of small objects to enhance performance of a paged memory system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(2) </volume> <pages> 155-180, </pages> <year> 1984. </year>
Reference-contexts: As a result there has been much interest in placing objects that are `close' on the same page, to reduce the page miss rate. Many researchers have observed that some links are more often traversed than others, and have proposed algorithms that use this kind of information <ref> [26, 17, 16, 25] </ref>. Tsangaris and Naughton [27] have found that stochastic clustering out-performs other methods. The reference stream for accessing blocks in a file system often shows a great deal of regularity.
Reference: [26] <author> M.M. Tsangaris and J.F. Naughton. </author> <title> A stochastic approach for clustering in object bases. </title> <booktitle> In ACM SIGMOD Conf., </booktitle> <pages> pages 12-21, </pages> <year> 1991. </year>
Reference-contexts: As a result there has been much interest in placing objects that are `close' on the same page, to reduce the page miss rate. Many researchers have observed that some links are more often traversed than others, and have proposed algorithms that use this kind of information <ref> [26, 17, 16, 25] </ref>. Tsangaris and Naughton [27] have found that stochastic clustering out-performs other methods. The reference stream for accessing blocks in a file system often shows a great deal of regularity.
Reference: [27] <author> M.M. Tsangaris and J.F. Naughton. </author> <title> On the performance of object clustering techniques. </title> <booktitle> In ACM SIGMOD Conf., </booktitle> <pages> pages 144-153, </pages> <year> 1992. </year>
Reference-contexts: Many researchers have observed that some links are more often traversed than others, and have proposed algorithms that use this kind of information [26, 17, 16, 25]. Tsangaris and Naughton <ref> [27] </ref> have found that stochastic clustering out-performs other methods. The reference stream for accessing blocks in a file system often shows a great deal of regularity.
Reference: [28] <author> C.K. Wong. </author> <title> Algorithmic Studies in Mass Storage Systems. </title> <publisher> Computer Science Press, </publisher> <year> 1983. </year> <month> 19 </month>
Reference-contexts: The layout of objects on a disk drive that minimizes head travel time is the "organ pipe arrangement" (place the most frequently accessed object in the middle, and more frequently accessed objects are placed closer to the center than less frequently accessed objects) <ref> [28] </ref>. Shared-nothing parallel database systems need to decluster the data stored in a relation, and scatter it across the processors so that the proper number of processors is involved in an average query. The declustering algorithms make use of statistics on average queries as part of their input [11, 7].
References-found: 28

