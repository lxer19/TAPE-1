URL: ftp://cns.brown.edu/nin/papers/phys-a.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: On the Combination of Supervised and Unsupervised Learning  reducing the overall error measure of a classifier.  
Author: Nathan Intrator 
Note: for  
Address: Ramat-Aviv, 69978 ISRAEL  
Affiliation: School of Mathematical Sciences Tel-Aviv University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Bichsel, M. and Seitz, P. </author> <year> (1989). </year> <title> Minimum class entropy: A maximum information approach to layered netowrks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 133-141. </pages>
Reference-contexts: Such methods include principal components constraints, in which one seeks an estimator which is decomposed out of functions biased towards projections onto the principal components of the covariance matrix of the data. Similarly, constraints can be added for seeking projections that maximize entropy <ref> (Bichsel and Seitz, 1989) </ref>. <p> An approach of this type has been used in image compression, with a penalty aimed at minimizing the entropy of the projected distribution <ref> (Bichsel and Seitz, 1989) </ref>. This penalty certainly measures deviation from normality, since entropy is maximized for a Gaussian distribution. The neuronal activity (in the linear region) is given by c = md. where d is the input vector and m is the synaptic weight vector (including a bias).
Reference: <author> Bienenstock, E. L., Cooper, L. N., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference: <author> Clothiaux, E. E., Cooper, L. N., and Bear, M. F. </author> <year> (1991). </year> <title> Synaptic plasticity in visual cortex: Comparison of theory with experiment. </title> <journal> Journal of Neurophysiology, </journal> <volume> 66 </volume> <pages> 1785-1804. </pages>
Reference-contexts: Such neuron allows modeling and theoretical analysis of various visual deprivation experiments (Intrator and Cooper, 1992), and is in agreement with the vast experimental results on visual cortical plasticity <ref> (Clothiaux et al., 1991) </ref>. A network implementation which can find several projections in parallel while retaining its computational efficiency, was found to be applicable for extracting features from very high dimensional vector spaces (Intrator et al., 1991; Intrator, 1992).
Reference: <author> Friedman, J. H. </author> <year> (1987). </year> <title> Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference-contexts: Based on prior knowledge about the underlying distribution of the data, a specific bias constraint is discussed as a mean for reducing generalization error for classification. The application to neural networks of the general statistical framework from which the bias constraints are drawn the Exploratory Projection Pursuit framework <ref> (Friedman, 1987) </ref> is discussed in more detail in (Intrator, 1993). 2 The Bias/Variance Dilemma In this section we present the bias/variance decomposition of a non-parametric estimator. For a thorough discussion of this problem in the context of neural networks see Geman et al. (1992).
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: To further see the performance under MSE we decompose the error to bias and variance <ref> (see 2 for example Geman et al., 1992) </ref> to get E D [(f D (x) E [yjx]) 2 ] = (E D [f D (x)] E [yjx]) 2 + E D [(f D (x) E D [f D (x)]) 2 ]: The first RHS term is called the bias of the
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Giles, C. L., Hanson, S. J., and Cowan, J. D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: the neural network framework they include methods such as weight decay and magnitude control of the weights (Plaut et al., 1986; Mozer and Smolensky, 1989) network pruning via weight elimination based on a simple threshold (Le Cun et al., 1990; Weigend et al., 1991) or based on the Hessian matrix <ref> (Hassibi and Stork, 1993) </ref>. A different approach for reducing the effective number of weights is weight sharing, in which a single weight is shared among many connections in the network (Waibel et al., 1989; Le Cun et al., 1989).
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit. (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference: <author> Intrator, N. </author> <year> (1992). </year> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 98-107. </pages>
Reference-contexts: It has been recently shown that a variant of the Bienenstock, Cooper and Munro neuron (Bi-enenstock et al., 1982, BCM) performs exploratory projection pursuit using a projection index that measures multi-modality. Such neuron allows modeling and theoretical analysis of various visual deprivation experiments <ref> (Intrator and Cooper, 1992) </ref>, and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991). <p> In the context of the hybrid network, this is an additional penalty to the energy minimization of the supervised network. Some related statistical and computational issues of this projection index as well as some applications are discussed in <ref> (Intrator, 1992) </ref>. 4 Summary A framework for introducing additional bias into a neural network architecture was presented. It is based on a penalty that allows the incorporation of additional prior information regarding the underlying data distribution, or model.
Reference: <author> Intrator, N. </author> <year> (1993). </year> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 509-521. </pages> <note> 5 Intrator, </note> <author> N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> 5:3-17. 
Reference-contexts: The application to neural networks of the general statistical framework from which the bias constraints are drawn the Exploratory Projection Pursuit framework (Friedman, 1987) is discussed in more detail in <ref> (Intrator, 1993) </ref>. 2 The Bias/Variance Dilemma In this section we present the bias/variance decomposition of a non-parametric estimator. For a thorough discussion of this problem in the context of neural networks see Geman et al. (1992).
Reference: <author> Intrator, N., Gold, J. I., Bulthoff, H. H., and Edelman, S. </author> <year> (1991). </year> <title> Three-dimensional object recognition using an unsupervised neural network: Understanding the distinguishing features. </title> <editor> In Feldman, Y. and Bruckstein, A., editors, </editor> <booktitle> Proceedings of the 8th Israeli Conference on AICV, </booktitle> <pages> pages 113-123. </pages> <publisher> Elsevier. </publisher>
Reference: <author> Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551. </pages>
Reference: <author> Le Cun, Y., Denker, J., and Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. C. and Smolensky, P. </author> <year> (1989). </year> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 3-16. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: An extension of this idea is the "soft weight sharing" which favors irregularities in the weight distribution in the form of multimodality <ref> (Nowlan and Hinton, 1992) </ref>. All these methods make explicit assumptions about the structure of the weight space, but with no regard to the structure of the input space. The second category contains methods that have general assumptions about the underlying distribution and important structure in the data.
Reference: <author> Plaut, D. C., Nowlan, S. J., and Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by back-propagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on ASSP, </journal> <volume> 37 </volume> <pages> 328-339. </pages>
Reference: <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
References-found: 17

