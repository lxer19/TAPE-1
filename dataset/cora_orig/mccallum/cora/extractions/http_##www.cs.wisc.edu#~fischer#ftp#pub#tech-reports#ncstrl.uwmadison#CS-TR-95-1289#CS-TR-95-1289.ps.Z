URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1289/CS-TR-95-1289.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1289/
Root-URL: http://www.cs.wisc.edu
Keyword: synchronization, networks of workstations, massively-parallel processors, cost/performance.  
Date: Abstract  
Pubnum: 1  
Abstract: Networks of workstations (NOWs) are gaining popularity as lower-cost alternatives to massively-parallel processors (MPPs) because of their ability to leverage high-performance commodity workstations and data networks. However, fast data networks will not suffice if applications require frequent global synchronization, e.g., barriers, reductions, and broadcasts. Many MPPs provide hardware support specifically to accelerate these operations. Separate synchronization networks have also been proposed for NOWs, but such add-on hardware only makes sense if the performance improvement is commensurate with its cost. In this study, we examine the cost/performance tradeoff of add-on synchronization hardware for an emulated 32-node NOW, running an aggregate workload of nine shared-memory, message-passing, and data-parallel applications. For low-latency messaging (e.g., ~10 ms), add-on hardware is cost-effective only if its per-node cost is less than 8% of the base workstation cost. For higher-latency messages (e.g., ~100 ms), add-on hardware is cost-effective if it costs less than 23% of the base cost. Of course, individual applications behave differently: four of the nine applications show no benefit from the extra hardware, while one application improves by over a factor of three with higher-latency messages. This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. B550, NSF PYI Award CCR-9157366, NSF Grant MIP-9225097, and donations from A.T.&T. Bell Laboratories, Digital Equipment Corporation, Sun Microsystems, Thinking Machines Corporation, and Xerox Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the University of Wiscon-sin Graduate School. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Applu is a computational uid dynamics code which solves five coupled parabolic/elliptic partial differential equations <ref> [1] </ref>. The computation consists of successive over-relaxation iterations with a barrier after each iteration. Dycore computes the equations of motion for a grid-point atmospheric global climate model [28]. Barriers are invoked between phases of computation and near-neighbor communication.
Reference: [2] <author> C. Beaumont, P. Boronat, J. Champeau, J.-M. Filloque, and B. Pottier. </author> <title> Reconfigurable technology: An innovative solution for parallel discrete event simulation support. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation (PADS 94), </booktitle> <pages> pages 160163, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Reynolds proposed a separate synchronization network to compute minimum next-event times, minimum times-tamps of unacknowledged messages, and to compute Network-Done [23]. Beaumont et al. propose to dynamically synthesize application-specific hardware synchronization for a desired simulator by using FPGAs <ref> [2] </ref>. For synchronous parallel discrete event simulators, they suggest synthesizing a barrier and a MIN reduction to determine the minimum of all next-event times. 7 Summary and conclusions This paper examined the cost/performance tradeoffs of adding a separate synchronization network to a network of workstations.
Reference: [3] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathon Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: While current local-area network performance is poor by MPP standards (latencies in the 100s to 1000s of microseconds and bandwidths in the 10s to 100s of Mbits/ second), emerging networks promise better performance. For example, repackaged multicomputer interconnects, such as Myrinet [4] and Shrimp <ref> [3] </ref>, may yield up to two orders-of-magnitude performance improvement over previous-generation local-area networks. However, high-performance CPUs and data networks will not be sufficient for NOWs to achieve good speedups for all existing parallel applications.
Reference: [4] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: While current local-area network performance is poor by MPP standards (latencies in the 100s to 1000s of microseconds and bandwidths in the 10s to 100s of Mbits/ second), emerging networks promise better performance. For example, repackaged multicomputer interconnects, such as Myrinet <ref> [4] </ref> and Shrimp [3], may yield up to two orders-of-magnitude performance improvement over previous-generation local-area networks. However, high-performance CPUs and data networks will not be sufficient for NOWs to achieve good speedups for all existing parallel applications.
Reference: [5] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 858867, </pages> <address> Cancun, Mexico, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: successive communication operations, such as cyclic shifts, actually improved performance by up to a factor of three, because a message destined for a processor A still working on the first operation could not be delayed by a message sent to A from a processor B working on the second operation <ref> [5] </ref>. We encounter similar conicts between sets of messages in our benchmarks, and throttling yields substantial benefit. We determine the delaying parameters experimentally. We first examine the HW-1 system. With zero-delayed messages, the hardware synchronization provides only modest benefit: 16% for metspin and 4% for nbody. <p> Fast barrier synchronization has been found to speed up specific patterns of communication and computation. Brewer and Kuszmaul found that using the hardware barrier on the CM-5 to limit the rate of message injection and limit congestion improved performance by more than a factor of three <ref> [5] </ref>. The direct deposit message-passing library of Stricker et al. [27] uses hardware barriers, rather than employing buffering or handshaking, to ensure that messages have been delivered to their destination. For a 2-D FFT code, their system runs approximately 2.8 times faster than an optimized request-response message-passing library.
Reference: [6] <author> Edward C. Bronson, Thomas L. Casavant, and Leah H. Jamieson. </author> <title> Experimental Application-Driven Architecture Analysis of an SIMD/MIMD Parallel Processing System. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2):202215, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Other machines besides the T3D, VPP500, and CM-5 have provided synchronization hardware separate from the data network. PASM, a hybrid SIMD/MIMD machine, uses its SIMD synchronous instruction-fetch mechanism as a barrier when in MIMD mode <ref> [6] </ref>. A MIMD version of a FFT benchmark with the hardware barrier support was 39% faster than a MIMD version without the hardware barrier. Fast barrier synchronization has been found to speed up specific patterns of communication and computation.
Reference: [7] <author> E. D. Brooks III. </author> <title> The butterfly barrier. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4):295307, </volume> <year> 1986. </year>
Reference-contexts: Consider Figure 1, which depicts radix-2 and radix-4 tournament barriers for 4 processors, with significant send and receive overhead. The radix-4 solution hides much more of the messaging overhead, resulting in lower latency: O (2 log 4 N (L + S + R)). Buttery barriers <ref> [7] </ref> eliminate the factor of 2 by effectively performing multiple tournament-arrival binary trees in parallel, with each processor at the root of a different arrival tree. Figure 2 illustrates the messaging pattern for radix-2 and radix-4 buttery barriers for 4 nodes.
Reference: [8] <author> David Culler, Richard Karp, David Patterson, Abhijit Sahay, Klaus Erik Schauser, Eunice Santos, Ramesh Subramonian, and Thorsten von Eicken. </author> <title> LogP: Toward a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 112, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Reductions which require delivering the result to all nodes are essentially simple barriers which also send data, and hence we implement them with a buttery-style combining pattern. Reductions which deliver the result to a single node and broadcasts are implemented via unbalanced trees <ref> [8] </ref> in which the fanout of nodes is set according to the latency and overhead of messages. 2.2 Hardware synchronization Add-on synchronization hardware for a NOW consists of two separate components: the workstation interface and the synchronization network itself.
Reference: [9] <author> H. G. Dietz, W. E. Cohen, T. Muhammad, and T. I. Mattox. </author> <title> Compiler Techniques For Fine-Grain Execution On Workstation Clusters Using PAPERS. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: NOWs can also employ synchronization hardware in the form of a separate add-on synchronization network. For example, Dietz, et al., have developed barrier hardware which connects to the standard Centronics parallel port of an IBM-compatible PC <ref> [9] </ref>. Hall and Driscoll have proposed a synchronization network for Sun workstation clusters that suppors barriers, 64-bit reductions, and broadcasts [11]. Shang and Hwang have proposed barrier hardware for cluster-based multiprocessors, including workstation clusters [25]. <p> At the low-end, most workstations provide a parallel port that can be used for low-bandwidth operations such as simple barriers. Parallel ports typically require system calls for user-level access, but in some systems can be memory-mapped directly into user space <ref> [9] </ref>. Higher performance, at higher cost, can be obtained by interfacing to the workstations I/ time FIGURE 1. Tournament barriers of radix-2 (left) and radix-4 (right) for 4 nodes. The I-bars represent send/receive overhead. P0 P2 P3P1 P0 P2 P3P1 - 4 - O bus. <p> For example, Dietz, et al., estimate that the PAPERS add-on hardwarewhich interfaces to the workstations parallel port and implements simple barriers and binary reductionshas a parts cost of less than $50 <ref> [9] </ref>. We assume a commercial implementation of PAPERS would cost at least $100 per node. More complex func-tionalityfor example, integer or oating-point reduc-tionsrequires more expensive hardware such as FPGAs or ASICs. Interfacing to the workstations I/O or memory bus can also be expensive, particularly for low-volume parts like synchronization hardware. <p> The parallel function invocations from the host processor necessitate the HW-All network; a decentralized SPMD-style computational model would lessen this necessity. 6 Related work Numerous proposals for add-on synchronization hardware have recently appeared. PAPERS <ref> [9] </ref> is a low-cost synchronization network which supports fine-grain execution on workstation clusters, specifically operations on data aggregates as in a data-parallel program and VLIW-style execution. Among other operations, PAPERS supports a simple barrier and a single-bit AND operation, both with latencies of 2 ms.
Reference: [10] <author> G. Fox et al. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The basic computation is successive over-relaxation, with red-black iterations performed on a 2-D grid. Nbody calculates the force between N bodies interacting via long-range forces <ref> [10] </ref>. Both metspin and nbody use barriers after cyclic shifts of arrays, broadcasts to distribute both code pointers and data to nodes, and a Network-Done operation to determine the completion of cyclic shifts of arrays.
Reference: [11] <author> Douglas V. Hall and Michael A. Driscoll. </author> <title> Hardware for Fast Global Operations on Multicomputers. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 673679, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: For example, Dietz, et al., have developed barrier hardware which connects to the standard Centronics parallel port of an IBM-compatible PC [9]. Hall and Driscoll have proposed a synchronization network for Sun workstation clusters that suppors barriers, 64-bit reductions, and broadcasts <ref> [11] </ref>. Shang and Hwang have proposed barrier hardware for cluster-based multiprocessors, including workstation clusters [25]. However, whether or not such add-on synchronization hardware is cost-effective depends upon its cost, the cost of the base NOW, and the performance improvement which the synchronization hardware provides. <p> Among other operations, PAPERS supports a simple barrier and a single-bit AND operation, both with latencies of 2 ms. The design interfaces to a network of PCs running Linux, with connections through each PCs parallel port. Hall and Driscolls COP network <ref> [11] </ref> provides synchronization support equivalent to the HW-All network; they claim that its cost is 2-3% of overall system cost, which for our workload is clearly feasible. Shang and Hwang have designed add-on synchronization hardware for cluster-based multiprocessors, allowing synchronization to be performed both within and between clusters [25].
Reference: [12] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1):117, </volume> <year> 1988. </year>
Reference-contexts: Then the barrier latency is 2L + (N+1)R + (N+1)S. Assuming sufficient network bandwidth, the message latencies can overlap, but the overheads which the master incurs must serialize. The overheads cannot be eliminated, but their impact can be lessened by distributing them among the processors. Tournament barriers <ref> [12, 15] </ref> distribute these overheads by having processors perform the arrival phase in pairs (radix-2 combining), forming a tree. With sufficient network bandwidth, the wakeup phase can also be performed in a tree fashion [15].
Reference: [13] <author> R. E. Kessler and J. L. Schwarzmeier. </author> <title> CRAY T3D: A New Dimension for Cray Research. </title> <booktitle> In Proceedings of COMPCON 93, </booktitle> <pages> pages 176182, </pages> <address> San Francisco, California, </address> <month> Spring </month> <year> 1993. </year>
Reference-contexts: Some applications require frequent synchronization to coordinate computation (e.g. barriers), to compute global results (e.g. reductions), or update common data structures (e.g. broadcasts). To address the requirements of these applications, many MPPsthe Cray T3D <ref> [13] </ref>, the Fujitsu VPP500 [29], and the Thinking Machines CM-5 [14]provide explicit hardware support for global synchronization. NOWs can also employ synchronization hardware in the form of a separate add-on synchronization network.
Reference: [14] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> July </month> <year> 1992. </year>
Reference: [15] <author> B. Lubachevsky. </author> <title> Synchronization barrier and related tools for shared memory parallel programming. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II175II179, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Then the barrier latency is 2L + (N+1)R + (N+1)S. Assuming sufficient network bandwidth, the message latencies can overlap, but the overheads which the master incurs must serialize. The overheads cannot be eliminated, but their impact can be lessened by distributing them among the processors. Tournament barriers <ref> [12, 15] </ref> distribute these overheads by having processors perform the arrival phase in pairs (radix-2 combining), forming a tree. With sufficient network bandwidth, the wakeup phase can also be performed in a tree fashion [15]. <p> Tournament barriers [12, 15] distribute these overheads by having processors perform the arrival phase in pairs (radix-2 combining), forming a tree. With sufficient network bandwidth, the wakeup phase can also be performed in a tree fashion <ref> [15] </ref>. Since for every message sent there is exactly one send and one receive overhead latency incurred, the latency of this barrier is then O (2 log 2 N (L + S + R)), where the factor of 2 comes from the two separate trees.
Reference: [16] <author> Shubhendu S. Mukherjee, Shamik D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Bar-nes invokes barriers after creation and traveral of octrees, and after updates are made to global values. DSMC is a rarefied gas simulation, computing interactions between molecules in a 3D box <ref> [16] </ref>. DSMC invokes barriers after new molecules enter the box, after simulating the collision of molecules, and after moving molecules into new space cells. Moldyn calculates the motion of atomic particles based on forces acting on each particle from particles within a certain radius [16]. <p> between molecules in a 3D box <ref> [16] </ref>. DSMC invokes barriers after new molecules enter the box, after simulating the collision of molecules, and after moving molecules into new space cells. Moldyn calculates the motion of atomic particles based on forces acting on each particle from particles within a certain radius [16]. Barriers are invoked after calculating the forces on and velocities of molecules, and after updating the coordinates of molecules. Water is a molecular dynamics simulation, computing the interactions among water molecules [26].
Reference: [17] <author> Howard T. Olnowich. </author> <title> ALLNODE Barrier Synchronization Network. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 265269, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The ALLNODE barrier synchronization hardware uses the broadcast facility of the Allnode switch to perform barrier operations; an arbitrary number of nodes can synchronize in a few microseconds, and the mechanism consumes less than 5% of network bandwidth <ref> [17] </ref>. Other machines besides the T3D, VPP500, and CM-5 have provided synchronization hardware separate from the data network. PASM, a hybrid SIMD/MIMD machine, uses its SIMD synchronous instruction-fetch mechanism as a barrier when in MIMD mode [6].
Reference: [18] <author> G. Parisi. </author> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: However, WWT does not employ this feature. NOW is connected to an additional workstation which functions as the host. Metspin uses the Metropolis Monte Carlo algorithm to simulate an Ising spin model of a ferromagnet and calculate the energy and magnetization at a particular temperature <ref> [18] </ref>. The basic computation is successive over-relaxation, with red-black iterations performed on a 2-D grid. Nbody calculates the force between N bodies interacting via long-range forces [10].
Reference: [19] <author> Steven Przybylski, Mark Horowitz, and John Hennessy. </author> <title> Performance Tradeoffs in Cache Design. </title> <booktitle> In 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 290298, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To make our results independent of any particular hardware implementation, we calculate the break-even cost <ref> [19] </ref> for add-on synchronization hardware by making Equation 1 an equality and solving for C synch : Thus synchronization hardware is cost-effective (for a particular workload W) if its actual cost is less than the break-even cost C * synch . 5.2 System cost Without loss of generality, we make C
Reference: [20] <author> Vara Ramakrishnan, Isaac D. Scherson, and Raghu Subramanian. </author> <title> Efficient Techniques for Fast Nested Barrier Synchronization. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <year> 1995. </year>
Reference-contexts: For a 2-D FFT code, their system runs approximately 2.8 times faster than an optimized request-response message-passing library. Ramakrishnan et al. <ref> [20] </ref> present two methods for efficiently supporting deep control nesting in data-parallel programs by using synchronization hardware. The first solution employs a pair of single-bit OR and AND reductions and code transformations, and the second solution requires a MAX reduction but no code transformations.
Reference: [21] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM - 14 - Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, for the one code we examined in detail, water [26], the barrier version is 58% faster than the (original) locking ver sion even without hardware barrier support. Other applications cannot be restructured as easily to avoid global synchronization, such as the Wisconsin Wind Tunnel (WWT) <ref> [21] </ref>. While most applications synchronize to ensure that messages have been delivered, WWT frequently synchronizes to ensure that no messages are in ight. <p> Barriers are invoked between phases of computation and near-neighbor communication. We ran applu and dycore on Blizzard, using Blizzard-provided mechanisms to implement message-passing functions for these applications. Unlike the above applications, WWT is a native CM-5 program. WWT is a parallel discrete-event simulator which simulates cache-coherent distributed-shared-memory multiprocessors <ref> [21] </ref>.
Reference: [22] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Blizzard uses the user-level Stache protocola COMA-like invalidation proto-colto maintain sequentially consistent shared memory <ref> [22] </ref>. Barnes and water use Stache to maintain coherence on 128-byte blocks, and DSMC and moldyn maintain coherence on 1024-byte blocks. 3.2 Message-passing benchmarks The message-passing benchmarks are applu, dycore, and the Wisconsin Wind Tunnel (WWT).
Reference: [23] <author> Paul F. Reynolds, Jr. </author> <title> An Efficient Framework for Parallel Simulations. </title> <booktitle> In Proceedings of the SCS multi-conference on Advances in Parallel and Distributed Simulation, </booktitle> <pages> pages 167174, </pages> <address> Anaheim, CA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: Additional work has looked at synchronization hardware specifically for parallel discrete-event simulators. Reynolds proposed a separate synchronization network to compute minimum next-event times, minimum times-tamps of unacknowledged messages, and to compute Network-Done <ref> [23] </ref>. Beaumont et al. propose to dynamically synthesize application-specific hardware synchronization for a desired simulator by using FPGAs [2].
Reference: [24] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297307, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Barriers are invoked after updates to molecules. All of these benchmarks were parallelized by hand using a locally-modified version of the PARMACS macro package and were run on Blizzard, a software system that provides fine-grain distributed shared memory on the Thinking Machines CM-5 <ref> [24] </ref>. Blizzard uses the user-level Stache protocola COMA-like invalidation proto-colto maintain sequentially consistent shared memory [22].
Reference: [25] <author> Shisheng Shang and Kai Hwang. </author> <title> Distributed Hardwired Barrier Synchronization for Scalable Multiprocessor Clusters. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(6):591605, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: Hall and Driscoll have proposed a synchronization network for Sun workstation clusters that suppors barriers, 64-bit reductions, and broadcasts [11]. Shang and Hwang have proposed barrier hardware for cluster-based multiprocessors, including workstation clusters <ref> [25] </ref>. However, whether or not such add-on synchronization hardware is cost-effective depends upon its cost, the cost of the base NOW, and the performance improvement which the synchronization hardware provides. <p> Shang and Hwang have designed add-on synchronization hardware for cluster-based multiprocessors, allowing synchronization to be performed both within and between clusters <ref> [25] </ref>. The ALLNODE barrier synchronization hardware uses the broadcast facility of the Allnode switch to perform barrier operations; an arbitrary number of nodes can synchronize in a few microseconds, and the mechanism consumes less than 5% of network bandwidth [17].
Reference: [26] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The HW-All hardware yields further improvement, increasing performance by factors of up to 2.5 and 3.6, for the two network latencies. Some applications can be restructured to avoid using global synchronization operations, decreasing the performance benefit. However, for the one code we examined in detail, water <ref> [26] </ref>, the barrier version is 58% faster than the (original) locking ver sion even without hardware barrier support. Other applications cannot be restructured as easily to avoid global synchronization, such as the Wisconsin Wind Tunnel (WWT) [21]. <p> Barnes uses the Barnes-Hut algorithm to calculate N-body interactions <ref> [26] </ref>. To obtain better speedup from barnes, we used a modified version which allocates free cells in a processors local portion of shared memory, obviating the original global free cell pool 1 . Bar-nes invokes barriers after creation and traveral of octrees, and after updates are made to global values. <p> Barriers are invoked after calculating the forces on and velocities of molecules, and after updating the coordinates of molecules. Water is a molecular dynamics simulation, computing the interactions among water molecules <ref> [26] </ref>. For water, we use a modified version which is restructured to use barriers instead of locks; this version provides better performance 1. This modification is similar to the newly-released SPLASH-2 version [30]. on our platform. Barriers are invoked after updates to molecules. <p> On the other hand, water synchronizes much more frequently, and thus hardware barrier support improves performance substantially. Performance improves 15% with fast messages and 41% with 100 ms-delayed messages. In this study we used a restructured version of water <ref> [26] </ref> that uses barriers instead of locks as the primary synchronization operation. Surprisingly, this version runs consistently faster than the (original) locking version, even without hardware barrier support.
Reference: [27] <author> T. Stricker, J. Stichnoth, D. OHallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling Synchronization and Data Transfer in Message Passing Systems of Parallel Computers. </title> <booktitle> In Proceedings of the Ninth International Conference on Supercomputing (ICS), </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Brewer and Kuszmaul found that using the hardware barrier on the CM-5 to limit the rate of message injection and limit congestion improved performance by more than a factor of three [5]. The direct deposit message-passing library of Stricker et al. <ref> [27] </ref> uses hardware barriers, rather than employing buffering or handshaking, to ensure that messages have been delivered to their destination. For a 2-D FFT code, their system runs approximately 2.8 times faster than an optimized request-response message-passing library.
Reference: [28] <editor> Max J. Suarez and Lawrence L. Takacs. </editor> <title> Documentation of the ARIES/GEOS Dynamical Core: </title> <type> Version 2. Technical Report 104606, Vol. 5, </type> <institution> NASA Goddard Space Flight Center, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Applu is a computational uid dynamics code which solves five coupled parabolic/elliptic partial differential equations [1]. The computation consists of successive over-relaxation iterations with a barrier after each iteration. Dycore computes the equations of motion for a grid-point atmospheric global climate model <ref> [28] </ref>. Barriers are invoked between phases of computation and near-neighbor communication. We ran applu and dycore on Blizzard, using Blizzard-provided mechanisms to implement message-passing functions for these applications. Unlike the above applications, WWT is a native CM-5 program. WWT is a parallel discrete-event simulator which simulates cache-coherent distributed-shared-memory multiprocessors [21].
Reference: [29] <author> Teruo Utsumi, Masayuki Ikeda, and Moriyuki Takamura. </author> <title> Architecture of the VPP500 Parallel Supercomputer. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 478487, </pages> <address> Washington, D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Some applications require frequent synchronization to coordinate computation (e.g. barriers), to compute global results (e.g. reductions), or update common data structures (e.g. broadcasts). To address the requirements of these applications, many MPPsthe Cray T3D [13], the Fujitsu VPP500 <ref> [29] </ref>, and the Thinking Machines CM-5 [14]provide explicit hardware support for global synchronization. NOWs can also employ synchronization hardware in the form of a separate add-on synchronization network. For example, Dietz, et al., have developed barrier hardware which connects to the standard Centronics parallel port of an IBM-compatible PC [9].
Reference: [30] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Water is a molecular dynamics simulation, computing the interactions among water molecules [26]. For water, we use a modified version which is restructured to use barriers instead of locks; this version provides better performance 1. This modification is similar to the newly-released SPLASH-2 version <ref> [30] </ref>. on our platform. Barriers are invoked after updates to molecules. All of these benchmarks were parallelized by hand using a locally-modified version of the PARMACS macro package and were run on Blizzard, a software system that provides fine-grain distributed shared memory on the Thinking Machines CM-5 [24].
Reference: [31] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156168, </pages> <month> May </month> <year> 1993. </year> <note> Also appeared in CMG Transactions, Spring 1994. </note>
Reference-contexts: Table 5 presents the speedups for WWT for the three systems. The experiments involve WWT simulating a 32-node shared-memory multiprocessor, running four shared-memory programs with the Dir1SW+ protocol <ref> [31] </ref>. We use four different programs in order to observe how different communication patterns affect WWTs performance. We first examine the HW-1 system.
Reference: [32] <author> David A. Wood and Mark D. Hill. </author> <title> Cost-Effective Parallel Computing. </title> <journal> IEEE Computer, </journal> <volume> 28(2):6972, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Cost Rahmat S. Hyder and David A. Wood Computer Sciences Department University of WisconsinMadison 1210 West Dayton Street Madison, WI 53706 wwt@cs.wisc.edu - 2 - costs $2000 per node, then performance must improve by at least 10% for the synchronization hardware to be cost-effective <ref> [32] </ref>. While individual applications may improve this much, or more, the add-on hardware is only cost-effective if the aggregate performance of the NOWs entire workload improves by 10%. In this paper, we examine the cost/performance tradeoff of add-on synchronization hardware for a NOW. <p> Section 5.3 presents the resulting cost/ performance break-even points for our applications and for the entire workload. 5.1 A cost/performance model Intuitively, a performance enhancement to a computer system is cost-effective only if the increase in performance exceeds the increase in cost. Wood and Hill <ref> [32] </ref> recently formalized this intuition by showing that parallel comput ing is more cost-effective than uniprocessor computing whenever the following inequality holds: speedup (p) &gt; costup (p) where speedup (p) is the runtime on one processor divided by the runtime on p processors, and costup (p) is the cost of a
References-found: 32

