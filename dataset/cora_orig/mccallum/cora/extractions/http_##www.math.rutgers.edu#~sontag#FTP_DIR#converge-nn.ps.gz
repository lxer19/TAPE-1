URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/converge-nn.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: BACKPROPAGATION SEPARATES WHERE PERCEPTRONS DO  
Author: Eduardo D. Sontag Hector J. Sussmann 
Address: New Brunswick, NJ 08903  
Affiliation: Rutgers Center for Systems and Control Department of Mathematics, Rutgers University,  
Abstract: Feedforward nets with sigmoidal activation functions are often designed by minimizing a cost criterion. It has been pointed out before that this technique may be outperformed by the classical perceptron learning rule, at least on some problems. In this paper, we show that no such pathologies can arise if the error criterion is of a threshold LMS type, i.e., is zero for values "beyond" the desired target values. More precisely, we show that if the data are linearly separable, and one considers nets with no hidden neurons, then an error function as above cannot have any local minima that are not global. Simulations of networks with hidden units are consistent with these results, in that often data which can be classified when minimizing a threshold LMS criterion may fail to be classified when using instead a simple LMS cost. In addition, the proof gives the following stronger result, under the stated hypotheses: the continuous gradient adjustment procedure is such that from any initial weight configuration a separating set of weights is obtained in finite time. This is a precise analogue of the Perceptron Learning Theorem. The results are then compared with the more classical pattern recognition problem of threshold LMS with linear activations, where no spurious local minima exist even for nonseparable data: here it is shown that even if using the threshold criterion, such bad local minima may occur, if the data are not separable and sigmoids are used. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blum, E.K. </author> <title> (1989) Approximation of Boolean functions by sigmoidal networks: Part I: XOR and other two-variable functions. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 532-540. </pages>
Reference-contexts: For more on the rigorous analysis of local minima in the multilayer case, the reader is referred to <ref> (Blum, 1989) </ref>. 2 Perceptrons Versus Error Minimization Neural nets are typically applied to solve binary classification problems. In these, labeled examples and counterexamples are presented during a training stage, and weights are adjusted so as to make the network's numerical output match in some sense the desired classification. <p> One may expect that the same pathologies will arise in the more general case; indeed, this has recently been shown by <ref> (Blum, 1989) </ref>. There are two very different reasons for the above local minima to appear. One of the reasons has to do with the highly nonconvex nature of the problem, when using nonlinear functions . We discuss this in Section 4.
Reference: <author> Brady, M., Raghavan, R., and Slawny, J. </author> <title> (1989) Backpropagation fails to separate where perceptrons succeed. </title> <journal> IEEE Trans. Circuits and Systems, </journal> <volume> 36, </volume> <pages> 665-674. </pages>
Reference-contexts: Keywords: Backpropagation, pattern classification, nonlinear least squares, neural networks Phone: (908)932-3072; email: sontag@hilbert.rutgers.edu 1 targets, and (b) in the threshold case one indeed has, under certain assumptions on the activa-tion functions, a convergence theorem that closely parallels that for perceptrons. The apparent contradiction with the title of <ref> (Brady, Raghavan, & Slawny, 1989) </ref> is explained by the fact that this latter reference did not use threshold but rather "exact" LMS. In this paper, we extend the result in (Wittner & Denker, 1987) so as to include sigmoidal neurons, which were excluded by their assumptions. <p> Spurious (i.e., non-global) local minima can occur even if the data are separable. Moreover, even when there happens to be only one local (and thus necessarily global) minimum, the resulting solution may fail to separate <ref> (hence the title of Brady, Raghavan, & Slawny, 1989) </ref>. <p> 1) ; + for which, with " i = 1 for i 33 and " i = 1 otherwise, E (x) = i=1 has a local minimum which is not global, when = tanh. (Note that one must use the examples from (Sontag & Sussmann, 1989) rather than those from <ref> (Brady, Raghavan, & Slawny, 1989) </ref> or (Sontag, 1988), not just because of the interest in binary examples, but also because in the latter references outputs are not allowed to take limiting values f1; 1g, which will be critical below.) The main result from (Sontag & Sussmann, 1989) is then: The above
Reference: <author> Duda, R.O., and Hart, P.E. </author> <title> (1973) Pattern Classification and Scene Analysis, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Then we provide some examples, and in particular we show that if the training set is not separable, there may be nonglobal local minima even if a threshold LMS is used. We also compare the situation with the case of linear response units <ref> (Duda & Hart, 1973, pp.148-149) </ref>, and remark that a basic difference with that case is due to the lack of convexity in the cost function: for linear activations, there are no spurious local minima even for nonseparable data, in contrast to sigmoidal nets. <p> The problem of determining if there exists such an x fl is a simple linear programming question, and there are very efficient methods for solving it as well as for finding explicit solutions x fl . More in connection with nets, the classical perceptron learning procedure <ref> (see e.g. Duda & Hart, 1973) </ref> provides a recursive rule for finding one such solution provided that any exist. Although the perceptron rule is very simple, we will briefly recall it, so we can compare it with error minimization. <p> It is well-known <ref> (see e.g. Duda & Hart, 1973) </ref> that this procedure converges in finitely many steps to a solution x fl if the original data are linearly separable. <p> Results Th above discussion serves also to illustrate the substantial difference that exists between the case of interest in neural nets, when a nonlinear function is used, and a standard case in pattern recognition, that of threshold cost functions as before but with (a) = a. (The "relaxation case" in <ref> (Duda & Hart, 1973, pp.147ff) </ref>.) In that case, there are no nonglobal local minima even if the data are not separable. This is proved as follows.
Reference: <author> Hinton, G.E. </author> <title> (1987) Connectionist learning procedures (Technical Report CMU-CS-87-115, </title> <institution> Comp.Sci. Dept.), Pittsburg: Carnegie-Mellon University. </institution>
Reference: <author> Isaacson, E., and Keller, H.B. </author> <title> (1966) Analysis of Numerical methods, </title> <address> New York: </address> <publisher> Wiley. </publisher> <address> 11 LaSalle, </address> <month> J.P. </month> <title> (1976) The Stability of Dynamical Systems, </title> <address> Philadelphia: </address> <publisher> SIAM Publications. </publisher>
Reference-contexts: Euler algorithm for calculating the solution of (8) and one knows that, if x k denotes the solution of the Euler iteration at time k using := t 0 =k, then kx (t 0 ) x k k = O ( k which goes to zero as k ! 1 <ref> (Isaacson & Keller, 1966, chapter 8) </ref>.
Reference: <editor> Rumelhart, D.E., and McClelland, J.L. </editor> <booktitle> (1986) Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> Cambridge: MIT Press. </publisher>
Reference: <author> Shrivastava, Y., and S. </author> <title> Dasgupta (1987) Convergence issues in perceptron based adap-tive neural network models. </title> <booktitle> In Proc.25th. Allerton Conf. Comm. Contr. and Comp. </booktitle> <pages> (pp. 1133-1141), </pages> <address> Urbana: U.of Illinois. </address>
Reference-contexts: The first of these papers <ref> (see also Shrivastava & Dasgupta, 1987) </ref> pointed out that (a) it might be possible to overcome these difficulties by using a threshold LMS procedure, where one does not penalize numerical values which are already beyond the 1 This work was partially supported by NSF grants DMS88-03396 and DMS89-02994, and by the <p> This will not happen in general in the nonlinear case. As we pointed out, the convergence result for the threshold-LMS problem is the one that has more interest. For the non-threshold case, the authors of the paper <ref> (Shrivastava & Dasgupta, 1987) </ref> already had established a related convergence result for nonlinear units. They dealt with discrete stochastic approximation rather than the gradient descent differential equation itself, which makes the techniques quite different.
Reference: <author> Sontag, E.D. </author> <title> (1988) Some remarks on the backpropagation algorithm for neural net learn-ing, </title> <type> (Report SYCON-88-02, </type> <institution> Rutgers Center for Systems and Control), New Brunswick: Rutgers University. </institution>
Reference-contexts: " i = 1 for i 33 and " i = 1 otherwise, E (x) = i=1 has a local minimum which is not global, when = tanh. (Note that one must use the examples from (Sontag & Sussmann, 1989) rather than those from (Brady, Raghavan, & Slawny, 1989) or <ref> (Sontag, 1988) </ref>, not just because of the interest in binary examples, but also because in the latter references outputs are not allowed to take limiting values f1; 1g, which will be critical below.) The main result from (Sontag & Sussmann, 1989) is then: The above error function E has at least
Reference: <author> Sontag, E.D. </author> <title> (1990) Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Furthermore, it remains in the compact set fx j V (x) V (x (0))g so it is defined for all t 0. (See for instance <ref> (Sontag, 1990) </ref>, Proposition C.3.9.) The LaSalle Invariance Principle (see for instance (LaSalle, 1976), Theorem 6.4, or the particular case in (Sontag, 1990), Lemma 4.6.6) says that if the trajectory is bounded and V is nonincreasing along this trajectory, then there is some real number such that the solution x (t) converges <p> Furthermore, it remains in the compact set fx j V (x) V (x (0))g so it is defined for all t 0. (See for instance <ref> (Sontag, 1990) </ref>, Proposition C.3.9.) The LaSalle Invariance Principle (see for instance (LaSalle, 1976), Theorem 6.4, or the particular case in (Sontag, 1990), Lemma 4.6.6) says that if the trajectory is bounded and V is nonincreasing along this trajectory, then there is some real number such that the solution x (t) converges to the set where _ V (x) = 0 and V (x) = .
Reference: <author> Sontag, E.D. and H.J. </author> <title> Sussmann (1989) Backpropagation can give rise to spurious local minima even for networks without hidden layers. </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <pages> 91-106. </pages>
Reference-contexts: For instance, in <ref> (Sontag & Sussmann, 1989) </ref>, the following labeled sequence of m = 125 vectors is given: w 1 ; : : : ; w 15 = (1; 1; 1; 1) ; w 31 = (1; 1; 1; 1) ; w 33 = (1; 1; 1; 1) ; w 49 ; : : <p> ; + w 80 = (1; 1; 1; 1) ; + for which, with " i = 1 for i 33 and " i = 1 otherwise, E (x) = i=1 has a local minimum which is not global, when = tanh. (Note that one must use the examples from <ref> (Sontag & Sussmann, 1989) </ref> rather than those from (Brady, Raghavan, & Slawny, 1989) or (Sontag, 1988), not just because of the interest in binary examples, but also because in the latter references outputs are not allowed to take limiting values f1; 1g, which will be critical below.) The main result from <p> & Sussmann, 1989) rather than those from (Brady, Raghavan, & Slawny, 1989) or (Sontag, 1988), not just because of the interest in binary examples, but also because in the latter references outputs are not allowed to take limiting values f1; 1g, which will be critical below.) The main result from <ref> (Sontag & Sussmann, 1989) </ref> is then: The above error function E has at least one local minimum which is not a global minimum.
Reference: <author> Wittner, B.S., and J.S. </author> <title> Denker (1987) Strategies for teaching layered networks classifica-tion tasks. </title> <editor> In Dana Anderson (Ed.), </editor> <booktitle> Proc. Conf. Neural Info. Proc. </booktitle> <institution> Systems New York: American Institute of Physics. </institution> <month> 12 </month>
Reference-contexts: The apparent contradiction with the title of (Brady, Raghavan, & Slawny, 1989) is explained by the fact that this latter reference did not use threshold but rather "exact" LMS. In this paper, we extend the result in <ref> (Wittner & Denker, 1987) </ref> so as to include sigmoidal neurons, which were excluded by their assumptions. <p> The use of E fl was first suggested in <ref> (Wittner & Denker, 1987) </ref>, who also proved a convergence result under somewhat restrictive hypotheses which do not allow for sigmoids. <p> Finally, we compare with the results in <ref> (Wittner & Denker, 1987) </ref>.
References-found: 11

