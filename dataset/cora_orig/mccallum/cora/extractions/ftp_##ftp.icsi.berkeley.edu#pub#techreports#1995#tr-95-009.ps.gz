URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1995/tr-95-009.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1995.html
Root-URL: http://www.icsi.berkeley.edu
Title: Adaptive Parameter Pruning in Neural Networks  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Lutz Prechelt 
Note: prechelt@icsi.berkeley.edu; permanent address: prechelt@ira.uka.de  
Date: March 1995  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-95-009  
Abstract: Neural network pruning methods on the level of individual network parameters (e.g. connection weights) can improve generalization. An open problem in the pruning methods known today (OBD, OBS, autoprune, epsiprune) is the selection of the number of parameters to be removed in each pruning step (pruning strength). This paper presents a pruning method lprune that automatically adapts the pruning strength to the evolution of weights and loss of generalization during training. The method requires no algorithm parameter adjustment by the user. The results of extensive experimentation indicate that lprune is often superior to autoprune (which is superior to OBD) on diagnosis tasks unless severe pruning early in the training process is required. Results of statistical significance tests comparing autoprune to the new method lprune as well as to backpropagation with early stopping are given for 14 different problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> J.A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [2] <author> Yann Le Cun, John S. Denker, and Sara A. Solla. </author> <title> Optimal brain damage. </title> <booktitle> In [13], </booktitle> <pages> pages 598-605, </pages> <year> 1990. </year> <month> 11 </month>
Reference-contexts: Several such methods have been suggested. The simplest one is to assume the importance to be proportional to the magnitude of a weight. More sophisticated approaches are the well-known optimal brain damage (OBD) and optimal brain surgeon (OBS) methods. OBD <ref> [2] </ref> uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS [6] avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly.
Reference: [3] <author> Scott E. Fahlman and Christian Lebiere. </author> <booktitle> The Cascade-Correlation learning architec-ture. In [13], </booktitle> <pages> pages 524-532, </pages> <year> 1990. </year>
Reference-contexts: Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method [4, 8], explicit regularization [4, 9, 14], additive (constructive) learning <ref> [3] </ref>, and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. If applied properly, this approach often reduces overfitting and improves generalization.
Reference: [4] <author> William Finnoff, Ferdinand Hergert, and Hans Georg Zimmermann. </author> <title> Improving model selection by nonconvergent methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783, </pages> <year> 1993. </year>
Reference-contexts: Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method <ref> [4, 8] </ref>, explicit regularization [4, 9, 14], additive (constructive) learning [3], and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. <p> Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method [4, 8], explicit regularization <ref> [4, 9, 14] </ref>, additive (constructive) learning [3], and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. If applied properly, this approach often reduces overfitting and improves generalization. <p> Both methods have the disadvantage of requiring training to the error minimum before pruning may occur. For many problems, this introduces massive overfitting which often cannot be repaired by subsequent pruning. The autoprune method <ref> [4] </ref> avoids this problem. <p> A large value of T indicates high importance of the connection with weight w i . Connections with small T can be pruned. <ref> [4] </ref> has convincingly shown autoprune to be superior to OBD. Given the importance T of each weight at any time during training, two questions remain to be answered: 1. When should we prune? 2. <p> Significantly lower pruning strengths could avoid this, but would exhibit another problem: namely that overfitting cannot be reduced as fast as it builds up. Therefore, 8 pruning with very small pruning strength and static schedule would probably be similar to OBD, which has been shown inferior to autoprune in <ref> [4] </ref>.
Reference: [5] <editor> Stephen J. Hanson, Jack D. Cowan, and C. Lee Giles, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [6] <author> Babak Hassibi and David G. Stork. </author> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <booktitle> In [5], </booktitle> <pages> pages 164-171, </pages> <year> 1993. </year>
Reference-contexts: OBD [2] uses an approximation to the second derivative of the error with respect to each weight to determine the saliency of the removal of that weight. Low saliency means low importance of a weight. OBS <ref> [6] </ref> avoids the drawbacks of the approximation by computing the second derivatives (almost) exactly. However, OBS is computationally very expensive, because for n weights, it requires n 2 computations per pruning step to determine the saliencies.
Reference: [7] <editor> Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors. </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [8] <author> N. Morgan and H. Bourlard. </author> <title> Generalization and parameter estimation in feedforward nets: Some experiments. </title> <booktitle> In [13], </booktitle> <pages> pages 630-637, </pages> <year> 1990. </year>
Reference-contexts: Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method <ref> [4, 8] </ref>, explicit regularization [4, 9, 14], additive (constructive) learning [3], and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones.
Reference: [9] <author> Steven J. Nowlan and Geoffry E. Hinton. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 473-493, </pages> <year> 1992. </year>
Reference-contexts: Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method [4, 8], explicit regularization <ref> [4, 9, 14] </ref>, additive (constructive) learning [3], and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. If applied properly, this approach often reduces overfitting and improves generalization.
Reference: [10] <author> Lutz Prechelt. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Technical Report 21/94, </type> <institution> Universitat Karlsruhe, Germany, </institution> <month> September </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: It then presents and interprets the results of a quantitative comparison of the algorithms. 3.1 Experiment Setup Extensive benchmark comparisons were made between autoprune, lprune, and static backpropagation with early stopping. 14 different problems were used, all from the Proben1 benchmark set <ref> [10] </ref>, a collection of diagnosis problems. Most of these problems stem from the UCI machine learning databases archive. <p> In three different random ways, the examples of each problem were partitioned into training set (50%), validation set (25%), and test set (25% of examples), resulting in 42 datasets (cancer1, cancer2, cancer3, card1, card2, card3 etc.). Each of these datasets was trained with its pivot architecture network topology <ref> [10] </ref>, as well as with the noshortcut pivot architecture network topology, which is derived from the former by excluding all connections (except bias connections) that do not go from one layer to the immediately following layer.
Reference: [11] <author> Lutz Prechelt. </author> <title> A study of experimental evaluations of neural network learning algorithms: Current research practice. </title> <type> Technical Report 19/94, </type> <institution> Universitat Karlsruhe, Ger-many, </institution> <month> August </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-19.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: As the very different results for the various problems and even for the dataset permutations show, benchmarking has to be extensive and careful in order to yield significant and correct results | this is in sharp contrast to the state of the practice <ref> [11] </ref>. Acknowledgements The research presented in this report was performed at the University of Karlsruhe, but the report itself was written at ICSI. Thanks to Walter Tichy and Jerry Feldman for making the visit possible.
Reference: [12] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Neural Networks, </booktitle> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: the problems are classification tasks using 1-of-n output encoding (cancer, card, diabetes, gene, glass, heart, heartc, horse, soybean, and thyroid ), 4 are approximation tasks (building, flare, hearta, and heartac); all problems are real datasets from realistic application domains. 6 All runs were done using the RPROP weight update rule <ref> [12] </ref>, squared error function, and the RPROP parameters + = 1:2, = 0:5, 0 2 [0:05 : : :0:2] randomly per weight, max = 50, min = 0, initial weights from [-0.1: : :0.1] randomly. 1 RPROP is a fast backpropagation variant that is about as fast as quickprop but more
Reference: [13] <editor> David S. Touretzky, editor. </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufman Publishers Inc. </publisher>
Reference: [14] <author> Andreas S. Weigend, David E. Rumelhart, and Bernardo A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <booktitle> In [7], </booktitle> <pages> pages 875-882, </pages> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Antoine de St. Exupery 1 Pruning and Generalization Once an input/output encoding has been chosen, there are several classes of methods for improving the generalization obtained from neural networks. The most important of these are the early stopping method [4, 8], explicit regularization <ref> [4, 9, 14] </ref>, additive (constructive) learning [3], and network pruning. We consider the last technique here. The principal idea of pruning is to reduce the number of free parameters in the network by removing dispensable ones. If applied properly, this approach often reduces overfitting and improves generalization.
References-found: 14

