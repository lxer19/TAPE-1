URL: http://www.ai.univie.ac.at/~juffi/publications/ecai-94.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/publications/publications.html
Root-URL: 
Title: Top-Down Pruning in Relational Learning  
Author: Johannes Furnkranz 
Abstract: Pruning is an effective method for dealing with noise in Machine Learning. Recently pruning algorithms, in particular Reduced Error Pruning, have also attracted interest in the field of Inductive Logic Programming. However, it has been shown that these methods can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. We introduce a new method which searches for good theories in a top-down fashion to get a better starting point for the pruning algorithm. Experiments show that this approach can significantly lower the complexity of the task without losing predictive accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ivan Bratko and Igor Kononenko, </author> <title> `Learning diagnostic rules from incomplete and noisy data', in Interactions in AI and Statistical Methods, </title> <editor> ed., B. </editor> <booktitle> Phelps, </booktitle> <pages> pp. 142-153, </pages> <address> London, </address> <year> (1986). </year>
Reference-contexts: 1 INTRODUCTION Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e.g. [13] or [7]). Two basic approaches can be discerned <ref> [1] </ref>. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [18], mFoil [6], and Fossil [10]). <p> There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly <ref> [16, 1] </ref>. Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree [17, 2].
Reference: [2] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree <ref> [17, 2] </ref>. In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length [18], significance testing [6], or compression [14]. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [3]. <p> thus * speed up the growing phase, because the most expensive theories will not be generated, 6 * speed up the pruning phase, because pruning starts from a simpler theory and thus the number of possible pruning operations is much smaller. 4 This is based on an idea in CART <ref> [2] </ref>, where the most general pruned decision tree within one SE of the best will be selected.
Reference: [3] <author> Clifford A. Brunk and Michael J. Pazzani, </author> <title> `An investigation of noise-tolerant relational concept learning algorithms', </title> <booktitle> in Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pp. 389-393, </pages> <address> Evanston, Illinois, </address> <year> (1991). </year>
Reference-contexts: The most prominent use of this method in ILP is the adaptation <ref> [3] </ref> of Reduced Error Pruning (REP) [17]. However, it has been shown in [4] that REP can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. <p> In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length [18], significance testing [6], or compression [14]. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning <ref> [3] </ref>. First the training set is split into two subsets: a growing set and a pruning set . A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm. <p> Prune the theory obtained in step 4. using Reduced Error Pruning as described in <ref> [3] </ref>. In our experiments we used a slightly modified version of this basic algorithm. Only theories that cover more than 50% of the positive examples in the growing set will be evaluated on the pruning set. <p> Both algorithms split the supplied data sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3). REP generated the most specific theory first (Cutoff = 0), while TDP searched for a suitable starting theory as described in Section 3. Both algorithms used the method described in <ref> [3] </ref> for pruning. All programs were implemented in SICStus PROLOG 2.1 and the run-times were measured on a SUN SPARCstation IPX. 4.1 The KRK domain The experiments in the KRK domain followed the setup described in [10]. Experiments were performed with 10% of the examples having their classification reversed.
Reference: [4] <author> William W. Cohen, </author> <title> `Efficient pruning methods for separate-and-conquer rule learning systems', </title> <booktitle> in Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 988-994, </pages> <address> Chambery, France, </address> <year> (1993). </year>
Reference-contexts: The most prominent use of this method in ILP is the adaptation [3] of Reduced Error Pruning (REP) [17]. However, it has been shown in <ref> [4] </ref> that REP can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. <p> The resulting concept is then generalized by deleting literals and clauses from the theory until all possible deletions would result in a decrease of predictive accuracy, measured on the pruning set. While this method proved to be very effective in avoiding noise-fitting in several domains, <ref> [4] </ref> has shown that REP is a very costly process. Its time complexity on random data is as bad as (n 2 log n) for generating a concept description from n examples and (n 4 log n) for pruning the resulting set of rules. [4] has then suggested a more efficient <p> in avoiding noise-fitting in several domains, <ref> [4] </ref> has shown that REP is a very costly process. Its time complexity on random data is as bad as (n 2 log n) for generating a concept description from n examples and (n 4 log n) for pruning the resulting set of rules. [4] has then suggested a more efficient pruning method, which only has a worst-case time complexity of O (n 2 log n). This algorithm was further improved by adding some pre-pruning methods to speed up the concept generation phase. <p> This result is consistent with the findings of <ref> [4] </ref> (see Section 3.1). TDP on the other hand, even manages to decrease pruning time with growing training set sizes. <p> This method | Top-Down Pruning | results in a significant speed-up compared to Reduced Error Pruning without losing accuracy. Similar results have been obtained in <ref> [4] </ref>, but concentrated mostly on lowering the pruning cost only. In our approach, the entire process of TDP may be significantly faster than REP's growing phase alone. However, TDP has not yet been tested as extensively as the methods proposed in [4]. <p> Similar results have been obtained in <ref> [4] </ref>, but concentrated mostly on lowering the pruning cost only. In our approach, the entire process of TDP may be significantly faster than REP's growing phase alone. However, TDP has not yet been tested as extensively as the methods proposed in [4]. Another open question is whether this top-down approach to pruning could be used for other ILP algorithms as well. <p> Whether this is useful or not remains to be investigated. Of course pruning methods in general are subject to the problem that learning general theories in order to avoid over-fitting the noise might be inappropriate in some domains [19]. In fact it has been observed in <ref> [4] </ref> that pruning sometimes leads to a decrease in predictive accuracy.
Reference: [5] <author> Bojan Dolsak and Stephen Muggleton, </author> <title> `The application of Inductive Logic Programming to finite-element mesh design', in Inductive Logic Programming, </title> <editor> ed., Stephen Muggleton, </editor> <address> 453-472, </address> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> (1992). </year>
Reference-contexts: Machine Learning 455 J. Furnkranz 4 EXPERIMENTS We compared Top-Down Pruning (TDP) to Reduced Error Pruning (REP) in terms of accuracy and run-time on the chess king-rook-king endgame domain [15] that has been extensively used in ILP and on the finite element mesh design problem <ref> [5, 6] </ref>. Both algorithms split the supplied data sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3). REP generated the most specific theory first (Cutoff = 0), while TDP searched for a suitable starting theory as described in Section 3. <p> This supports the two hypotheses stated at the end of the last section. 4.2 The Mesh domain We have also tested our algorithm on the finite element mesh design problem first studied in <ref> [5] </ref> to have an evaluation of the performance of TDP on a real-world learning problem.
Reference: [6] <author> Saso Dzeroski and Ivan Bratko, </author> <title> `Handling noise in Inductive Logic Programming', </title> <booktitle> in Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> (1992). </year>
Reference-contexts: Two basic approaches can be discerned [1]. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [18], mFoil <ref> [6] </ref>, and Fossil [10]). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree [17, 2]. In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length [18], significance testing <ref> [6] </ref>, or compression [14]. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [3]. First the training set is split into two subsets: a growing set and a pruning set . <p> Machine Learning 455 J. Furnkranz 4 EXPERIMENTS We compared Top-Down Pruning (TDP) to Reduced Error Pruning (REP) in terms of accuracy and run-time on the chess king-rook-king endgame domain [15] that has been extensively used in ILP and on the finite element mesh design problem <ref> [5, 6] </ref>. Both algorithms split the supplied data sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3). REP generated the most specific theory first (Cutoff = 0), while TDP searched for a suitable starting theory as described in Section 3. <p> The setup of the experiments was the same as described in <ref> [6] </ref>, i.e. we learned rules from four of the five objects in the data set and tested the learned concept on the fifth data set. The five results from testing on each of the five objects (and learning on the other four) were averaged. <p> The five results from testing on each of the five objects (and learning on the other four) were averaged. The results for runtime, accuracy on positive examples (for a comparison with the results given in <ref> [6] </ref>) and total accuracy are given in Table 3. In general ILP algorithms do not perform well on this task (default accuracy ca. 90%). For a discussion of some of the problems see [6]. However, all of the pruning algorithms seem to do better than the results given there. <p> The results for runtime, accuracy on positive examples (for a comparison with the results given in <ref> [6] </ref>) and total accuracy are given in Table 3. In general ILP algorithms do not perform well on this task (default accuracy ca. 90%). For a discussion of some of the problems see [6]. However, all of the pruning algorithms seem to do better than the results given there. TDP again is sig nificantly faster than REP without losing accuracy. Machine Learning 456 J. Furnkranz Table 3.
Reference: [7] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro, </author> <title> `Decision tree pruning as a search in the state space', </title> <booktitle> in Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 165-184, </pages> <address> Vienna, Austria, (1993). </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 INTRODUCTION Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e.g. [13] or <ref> [7] </ref>). Two basic approaches can be discerned [1]. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [18], mFoil [6], and Fossil [10]). <p> Furnkranz ECAI 94. 11th European Conference on Artificial Intelligence Edited by A. Cohn Published in 1994 by John Wiley & Sons, Ltd. 3 TOP-DOWN PRUNING 3.1 Pruning in Inductive Logic Programming Pruning is a standard way of dealing with noise in Decision Tree learning (see e.g. [13] or <ref> [7] </ref>). There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly [16, 1].
Reference: [8] <author> Peter A. Flach, </author> <title> `Generality revisited', in Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> (1992). </year>
Reference-contexts: We consider the empty theory to be most general, because "Everything is false." is a very general statement. However, our "most specific" theory will cover more ground instances than the empty theory, and thus may be considered (extensionally) more general. See <ref> [8] </ref> for a discussion of related matters. Machine Learning 454 J. Furnkranz successively generalize it (bottom-up).
Reference: [9] <author> Johannes Furnkranz, `Fossil: </author> <title> A robust relational learner', </title> <type> Technical Report OEFAI-TR-93-28, </type> <institution> Austrian Research Institute for Artificial Intelligence, </institution> <year> (1993). </year> <note> Extended version. </note>
Reference-contexts: is that we get a series of different concept descriptions in a | roughly | general to specific order (top-down) 3 as opposed to pruning methods that generate a most specific theory first and then 2 A detailed description of the domain can be found in the Ap pendix of <ref> [9] </ref>. 3 Note that we use the terms "general" and "specific" in an intuitive way. We consider the empty theory to be most general, because "Everything is false." is a very general statement.
Reference: [10] <author> Johannes Furnkranz, `Fossil: </author> <title> A robust relational learner', </title> <booktitle> in Proceedings of the European Conference on Machine Learning, Catania, </booktitle> <address> Italy, (1994). </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Two basic approaches can be discerned [1]. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [18], mFoil [6], and Fossil <ref> [10] </ref>). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> Some experiments show the applicability of this algorithm (Section 4) and from their results we draw some conclusions (Section 5). 1 Austrian Research Institute for Artificial Intelligence, Schotten gasse 3, A-1010 Vienna, Austria. E-mail: juffi@ai.univie.ac.at 2 Fossil AND THE CUTOFF STOPPING CRITERION Fossil <ref> [10] </ref> is a Foil-like ILP system that uses a search heuristic based on statistical correlation. <p> Experiments have shown that this method is very noise-tolerant and that a good value for the Cutoff seems to be independent of the noise level as well as independent of the size of the training set. A more detailed description of the algorithm can be found in <ref> [10] </ref>. <p> Both algorithms used the method described in [3] for pruning. All programs were implemented in SICStus PROLOG 2.1 and the run-times were measured on a SUN SPARCstation IPX. 4.1 The KRK domain The experiments in the KRK domain followed the setup described in <ref> [10] </ref>. Experiments were performed with 10% of the examples having their classification reversed. Testing was done on sets of 5000 noise-free examples. REP and TDP were both given the same 10 training sets for each of the 4 different training set sizes. <p> The top-down search for a good theory (without pruning) could even yield better results if we did not use the most specific theory within one standard error of the best theory, but the best itself (see <ref> [10] </ref> for some preliminary experiments along these lines). However, this could lead to over-generalization. Starting with a more specific theory is better for the subsequent pruning process, which still is quite inexpensive, as can be seen from Table 2. Table 2. Run-time in the KRK domain with 10% noise.
Reference: [11] <author> Robert C. Holte, </author> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91, </pages> <year> (1993). </year>
Reference-contexts: However, there is some evidence that simple rules perform well in many real world domains (or at least in those domains that are commonly used as a test bed for machine learning algorithms) <ref> [11] </ref>, and TDP's general-to-specific search might be a good method in those domains. ACKNOWLEDGEMENTS This research is sponsored by the Austrian Fonds zur Forderung der Wissenschaftlichen Forschung (FWF) under grant P8756-TEC.
Reference: [12] <author> Igor Kononenko and Ivan Bratko, </author> <title> `Information-based evaluation criterion for classifier's performance', </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80, </pages> <year> (1991). </year>
Reference-contexts: This may cause the percentage of correctly classified positive examples to go down, while the overall accuracy increases. This might be undesirable in various domains. <ref> [12] </ref> discusses some alternatives to an accuracy-based evaluation of classifiers that could easily be adapted for pruning algorithms. 5 CONCLUSION We have shown that the Inductive Logic Programming algorithm Fossil allows a top-down generation of a series of theories that can be used to find a good starting point for a
Reference: [13] <author> John Mingers, </author> <title> `An empirical comparison of pruning methods for decision tree induction', </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243, </pages> <year> (1989). </year>
Reference-contexts: 1 INTRODUCTION Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e.g. <ref> [13] </ref> or [7]). Two basic approaches can be discerned [1]. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [18], mFoil [6], and Fossil [10]). <p> Furnkranz ECAI 94. 11th European Conference on Artificial Intelligence Edited by A. Cohn Published in 1994 by John Wiley & Sons, Ltd. 3 TOP-DOWN PRUNING 3.1 Pruning in Inductive Logic Programming Pruning is a standard way of dealing with noise in Decision Tree learning (see e.g. <ref> [13] </ref> or [7]). There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly [16, 1].
Reference: [14] <author> Stephen Muggleton, Ashwin Srinivasan, and Michael Bain, </author> <title> `Compression, significance and accuracy', </title> <booktitle> in Proceedings of the 9th International Workshop on Machine Learning, </booktitle> <pages> pp. 338-347, </pages> <year> (1992). </year>
Reference-contexts: This will be subsequently generalized by cutting off branches of the decision tree [17, 2]. In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length [18], significance testing [6], or compression <ref> [14] </ref>. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [3]. First the training set is split into two subsets: a growing set and a pruning set .
Reference: [15] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie, </author> <title> `An experimental comparison of human and machine learning formalisms', </title> <booktitle> in Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pp. 113-118, </pages> <year> (1989). </year>
Reference-contexts: Machine Learning 455 J. Furnkranz 4 EXPERIMENTS We compared Top-Down Pruning (TDP) to Reduced Error Pruning (REP) in terms of accuracy and run-time on the chess king-rook-king endgame domain <ref> [15] </ref> that has been extensively used in ILP and on the finite element mesh design problem [5, 6]. Both algorithms split the supplied data sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3).
Reference: [16] <author> John Ross Quinlan, </author> <title> `The effect of noise on concept learning', in Machine Learning: An Artificial Intelligence Approach, </title> <editor> eds., Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, </editor> <volume> volume II, </volume> <pages> 149-166, </pages> <publisher> Morgan Kaufmann, </publisher> <year> (1986). </year>
Reference-contexts: There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly <ref> [16, 1] </ref>. Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree [17, 2].
Reference: [17] <author> John Ross Quinlan, </author> <title> `Simplifying decision trees', </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234, </pages> <year> (1987). </year>
Reference-contexts: The most prominent use of this method in ILP is the adaptation [3] of Reduced Error Pruning (REP) <ref> [17] </ref>. However, it has been shown in [4] that REP can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. <p> Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree <ref> [17, 2] </ref>. In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length [18], significance testing [6], or compression [14]. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [3].
Reference: [18] <author> John Ross Quinlan, </author> <title> `Learning logical definitions from relations', </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266, </pages> <year> (1990). </year>
Reference-contexts: Two basic approaches can be discerned [1]. Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil <ref> [18] </ref>, mFoil [6], and Fossil [10]). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently generalized by cutting off branches of the decision tree [17, 2]. In ILP, Pre-Pruning has been common in the form of stopping criteria based on encoding length <ref> [18] </ref>, significance testing [6], or compression [14]. Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [3]. First the training set is split into two subsets: a growing set and a pruning set .
Reference: [19] <author> Cullen Schaffer, </author> <title> `Overfitting avoidance as bias', </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178, </pages> <year> (1993). </year> <note> Machine Learning 457 J. Furnkranz </note>
Reference-contexts: If no new clause can be started with a literal above the threshold, learning stops. In general it holds that higher values of the Cutoff produce simpler theories. Thus the Cutoff may be viewed as a simple means of controlling the Overfit-ting Avoidance Bias <ref> [19] </ref>. Experiments have shown that this method is very noise-tolerant and that a good value for the Cutoff seems to be independent of the noise level as well as independent of the size of the training set. A more detailed description of the algorithm can be found in [10]. <p> These systematic changes to the values of the Cutoff parameter may be viewed as a means of systematically varying the Overfitting Avoidance Bias <ref> [19] </ref>. <p> Whether this is useful or not remains to be investigated. Of course pruning methods in general are subject to the problem that learning general theories in order to avoid over-fitting the noise might be inappropriate in some domains <ref> [19] </ref>. In fact it has been observed in [4] that pruning sometimes leads to a decrease in predictive accuracy.
References-found: 19

