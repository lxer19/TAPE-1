URL: http://siesta.cs.wustl.edu:80/~sg/general-geometric.ps.Z
Refering-URL: http://siesta.cs.wustl.edu:80/~sg/
Root-URL: 
Email: bshouty@cpsc.ucalgary.ca  sg@cs.wustl.edu  dmath@cs.wustl.edu  suri@cs.wustl.edu  htamaki@trl.ibm.co.jp  
Title: Noise-Tolerant Distribution-Free Learning of General Geometric Concepts  
Author: Nader H. Bshouty Sally A. Goldman H. David Mathias Subhash Suri Hisao Tamaki 
Address: Calgary, Alberta, Canada T2N 1N4  St. Louis, MO 63130  St. Louis, MO 63130  St. Louis, MO 63130  Yamato 242, Japan  
Affiliation: The University of Calgary  Washington University  Washington University  Washington University  IBM Tokyo Research Laboratory  
Note: |in: Proceedings of the 28th Annual ACM Symposium on Theory of Computing, to appear. c fl1996 ACM.  
Abstract: We present an efficient algorithm for PAC-learning a very general class of geometric concepts over &lt; d for fixed d. More specifically, let T be any set of s halfspaces. Let x = (x 1 ; : : : ; x d ) be an arbitrary point in &lt; d . With each t 2 T we associate a boolean indicator function I t (x) which is 1 if and only if x is in the halfspace t. The concept class, C d s , that we study consists of all concepts formed by any boolean function over I t 1 ; : : : ; I t s for t i 2 T . This concept class is much more general than any geometric concept class known to be PAC-learnable. Our results can be easily extended to efficiently learn any boolean combination of a polynomial number of concepts selected from any concept class C over &lt; d given that the VC-dimension of C has dependence only on d (and is thus constant for any constant d), and there is a polynomial time algorithm to determine if there is a concept from C consistent with a given set of labeled examples. We also present a statistical query version of our algorithm that can tolerate random classification noise for any noise rate strictly less than 1/2. Finally we present a generalization of the standard *-net result of Haussler and Welzl [25] and apply it to give an alternative noise-tolerant algorithm for d = 2 based on geometric subdivisions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Noga Alon, Joel H. Spencer, and Paul Erd-os, </author> <title> editors. The Probabilistic Method. </title> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: Let ~y = (y 1 ; : : : ; y d ) denote the d dimension variables and ~x = (x 1 ; : : : ; x d ) denote an element of <ref> [0; 1] </ref> d , the in stance space. Let ~a = (a 1 ; : : : ; a d ) be a vector where each a i is a constant. A d-dimensional hyperplane is f~y j ~a ~y = bg for b a constant. <p> A d-dimensional hyperplane is f~y j ~a ~y = bg for b a constant. A d-dimensional halfspace is f~y j ~a ~y bg where 2 f&gt;; ; &lt;; g and b is a constant. Let H be a finite set of hyperplanes in <ref> [0; 1] </ref> d . The arrangement A (H) is the decomposition of [0; 1] d into features of dimension k, 0 k d. We define a region as a d-dimensional feature in A (H). <p> A d-dimensional halfspace is f~y j ~a ~y bg where 2 f&gt;; ; &lt;; g and b is a constant. Let H be a finite set of hyperplanes in <ref> [0; 1] </ref> d . The arrangement A (H) is the decomposition of [0; 1] d into features of dimension k, 0 k d. We define a region as a d-dimensional feature in A (H).
Reference: [2] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: One significant open problem is whether or not an algorithm for C d s exists in the query learning model of An-gluin <ref> [2] </ref>. Since in this model the learner is required to achieve exact identification of the target concept (as opposed to the approximation achieved in the PAC model), the domain must be discretized. We currently know of no efficient query algorithms for this class.
Reference: [3] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: In this work we also consider a variant of the PAC model in which the labeled examples that the learner receives are corrupted by random classification noise <ref> [3] </ref>. In this noise model, the each example is still drawn at random from D. However, with probability (where 0 &lt; 1=2 is called the noise rate), the learner receives the incorrect label. And with probability 1 , the learner receives the correct label.
Reference: [4] <author> Javed A. Aslam and Scott E. Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus the example drawn is labeled incorrectly, at random, with probability . In the malicious noise model [32], with probability the adversary can provide an example and label of its choice. To obtain a noise-tolerant version of our algorithm we use the statistical query model <ref> [28, 20, 4, 5] </ref>. In this model, rather than sampling labeled examples, the learner requests the value of various statistics on the distribution from an oracle. A statistical query oracle returns the probability, within some additive constant, that some predicate is true relative to the distribution.
Reference: [5] <author> Javed A. Aslam and Scott E. Decatur. </author> <title> Specification and simulation of statistical query algorithms for efficiency and noise tolerance. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 437-446, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus the example drawn is labeled incorrectly, at random, with probability . In the malicious noise model [32], with probability the adversary can provide an example and label of its choice. To obtain a noise-tolerant version of our algorithm we use the statistical query model <ref> [28, 20, 4, 5] </ref>. In this model, rather than sampling labeled examples, the learner requests the value of various statistics on the distribution from an oracle. A statistical query oracle returns the probability, within some additive constant, that some predicate is true relative to the distribution. <p> A statistical query oracle returns the probability, within some additive constant, that some predicate is true relative to the distribution. The particular queries we use are known as relative statistical queries <ref> [5] </ref>. These take the form rel-stat D (; ; ) where is a pred icate over labeled examples drawn from D, is the relative error bound, and is the threshold. For target function f , let P = Pr D [(x; f (x)) = 1]. <p> If ? is not returned, then rel-stat D (; ; ) must return an estimate ^ P such that P (1 ) ^ P P (1 + ). The learner may also request unlabeled examples. Aslam and Decatur <ref> [5] </ref> have shown that all relative error SQ algorithms are robust against random classification noise for any noise rate &lt; 1=2. They have also shown that relative error SQ algorithms are robust against small amounts of malicious errors.
Reference: [6] <author> Peter Auer, Stephen Kwek, Wolfgang Maass, and Manfred Warmuth. </author> <title> On-line prediction of depth two linear threshold circuits. </title> <type> Unpublished manuscript, </type> <year> 1995. </year>
Reference-contexts: While most such work assumes that there are a constant number of dimensions, recently Auer, Kwek, Maass and Warmuth <ref> [6] </ref> give a polynomial time algorithm that is robust against noise to learn the class of depth two linear threshold circuits with a polynomial number of variables given that the input gates have constant fan-in. 5 PAC-Learning C d s In this section we present an algorithm for learning C d
Reference: [7] <author> Eric B. Baum. </author> <title> On learning a union of half spaces. </title> <journal> Journal of Complexity, </journal> <volume> 6(1) </volume> <pages> 67-101, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: To illustrate the key technique used in most of this work consider the problem of learning unions of s halfspaces in d-dimensional space for any constant d <ref> [13, 7, 14] </ref> 1 . The standard Occam algorithm draws a sufficiently large sample S of m points (where m is chosen to satisfy the bound of Blumer et al. [13]) and then finds a hypothesis consistent with the sample by formulating a set covering problem. <p> They also give an algorithm to PAC-learn the xor of two halfspaces by transforming the examples so that positive and negative points are linearly separable. Baum <ref> [7] </ref> gives an algorithm that efficiently learns a union of s halfspaces in constant dimensional space. Blumer et al. [13] give a similar result. Both algorithms return hypotheses containing O (s lg m) halfspaces where m is the size of the sample.
Reference: [8] <author> Eric B. Baum. </author> <title> The perceptron algorithm is fast for non-malicious distributions. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 248-260, </pages> <year> 1990. </year>
Reference-contexts: Bronnimann and Goodrich [14] present a set covering algorithm that allows them to return a hypothesis containing O (ds lg (ds)) halfspaces. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions <ref> [8] </ref>. Haus-sler [26] also gives distribution specific algorithms for several classes of functions. Bshouty, Goldman and Math-ias [17] have given noise-tolerant algorithms for several ge ometric classes.
Reference: [9] <author> Eric B. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 5-19, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Finally, under a variation of the PAC model in which membership queries can be made, Baum <ref> [9] </ref> gives an algorithm that PAC-learns the union of s halfspaces in &lt; n in time polynomial in s, n, and a parameter that characterizes the number of bits of accuracy with which the target hyperplanes are specified.
Reference: [10] <author> Bonnie Berger, John Rompel, and Peter W. Shor. </author> <title> Efficient NC algorithms for set cover with applications to learning and geometry. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49(2) </volume> <pages> 454-477, </pages> <year> 1994. </year>
Reference-contexts: Thus, with the exception of some subclasses of C d s with polynomially sized concepts, our algorithm runs in time polynomial in the size of the target. Note, we can use parallel set covering techniques <ref> [10] </ref> to get our algorithm to run efficiently in parallel. A second contribution of our work is the conversion of our basic algorithm to a statistical query algorithm giving noise tolerance.
Reference: [11] <author> Avrim L. Blum and Ronald L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(1) </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: There have been many results for the classes of unions and intersections of half-spaces. Blum and Rivest <ref> [11] </ref> show that there does not exist an efficient proper 4 learning algorithm for unions of s halfspaces, unless RP = N P . They also give an algorithm to PAC-learn the xor of two halfspaces by transforming the examples so that positive and negative points are linearly separable.
Reference: [12] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In particular, the complexity of the hypothesis class cannot depend on the size of the sample. However, when using a set covering approach the size of the hypothesis often depends on the size of the sample. Blumer et al. <ref> [12, 13] </ref> show that finding a hypothesis whose size is sublinear in the sample size is sufficient to guarantee polynomial PAC learnability. Let H A s;m be the hypothesis space used by algorithm A for a target of complexity s and sample size m.
Reference: [13] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: To illustrate the key technique used in most of this work consider the problem of learning unions of s halfspaces in d-dimensional space for any constant d <ref> [13, 7, 14] </ref> 1 . The standard Occam algorithm draws a sufficiently large sample S of m points (where m is chosen to satisfy the bound of Blumer et al. [13]) and then finds a hypothesis consistent with the sample by formulating a set covering problem. <p> The standard Occam algorithm draws a sufficiently large sample S of m points (where m is chosen to satisfy the bound of Blumer et al. <ref> [13] </ref>) and then finds a hypothesis consistent with the sample by formulating a set covering problem. For the learning problem, one first generates a set F of halfspaces such that each possible partition of the points in S into two sets by a halfspace is realized. <p> However, for a concept class C, jF j has an exponential dependence on the VC-dimension of C. Thus, this approach yields a polynomial time algorithm only when the VC-dimension of C is constant. Blumer et al. <ref> [13] </ref> prove that this set covering approach can be used to efficiently PAC-learn the union (or intersection) of concepts from a 1 By a dual argument everything discussed applies to intersections of halfspaces. base class C given that the VC-dimension of C is constant and there is a polynomial time algorithm <p> d given that the VC-dimension of C depends only on d (and thus is constant for d constant), and there is a polynomial time algorithm to determine if there is a concept from C consistent with a given set of labeled examples. 3 Applying a result from Blumer et al. <ref> [13] </ref> gives that for classes C with finite VC-dimension and a polynomial time consistency algorithm, the elements of F can be listed in polynomial time. <p> That is, with high probability, the hypothesis must correctly classify most of the instances (by weight under distribution D). We now describe several relevant results relating learnability and the Vapnik-Chervonenkis (VC) dimension, an important complexity measure. The paper of Blumer et al. <ref> [13] </ref> identifies a combinatorial parameter of a class of hypotheses called the Vapnik-Chervonenkis (VC) dimension, which originated in the paper of Vapnik and Chervonenkis [33], that bounds how large a sample size is required in order to have enough information for accurate generalization. <p> In particular, the complexity of the hypothesis class cannot depend on the size of the sample. However, when using a set covering approach the size of the hypothesis often depends on the size of the sample. Blumer et al. <ref> [12, 13] </ref> show that finding a hypothesis whose size is sublinear in the sample size is sufficient to guarantee polynomial PAC learnability. Let H A s;m be the hypothesis space used by algorithm A for a target of complexity s and sample size m. <p> They also give an algorithm to PAC-learn the xor of two halfspaces by transforming the examples so that positive and negative points are linearly separable. Baum [7] gives an algorithm that efficiently learns a union of s halfspaces in constant dimensional space. Blumer et al. <ref> [13] </ref> give a similar result. Both algorithms return hypotheses containing O (s lg m) halfspaces where m is the size of the sample. Bronnimann and Goodrich [14] present a set covering algorithm that allows them to return a hypothesis containing O (ds lg (ds)) halfspaces. <p> We now argue that we can construct an appropriate set of candidate hyperplanes. (Recall that the results of Blumer et al. <ref> [13] </ref> allow us to build F in the more general setting where the VC-dimension of the class C from which we combine concepts is constant and there is a polynomial time algorithm to determine if there is a concept from C consistent with a given set of points.) A hyperplane partitions <p> Thus, by the results of Blumer et al. <ref> [13] </ref>, the VC dimension of the hypothesis class is at most O (ds lg m lg (s lg m)) = O ds (lg m) 2 . Thus by applying Theorem 2 [Blumer et al.] we get that the stated sample size suffices. <p> Then E += (h) *(1 *) *. Let e i be the value E += (h) after the ith hyperplane has been added to the hypothesis. By the construction of F and the fact that m u was chosen (based on the bounds of <ref> [13] </ref>) so that, with probability at least 1 ffi=3, any hypothesis consistent with m u (if we knew the correct la bels) would have error at most 3* 4 (t d +1) , we know that with probability at least 1 ffi=3, there exist s hyperplanes in F , that when <p> Since h can be viewed as a boolean combination of at most t = max (3sd) ; 3s ln * halfspaces (obtained by arbitrarily orienting each of the hy-perplanes), the vcd (H) is at most 2 (d+1)t lg (3t) (by <ref> [13] </ref>). After substituting the above values for * and ffi, m u = 16 (t d +1) 48 (t d +1) ffi is sufficient to ensure a hypothesis consistent with the sample has error at most 3* 4 (t d +1) with probability at most ffi=3.
Reference: [14] <author> Herve Bronnimann and Michael T. Goodrich. </author> <title> Almost optimal set covers in finite VC-dimenison. </title> <booktitle> In 10th Annual Symposium on Computational Geometry, </booktitle> <pages> pages 293-302, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: To illustrate the key technique used in most of this work consider the problem of learning unions of s halfspaces in d-dimensional space for any constant d <ref> [13, 7, 14] </ref> 1 . The standard Occam algorithm draws a sufficiently large sample S of m points (where m is chosen to satisfy the bound of Blumer et al. [13]) and then finds a hypothesis consistent with the sample by formulating a set covering problem. <p> By applying a greedy covering technique, we obtain a set of hyperplanes that separates every pair hx + ; x i. By using the set covering algorithm of Bronnimann and Goodrich <ref> [14] </ref> (versus the standard greedy covering) we obtain a cover of size O (sd lg (sd)). Our approach does not have the limitations discussed above, yet maintains the desirable feature that the sample complexity is polynomial in s and d. <p> Baum [7] gives an algorithm that efficiently learns a union of s halfspaces in constant dimensional space. Blumer et al. [13] give a similar result. Both algorithms return hypotheses containing O (s lg m) halfspaces where m is the size of the sample. Bronnimann and Goodrich <ref> [14] </ref> present a set covering algorithm that allows them to return a hypothesis containing O (ds lg (ds)) halfspaces. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions [8]. <p> The set covering is the dominant step in our learning algorithm, and since m is polynomial in s, d, 1=* and 1=ffi, the theorem has been established. 2 Replacing the standard greedy set covering algorithm with one due to Bronnimann and Goodrich <ref> [14] </ref> we obtain the following result. (Note that here the complexity of the hypothesis class does not depend on the sample size m. Thus instead of using an Occam algorithm as above, we can apply Theorem 1 [Blumer et al.]).
Reference: [15] <author> Nader H. Bshouty, Zhixiang Chen, and Steve Homer. </author> <title> On learning discretized geometric concepts. </title> <booktitle> In 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 54-63, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: There has been substantial work on exactly learning using equivalence queries (and in some cases also membership queries) unions of boxes [18, 24] and other more complex discretized geometric concepts <ref> [15, 16] </ref>.
Reference: [16] <author> Nader H. Bshouty, Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Exact learning of discretized concepts. </title> <type> Technical Report WUCS-94-19, </type> <institution> Washington University, </institution> <year> 1994. </year> <note> An abbreviated version appeared in COLT 94. </note>
Reference-contexts: There has been substantial work on exactly learning using equivalence queries (and in some cases also membership queries) unions of boxes [18, 24] and other more complex discretized geometric concepts <ref> [15, 16] </ref>.
Reference: [17] <author> Nader H. Bshouty, Sally A. Goldman, and H. David Math-ias. </author> <title> Noise-tolerant parallel learning of geometric concepts. </title> <booktitle> In Proceedings of the Eighth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 345-352, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions [8]. Haus-sler [26] also gives distribution specific algorithms for several classes of functions. Bshouty, Goldman and Math-ias <ref> [17] </ref> have given noise-tolerant algorithms for several ge ometric classes. In particular, they studied C d s against the product distribution and a restricted version of this class, in which the hyperplanes have slopes from a set of r known slopes, against arbitrary distributions.
Reference: [18] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learnability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: However, we require that d be constant. 4 A learning algorithm is proper if all hypotheses come from the concept class. There has been substantial work on exactly learning using equivalence queries (and in some cases also membership queries) unions of boxes <ref> [18, 24] </ref> and other more complex discretized geometric concepts [15, 16].
Reference: [19] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Replace h by h [f and remove from P the points correctly classi fied by f . Since the target concept gives a covering of size s, it follows that the greedy set covering algorithm <ref> [19] </ref> produces a cover of size O (s ln jP j) = O (s lg m). To see a fundamental limitation of the standard technique, consider the geometric concept shown in Figure 1. <p> An instance of the set covering problem is a ground set U and a family F of subsets of U such that S A solution is a smallest cardinality subset G of F such that S g2G g = U . The greedy approximation algorithm <ref> [19] </ref> for this problem has a ratio bound of ln jU j + 1 on the size of the approximation. 4 Previous Work In this section we highlight some of the relevant learning results for geometric concepts. <p> We will describe shortly our choice of the set F of covering hyperplanes, which ensures that F contains a subset of s hyperplanes that separate every += pair in the sample. The set covering problem, however, is NP-complete, and so we use a greedy approximation algorithm <ref> [27, 30, 19] </ref>, which gives an approximation ratio bound of O (lg m) for our application. The set of hyperplanes returned by the set covering algorithm partitions the space into O ((s lg m) d ) regions.
Reference: [20] <author> Scott E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Thus the example drawn is labeled incorrectly, at random, with probability . In the malicious noise model [32], with probability the adversary can provide an example and label of its choice. To obtain a noise-tolerant version of our algorithm we use the statistical query model <ref> [28, 20, 4, 5] </ref>. In this model, rather than sampling labeled examples, the learner requests the value of various statistics on the distribution from an oracle. A statistical query oracle returns the probability, within some additive constant, that some predicate is true relative to the distribution.
Reference: [21] <author> Herbert Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: For this purpose, we pick one representative hyperplane from each equivalence class. To achieve this goal we use a geometric duality argument (see, for example, Edelsbrunner <ref> [21] </ref>). Each point p in our original or primal space is mapped into a hyperplane g (p) in the dual space, and each hyperplane q in the primal space is mapped to a point g 0 (q) in the dual space.
Reference: [22] <author> Andrzej Ehrenfeucht and David Haussler. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: Let d be the VC-dimension of H. Any concept f 2 H consistent with a sample of size max 4 ffi ; 8d * will have error at most * with probability at least 1 ffi. Furthermore, Ehrenfeucht et al. <ref> [22] </ref> prove that any con cept class C of VC dimension d must use 1 ffi + d examples in the worst case. One drawback with the above approach is that the hypothesis must be drawn from a fixed hypothesis class H.
Reference: [23] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Also, Frazier et al. <ref> [23] </ref> have given an algorithm to PAC-learn the s-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. Their algorithm learns this subclass of general unions of boxes in time polynomial in both s and d.
Reference: [24] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Math-ias. </author> <title> Learning unions of rectangles with membership and equivalence queries. </title> <type> Technical Report WUCS-93-46, </type> <institution> Wash-ington University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: However, we require that d be constant. 4 A learning algorithm is proper if all hypotheses come from the concept class. There has been substantial work on exactly learning using equivalence queries (and in some cases also membership queries) unions of boxes <ref> [18, 24] </ref> and other more complex discretized geometric concepts [15, 16].
Reference: [25] <author> David Haussler and Emo Welzl. </author> <title> Epsilon-nets and simplex range queries. </title> <journal> Discrete Computational Geometry, </journal> <volume> 2 </volume> <pages> 127-151, </pages> <year> 1987. </year>
Reference-contexts: This variation takes advantage of the noise tolerance inherent in the statistical (SQ) model [28]. Finally we present a generalization of the standard *-net result of Haussler and Welzl <ref> [25] </ref> and apply it to give an alternative noise-tolerant algorithm for d = 2 based on geometric subdivisions. 3 formed from the three halfspaces a, b, and c. <p> Thus the conversion to an statistical query algorithm is more complex than that needed with the standard covering approach. Finally we present a generalization of the standard *-net result of Haussler and Welzl <ref> [25] </ref> and apply it to give an alternative noise-tolerant algorithm for d = 2 based on geometric subdivisions. 3 Preliminaries The learning model we use in this work is the probably approximately correct (PAC) model of Valiant [31].
Reference: [26] <author> David Haussler. </author> <title> Generalizing the PAC model: sample size bounds from metric dimension-based uniform convergence results. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 40-45, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Bronnimann and Goodrich [14] present a set covering algorithm that allows them to return a hypothesis containing O (ds lg (ds)) halfspaces. Baum gives efficient algorithms for learning several classes with infinite VC-dimension (such as convex polyhedral sets) under uniform distributions [8]. Haus-sler <ref> [26] </ref> also gives distribution specific algorithms for several classes of functions. Bshouty, Goldman and Math-ias [17] have given noise-tolerant algorithms for several ge ometric classes.
Reference: [27] <author> David S. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer and System Sciences, </journal> <pages> pages 256-278, </pages> <year> 1974. </year>
Reference-contexts: We will describe shortly our choice of the set F of covering hyperplanes, which ensures that F contains a subset of s hyperplanes that separate every += pair in the sample. The set covering problem, however, is NP-complete, and so we use a greedy approximation algorithm <ref> [27, 30, 19] </ref>, which gives an approximation ratio bound of O (lg m) for our application. The set of hyperplanes returned by the set covering algorithm partitions the space into O ((s lg m) d ) regions.
Reference: [28] <author> Micahel Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: This variation takes advantage of the noise tolerance inherent in the statistical (SQ) model <ref> [28] </ref>. Finally we present a generalization of the standard *-net result of Haussler and Welzl [25] and apply it to give an alternative noise-tolerant algorithm for d = 2 based on geometric subdivisions. 3 formed from the three halfspaces a, b, and c. <p> Thus the example drawn is labeled incorrectly, at random, with probability . In the malicious noise model [32], with probability the adversary can provide an example and label of its choice. To obtain a noise-tolerant version of our algorithm we use the statistical query model <ref> [28, 20, 4, 5] </ref>. In this model, rather than sampling labeled examples, the learner requests the value of various statistics on the distribution from an oracle. A statistical query oracle returns the probability, within some additive constant, that some predicate is true relative to the distribution.
Reference: [29] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and polynomial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Long and War-muth <ref> [29] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most s (2d) s boxes consistent with the sample.
Reference: [30] <author> Laszlo Lovasz. </author> <title> On the ratio of optimal integral and fractional covers. </title> <booktitle> Discrete Mathematics, </booktitle> <pages> pages 383-390, </pages> <year> 1975. </year>
Reference-contexts: We will describe shortly our choice of the set F of covering hyperplanes, which ensures that F contains a subset of s hyperplanes that separate every += pair in the sample. The set covering problem, however, is NP-complete, and so we use a greedy approximation algorithm <ref> [27, 30, 19] </ref>, which gives an approximation ratio bound of O (lg m) for our application. The set of hyperplanes returned by the set covering algorithm partitions the space into O ((s lg m) d ) regions.
Reference: [31] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: we present a generalization of the standard *-net result of Haussler and Welzl [25] and apply it to give an alternative noise-tolerant algorithm for d = 2 based on geometric subdivisions. 3 Preliminaries The learning model we use in this work is the probably approximately correct (PAC) model of Valiant <ref> [31] </ref>. In this model, the learner is presented with examples, chosen randomly from instance space X according to unknown probability distribution D. Let f be an unknown target function from known concept class C.
Reference: [32] <author> Leslie Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelleigence, </booktitle> <volume> vol. 1, </volume> <pages> pages 560-566, </pages> <year> 1985. </year>
Reference-contexts: However, with probability (where 0 &lt; 1=2 is called the noise rate), the learner receives the incorrect label. And with probability 1 , the learner receives the correct label. Thus the example drawn is labeled incorrectly, at random, with probability . In the malicious noise model <ref> [32] </ref>, with probability the adversary can provide an example and label of its choice. To obtain a noise-tolerant version of our algorithm we use the statistical query model [28, 20, 4, 5].
Reference: [33] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: We now describe several relevant results relating learnability and the Vapnik-Chervonenkis (VC) dimension, an important complexity measure. The paper of Blumer et al. [13] identifies a combinatorial parameter of a class of hypotheses called the Vapnik-Chervonenkis (VC) dimension, which originated in the paper of Vapnik and Chervonenkis <ref> [33] </ref>, that bounds how large a sample size is required in order to have enough information for accurate generalization.
Reference: [34] <author> Dan E. Willard. </author> <title> Polygon retrieval. </title> <journal> SIAM Journal on Computing, </journal> <volume> 11(1) </volume> <pages> 149-165, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: The learner subdivides the space in the following manner. Given that the points are in general position (namely, no three points are collinear), there exist two lines that divides the points into four groups of equal size <ref> [34] </ref>. We build the subdivision by finding such a pair of lines, and then we recursively apply the same technique (independently) to each of the four regions obtained until there are only q points in each region.
References-found: 34

