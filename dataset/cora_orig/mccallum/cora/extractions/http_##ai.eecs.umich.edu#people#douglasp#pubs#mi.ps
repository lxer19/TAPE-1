URL: http://ai.eecs.umich.edu/people/douglasp/pubs/mi.ps
Refering-URL: http://ai.eecs.umich.edu/people/douglasp/pubs/machine-int.html
Root-URL: 
Email: E-mail: dpearson@umich.edu, laird@umich.edu  
Phone: FAX: (313) 763-1260 Tel: (313) 747-1761  
Title: Toward Incremental Knowledge Correction for Agents in Complex Environments  
Author: Douglas J. Pearson and John E. Laird 
Keyword: Machine Learning, Theory Revision, Error Detection, Error Correction, Domain Theory, Procedural Knowledge, Autonomous Agents  
Address: 1101 Beal Ave. Ann Arbor, Michigan 48109-2122  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  
Abstract: In complex, dynamic environments, an agent's domain knowledge will rarely be complete and correct. Existing deliberate approaches to domain theory correction are significantly restricted in the environments where they can be used. These systems are typically not used in agent-based tasks and rely on declarative representations to support non-incremental learning. This research investigates the use of procedural knowledge to support deliberate incremental error correction in complex environments. We describe a series of domain properties that constrain the error correction process and that are violated by existing approaches. We then present a procedural representation for domain knowledge which is sufficiently expressive, yet tractable. We develop a general framework for error detection and correction and then describe an error correction system, IMPROV, that uses our procedural representation to meet the constraints imposed by complex environments. Finally, we test the system in two sample domains and empirically demonstrate that it satisfies many of the constraints faced by agents in complex and challenging environments. 
Abstract-found: 1
Intro-found: 1
Reference: [Carbonell et al., 1991] <author> Jaime G. Carbonell, Craig A. Knoblock, and Steven Minton. </author> <title> Prodigy: An integrated architecture for planning and learning. </title> <editor> In Kurt VanLehn, editor, </editor> <booktitle> Architectures for Intelligence. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <year> 1991. </year>
Reference-contexts: However, there is a danger that as these representations are enriched (in response to D1, D2, D3) and as the number of operators grows, the cost of full first-order access to the preconditions and actions of all operators (as in the "glass box" assumption of PRODIGY <ref> [Carbonell et al., 1991] </ref>) will be prohibitive during error correction. We are pursuing an alternative in which we develop approaches to planning, error detection and correction which depend only on limited and efficient procedural access to the preconditions and actions of an operator.
Reference: [Clearwater et al., 1989] <author> Scott H. Clearwater, Tze-Pin Cheng, Haym Hirsh, and Bruce G. Buchanan. </author> <title> Incremental batch learning. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 366-370, </pages> <year> 1989. </year>
Reference-contexts: Thus IMPROV and EXPO define two interesting, k-incremental learners along the spectrum from pure incremental learners to pure non-incremental learners (Figure 6). K-incremental learning is related to incremental batch learners, such as RL <ref> [Clearwater et al., 1989] </ref>. However, those learners train on a set of randomly selected instances and therefore are passive learners. IMPROV and EXPO are actively creating instances as they act in the world. This means the agents must be more careful in their learning.
Reference: [Doorenbos, 1993] <author> R. Doorenbos. </author> <title> Matching 100,000 learned rules. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI, AAAI Press, </publisher> <year> 1993. </year>
Reference-contexts: Once selected, an operator's actions are performed by "operator implementation" rules which support actions with conditional effects or duration. The sufficiency of this representation for constraints D1-D3 (and efficient matching <ref> [Doorenbos, 1993] </ref>) has been demonstrated for complex, real-time domains including control of simulated aircraft [Pearson et al., 1993] and tactical air combat [Laird et al., 1995; Tambe et al., 1995]. An IMPROV agent will use this operator knowledge to both plan and behave in its world.
Reference: [Gil, 1991] <author> Yolanda Gil. </author> <title> A domain-independent framework for effective experimentation in planning. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> pages 13-17, </pages> <year> 1991. </year>
Reference-contexts: Existing domain theory correction systems use declarative representations of operators as their domain knowledge. These representations, in turn, support deliberate error correction strategies (e.g. EITHER [Ourston and Mooney, 1990], EXPO <ref> [Gil, 1991] </ref>, OCCAM [Pazzani, 1988], CLIPS-R [Murphy and Pazzani, 1994], FOIL [Quinlan, 1990]). <p> based on polygons, would not have this property and this bias would not assist, and could hinder, learning. 9 If training is delayed even further, until a unique cause has been identified, then this would result in a system which is actively experimenting (such as the simple induction in EXPO <ref> [Gil, 1991] </ref>). Thus IMPROV and EXPO define two interesting, k-incremental learners along the spectrum from pure incremental learners to pure non-incremental learners (Figure 6). K-incremental learning is related to incremental batch learners, such as RL [Clearwater et al., 1989].
Reference: [Holland, 1986] <author> John H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Deliberate systems also typically consider multiple training episodes when making a correction. These approaches can be contrasted with implicit approaches for correcting behavior, such as Classifiers <ref> [Holland, 1986] </ref>, Backpropogation [Rumelhart et al., 1986], Q-learning [Watkins and Dayan, 1992], that are restricted to only detecting errors when an additional training signal is provided and are limited to correcting errors in execution knowledge, rather than planning knowledge which restricts them to purely reactive systems.
Reference: [Laird and Rosenbloom, 1990] <author> J. E. Laird and P. S. Rosenbloom. </author> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Errors in the planning knowledge lead to errors in behavior, so it is planning knowledge that IMPROV learns to correct. In IMPROV, planning does not create a monolithic plan. Instead, situation dependent control rules are learned that will propose the operators when appropriate during execution <ref> [Laird and Rosenbloom, 1990] </ref>. For example, a possible "plan" to drive through an intersection (where the rules are preconditions for the operators) is shown in Figure 3. <p> Thus, in a well-trained agent, planning will be the exception, with only enough planning to fill in for novel situations <ref> [Laird and Rosenbloom, 1990] </ref>. 3 To create a plan, IMPROV does not search through a space of plans because of the restrictions it has on access to its operators. IMPROV could potentially use a variety of state-space planning methods.
Reference: [Laird et al., 1986] <author> John E. Laird, Paul S. Rosenbloom, and Allen Newell. </author> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: Loops are detected if the agent returns to a state in a previously visited equivalence class. IMPROV efficiently calculates these equivalence classes by explaining, at each step during execution, why the next operator in the operator is being selected. This explanation leads, through Soar's EBL method chunking <ref> [Laird et al., 1986] </ref>, to a new rule being learned which matches exactly the precondition of the operator being chosen. Whenever one of these rules fire, a cycle is detected.
Reference: [Laird et al., 1995] <author> J. E. Laird, W. L. Johnson, R. M. Jones, F. Koss, J. F. Lehman, P. E. Nielsen, P. S. Rosenbloom, R. Rubinoff, K. Schwamb, M. Tambe, J. Van Dyke, M. van Lent, and R. E. Wray. </author> <title> Simulated intelligent forces for air: </title> <booktitle> The Soar/IFOR project 1995. In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> pages 27-36, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The sufficiency of this representation for constraints D1-D3 (and efficient matching [Doorenbos, 1993]) has been demonstrated for complex, real-time domains including control of simulated aircraft [Pearson et al., 1993] and tactical air combat <ref> [Laird et al., 1995; Tambe et al., 1995] </ref>. An IMPROV agent will use this operator knowledge to both plan and behave in its world. Errors in the planning knowledge lead to errors in behavior, so it is planning knowledge that IMPROV learns to correct.
Reference: [Laird, 1988] <author> John E. Laird. </author> <title> Recovery from incorrect knowledge in Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 618-623, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: IMPROV therefore corrects the knowledge by learning additional rules that correct the decision about which operator to select. A rule is learned to reject the original (incorrect) operator and another rule is learned which suggests the new (correct) operator. This approach is explained in detail in Laird 1988 <ref> [Laird, 1988] </ref>. 10 4 Results IMPROV has been implemented in two test domains. The first was the toy domain of a robot in the blocks world, which provided a good initial test-bed during development.
Reference: [Miller, 1991] <author> Craig M. Miller. </author> <title> A constraint-motivated model of concept formation. </title> <booktitle> In The Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 827-831, </pages> <year> 1991. </year>
Reference-contexts: Secondly, as rules within the inductive learner, that are less efficient to access (requiring a deliberate search) but support learning well. As the inductive 7 learner must be incremental and able to represent disjunctive sets of preconditions (D1) we have used the symbolic category learner SCA <ref> [Miller, 1991; Miller, 1993] </ref>, which can learn arbitrarily complex categories 5 . SCA is also incremental, tolerant of noise and is guaranteed to converge to the correct category, if that category can be represented in its description language. The full correction algorithm is summarized in Figure 4. <p> However, these distinctions are not critical to understanding the important aspects of the algorithm. During a correction episode, IMPROV searches for a correct plan, P success . This is done 5 In the limit SCA can represent each exemplar individually <ref> [Miller, 1991] </ref>. 8 by generating a series of plans, P i , using the UBID planning method mentioned earlier. UBID generates plans in decreasing order of probability of successfully reaching the goal.
Reference: [Miller, 1993] <author> Craig M. Miller. </author> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> PhD thesis, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering, </institution> <year> 1993. </year>
Reference-contexts: Secondly, as rules within the inductive learner, that are less efficient to access (requiring a deliberate search) but support learning well. As the inductive 7 learner must be incremental and able to represent disjunctive sets of preconditions (D1) we have used the symbolic category learner SCA <ref> [Miller, 1991; Miller, 1993] </ref>, which can learn arbitrarily complex categories 5 . SCA is also incremental, tolerant of noise and is guaranteed to converge to the correct category, if that category can be represented in its description language. The full correction algorithm is summarized in Figure 4.
Reference: [Murphy and Pazzani, 1994] <author> Patrick M. Murphy and Michael J. Pazzani. </author> <title> Revision of production system rule-bases. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 199-207, </pages> <year> 1994. </year>
Reference-contexts: Existing domain theory correction systems use declarative representations of operators as their domain knowledge. These representations, in turn, support deliberate error correction strategies (e.g. EITHER [Ourston and Mooney, 1990], EXPO [Gil, 1991], OCCAM [Pazzani, 1988], CLIPS-R <ref> [Murphy and Pazzani, 1994] </ref>, FOIL [Quinlan, 1990]).
Reference: [Ourston and Mooney, 1990] <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: If an error is detected, the agent must determine which aspect of its knowledge is in error (credit assignment) and then correct that knowledge. Existing domain theory correction systems use declarative representations of operators as their domain knowledge. These representations, in turn, support deliberate error correction strategies (e.g. EITHER <ref> [Ourston and Mooney, 1990] </ref>, EXPO [Gil, 1991], OCCAM [Pazzani, 1988], CLIPS-R [Murphy and Pazzani, 1994], FOIL [Quinlan, 1990]).
Reference: [Pazzani, 1988] <author> Michael J. Pazzani. </author> <title> Integrated learning with incorrect and incomplete theories. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 291-297, </pages> <year> 1988. </year>
Reference-contexts: Existing domain theory correction systems use declarative representations of operators as their domain knowledge. These representations, in turn, support deliberate error correction strategies (e.g. EITHER [Ourston and Mooney, 1990], EXPO [Gil, 1991], OCCAM <ref> [Pazzani, 1988] </ref>, CLIPS-R [Murphy and Pazzani, 1994], FOIL [Quinlan, 1990]).
Reference: [Pearson et al., 1993] <author> D. J. Pearson, S. B. Huffman, M. B. Willis, J. E. Laird, and R. M. Jones. </author> <title> A symbolic solution to intelligent real-time control. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 11 </volume> <pages> 279-291, </pages> <year> 1993. </year>
Reference-contexts: Once selected, an operator's actions are performed by "operator implementation" rules which support actions with conditional effects or duration. The sufficiency of this representation for constraints D1-D3 (and efficient matching [Doorenbos, 1993]) has been demonstrated for complex, real-time domains including control of simulated aircraft <ref> [Pearson et al., 1993] </ref> and tactical air combat [Laird et al., 1995; Tambe et al., 1995]. An IMPROV agent will use this operator knowledge to both plan and behave in its world.
Reference: [Quinlan, 1990] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Existing domain theory correction systems use declarative representations of operators as their domain knowledge. These representations, in turn, support deliberate error correction strategies (e.g. EITHER [Ourston and Mooney, 1990], EXPO [Gil, 1991], OCCAM [Pazzani, 1988], CLIPS-R [Murphy and Pazzani, 1994], FOIL <ref> [Quinlan, 1990] </ref>). In deliberate error correction, the system detects errors without any special signal from the environment (such as a payoff function, or a training signal), and corrects its internal planning knowledge of its actions by determining which operator was in error and then modifying it.
Reference: [Rosenbloom et al., 1993] <author> P. S. Rosenbloom, J. E. Laird, and A. </author> <title> Newell. </title> <booktitle> The Soar Papers: Research on Integrated Intelligence. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: The approach also releases us from the assumption that a declarative representation is always available. Our hypothesis, born out in our implementation, is that declarative access to just the names of operators is sufficient for error correction. This type of restricted access is provided by Soar <ref> [Rosenbloom et al., 1993] </ref>, the architecture in which IMPROV is implemented. In Soar, an operator's preconditions are defined by rules which test for situations in which the operator should be selected, and therefore include control knowledge.
Reference: [Rumelhart et al., 1986] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propogation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Deliberate systems also typically consider multiple training episodes when making a correction. These approaches can be contrasted with implicit approaches for correcting behavior, such as Classifiers [Holland, 1986], Backpropogation <ref> [Rumelhart et al., 1986] </ref>, Q-learning [Watkins and Dayan, 1992], that are restricted to only detecting errors when an additional training signal is provided and are limited to correcting errors in execution knowledge, rather than planning knowledge which restricts them to purely reactive systems.
Reference: [Tambe et al., 1995] <author> M. Tambe, W. L. Johnson, R. M. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb. </author> <title> Intelligent agents for interactive simulation environments. </title> <note> To appear in AI Magazine, </note> <year> 1995. </year>
Reference-contexts: The sufficiency of this representation for constraints D1-D3 (and efficient matching [Doorenbos, 1993]) has been demonstrated for complex, real-time domains including control of simulated aircraft [Pearson et al., 1993] and tactical air combat <ref> [Laird et al., 1995; Tambe et al., 1995] </ref>. An IMPROV agent will use this operator knowledge to both plan and behave in its world. Errors in the planning knowledge lead to errors in behavior, so it is planning knowledge that IMPROV learns to correct.
Reference: [Watkins and Dayan, 1992] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: Deliberate systems also typically consider multiple training episodes when making a correction. These approaches can be contrasted with implicit approaches for correcting behavior, such as Classifiers [Holland, 1986], Backpropogation [Rumelhart et al., 1986], Q-learning <ref> [Watkins and Dayan, 1992] </ref>, that are restricted to only detecting errors when an additional training signal is provided and are limited to correcting errors in execution knowledge, rather than planning knowledge which restricts them to purely reactive systems.
References-found: 20

