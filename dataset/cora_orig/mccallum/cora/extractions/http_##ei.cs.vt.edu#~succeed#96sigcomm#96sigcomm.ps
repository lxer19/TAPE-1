URL: http://ei.cs.vt.edu/~succeed/96sigcomm/96sigcomm.ps
Refering-URL: http://ei.cs.vt.edu/~succeed/96sigcomm/96sigcomm.html
Root-URL: http://www.cs.vt.edu
Email: fwilliams,abramsg@cs.vt.edu, stand@eng.fsu.edu, fabdulla,foxg@cs.vt.edu  
Title: Removal Policies in Network Caches for World-Wide Web Documents  
Author: Stephen Williams, Marc Abrams Charles R. Standridge Ghaleb Abdulla, Edward A. Fox 
Address: VA 24061-0106)  FL 32310)  VA 24061-0106)  
Affiliation: (Computer Science, Virginia Tech, Blacksburg,  (Industrial Eng., FAMU-FSU College of Eng., Tallahassee,  (Computer Science, Virginia Tech, Blacksburg,  
Abstract: World-Wide Web proxy servers that cache documents can potentially reduce three quantities: the number of requests that reach popular servers, the volume of network traffic resulting from document requests, and the latency that an end-user experiences in retrieving a document. This paper examines the first two using the measures of cache hit rate and weighted hit rate (or fraction of client-requested bytes returned by the proxy). A client request for an uncached document may cause the removal of one or more cached documents. Variable document sizes and types allow a rich variety of policies to select a document for removal, in contrast to policies for CPU caches or demand paging, that manage homogeneous objects. We present a taxonomy of removal policies. Through trace-driven simulation, we determine the maximum possible hit rate and weighted hit rate that a cache could ever achieve, and the removal policy that maximizes hit rate and weighted hit rate. The experiments use five traces of 37 to 185 days of client URL requests. Surprisingly, the criteria used by several proxy-server removal policies (LRU, Hyper-G, and a proposal by Pitkow and Recker) are among the worst performing criteria in our simulation; instead, replacing documents based on size maximizes hit rate in each of the studied workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abrams, C. R. Standridge, G. Abdulla, S. Williams, and E. A. Fox. </author> <title> Caching proxies: Limitations and potentials. </title> <booktitle> In 4th International World-wide Web Conference, </booktitle> <pages> pages 119-133, </pages> <address> Boston, </address> <month> Dec. </month> <year> 1995. </year> <note> &lt;URL: http:- //ei.cs.vt.edu/~succeed/WWW4/WWW4.html&gt;. </note>
Reference-contexts: On the other hand, proxy caches are simpler in that there are never "dirty" documents to write back. We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) [16]; least recently used (LRU) [16]; LRU-MIN <ref> [1] </ref>, a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G [3] server's policy; and the policy proposed in [13], denoted Pitkow/Recker. <p> The requests are for servers both within and outside the .cs.vt.edu domain. Workloads G and C are identical to those used in <ref> [1] </ref>, while workload U in [1] is a subset of workload U used here. There are a few caveats in the interpretation of our data based on these workloads. <p> The requests are for servers both within and outside the .cs.vt.edu domain. Workloads G and C are identical to those used in <ref> [1] </ref>, while workload U in [1] is a subset of workload U used here. There are a few caveats in the interpretation of our data based on these workloads. <p> This behavior is confirmed by histograms of how many references went to partitioned cache with total size of 10% of MaxNeeded. files of different sizes, where the mass is concentrated in file sizes of under 1KB (Fig. 13). Therefore the previously proposed LRU-MIN policy <ref> [1] </ref> (that uses a primary key like blog 2 (SIZE)c) is one of the best policies, but the computationally simpler rule of just using SIZE as a key also works well.
Reference: [2] <author> M. Abrams, S. Williams, G. Abdulla, S. Patel, R. Ri-bler, and E. A. Fox. </author> <title> Multimedia traffic analysis using Chitra95. </title> <booktitle> In Proc. ACM Multimedia '95, </booktitle> <pages> pages 267-276, </pages> <address> San Francisco, </address> <month> Nov. </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: BL in Fig. 13 shows that most requests in the trace are for small documents; generally similar results for workloads U and G are reported in <ref> [2, Figs. 1,2,4] </ref>. The fact that SIZE keeps small files in the cache makes HR high for two reasons: most references go to small files, and removal of a large file makes room for many small files, which increases the percentage of URLs in the cache that can be hit. <p> First, users probably tend to avoid using large documents due to the time required to download them. Second, many popular Web pages are created by professional designers who keep graphics small to increase their usability. It is also apparent from the size distribution and file type distribution <ref> [2] </ref> that a many URL references are for embedded iconic images. Third, even in the case that users make frequent use of multi-page documents, it may be that users prefer to print the documents and repeatedly reference hard-copy rather than on-line copies.
Reference: [3] <author> K. Andrews, F. Kappe, H. Maurer, and K. Schmaranz. </author> <booktitle> On second generation hypermedia systems. In Proc. ED-MEDIA 95, World Conference on Educational Multimedia and Hypermedia, </booktitle> <address> Graz, Austria, </address> <month> June </month> <year> 1995. </year> <note> &lt;URL: http://www.ncsa.uiuc.edu/SDG/IT94/- Proceedings/DDay/claffy/main.html&gt;. </note>
Reference-contexts: We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) [16]; least recently used (LRU) [16]; LRU-MIN [1], a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G <ref> [3] </ref> server's policy; and the policy proposed in [13], denoted Pitkow/Recker.
Reference: [4] <author> M. F. Arlitt. </author> <title> A performance study of internet web servers. </title> <type> Master's thesis, </type> <institution> Computer Sci. Dept., University of Saskatchewan, Saskatoon, Saskatchewan, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Fig. 1 suggests that the number of requests to each server in workload BL follows a Zipf distribution. (In comparison, the requested URL in traces studied elsewhere <ref> [4, 9] </ref> also follows a Zipf distribution.) workload BL. Many clients in Fig. 1 are requesting URLs from a few servers, suggesting a high degree of concentration in workload BL.
Reference: [5] <author> M. F. Arlitt and C. L. Williamson. </author> <title> Web server workload characterization: The search for invariants. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <address> Philadelphia, PA, </address> <month> Apr. </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Caching proxies can reduce three different criteria: the number of requests that reach servers, the volume of network traffic resulting from document requests, and the latency that an end-user experiences in retrieving a document. We consider the first two in this paper. Arlitt and Williamson <ref> [5] </ref> also consider the first two, and observe that "optimizing one criterion does not necessarily optimize the other. <p> We study cache performance by trace-driven simulation of a proxy cache. Considered in the first experiment is the maximum values of HR and WHR for a given workload, which is obtained by simulating an infinite size cache. This represents the inherent "concentration" (as defined in <ref> [5] </ref>) in URL references by clients, arising when multiple clients request the same URL, or "temporal locality" by a single client that requests the same URL multiple times. <p> A server-based study examines logs from individual Web servers <ref> [5, 10, 13] </ref>. A client-based study examines logs collected by instrumenting a set of Web clients [9]. Server-based traffic usually contains a much higher request rate than client-based traffic, but server-based traffic is more homogeneous because it names a single server. <p> So the histogram would have a tail as interreference time goes to 0. Therefore, we expect ATIME to be a poor sorting key, and this turns out to be the case in Figures 8 to 12. This result is also consistent with <ref> [5] </ref>, which observes that LRU is not a good server cache removal policy, and concludes that temporal locality does not exist in requests reaching a server because servers multiplex requests of many clients and because client-side caching removes temporal locality from the reference stream.
Reference: [6] <author> T. Berners-Lee. </author> <title> Propagation, replication and caching. </title> <address> &lt;URL: http://www.w3.org/hypertext/WWW/Propagation/Activity.html&gt;, Mar. </address> <year> 1995. </year> <title> World Wide Web Consortium. </title>
Reference-contexts: The issue is discussed in <ref> [6] </ref>. 3. We observed a 15% to 55% WHR in a second level cache with a primary cache that is 10% of the size needed for no replacement.
Reference: [7] <author> T. Berners-Lee, R. Fielding, and H. Frystyk. </author> <title> Hypertext transfer protocol - HTTP 1.0. </title> <address> &lt;URL: http://www.w3.org/pub/WWW/- Protocols/HTTP/1.0/spec.html&gt;, </address> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: 1 Introduction It is ironic that what is arguably the most popular application of the Internet, namely the World-Wide Web (WWW or "Web"), is inherently unscalable. The protocol underlying the Web, HTTP <ref> [7] </ref>, is designed for a setting in which users access a large body of documents, each with a low volume of readership. However the Web today is characterized by high volume of access to popular Web pages. Thus identical copies of many documents pass through the same network links.
Reference: [8] <author> A. Chankhuthod, P. Danzig, C. Neerdaels, M. Schwartz, and K. Worrel. </author> <title> A hierarchical internet object cache. </title> <address> &lt;URL: ftp://ftp.cs.colorado.edu/pub/- cs/techreports/schwartz/HarvestCahce.ps.Z &gt;. </address>
Reference-contexts: A final open problem is to study the interaction of removal algorithms with algorithms that identify when cached copies may be inconsistent, such as expiration times or the time of last modification for documents. For example, the Harvest cache <ref> [8] </ref> tries to remove ex pired documents first. Caching has a bright future because Web users do not aimlessly and randomly request Web pages, according to our workloads. In fact, there is a significant (and to us unexpected) amount of concentration exhibited by all of the collected workloads.
Reference: [9] <author> C. R. Cunha, A. Bestavros, and M. E. Crovella. </author> <title> Characteristics of www client-based traces. </title> <type> Technical Report TR-95-010, </type> <institution> Computer Sci. Dept., Boston Univ., </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: A server-based study examines logs from individual Web servers [5, 10, 13]. A client-based study examines logs collected by instrumenting a set of Web clients <ref> [9] </ref>. Server-based traffic usually contains a much higher request rate than client-based traffic, but server-based traffic is more homogeneous because it names a single server. <p> Fig. 1 suggests that the number of requests to each server in workload BL follows a Zipf distribution. (In comparison, the requested URL in traces studied elsewhere <ref> [4, 9] </ref> also follows a Zipf distribution.) workload BL. Many clients in Fig. 1 are requesting URLs from a few servers, suggesting a high degree of concentration in workload BL.
Reference: [10] <author> T. T. Kwan, R. E. McGrath, and D. A. Reed. </author> <title> NCSA's world wide web server: Design and performance. </title> <journal> IEEE Computer, </journal> <volume> 28(11) </volume> <pages> 68-74, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Distribution is popular with commercial users that want to protect material with copyrights, and will only trust certain sites to keep copies of such material. However the cache model is the most popular method to date: caching at a server (e.g., <ref> [10] </ref>), caching at a client (e.g., caches built into Web browsers), and caching in the network itself through so-called proxy servers (e.g., [11]). This paper examines the last of these. <p> Widespread deployment of caching proxies within a large organization will lead to some caches handling misses from other caches. In experiment three we find the theoretical maximum HR and WHR of a second level cache. The fourth and final experiment considers a question raised in <ref> [10] </ref>: Video and audio files often represent a small fraction of requests in a workload, but due to their large file sizes may represent a majority of bytes transferred. Thus large video and audio files could displace many smaller documents of other types in a cache. <p> A server-based study examines logs from individual Web servers <ref> [5, 10, 13] </ref>. A client-based study examines logs collected by instrumenting a set of Web clients [9]. Server-based traffic usually contains a much higher request rate than client-based traffic, but server-based traffic is more homogeneous because it names a single server. <p> Meanwhile audio and graphics represented 96% of the bytes transferred in workload BR. The workloads illustrate a phenomena noted elsewhere (e.g., <ref> [10] </ref>) that a media type may account for a small fraction of references but a large fraction of bytes transferred. <p> Given a fixed amount of disk space for a cache, should all document types be cached together, or should the cache be partitioned by document type? (A partitioned cache would avoid the situation where one very large file displaces all other files in the cache <ref> [10] </ref>, pos sibly increasing overall hit rate.) 3.2 Experiment Design Table 5 defines four experiments. Experiment 1 addresses objectives 1 and 2 in x3.1. To compute the maximum possible weighted hit rate, we simulate each workload with an infinite size cache.
Reference: [11] <author> A. Luotonen and K. Altis. </author> <title> World-Wide Web proxies. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2), </volume> <year> 1994. </year> <note> &lt;URL: http://www1.cern.ch/PapersWWW94/- luotonen.ps&gt;. </note>
Reference-contexts: However the cache model is the most popular method to date: caching at a server (e.g., [10]), caching at a client (e.g., caches built into Web browsers), and caching in the network itself through so-called proxy servers (e.g., <ref> [11] </ref>). This paper examines the last of these. In the following, a document is any item retrieved by a Universal Resource Locator (URL), such as a dynamically created page or an audio file.
Reference: [12] <author> R. Malpani, J. Lorch, and D. Berger. </author> <title> Making World Wide Web caching servers cooperate. </title> <booktitle> In 4th International World-wide Web Conference, </booktitle> <pages> pages 107-117, </pages> <address> Boston, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Otherwise, P serves the local copy; this is also a hit. In (3), P either forwards the GET message to another proxy server (as in <ref> [12] </ref>) or to S (also a miss). Proxy caches can dramatically reduce network load.
Reference: [13] <author> J. E. Pitkow and M. M. Recker. </author> <title> A simple yet robust caching algorithm based on dynamic access patterns. </title> <booktitle> In Proc. 2nd Int. WWW Conf., </booktitle> <pages> pages 1039-1046, </pages> <address> Chicago, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) [16]; least recently used (LRU) [16]; LRU-MIN [1], a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G [3] server's policy; and the policy proposed in <ref> [13] </ref>, denoted Pitkow/Recker. <p> Periodically: Run policy every T time units, for some T . Both on-demand and periodically: Run policy at the end of each day and on-demand (Pitkow/Recker <ref> [13] </ref>). The question of how many documents to remove from the cache has been answered in two ways in policies proposed so far. For on-demand, one policy is to stop when the free cache area equals or exceeds the requested document size. <p> Sort Order SIZE size of a cached document (in bytes) largest file removed first blog 2 (SIZE)c floor of the log (base 2) of SIZE one of the largest files removed first ETIME time document entered the cache oldest access removed first (FIFO) ATIME time of last document access (recency <ref> [13] </ref>) least recently used files removed first (LRU) DAY (ATIME) day of last document access files last accessed the most days ago removed first NREF number of document references least referenced files removed first (LFU) Table 2: Example of removal policy for 42.5kB cache. Top: Sample trace. <p> A server-based study examines logs from individual Web servers <ref> [5, 10, 13] </ref>. A client-based study examines logs collected by instrumenting a set of Web clients [9]. Server-based traffic usually contains a much higher request rate than client-based traffic, but server-based traffic is more homogeneous because it names a single server.
Reference: [14] <author> A. A. B. Pritsker. </author> <title> Introduction to Simulation and SLAM II. </title> <publisher> John Wiley, </publisher> <address> Halsted NY, third edition, </address> <year> 1987. </year>
Reference: [15] <author> L. Schruben. </author> <title> Simulation modeling with event graphs. </title> <journal> Commun. ACM, </journal> <volume> 26 </volume> <pages> 957-963, </pages> <year> 1988. </year>
Reference: [16] <author> A. Silberschatz and P. B. Galvin. </author> <title> Operating Systems Concepts. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <note> fourth edition, </note> <year> 1994. </year>
Reference-contexts: On the other hand, proxy caches are simpler in that there are never "dirty" documents to write back. We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) <ref> [16] </ref>; least recently used (LRU) [16]; LRU-MIN [1], a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G [3] server's policy; and the policy proposed in [13], denoted Pitkow/Recker. <p> On the other hand, proxy caches are simpler in that there are never "dirty" documents to write back. We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) <ref> [16] </ref>; least recently used (LRU) [16]; LRU-MIN [1], a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G [3] server's policy; and the policy proposed in [13], denoted Pitkow/Recker. <p> We could directly compare removal policies by simulat ing various removal policies, such as the first-in, first-out (FIFO) <ref> [16] </ref>; least recently used (LRU) [16]; LRU-MIN [1], a variant of LRU that considers document size; least frequently used (LFU) [16]; the Hyper-G [3] server's policy; and the policy proposed in [13], denoted Pitkow/Recker.
References-found: 16

