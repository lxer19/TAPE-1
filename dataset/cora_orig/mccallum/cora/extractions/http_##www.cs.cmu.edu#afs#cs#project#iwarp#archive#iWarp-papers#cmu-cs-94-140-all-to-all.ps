URL: http://www.cs.cmu.edu/afs/cs/project/iwarp/archive/iWarp-papers/cmu-cs-94-140-all-to-all.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/user/tomstr/Mosaic/transposes.html
Root-URL: 
Title: An Architecture for Optimal All-to-All Personalized Communication  
Author: Susan Hinrichs, Corey Kosak, David R. O'Hallaron, Thomas M. Stricker, and Riichiro Takez 
Note: This report is an extended version of a paper that appeared in SPAA '94. Author's current address: Riichiro Take,  This research was sponsored in part by the Advanced Research Projects Agency/CSTO monitored by SPAWAR under contract N00039-93-C-0152, and in part by the Air Force Office of Scientific Research under Contract F49620-92-J-0131. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.  
Address: Pittsburgh, PA 15213  211, Japan.  
Affiliation: School of Computer Science Carnegie Mellon University  Fujitsu Laboratories Ltd., 1015 Kamikodanaka, Nakahara-ku, Kawasaki  
Pubnum: CMU-CS-94-140  
Email: email: riro@flab.fujitsu.co.jp.  
Date: September, 1994  
Abstract-found: 0
Intro-found: 1
Reference: [Ada93] <author> D. Adams. </author> <title> Cray T3D System Architecture Overview. </title> <type> Technical report, </type> <institution> Cray Research Inc., </institution> <month> September </month> <year> 1993. </year> <note> Revision 1.C. </note>
Reference-contexts: The Intel Paragon architecture uses a fast two-dimensional mesh connection and routes long messages with a basic routing scheme [Int91]. Similarly, the Cray T3D uses fast interconnects linked to a three-dimensional torus and a virtual channel wormhole router <ref> [Ada93] </ref>. The Fujitsu AP-1000 system uses a two-dimensional torus and has a large, structured buffer pool that provides the mechanisms for virtual channel, wormhole routing and for special routers like broadcast and AAPC. The IBM SP1 uses an Omega-like, multistage switch with flexible but static routing [Sni93]. <p> Specifically, the computation agent explicitly waits for all of the input queues to be NotInMessage before proceeding to the next phase and explicitly forwards the input queues for the next phase. In several other distributed memory systems <ref> [LAD + 92, Int91, Ada93, HII92, Sni93] </ref>, the communication and computation agents are not as tightly coupled, so the computation agent cannot directly observe and control the input queues. <p> and the synchronizing switch should be a simpler and more scalable design addition. 20 synchronization on iWarp. 4.3 AAPC on other machines In addition to measuring AAPC performance on iWarp, we compared the iWarp AAPC performance with all-to-all communication performance on three other commercial distributed memory systems: the Cray T3D <ref> [Ada93] </ref>, Thinking Machine's CM-5 [L + 93], and IBM's SP1 [Sni93]. Figure 16 compares the performance of these systems over a range of message sizes. All systems use 64 nodes. The T3D is a 2 fi 4 fi 8 3-dimensional submesh with a bisection bandwidth of 1.6 GB/s.
Reference: [B + 88] <author> S. Borkar et al. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 330-339, </pages> <year> 1988. </year>
Reference-contexts: The dragon switch is reconfigured for the next message when the tail of the current message has passed. This reconfiguration can be safely performed with only local information by relying on the AAPC structure. We have studied the performance of various AAPC algorithms on iWarp <ref> [B + 88, B + 90] </ref>, a distributed memory computer connected by a k-ary 2-cube or torus topology. iWarp's communication architecture includes many interesting features included or planned for other systems, including 1 program control of routing, block DMA transfer, and low message overhead.
Reference: [B + 90] <author> S. Borkar et al. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <type> Technical Report CMU-CS-90-197, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1990. </year> <title> Revision of a paper that appeared in the 17th Annual Intl. </title> <booktitle> Symposium on Computer Architecture, </booktitle> <address> Seattle, </address> <year> 1990, </year> <pages> pp. 70-81. </pages>
Reference-contexts: The dragon switch is reconfigured for the next message when the tail of the current message has passed. This reconfiguration can be safely performed with only local information by relying on the AAPC structure. We have studied the performance of various AAPC algorithms on iWarp <ref> [B + 88, B + 90] </ref>, a distributed memory computer connected by a k-ary 2-cube or torus topology. iWarp's communication architecture includes many interesting features included or planned for other systems, including 1 program control of routing, block DMA transfer, and low message overhead.
Reference: [BB92] <author> S. H. Bokhari and H. Berryman. </author> <title> Complete Exchange on a Circuit Switched Mesh. </title> <booktitle> In Proc. Scalable High Performance Computing Conference, </booktitle> <pages> pages 300-306, </pages> <address> Williams-burg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Horie and Hayashi [HH91] and Scott [Sco91] have proposed algorithms that directly send blocks of data to their destinations. These messages are partitioned into contention-free phases. If these phases are separated by global synchronization or some other method, this approach also optimally uses the network bandwidth. Bokhari and Berryman <ref> [BB92] </ref> present a hybrid approach for a two-dimensional mesh that does not optimally use the network bandwidth but requires fewer message start ups. This approach requires additional buffer allocation and address calculation on the intermediate nodes. <p> While a block of data may be sent to several intermediate processors, the size of each message is larger and the actual number of message start-ups is smaller. Several of these multistep algorithms for a mesh are described in <ref> [BB92] </ref>. Here is one simple two step algorithm for a torus. First, perform an AAPC along the rows to arrange all the data in its target column. Then perform an AAPC along the columns to move all the data to its target row.
Reference: [BGPS92] <author> P. Berman, L. Gravano, G. Pifarre, and J. Sanz. </author> <title> Adaptive Deadlock- and Livelock-Free Routing with All Minimal Paths in Torus Networks. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-12, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Previous work on mesh routers has shown that the non-adaptive e-cube routers can have severe performance limitations, due to imbalanced congestion. Several advanced, adaptive wormhole routers have been proposed for machines with virtual channels <ref> [BGPS92] </ref>, but only few of them have been implemented in hardware. We have tested some of these advanced routers with our iWarp message passing system.
Reference: [BHKW94] <author> J. Bruck, C. T. Ho, S. Kipnis, and D. Weathersby. </author> <title> Efficient Algorithms for All-to-All Communications in Multi-Port Message-Passing Systems. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 298-309, </pages> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: The CM-5 is a 64 node fat tree with a bisection bandwidth of 320 MB/s. The AAPC algorithm on the CM-5 was optimized for use in the CM-5 scientific library. The algorithm and performance numbers are due to Unger [Ung94]. The SP1 AAPC algorithms and measurements are from <ref> [BHKW94] </ref>. This algorithm attempts to minimize endpoint processing. Since SP1 is a switch-connected machine, it does not attempt to optimize network use. We made two preliminary measurements on the T3D. In the first naive implementation (labeled unphased), each node sends messages with no synchronization.
Reference: [BNK92] <author> A. Bar-Noy and S. Kipnis. </author> <title> Designing Broadcating Algorithms in the Postal Model for Message-Passing Systems. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 13-22, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year> <journal> ACM. </journal> <volume> 26 </volume>
Reference-contexts: Common models of message passing quantitatively characterize the message passing system by key parameters like message throughput and processing overhead and suggest that the programmer does not rely on the particular details of the target architecture <ref> [CKP + 92, BNK92] </ref>.
Reference: [Bok91] <author> S. Bokhari. </author> <title> Multiphase complete exchange on a circuit switched hypercube. </title> <type> Technical Report 91-5, </type> <institution> ICASE, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. Most algorithms have concentrated on machines with a hypercube topology <ref> [JH89, VB92, Tak87, Bok91] </ref>. More recent work has explored machines with a k-ary n-cube topology. In [VB92] Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth.
Reference: [CKP + 92] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: towards a realistic model of parallel computation. </title> <type> Technical Report UCBC 92-713, </type> <institution> Univ. of California, Berkeley, </institution> <year> 1992. </year> <note> expanded version of paper in 4th Symp. on PPoPP. </note>
Reference-contexts: Common models of message passing quantitatively characterize the message passing system by key parameters like message throughput and processing overhead and suggest that the programmer does not rely on the particular details of the target architecture <ref> [CKP + 92, BNK92] </ref>.
Reference: [FSW93] <author> A. Feldmann, T. Stricker, and T. Warfel. </author> <title> Supporting sets of arbitrary connections on iWarp through communication context switches. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Schloss Velen, Westfalia, Germany, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: We looked at a nearest neighbor communication step, hypercube exchange communication step, and a communication step from a finite element method application described in <ref> [FSW93] </ref> 4 . Table 1 shows the performance of these communication steps. <p> The message passing program could generate 13 frames/sec, while the program using phased AAPC can generate 21 frames/sec. 8 fi 8 iWarp. 4 The performance of the FEM communication step in this paper differs from the performance in <ref> [FSW93] </ref> because this implementation does not measure the application buffering costs. 24 a code based on message passing AAPC. 5 Conclusions In this report, we have examined a phased AAPC algorithm and the architectural requirements for its efficient execution.
Reference: [GHH + 94] <author> T. Gross, A. Hasegawa, S. Hinrichs, D. O'Hallaron, and T. Stricker. </author> <title> The Impact of Communication Style on Machine Resource Usage for the iWwarp Parallel Processor. </title> <booktitle> Computer, </booktitle> <month> December </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Therefore, the complete per phase overhead on an 8 fi 8 iWarp system is 453 cycles. In principle the iWarp architecture can source and sink messages at 40 MBytes/s by either systolic 15 or memory communication <ref> [GHH + 94] </ref>. Systolic communication reads and writes data directly from the computation; network communicaiton looks like another register access. Memory computation relies on DMA agents (spoolers) to move data between the node's memory and the network without additional action by the computation agent.
Reference: [HH91] <author> T. Horie and K. Hayashi. </author> <title> All-to-All Personalized Communication on a Wrap-around Mesh. </title> <booktitle> In Proceedings of CAP Workshop, </booktitle> <address> Canberra, Austrailia, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: In [VB92] Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth. However, to utilize all network bandwidth, each node must be able to source and sink four messages simultaneously, i.e. have twice the memory bandwidth as incoming network bandwidth. Horie and Hayashi <ref> [HH91] </ref> and Scott [Sco91] have proposed algorithms that directly send blocks of data to their destinations. These messages are partitioned into contention-free phases. If these phases are separated by global synchronization or some other method, this approach also optimally uses the network bandwidth. <p> To be optimal, the AAPC phases proposed in <ref> [HH91] </ref> and [Sco91] must be carefully separated to preserve the contention-free schedule. In this paper, we introduce a synchronizing switch for torus networks based on the ideas of the dragon switch for multistage networks described in [TNY91].
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification Version 1.0., </title> <month> May </month> <year> 1993. </year>
Reference-contexts: The AAPC step occurs in multi-dimensional convolutions and in array transposes where only one dimension of the array is distributed. Recent implementations of data parallel compilers for High Performance Fortran <ref> [Hig93] </ref> include directives for general block-cyclic array distribution. Changing the distribution of an array often results in a communication where all processors or nearly all processors exchange unique blocks of data [SOG94].
Reference: [HII92] <author> T. Horie, H. Ishihata, and M. Ikesaka. </author> <title> Design and implementation of an interconnection network for the AP1000. </title> <booktitle> In Proc. IFIP World Computer Congress, </booktitle> <volume> volume I, </volume> <pages> pages 555-561. </pages> <note> Information Processing, </note> <year> 1992. </year>
Reference-contexts: Specifically, the computation agent explicitly waits for all of the input queues to be NotInMessage before proceeding to the next phase and explicitly forwards the input queues for the next phase. In several other distributed memory systems <ref> [LAD + 92, Int91, Ada93, HII92, Sni93] </ref>, the communication and computation agents are not as tightly coupled, so the computation agent cannot directly observe and control the input queues.
Reference: [Hin94] <author> S. Hinrichs. </author> <title> Compiler Resource Management for Connection-Based Communication. Internal document, </title> <year> 1994. </year>
Reference-contexts: Changing the distribution of an array often results in a communication where all processors or nearly all processors exchange unique blocks of data [SOG94]. The compiler can often detect when an AAPC step is required, so compile time recognition of AAPC is a reasonable assumption <ref> [Hin94] </ref>. Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. Most algorithms have concentrated on machines with a hypercube topology [JH89, VB92, Tak87, Bok91].
Reference: [Int91] <author> Intel Corp. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: The topologies of these machines differ. Thinking Machine's CM-5 architecture relies on a fat tree topology, sends short messages and uses randomized routing to deal with congestion [LAD + 92]. The Intel Paragon architecture uses a fast two-dimensional mesh connection and routes long messages with a basic routing scheme <ref> [Int91] </ref>. Similarly, the Cray T3D uses fast interconnects linked to a three-dimensional torus and a virtual channel wormhole router [Ada93]. <p> Specifically, the computation agent explicitly waits for all of the input queues to be NotInMessage before proceeding to the next phase and explicitly forwards the input queues for the next phase. In several other distributed memory systems <ref> [LAD + 92, Int91, Ada93, HII92, Sni93] </ref>, the communication and computation agents are not as tightly coupled, so the computation agent cannot directly observe and control the input queues.
Reference: [JH89] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Optimum Broadcasting and Personalized Communication in Hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. Most algorithms have concentrated on machines with a hypercube topology <ref> [JH89, VB92, Tak87, Bok91] </ref>. More recent work has explored machines with a k-ary n-cube topology. In [VB92] Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth.
Reference: [L + 93] <author> C. E. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-85, </pages> <year> 1993. </year>
Reference-contexts: should be a simpler and more scalable design addition. 20 synchronization on iWarp. 4.3 AAPC on other machines In addition to measuring AAPC performance on iWarp, we compared the iWarp AAPC performance with all-to-all communication performance on three other commercial distributed memory systems: the Cray T3D [Ada93], Thinking Machine's CM-5 <ref> [L + 93] </ref>, and IBM's SP1 [Sni93]. Figure 16 compares the performance of these systems over a range of message sizes. All systems use 64 nodes. The T3D is a 2 fi 4 fi 8 3-dimensional submesh with a bisection bandwidth of 1.6 GB/s.
Reference: [LAD + 92] <author> C. Leiserson, A. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, D. Hillis, B. Kuszmaul, M. St.Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: A number of recent parallel machines have been built with general purpose processors and a routing backplane. The topologies of these machines differ. Thinking Machine's CM-5 architecture relies on a fat tree topology, sends short messages and uses randomized routing to deal with congestion <ref> [LAD + 92] </ref>. The Intel Paragon architecture uses a fast two-dimensional mesh connection and routes long messages with a basic routing scheme [Int91]. Similarly, the Cray T3D uses fast interconnects linked to a three-dimensional torus and a virtual channel wormhole router [Ada93]. <p> Specifically, the computation agent explicitly waits for all of the input queues to be NotInMessage before proceeding to the next phase and explicitly forwards the input queues for the next phase. In several other distributed memory systems <ref> [LAD + 92, Int91, Ada93, HII92, Sni93] </ref>, the communication and computation agents are not as tightly coupled, so the computation agent cannot directly observe and control the input queues.
Reference: [MPI93] <author> The Message Passing Interface Forum. </author> <title> Draft Document for a Standard Message Passing Interface, </title> <month> November </month> <year> 1993. </year>
Reference-contexts: To compare a specialized AAPC architecture with a general message passing system, we are most concerned with the issues of routing and data transfer. For this reason, we did not use a standard message passing interface such as PVM [Sun90] or MPI <ref> [MPI93] </ref>. These standard interfaces have 18 software overheads resulting from error- and protection-checking in the operating system, buffer management, and standardized synchronization semantics, which result in copying data.
Reference: [Sco91] <author> D. S. Scott. </author> <title> Efficient All-to-All Communication Patterns in Hypercube and Mesh Topologies. </title> <booktitle> In The Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 398-403, </pages> <year> 1991. </year>
Reference-contexts: Theoretically, this algorithm optimally uses network bandwidth. However, to utilize all network bandwidth, each node must be able to source and sink four messages simultaneously, i.e. have twice the memory bandwidth as incoming network bandwidth. Horie and Hayashi [HH91] and Scott <ref> [Sco91] </ref> have proposed algorithms that directly send blocks of data to their destinations. These messages are partitioned into contention-free phases. If these phases are separated by global synchronization or some other method, this approach also optimally uses the network bandwidth. <p> To be optimal, the AAPC phases proposed in [HH91] and <ref> [Sco91] </ref> must be carefully separated to preserve the contention-free schedule. In this paper, we introduce a synchronizing switch for torus networks based on the ideas of the dragon switch for multistage networks described in [TNY91].
Reference: [Sni93] <author> M. Snir. </author> <title> Scalable Parallel Computing The IBM 9076 Scalable POWERParallel-1. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> page 42. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year> <month> 27 </month>
Reference-contexts: The Fujitsu AP-1000 system uses a two-dimensional torus and has a large, structured buffer pool that provides the mechanisms for virtual channel, wormhole routing and for special routers like broadcast and AAPC. The IBM SP1 uses an Omega-like, multistage switch with flexible but static routing <ref> [Sni93] </ref>. In spite of the differing interconnect topologies and technologies, the communication architectures of these machines are quite similar. They all use wormhole routing to keep the per-hop hardware latency low. <p> Specifically, the computation agent explicitly waits for all of the input queues to be NotInMessage before proceeding to the next phase and explicitly forwards the input queues for the next phase. In several other distributed memory systems <ref> [LAD + 92, Int91, Ada93, HII92, Sni93] </ref>, the communication and computation agents are not as tightly coupled, so the computation agent cannot directly observe and control the input queues. <p> scalable design addition. 20 synchronization on iWarp. 4.3 AAPC on other machines In addition to measuring AAPC performance on iWarp, we compared the iWarp AAPC performance with all-to-all communication performance on three other commercial distributed memory systems: the Cray T3D [Ada93], Thinking Machine's CM-5 [L + 93], and IBM's SP1 <ref> [Sni93] </ref>. Figure 16 compares the performance of these systems over a range of message sizes. All systems use 64 nodes. The T3D is a 2 fi 4 fi 8 3-dimensional submesh with a bisection bandwidth of 1.6 GB/s.
Reference: [SOG94] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating Communication for Array State--ments: Design, Implementation, and Evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. to appear. </note>
Reference-contexts: Recent implementations of data parallel compilers for High Performance Fortran [Hig93] include directives for general block-cyclic array distribution. Changing the distribution of an array often results in a communication where all processors or nearly all processors exchange unique blocks of data <ref> [SOG94] </ref>. The compiler can often detect when an AAPC step is required, so compile time recognition of AAPC is a reasonable assumption [Hin94]. Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. <p> A two-dimensional FFT for a 512 fi 512 video image at 30 frames per second requires roughly 700 MegaFlop/sec. Most current High Performance Fortran (HPF) compilers generate message passing library calls to execute array transposes and other dense communication steps. An HPF compiler exists for iWarp <ref> [SOG94] </ref>, and the communication segment of a two-dimensional FFT generated by this compiler consists of two AAPC steps to execute the array transposes. Here we calculate the impact of improved AAPC performance on this application. The full integration of our new phased AAPC primitive into this compiler is in progress.
Reference: [SSO + 94] <author> T. Stricker, J. Stichnoth, D. O'Hallaron, S. Hinrichs, and T. Gross. </author> <title> Decoupling Communication Services for Compiled Parallel Programs. </title> <type> Technical Report CMU-CS-94-139, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Instead of a standard library, we use the deposit message passing library written for the Fx compiler [SSOG93], based on the deposit model <ref> [SSO + 94] </ref>. The deposit library allows direct deposit of incoming data to its final destination. At the time it is sent, a message is guaranteed to encounter a receiver that is ready to extract it from the network immediately, minimizing buffering overheads and message congestion at the receiver.
Reference: [SSOG93] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting Task and Data Parallelism on a Multicomputer. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: These aspects of message passing libraries are an important issue of research, but for this study they would have obscured the architectural insights into the nature of an all-to-all communication. Instead of a standard library, we use the deposit message passing library written for the Fx compiler <ref> [SSOG93] </ref>, based on the deposit model [SSO + 94]. The deposit library allows direct deposit of incoming data to its final destination.
Reference: [Str91] <author> T. Stricker. </author> <title> Message Routing on Irregular 2D-Meshes and Tori. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <pages> pages 170-177, </pages> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year> <note> Also appeared as Technical Report CMU-CS-91-109, </note> <institution> Carnegie Mellon School of Computer Science. </institution>
Reference-contexts: Figure 13 compares the performance of AAPC message passing using the phased schedule with and without synchronization. 17 schedule. One synchronizes between phases, and the other does not. 3.1 The iWarp message passing system On iWarp, the basic message passing routers are based on a reverse e-cube scheme <ref> [Str91] </ref>. All routes start in the X-direction, eventually turn corners, and continue in the Y-direction. The queues can be assigned to multiple pools and the routers can use date-lines to break circular dependencies, avoiding routing deadlocks.
Reference: [Sun90] <author> V. S. Sunderam. </author> <title> PVM: a Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-39, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: To compare a specialized AAPC architecture with a general message passing system, we are most concerned with the issues of routing and data transfer. For this reason, we did not use a standard message passing interface such as PVM <ref> [Sun90] </ref> or MPI [MPI93]. These standard interfaces have 18 software overheads resulting from error- and protection-checking in the operating system, buffer management, and standardized synchronization semantics, which result in copying data.
Reference: [Tak87] <author> R. </author> <title> Take. A Routing Method for All-to-all Burst on Hypercube Networks. </title> <booktitle> In Proc. 35th National Conference of Information Processing Society of Japan, </booktitle> <pages> pages 151-152, </pages> <year> 1987. </year> <title> In Japanese. </title>
Reference-contexts: Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. Most algorithms have concentrated on machines with a hypercube topology <ref> [JH89, VB92, Tak87, Bok91] </ref>. More recent work has explored machines with a k-ary n-cube topology. In [VB92] Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth.
Reference: [TNY91] <author> R. Take, Y. Noguchi, and H. Yokota. </author> <title> An Architecture for Parallel Database Computing. </title> <booktitle> In Transputing '91, </booktitle> <pages> pages 1-14, </pages> <year> 1991. </year>
Reference-contexts: They all use wormhole routing to keep the per-hop hardware latency low. While some of these machines have special support for broadcast communication, none of these machines have support specifically for AAPC. In <ref> [TNY91] </ref>, Take, Noguchi, and Yokota propose a dragon switch for a multistage network that performs all communication as AAPC steps. The dragon switch is reconfigured for the next message when the tail of the current message has passed. <p> To be optimal, the AAPC phases proposed in [HH91] and [Sco91] must be carefully separated to preserve the contention-free schedule. In this paper, we introduce a synchronizing switch for torus networks based on the ideas of the dragon switch for multistage networks described in <ref> [TNY91] </ref>. The synchronizing switch uses the structure of the AAPC phases to perform this phase separation more efficiently and scalably than globally synchronous methods. We show how the synchronizing switch can be incorporated into a standard wormhole routing communication architecture.
Reference: [Ung94] <author> Leo Unger. </author> <title> Optimized Matrix Transpositions on the Connection Machine CM-5. </title> <type> Personal communication, </type> <month> February </month> <year> 1994. </year>
Reference-contexts: The CM-5 is a 64 node fat tree with a bisection bandwidth of 320 MB/s. The AAPC algorithm on the CM-5 was optimized for use in the CM-5 scientific library. The algorithm and performance numbers are due to Unger <ref> [Ung94] </ref>. The SP1 AAPC algorithms and measurements are from [BHKW94]. This algorithm attempts to minimize endpoint processing. Since SP1 is a switch-connected machine, it does not attempt to optimize network use. We made two preliminary measurements on the T3D.
Reference: [Val82] <author> V. G. Valiant. </author> <title> A Scheme for Fast Parallel Communication. </title> <journal> SIAM Journal on Computing, </journal> <volume> 11 </volume> <pages> 350-361, </pages> <year> 1982. </year>
Reference-contexts: Uninformed algorithms are for more adaptable and may be the only alternative if no compile time information is available. However, with dense communication steps uninformed methods tend to have problems with hot spots and congestion. In <ref> [Val82] </ref>, Valiant proves that uninformed or oblivious routing can statistically avoid these hot spots by first routing to a random location and then routing to the final destination.
Reference: [VB92] <author> E. A. Varvarigos and D. P. Bertsekas. </author> <title> Communication algorithms for isotropic tasks in hypercubes and wraparound meshes. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 1233-1257, </pages> <year> 1992. </year> <month> 28 </month>
Reference-contexts: Since AAPC steps are so prevalent, many algorithms have been developed to perform AAPC efficiently, though implementations and performance numbers for these algorithms are hard to find. Most algorithms have concentrated on machines with a hypercube topology <ref> [JH89, VB92, Tak87, Bok91] </ref>. More recent work has explored machines with a k-ary n-cube topology. In [VB92] Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth. <p> Most algorithms have concentrated on machines with a hypercube topology [JH89, VB92, Tak87, Bok91]. More recent work has explored machines with a k-ary n-cube topology. In <ref> [VB92] </ref> Varvarigos and Bertekas propose a store and forward algorithm. Theoretically, this algorithm optimally uses network bandwidth. However, to utilize all network bandwidth, each node must be able to source and sink four messages simultaneously, i.e. have twice the memory bandwidth as incoming network bandwidth. <p> One division of AAPC algorithms is between algorithms that utilize wormhole routing and those that only rely on store and forward routing. The phased AAPC algorithm requires a wormhole router to support sending messages directly between processors that are not physically adjacent. The algorithm proposed in <ref> [VB92] </ref> only requires a very simple network interface that supports communication between physically adjacent processors. In this algorithm, all processors communicate to the same set of relative processors in each step. To send to relative processor (x,y) takes jxj + jyj steps.
References-found: 32

