URL: http://www.sls.lcs.mit.edu/~jwc/pub/icslp94.ps
Refering-URL: http://www.sls.lcs.mit.edu/~jwc/pub/pub.html
Root-URL: 
Email: email: jwc@goldilocks.lcs.mit.edu  
Title: A STUDY OF SPEECH RECOGNITION SYSTEM ROBUSTNESS TO MICROPHONE VARIATIONS: EXPERIMENTS IN PHONETIC CLASSIFICATION 1  
Author: Jane Chang and Victor Zue 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper presents experiments in phonetic classification conducted as part of a study on the effects of microphone variations on performance in speech recognition systems. The TIMIT corpus provides data recorded on a close-talking microphone, on a free field microphone and over telephone lines. The study focuses on the unmatched training and testing conditions under which degradation is most severe. Analysis of baseline performance characterizes the effects of microphone variations. Downsampling is shown to significantly improve performance for bandlimited conditions at the cost of some degradation for non-bandlimited conditions. Comparative analysis of microphone independent preprocessing techniques, including cepstral mean normalization, RASTA processing, spectral subtraction and codebook dependent cepstral normalization, reveals the effects and tradeoffs of different compensation techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Chang, </author> <title> "Speech recognition system robustness to microphone variations", </title> <type> S.M. Thesis, </type> <institution> MIT, </institution> <note> expected 1994. </note>
Reference-contexts: We conclude with a summary of our salient findings and a discussion of future work. Due to space limitations, this paper presents only classification results. Interested readers are referred to <ref> [1] </ref> for more recognition results and detailed discussions. TASK, CORPUS AND SYSTEM We have chosen phonetic classification and recognition tasks as the basis for a comparative study of microphone variabilities and a standardized benchmark of compensa tion techniques. <p> On the other hand, standard preprocessing techniques are not very effective when dealing with Telephone data, suggesting that further study is necessary in order to better understand the nature of telephone degradation. We have also conducted experiments on phonetic recognition, and the results are reported in <ref> [1] </ref>. In general, phonetic recognition results follow the same trend, except that the error rate, taking into account substitution, insertion and deletion, is higher by about 50%. Errors, mainly substitutions and deletions, become relatively more concentrated in closures/silences, suggesting difficulties in segmentation.
Reference: [2] <author> W. Fisher, G. Doddington and K. Goudie-Marshall, </author> <title> "The DARPA speech recognition research database: specifications and status", </title> <booktitle> Proc. DARPA Speech Recognition Workshop, </booktitle> <pages> 93-99, </pages> <month> Feb </month> <year> 1986. </year>
Reference-contexts: Experiments are conducted on matched and unmatched training and testing conditions with particular emphasis placed on the conditions where the training microphone is of higher quality than the testing microphone, since this is the most likely scenario for technology deployment. Experiments are conducted using TIMIT <ref> [2] </ref>, a phonetically rich, continuous speech corpus with time-aligned orthographic and phonetic transcriptions, making it possible to gather statistics on specific speech sounds and to conduct phonetic studies. TIMIT is particularly useful for this study because there exist essentially three simultaneous recordings made by each subject using three different transducers. <p> For example, convolutional distortion may be introduced by speakers' vocal tracts, room acoustics, and microphone transfer functions. Environmental noise can add distortions to the signal, and transmission channel can impose bandwidth limitations. The Sennheiser and B&K corpora were recorded in a relatively noise-free environment <ref> [2] </ref>. The Telephone data training set for the Sennheiser (s), B&K (b) and Telephone (t) corpora. was obtained by playing the Sennheiser data through an artificial mouth, recording the resulting signal using a telephone handset, and then transmitting it over a telephone network [3].
Reference: [3] <author> C. Jankowski, A. Kalyanswamy, S. Basson and J. Spitz, "N-TIMIT: </author> <title> A phonetically balanced, continuous speech, telephone bandwidth speech database", </title> <booktitle> Proc. ICASSP, </booktitle> <pages> 109-112, </pages> <year> 1990. </year>
Reference-contexts: The original TIMIT corpus released by NIST was recorded using a Sennheiser HMD-414 microphone. This data was subsequently passed through the telephone channel with the timing information preserved by researchers at NYNEX, resulting in the NTIMIT corpus <ref> [3] </ref>. The third set of data, less known to the research community, was recorded in stereo with the Sennheiser using a Bruel and Kjaer (B&K) Model 4165 microphone. This data has not been released by NIST and was only recently retrieved from archive tapes. <p> The Telephone data training set for the Sennheiser (s), B&K (b) and Telephone (t) corpora. was obtained by playing the Sennheiser data through an artificial mouth, recording the resulting signal using a telephone handset, and then transmitting it over a telephone network <ref> [3] </ref>. The Sennheiser is a noise-canceling, pressure-gradient microphone with a flat response in the frequency range of interest (approximately 150-7000 Hz). Since it is a close-talking, headset-mounted microphone, the Sennheiser records from a relatively constant distance and position near the mouth and is not very sensitive to environmental noise.
Reference: [4] <author> K. Lee and H. Hon, </author> <title> "Speaker-independent phone recognition using Hidden Markov Models", </title> <journal> Tran. ASSP, </journal> <pages> 1641-1648, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: Full covariance Gaussian models are used for the 39 phonetic classes commonly used in the literature <ref> [4] </ref> to facilitate easy comparison. GENERAL CHARACTERISTICS Observable acoustic variations come from diverse sources, each affecting the speech signal in a different way. For example, convolutional distortion may be introduced by speakers' vocal tracts, room acoustics, and microphone transfer functions.
Reference: [5] <author> H. Meng, </author> <title> "The Use of Distinctive Features for Automatic Speech Recognition", </title> <type> S.M. Thesis, </type> <institution> MIT, </institution> <year> 1991. </year>
Reference-contexts: The Telephone data are characterized by the aggregate of the Sennheiser and the telephone handset, as well as the noise and bandlimiting effects, to a frequency range of about 3 kHz, of a telephone network. three corpora. The spectral vectors are obtained by averaging the Mel-frequency spectral coefficients (MFSCs) <ref> [5] </ref>, computed at 5 ms intervals, for all the data in the training sets. Since the three data sets are identical in speaker and content, the mean spectral vectors shed direct light on the only difference between them i.e., the ways the data were recorded.
Reference: [6] <author> B. Chigier, </author> <title> "Phonetic classification on wide-band and telephone quality speech", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> 291-295, </pages> <year> 1992. </year>
Reference-contexts: Specifically, the computation of the MFCC signal representation at 16 kHz requires taking the logarithm of the small spectral values outside of the telephone bandwidth and applying 2 Note that this baseline error rate is comparable to those reported in the literature <ref> [6] </ref>, giving us some assurance that our signal representation is reasonable. techniques (see text for explanation). a cosine transform, thereby corrupting all of the telephone cepstral coefficients.
Reference: [7] <author> P. Moreno and R. Stern, </author> <title> "Sources of degradation of speech recognition in the telephone network", </title> <booktitle> Proc. </booktitle> <address> ICAASP, I-109-112, </address> <year> 1994. </year>
Reference-contexts: Even after downsampling, the error rate for condition [s:t] is still much greater than condition [s:b], suggesting additional sources of degradation other than bandlimiting, such as higher levels of network distortion and noise <ref> [7] </ref>. Without high frequency information, the low energy events, including bandlimited fricatives, stops and affricates in addition to closures and silences, are especially susceptible to these microphone effects.
Reference: [8] <author> F. Liu, R. Stern, X. Huang and A. Acero, </author> <title> "Efficient cepstral normalization for robust speech recognition", </title> <booktitle> Proc. DARPA Human Language Technology Workshop, </booktitle> <month> Mar </month> <year> 1993. </year>
Reference-contexts: Normalization techniques compensate for convolutional differences by estimating correction vectors from long term statistics of slowly varying microphone effects. These techniques vary in domain, weighting and amount of data across which averages are computed. Of the variations we implemented, cepstral mean normalization <ref> [8] </ref> is most simple and effective, taking advantage of the additive cepstral correlate to convolution. RASTA [9] processing is another normaliza tion technique that uses a highpass filter to remove slowly varying microphone effects. <p> We found subtraction to be most effective in the MFSC domain. Subtraction and normalization can be combined to compensate for both additive and convolutional effects by cascading preprocessing blocks. Codebook dependent cepstral normalization (CDCN) <ref> [8] </ref> is a more complex preprocessing technique that compensates jointly, rather than in cascade, for convolutional and additive effects. Condition [S:B] All of the preprocessing techniques improved the baseline condition [s:b] (34.5%) without overly degrading the condition [s:s].
Reference: [9] <author> H. Hermansky, N. Morgan, A. Bayya and P. Kohn, </author> <title> "Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)", </title> <booktitle> Proc. Eurospeech, </booktitle> <pages> 1367-1370, </pages> <year> 1991. </year>
Reference-contexts: These techniques vary in domain, weighting and amount of data across which averages are computed. Of the variations we implemented, cepstral mean normalization [8] is most simple and effective, taking advantage of the additive cepstral correlate to convolution. RASTA <ref> [9] </ref> processing is another normaliza tion technique that uses a highpass filter to remove slowly varying microphone effects. Spectral subtraction [10] techniques compensate for additive microphone effects by estimating correction vectors from short term noise and signal levels.
Reference: [10] <author> D. Compernolle, </author> <title> "Increased noise immunity in large vocabulary speech recognition with the aid of spectral subtraction", </title> <booktitle> Proc ICAASP, </booktitle> <address> 27.6.1-4, </address> <year> 1987. </year>
Reference-contexts: Of the variations we implemented, cepstral mean normalization [8] is most simple and effective, taking advantage of the additive cepstral correlate to convolution. RASTA [9] processing is another normaliza tion technique that uses a highpass filter to remove slowly varying microphone effects. Spectral subtraction <ref> [10] </ref> techniques compensate for additive microphone effects by estimating correction vectors from short term noise and signal levels. These techniques vary in domain and amount of data across which noise and signal levels are computed. We found subtraction to be most effective in the MFSC domain.
References-found: 10

