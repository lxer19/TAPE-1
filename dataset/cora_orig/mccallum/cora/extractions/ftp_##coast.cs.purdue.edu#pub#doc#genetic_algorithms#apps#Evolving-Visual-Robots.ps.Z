URL: ftp://coast.cs.purdue.edu/pub/doc/genetic_algorithms/apps/Evolving-Visual-Robots.ps.Z
Refering-URL: http://www.cs.purdue.edu/coast/archive/data/categ20.html
Root-URL: http://www.cs.purdue.edu
Title: Evolving Visually Guided Robots  
Author: D. Cliff, P. Husbands, I. Harvey 
Address: Falmer Brighton BN1 9QH England, U.K.  
Affiliation: The University of Sussex School of Cognitive and Computing Sciences  
Date: 220, July 1992  
Pubnum: CSRP  Cognitive Science Research Paper Serial No. CSRP 220  
Abstract: A version of this paper appears in: Proceedings of SAB92, the Second International Conference on Simulation of Adaptive Behaviour J.-A. Meyer, H. Roitblat, and S. Wilson, editors, MIT Press Bradford Books, Cambridge, MA, 1993. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. </author> <title> Braitenberg. Vehicles: Experiments in Synthetic Psychology. </title> <publisher> M.I.T. Press | Bradford Books, </publisher> <address> Cam-bridge MA, </address> <year> 1984. </year>
Reference-contexts: Of course, the animal doesn't have to construct any internal representations or reason about the cause of the darkness; it just has to do something useful. For this reason, our work to date on evolving visually guided robots has concentrated on ultra-low-resolution vision, close in spirit to Braitenberg's Vehicles <ref> [1] </ref>. The simulated robot has been given a few photoreceptor units, which could realistically be added to the physical robot.
Reference: [2] <author> D. T. Cliff. </author> <title> The computational hoverfly; a study in computational neuroethology. </title> <editor> In J.-A. Meyer and S. W. Wilson, editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior (SAB90), </booktitle> <pages> pages 87-96, </pages> <address> Cambridge MA, 1991. </address> <publisher> M.I.T. Press | Bradford Books. </publisher>
Reference-contexts: The simulation system incorporates accurate physics, based on empirical observations of the real system, with added noise and uncertainty. The visual sensor capabilities are simulated using a ray-tracing computer graphics system <ref> [2] </ref>. Parameters governing the robot's sampling of its visual field are under genetic control, and the resultant specifications could readily be constructed from discrete components. Our results demonstrate that it is possible to evolve control architectures for visual guidance, using high-level evaluation functions which make no explicit reference to vision. <p> The simulated robot was equipped with vision by embedding it within the SyCo vision simulator system described in <ref> [2] </ref>.
Reference: [3] <author> D. T. Cliff and S. Bullock. </author> <title> Adding `foveal vision' to Wilson's animat, 1992. </title> <publisher> Forthcoming. </publisher>
Reference-contexts: field of view is available, although software sampling (under `genetic' control) can provide any number of virtual pixels or photoreceptors facing any specified direction; rotation of this visual field about the vertical axis can be effected in software, as can any number of strategies for sampling the visual field (cf. <ref> [3] </ref>). A system of servo motors, racks and pinions can provide an accuracy of movement of plus or minus one millimetre. Touch sensors around the conical mirror complete the `body' of the robot. The robot's control network is simulated off-board on a computer.
Reference: [4] <author> N. Franceschini, J.-M. Pichon, and C. Blanes. </author> <title> Real time visuomotor control: from flies to robots. </title> <booktitle> In Proceedings of the 1991 International Conference on Advanced Robotics, </booktitle> <address> Pisa, </address> <year> 1991. </year>
Reference-contexts: The results presented here are all for robots operating in relatively simple environments, comparable to those used for testing some real visually guided robots (e.g. <ref> [4] </ref>); but not as visually complex as a typical cluttered office environment. The computational costs of providing appropriately accurate simulation data scales very poorly as the complexity of the environment increases. <p> The simulated robot has been given a few photoreceptor units, which could realistically be added to the physical robot. This could be done using discrete components (e.g. photodiodes, phototransistors, or ldr's) with individual lenses, thereby creating an electronic compound eye, cf. <ref> [4] </ref>; or by using conventional ccd cameras but impairing their optics by mounting sand-blasted glass screens in front of the lens so as to generate input images with focus-independent blur, prior to some coarse sub-sampling scheme.
Reference: [5] <author> A. S. Glassner, </author> <title> editor. An Introduction to Ray Tracing. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: The SyCo simulator synthesizes vision by means of a computer graphics technique called ray-tracing (see e.g. <ref> [5] </ref>). This is a method which involves instantaneous point-sample estimates (`rays') of the relevant projection integrals, and so aliasing is a common problem.
Reference: [6] <author> I. Harvey. </author> <title> Species adaptation genetic algorithms: A basis for a continuing SAGA. </title> <editor> In F.J. Varela and P. Bourgine, editors, </editor> <title> Towards a Practice of Autonomous Systems: </title> <booktitle> Proceedings of the First Euro-pean Conference on Artificial Life (ECAL91), </booktitle> <pages> pages 346-354. </pages> <publisher> M.I.T. Press | Bradford Books, </publisher> <address> Cambridge MA, </address> <year> 1992. </year>
Reference-contexts: Using a suitably extended form of genetic algorithm, viable architectures may result. In this paper we present results which validate our proposals. We employed the Saga evolutionary principle <ref> [6] </ref> to develop neural-network control architectures for a simulation model of a real robot under construction at Sussex. The simulation system incorporates accurate physics, based on empirical observations of the real system, with added noise and uncertainty. The visual sensor capabilities are simulated using a ray-tracing computer graphics system [2]. <p> The veto threshold is always significantly higher than the lower threshold for a neuron's sigmoid transfer function. The genetic algorithm used is in accordance with the Saga principles <ref> [6] </ref>: crossover allows only gradual changes in genotype length. Although we only present results here from simple preliminary experiments, we are currently evolving more complex nets from those developed here, still in keeping with the incremental Saga 5 approach.
Reference: [7] <author> I. Harvey, P. Husbands, and D. T. Cliff. </author> <title> Issues in evolutionary robotics. </title> <type> Technical Report CSRP 219, </type> <institution> University of Sussex School of Cognitive and Computing Sciences, </institution> <year> 1992. </year>
Reference-contexts: Section 6 describes this work, which we have only recently commenced. This paper deals largely with practical issues: our methodological position is expressed in more depth in a separate paper <ref> [7] </ref>. For the sake of completeness, it is summarised briefly in the next section. 1 2 Background In another paper [7], we have presented arguments supporting the notion that an evolutionary approach to the design of robot control systems can be expected to supersede design by hand. <p> This paper deals largely with practical issues: our methodological position is expressed in more depth in a separate paper <ref> [7] </ref>. For the sake of completeness, it is summarised briefly in the next section. 1 2 Background In another paper [7], we have presented arguments supporting the notion that an evolutionary approach to the design of robot control systems can be expected to supersede design by hand. <p> In that paper we also explored issues arising from the adoption of an evolutionary approach and gave results of preliminary simulation experiments in evolving control architectures for simple robots equipped with a few touch-sensors: four `whiskers' and two `bumpers'. For reasons explained in <ref> [7] </ref>, the control architectures were based on a particular kind of `neural' network, and central to the evolutionary mechanisms is the notion of a gradual incremental development, building on already existing capabilities. <p> The simulated robot must be able to cope with noisy limited precision perception, because that is all the real world has to offer. 3.2 Particulars 3.2.1 Vision In keeping with the minimal incremental approach advocated in <ref> [7] </ref>, we have commenced our studies by exploring the effects of adding just two photoreceptors to the sensor suite (bumpers and whiskers) described above. Taking a cue from biological vision, the sensors are situated in positions which are bilaterally symmetric about the robot's longitudinal midline. <p> As described in more detail in <ref> [7] </ref>, the simulated robot is cylindrical in shape with two wheels towards the front and a single trailing rear castor. The wheels have independent drives allowing turning on the spot and fairly unrestricted movement across a flat floor. Outputs from the robot's control networks feed direct to the wheel drives. <p> reflection with a rotation determined by its original direction of motion; if it collides at low speed its behaviour depends on the angle of incidence it may rotate until normal to the obstacle or it may skid around until it is parallel. 4 Experiments Results from earlier experiments discussed in <ref> [7] </ref> demonstrated that our methods could be used to evolve robots which could engage in primitive tactile-based navigation patterns such as wall-following. <p> The neural architecture chromosome is more complex, needing a fairly involved process of decoding. The coding and its interpretation are described briefly below, but further details can be found in <ref> [7] </ref>. The robots `neural-style' control networks have a fixed number of input units: one for each sensor. In this case there are eight: front and back bumper, two whiskers toward the front and two whiskers toward the back, and the two photoreceptors, or `eyes' (cf. Figure 1). <p> Figure 1). The networks also have a fixed number of outputs; two for each of the motor drives. As all of the units are noisy linear threshold devices (as described in <ref> [7] </ref>) with outputs in the range [0:0; 1:0] R, two units are needed to give the motors a signal in the range [1:0; 1:0] R, so that forwards and backwards motion is possible. <p> In contrast, the toytown robot has a (virtual) rotational degree of freedom, and can travel in and amongst the objects of a 3-D world, with a horizontal field of view manipulable between 0 ffi to 360 ffi . 7 Summary and Conclusions As further support of our claims in <ref> [7] </ref>, we have presented early results from experiments in evolving network processing architectures for mobile robots.
Reference: [8] <author> R. C. Nelson. </author> <title> Visual homing using an associative memory. </title> <journal> Biological Cybernetics, </journal> <volume> 65 </volume> <pages> 281-291, </pages> <year> 1991. </year>
Reference-contexts: However, although it does not `know' its absolute position and orientation, this information is always available to the experimenters. This is extremely useful for automatic fitness evaluation, re peatability, repositioning and so on. 9 The `Toytown' environment has some similarities with the `Tinytown' environment at Rochester <ref> [8] </ref>. However the latter has a camera pointing down that can move only in two dimensions, giving the equivalent of `low-flying aerial photographs'.
Reference: [9] <author> M. V. Srinivasan, S. B. Laughlin, and A. Dubs. </author> <title> Predictive coding: a fresh view of inhibition in the retina. </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 216 </volume> <pages> 427-459, </pages> <year> 1982. </year> <month> 10 </month>
Reference-contexts: First, ray-tracing is a computationally expensive process, so using fewer rays per receptor saves processing time. Second, real vision is not an arbitrary-precision process. In vision, noise is inescapable, and noise effectively reduces a continuum of brightness levels to a small number of discrete ranges (e.g. <ref> [9] </ref>). By limiting the number of rays per receptor, the precision of the brightness-value estimate is correspondingly reduced.
References-found: 9

