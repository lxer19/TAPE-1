URL: http://www.tue.nl/tm/vakgr/it/mwerkers/twe/cs97.ps
Refering-URL: http://www.tue.nl/tm/vakgr/it/mwerkers/twe/publ_ton.htm
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fweijters,antal,herikg@cs.rulimburg.nl  e-mail: weijters@cs.rulimburg.nl  
Phone: phone (voice): +31.43.3883303 phone (fax): +31.43.3252392  
Title: Behavioural Aspects of BP-SOM Running heading: Behavioural Aspects of BP-SOM Keywords: combining neural nets, overfitting,
Author: Ton Weijters, Antal van den Bosch, H. Jaap van den Herik Contact: A. J. M. M. Weijters 
Address: P.O. Box 616 NL-6200 MD Maastricht The Netherlands  
Affiliation: MATRIKS Department of Computer Science University of Maastricht  MATRIKS Department of Computer Science Faculty of General Sciences University of Limburg j Maastricht  
Abstract-found: 0
Intro-found: 1
Reference: [Carpenter et al., 1991] <author> Carpenter, G. A., Grossberg, S., and Reynolds, J. H. </author> <year> (1991). </year> <title> ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organising neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 565-588. </pages>
Reference: [Deco & Obradovic, 1996] <author> Deco, G. & Obradovic, D. </author> <year> (1996). </year> <title> An Information-theoretic Approach to Neural Computing. </title> <address> New York: </address> <publisher> Springer Verlag. </publisher>
Reference: [Fahlman & Lebiere, 1990] <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990). </year> <title> The Cascade-correlation Learning Architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Avoiding overfitting is closely correlated with finding an optimal network complexity. In this view, two types of avoiding overfitting, or regularisation, can be distinguished: (i) starting with an undersized network, and gradually increasing the network's complexity <ref> [Fahlman & Lebiere, 1990] </ref>, and (ii) starting with an oversized network and gradually decreasing its complexity by (a) pruning units or weights, (Mozer & Smolensky, 1989; Le Cun et al., 1990; Hassibi et al., 1992; Weigend, 1994), (b) adding penalty terms (e.g., Weigend et al., 1991), or (c) using early stopping
Reference: [Fritzke, 1995] <author> Fritzke, B. </author> <year> (1995). </year> <title> Growing grid: a self-organizing network with constant neighborhood range and adaptation strength. </title> <journal> Neural Processing Letters, </journal> <volume> 2, </volume> <pages> 9-13. </pages>
Reference-contexts: Obviously, one disadvantage of combining two learning algorithms is the increased number of parameters. Conversely we have indications that the bp-som parameters, except for the size of the soms, are task-independent. This exception could be solved by using growing soms or grids <ref> [Fritzke, 1995] </ref>. 3.1 Improving generalisation performance First experiment: date calculation Date calculation is an example of a task which easily leads to overfitting in mfns trained by bp; hence the trained mfn has a low generalisation performance (Norris, 1989).
Reference: [Hassibi et al., 1992] <author> Hassibi, B., Stork, D., & Wolff, G. </author> <title> Optimal brain surgeon and general network pruning. </title> <type> Technical Report 9235, </type> <institution> RICOH California Research Center, </institution> <address> Menlo Park, CA. </address>
Reference: [Hecht-Nielsen, 1988] <author> Hecht-Nielsen, R. </author> <year> (1988). </year> <title> Applications of counterpropagation networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 131-139. </pages>
Reference: [Hornik et al., 1989] <author> Hornik, K., Stinchcombe, M., & White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <type> Technical Report 88-45R, </type> <institution> Department of Economics, </institution> <address> San Diego, CA: UCSD. </address>
Reference: [Kohonen, 1989] <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-organisation and Associative Memory. </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference-contexts: To achieve this aim, the traditional mfn architecture (Rumelhart et al., 1986) is combined with self-organising maps (soms) <ref> [Kohonen, 1989] </ref>: each hidden layer of the mfn is associated with one som (see Figure 1). During training of the weights in the mfn, the corresponding som is trained on the hidden-unit activation patterns.
Reference: [Le Cun et al., 1990] <author> Le Cun, Y., Denker, J., & Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretsky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 2, </volume> <pages> 598-605. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Mood et al., 1950] <author> Mood, A. M., Graybill, F. A., & Boes, D. C. </author> <year> (1950). </year> <title> Introduction to the Theory of Statistics. </title> <address> Tokyo, Japan: McGraw-Hill. </address> <note> 3rd edition (1973). </note>
Reference: [Moody & Darken, 1989] <author> Moody, J., & Darken, C. </author> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-294. 17 </pages>
Reference: [Mozer & Smolensky, 1989] <author> Mozer, M. C., & Smolensky, P. </author> <year> (1989). </year> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 3-16. </pages>
Reference: [Norris, 1989] <author> Norris, D. </author> <year> (1989). </year> <title> How to build a connectionist idiot (savant). </title> <journal> Cognition, </journal> <volume> 35, </volume> <pages> 277-291. </pages>
Reference: [Prechelt, 1994] <author> Prechelt, L. </author> <year> (1994). </year> <title> Proben1: A set of neural network benchmark problems and benchmarking rules. </title> <type> Technical Report 24/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany. </institution>
Reference-contexts: Third experiment: gene-splice detection A third comparative experiment was performed by using the gene-1, gene-2, and gene-3 benchmark data sets extracted from the Proben1 benchmark collection <ref> [Prechelt, 1994] </ref>. The three data sets are different partitionings of a large data set representing the task of detecting splice boundaries between DNA exons and introns on the basis of a window of 60 DNA sequence elements (nucleotides). <p> Within subsets, all instances are associated with the same class (assuming a reliability of the som element of 1.0). This automatic division into homogeneous subsets can be a useful step in automatic rule extraction. 3.4.1 Experiment: monks-1 As an example, we trained bp-som networks on the monks-1 task <ref> [Prechelt, 1994] </ref>, a well-known benchmark problem for rule extraction. The task is to classify an instance (a1, a2, a3, a4, a5, a6) with six attributes.
Reference: [Quinlan, 1993] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Rissanen, 1983] <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 416-431. </pages>
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstruc-ture of Cognition, </booktitle> <volume> volume 1: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference: [Schaffer, 1993] <author> Shaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference: [Thornton, 1995] <author> Thornton, C. </author> <year> (1995). </year> <title> Measuring the difficulty of specific learning problems. </title> <journal> Connection Science, </journal> <volume> 7, </volume> <pages> 81-92. </pages>
Reference: [Weigend et al., 1991] <author> Weigend, A. S., Rumelhart, D. .E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In: R. P. Lippmann, J. E. Moody, and D. S. Touretzky (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> 875-882. </pages>
Reference: [Weigend, 1994] <author> Weigend, A. S. </author> <year> (1994). </year> <title> On overfitting and the effective number of hidden units. </title> <editor> In: M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A.S. Weigend (Eds.), </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> 335-342. </pages>
Reference: [Weijters, 1995] <author> Weijters, A. </author> <year> (1995). </year> <title> The BP-SOM architecture and learning rule. </title> <journal> Neural Processing Letters, Vol.2, </journal> <volume> No.6, </volume> <pages> 13-16. </pages>
Reference: [Weiss & Kulikowski, 1991] <author> Weiss, S. & Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems that Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Widrow & Hoff, 1960] <author> Widrow, B. & Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive Switching Circuits. </title> <booktitle> In 1960 IRE WESCON Convention Record, </booktitle> <volume> part 4, </volume> <pages> 96-104. </pages> <address> New York: IRE. </address>

References-found: 24

