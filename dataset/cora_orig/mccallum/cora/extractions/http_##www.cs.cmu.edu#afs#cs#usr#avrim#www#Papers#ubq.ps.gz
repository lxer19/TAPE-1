URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/ubq.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/pubs.html
Root-URL: 
Title: Learning With Unreliable Boundary Queries  
Author: Avrim Blum Prasad Chalasani Sally A. Goldman Donna K. Slonim 
Date: June 26, 1997  
Abstract: We introduce a model for learning from examples and membership queries in situations where the boundary between positive and negative examples is somewhat ill-defined. In our model, queries near the boundary of a target concept may receive incorrect or "don't care" responses, and the distribution of examples has zero probability mass on the boundary region. The motivation behind our model is that in many cases the boundary between positive and negative examples is complicated or "fuzzy." However, one may still hope to learn successfully, because the typical examples that one sees do not come from that region. We present several positive results in this new model. We show how to learn the intersection of two arbitrary halfspaces when membership queries near the boundary may be answered incorrectly. Our algorithm is an extension of an algorithm of Baum [7, 6] that learns the intersection of two halfspaces whose bounding planes pass through the origin in the PAC-with-membership-queries model. We also describe algorithms for learning several subclasses of monotone DNF formulas.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University Department of Computer Science, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Angluin and Krik~is [4] introduce a similar model of malicious membership queries in which the adversary may respond with incorrect answers instead of "don't know". Their paper proves that the class of monotone DNF formulas is learnable in this model. Angluin <ref> [1] </ref> has shown that read-once DNF formulas are also learnable with malicious membership queries. The main difference in motivation between our model and those above is that most previous work supposes that there is a clear boundary between the positive and negative examples with some noise included.
Reference: [2] <author> D. Angluin. </author> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 121-150, </pages> <year> 1990. </year>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [2, 20, 24] </ref>. Consequently, researchers have considered augmenting this learning protocol by allowing the learner to perform experiments.
Reference: [3] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: On the other hand, more generally we may allow a learning algorithm to output any polynomial-time algorithm as a hypothesis. This less constrained model is sometimes called "PAC-predictability" [17, 16]. Another well-investigated model of learning is that of exact learning from equivalence queries <ref> [3] </ref>. In this model, the learner proposes as a hypothesis some h 2 C, and in response is told "yes" if h = f , or is given a counterexample x such that h (x) 6= f (x). <p> It is not hard to see that any class learnable exactly from equivalence queries can be learned in the PAC setting <ref> [3] </ref> (though the 3 converse does not hold [8]). <p> Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [3, 21, 27, 18] </ref> and additional work in models that allow attribute noise [26, 13, 23]. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [4] <author> D. Angluin and M. Krik~is. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 57-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Sloan and Turan present algorithms in this model for learning the class of monotone k-term DNF formulas with membership queries alone and the class of monotone DNF formulas with membership and equivalence queries. Angluin and Krik~is <ref> [4] </ref> introduce a similar model of malicious membership queries in which the adversary may respond with incorrect answers instead of "don't know". Their paper proves that the class of monotone DNF formulas is learnable in this model. <p> difficult than those above in the sense that the membership query errors or omissions are chosen by an adversary (unlike the random noise models [5]), and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike <ref> [28, 4] </ref>). For example, in the case of a 1-term monotone DNF formula with the boundary radius r = 1, there may be exponentially many (in n) instances in the boundary region. (Example: let x 4 x 7 x 9 be the target term.
Reference: [5] <author> D. Angluin and D. K. </author> <title> Slonim. Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Their work uses membership queries to simulate a particular distribution. Frazier and Pitt [12] show that CLASSIC sentences are learnable in this noise model, using the fact that many distinct membership queries can be formulated that redundantly yield the same information. Angluin and Slonim <ref> [5] </ref> introduce a model of incomplete membership queries, in which a membership query on a given instance may persistently generate a "don't know" response. The "don't know" instances are chosen uniformly at random from the entire domain and may account for up to a constant fraction of the instances. <p> Our model is more difficult than those above in the sense that the membership query errors or omissions are chosen by an adversary (unlike the random noise models <ref> [5] </ref>), and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike [28, 4]). <p> One reason for studying the one-sided, false-positive error model (which is what we obtain from the IBQ model by treating all "don't care" responses as positive) is that the monotonicity of the target class provides some inherent ability to handle false-negative errors. In a related model, Angluin and Slonim <ref> [5] </ref> show how to learn monotone DNF with random false-negative answers to membership queries allowed anywhere in the domain (not just in the boundary region). However, it is not known how to extend their results to handle false positive errors.
Reference: [6] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: Our algorithm is an extension of an algorithm of Baum <ref> [7, 6] </ref> for learning the simpler class of intersections of two homogeneous halfspaces in the standard PAC-with-queries model 3 .
Reference: [7] <author> E. B. Baum. </author> <title> Polynomial time algorithms for learning neural nets. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 258-272, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our algorithm is an extension of an algorithm of Baum <ref> [7, 6] </ref> for learning the simpler class of intersections of two homogeneous halfspaces in the standard PAC-with-queries model 3 .
Reference: [8] <author> A. Blum. </author> <title> Separating distribution-free and mistake-bound learning models over the Boolean domain. </title> <journal> SIAM J. Comput., </journal> <volume> 23(5) </volume> <pages> 990-1000, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It is not hard to see that any class learnable exactly from equivalence queries can be learned in the PAC setting [3] (though the 3 converse does not hold <ref> [8] </ref>). The PAC and exact learning models are passive in that the learner cannot directly affect the type of examples it receives as input | in the PAC setting they are randomly generated, and in the exact setting they are chosen by an adversary.
Reference: [9] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year> <month> 20 </month>
Reference-contexts: The idea of Baum's algorithm is to reduce the problem of learning an intersection of two homogeneous halfspaces to the problem of learning an XOR of halfspaces, for which a PAC algorithm exists <ref> [9] </ref>. (That algorithm works by noticing that the XOR of ~v~x 0 and ~w~x 0 is equivalent to the degree-2 threshold function (~v ~x)( ~w ~x) &lt; 0 (except on degenerate inputs) which can be learned as a linear threshold function over an O (n 2 )-dimensional space.) The idea of <p> Then find a linear function P such that P (~x) &lt; 0 for all the marked (negative) examples and P (~x) 0 for all the positives. Finally, run the XOR-of-halfspaces learning algorithm of <ref> [9] </ref> to find a hypothesis H 0 that correctly classifies f~x 2 S : P (~x) 0g.
Reference: [10] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik--Chervonenkis Dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Now, attempt to find 3 A halfspace is homogeneous if its bounding hyperplane passes through the origin. 4 The VC-dimension of the hypothesis class is O (n 2 ) so a corresponding number of examples as given by the results of Blumer et al. <ref> [10] </ref> are needed. 6 which complicates the proof somewhat. Note that N EG nb is the negative region that is not darkly shaded.
Reference: [11] <author> M. Frazier, S. Goldman, N. Mishra, and L. Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <journal> J. of Comput. Syst. Sci., </journal> <volume> 52(3) </volume> <pages> 472-492, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: On the other hand, to partially compensate for this difficulty, we restrict membership query errors or omissions to the boundary region and we require that counterexamples to equivalence queries be chosen from outside the boundary region. In other related work, Frazier, Goldman, Mishra and Pitt <ref> [11] </ref> introduce a learning model in which there is incomplete information about the target function due to an ill-defined boundary. While the omissions in their model may be adversarially placed, all examples labeled with "?" (indicating unknown classification) must be consistent with knowledge about the concept class.
Reference: [12] <author> M. Frazier and L. Pitt. </author> <title> CLASSIC learning. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 23-34. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Their work uses membership queries to simulate a particular distribution. Frazier and Pitt <ref> [12] </ref> show that CLASSIC sentences are learnable in this noise model, using the fact that many distinct membership queries can be formulated that redundantly yield the same information.
Reference: [13] <author> S. Goldman and R. Sloan. </author> <title> Can PAC learning algorithms tolerate random noise. </title> <journal> Algorithmica, </journal> <volume> 14(1) </volume> <pages> 70-84, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [3, 21, 27, 18] and additional work in models that allow attribute noise <ref> [26, 13, 23] </ref>. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [14] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 705-726, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In models of persistent membership query noise, repeated queries to the same example receive the same answer as in the first call. Goldman, Kearns and Schapire <ref> [14] </ref> give a 2 Under the L 2 metric the distance d (~x; ~y) between ~x = fx 1 ; : : : ; x n g and ~y = fy 1 ; : : : ; y n g is p (x 1 y 1 ) 2 + + (x
Reference: [15] <author> S. A. Goldman and H. D. Mathias. </author> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 77-84. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: The "don't know" instances are chosen uniformly at random from the entire domain and may account for up to a constant fraction of the instances. Additional positive results in this model are obtained by Goldman and Mathias <ref> [15] </ref>. This model allows for a large number of "don't know" instances, but positive results in this model are typically highly dependent on the precisely uniform nature of the noise. Sloan and Turan [28] introduce the limited membership query model .
Reference: [16] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Inform. Comput., </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: On the other hand, more generally we may allow a learning algorithm to output any polynomial-time algorithm as a hypothesis. This less constrained model is sometimes called "PAC-predictability" <ref> [17, 16] </ref>. Another well-investigated model of learning is that of exact learning from equivalence queries [3].
Reference: [17] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0,1g functions on randomly drawn points. </title> <journal> Inform. Comput., </journal> <volume> 115(2) </volume> <pages> 284-293, </pages> <year> 1994. </year>
Reference-contexts: On the other hand, more generally we may allow a learning algorithm to output any polynomial-time algorithm as a hypothesis. This less constrained model is sometimes called "PAC-predictability" <ref> [17, 16] </ref>. Another well-investigated model of learning is that of exact learning from equivalence queries [3].
Reference: [18] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [3, 21, 27, 18] </ref> and additional work in models that allow attribute noise [26, 13, 23]. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [19] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In Proc. of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 382-391. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year>
Reference-contexts: The p-concepts model of Kearns and Schapire <ref> [19] </ref> also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [20] <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> J. ACM, </journal> <volume> 41(1) </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [2, 20, 24] </ref>. Consequently, researchers have considered augmenting this learning protocol by allowing the learner to perform experiments.
Reference: [21] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [3, 21, 27, 18] </ref> and additional work in models that allow attribute noise [26, 13, 23]. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [22] <author> K.J. Lang and E.B. Baum. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In Proceedings of International Joint Conference on Neural Networks, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: A problem with this type of approach 1 , as noticed by Lang and Baum <ref> [22] </ref>, is that questions of this sort that are near the concept boundary may result in unreliable answers. Merging an image of a 2 and a 3 tends to produce fl School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213-3891. avrim@theory.cs.cmu.edu.
Reference: [23] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [3, 21, 27, 18] and additional work in models that allow attribute noise <ref> [26, 13, 23] </ref>. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [24] <author> L. Pitt and M. K. Warmuth. </author> <title> Prediction preserving reducibility. </title> <journal> J. of Comput. Syst. Sci., </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year> <booktitle> Special issue for the Third Annual Conference of Structure in Complexity Theory (Washington, </booktitle> <address> DC., </address> <month> June </month> <year> 1988). </year> <month> 21 </month>
Reference-contexts: Evidence suggests that only relatively simple types of concepts can be learned passively in this way <ref> [2, 20, 24] </ref>. Consequently, researchers have considered augmenting this learning protocol by allowing the learner to perform experiments. <p> While this is clearly a highly-restrictive class, it is not difficult to show, using standard prediction preserving reductions <ref> [24] </ref>, that it is as hard to learn as general DNF formulas in the passive PAC model.
Reference: [25] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Inform. Proc. Lett., </journal> <volume> 37(5) </volume> <pages> 279-284, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries. Sakakibara <ref> [25] </ref> considers the case where each membership query is incorrectly answered with a fixed probability, but where one can increase reliability by asking the same membership query several times.
Reference: [26] <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [3, 21, 27, 18] and additional work in models that allow attribute noise <ref> [26, 13, 23] </ref>. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [27] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <journal> Inform. Proc. Lett., </journal> <volume> 54 </volume> <pages> 157-162, </pages> <year> 1995. </year>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [3, 21, 27, 18] </ref> and additional work in models that allow attribute noise [26, 13, 23]. The p-concepts model of Kearns and Schapire [19] also falls somewhat into this category. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [28] <author> R. H. Sloan and G. Turan. </author> <title> Learning with queries but incomplete information. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 237-245. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Additional positive results in this model are obtained by Goldman and Mathias [15]. This model allows for a large number of "don't know" instances, but positive results in this model are typically highly dependent on the precisely uniform nature of the noise. Sloan and Turan <ref> [28] </ref> introduce the limited membership query model . In this model, an adversary may arbitrarily select some number ` of examples on which it refuses to answer membership queries (or answers "don't know"), but the learner is now allowed to ask a number of queries polynomial in `. <p> difficult than those above in the sense that the membership query errors or omissions are chosen by an adversary (unlike the random noise models [5]), and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike <ref> [28, 4] </ref>). For example, in the case of a 1-term monotone DNF formula with the boundary radius r = 1, there may be exponentially many (in n) instances in the boundary region. (Example: let x 4 x 7 x 9 be the target term.
Reference: [29] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: For instance, C 2 probability mass in the shaded region, we can view the concept as an intersection of two halfspaces in our model. might be the class of monotone DNF formulas. In the distribution-free or PAC learning model <ref> [29] </ref>, to obtain information about an unknown target function f 2 C, the learner is provided access to labeled (positive and negative) examples of f , drawn randomly according to some unknown target distribution D over X . The learner is also given as input *; ffi &gt; 0.
Reference: [30] <author> P. M. Vaidya. </author> <title> Speeding-Up Linear Programming Using Fast Matrix Multiplication. </title> <booktitle> In Proc. of the 30th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 332-337. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Research Triangle Park, NC, </address> <year> 1989. </year> <month> 22 </month>
Reference-contexts: If we add some vector ~v to each ~x 2 S, this results in adding ~v to each point of the form 2~x pos ~x neg as well. In particular, this means that if we can prove that our algorithm 5 Using an algorithm due to Vaidya <ref> [30] </ref>, T LP (v; c), the time needed to solve a linear programming problem with v variables and c constraints (examples) each specified to b bits of precision, is O (bv (c + v) 1:5 ). 7 the dark-shaded region, which is the reflection of the positive and boundary regions through
References-found: 30

