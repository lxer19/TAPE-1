URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-328.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Title: GENERALIZATION OF TAYLOR'S THEOREM AND NEWTON'S METHOD VIA A NEW FAMILY OF DETERMINANTAL INTERPOLATION FORMULAS  
Author: Bahman Kalantari 
Keyword: Taylor's Theorem, Newton's Method, Interpolation, Confluent Divided Differences, Fixed-Point Iteration, Roots, Complex Roots, Generalized Fibonacci Numbers, Halley's Method, Pade Approximants.  
Note: AMS Subject Classification. 65H05, 65D05, 65Y20, 41A20, 41A21, 30C15, 30E10, 12D10.  
Address: New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Rutgers University,  
Abstract: The general form of Taylor's theorem gives the formula, f = P n + R n , where P n is the New-ton's interpolating polynomial, computed with respect to a confluent vector of nodes, and R n is the remainder. When f 0 6= 0, for each m = 2; : : : ; n + 1, we describe a "determinantal interpolation formula", f = P m;n + R m;n , where P m;n is a rational function in x and f itself. These formulas play a dual role in the approximation of f or its inverse. For m = 2, the formula is Taylor's and for m = 3 it gives Halley's iteration function, as well as a Pade approximant. By applying the formulas to P n , for each m 2, P m;m1 ; : : : ; P m;m+n2 , is a set of n rational approximations that includes P n , and may provide a better approximation to f, than P n . Thus each Taylor polynomial unfolds into an infinite spectrum of rational approximations. The formulas also give an infinite spectrum of rational inverse approximations, as well as a family of iteration functions for real or complex root finding, more fundamental than the Euler-Schroder family, or any other family. Given m 2, for each k m, we obtain a k-point iteration function, defined as the ratio of two determinants that depend on the first m k derivatives, and Toeplitz for k = 1. The order of convergence ranges from m to the limiting ratio of the generalized Fibonacci numbers of order m. By applying these formulas, Hadamard's inequality, Gerschgorin's theorem, and a new lower bound on determinants, we express roots of numbers, e, and , as the limiting ratio of Toeplitz determinants. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. ALEFELD, </author> <title> On the convergence of Halley's Method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 88 (1981), </volume> <pages> pp. 530-536. </pages>
Reference-contexts: Moreover, it gives an infinite expansion of e x at zero, different that the ordinary Taylor series. In general, H n (x; a) offers an approximation to Pade approximants <ref> [n; 1] </ref>, n 3. Example 1. Consider the case where f (x) = e x , a = 0 and any x 6= 2. <p> It was known that Halley's function can also be obtained by applying Newton's method to the function p= p p 0 , see Bateman [6], as well as Brown [12], and Alefeld <ref> [1] </ref>. Gerlach [19] gives a generalization of this approach and for m = 2; 3; : : :, inductively defines an iteration function G m (x) of order m. <p> Indeed in the case of m = 3, the determinantal formulas resulted in a Pade approximant [1=1], and at the same time, Halley's iteration function, and an alternative approximation to the Pade approximants <ref> [m; 1] </ref>, m 3. A very important application of the determinantal formulas is in terms of inverse approximation, as direct formulas, or as iteration functions. Firstly, the one-point version of Q m;m1 , when viewed as a sequence in m, gives determinantal approximation of the inverse of f .
Reference: [2] <author> L. AHLFORS, </author> <title> Complex Analysis. 2nd ed., </title> <publisher> McGraw-Hill Book Co., </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: For these formulas, see Ahlfors <ref> [2] </ref>, and Baker and Graves-Morris [5] (Hermite's formula), respectively. These formulas can be used to give estimate of the error in approximation by the Taylor polynomial (see [2]). Although we shall only speak of the case of real-valued functions defined on an interval, our determinantal formulas hold in more generality. <p> For these formulas, see Ahlfors <ref> [2] </ref>, and Baker and Graves-Morris [5] (Hermite's formula), respectively. These formulas can be used to give estimate of the error in approximation by the Taylor polynomial (see [2]). Although we shall only speak of the case of real-valued functions defined on an interval, our determinantal formulas hold in more generality. In particular the iteration functions to be developed can be used for the approximation of complex roots of analytic functions. Convention 1.
Reference: [3] <author> K.E. ATKINSON, </author> <title> An Introduction to Numerical Analysis. 2nd ed., </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1989. </year>
Reference-contexts: The treatment of Taylor's theorem for the case of distinct nodes may be found in standard numerical analysis textbooks such as Hildebrand [25], Dahlquist and Bjorck [14], and Atkinson <ref> [3] </ref>. Remark 1. It is important to note that in the case of distinct nodes, the identity f (x) = f (x 1 )+ P n+1 Q i l=1 (x x l ), holds over more general fields than the field of reals. <p> In order to evaluate P n (x) at a given x, one would proceed to form a table of confluent divided differences analogous to the case of distinct nodes, see e.g. [25], [14], <ref> [3] </ref>. Once the divided differences are computed, the efficient evaluation of the polynomial is as follows: Let fi n = f 1;n+1 . For i = n 1; : : : ; 0, compute fi i = f 1;i+1 + fi i+1 (x x i+1 ).
Reference: [4] <author> D.F. BAILEY, </author> <title> Historical survey of solution by functional iteration. </title> <journal> Math. Mag., </journal> <volume> 62 (1989), </volume> <pages> pp. 155-166. </pages>
Reference-contexts: Ironically, it was Halley's work on root finding that inspired Taylor to state the famous "Taylor's Theorem" (see [46], <ref> [4] </ref>, [37]) which in turn gives Newton's iteration function, its rate of convergence, and its asymptotic error constant. On the other hand, the limiting case of Newton's interpolation formula for the case of distinct nodes (ascribed to Newton, see e.g. [25]), reduces to Taylor's Theorem. <p> ) = 1 (a) 2f 0 (x 1 ) 2f 0 (x 1 ) 2 + f 00 (x 1 )f (x 1 ) This function apparently was obtained by Euler, Schroder, and Chebyshev (for which he received a silver medal in a student contest), see Traub [42], and Bailey <ref> [4] </ref>. If f 0 () 6= 0, and a (0) is properly chosen, the fixed-point iteration corresponding to 2 (a (t) ) is locally well-defined and converges to . <p> Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey <ref> [4] </ref>, and Scavo and Thoo [37].
Reference: [5] <author> G.A. BAKER AND P.R. GRAVES-MORRIS, </author> <title> Pade Approximants. Second Edition, </title> <journal> Encyclopedia of Mathematics and its Applications, </journal> <volume> Volume 59, </volume> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: For these formulas, see Ahlfors [2], and Baker and Graves-Morris <ref> [5] </ref> (Hermite's formula), respectively. These formulas can be used to give estimate of the error in approximation by the Taylor polynomial (see [2]). Although we shall only speak of the case of real-valued functions defined on an interval, our determinantal formulas hold in more generality. <p> For the vast theory of Pade approximants which usually 18 takes place over the complex plane, see Baker and Graves-Morris <ref> [5] </ref>. Upon solving the above equations we see that the corresponding Pade approximant, [1=1], is in fact H 2 (x; a). When viewed in the context of Pade approximants, it is interesting that the corresponding error, E 2 (x; a), is available. <p> However, we will need to prove four lemmas before the proof of Theorem 3 can begin. These lemmas consist of some determinantal identities. The first one is determinantal identity reminiscent of Sylvester's theorem (see <ref> [5] </ref>), but not equivalent to that theorem. Lemma 3. Let a = (a 1 ; a 2 ; 0; : : :; 0) T be a column vector in &lt; k , k 2. Let c and d, and e be arbitrary row vectors in &lt; k . <p> The family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris <ref> [5] </ref>, Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and
Reference: [6] <author> H. BATEMAN, </author> <title> Halley's method for solving equations, </title> <journal> Amer. Math. Monthly, </journal> <volume> 45 (1938), </volume> <pages> pp. 11-17. </pages>
Reference-contexts: But a simple inspection reveals that for f a polynomial, Halley's function when viewed as a rational iteration function, is the quotient of two polynomials of smaller degrees, than E 3 , viewed similarly. The method credited to Halley [21], has been derived through various means, see e.g. Bateman <ref> [6] </ref>, Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. <p> It was known that Halley's function can also be obtained by applying Newton's method to the function p= p p 0 , see Bateman <ref> [6] </ref>, as well as Brown [12], and Alefeld [1]. Gerlach [19] gives a generalization of this approach and for m = 2; 3; : : :, inductively defines an iteration function G m (x) of order m.
Reference: [7] <author> P. BECKMANN, </author> <title> History of . St. </title> <publisher> Martin's Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Using Q m;m1 as a sequence, we derived determinantal sequences converging to square roots, and the number . For , we could take f (x) = tan 1 (x), or other computationally more feasible trigonometric expansions used for approximation to great many digits (e.g. see Beckmann <ref> [7] </ref>). However, the proof of convergence of the corresponding determinantal sequence becomes a challenging and interesting problem in itself. The determinantal family, B (k) m , is a fundamental family of iteration functions.
Reference: [8] <author> G.E. BERGUM, </author> <title> A.N. PHILIPPOU, AND A.F. HORADAM, Applications of Fibonacci Numbers. Volumes 5. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: When m = k, P (t) is the characteristic polynomial of generalized Fibonacci of order m (see Miles [33] for generalized Fibonacci numbers). When m = 3, k = 2, the corresponding polynomial is the characteristic polynomial of Pell sequence (see Bergum et al. <ref> [8] </ref>). Newton, Halley and their multipoint versions correspond to B (k) B 3 , respectively. The family of one-point iteration functions derived in [29], the Basic Family, becomes a special case of the multipoint iteration functions of the present paper.
Reference: [9] <author> D. BINI AND V.Y. PAN, </author> <title> Polynomial and Matrix Computations, </title> <booktitle> Volume 1, </booktitle> <address> Brikhauser, Boston, MA, </address> <year> 1994. </year>
Reference-contexts: In general it takes O (m log 2 m) operation to compute the determinant of a Toeplitz matrix of dimension m. For results on sequential and parallel complexity regarding Toeplitz matrices see Golub and Van Loan [20], Chan and NG [13], and Bini and Pan <ref> [9] </ref>. In what follows we shall summarize some work related to the family of iteration function, B (k) Specific members such as Halley's method have been rediscovered many times. <p> In case P m1 is the classic one-point Taylor polynomial, the determinants in the corresponding (m) m1;m1 are Toeplitz determinants and can be computed in O (m log 2 m) or possibly even faster (see Golub and Van Loan [20], Chan and NG [13], and Bini and Pan <ref> [9] </ref>). Another family that may prove to be important is the (m) 40 family. Yet a third family can be considered, (m) 1;m1 , where for m 4, it is the quotient of two polynomials of degree m 1, and m 3, respectively.
Reference: [10] <author> C. BREZINSKI, </author> <title> History of Continued Fractions and Pade Approximants. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski <ref> [10] </ref> for general theory and history of continued fraction). Despite the fact that there are good practical and theoretical methods for finding all roots of polynomials, e.g. see the recent paper by Pan [35], there is no reason to believe that one cannot improve upon these algorithms.
Reference: [11] <author> E. BODEWIG, </author> <title> On types of convergence and on the behavior of approximations in the neighborhood of a multiple root of an equation, </title> <journal> Quart. App. Math., </journal> <volume> 7 (1949), </volume> <pages> pp. 325-333. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig <ref> [11] </ref>, Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28].
Reference: [12] <author> G.H. BROWN, </author> <title> On Halley's variation of Newton's method, </title> <journal> Amer. Math. Monthly, </journal> <note> 84 (1977), pp.726-728. </note>
Reference-contexts: It was known that Halley's function can also be obtained by applying Newton's method to the function p= p p 0 , see Bateman [6], as well as Brown <ref> [12] </ref>, and Alefeld [1]. Gerlach [19] gives a generalization of this approach and for m = 2; 3; : : :, inductively defines an iteration function G m (x) of order m.
Reference: [13] <author> R.H. CHAN AND M.K. NG, </author> <title> Conjugate gradient methods for Toeplitz systems, </title> <journal> SIAM Rev., </journal> <volume> 38 (1996), </volume> <pages> pp. 427-482. </pages>
Reference-contexts: In general it takes O (m log 2 m) operation to compute the determinant of a Toeplitz matrix of dimension m. For results on sequential and parallel complexity regarding Toeplitz matrices see Golub and Van Loan [20], Chan and NG <ref> [13] </ref>, and Bini and Pan [9]. In what follows we shall summarize some work related to the family of iteration function, B (k) Specific members such as Halley's method have been rediscovered many times. <p> In case P m1 is the classic one-point Taylor polynomial, the determinants in the corresponding (m) m1;m1 are Toeplitz determinants and can be computed in O (m log 2 m) or possibly even faster (see Golub and Van Loan [20], Chan and NG <ref> [13] </ref>, and Bini and Pan [9]). Another family that may prove to be important is the (m) 40 family. Yet a third family can be considered, (m) 1;m1 , where for m 4, it is the quotient of two polynomials of degree m 1, and m 3, respectively.
Reference: [14] <author> G. DAHLQUIST AND A. BJ ORCK, </author> <title> Numerical Methods, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year>
Reference-contexts: For an alternative description of the confluent divided differences see Traub [42], 4 and Ostroswski [34], two major books on iteration functions. The treatment of Taylor's theorem for the case of distinct nodes may be found in standard numerical analysis textbooks such as Hildebrand [25], Dahlquist and Bjorck <ref> [14] </ref>, and Atkinson [3]. Remark 1. It is important to note that in the case of distinct nodes, the identity f (x) = f (x 1 )+ P n+1 Q i l=1 (x x l ), holds over more general fields than the field of reals. <p> In order to evaluate P n (x) at a given x, one would proceed to form a table of confluent divided differences analogous to the case of distinct nodes, see e.g. [25], <ref> [14] </ref>, [3]. Once the divided differences are computed, the efficient evaluation of the polynomial is as follows: Let fi n = f 1;n+1 . For i = n 1; : : : ; 0, compute fi i = f 1;i+1 + fi i+1 (x x i+1 ).
Reference: [15] <author> M. DAVIES AND B. DAWSON, </author> <title> On the global convergence of Halley's iteration formula, </title> <journal> Numer. Math., </journal> <volume> 24 (1975), </volume> <pages> pp. 133-135. </pages>
Reference: [16] <author> W.F. FORD AND J.A. PENNLINE, </author> <title> Accelerated convergence in Newton's method, </title> <journal> SIAM Rev., </journal> <volume> 38 (1996), </volume> <pages> pp. 658-659. </pages>
Reference-contexts: Although for i = 2; 3; 4, G i coincides with B (1) no closed formula is given for m 3, and it is not even clear that Gerlach's approach should give a rational iteration function. Ford and Pennline <ref> [16] </ref> give a rational formulation of Gerlach's recursive approach in deriving iteration functions, but no closed formula is described.
Reference: [17] <author> J.S. </author> <title> FRAME, The solution of equations by continued fraction, </title> <journal> Amer. Math. Monthly, </journal> <volume> 60 (1953), </volume> <pages> pp. 293-305. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame <ref> [17] </ref>, Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37]. <p> In what follows we shall summarize some work related to the family of iteration function, B (k) Specific members such as Halley's method have been rediscovered many times. Frame <ref> [17] </ref>, de rived a family based on continued fractions whose second and third order agree with B (1) B 3 (Newton's and Halley's), but its forth order method does not coincide with B (1) 4 . <p> on the use of Pade approximants (see Baker and Graves-Morris [5], Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame <ref> [17] </ref> for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and history of continued fraction).
Reference: [18] <author> W. GANDER, </author> <title> On Halley's iteration method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 92 (1985), </volume> <pages> pp. 131-134. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander <ref> [18] </ref>. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37].
Reference: [19] <author> J. GERLACH, </author> <title> Accelerated convergence in Newton's method, </title> <journal> SIAM Rev., </journal> <volume> 36 (1994), </volume> <pages> pp. pp. 272-276. </pages>
Reference-contexts: It was known that Halley's function can also be obtained by applying Newton's method to the function p= p p 0 , see Bateman [6], as well as Brown [12], and Alefeld [1]. Gerlach <ref> [19] </ref> gives a generalization of this approach and for m = 2; 3; : : :, inductively defines an iteration function G m (x) of order m.
Reference: [20] <author> G. GOLUB AND C. VAN LOAN, </author> <title> Matrix Computations, </title> <publisher> 3rd ed., The John Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1996. </year> <month> 42 </month>
Reference-contexts: f (im+3) (a) 0 0 : : : f 0 (a) f (im+2) (a) fi fi fi fi fi fi fi ; (1:13) N (m2) (y; a) = D (m2) (y; a): (1:14) In particular, D (m1) and b D (m1) m are determinant corresponding to Toeplitz matrices (e.g. see <ref> [20] </ref>). <p> In general it takes O (m log 2 m) operation to compute the determinant of a Toeplitz matrix of dimension m. For results on sequential and parallel complexity regarding Toeplitz matrices see Golub and Van Loan <ref> [20] </ref>, Chan and NG [13], and Bini and Pan [9]. In what follows we shall summarize some work related to the family of iteration function, B (k) Specific members such as Halley's method have been rediscovered many times. <p> In case P m1 is the classic one-point Taylor polynomial, the determinants in the corresponding (m) m1;m1 are Toeplitz determinants and can be computed in O (m log 2 m) or possibly even faster (see Golub and Van Loan <ref> [20] </ref>, Chan and NG [13], and Bini and Pan [9]). Another family that may prove to be important is the (m) 40 family.
Reference: [21] <author> E. </author> <title> HALLEY, A new, exact, and easy method of finding roots of any equations generally, and that without any previous reduction (Latin, 1694). </title> <journal> Philos. Trans. Roy. Soc. London, </journal> <volume> 18, </volume> <pages> pp. 136-145. </pages>
Reference-contexts: But a simple inspection reveals that for f a polynomial, Halley's function when viewed as a rational iteration function, is the quotient of two polynomials of smaller degrees, than E 3 , viewed similarly. The method credited to Halley <ref> [21] </ref>, has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28].
Reference: [22] <author> H.J. HAMILTON, </author> <title> A type of variation of Newton's method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 57 (1950), </volume> <pages> pp. 517-522. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton <ref> [22] </ref>, Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28].
Reference: [23] <author> E. HANSEN AND M. PATRICK, </author> <title> A family of root finding methods, </title> <journal> Numer. Math., </journal> <volume> 27 (1977), </volume> <pages> pp. 257-269. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick <ref> [23] </ref>, Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37].
Reference: [24] <author> P. HENRICI, </author> <title> Applied and Computational Complex Analysis, Volume I, </title> <publisher> Wiley, </publisher> <address> New york, </address> <year> 1974. </year>
Reference-contexts: For the history of this family, as well as different schemes for its generates, see Traub [42], Henrici <ref> [24] </ref>, Householder [26], Shub and Smale [38]. In fact this family can also be generated from Taylor's theorem, via a simple recursive formula that was derived in Kalantari et al. [29]. Although the formula was described for f a polynomial of degree n, it is applicable to general smooth functions. <p> The family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici <ref> [24] </ref>, Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and history of
Reference: [25] <author> F.B. HILDEBRAND, </author> <title> Introduction to Numerical Analysis, 2nd, </title> <editor> ed., </editor> <publisher> McGraw-Hill, </publisher> <address> New york, </address> <year> 1974. </year>
Reference-contexts: On the other hand, the limiting case of Newton's interpolation formula for the case of distinct nodes (ascribed to Newton, see e.g. <ref> [25] </ref>), reduces to Taylor's Theorem. <p> Hildebrand <ref> [25] </ref>. Then, assuming again that the nodes are distinct, one proves f ij = f (ji) (~ ij )=(j i)!, where ~ ij lies in Span (x i ; : : : ; x j ). <p> For an alternative description of the confluent divided differences see Traub [42], 4 and Ostroswski [34], two major books on iteration functions. The treatment of Taylor's theorem for the case of distinct nodes may be found in standard numerical analysis textbooks such as Hildebrand <ref> [25] </ref>, Dahlquist and Bjorck [14], and Atkinson [3]. Remark 1. It is important to note that in the case of distinct nodes, the identity f (x) = f (x 1 )+ P n+1 Q i l=1 (x x l ), holds over more general fields than the field of reals. <p> In order to evaluate P n (x) at a given x, one would proceed to form a table of confluent divided differences analogous to the case of distinct nodes, see e.g. <ref> [25] </ref>, [14], [3]. Once the divided differences are computed, the efficient evaluation of the polynomial is as follows: Let fi n = f 1;n+1 . For i = n 1; : : : ; 0, compute fi i = f 1;i+1 + fi i+1 (x x i+1 ).
Reference: [26] <author> A.S. </author> <title> HOUSEHOLDER, The Numerical Treatment of a Single Nonlinear Equation., </title> <publisher> McGraw-Hill, </publisher> <address> New york, </address> <year> 1970. </year>
Reference-contexts: For the history of this family, as well as different schemes for its generates, see Traub [42], Henrici [24], Householder <ref> [26] </ref>, Shub and Smale [38]. In fact this family can also be generated from Taylor's theorem, via a simple recursive formula that was derived in Kalantari et al. [29]. Although the formula was described for f a polynomial of degree n, it is applicable to general smooth functions. <p> The family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder <ref> [26] </ref>, Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and history of continued fraction).
Reference: [27] <author> W.B. JONES AND W.J. THRON, </author> <title> Continued Fractions. </title> <journal> Encyclopedia of Mathematics and its Applications, </journal> <volume> Volume 11, </volume> <publisher> Addison Wesley, </publisher> <address> Reading, M.A., </address> <year> 1980. </year>
Reference-contexts: [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron <ref> [27] </ref> and Brezinski [10] for general theory and history of continued fraction). Despite the fact that there are good practical and theoretical methods for finding all roots of polynomials, e.g. see the recent paper by Pan [35], there is no reason to believe that one cannot improve upon these algorithms.
Reference: [28] <author> B. KALANTARI AND I. KALANTARI, </author> <title> High order iterative methods for approximating square roots, </title> <journal> BIT, </journal> <volume> 36 (1996), </volume> <pages> pp. 395-399. </pages>
Reference-contexts: Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in <ref> [28] </ref>. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37]. <p> Using continued fractions, Yeyios [45] derived a family of iteration functions for the approximation of square roots. In <ref> [28] </ref>, using a very simple algebraic scheme, a family of iteration functions were derived for square roots (as well as cube roots) which coincides with that of Yeyios, and by virtue of a uniqueness theorem proved in [29], must also coincide with B (1) m , corresponding to f (x) =
Reference: [29] <author> B. KALANTARI, I. KALANTARI, AND R. ZAARE-NAHANDI, </author> <title> A basic family of iteration functions for polynomial root finding and its characterizations, </title> <journal> J. of Comp. and Appl. Math., </journal> <volume> 80 (1997), </volume> <pages> pp. 209-226. </pages>
Reference-contexts: For the history of this family, as well as different schemes for its generates, see Traub [42], Henrici [24], Householder [26], Shub and Smale [38]. In fact this family can also be generated from Taylor's theorem, via a simple recursive formula that was derived in Kalantari et al. <ref> [29] </ref>. Although the formula was described for f a polynomial of degree n, it is applicable to general smooth functions. We describe this simple formula next. <p> We describe this simple formula next. For its validity, its generality, how it may be used to generate many other sequences of iteration functions, and its usage in deriving the asymptotic error constant of the Euler-Schroder family, see <ref> [29] </ref>. Suppose for m 2, we are given E m (x 1 ), having the following expansion, E m (x 1 ) = + i=m (m) where (m) i is a rational function in x 1 . <p> has cubic rate of convergence is the famous Halley's iteration function, H (x 1 ) = x 1 f (x 1 ) 2f 0 (x 1 ) 2 f 00 (x 1 )f (x 1 ) Halley's function is the second member of a family of iteration functions derived in <ref> [29] </ref>, called the "Basic Family", fB m g 1 m=2 , where B 2 is again Newton's and B 3 is Halley's. The Basic Family is a fundamental family of one-point iteration functions having many optimal properties with respect to several criteria. <p> Traub [42] has shown that any m-th order one-point iteration function for root-finding must depend on the first (m 1) derivatives explicitly. With respect to this result, both the Basic Family and the Euler-Schroder family are optimal. However, there are other criteria described in <ref> [29] </ref> (see also the beginning of Section 6) with respect to which the Basic Family is a more preferred family than the Euler-Schroder family. On the surface it is difficult to decide how E 3 compares with Halley's iteration function. <p> When m = 3, k = 2, the corresponding polynomial is the characteristic polynomial of Pell sequence (see Bergum et al. [8]). Newton, Halley and their multipoint versions correspond to B (k) B 3 , respectively. The family of one-point iteration functions derived in <ref> [29] </ref>, the Basic Family, becomes a special case of the multipoint iteration functions of the present paper. The deriva tion of the Basic Family, B (1) m , presented in [29] was based on a direct algebraic approach. <p> The family of one-point iteration functions derived in <ref> [29] </ref>, the Basic Family, becomes a special case of the multipoint iteration functions of the present paper. The deriva tion of the Basic Family, B (1) m , presented in [29] was based on a direct algebraic approach. <p> For polynomials, the one-point Basic Family B (1) m was derived algebraically in <ref> [29] </ref>, but without the explicit formula for fl (m) i . From the point of view of iteration functions it is enough to obtain a closed formula for fl (m) m (), the asymptotic error constant. <p> In [28], using a very simple algebraic scheme, a family of iteration functions were derived for square roots (as well as cube roots) which coincides with that of Yeyios, and by virtue of a uniqueness theorem proved in <ref> [29] </ref>, must also coincide with B (1) m , corresponding to f (x) = x 2 (f (x) = x 3 ). <p> Traub's motivation is to find high order iterative methods, which for large enough value of the parameter, converge globally to the dominant root of polynomials, assuming its uniqueness. The independent derivation of this family, called the Basic Family, in Kalantari et al. <ref> [29] </ref> is based on an algebraic approach that first establishes its existence, then uses this to derive a determinantal formula for the iteration functions, as well as a precise determinantal formula for its asymptotic error constant. Also, in [29] it is shown that the iteration functions as well as the asymptotic <p> derivation of this family, called the Basic Family, in Kalantari et al. <ref> [29] </ref> is based on an algebraic approach that first establishes its existence, then uses this to derive a determinantal formula for the iteration functions, as well as a precise determinantal formula for its asymptotic error constant. Also, in [29] it is shown that the iteration functions as well as the asymptotic error constants are valid for more general functions than polynomials. The algebraic approach of [29] reveals certain minimal properties on the general member of the Basic Family. <p> Also, in <ref> [29] </ref> it is shown that the iteration functions as well as the asymptotic error constants are valid for more general functions than polynomials. The algebraic approach of [29] reveals certain minimal properties on the general member of the Basic Family. Theorem 3 describes a more general development of the Basic Family, and its multipoint version. <p> Theorem 3 describes a more general development of the Basic Family, and its multipoint version. Even for k = 1, the expansion formula for B (k) m in Theorem 3 becomes a more precise formula than the one given in <ref> [29] </ref>. In particular, the formula in Theorem 3 allows the definition of a "corrected" family of k-point iteration functions, b B (k) (k) (m) Q m (k) In particular, one can define a corrected Halley method with supercubic rate of convergence. 5. <p> We then prove Theorem 3 by deriving the determi-nantal formula corresponding to B (k) m , via a recursive formula. Although the recursive formula is simple, its validity, and the derivation of the determinantal formula is tedious. In particular it requires the validity of some determinantal identities. In <ref> [29] </ref> it was shown that given a polynomial p (x) of degree n, there exists a unique (one-point) iteration function that can be written as + P m+n2 (m) i (x)( x) i , with each fl (m) i (x) a rational function, well-defined at all simple roots of p (x). <p> Without explicitly deriving fl (m) i , in <ref> [29] </ref> the determinantal formula for this unique function, B (1) m , was derived algebraically. By virtue of uniqueness, in [29] it was argued that a one-point version of the recursive formula must necessarily produce the Basic Family, B (1) m , m 2. <p> Without explicitly deriving fl (m) i , in <ref> [29] </ref> the determinantal formula for this unique function, B (1) m , was derived algebraically. By virtue of uniqueness, in [29] it was argued that a one-point version of the recursive formula must necessarily produce the Basic Family, B (1) m , m 2. Ironically, while the algebraic approach used in [29] does not appear to be generalizable to the case of arbitrary differentiable function, and the more general case of <p> By virtue of uniqueness, in <ref> [29] </ref> it was argued that a one-point version of the recursive formula must necessarily produce the Basic Family, B (1) m , m 2. Ironically, while the algebraic approach used in [29] does not appear to be generalizable to the case of arbitrary differentiable function, and the more general case of interpolation considered in the present paper, the recursive approach is tediously applicable, requiring several technical lemmas. We first need to define some matrices. <p> From this and continuity, there exists a neighborhood I of such that for any admissible a with coordinates contained in I , the corresponding D (m1) m1 is nonzero. Assuming that f (x) is a polynomial, it is shown in <ref> [29] </ref> ( Theorem 4.3) that the corresponding one-point fl (m) m is not identically zero. Since fl (m) m is not identically zero for polynomials, it follows that it is not identically zero for the general case of functions considered here. <p> m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. <ref> [29] </ref> for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and history of continued fraction).
Reference: [30] <author> B. KALANTARI, </author> <title> On the order of convergence of a determinantal family of root-finding methods. </title> <type> Technical Report DCS-TR 329, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ., </address> <year> 1997. </year>
Reference-contexts: However, for &gt; 1, one needs to prove new results. In Section 4 we state a result (Theorem 5) that generalizes Traub's theorem. The proof which requires results from the theory of difference equations and the analysis of roots of characteristic polynomials, appears as a separate paper in <ref> [30] </ref>. 1.1. Preliminary Definitions and Summary of Results. In this subsection we describe the determinantal formulas and their applications. The description of these formulas require the definition of the matrix of divided differences and some of its submatrices. The reader needs to 9 become familiarized with these submatrices. <p> The following theorem covers the case of general k. Theorem 5. (Kalantari <ref> [30] </ref>) The sequence of fixed-point iterates fx (t) t=0 satisfy lim ( x 1 ) (t) = c m () ( p1 where p is the unique positive root of P (z) = z k (m k + 1)z k1 P k2 j=0 z j .
Reference: [31] <author> B. KALANTARI, </author> <title> A lower bound on determinants from linear programming. </title> <type> Technical Report DCS-TR 330, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ., </address> <year> 1997. </year>
Reference-contexts: (k) m presented in this paper, is based on a simple recursive 13 formula that utilizes Theorem 1, but requires the derivation of many determinantal identities. (III): Determinantal approximation of numbers: In particular, we will use the determinantal formulas, Hadamard inequality, and a new lower bound on determinants derived in <ref> [31] </ref>, in order to describe special numbers such as roots of numbers, and the numbers and e, as the limiting ratio of the consecutive terms of a sequence of Toeplitz matrix determinants of increasing dimensions. In Section 2, we describe the determinantal formulas through two main theorems. <p> Using the determinantal formulas one can derive other convergent sequences, but this will not be considered in the present paper. In establishing the convergence results we utilize the following theorem, a theorem which is of independent interest and is proved separately. Theorem 6. (Kalantari <ref> [31] </ref> Let A be an n fi n real or complex matrix. Assume we are given positive numbers l and u such that if is an eigenvalue of A, then l jj u, where jj is the modulus of .
Reference: [32] <author> M. MARCUS AND H. </author> <title> MINC, A Survey of Matrix Theory and Matrix Inequalities. </title> <publisher> Allyn and Bacon, </publisher> <address> Boston, Mass., </address> <year> 1964. </year>
Reference-contexts: First observe that each column of the matrix corresponding to b ffi (m1) m (1=2; b ~; 0) is a vector whose Euclidean norm is bounded above by 1 1 1 1=2 1 1 1 1=2 p Given a matrix A, Hadamard's inequality gives (see Marcus and Minc <ref> [32] </ref>) jdet (A)j n Y n X ja ij j 2 : Hence from Hadamard's inequality we have j b D (m1) 1 ; 0)j (1:5) 2 : To find a lower bound on D (m1) (1=2; 0), we apply Theorem 6 with n = m 1, trace = m 1.
Reference: [33] <author> E.P. MILES, </author> <title> Generalized Fibonacci numbers and associated matrices, </title> <journal> Amer. Math. Monthly, </journal> <volume> 67 (1960), </volume> <pages> pp. 745-757. </pages>
Reference-contexts: When m = k = 2, P (t) is the characteristic polyno mial of the ordinary Fibonacci sequence. When m = k, P (t) is the characteristic polynomial of generalized Fibonacci of order m (see Miles <ref> [33] </ref> for generalized Fibonacci numbers). When m = 3, k = 2, the corresponding polynomial is the characteristic polynomial of Pell sequence (see Bergum et al. [8]). Newton, Halley and their multipoint versions correspond to B (k) B 3 , respectively. <p> For the special case where k = m, one can make use of a theorem of Traub [42] (Theorem 3-3, page 56) to conclude that the order of convergence is the positive root of the characteristic polynomial of generalized Fibonacci numbers of order m (see <ref> [33] </ref>). The following theorem covers the case of general k.
Reference: [34] <author> A.M. OSTROWSKI, </author> <title> Solution of Equations and System of Equations, 2nd ed., </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: Once the proof of the first part of Theorem 1 is established, the inverse form of the theorem is obtained from a straightforward manipulation. For an alternative description of the confluent divided differences see Traub [42], 4 and Ostroswski <ref> [34] </ref>, two major books on iteration functions. The treatment of Taylor's theorem for the case of distinct nodes may be found in standard numerical analysis textbooks such as Hildebrand [25], Dahlquist and Bjorck [14], and Atkinson [3]. Remark 1. <p> The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski <ref> [34] </ref>, Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37]. <p> The conversion of sub-order of convergence into a statement on the order of convergence, is a nontrivial task. In fact even in the case of secant method, the order of convergence is seldom derived rigorously in the numerical analysis textbooks. Ostrowski <ref> [34] </ref> gives a complete proof for the secant method (page 95). However, his proof makes use of the specific way in which the iterates are defined, and the mean-value theorem.
Reference: [35] <author> V.Y. PAN, </author> <title> Solving a polynomial equation: some history and recent progress, </title> <journal> SIAM Rev., </journal> <volume> 39 (1977), </volume> <pages> pp. 187-220. </pages>
Reference-contexts: Despite the fact that there are good practical and theoretical methods for finding all roots of polynomials, e.g. see the recent paper by Pan <ref> [35] </ref>, there is no reason to believe that one cannot improve upon these algorithms. In particular, it is natural to expect practical applications of 41 the family B (k) m , even in polynomial root-finding.
Reference: [36] <author> D.B. POPOVSKI, </author> <title> A family of one-point iteration formulae for finding roots, </title> <journal> Internat. J. Computer Math., </journal> <volume> 8 (1980), </volume> <pages> 85-88. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski <ref> [36] </ref>, Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37].
Reference: [37] <author> T.R. SCAVO AND J.B. THOO, </author> <title> On the geometry of Halley's method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 102 (1995), </volume> <pages> pp. 417-426. </pages>
Reference-contexts: Ironically, it was Halley's work on root finding that inspired Taylor to state the famous "Taylor's Theorem" (see [46], [4], <ref> [37] </ref>) which in turn gives Newton's iteration function, its rate of convergence, and its asymptotic error constant. On the other hand, the limiting case of Newton's interpolation formula for the case of distinct nodes (ascribed to Newton, see e.g. [25]), reduces to Taylor's Theorem. <p> Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo <ref> [37] </ref>.
Reference: [38] <author> M. SHUB AND S. SMALE, </author> <title> On the geometry of polynomials and a theory of cost: Part I, Ann. Scient. Ec. Norm. Sup., </title> <booktitle> 18 (1985), </booktitle> <pages> pp. 107-142. </pages>
Reference-contexts: For the history of this family, as well as different schemes for its generates, see Traub [42], Henrici [24], Householder [26], Shub and Smale <ref> [38] </ref>. In fact this family can also be generated from Taylor's theorem, via a simple recursive formula that was derived in Kalantari et al. [29]. Although the formula was described for f a polynomial of degree n, it is applicable to general smooth functions. We describe this simple formula next. <p> family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub [42]); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale <ref> [38] </ref>, also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and history of continued fraction).
Reference: [39] <author> S. SMALE, </author> <title> Newton's method estimates from data at one point, in The merging of Disciplines: New Directions in Pure, Applied, and Computational Mathematics, R.E. </title> <editor> Ewing, K.I. Gross, C.F. Martin, </editor> <booktitle> eds., </booktitle> <pages> pp. 185-196, </pages> <year> 1986. </year>
Reference-contexts: In particular, it is natural to expect practical applications of 41 the family B (k) m , even in polynomial root-finding. An interesting problem regarding the B (k) family is to investigate regions of fast convergence for polynomial root-finding, analogous to the case of Newton's method, derived by Smale <ref> [39] </ref>. Many interesting theoretical and practical problems regarding the determinantal formulas remain to be further researched. Hopefully, the present results offer sufficient evidence to the importance of the new formulas.
Reference: [40] <author> R.W. SNYDER, </author> <title> One more correction formula, </title> <journal> Amer. Math. Monthly, </journal> <volume> 62 (1955), </volume> <pages> pp. 722-725. </pages>
Reference: [41] <author> J.K. STEWART, </author> <title> Another variation of Newton's method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 58 (1951), </volume> <pages> pp. 331-334. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart <ref> [41] </ref>, Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37].
Reference: [42] <author> J.F. TRAUB, </author> <title> Iterative Methods for the Solution of Equations, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1964. </year>
Reference-contexts: This justifies the definition of divided differences under complete or partial confluence. Once the proof of the first part of Theorem 1 is established, the inverse form of the theorem is obtained from a straightforward manipulation. For an alternative description of the confluent divided differences see Traub <ref> [42] </ref>, 4 and Ostroswski [34], two major books on iteration functions. The treatment of Taylor's theorem for the case of distinct nodes may be found in standard numerical analysis textbooks such as Hildebrand [25], Dahlquist and Bjorck [14], and Atkinson [3]. Remark 1. <p> 2 (x 1 ) = 1 (a) 2f 0 (x 1 ) 2f 0 (x 1 ) 2 + f 00 (x 1 )f (x 1 ) This function apparently was obtained by Euler, Schroder, and Chebyshev (for which he received a silver medal in a student contest), see Traub <ref> [42] </ref>, and Bailey [4]. If f 0 () 6= 0, and a (0) is properly chosen, the fixed-point iteration corresponding to 2 (a (t) ) is locally well-defined and converges to . <p> It is the second member of the family of Euler-Schroder iteration functions, fE m g 1 m=2 , where E m has order of convergence equal to m, and E 2 is Newton's. For the history of this family, as well as different schemes for its generates, see Traub <ref> [42] </ref>, Henrici [24], Householder [26], Shub and Smale [38]. In fact this family can also be generated from Taylor's theorem, via a simple recursive formula that was derived in Kalantari et al. [29]. <p> The Basic Family is a fundamental family of one-point iteration functions having many optimal properties with respect to several criteria. Traub <ref> [42] </ref> has shown that any m-th order one-point iteration function for root-finding must depend on the first (m 1) derivatives explicitly. With respect to this result, both the Basic Family and the Euler-Schroder family are optimal. <p> The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub <ref> [42] </ref>, Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37]. <p> Bateman [6], Wall [44], Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub <ref> [42] </ref>, Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma [46], Bailey [4], and Scavo and Thoo [37]. <p> Ostrowski [34] gives a complete proof for the secant method (page 95). However, his proof makes use of the specific way in which the iterates are defined, and the mean-value theorem. A theorem of Traub <ref> [42] </ref> (Theorem 3-3, page 56) substantially simplifies the analysis of the rate of convergence of the secant method, and more generally multipoint iteration methods based on hyperosculatory interpolating polynomials. In case = 1, we can apply Traub's theorem to derive the order of convergence. <p> One needs to convert the sub-order (see Definition 3) into an statement on order of convergence. For the special case where k = m, one can make use of a theorem of Traub <ref> [42] </ref> (Theorem 3-3, page 56) to conclude that the order of convergence is the positive root of the characteristic polynomial of generalized Fibonacci numbers of order m (see [33]). The following theorem covers the case of general k. <p> The family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub <ref> [42] </ref>); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] <p> The family of iteration function B m has many ideal features that make it a more fundamental family of iteration functions than those based on the use of Pade approximants (see Baker and Graves-Morris [5], Traub <ref> [42] </ref>); or the Euler-Schroder family (see Traub [42], Henrici [24], Householder [26], Shub and Smale [38], also Kalantari et al. [29] for a simple recursive formula that generates this family); or those based on continued fractions (see Frame [17] for a special application of continued fractions, and Jones and Thron [27] and Brezinski [10] for general theory and
Reference: [43] <author> J.F. </author> <title> TRAUB (1966). A class of globally convergent iteration functions for the solution of polynomial equations, Mathematics of Computations, </title> <booktitle> 20 (1966), </booktitle> <pages> pp. 113-138. </pages>
Reference-contexts: Ford and Pennline [16] give a rational formulation of Gerlach's recursive approach in deriving iteration functions, but no closed formula is described. Indeed 26 the one-point family, fB (1) m=2 , was derived by Traub <ref> [43] </ref> (page 130) as a special case of parameterized family of iteration function for polynomials, with the parameter value equal zero. Traub's motivation is to find high order iterative methods, which for large enough value of the parameter, converge globally to the dominant root of polynomials, assuming its uniqueness.
Reference: [44] <author> H.S. WALL, </author> <title> A modification of Newton's method, </title> <journal> Amer. Math. Monthly, </journal> <volume> 55 (1948), </volume> <pages> pp. 90-94. </pages>
Reference-contexts: The method credited to Halley [21], has been derived through various means, see e.g. Bateman [6], Wall <ref> [44] </ref>, Bodewig [11], Hamilton [22], Stewart [41], Frame [17], Traub [42], Ostrowski [34], Hansen and Patrick [23], Popovski [36], Gander [18]. Its derivation and parallel complexity for the approximation of square roots is discussed in [28].
Reference: [45] <author> A.K. YEYIOS, </author> <title> On two sequences of algorithms for approximating square roots, </title> <journal> J. of Comp. and Appl. Math., </journal> <volume> 40 (1992), </volume> <pages> pp. 63-72. </pages>
Reference-contexts: Frame [17], de rived a family based on continued fractions whose second and third order agree with B (1) B 3 (Newton's and Halley's), but its forth order method does not coincide with B (1) 4 . Using continued fractions, Yeyios <ref> [45] </ref> derived a family of iteration functions for the approximation of square roots.
Reference: [46] <author> T.J. YPMA, </author> <title> Historical development of Newton-Raphson method, </title> <journal> SIAM Rev., </journal> <volume> 37 (1995), </volume> <pages> pp. 531-551. 43 </pages>
Reference-contexts: paper we describe a family of "determinantal interpolation formulas" which includes Taylor's formula, and gives rise to new schemes for approximation of functions, or their inverses. fl From the historical point of view, what is so widely known as "Newton's method", should also be credited to Raphson, and Simpson (see <ref> [46] </ref>). Ironically, it was Halley's work on root finding that inspired Taylor to state the famous "Taylor's Theorem" (see [46], [4], [37]) which in turn gives Newton's iteration function, its rate of convergence, and its asymptotic error constant. <p> for approximation of functions, or their inverses. fl From the historical point of view, what is so widely known as "Newton's method", should also be credited to Raphson, and Simpson (see <ref> [46] </ref>). Ironically, it was Halley's work on root finding that inspired Taylor to state the famous "Taylor's Theorem" (see [46], [4], [37]) which in turn gives Newton's iteration function, its rate of convergence, and its asymptotic error constant. On the other hand, the limiting case of Newton's interpolation formula for the case of distinct nodes (ascribed to Newton, see e.g. [25]), reduces to Taylor's Theorem. <p> Its derivation and parallel complexity for the approximation of square roots is discussed in [28]. For the interesting history of Halley's function see Traub [42], Ypma <ref> [46] </ref>, Bailey [4], and Scavo and Thoo [37].
References-found: 46

