URL: http://www.cs.umn.edu/Research/arpa/p_sparslib/psp/DOCS/Workstation_cluster.ps
Refering-URL: http://www.cs.umn.edu/Research/arpa/p_sparslib/psp-abs.html
Root-URL: http://www.cs.umn.edu
Title: Iterative Solution of General Sparse Linear Systems on Clusters of Workstations  
Author: Gen-Ching Lo and Yousef Saad 
Date: August 20, 1996  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science, and Minnesota Supercomputer Institute University of Minnesota  
Abstract: Solving sparse irregularly structured linear systems on parallel platforms poses several challenges. First, sparsity makes it difficult to exploit data locality and this is true for both distributed and shared memory environments. Second, it is difficult to find efficient ways to precondition the system. For example, preconditioning techniques that have a high degree of parallelism often lead to slower convergence than their sequential counterparts. Finally, a number of other `global' computational kernels such as inner products can outweigh any gains due to parallelism, and this is especially true on workstation clusters where latency times may be high. In this paper we discuss these issues and report on our experience with PSPARSLIB, an on-going project for building a library of parallel iterative sparse matrix solvers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. E. </author> <title> Bjorstad. Multiplicative and Additive Schwarz Methods: Convergence in the 2 domain case. </title> <editor> In Tony Chan, Roland Glowinski, Jacques Periaux, and O. Widlund, editors, </editor> <title> Domain Decomposition Methods, </title> <address> Philadelphia, PA, </address> <year> 1989. </year> <note> SIAM. </note>
Reference-contexts: This is illustrated in the numerical experiments section. Of particular interest in this context are the overlapping Jacobi methods. In the domain decomposition literature <ref> [1, 3, 6, 11] </ref> it is known that overlapping is a good strategy to reduce the number of steps. There are however several different ways of implementing overlapping block Jacobi iterations. The illustration of Figure 4 will help understand the options. For simplicity, only three subdomains are shown.
Reference: [2] <author> P. E. Bjorstad and O. B. Widlund. </author> <title> Iterative methods for the solution of elliptic problems on regions partitioned into substructures. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 23(6) </volume> <pages> 1093-1120, </pages> <year> 1986. </year> <title> Sparse Iterative Solvers on Workstations 21 </title>
Reference-contexts: EndIf Algorithm 4.2 is executed on each processor and a convergence test on the global residual or some measure of the error must be included. This block Gauss-Seidel algorithm is the simplest form of the Multiplicative Schwarz procedures used in domain decomposition techniques <ref> [2, 9, 10, 7] </ref>. Many variations are possible, including overlapping of the domains, inaccurate solves in step 5, inclusion of a relaxation parameter !, etc. Normally, after a step is done with an active color, the processors of this active color need only send data to the (inactive) neighboring processors.
Reference: [3] <author> P. E. Bjorstad and O. B. Widlund. </author> <title> To overlap or not to overlap: A note on a domain decomposition method for elliptic problems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(5) </volume> <pages> 1053-1061, </pages> <year> 1989. </year>
Reference-contexts: This is illustrated in the numerical experiments section. Of particular interest in this context are the overlapping Jacobi methods. In the domain decomposition literature <ref> [1, 3, 6, 11] </ref> it is known that overlapping is a good strategy to reduce the number of steps. There are however several different ways of implementing overlapping block Jacobi iterations. The illustration of Figure 4 will help understand the options. For simplicity, only three subdomains are shown.
Reference: [4] <author> X. C. Cai and Y. Saad. </author> <title> Overlapping domain decomposition algorithms for general sparse matrices. Numerical Linear Algebra with Applications, </title> <note> 1996. To appear. </note>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [6, 5, 4, 13, 19] </ref>. Thus, if the domains are colored and the global ordering of the domains is the ordering defined by the colors, the Gauss-Seidel iteration as executed in each processor would be as follows: Algorithm 4.2 Multicolor Block Gauss-Seidel Iteration 1.
Reference: [5] <author> X. C. Cai and O. Widlund. </author> <title> Multiplicative Schwarz algorithms for some nonsymmetric and indefinite problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 30(4), </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [6, 5, 4, 13, 19] </ref>. Thus, if the domains are colored and the global ordering of the domains is the ordering defined by the colors, the Gauss-Seidel iteration as executed in each processor would be as follows: Algorithm 4.2 Multicolor Block Gauss-Seidel Iteration 1.
Reference: [6] <author> Xiao-Chuan Cai, William D. Gropp, and David E. Keyes. </author> <title> A comparison of some domain decomposition and ILU preconditioned iterative methods for nonsymmetric elliptic problems. </title> <journal> J. Numer. Lin. Alg. Appl., </journal> <month> June </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: This is illustrated in the numerical experiments section. Of particular interest in this context are the overlapping Jacobi methods. In the domain decomposition literature <ref> [1, 3, 6, 11] </ref> it is known that overlapping is a good strategy to reduce the number of steps. There are however several different ways of implementing overlapping block Jacobi iterations. The illustration of Figure 4 will help understand the options. For simplicity, only three subdomains are shown. <p> The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [6, 5, 4, 13, 19] </ref>. Thus, if the domains are colored and the global ordering of the domains is the ordering defined by the colors, the Gauss-Seidel iteration as executed in each processor would be as follows: Algorithm 4.2 Multicolor Block Gauss-Seidel Iteration 1.
Reference: [7] <author> T. F. Chan and T. P. Mathew. </author> <title> Domain decomposition algorithms. </title> <journal> Acta Numerica, </journal> <pages> pages 61-143, </pages> <year> 1994. </year>
Reference-contexts: EndIf Algorithm 4.2 is executed on each processor and a convergence test on the global residual or some measure of the error must be included. This block Gauss-Seidel algorithm is the simplest form of the Multiplicative Schwarz procedures used in domain decomposition techniques <ref> [2, 9, 10, 7] </ref>. Many variations are possible, including overlapping of the domains, inaccurate solves in step 5, inclusion of a relaxation parameter !, etc. Normally, after a step is done with an active color, the processors of this active color need only send data to the (inactive) neighboring processors.
Reference: [8] <author> J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. </author> <title> Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization. </title> <journal> Math. Comput, </journal> <volume> 30 </volume> <pages> 772-795, </pages> <year> 1976. </year>
Reference-contexts: However, if a large Krylov subspace dimension must be used, the compromise offered by the classical Gram-Schmidt with reorthogonalization may be safer. Reorthogonalization may be performed only when needed, as determined by a test suggested by Daniel et al. <ref> [8] </ref>. 6.3 Different cluster configurations When a fixed number of processors (e.g. 16) is available, there are many different ways to configure a cluster. We can put all 16 processors into one box or have 8 in each of 2 boxes or 4 in each of 4 boxes.
Reference: [9] <author> Maksymilian Dryja and Olof B. Widlund. </author> <title> Towards a unified theory of domain decomposition algorithms for elliptic problems. </title> <editor> In Tony Chan, Roland Glowinski, Jacques Periaux, and Olof Widlund, editors, </editor> <title> Third International Symposium on Domain Decomposition Methods for Partial Differential Equations, held in Houston, </title> <address> Texas, March 20-22, 1989. </address> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: EndIf Algorithm 4.2 is executed on each processor and a convergence test on the global residual or some measure of the error must be included. This block Gauss-Seidel algorithm is the simplest form of the Multiplicative Schwarz procedures used in domain decomposition techniques <ref> [2, 9, 10, 7] </ref>. Many variations are possible, including overlapping of the domains, inaccurate solves in step 5, inclusion of a relaxation parameter !, etc. Normally, after a step is done with an active color, the processors of this active color need only send data to the (inactive) neighboring processors.
Reference: [10] <author> Maksymilian Dryja and Olof B. Widlund. </author> <title> Some recent results on Schwarz type domain decomposition algorithms. In Alfio Quarteroni, editor, Sixth Conference on Domain Decomposition Methods for Partial Differential Equations. </title> <publisher> AMS, </publisher> <year> 1993. </year> <title> Held in Como, </title> <address> Italy, June 15-19,1992. </address> <note> To appear. Technical report 615, </note> <institution> Department of Computer Science, Courant Institute. </institution>
Reference-contexts: EndIf Algorithm 4.2 is executed on each processor and a convergence test on the global residual or some measure of the error must be included. This block Gauss-Seidel algorithm is the simplest form of the Multiplicative Schwarz procedures used in domain decomposition techniques <ref> [2, 9, 10, 7] </ref>. Many variations are possible, including overlapping of the domains, inaccurate solves in step 5, inclusion of a relaxation parameter !, etc. Normally, after a step is done with an active color, the processors of this active color need only send data to the (inactive) neighboring processors.
Reference: [11] <author> William D. Gropp and Barry F. Smith. </author> <title> Experiences with domain decomposition in three dimensions: Overlapping Schwarz methods. </title> <type> Technical report, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year> <booktitle> To appear in the Proceedings of the Sixth International Symposium on Domain Decomposition Methods. </booktitle>
Reference-contexts: This is illustrated in the numerical experiments section. Of particular interest in this context are the overlapping Jacobi methods. In the domain decomposition literature <ref> [1, 3, 6, 11] </ref> it is known that overlapping is a good strategy to reduce the number of steps. There are however several different ways of implementing overlapping block Jacobi iterations. The illustration of Figure 4 will help understand the options. For simplicity, only three subdomains are shown.
Reference: [12] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: This is discussed in detail in Section 3.3. 3.1 FGMRES The main Krylov accelerator used in this paper is the flexible variant of GMRES [18] known as FGMRES <ref> [12] </ref>. This is a right-preconditioned variant that allows the preconditioning to vary at each step. Since the preconditioning operations require solving systems associated with entire subdomains it becomes important to allow the preconditioner itself to be an iterative solver. <p> This means that the GMRES iteration should allow the preconditioner to vary from step to step within the inner GMRES process. One variant of GMRES which allows this is called the flexible variant of GMRES (FGMRES) <ref> [12] </ref>. <p> If the preconditioner varies at every step, then we need to save the `preconditioned' vectors z j = M 1 j v j to use them when computing the approximate solution. For further details on the algorithm, see <ref> [12] </ref>. 3.2 Reverse Communication An additional feature of our implementation of FGMRES is that we use "reverse communication", a mechanism whose goal is to avoid passing data structures to the accelerator.
Reference: [13] <author> Y. Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <editor> In G. Golub, M. Luskin, and A. Greenbaum, editors, </editor> <title> Recent Advances in Iterative Methods, </title> <journal> IMA Volumes in Mathematics and Its Applications, </journal> <volume> volume 60, </volume> <pages> pages 165-199, </pages> <address> New York, 1994. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [6, 5, 4, 13, 19] </ref>. Thus, if the domains are colored and the global ordering of the domains is the ordering defined by the colors, the Gauss-Seidel iteration as executed in each processor would be as follows: Algorithm 4.2 Multicolor Block Gauss-Seidel Iteration 1.
Reference: [14] <author> Y. Saad. ILUT: </author> <title> a dual threshold incomplete ILU factorization. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 387-402, </pages> <year> 1994. </year>
Reference-contexts: To solve the systems which arise in line 3 of the above algorithm, a standard (sequential) ILUT preconditioner <ref> [14] </ref> combined with GMRES for the solves associated with the blocks is used. A factor which can affect convergence is the tolerance used for the inner solve. As accuracy increases the number of outer steps may decrease.
Reference: [15] <author> Y. Saad. ILUM: </author> <title> a parallel multi-elimination ILU preconditioner for general sparse matrices. </title> <journal> SIAM Journal on Scientific Computing, </journal> <year> 1996. </year> <title> To appear. Sparse Iterative Solvers on Workstations 22 </title>
Reference-contexts: Within each subdomain an effective iterative solver can be used. However, we found that using one step of an ILU solve with an accurate ILUT factorization is usually less expensive. Other preconditioning options exist, see e.g. <ref> [15] </ref>, which have not been tested here. As the number of processors increases these alternatives may become preferable.
Reference: [16] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: A global system involving these interface unknowns can be easily obtained by eliminating internal variables from the local equations, see <ref> [16] </ref>. Here we will focus on a general strategy for deriving Schur complement techniques associated with arbitrary global fixed point iterations. Consider the simplest case of a block-Jacobi iteration described earlier. <p> As is known, with a consistent choice of the initial guess, a block-Jacobi (or Gauss-Seidel) iteration with the reduced system is equivalent with a block Jacobi iteration on the global system, see, e.g., <ref> [16] </ref>.
Reference: [17] <author> Y. Saad and A. Malevsky. PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers. </title> <editor> In V. E. Malyshkin et al., editor, </editor> <booktitle> Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, </address> <month> Sept. </month> <year> 1995, 1995. </year>
Reference-contexts: The complete description of the data structure associated with this boundary information is given in <ref> [17] </ref> along with additional implementation details. 2.2 Matrix-vector products Consider now the matrix-vector operation for a distributed matrix. This is an essentially local operation which takes a distributed vector x and produces the result w = Ax, distributed conformally to the mapping of all vectors. <p> Note that steps 1 and 2 can be performed simultaneously. A processor can be multiplying A i by the local variables while waiting for the external variables to be received. For additional details on the matrix-vector product operation see <ref> [17] </ref>. Sparse Iterative Solvers on Workstations 6 3 Distributed Krylov accelerators The main operations in a standard Krylov subspace acceleration are (1) vector updates, (2) dot-products, (3) matrix-vector products and (4) preconditioning operations.
Reference: [18] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: This is discussed in detail in Section 3.3. 3.1 FGMRES The main Krylov accelerator used in this paper is the flexible variant of GMRES <ref> [18] </ref> known as FGMRES [12]. This is a right-preconditioned variant that allows the preconditioning to vary at each step. Since the preconditioning operations require solving systems associated with entire subdomains it becomes important to allow the preconditioner itself to be an iterative solver. <p> by observing that in the last step of the standard GMRES algorithm, the approximate solution is formed as a linear combination of the preconditioned vectors z i = M 1 v i ; i = 1; : : : ; m, where the v i 's are the Arnoldi vectors <ref> [18] </ref>. Since these vectors are all obtained by applying the same preconditioning matrix M 1 to the v's, we need not save them. We only need to apply M 1 to the linear combination of the v 0 s.
Reference: [19] <author> J. N. Shadid and R. S. Tuminaro. </author> <title> A comparison of preconditioned nonsymmetric krylov methods on a large-scale mimd machine. </title> <journal> SIAM J. Sci Comput., </journal> <volume> 15(2) </volume> <pages> 440-449, </pages> <year> 1994. </year>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [6, 5, 4, 13, 19] </ref>. Thus, if the domains are colored and the global ordering of the domains is the ordering defined by the colors, the Gauss-Seidel iteration as executed in each processor would be as follows: Algorithm 4.2 Multicolor Block Gauss-Seidel Iteration 1.
References-found: 19

