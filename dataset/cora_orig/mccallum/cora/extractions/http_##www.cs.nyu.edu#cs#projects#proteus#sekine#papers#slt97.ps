URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/slt97.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Title: NYU Language Modeling Experiments for the 1996 CSR Evaluation  
Author: Satoshi Sekine, Andrew Borthwick and Ralph Grishman 
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Abstract: This paper describesNYU's effort toward improving recognition accuracy for the 1996 ARPA Large Vocabulary Continuous Speech Recognition evaluation. We are trying to develop different kinds of language models including longer-range models and a linguistically motivated model. For the system described here, we used as a starting point the scores produced by SRI's acoustic and language models. These are linearly combined with the scores produced by the NYU language models. This paper also describes some experiments we tried which were not used in the official experiment, including experiments with perplexity minimization, Maximum Entropy modeling and parsing. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Sankar, A. Stolcke, L. </author> <note> Heck and F.Weng SRI H4-PE System Overview in this proceedings (1997) </note>
Reference-contexts: Our goal has been to study some longer-range language models and determine whether they can be a useful component of the language models used for speech recognition. We will explain the model we used for the official evaluation, done in collaboration with SRI. (SRI's system is described in <ref> [1] </ref>) The technique used for the official evaluation is essentially the same as last year's model. We used a topic coherence model, cache model and weighted cache model.
Reference: 2. <author> F Jelinek, B Merialdo, S Roukos, and M Strauss: </author> <title> A Dynamic Language Model for Speech Recognition Proceedings of DARPA Speech and Natural Language Workshop (1991) </title>
Reference-contexts: We summarize the techniques in three categories: * Cache Use previously uttered word information to supplement the language model. The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. Site (Year) Description Result Ref. IBM (91) cache model <ref> [2] </ref> CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8]
Reference: 3. <institution> Ronald Rosenfeld Adaptive Statistical Language Modeling Proceedings of Human Language Technology Workshop (1994) </institution>
Reference-contexts: The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 <ref> [3] </ref> BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model
Reference: 4. <editor> M.Ostendorf, F.Richardson, R.Iyer, A.Kannan, </editor> <booktitle> O.Ronen and R.Bates The 1994 BU NAB News Benchmark System Proceedings of the ARPA Spoken Language Systems Technology Workshop (1995) </booktitle>
Reference-contexts: We have to be very careful about the conclusions we draw from the table. In general, we can find improvements using these techniques, although many are relatively small (except for the CMU experiment <ref> (94) </ref>, which uses only long texts and different conditions). We summarize the techniques in three categories: * Cache Use previously uttered word information to supplement the language model. The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. <p> We summarize the techniques in three categories: * Cache Use previously uttered word information to supplement the language model. The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. Site (Year) Description Result Ref. IBM (91) cache model [2] CMU <ref> (94) </ref> trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic <p> The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 <ref> [4] </ref> (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related <p> The weighted cache model also uses information about the differing likelihoods of words to reoccur in an article. Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU <ref> (94-96) </ref> sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related works In transcription mode, the
Reference: 5. <author> Satoshi Sekine, </author> <booktitle> John Sterling and Ralph Grishman NYU/BBN 1994 CSR evaluation Proceedings of the ARPA Spoken Language Systems Technology Workshop (1995) </booktitle>
Reference-contexts: Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 <ref> [5] </ref> and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related works In transcription mode, the information in the following input
Reference: 6. <institution> Satoshi Sekine and Ralph Grishman NYU Language Modeling Experiments for the 1995 CSR Evaluation Proceedings of the DARPA Speech Recognition Workshop (1996) </institution>
Reference-contexts: The re trieved articles are assembled into a sublanguage mini-corpus for the current article. We then analyze the mini-corpus in order to determine word preferences which will be used in analyzing the sentence currently being processed. The details of each step were described in last year's paper <ref> [6] </ref>, although some minor parameters were set to different values to fit this year's evaluation. 2.2. Weighted cache model We combined this sublanguage model with a cache model and a weighted cache model. <p> Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 <ref> [6] </ref> cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related works In transcription mode, the information in the following input can also be used. *
Reference: 7. <author> Kristie Seymore, Stanley Chen, </author> <booktitle> Maxine Eskenazi and Roni Rosenfeld Languageand Pronunciation Modeling in the CMU 1996 Hub 4 Evaluation Proceedings of the DARPA Speech Recognition Workshop (1997) </booktitle>
Reference-contexts: Site (Year) Description Result Ref. IBM (91) cache model [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. <ref> [7] </ref> (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related works In transcription mode, the information in the following input can also be used. * Dynamic Topic Adaptation (trigger, sublanguage) Dynamically consult a database to build
Reference: 8. <author> Fuliang Weng, </author> <title> Andreas Stolcke and Ananth Sankar Hub-4 Language Modeling using Domain Interpolation and Data Clustering Proceedings of the DARPA Speech Recognition Workshop (1997) </title>
Reference-contexts: [2] CMU (94) trigger model 19.9 &gt;17.8 [3] BU (93-94) clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 <ref> [8] </ref> (4 topic LM) CU (96) cache model 27.7 &gt;27.5 [9] Table 2: Related works In transcription mode, the information in the following input can also be used. * Dynamic Topic Adaptation (trigger, sublanguage) Dynamically consult a database to build a language model for the topic.
Reference: 9. <author> Steve Young, </author> <title> M ark Gales, David Pye and Phil Woodland HTK Broadcast News Language Model Proceedings of the DARPA Speech Recognition Workshop (1997) </title>
Reference-contexts: clustering 11.3 &gt;11.2 [4] (4 topic LM) NYU (94-96) sublanguage, cache 11.0 &gt;10.6 [5] and weighted 24.6 &gt;24.0 [6] cache model 33.3 &gt;33.0 CMU (96) hand clustering 0.1,0.6% improve. [7] (5883 topic) in 2 story SRI (96) clustering 33.1 &gt;33.0 [8] (4 topic LM) CU (96) cache model 27.7 &gt;27.5 <ref> [9] </ref> Table 2: Related works In transcription mode, the information in the following input can also be used. * Dynamic Topic Adaptation (trigger, sublanguage) Dynamically consult a database to build a language model for the topic.
Reference: 10. <institution> Slava M.Katz Distribution of content words and phrases in text and language modeling Natural Language Engineering, Vol.2 Part.1, </institution> <month> pp15-60 </month> <year> (1996) </year>
Reference-contexts: These quantities are similar to those described in <ref> [10] </ref>. Note that we are using the same formula for 2+ appearances of w as for 2 appearances. We did this due to an intuition that further appearances probably contained no new information and also for simplicity of implementation.
Reference: 11. <author> Satoshi Sekine, </author> <booktitle> Ralph Grishman A Corpus-based Probabilistic Grammar with Only Two Non-terminals Proceedings of the Fourth International Workshop on Parsing Technologies (1995) </booktitle>
Reference-contexts: Parsing As part of our effort to apply natural language techniques in order to improve recognition accuracy, we are developing a corpus-based statistical parser <ref> [11] </ref>. It is a probabilistic, bottom-up, best-first search chart parser, and its grammar is acquired from syntactically bracketed corpus. The special feature of the parser is that the number of non-terminals is relatively small (5 in the current version) in order to capture larger context.
Reference: 12. <author> Satoshi Sekine Apple Pie Parser: </author> <note> home page http://cs.nyu.edu/cs/projects/proteus/app (1996) </note>
Reference-contexts: It is a probabilistic, bottom-up, best-first search chart parser, and its grammar is acquired from syntactically bracketed corpus. The special feature of the parser is that the number of non-terminals is relatively small (5 in the current version) in order to capture larger context. This parser is publically available <ref> [12] </ref>. Recently, we implemented a technique to incorporate the probabilities of lexical dependencies into the parser. We created a simple set of rules to identify the head of each constituent, and assigned dependency relationships between the head and all the other elements.
Reference: 13. <author> Edwin T. </author> <title> Jaynes Information Theory and Statistical Mechanics Physics Reviews 106, </title> <address> pp620-630, </address> <year> (1957) </year>
Reference-contexts: as many times in an article? Furthermore, our team has had continuing internal debates about how to handle the interaction of the cache and sublanguage models: i.e. should the sublanguage model predict cache words or should it leave the prediction of those words entirely to the cache component? M.E. theory <ref> [13] </ref> [14] offers an intuitively and theoretically satisfying answer to these sorts of questions which vex language modelers.
Reference: 14. <institution> Ronald Rosenfeld Adaptive Statistical Language Modeling: </institution>
Reference-contexts: many times in an article? Furthermore, our team has had continuing internal debates about how to handle the interaction of the cache and sublanguage models: i.e. should the sublanguage model predict cache words or should it leave the prediction of those words entirely to the cache component? M.E. theory [13] <ref> [14] </ref> offers an intuitively and theoretically satisfying answer to these sorts of questions which vex language modelers. <p> w)C k (h; w) = ff k We then constrain our M.E. language model to only consider conditional models, P (wjh), which conform to this constraint: X ~ P (h)P (wjh)C k (h; w) = ff k One departure of this work from that of other work in the field <ref> [14] </ref> is that we build a very small, and hence computationally tractable model, using only c. 200 constraints/parameters. Rosenfeld, by contrast, built a model which had c. 2.2 million parameters.
References-found: 14

