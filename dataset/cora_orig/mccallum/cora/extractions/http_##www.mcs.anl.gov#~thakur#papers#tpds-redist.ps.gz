URL: http://www.mcs.anl.gov/~thakur/papers/tpds-redist.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Email: email: thakur@mcs.anl.gov.  email: choudhar@cat.syr.edu.  email: jxr@gate.ee.lsu.edu.  
Title: Efficient Algorithms for Array Redistribution  
Author: Rajeev Thakur Alok Choudhary J. Ramanujam Alok Choudhary J. Ramanujam 
Keyword: Index Terms: array redistribution, distributed-memory computers, High Performance Fortran (HPF), data distribution, runtime support  
Address: IL 60439;  13244;  LA 70803;  
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory, Argonne,  Dept. of Electrical and Computer Engineering, Syracuse University, Syracuse, NY  Dept. of Electrical and Computer Engineering, Louisiana State University, Baton Rouge,  
Note: IEEE Transactions on Parallel and Distributed Systems, (7)6:587-594, June 1996. Copyright 1996 IEEE.  Rajeev Thakur is with the  is with the  is with the  
Abstract: Dynamic redistribution of arrays is required very often in programs on distributed memory parallel computers. This paper presents efficient algorithms for redistribution between different cyclic(k) distributions, as defined in High Performance Fortran. We first propose special optimized algorithms for a cyclic(x) to cyclic(y) redistribution when x is a multiple of y, or y is a multiple of x. We then propose two algorithms, called the GCD method and the LCM method, for the general cyclic(x) to cyclic(y) redistribution when there is no particular relation between x and y. We have implemented these algorithms on the Intel Touchstone Delta, and find that they perform well for different array sizes and number of processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating Local Addresses and Communication Sets for Data Parallel Programs. </title> <booktitle> In Proceedings of Principles and Practices of Parallel Programming (PPoPP) '93, </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Chatterjee et al <ref> [1] </ref> present an approach to calculate the sequence of local memory addresses that a given processor must access while doing a computation involving the regular array section A (l : h : s), when the array A has a cyclic (k) distribution.
Reference: [2] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D Language Specifications. </title> <type> Technical Report COMP TR90-141, CRPC, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction In distributed-memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic, or block-cyclic, as in Fortran D <ref> [2] </ref> and High Performance Fortran (HPF) [4, 9], or irregular in which there is no simple arithmetic function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program.
Reference: [3] <author> S. Gupta, S. Kaushik, S. Mufti, S. Sharma, C. Huang, and P. Sadayappan. </author> <title> On the Generation of Efficient Data Communication for Distributed Memory Machines. </title> <booktitle> In Proceedings of International Computing Symposium, </booktitle> <pages> pages 504-513, </pages> <year> 1992. </year>
Reference-contexts: processors is 64, and the array size is varied. algorithm, since the condition ((k P ) and (mod (P; k) = 0)) is not satisfied, and the asynchronous method was used in X TO KX algorithm, since it does not require the above condition. 6 Related Work Gupta et al. <ref> [3] </ref> and Koelbel [8] provide closed form expressions for determining the send and receive processor sets and data index sets for redistributing arrays between block and cyclic distributions. Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in [16]. <p> Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in [16]. A model for evaluating the communication cost of data redistribution is given in [6]. A virtual processor approach for the general block-cyclic redistribution is proposed in <ref> [3] </ref>. Wakatani and Wolfe [18] describe a method of array redistribution, called strip mining redistribution, in which parts an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole.
Reference: [4] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction In distributed-memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic, or block-cyclic, as in Fortran D [2] and High Performance Fortran (HPF) <ref> [4, 9] </ref>, or irregular in which there is no simple arithmetic function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program.
Reference: [5] <author> E. Kalns and L. Ni. </author> <title> Processor Mapping Techniques Toward Efficient Data Redistribution. </title> <booktitle> In Proceedings of the 8 th International Parallel Processing Symposium, </booktitle> <pages> pages 469-476, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The reason for doing this is to try to overlap the communication involved in redistribution with some of the computation in the program. Kalns and Ni <ref> [5] </ref> present a technique for mapping data to processors so as to minimize the total amount of data that must be communicated during redistribution. A multiphase approach to redistribution is discussed in [7].
Reference: [6] <author> S. Kaushik, C. Huang, R. Johnson, and P. Sadayappan. </author> <title> An Approach to Communication Efficient Data Redistribution. </title> <booktitle> In Proceedings of the 8 th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in [16]. A model for evaluating the communication cost of data redistribution is given in <ref> [6] </ref>. A virtual processor approach for the general block-cyclic redistribution is proposed in [3]. Wakatani and Wolfe [18] describe a method of array redistribution, called strip mining redistribution, in which parts an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole.
Reference: [7] <author> S. Kaushik, C. Huang, J. Ramanujam, and P. Sadayappan. </author> <title> Multi-phase Array Redistribu tion: Modeling and Evaluation. </title> <booktitle> In Proceedings of the 9 th International Parallel Processing Symposium, </booktitle> <pages> pages 441-445, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Kalns and Ni [5] present a technique for mapping data to processors so as to minimize the total amount of data that must be communicated during redistribution. A multiphase approach to redistribution is discussed in <ref> [7] </ref>. Algorithms for redistribution, based on a mathematical representation for regular distributions called PITFALLS, are proposed in [11] .
Reference: [8] <author> C. Koelbel. </author> <title> Compile-Time Generation of Regular Communication Patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: and the array size is varied. algorithm, since the condition ((k P ) and (mod (P; k) = 0)) is not satisfied, and the asynchronous method was used in X TO KX algorithm, since it does not require the above condition. 6 Related Work Gupta et al. [3] and Koelbel <ref> [8] </ref> provide closed form expressions for determining the send and receive processor sets and data index sets for redistributing arrays between block and cyclic distributions. Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in [16].
Reference: [9] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction In distributed-memory parallel computers, arrays have to be distributed among processors in some fashion. The distribution can either be regular i.e. block, cyclic, or block-cyclic, as in Fortran D [2] and High Performance Fortran (HPF) <ref> [4, 9] </ref>, or irregular in which there is no simple arithmetic function specifying the mapping of arrays to processors. The distribution of an array does not need to remain fixed throughout the program.
Reference: [10] <author> R. Ponnusamy, R. Thakur, A. Choudhary, and G. Fox. </author> <title> Scheduling Regular and Irregular Communication Patterns on the CM-5. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 394-402, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: local index (l) (g=(m P ))m local index (l) to g = l + m p g = (l 1)P + p + 1 g = mod (l 1; m) + 1+ global index (g) (P ((l 1)=m) + p)m to implement these communication patterns are described in detail in <ref> [15, 10, 17, 12, 13] </ref>. The performance results presented in this paper were obtained using the communication algorithms given in [15, 10, 17]. <p> The performance results presented in this paper were obtained using the communication algorithms given in <ref> [15, 10, 17] </ref>. We do assume that all the data to be sent from any processor i to processor j has to be collected in a packet in processor i and sent in one communication operation, so as to minimize the communication startup cost.
Reference: [11] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic Generation of Efficient Array Redistribution Routines for Distributed Memory Multicomputers. </title> <booktitle> In Proceedings of The Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 342-349, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: A multiphase approach to redistribution is discussed in [7]. Algorithms for redistribution, based on a mathematical representation for regular distributions called PITFALLS, are proposed in <ref> [11] </ref> .
Reference: [12] <author> S. Ranka, J. Wang, and M. Kumar. </author> <title> Irregular Personalized Communication on Distributed Memory Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 25(1) </volume> <pages> 58-71, </pages> <month> February 15, </month> <year> 1995. </year>
Reference-contexts: local index (l) (g=(m P ))m local index (l) to g = l + m p g = (l 1)P + p + 1 g = mod (l 1; m) + 1+ global index (g) (P ((l 1)=m) + p)m to implement these communication patterns are described in detail in <ref> [15, 10, 17, 12, 13] </ref>. The performance results presented in this paper were obtained using the communication algorithms given in [15, 10, 17].
Reference: [13] <author> D. Scott. </author> <title> Efficient All-to-All Communication Patterns in Hypercube and Mesh Topologies. </title> <booktitle> In Proceedings of the 6 th Distributed Memory Computing Conference, </booktitle> <pages> pages 398-403, </pages> <year> 1991. </year>
Reference-contexts: local index (l) (g=(m P ))m local index (l) to g = l + m p g = (l 1)P + p + 1 g = mod (l 1; m) + 1+ global index (g) (P ((l 1)=m) + p)m to implement these communication patterns are described in detail in <ref> [15, 10, 17, 12, 13] </ref>. The performance results presented in this paper were obtained using the communication algorithms given in [15, 10, 17].
Reference: [14] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating Communication for Array Statements: Design, Implementation, and Evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 150-159, </pages> <month> April </month> <year> 1994. </year> <month> 16 </month>
Reference-contexts: They show that the local memory access sequence is characterized by a finite state machine of at most k states. Stichnoth <ref> [14] </ref> defines a cyclic (k) distribution as a disjoint union of slices, where a slice is a sequence of array indices specified by a lower bound, upper bound and stride (l : h : s).
Reference: [15] <author> R. Thakur and A. Choudhary. </author> <title> All-to-All Communication on Meshes with Wormhole Routing. </title> <booktitle> In Proceedings of the 8 th International Parallel Processing Symposium, </booktitle> <pages> pages 561-565, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: local index (l) (g=(m P ))m local index (l) to g = l + m p g = (l 1)P + p + 1 g = mod (l 1; m) + 1+ global index (g) (P ((l 1)=m) + p)m to implement these communication patterns are described in detail in <ref> [15, 10, 17, 12, 13] </ref>. The performance results presented in this paper were obtained using the communication algorithms given in [15, 10, 17]. <p> The performance results presented in this paper were obtained using the communication algorithms given in <ref> [15, 10, 17] </ref>. We do assume that all the data to be sent from any processor i to processor j has to be collected in a packet in processor i and sent in one communication operation, so as to minimize the communication startup cost.
Reference: [16] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime Array Redistribution in HPF Programs. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 309-316, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in <ref> [16] </ref>. A model for evaluating the communication cost of data redistribution is given in [6]. A virtual processor approach for the general block-cyclic redistribution is proposed in [3].
Reference: [17] <author> R. Thakur, R. Ponnusamy, A. Choudhary, and G. Fox. </author> <title> Complete Exchange on the CM-5 and Touchstone Delta. </title> <journal> The Journal of Supercomputing, </journal> <volume> 8(4) </volume> <pages> 305-328, </pages> <year> 1995. </year>
Reference-contexts: local index (l) (g=(m P ))m local index (l) to g = l + m p g = (l 1)P + p + 1 g = mod (l 1; m) + 1+ global index (g) (P ((l 1)=m) + p)m to implement these communication patterns are described in detail in <ref> [15, 10, 17, 12, 13] </ref>. The performance results presented in this paper were obtained using the communication algorithms given in [15, 10, 17]. <p> The performance results presented in this paper were obtained using the communication algorithms given in <ref> [15, 10, 17] </ref>. We do assume that all the data to be sent from any processor i to processor j has to be collected in a packet in processor i and sent in one communication operation, so as to minimize the communication startup cost.
Reference: [18] <author> A. Wakatani and M. Wolfe. </author> <title> A New Approach to Array Redistribution: Strip Mining Redistribution. </title> <booktitle> In Proceedings of Parallel Architectures and Languages Europe (PARLE 94), </booktitle> <pages> pages 323-335, </pages> <month> July </month> <year> 1994. </year> <month> 17 </month>
Reference-contexts: Efficient algorithms for block (m) to cyclic, and cyclic to block (m) redistributions are described in [16]. A model for evaluating the communication cost of data redistribution is given in [6]. A virtual processor approach for the general block-cyclic redistribution is proposed in [3]. Wakatani and Wolfe <ref> [18] </ref> describe a method of array redistribution, called strip mining redistribution, in which parts an array are redistributed in sequence, instead of redistributing the entire array at one time as a whole.
References-found: 18

