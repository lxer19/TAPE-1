URL: http://www.cs.berkeley.edu/~beymer/publications/aim1537.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~beymer/publications.html
Root-URL: 
Email: email: beymer@ai.mit.edu  
Title: Vectorizing Face Images by Interleaving Shape and Texture Computations  
Author: David Beymer 
Note: Copyright c Massachusetts Institute of Technology, 1995  
Date: 1537 September, 1995  122  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: The correspondence problem in computer vision is basically a matching task between two or more sets of features. Computing feature correspondence is of great importance in computer vision, especially in the subfields of object recognition, stereo, and motion. In this paper, we introduce a vectorized image representation, which is a feature-based representation where correspondence has been established with respect to a reference image. The representation consists of two image measurements made at the feature points: shape and texture. Feature geometry, or shape, is represented using the (x; y) locations of features relative to the some standard reference shape. Image grey levels, or texture, are represented by mapping image grey levels onto the standard reference shape. Computing this representation is essentially a correspondence task, and in this paper we explore an automatic technique for "vectorizing" face images. Our face vectorizer alternates back and forth between computation steps for shape and texture, and a key idea is to structure the two computations so that each one uses the output of the other. Namely, the texture computation uses shape for geometrical normalization, and the shape computation uses the texture analysis to synthesize a "reference" image for finding correspondences. A hierarchical coarse-to-fine implementation is discussed, and applications are presented to the problems of facial feature detection and registration of two arbitrary faces. This report describes research done at the Artificial Intelligence Laboratory and within the Center for Biological and Computational Learning. This research is sponsored by grants from the Office of Naval Research under contracts N00014-91-J-1270 and N00014-92-J-1879; by a grant from the National Science Foundation under contract ASC-9217041. Support for the A.I. Laboratory's artificial intelligence research is provided by ONR contract N00014-91-J-4038. The author is supported by a Howard Hughes Doctoral Fellowship from the Hughes Aircraft Company. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Aizawa, H. Harashima, and T. Saito. </author> <title> Model-based analysis synthesis image coding (MBASIC) system for a person's face. Signal Processing: </title> <journal> Image Communication, </journal> <volume> 1 </volume> <pages> 139-152, </pages> <year> 1989. </year> <month> 14 </month>
Reference-contexts: A standard approach used in the computer graphics and computer vision communities for representing this prior knowledge is to use a 3D model of the face (Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [36][39], Aizawa, Harashima, and Saito <ref> [1] </ref>, Essa and Pentland [20]). After the single available 2D image is texture mapped onto a 3D polygonal or multilayer mesh model of the face, rotated views can be synthesized by rotating the 3D model and rendering.
Reference: [2] <author> Shigeru Akamatsu, Tsutomu Sasaki, Hideo Fuka--machi, Nobuhiko Masui, and Yasuhito Suenaga. </author> <title> An accurate and robust face identification scheme. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 217-220, </pages> <address> The Hague, The Nether-lands, </address> <year> 1992. </year>
Reference-contexts: The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. <ref> [2] </ref>, Pentland, et al. [26]), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio [35], Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). <p> Most normalization methods use a global transform, usually a similarity or affine transform, to align two or three major facial features. For example, in Pent-land, et al. [26], the imaging apparatus effectively registers eyes, and Akamatsu, et al. <ref> [2] </ref> register the eyes and mouth. However, because of the inherent variability of facial geometries across different people, aligning just a couple of features such as the eyes leaves other features misaligned.
Reference: [3] <author> Takaaki Akimoto, Yasuhito Suenaga, and Richard S. Wal-lace. </author> <title> Automatic creation of 3D facial models. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 13(5) </volume> <pages> 16-22, </pages> <year> 1993. </year>
Reference-contexts: Figure from [11]. knowledge of a facial transformation such as head rotation or expression change. A standard approach used in the computer graphics and computer vision communities for representing this prior knowledge is to use a 3D model of the face (Akimoto, Suennaga, and Wallace <ref> [3] </ref>, Waters and Terzopoulos [36][39], Aizawa, Harashima, and Saito [1], Essa and Pentland [20]). After the single available 2D image is texture mapped onto a 3D polygonal or multilayer mesh model of the face, rotated views can be synthesized by rotating the 3D model and rendering.
Reference: [4] <author> Adam Baumberg and David Hogg. </author> <title> Learning flexible models from image sequences. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <pages> pages 299-308, </pages> <address> Stockholm, Sweden, </address> <year> 1994. </year>
Reference-contexts: This technique for modeling shape is similar to the work of Cootes, et al. [17], Blake and Isard [12], Baumberg and Hogg <ref> [4] </ref>, and Jones and Poggio [21].
Reference: [5] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: The features on new faces will then be measured relative to the standard geometry. In this paper, the standard geometry for frontal views of faces is 1 manual line segment features are used. After Beier and Neely <ref> [5] </ref>. defined by averaging a set of line segment features over an ensemble of "prototype" faces. Fig. 1 shows the line segment features for a particular individual, and Fig. 2 shows the average over a set of 14 prototype people. <p> Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [24], Wol-berg [40]), radial basis functions (Reisfeld, Arad, and Yeshurun [31]), and inverse weighted distance metrics (Beier and Neely <ref> [5] </ref>). <p> We now go over these steps in more detail. First, to define the shape of the example faces, a set of line segment features are positioned manually for each. The features, shown in Fig. 1, follow Beier and Neely's <ref> [5] </ref> manual correspondence technique for morphing face images. Pairing up image feature points into line segments gives one a natural control over local scale and rotation of example images. <p> However, averaging shape should minimize the total amount of distortion required in the next step of geometrical normalization. Finally, images are geometrically normalized using the local deformation technique of Beier and Neely <ref> [5] </ref>. This deformation technique is driven by the pairing of line segments in the example image with line segments in the standard shape. Consider a single pairing of line segments, one segment from the example image l ex and one from the standard shape l std . <p> We have used the vec-torizer to automatically locate the set of facial features shown in Fig. 14 in both the prototype and novel faces. From this sparse set of correspondences, the interpolation technique from Beier and Neely <ref> [5] </ref> is used to generate a dense, pixelwise mapping between the two faces. We then used the dense set of correspondences to map rotation deformations from a single prototype to a group of 61 other faces for generating virtual views.
Reference: [6] <author> Jezekiel Ben-Aire and K. Raghunath Rao. </author> <title> On the recognition of occluded shapes and generic faces using multiple-template expansion matching. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 214-219, </pages> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: For the more general case of cluttered backgrounds, see the face detection work of Reisfeld and Yeshurun [32], Ben-Arie and Rao <ref> [6] </ref>, Sung and Pog-gio [35], Sinha [34], and Moghaddam and Pentland [25]. For our test images, we found that normalized correlation using two face templates works well.
Reference: [7] <author> James R. Bergen and Edward H. Adelson. </author> <title> Hierarchical, computationally efficient motion estimation algorithm. </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> 4(13):P35, </volume> <year> 1987. </year>
Reference-contexts: Next, optical flow is computed between b t a , which is geometrically normalized, and i a , which updates the pixelwise correspondences y std astd . For optical flow, we used the gradient-based hierarchical scheme of Bergen and Adelson <ref> [7] </ref>, Bergen and Hingorani [9], and Bergen, et al. [8]. The new correspondences should provide bet ter geometrical normalization in the next texture step.
Reference: [8] <author> James R. Bergen, P. Anandan, Keith J. Hanna, and Rajesh Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Next, optical flow is computed between b t a , which is geometrically normalized, and i a , which updates the pixelwise correspondences y std astd . For optical flow, we used the gradient-based hierarchical scheme of Bergen and Adelson [7], Bergen and Hingorani [9], and Bergen, et al. <ref> [8] </ref>. The new correspondences should provide bet ter geometrical normalization in the next texture step.
Reference: [9] <author> J.R. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, Princeton, </institution> <address> New Jersey, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Next, optical flow is computed between b t a , which is geometrically normalized, and i a , which updates the pixelwise correspondences y std astd . For optical flow, we used the gradient-based hierarchical scheme of Bergen and Adelson [7], Bergen and Hingorani <ref> [9] </ref>, and Bergen, et al. [8]. The new correspondences should provide bet ter geometrical normalization in the next texture step. <p> Qualitatively, the eyebrow and nose errors were misalignments, while the mouth error did involve a complete miss. real views virtual views by finding correspondence between the prototype face images. We use the same gradient-based optical flow algorithm <ref> [9] </ref> used in the vectorizer to find a dense set of pixelwise correspondences. Next, the prototype flow is mapped onto the "novel" face, the individual for which we wish to generate virtual views. This step requires "in-terperson" correspondence between the prototype and novel faces.
Reference: [10] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The main idea is to interpolate among the b t a images of the different vectorizers to produce a new image that reconstructs both the grey levels and the pose of the input image (see Beymer, Shashua and Poggio <ref> [10] </ref> for examples of interpolation across different poses). Correspondence is then found between the input and this new interpolated image using optical flow.
Reference: [11] <author> David Beymer and Tomaso Poggio. </author> <title> Face recognition from one example view. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1536, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: In section 5, we demonstrate two applications of the vectorizer, facial feature detection and the registration of two arbitrary faces. The latter application is used to map prototypical face transformations onto a face so that new "virtual" views can be synthesized (see Beymer and Poggio <ref> [11] </ref>). The paper closes with suggestions for future work, including an idea to generalize the vectorizer to multiple poses. 2 Preliminaries 2.1 Vectorized representation As mentioned in the introduction, the vectorized representation is a feature-based representation where correspondence has been established relative to a fixed reference object or reference image. <p> In this example, the transformation is rotation and optical flow was used to find a dense set of correspondences. Next, in (b), the flow is mapped onto the novel face, and (c) the novel face is 2D warped to a "virtual" view. Figure from <ref> [11] </ref>. knowledge of a facial transformation such as head rotation or expression change. <p> We have investigated an alternative approach that uses example 2D views of prototype faces as a substitute for 3D models (Poggio and Vetter [30], Poggio and Brunelli [29], Beymer and Poggio <ref> [11] </ref>). In parallel deformation, one of the example-based techniques discussed in Beymer and Poggio [11], prior knowledge of a facial transformation such as a rotation or change in expression is extracted from views of a prototype face undergoing the transformation. <p> We have investigated an alternative approach that uses example 2D views of prototype faces as a substitute for 3D models (Poggio and Vetter [30], Poggio and Brunelli [29], Beymer and Poggio <ref> [11] </ref>). In parallel deformation, one of the example-based techniques discussed in Beymer and Poggio [11], prior knowledge of a facial transformation such as a rotation or change in expression is extracted from views of a prototype face undergoing the transformation. <p> Fig. 16 shows some example pairs of real and virtual views. To evaluate these virtual views, they were used as example views in a view-based, pose-invariant face rec-ognizer (see <ref> [11] </ref> for details). The problem is this: given one real view of each person, can we recognize the person under a variety of poses? Virtual views were used to generate a set of rotated example views to augment the single real view.
Reference: [12] <author> Andrew Blake and Michael Isard. </author> <title> 3D position, attitude and shape input using video tracking of hands and lips. </title> <booktitle> In SIGGRAPH '94 Proceedings, </booktitle> <pages> pages 185-192, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: This technique for modeling shape is similar to the work of Cootes, et al. [17], Blake and Isard <ref> [12] </ref>, Baumberg and Hogg [4], and Jones and Poggio [21].
Reference: [13] <author> Peter J. Burt. </author> <title> Smart sensing within a pyramid vision machine. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76(8) </volume> <pages> 1006-1015, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This template decomposition across scales is similar to Burt's pattern tree approach <ref> [13] </ref> for template matching on a pyramid representation. At a coarse scale, faces are small, so full face templates are needed to provide enough spatial support for texture analysis.
Reference: [14] <author> Peter J. Burt and Edward H. Adelson. </author> <title> The lapla-cian pyramid as a compact image code. </title> <journal> IEEE Trans. on Communications, </journal> <volume> COM-31(4):532-540, </volume> <month> April </month> <year> 1983. </year>
Reference-contexts: Later, in section 6.3, we suggest a multiple-pose vectorizer that connects different pose-specific vectorizers through interpolation. 4 Hierarchical implementation For optimization purposes, the vectorization procedure is implemented using a coarse-to-fine strategy. Given an input image to vectorize, first the Gaussian pyramid (Burt and Adelson <ref> [14] </ref>) is computed to provide a mul-tiresolution representation over 4 scales, the original image plus 3 reductions by 2. A face finder is then run over the coarsest level to provide an initial estimate for the similarity transform P .
Reference: [15] <author> T.F. Cootes and C.J. Taylor. </author> <title> Active shape models - `Smart snakes'. </title> <editor> In David Hogg and Roger Boyle, editors, </editor> <booktitle> Proc. British Machine Vision Conference, </booktitle> <pages> pages 266-275. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: y a = B B @ y 1 x n 1 C C : This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [28]), the linear combinations approach to recognition (Ullman and Basri [38], Pog-gio [27]), active shape models (Cootes and Taylor <ref> [15] </ref>, with respect to the reference image i std at standard shape. First, pixelwise correspondence is computed between i std and i a , as indicated by the grey arrow.
Reference: [16] <author> T.F. Cootes and C.J. Taylor. </author> <title> Using grey-level models to improve active shape model search. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 63-67, </pages> <address> Jerusalem, Israel, </address> <year> 1994. </year>
Reference: [17] <author> T.F. Cootes, C.J. Taylor, A. Lanitis, D.H. Cooper, and J. Graham. </author> <title> Building and using flexible models incorporating grey-level information. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 242-246, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Shape y std astd is a vector field that specifies a corresponding pixel in i a for each pixel in i std . Texture t a consists of the grey levels of i a mapped onto the standard shape. Cootes, et al. <ref> [17] </ref>) and face recognition (Craw and Cameron [18][19]). A relative shape measured with respect to a standard reference shape y std is simply the difference y a y std ; which we denote using the shorthand notation y astd . <p> This technique for modeling shape is similar to the work of Cootes, et al. <ref> [17] </ref>, Blake and Isard [12], Baumberg and Hogg [4], and Jones and Poggio [21].
Reference: [18] <author> Ian Craw and Peter Cameron. </author> <title> Parameterizing images for recognition and reconstruction. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 367-370, </pages> <year> 1991. </year>
Reference-contexts: That is, the geometrical differences among face images are factored out by warping the images to the standard reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron <ref> [18] </ref>, and Shackleton and Welsh [33]. <p> For example, a new face may match the texture of one particular linear combination of eigenimages but the shape may require another linear combination. To decouple texture and shape, Craw and Cameron <ref> [18] </ref> and Shack-elton and Welsh [33] represent shape separately and use it to geometrically normalize face texture by deforming it to a standard shape. Shape is defined by the (x; y) locations of a set of feature points, as in our definition for shape. In Craw and Cameron [18], 76 points <p> and Cameron <ref> [18] </ref> and Shack-elton and Welsh [33] represent shape separately and use it to geometrically normalize face texture by deforming it to a standard shape. Shape is defined by the (x; y) locations of a set of feature points, as in our definition for shape. In Craw and Cameron [18], 76 points outlining the eyes, nose, mouth, eyebrows, and head are used. To geometrically normalize texture using shape, image texture is deformed to a standard face shape, making it "shape free". This is done by first triangulating the image using the features and then texture mapping. <p> However, they did not demonstrate an effective automatic method for computing the vectorized shape/texture representation. This is mainly due to difficulties in finding correspondences for shape, where probably on the order of tens of features need to be located. Craw and Cameron <ref> [18] </ref> manually locate their features. Shackelton and Welsh [33], who focus on eye images, use the deformable template approach of Yuille, Cohen, and Hallinan [41] to locate eye features. However, for 19/60 of their example eye images, feature localization is either rated as "poor" or "no fit".
Reference: [19] <author> Ian Craw and Peter Cameron. </author> <title> Face recognition by computer. </title> <editor> In David Hogg and Roger Boyle, editors, </editor> <booktitle> Proc. British Machine Vision Conference, </booktitle> <pages> pages 498-507. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference: [20] <author> Irfan A. Essa and Alex Pentland. </author> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 76-83, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: A standard approach used in the computer graphics and computer vision communities for representing this prior knowledge is to use a 3D model of the face (Akimoto, Suennaga, and Wallace [3], Waters and Terzopoulos [36][39], Aizawa, Harashima, and Saito [1], Essa and Pentland <ref> [20] </ref>). After the single available 2D image is texture mapped onto a 3D polygonal or multilayer mesh model of the face, rotated views can be synthesized by rotating the 3D model and rendering. In addition, facial expressions have been modeled [36][39][20] by embedding muscle forces that deform the 3D model in
Reference: [21] <author> Michael J. Jones and Tomaso Poggio. </author> <title> Model-based matching of line drawings by linear combinations of prototypes. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 531-536, </pages> <address> Boston, Massachusetts, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: This technique for modeling shape is similar to the work of Cootes, et al. [17], Blake and Isard [12], Baumberg and Hogg [4], and Jones and Poggio <ref> [21] </ref>.
Reference: [22] <author> M. Kirby and L. Sirovich. </author> <title> Application of the Karhunen-Loeve procedure for the characterization of human faces. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(1) </volume> <pages> 103-108, </pages> <year> 1990. </year>
Reference-contexts: The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. [2], Pentland, et al. [26]), reconstruction (Kirby and Sirovich <ref> [22] </ref>), face detection (Sung and Pog-gio [35], Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). The main assumption behind this modeling approach is that the space of grey level images of faces is linearly spanned by a set of example face images.
Reference: [23] <author> A. Lanitis, C.J. Taylor, and T.F. Cootes. </author> <title> A unified approach to coding and interpreting face images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 368-373, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference: [24] <author> Peter Litwinowicz and Lance Williams. </author> <title> Animating images with drawings. </title> <booktitle> In SIGGRAPH '94 Proceedings, </booktitle> <pages> pages 409-412, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representation. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams <ref> [24] </ref>, Wol-berg [40]), radial basis functions (Reisfeld, Arad, and Yeshurun [31]), and inverse weighted distance metrics (Beier and Neely [5]).
Reference: [25] <author> Baback Moghaddam and Alex Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 786-793, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. [2], Pentland, et al. [26]), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio [35], Moghaddam and Pentland <ref> [25] </ref>), and facial feature detection (Pentland, et al. [26]). The main assumption behind this modeling approach is that the space of grey level images of faces is linearly spanned by a set of example face images. <p> For the more general case of cluttered backgrounds, see the face detection work of Reisfeld and Yeshurun [32], Ben-Arie and Rao [6], Sung and Pog-gio [35], Sinha [34], and Moghaddam and Pentland <ref> [25] </ref>. For our test images, we found that normalized correlation using two face templates works well. <p> To accomplish this, both the face detection and vector-izer should be made more robust to the presense of false positive matches. To improve face detection, we would probably incorporate the learning approaches of Sung and Poggio [35] or Moghaddam and Pentland <ref> [25] </ref>. Both of these techniques model the space of grey level face images using principal components analysis.
Reference: [26] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Our texture model relies on the assumption, commonly made in the eigenface approach to face recognition and detection (Turk and Pentland [37], Pentland, et al. <ref> [26] </ref>), that the space of grey level images of faces is linearly spanned by a set of example views. <p> The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. [2], Pentland, et al. <ref> [26] </ref>), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio [35], Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). <p> face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. [2], Pentland, et al. <ref> [26] </ref>), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio [35], Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). The main assumption behind this modeling approach is that the space of grey level images of faces is linearly spanned by a set of example face images. <p> Most normalization methods use a global transform, usually a similarity or affine transform, to align two or three major facial features. For example, in Pent-land, et al. <ref> [26] </ref>, the imaging apparatus effectively registers eyes, and Akamatsu, et al. [2] register the eyes and mouth. However, because of the inherent variability of facial geometries across different people, aligning just a couple of features such as the eyes leaves other features misaligned.
Reference: [27] <author> T. Poggio. </author> <title> 3D object recognition: on a result by Basri and Ullman. </title> <type> Technical Report # 9005-03, IRST, </type> <institution> Povo, Italy, </institution> <year> 1990. </year>
Reference-contexts: of the x and y coordinate values y a = B B @ y 1 x n 1 C C : This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [28]), the linear combinations approach to recognition (Ullman and Basri [38], Pog-gio <ref> [27] </ref>), active shape models (Cootes and Taylor [15], with respect to the reference image i std at standard shape. First, pixelwise correspondence is computed between i std and i a , as indicated by the grey arrow.
Reference: [28] <author> T. Poggio and S. Edelman. </author> <title> A network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343(6255) </volume> <pages> 263-266, </pages> <month> January </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: by a vector y a of length 2n consisting of the concatenation of the x and y coordinate values y a = B B @ y 1 x n 1 C C : This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman <ref> [28] </ref>), the linear combinations approach to recognition (Ullman and Basri [38], Pog-gio [27]), active shape models (Cootes and Taylor [15], with respect to the reference image i std at standard shape. First, pixelwise correspondence is computed between i std and i a , as indicated by the grey arrow.
Reference: [29] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A novel ap-proach to graphics. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1354, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: We have investigated an alternative approach that uses example 2D views of prototype faces as a substitute for 3D models (Poggio and Vetter [30], Poggio and Brunelli <ref> [29] </ref>, Beymer and Poggio [11]). In parallel deformation, one of the example-based techniques discussed in Beymer and Poggio [11], prior knowledge of a facial transformation such as a rotation or change in expression is extracted from views of a prototype face undergoing the transformation.
Reference: [30] <author> Tomaso Poggio and Thomas Vetter. </author> <title> Recognition and structure from one 2D model view: Observations on prototypes, object classes, and symmetries. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1347, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: We have investigated an alternative approach that uses example 2D views of prototype faces as a substitute for 3D models (Poggio and Vetter <ref> [30] </ref>, Poggio and Brunelli [29], Beymer and Poggio [11]). In parallel deformation, one of the example-based techniques discussed in Beymer and Poggio [11], prior knowledge of a facial transformation such as a rotation or change in expression is extracted from views of a prototype face undergoing the transformation.
Reference: [31] <author> Daniel Reisfeld, Nur Arad, and Yehezkel Yeshurun. </author> <title> Normalization of face images using few anchors. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 761-763, </pages> <address> Jerusalem, Israel, </address> <year> 1994. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representation. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [24], Wol-berg [40]), radial basis functions (Reisfeld, Arad, and Yeshurun <ref> [31] </ref>), and inverse weighted distance metrics (Beier and Neely [5]).
Reference: [32] <author> Daniel Reisfeld and Yehezkel Yeshurun. </author> <title> Robust detection of facial features by generalized symmetry. </title> <booktitle> In Proceedings Int. Conf. on Pattern Recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 117-120, </pages> <address> The Hague, The Nether-lands, </address> <year> 1992. </year>
Reference-contexts: For the more general case of cluttered backgrounds, see the face detection work of Reisfeld and Yeshurun <ref> [32] </ref>, Ben-Arie and Rao [6], Sung and Pog-gio [35], Sinha [34], and Moghaddam and Pentland [25]. For our test images, we found that normalized correlation using two face templates works well.
Reference: [33] <author> M.A. Shackleton and W.J. Welsh. </author> <title> Classification of facial features for recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 573-579, </pages> <address> Lahaina, Maui, Hawaii, </address> <year> 1991. </year>
Reference-contexts: That is, the geometrical differences among face images are factored out by warping the images to the standard reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron [18], and Shackleton and Welsh <ref> [33] </ref>. <p> For example, a new face may match the texture of one particular linear combination of eigenimages but the shape may require another linear combination. To decouple texture and shape, Craw and Cameron [18] and Shack-elton and Welsh <ref> [33] </ref> represent shape separately and use it to geometrically normalize face texture by deforming it to a standard shape. Shape is defined by the (x; y) locations of a set of feature points, as in our definition for shape. <p> This is mainly due to difficulties in finding correspondences for shape, where probably on the order of tens of features need to be located. Craw and Cameron [18] manually locate their features. Shackelton and Welsh <ref> [33] </ref>, who focus on eye images, use the deformable template approach of Yuille, Cohen, and Hallinan [41] to locate eye features. However, for 19/60 of their example eye images, feature localization is either rated as "poor" or "no fit".
Reference: [34] <author> Pawan Sinha. </author> <title> Object recognition via image invari-ances. </title> <institution> Investigative Ophthalmology and Visual Science, 35(4):1626, </institution> <year> 1994. </year>
Reference-contexts: For the more general case of cluttered backgrounds, see the face detection work of Reisfeld and Yeshurun [32], Ben-Arie and Rao [6], Sung and Pog-gio [35], Sinha <ref> [34] </ref>, and Moghaddam and Pentland [25]. For our test images, we found that normalized correlation using two face templates works well.
Reference: [35] <author> Kah-Kay Sung and Tomaso Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <booktitle> In Proceedings Image Understanding Workshop, </booktitle> <volume> volume II, </volume> <pages> pages 843-850, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland [37], Aka-matsu, et al. [2], Pentland, et al. [26]), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio <ref> [35] </ref>, Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). The main assumption behind this modeling approach is that the space of grey level images of faces is linearly spanned by a set of example face images. <p> For the more general case of cluttered backgrounds, see the face detection work of Reisfeld and Yeshurun [32], Ben-Arie and Rao [6], Sung and Pog-gio <ref> [35] </ref>, Sinha [34], and Moghaddam and Pentland [25]. For our test images, we found that normalized correlation using two face templates works well. <p> It would be nice to demonstrate the vectorizer working in cluttered environments. To accomplish this, both the face detection and vector-izer should be made more robust to the presense of false positive matches. To improve face detection, we would probably incorporate the learning approaches of Sung and Poggio <ref> [35] </ref> or Moghaddam and Pentland [25]. Both of these techniques model the space of grey level face images using principal components analysis.
Reference: [36] <author> Demetri Terzopoulos and Keith Waters. </author> <title> Analysis of facial images using physical and anatomical models. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 727-732, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference: [37] <author> Matthew Turk and Alex Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: The two primary components of the vectorized representation are shape and texture. Previous approaches in analyzing faces have stressed either one component or the other, such as feature localization or decomposing texture as a linear combination of eigenfaces (see Turk and Pentland <ref> [37] </ref>). The key aspect of our vectorization algorithm, or "vectorizer", is that the two processes for the analysis of shape and texture are coupled. That is, the shape and texture processes are coupled by making each process use the output of the other. <p> Our texture model relies on the assumption, commonly made in the eigenface approach to face recognition and detection (Turk and Pentland <ref> [37] </ref>, Pentland, et al. [26]), that the space of grey level images of faces is linearly spanned by a set of example views. <p> The eigenface approach for modeling face images has been used recently for a variety of facial analysis tasks, including face recognition (Turk and Pentland <ref> [37] </ref>, Aka-matsu, et al. [2], Pentland, et al. [26]), reconstruction (Kirby and Sirovich [22]), face detection (Sung and Pog-gio [35], Moghaddam and Pentland [25]), and facial feature detection (Pentland, et al. [26]). <p> Both of these techniques model the space of grey level face images using principal components analysis. To judge the "faceness" of a image, they use a distance metric that includes two terms, "distance from face space" (see Turk and Pentland <ref> [37] </ref>) kt a b t a k and the Mahalanobis distance P N fi 2 i ; where the fi i are the eigenspace projection coefficients and i are the eigenvalues from principal component analysis.
Reference: [38] <author> Shimon Ullman and Ronen Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: the concatenation of the x and y coordinate values y a = B B @ y 1 x n 1 C C : This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [28]), the linear combinations approach to recognition (Ullman and Basri <ref> [38] </ref>, Pog-gio [27]), active shape models (Cootes and Taylor [15], with respect to the reference image i std at standard shape. First, pixelwise correspondence is computed between i std and i a , as indicated by the grey arrow.
Reference: [39] <author> Keith Waters and Demetri Terzopoulos. </author> <title> Mod-elling and animating faces using scanned data. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 123-128, </pages> <year> 1991. </year>
Reference: [40] <author> George Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: If shape is sparsely defined, then texture mapping or sparse data interpolation techniques can be employed to create the necessary pixelwise level representation. Example sparse data interpolation techniques include using splines (Litwinowicz and Williams [24], Wol-berg <ref> [40] </ref>), radial basis functions (Reisfeld, Arad, and Yeshurun [31]), and inverse weighted distance metrics (Beier and Neely [5]).
Reference: [41] <author> Alan L. Yuille, Peter W. Hallinan, and David S. Co-hen. </author> <title> Feature extraction from faces using deformable templates. </title> <journal> International Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 99-111, </pages> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Craw and Cameron [18] manually locate their features. Shackelton and Welsh [33], who focus on eye images, use the deformable template approach of Yuille, Cohen, and Hallinan <ref> [41] </ref> to locate eye features. However, for 19/60 of their example eye images, feature localization is either rated as "poor" or "no fit". Note that in both of these approaches, computation of the shape and texture components have been separated, with shape being computed first.
References-found: 41

