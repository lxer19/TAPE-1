URL: ftp://ftp.ai.sri.com/pub/papers/leclerc-ijcv89.ps.gz
Refering-URL: http://www.ai.sri.com/~leclerc/mdl-segmentation/
Root-URL: 
Title: Constructing Simple Stable Descriptions for Image Partitioning  
Author: By: Yvan G. Leclerc 
Address: Menlo Park, CA 94025  
Affiliation: Artificial Intelligence Center SRI International,  
Date: April 10, 1994  
Note: FINAL SUBMISSION TO IJCV  Support for this work was provided by the Defense Advanced Research Projects Agency under contract MDA903-86-C-0084.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P.J. Besl and R.C. Jain, </author> <title> "Segmentation through variable-order surface fitting," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 10(2), </volume> <pages> pp. 167-192, </pages> <year> 1988. </year>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The works of Besl and Jain <ref> [1] </ref>, Grimson and Pavlidis [10], Langridge [12], Lee and Pavlidis [17], and Terzopoulos [27] view the problem as one of smoothing (or regularization) with embedded discontinuities, but for which the discontinuities are first "detected" in some way from the data, with perhaps some attempt at improving the results iteratively.
Reference: [2] <author> T.O. Binford, </author> <title> "Inferring surfaces from images," </title> <journal> Artificial Intelligence, </journal> <volume> vol. </volume> <pages> 17(1-3), pp. 205-244, </pages> <year> 1981. </year>
Reference: [3] <author> A. Blake, </author> <title> "Comparison of the efficiency of deterministic and stochastic algorithms for visual reconstruction," </title> <journal> submitted to IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <month> August </month> <year> 1987. </year>
Reference-contexts: Furthermore, because of the Kronecker delta term, L (u) has many local minima. Thus, standard descent-based optimization techniques are useless. Also, the simulated-annealing style of algorithms exemplified in Geman and Geman [8] are inappropriate, because the time complexity is much too high for this type of function <ref> [3] </ref>. Intuitively, the reason that stochastic gradient-descent algorithms are inappropriate for this particular objective function is that the function has extremely narrow (in fact, infinitesimally narrow) valleys, so that even stochastic sampling of the surface provides no guidance for the search.
Reference: [4] <author> A. Blake and A. Zisserman, </author> <title> Visual Reconstruction, </title> <publisher> MIT Press: </publisher> <address> Cambridge, MA, </address> <year> 1987. </year> <note> Leclerc, final IJCV April 10, 1994 48 </note>
Reference-contexts: It belongs to a class of optimization techniques generally called continuation methods [6, 29]. This algorithm is similar in spirit to the algorithm described in Blake and Zisserman <ref> [4] </ref> as the "graduated nonconvexity," or GNC algorithm. <p> Far from the boundaries of the image, the Green's function (or impulse response function) for this equation is <ref> [4] </ref> G (x; x 0 ; ) = 2 2 K 0 jx x 0 j ! Thus, far from the image boundaries, local minima in L correspond approximately to the convolution of the image data with G (x; x 0 ; ), which is a strictly positive circularly symmetric function <p> Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> Perhaps the closest in spirit to the work presented here are the excellent book and papers of Blake and Zisserman <ref> [4] </ref>, Marroquin et al. [19], and Mumford and Shah [20]. In these works, the problem is posed as an optimization problem that resembles ours, in kind, but one in which the data are weighted uniformly, in essence independently of the data. <p> Furthermore, no method of determining the appropriate amount of smoothing (i.e., the order of the smoothing term in their cost functionals) is mentioned. The advantage of these simplifications, however, is that the authors of <ref> [4, 20] </ref> were able to prove that their algorithms found the global minimum for restricted classes of one-dimensional signals, something that has not been possible yet for our method. No proofs were given for two-dimensional signals.
Reference: [5] <author> J.F. Canny, </author> <title> "A computational approach to edge detection," </title> <journal> IEEE Trans. Pattern Anal ysis and Machine Intelligence, </journal> <volume> vol. 8(6), </volume> <pages> pp. 679-698, </pages> <year> 1986. </year>
Reference-contexts: This is in contradistinction to the output of the Canny edge detector <ref> [5] </ref>.
Reference: [6] <author> G. Dahlquist and A. Bjorck, </author> <title> Numerical Methods, </title> <editor> N. Anderson (trans.), </editor> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: Instead, I have devised an algorithm that yields something very close or equal to the optimal solution for a large class of inputs. It belongs to a class of optimization techniques generally called continuation methods <ref> [6, 29] </ref>. This algorithm is similar in spirit to the algorithm described in Blake and Zisserman [4] as the "graduated nonconvexity," or GNC algorithm.
Reference: [7] <author> P. Fua and A.J. Hanson, </author> <title> "Generic feature extraction using probability-based objective functions," </title> <note> submitted to Machine Vision and Applications, </note> <year> 1988. </year>
Reference: [8] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6(6), </volume> <pages> pp. 721-741, </pages> <year> 1984. </year>
Reference-contexts: Furthermore, because of the Kronecker delta term, L (u) has many local minima. Thus, standard descent-based optimization techniques are useless. Also, the simulated-annealing style of algorithms exemplified in Geman and Geman <ref> [8] </ref> are inappropriate, because the time complexity is much too high for this type of function [3].
Reference: [9] <author> M.P. Georgeff and C.S. Wallace, </author> <title> "A general selection criterion for induction inference," </title> <type> SRI Technical Note 372, </type> <institution> SRI International, </institution> <address> Menlo Park, CA, </address> <year> 1985. </year>
Reference: [10] <author> W.E.L. Grimson and T. Pavlidis, </author> <title> "Discontinuity detection for visual surface recon struction," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 30, </volume> <pages> pp. 316-330, </pages> <year> 1985. </year>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The works of Besl and Jain [1], Grimson and Pavlidis <ref> [10] </ref>, Langridge [12], Lee and Pavlidis [17], and Terzopoulos [27] view the problem as one of smoothing (or regularization) with embedded discontinuities, but for which the discontinuities are first "detected" in some way from the data, with perhaps some attempt at improving the results iteratively.
Reference: [11] <author> R.M. Haralick, </author> <title> "Digital step edges from zero crossings of second directional derivatives," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 1, </volume> <pages> pp. 58-68, </pages> <year> 1984. </year>
Reference: [12] <author> D.J. Langridge, </author> <title> "Detection of discontinuities in the first derivatives of surfaces," Com puter Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 27, </volume> <pages> pp. 291-308, </pages> <year> 1984. </year>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The works of Besl and Jain [1], Grimson and Pavlidis [10], Langridge <ref> [12] </ref>, Lee and Pavlidis [17], and Terzopoulos [27] view the problem as one of smoothing (or regularization) with embedded discontinuities, but for which the discontinuities are first "detected" in some way from the data, with perhaps some attempt at improving the results iteratively.
Reference: [13] <author> K.I. </author> <title> Laws, "Textured Image Segmentation," </title> <type> Ph. D. Thesis, Report USCIPI 940, </type> <institution> Image Processing Institute, U. Southern California, </institution> <address> Los Angelos, CA, </address> <year> 1980. </year> <note> Leclerc, final IJCV April 10, 1994 49 </note>
Reference: [14] <author> Y.G. Leclerc, </author> <title> "Capturing the local structure of image discontinuities in two dimen sions," </title> <booktitle> in Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San Francisco, CA, </address> <month> June, </month> <year> 1985, </year> <pages> pp. 34-38. </pages>
Reference: [15] <author> Y.G. Leclerc, </author> <title> "The Local Structure of Image Intensity Discontinuities," </title> <publisher> Ph. </publisher> <address> D. </address> <institution> disser tation, McGill University, Montreal, Quebec, Canada, </institution> <note> in preparation. </note>
Reference-contexts: of the bifurcations that may occur along the path of the local minimum, Leclerc, final IJCV April 10, 1994 25 and a direct comparison of the results of this continuation method with the global optimum when n or m is one (where a dynamic-programming solution is feasible) is presented in <ref> [15] </ref>. Briefly, the comparison shows that for sufficiently large signal-to-noise ratios (where sufficiently large is a function of the distance between discontinuities), the continuation method always finds the global minimum.
Reference: [16] <author> Y.G. Leclerc and S.W. Zucker, </author> <title> "The local structure of image discontinuities in one dimension," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 9(3), </volume> <pages> pp. 341-355, </pages> <year> 1987. </year>
Reference: [17] <author> D. Lee and T. Pavlidis, </author> <title> "One-dimensional regularization with discontinuities," </title> <booktitle> in Proc. First International Conference on Computer Vision, </booktitle> <address> London, England, </address> <year> 1987, </year> <pages> pp. 572-577. </pages>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The works of Besl and Jain [1], Grimson and Pavlidis [10], Langridge [12], Lee and Pavlidis <ref> [17] </ref>, and Terzopoulos [27] view the problem as one of smoothing (or regularization) with embedded discontinuities, but for which the discontinuities are first "detected" in some way from the data, with perhaps some attempt at improving the results iteratively.
Reference: [18] <author> D.G. Luenberger, </author> <title> Linear and Nonlinear Programming (second edition), </title> <publisher> Addison Wesley: </publisher> <address> Menlo Park, CA, </address> <year> 1984. </year>
Reference-contexts: Experimental results for larger images are discussed in the next section. Although any iterative descent algorithm can be used for the continuation method (see, for example, the wide variety described in Luenberger's excellent book <ref> [18] </ref>), the following algorithm has proven to be quite efficient for the objective function examined here.
Reference: [19] <author> J. Marroquin, S. Mitter, and T. Poggio, </author> <title> "Probabilistic solution of ill-posed problems in computational vision," </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 82(397), </volume> <pages> pp. 76-89, </pages> <year> 1987. </year>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> Perhaps the closest in spirit to the work presented here are the excellent book and papers of Blake and Zisserman [4], Marroquin et al. <ref> [19] </ref>, and Mumford and Shah [20]. In these works, the problem is posed as an optimization problem that resembles ours, in kind, but one in which the data are weighted uniformly, in essence independently of the data.
Reference: [20] <author> D. Mumford and J. Shah, </author> <title> "Boundary detection by minimizing functionals, I," </title> <booktitle> in Proc. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San Francisco, CA, </address> <year> 1985, </year> <pages> pp. 22-26. </pages>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> Perhaps the closest in spirit to the work presented here are the excellent book and papers of Blake and Zisserman [4], Marroquin et al. [19], and Mumford and Shah <ref> [20] </ref>. In these works, the problem is posed as an optimization problem that resembles ours, in kind, but one in which the data are weighted uniformly, in essence independently of the data. <p> Furthermore, no method of determining the appropriate amount of smoothing (i.e., the order of the smoothing term in their cost functionals) is mentioned. The advantage of these simplifications, however, is that the authors of <ref> [4, 20] </ref> were able to prove that their algorithms found the global minimum for restricted classes of one-dimensional signals, something that has not been possible yet for our method. No proofs were given for two-dimensional signals.
Reference: [21] <author> R. Ohlander, K. Price, and D.R. Reddy, </author> <title> "Picture segmentation using a recursive region splitting method," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> vol. 8(3), </volume> <pages> pp. 313-333, </pages> <year> 1978. </year> <note> Leclerc, final IJCV April 10, 1994 50 </note>
Reference: [22] <author> T. Poggio, V. Torre, and C. Koch, </author> <title> "Computational vision and regularization theory," </title> <journal> Nature, </journal> <volume> vol. 317, </volume> <year> 1985. </year>
Reference: [23] <author> J. Rissanen, </author> <title> "A universal prior for integers and estimation by minimum description length," </title> <journal> The Annals of Statistics, </journal> <volume> vol. 11(2), </volume> <pages> pp. 416-431, </pages> <year> 1983. </year>
Reference-contexts: Rissanen <ref> [23] </ref> has shown that such a scheme not only produces intuitively pleasing results for observations of real-world processes (when the underlying process is actually unknown), but it is also a MAP solution for a particular class of prior distributions on the parameters of the polynomials. 2 An optimal descriptive language is <p> the M i that minimizes jL c (DjM i )j + jL m (M i )j : When we know the prior probabilities of the thing we are describing, information theory tells us that we can design an optimal descriptive language that minimizes the expected number of bits per description <ref> [23] </ref>. For such optimal languages, the number of bits in the description equals the negative base-two logarithm of the probability of the thing being described.
Reference: [24] <author> J. Rissanen, </author> <title> "Minimum-Description-Length Principle," in Encyclopedia of Statistical Sciences, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <address> New York, NY, </address> <booktitle> vol. </booktitle> <volume> 5, </volume> <pages> pp. 523-527, </pages> <year> 1987. </year>
Reference: [25] <author> P. Saint-Marc and G. Medioni, </author> <title> "Adaptive Smoothing for feature extraction," </title> <booktitle> in Proc. DARPA Image Understanding Workshop, </booktitle> <address> Boston, MA, </address> <year> 1988, </year> <pages> pp. 1100-1113. </pages>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The heart of the problem|formally including the cost of introducing discontinuities as part of the optimization process|is missing (although Terzopoulos [27] devotes some attention to the problem). Finally, Saint-Marc and Medioni <ref> [25] </ref> present a simple adaptive smoothing technique that bears a certain resemblance to our special case of a piecewise-constant underlying image with Leclerc, final IJCV April 10, 1994 46 known variance.
Reference: [26] <author> D. Terzopoulos, </author> <title> "Multilevel computational processes for visual surface reconstruction," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 24, </volume> <pages> pp. 52-96, </pages> <year> 1983. </year>
Reference-contexts: t+1 i;i c t One advantage of using an explicit representation of the Taylor coefficients can now be seen: the minimization procedure requires information only from the immediate neighbors of an element, rather than information from elements within a given radius, as required for implicit finite-element representations such as in <ref> [26] </ref>. This is especially advantageous for massively parallel architectures in which the communication cost between nonadjacent units is high.
Reference: [27] <author> D. Terzopoulos, </author> <title> "Regularization of inverse visual problems involving discontinuities," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 8(4), </volume> <pages> pp. 413-424, </pages> <year> 1986. </year>
Reference-contexts: Insert Fig. 7 and Fig. 8 about here. 8 Relation to Previous Work Much work has been done recently on the problem of reconstructing piecewise-smooth surfaces in one or more dimensions, given corrupted samples of the surface <ref> [1, 4, 10, 12, 17, 19, 20, 25, 27] </ref>. There are several especially difficult aspects to the problem. The first is to determine automatically the appropriate degree of smoothness of the surface as a function of the given data. <p> The works of Besl and Jain [1], Grimson and Pavlidis [10], Langridge [12], Lee and Pavlidis [17], and Terzopoulos <ref> [27] </ref> view the problem as one of smoothing (or regularization) with embedded discontinuities, but for which the discontinuities are first "detected" in some way from the data, with perhaps some attempt at improving the results iteratively. <p> The heart of the problem|formally including the cost of introducing discontinuities as part of the optimization process|is missing (although Terzopoulos <ref> [27] </ref> devotes some attention to the problem). Finally, Saint-Marc and Medioni [25] present a simple adaptive smoothing technique that bears a certain resemblance to our special case of a piecewise-constant underlying image with Leclerc, final IJCV April 10, 1994 46 known variance.
Reference: [28] <author> A.W. Witkin, </author> <title> "Scale space filtering," </title> <booktitle> in Proc. Eighth International Joint Conference Artificial Intelligence, </booktitle> <address> Karlsruhe, West Germany, </address> <year> 1983, </year> <pages> pp. 1019-1021. </pages>
Reference-contexts: Thus, the continuation method creates a kind of "scale space" representation of the objective function L (u) (in analogy to Witkin's scale-space representation of a signal <ref> [28] </ref>) and tracks a local minimum from the coarsest scale (where there is only one local minimum) to the finest scale (where there are many).

References-found: 28

