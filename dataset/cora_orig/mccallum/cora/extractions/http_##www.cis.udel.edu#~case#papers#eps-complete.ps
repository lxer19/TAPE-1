URL: http://www.cis.udel.edu/~case/papers/eps-complete.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: fcase,surajg@cis.udel.edu  sanjay@iscs.nus.sg  
Phone: 2  
Title: Control Structures in Hypothesis Spaces: The Influence on Learning  
Author: John Case and Sanjay Jain and Mandayam Suraj 
Address: 19716, USA  Singapore 119260 Republic of Singapore  
Affiliation: 1 Department of Computer and Information Sciences University of Delaware Newark, DE  Department of Information Systems and Computer Science National University of Singapore  
Abstract: In any learnability setting, hypotheses are conjectured from some hypothesis space. Studied herein are the effects on learnability of the presence or absence of certain control structures in the hypothesis space. First presented are control structure characterizations of some rather specific but illustrative learnability results. Then presented are the main theorems. Each of these characterizes the invariance of a learning class over hypothesis space V (and a little more about V ) as: V has suitable instances of all denotational control structures.
Abstract-found: 1
Intro-found: 1
Reference: [All78] <author> J. Allen. </author> <title> Anatomy of Lisp. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: In this paper we freely use Church's lambda notation <ref> [Chu41, Rog67, All78] </ref> to define functions: N ! N . For example, x x + 1 denotes the function that maps each x 2 N to x + 1. 2.1 Computable Recognizing Systems As we noted in Section 1, in any learnability setting, hypotheses are conjectured from some hypothesis space.
Reference: [Ang80a] <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 46-62, </pages> <year> 1980. </year>
Reference-contexts: Uniformly decidable classes of languages are ubiquitous in computational learning theory [ZL95] and are many times also called indexed families of recursive languages. Important examples of such classes are the class of all context free languages [HU79] and the class of all pattern languages <ref> [Ang80b, Ang80a] </ref>. Other examples, of relevance to the present section, are FiniteSets, Co-Init, and Co-Single. Next, we define a class of control structures useful for uniformly decidable classes. Just after that we provide Theorem 25 which generalizes Theorem 22 above.
Reference: [Ang80b] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: Uniformly decidable classes of languages are ubiquitous in computational learning theory [ZL95] and are many times also called indexed families of recursive languages. Important examples of such classes are the class of all context free languages [HU79] and the class of all pattern languages <ref> [Ang80b, Ang80a] </ref>. Other examples, of relevance to the present section, are FiniteSets, Co-Init, and Co-Single. Next, we define a class of control structures useful for uniformly decidable classes. Just after that we provide Theorem 25 which generalizes Theorem 22 above.
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: Then one can effectively construct a c.r.s. V such that (a) M TxtEx V -identifies L, and (b) For infinite L 2 L, M TxtMinEx V -identifies L. Proof of Lemma 45. Note that one can, effectively from M , construct a rearrangement independent and order independent <ref> [Ful90, BB75, Cas96] </ref> machine M 0 such that TxtEx (M ) TxtEx (M 0 ). Thus assume, without loss of generality, that M is rearrangement independent and order independent. Let T j denote a text for W j , which can be obtained effectively from j.
Reference: [Blu67] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: Then W p is the set N recognized [HU79, Wei87] by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts. Let denote a step-counting Blum complexity measure for ' p <ref> [Blu67, DSW94] </ref>. We let def n undefined otherwise. We then let W p;s be the domain of ' p;s . The set of all recursively enumerable languages is denoted by E. L and S, with or without decorations, range over E. <p> For a set L, we use L to denote the characteristic function of L, the function which is 1 on L and 0 off L. L denotes complement of L, i.e., N L. The quantifiers ` 1 1 9 ' essentially from <ref> [Blu67] </ref>, mean `for all but finitely many' and `there exist infinitely many', respectively. We next define a limiting-computable function. First, we define lim t!1 h (x; t) = y if ( 8 t)[h (x; t) = y]; undefined otherwise.
Reference: [BS94] <author> G. Baliga and A. Shende. </author> <title> Learning-theoretic perspectives of acceptable num-berings. </title> <booktitle> In Third International Symposium on Artificial Intelligence and Mathematics, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: We consider herein general purpose systems V for the entire class of r.e. languages, which systems may or may not have available particular control structures. <ref> [BS94] </ref> considered, in effect, whether a particular learnability result P characterized the general purpose hypothesis spaces having available all possible control structures; they discovered their particular P failed very badly to do so.
Reference: [Cas96] <author> J. </author> <title> Case. </title> <booktitle> The power of vacillation in language learning. Technical Report LP-96-08, Logic, Philosophy and Linguistics Series of the Institute for Logic, Language and Computation, </booktitle> <institution> University of Amsterdam, </institution> <year> 1996. </year> <note> To appear revised in SIAM Journal on Computing. </note>
Reference-contexts: Then one can effectively construct a c.r.s. V such that (a) M TxtEx V -identifies L, and (b) For infinite L 2 L, M TxtMinEx V -identifies L. Proof of Lemma 45. Note that one can, effectively from M , construct a rearrangement independent and order independent <ref> [Ful90, BB75, Cas96] </ref> machine M 0 such that TxtEx (M ) TxtEx (M 0 ). Thus assume, without loss of generality, that M is rearrangement independent and order independent. Let T j denote a text for W j , which can be obtained effectively from j.
Reference: [Chu41] <author> A. Church. </author> <title> The Calculi of Lambda Conversion. </title> <publisher> Princeton Univ. Press, </publisher> <year> 1941. </year>
Reference-contexts: In this paper we freely use Church's lambda notation <ref> [Chu41, Rog67, All78] </ref> to define functions: N ! N . For example, x x + 1 denotes the function that maps each x 2 N to x + 1. 2.1 Computable Recognizing Systems As we noted in Section 1, in any learnability setting, hypotheses are conjectured from some hypothesis space.
Reference: [CSM92] <author> D. Kedzier C. Sammut, S. Hurst and D. Michie. </author> <title> Learning to fly. </title> <editor> In D. Sleeman and P. Edwards, editors, </editor> <booktitle> Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: mind of seeing if certain control structures (in general purpose systems) were necessary and sufficient to maintain the invariance (compared with a system with all possible control structures available) of stan 3 For example, with the latter one can, nonetheless, train an autopilot from flight simulator data on real pilots <ref> [CSM92] </ref>. dard learning classes. We haven't quite achieved that, and our paper is an initial progress report on the endeavor. 4 In Section 2.1 we present the basics of the sorts of general purpose recognizing systems we consider.
Reference: [DSW94] <author> M. Davis, R. Sigal, and E. Weyuker. </author> <title> Computability, Complexity, and Languages. </title> <publisher> Academic Press, </publisher> <address> second edition, </address> <year> 1994. </year>
Reference-contexts: Then W p is the set N recognized [HU79, Wei87] by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts. Let denote a step-counting Blum complexity measure for ' p <ref> [Blu67, DSW94] </ref>. We let def n undefined otherwise. We then let W p;s be the domain of ' p;s . The set of all recursively enumerable languages is denoted by E. L and S, with or without decorations, range over E. <p> Then, from the definition of a.r.s. (Definition 4), W V . Suppose t computable such that t : W V . Then, W i = f0g , t (i) = 0, and hence, fi j W i = f0gg is recursive, a contradiction to Rice's Theorem <ref> [Rog67, DSW94] </ref>. By replacing f0g in the above proof to fjg, for arbitrary j 2 N , we obtain 12 An explanation for this and the next contrasting result is that in learning computable functions: there are no finite objects to be learned. Theorem 36.
Reference: [FKS94] <author> R. Freivalds, M. Karpinski, and C. H. Smith. </author> <title> Co-learning of total recursive functions. </title> <booktitle> In Proceedings of the Seventh Annual Conference on Computational Learning Theory, </booktitle> <address> New Brunswick, New Jersey, </address> <pages> pages 190-197. </pages> <address> ACM-Press, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: We then have the following Corollary 32. There exists a c.r.s. V such that Co-Single 62 TxtEx V . In another vein, Theorem 26 gives us the following Corollary 33. (9L j card (L) infinite)(8 c.r.s. V )[L 2 TxtEx V ]. The immediately above corollary contrasts with <ref> [FKS94, Lemmas 25 & 26] </ref> which yield programming systems (for the partial computable functions) with respect to which one cannot learn in the limit any infinite class of (total) com-putable functions. 12 As an even more contrasting result, an easy generalization of the proof of Theorem 26 gives, Theorem 34.
Reference: [FKS95] <author> R Freivalds, E. Kinber, and C. H. Smith. </author> <title> On the intrinsic complexity of learning. </title> <editor> In Paul Vitanyi, editor, </editor> <booktitle> Proceedings of the Second European Conference on Computational Lear ning Theory, </booktitle> <pages> pages 154-169. </pages> <publisher> Springer-Verlag, </publisher> <month> March </month> <year> 1995. </year> <booktitle> Lecture Notes in Artificial Intelligence 904. </booktitle>
Reference-contexts: From Theorem 26 and Theorem 28 below, we will see that FiniteSets can be TxtEx-identified in all c.r.s.'s, but that there is a c.r.s. in which Co-Init cannot be TxtEx-identified. From this perspective, then, FiniteSets is easier than Co-Init. By contrast, with respect to an intrinsic complexity notion from <ref> [FKS95, JS94] </ref>, FiniteSets is harder than Co-Init for TxtEx-identification. Theorem 26. For all c.r.s.'s V , FiniteSets 2 TxtEx V . Proof. Suppose V is a c.r.s. We define a machine M such that M TxtEx V - identifies FiniteSets.
Reference: [FKW82] <author> R. Freivalds, E. Kinber, and R. Wiehagen. </author> <title> Inductive inference and computable one-one numberings. </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, </journal> <volume> 28 </volume> <pages> 463-479, </pages> <year> 1982. </year>
Reference-contexts: Theorem 49 is a consequence of a straightforward modification of the proof of Theorem 4 from <ref> [FKW82] </ref>. Theorem 49. For all Friedberg c.r.s.'s U , TxtEx U TxtEx. The next result shows us that a c.r.s. V is limiting-acceptable just in case one can computably (or equivalently, limiting-computably) translate TxtEx-identifying machines to TxtEx V -identifying machines. Theorem 50. The following three clauses are equivalent 1.
Reference: [FKW84] <author> R. Freivalds, E. Kinber, and R. Wiehagen. </author> <title> Connections between iden-tifying functionals, standardizing operations, and computable number-ings. </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, </journal> <volume> 30 </volume> <pages> 145-164, </pages> <year> 1984. </year>
Reference-contexts: Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines [Rog58, Rog67, Ric80, Ric81, Roy87]. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in <ref> [Wie78, FKW84] </ref>, not in terms of control structures. denote the domain of ' p . Then W p is the set N recognized [HU79, Wei87] by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts.
Reference: [Fri58] <author> R. M. Friedberg. </author> <title> Three theorems on recursive enumeration. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23(3) </volume> <pages> 309-316, </pages> <month> September </month> <year> 1958. </year>
Reference-contexts: Definition 6. Friedberg computable recognizing systems are (by definition) c.r.s.'s in which there exists exactly one recognizer for each r.e. set. Such systems were first shown to exist by Friedberg <ref> [Fri58] </ref>, and they are useful in providing counterexamples.
Reference: [Ful90] <author> M. Fulk. </author> <title> Prudence and other conditions on formal language learning. </title> <journal> Information and Computation, </journal> <volume> 85 </volume> <pages> 1-11, </pages> <year> 1990. </year>
Reference-contexts: Then one can effectively construct a c.r.s. V such that (a) M TxtEx V -identifies L, and (b) For infinite L 2 L, M TxtMinEx V -identifies L. Proof of Lemma 45. Note that one can, effectively from M , construct a rearrangement independent and order independent <ref> [Ful90, BB75, Cas96] </ref> machine M 0 such that TxtEx (M ) TxtEx (M 0 ). Thus assume, without loss of generality, that M is rearrangement independent and order independent. Let T j denote a text for W j , which can be obtained effectively from j.
Reference: [Gol67] <author> E. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: TxtEx V identifies L def , (8 texts T for L)(9i j V i = L)[M (T )# = i]. (b) For all M , TxtEx V (M ) = fL j M TxtEx V -identifies Lg. (c) TxtEx V = fL j (9M )[L TxtEx V (M )]g: Gold <ref> [Gol67] </ref> introduced the criterion we call TxtEx W . We next introduce one-shot language identification for which the first pro gram conjectured must be correct. Definition 8.
Reference: [HU79] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: We treat (see Section 2.2) mostly the standard learning criteria of learning in the limit and learning in one-shot, recognizers (or grammars <ref> [HU79, Wei87] </ref>) for r.e. languages | from text (or positive information). In Section 2.3 we provide sufficient background material from [Ric80, Ric81, Roy87] about control structures in general purpose programming systems. In Section 3 we first present control structure characterizations of some rather specific but illustrative learnability results. <p> Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . Then W p is the set N recognized <ref> [HU79, Wei87] </ref> by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts. Let denote a step-counting Blum complexity measure for ' p [Blu67, DSW94]. We let def n undefined otherwise. We then let W p;s be the domain of ' p;s . <p> Uniformly decidable classes of languages are ubiquitous in computational learning theory [ZL95] and are many times also called indexed families of recursive languages. Important examples of such classes are the class of all context free languages <ref> [HU79] </ref> and the class of all pattern languages [Ang80b, Ang80a]. Other examples, of relevance to the present section, are FiniteSets, Co-Init, and Co-Single. Next, we define a class of control structures useful for uniformly decidable classes. Just after that we provide Theorem 25 which generalizes Theorem 22 above.
Reference: [JS94] <author> S. Jain and A. Sharma. </author> <title> On the intrinsic complexity of language identification. </title> <booktitle> In Proceedings of the Seventh Annual Conference on Computational Learning Theory, </booktitle> <address> New Brunswick, New Jersey, </address> <pages> pages 278-286. </pages> <address> ACM-Press, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: From Theorem 26 and Theorem 28 below, we will see that FiniteSets can be TxtEx-identified in all c.r.s.'s, but that there is a c.r.s. in which Co-Init cannot be TxtEx-identified. From this perspective, then, FiniteSets is easier than Co-Init. By contrast, with respect to an intrinsic complexity notion from <ref> [FKS95, JS94] </ref>, FiniteSets is harder than Co-Init for TxtEx-identification. Theorem 26. For all c.r.s.'s V , FiniteSets 2 TxtEx V . Proof. Suppose V is a c.r.s. We define a machine M such that M TxtEx V - identifies FiniteSets. <p> So, V j= lim-proj. ((): Suppose TxtEx V = TxtEx W and h is a limiting-computable instance of proj in V (as witnessed by computable h 2 (:; :)). We use the class L TxtEx from <ref> [JS94] </ref>, which we describe below. This class L TxtEx is shown to be in TxtEx W in [JS94]. Let S L = fhx; ji j x 2 Lg. Then, L TxtEx = fS j L j L 2 TxtEx (M j )g. <p> We use the class L TxtEx from <ref> [JS94] </ref>, which we describe below. This class L TxtEx is shown to be in TxtEx W in [JS94]. Let S L = fhx; ji j x 2 Lg. Then, L TxtEx = fS j L j L 2 TxtEx (M j )g. Let M be a learning machine that TxtEx V -identifies L TxtEx .
Reference: [Kum89] <author> M. Kummer. </author> <title> A note on direct sums of friedbergnumberings. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 54(3) </volume> <pages> 1009-1010, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The direct sum of an r.e. sequence of c.r.s.'s, V 0 ; V 1 ; : : : is defined to be the c.r.s. V such that for all i; j, V hi;ji = V i Finally, by an straightforward modification of the proof of the main theorem in <ref> [Kum89] </ref>, we get the following Lemma 48. The direct sum of an r.e. sequence of Friedberg c.r.s.'s is never limiting-acceptable. Proof of Theorem 40.
Reference: [Mar89] <author> Y. Marcoux. </author> <title> Composition is almost as good as s-1-1. </title> <booktitle> In Proceedings, Structure in Complexity Theory-Fourth Annual Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: Similarly, in typical, practical programming languages, one has instances of while-loop and if-then-else which are not only computable, but, since they can be realized by simple substitution of the constituent programs into some fixed template, they are computable in linear-time <ref> [Roy87, Mar89] </ref>. The learning criteria we consider in Section 3 below feature converging to a correct hypothesis in the limit. Hence, it is not surprising that only limiting-computable instances of the control structures are relevant there. However, in Section 4 further below, computable instances are sometimes relevant.
Reference: [Odi89] <author> P. Odifreddi. </author> <title> Classical Recursion Theory, </title> <booktitle> volume 125 of Studies in Logic and the Foundations of Mathematics. </booktitle> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: U and a limiting recursive function f such that, (8i j V i 62 (fN g [ Init) ^ i = MinGram V (V i ))[U f (i) = V i ]. Proof. Odifreddi's construction ( <ref> [Odi89, Theorem II.5.22, Page 230] </ref>) proves this lemma. (Lemma 41) Proposition 42. Suppose L 0 is finite, U is a c.r.s., L [ L 0 2 TxtEx, and L 2 TxtEx U . Then L [ L 0 2 TxtEx U . Proof of Proposition 42. Suppose the hypothesis.
Reference: [OSW86] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: 1 Introduction In any learnability setting, hypotheses are conjectured from some hypothesis space, for example, in <ref> [OSW86] </ref> from general purpose programming systems, in [ZL95, Wie78] from subrecursive systems, and in [Qui92] from very simple classes of classificatory decision trees. 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces [ZL95]. <p> We need the following Lemma 10 (Lemma 4.2.2B of <ref> [OSW86] </ref>). There exists a computable enumeration M 0 ; M 1 ; : : : of (total) learning machines such that, for each learning criterion I used in the present paper, for every L 2 I, L is I-identified by some machine in this enumeration.
Reference: [Qui92] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In any learnability setting, hypotheses are conjectured from some hypothesis space, for example, in [OSW86] from general purpose programming systems, in [ZL95, Wie78] from subrecursive systems, and in <ref> [Qui92] </ref> from very simple classes of classificatory decision trees. 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces [ZL95].
Reference: [Ric80] <author> G. Riccardi. </author> <title> The Independence of Control Structures in Abstract Programming Systems. </title> <type> PhD thesis, </type> <institution> SUNY Buffalo, </institution> <year> 1980. </year>
Reference-contexts: We treat (see Section 2.2) mostly the standard learning criteria of learning in the limit and learning in one-shot, recognizers (or grammars [HU79, Wei87]) for r.e. languages | from text (or positive information). In Section 2.3 we provide sufficient background material from <ref> [Ric80, Ric81, Roy87] </ref> about control structures in general purpose programming systems. In Section 3 we first present control structure characterizations of some rather specific but illustrative learnability results. In the remainder we consider, for the control structures involved, whether or not they must be available in any hypothesis space. <p> Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines <ref> [Rog58, Rog67, Ric80, Ric81, Roy87] </ref>. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . <p> Moreover, this enumeration satisfies an S-m-n property: given a description, computable in x, of the behavior of a machine M , one can computably find a machine M f (x) whose I-identification behavior is identical to that of M . 2.3 Control Structures in C.R.S.'s <ref> [Ric80, Ric81, Roy87] </ref> show how to define control structures in the context of programming systems (effective numberings) for the partial computable functions [Rog58]. These ideas can be straightforwardly adapted to the context of c.r.s.'s. <p> The extensional control structure determined by (m; n; fi) is (by definition) f (V; f ) j V is a c.r.s. ^ f : N n ! N is an instance of the extensional control structure in V determined by (m; n; fi)g. 7 <ref> [Ric80, Ric81, Roy87] </ref> provide an even more general type of control structure called intensional (synonym: connotational). <p> In <ref> [Ric80, Ric81, Roy87] </ref> we see that control structures in the context of programming systems for the partial computable functions are determined instead by recursive operators [Rog67]. 10 Clearly, p 1 ; : : : ; p m are program arguments, and x 1 ; : : : ; x nm are <p> The learning criteria we consider in Section 3 below feature converging to a correct hypothesis in the limit. Hence, it is not surprising that only limiting-computable instances of the control structures are relevant there. However, in Section 4 further below, computable instances are sometimes relevant. Case showed <ref> [Ric80, Roy87] </ref> that the acceptable programming systems (for the partial computable functions) are characterized by having a computable instance of each control structure. This result easily carries over to a corresponding control structure characterization of acceptable c.r.s.'s. It is a straightforward lift to show the following Theorem 13.
Reference: [Ric81] <author> G. Riccardi. </author> <title> The independence of control structures in abstract programming systems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 107-143, </pages> <year> 1981. </year>
Reference-contexts: We treat (see Section 2.2) mostly the standard learning criteria of learning in the limit and learning in one-shot, recognizers (or grammars [HU79, Wei87]) for r.e. languages | from text (or positive information). In Section 2.3 we provide sufficient background material from <ref> [Ric80, Ric81, Roy87] </ref> about control structures in general purpose programming systems. In Section 3 we first present control structure characterizations of some rather specific but illustrative learnability results. In the remainder we consider, for the control structures involved, whether or not they must be available in any hypothesis space. <p> Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines <ref> [Rog58, Rog67, Ric80, Ric81, Roy87] </ref>. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . <p> Moreover, this enumeration satisfies an S-m-n property: given a description, computable in x, of the behavior of a machine M , one can computably find a machine M f (x) whose I-identification behavior is identical to that of M . 2.3 Control Structures in C.R.S.'s <ref> [Ric80, Ric81, Roy87] </ref> show how to define control structures in the context of programming systems (effective numberings) for the partial computable functions [Rog58]. These ideas can be straightforwardly adapted to the context of c.r.s.'s. <p> The extensional control structure determined by (m; n; fi) is (by definition) f (V; f ) j V is a c.r.s. ^ f : N n ! N is an instance of the extensional control structure in V determined by (m; n; fi)g. 7 <ref> [Ric80, Ric81, Roy87] </ref> provide an even more general type of control structure called intensional (synonym: connotational). <p> In <ref> [Ric80, Ric81, Roy87] </ref> we see that control structures in the context of programming systems for the partial computable functions are determined instead by recursive operators [Rog67]. 10 Clearly, p 1 ; : : : ; p m are program arguments, and x 1 ; : : : ; x nm are
Reference: [Rog58] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines <ref> [Rog58, Rog67, Ric80, Ric81, Roy87] </ref>. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . <p> Definition 2. Suppose V is a c.r.s. For L r.e., MinGram V (L) denotes min (fp j V p = Lg). We define next some interesting senses in which one can translate from one c.r.s. into another. Part (b) of this definition is based on a definition in <ref> [Rog58] </ref>. [ZL95] notes the relevance to learning theory of the sense in part (c). Definition 3. <p> The next definition is also based on a definition in <ref> [Rog58] </ref>. Definition 4. (a) V is an acceptable recognizing system (abbreviated a.r.s.) def (b) V is a limiting-acceptable recognizing system (abbreviated lim-a.r.s.) def is a c.r.s. and (8 c.r.s. U )[U lim V ]. <p> a machine M , one can computably find a machine M f (x) whose I-identification behavior is identical to that of M . 2.3 Control Structures in C.R.S.'s [Ric80, Ric81, Roy87] show how to define control structures in the context of programming systems (effective numberings) for the partial computable functions <ref> [Rog58] </ref>. These ideas can be straightforwardly adapted to the context of c.r.s.'s. We will omit some of the details of this adaptation, but Definition 12 below will provide all that is really essential to the present paper. <p> W , Kleene's S-m-n function [Rog67] essentially provides a computable instance. Definition 15. An instance of the control structure s-1-1 in V is (by definition) a function f such that, for all p and x, V f (p;x) = fy j hx; yi 2 V p g. <ref> [Rog58] </ref> characterized acceptability for programming systems (numberings) of the partial recursive functions in terms of Kleene's S-m-n Theorem. His proof straightforwardly adapts to show the following Theorem 16. (a) For all c.r.s. V , V is acceptable , V j= s-1-1. (b) For all c.r.s.
Reference: [Rog67] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <address> Mc-Graw Hill, New York, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: For sets A and B, A B def = (f2x j x 2 Ag [f2x+1 j x 2 Bg) <ref> [Rog67] </ref>. When iterating the operator, we will assume left-associativity (to avoid excessive parenthesization). <p> For S, a subset of N , card (S) denotes the cardinality of S. max (S) and min (S) denote, respectively, the maximum and minimum of the set S, where max (;) = 0 and min (;) = 1. D x denotes the finite set with canonical index x <ref> [Rog67] </ref>. h; i denotes a fixed pairing function [Rog67], a computable, surjective and injective mapping from N fi N into N . h; i is useful, for example, for speaking of two inputs to a one-input program. f; g; h and t with or without decorations range over total (not necessarily <p> D x denotes the finite set with canonical index x <ref> [Rog67] </ref>. h; i denotes a fixed pairing function [Rog67], a computable, surjective and injective mapping from N fi N into N . h; i is useful, for example, for speaking of two inputs to a one-input program. f; g; h and t with or without decorations range over total (not necessarily computable) functions with arguments and values from N <p> Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines <ref> [Rog58, Rog67, Ric80, Ric81, Roy87] </ref>. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . <p> In this paper we freely use Church's lambda notation <ref> [Chu41, Rog67, All78] </ref> to define functions: N ! N . For example, x x + 1 denotes the function that maps each x 2 N to x + 1. 2.1 Computable Recognizing Systems As we noted in Section 1, in any learnability setting, hypotheses are conjectured from some hypothesis space. <p> Also, instances of each combine two programs (and no data) to form a third (composite) recognizer program. Formally, each control structure for c.r.s.'s is determined by an enumeration operator fi. <ref> [Rog67] </ref> provides an excellent discussion of enumeration operators. 9 As noted earlier, we provide below the definition of extensional (or denotational) control structures only since that is all that is really essential to the present paper. <p> In [Ric80, Ric81, Roy87] we see that control structures in the context of programming systems for the partial computable functions are determined instead by recursive operators <ref> [Rog67] </ref>. 10 Clearly, p 1 ; : : : ; p m are program arguments, and x 1 ; : : : ; x nm are data arguments. f (p 1 ; : : : ; p m ; x 1 ; : : : ; x nm ) is the <p> The first example, s-1-1, is a control structure intuitively for storing a datum x in a recognizing program p, more specifically, for replacing the first of two (coded) input parameters to p by the constant x. In the c.r.s. W , Kleene's S-m-n function <ref> [Rog67] </ref> essentially provides a computable instance. Definition 15. <p> Then, from the definition of a.r.s. (Definition 4), W V . Suppose t computable such that t : W V . Then, W i = f0g , t (i) = 0, and hence, fi j W i = f0gg is recursive, a contradiction to Rice's Theorem <ref> [Rog67, DSW94] </ref>. By replacing f0g in the above proof to fjg, for arbitrary j 2 N , we obtain 12 An explanation for this and the next contrasting result is that in learning computable functions: there are no finite objects to be learned. Theorem 36.
Reference: [Roy87] <author> J. Royer. </author> <title> A Connotational Theory of Program Structure. </title> <booktitle> Lecture Notes in Computer Science 273. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We treat (see Section 2.2) mostly the standard learning criteria of learning in the limit and learning in one-shot, recognizers (or grammars [HU79, Wei87]) for r.e. languages | from text (or positive information). In Section 2.3 we provide sufficient background material from <ref> [Ric80, Ric81, Roy87] </ref> about control structures in general purpose programming systems. In Section 3 we first present control structure characterizations of some rather specific but illustrative learnability results. In the remainder we consider, for the control structures involved, whether or not they must be available in any hypothesis space. <p> Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines <ref> [Rog58, Rog67, Ric80, Ric81, Roy87] </ref>. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . <p> Moreover, this enumeration satisfies an S-m-n property: given a description, computable in x, of the behavior of a machine M , one can computably find a machine M f (x) whose I-identification behavior is identical to that of M . 2.3 Control Structures in C.R.S.'s <ref> [Ric80, Ric81, Roy87] </ref> show how to define control structures in the context of programming systems (effective numberings) for the partial computable functions [Rog58]. These ideas can be straightforwardly adapted to the context of c.r.s.'s. <p> In the present paper, it will suffice for us to consider the extensional <ref> [Roy87] </ref> (synonym: denotational [Sto77]) control structures. 7 Instances of extensional control structures provide a means of forming a composite program from given constituent programs (and/or data), where the I/O behavior of that composite program depends only on the I/O behavior of the constituent programs (and on the data). 8 Clearly, in <p> Also, as noted above, this definition is the obvious analog for c.r.s.'s of the corresponding concepts in <ref> [Roy87] </ref>. Definition 12. (a) Suppose n &gt; 0. Suppose 0 m n. Suppose fi is an enumeration operator. <p> The extensional control structure determined by (m; n; fi) is (by definition) f (V; f ) j V is a c.r.s. ^ f : N n ! N is an instance of the extensional control structure in V determined by (m; n; fi)g. 7 <ref> [Ric80, Ric81, Roy87] </ref> provide an even more general type of control structure called intensional (synonym: connotational). <p> Also, the extensional control structures, as rigorously defined in <ref> [Roy87] </ref>, include ([Roy87, Theorem 2.3.3]) the recursive exten sional control structures under minimal fixed point semantics. 8 So, for example, when applying extensional control structures, the I/O behavior of a composite program cannot generally depend on the number of symbols in or the run-time complexity of a constituent program. 9 Roughly, <p> In <ref> [Ric80, Ric81, Roy87] </ref> we see that control structures in the context of programming systems for the partial computable functions are determined instead by recursive operators [Rog67]. 10 Clearly, p 1 ; : : : ; p m are program arguments, and x 1 ; : : : ; x nm are <p> Similarly, in typical, practical programming languages, one has instances of while-loop and if-then-else which are not only computable, but, since they can be realized by simple substitution of the constituent programs into some fixed template, they are computable in linear-time <ref> [Roy87, Mar89] </ref>. The learning criteria we consider in Section 3 below feature converging to a correct hypothesis in the limit. Hence, it is not surprising that only limiting-computable instances of the control structures are relevant there. However, in Section 4 further below, computable instances are sometimes relevant. <p> The learning criteria we consider in Section 3 below feature converging to a correct hypothesis in the limit. Hence, it is not surprising that only limiting-computable instances of the control structures are relevant there. However, in Section 4 further below, computable instances are sometimes relevant. Case showed <ref> [Ric80, Roy87] </ref> that the acceptable programming systems (for the partial computable functions) are characterized by having a computable instance of each control structure. This result easily carries over to a corresponding control structure characterization of acceptable c.r.s.'s. It is a straightforward lift to show the following Theorem 13. <p> Find a set of control structures S such that TxtEx V = TxtEx , (8s 2 S)[V j= lim-s]. This remains to be done. It would be interesting to get learnability results about control structures in subrecursive hypothesis spaces <ref> [Roy87] </ref>. Are there pure learning-theoretic results completely characterizing each of acceptability and limiting-acceptability? Acknowledgements We would like to thank Ganesh Baliga for helpful discussions, encouragement, and for suggesting some lines of research that, in part, led to the present paper.
Reference: [Sto77] <author> J. Stoy. </author> <title> Denotational Semantics: The Scott-Strachey Approach to Programming Language Theory. </title> <publisher> MIT Press, </publisher> <year> 1977. </year>
Reference-contexts: In the present paper, it will suffice for us to consider the extensional [Roy87] (synonym: denotational <ref> [Sto77] </ref>) control structures. 7 Instances of extensional control structures provide a means of forming a composite program from given constituent programs (and/or data), where the I/O behavior of that composite program depends only on the I/O behavior of the constituent programs (and on the data). 8 Clearly, in the context of
Reference: [Wei87] <author> K. Weihrauch. </author> <title> Computability. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We treat (see Section 2.2) mostly the standard learning criteria of learning in the limit and learning in one-shot, recognizers (or grammars <ref> [HU79, Wei87] </ref>) for r.e. languages | from text (or positive information). In Section 2.3 we provide sufficient background material from [Ric80, Ric81, Roy87] about control structures in general purpose programming systems. In Section 3 we first present control structure characterizations of some rather specific but illustrative learnability results. <p> Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p . Then W p is the set N recognized <ref> [HU79, Wei87] </ref> by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts. Let denote a step-counting Blum complexity measure for ' p [Blu67, DSW94]. We let def n undefined otherwise. We then let W p;s be the domain of ' p;s .
Reference: [Wie78] <author> R. Wiehagen. </author> <title> Characterization problems in the theory of inductive inference. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 62 </volume> <pages> 494-508, </pages> <year> 1978. </year>
Reference-contexts: 1 Introduction In any learnability setting, hypotheses are conjectured from some hypothesis space, for example, in [OSW86] from general purpose programming systems, in <ref> [ZL95, Wie78] </ref> from subrecursive systems, and in [Qui92] from very simple classes of classificatory decision trees. 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces [ZL95]. <p> Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines [Rog58, Rog67, Ric80, Ric81, Roy87]. Let W p 4 [Wie96] quite interestingly characterizes learning criteria invariances, but as in <ref> [Wie78, FKW84] </ref>, not in terms of control structures. denote the domain of ' p . Then W p is the set N recognized [HU79, Wei87] by Turing machine number p, i.e., the set of natural number inputs on which Turing machine p halts.
Reference: [Wie96] <author> R. Wiehagen. </author> <title> Characterizations of learnability in various hypothesis spaces. </title> <type> Private communication, </type> <year> 1996. </year>
Reference-contexts: Let ' p be the partial computable function: N ! N computed (according to some standard I/O conventions) by Turing machine number p is some standard numbering of Turing machines [Rog58, Rog67, Ric80, Ric81, Roy87]. Let W p 4 <ref> [Wie96] </ref> quite interestingly characterizes learning criteria invariances, but as in [Wie78, FKW84], not in terms of control structures. denote the domain of ' p .
Reference: [ZL95] <author> T. Zeugmann and S. Lange. </author> <title> A guided tour across the boundaries of learning recursive languages. </title> <editor> In Klaus P. Jantke and Steffen Lange, editors, </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 190-258. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction In any learnability setting, hypotheses are conjectured from some hypothesis space, for example, in [OSW86] from general purpose programming systems, in <ref> [ZL95, Wie78] </ref> from subrecursive systems, and in [Qui92] from very simple classes of classificatory decision trees. 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces [ZL95]. <p> setting, hypotheses are conjectured from some hypothesis space, for example, in [OSW86] from general purpose programming systems, in [ZL95, Wie78] from subrecursive systems, and in [Qui92] from very simple classes of classificatory decision trees. 3 Much is known theoretically about the restrictions on learning power resulting from restricted hypothesis spaces <ref> [ZL95] </ref>. In the present paper we begin to study the effects on learnability of the presence or absence of certain control structures in the hypothesis space. <p> Definition 2. Suppose V is a c.r.s. For L r.e., MinGram V (L) denotes min (fp j V p = Lg). We define next some interesting senses in which one can translate from one c.r.s. into another. Part (b) of this definition is based on a definition in [Rog58]. <ref> [ZL95] </ref> notes the relevance to learning theory of the sense in part (c). Definition 3. <p> Definition 23. A class L of languages is uniformly decidable def , L can be written as fL 0 ; L 1 ; : : :g, where (9 computable f )(8i)[x f (i; x) = L i ]. Uniformly decidable classes of languages are ubiquitous in computational learning theory <ref> [ZL95] </ref> and are many times also called indexed families of recursive languages. Important examples of such classes are the class of all context free languages [HU79] and the class of all pattern languages [Ang80b, Ang80a]. Other examples, of relevance to the present section, are FiniteSets, Co-Init, and Co-Single.
References-found: 34

