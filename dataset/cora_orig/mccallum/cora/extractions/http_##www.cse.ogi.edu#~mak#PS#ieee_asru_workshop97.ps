URL: http://www.cse.ogi.edu/~mak/PS/ieee_asru_workshop97.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/mak.html
Root-URL: http://www.cse.ogi.edu
Title: STREAM DERIVATION AND CLUSTERING SCHEME FOR SUBSPACE DISTRIBUTION CLUSTERING HIDDEN MARKOV MODEL  
Author: Brian Mak Enrico Bocchieri Etienne Barnard 
Address: 180 Park Ave 20000 NW Walker Rd Florham Park, NJ 07932, USA Portland, OR 97006, USA  
Affiliation: AT&T Labs Research Oregon Graduate Institute  
Abstract: In [1], our novel subspace distribution clustering hidden Markov model (SDCHMM) made its debut as an approximation to continuous density HMM (CDHMM). Deriving SDCHMMs from CDHMMs requires a definition of multiple streams and a Gaussian clustering scheme. Previously we have tried 4 and 13 streams, which are common but ad hoc choices. Here we present a simple and coherent definition for streams of any dimension: the streams comprise the most correlated features. The new definition is shown to give better performance in two recognition tasks. The clustering scheme in [1] is an O(n 2 ) algorithm which can be slow when the number of Gaussians in the original CDHMMs is large. Now we have devised a modified k-means clustering scheme using the Bhattacharyya distance as the distance measure between Gaussian clusters. Not only is the new clustering scheme faster, when combined with the new stream definitions, we now obtain SDCHMMs which perform at least as well as the original CDHMMs (with better results in some cases). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Bocchieri and B. Mak. </author> <title> Subspace Distribution Clustering for Continuous Observation Density Hidden Markov Models. </title> <booktitle> In Proc. of Eurospeech, </booktitle> <volume> volume 1, </volume> <pages> pages 107110, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction In our SDCHMM debut paper <ref> [1] </ref>, we presented a novel derivative of the continuous density HMM (CDHMM) which we call subspace distribution clustering hidden Markov model (SDCHMM). SDCHMMs are derived from CDHMMs by projecting mixture Gaussians of CDHMMs into disjoint subspaces, and the subspace Gaussians are then clustered into a small number of Gaussian prototypes. <p> SDCHMM requires a definition of the disjoint subspaces (or streams) and a Gaussian clustering scheme. We extend our previous work in <ref> [1] </ref> by: * a simple and coherent definition for streams of any dimension; * an O (nkN ) modified k-means subspace Gaussian clustering algorithm to replace the previous O (n 2 ) clustering scheme where kN t n in large vocabu lary recognition system; and, * SDCHMM recognition on two tasks, <p> Since it has been proved by years of research that CDHMM is a good model for speech recognition, a carefully designed approximation to the CDHMM formulation SDCHMM should, in principle, also deliver high performance. 3 Issue I: Subspaces Definition In our previous paper <ref> [1] </ref>, SDCHMMs were tested with common stream definitions which are designed in an ad hoc fashion. Here we use the heuristics that correlated features, by definition, should tend to cluster in a similar manner, and we require each stream to comprise the most correlated features. <p> Step 4. Repeat Step 3 until all features appear in the solution list. Step 5. The feature-tuples in the solution list are the K-stream definition. 4 Issue II: Subspace Gaussian Clustering Previously in <ref> [1] </ref>, subspace Gaussians were clustered by a bottom-up agglomerative clustering scheme of O (n 2 ) complexity in a similar way as in [2] in which two Gaussians are merged if they result in minimum distortion (scatter) increase. <p> Clustering Fig. 1 and Fig. 2 show incremental improvements in recognition performance on ATIS and HMIHY obtained by 13-stream context-independent SDCHMMs with the four combinations of using the old or the new 13-stream definitions, and the old or the new clustering schemes. (The common 13-stream definition is defined as in <ref> [1] </ref> as follows: 12 streams of triplets fcep, cep, cepg and 1 stream composed of fpower, power, powerg.) ATIS with various stream definitions and clustering schemes HMIHY with various stream definitions and clustering schemes Notice that the improvement is bigger with fewer prototypes and this is desirable as fewer prototypes usually
Reference: [2] <author> E. Bocchieri and G. Riccardi. </author> <title> State Tying of Triphone HMM's for the 1994 AT&T ARPA ATIS Recognizer. </title> <booktitle> In Proc. of Eurospeech, </booktitle> <pages> pages 14991502, </pages> <year> 1995. </year>
Reference-contexts: Step 5. The feature-tuples in the solution list are the K-stream definition. 4 Issue II: Subspace Gaussian Clustering Previously in [1], subspace Gaussians were clustered by a bottom-up agglomerative clustering scheme of O (n 2 ) complexity in a similar way as in <ref> [2] </ref> in which two Gaussians are merged if they result in minimum distortion (scatter) increase. To avoid an otherwise O (n 3 ) complexity, we introduced in [2] the heuristic that at each iteration, the Gaussian corresponding to the smallest training ensemble must be merged first. <p> [1], subspace Gaussians were clustered by a bottom-up agglomerative clustering scheme of O (n 2 ) complexity in a similar way as in <ref> [2] </ref> in which two Gaussians are merged if they result in minimum distortion (scatter) increase. To avoid an otherwise O (n 3 ) complexity, we introduced in [2] the heuristic that at each iteration, the Gaussian corresponding to the smallest training ensemble must be merged first.
Reference: [3] <editor> D. Dahl et al. </editor> <title> Expanding the Scope of the ATIS Task: The ATIS-3 Corpus. </title> <booktitle> Proc. of ARPA Human Language Technology Workshop, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Step 5. Update: Merge all subspace Gaussians which share the same nearest proto type to become the new subspace Gaussian prototypes. 5 Recognition Evaluation Two recognition tasks are used for evaluation: ARPAATIS <ref> [3] </ref> and ATTHMIHY [4]. ATIS (Airline Travel Information Service) is a task containing spontaneous speech for air travel information queries, with a 1532-word vocabulary. In these experiments, we have used word-class bigrams language model with a perplexity of about 20.
Reference: [4] <author> A. Ljolje G. Riccardi, A.L. Gorin and M. Riley. </author> <title> A Spoken language system for automated call routing. </title> <booktitle> In Proc. of ICASSP, </booktitle> <pages> pages 11431146, </pages> <year> 1997. </year>
Reference-contexts: Step 5. Update: Merge all subspace Gaussians which share the same nearest proto type to become the new subspace Gaussian prototypes. 5 Recognition Evaluation Two recognition tasks are used for evaluation: ARPAATIS [3] and ATTHMIHY <ref> [4] </ref>. ATIS (Airline Travel Information Service) is a task containing spontaneous speech for air travel information queries, with a 1532-word vocabulary. In these experiments, we have used word-class bigrams language model with a perplexity of about 20.
Reference: [5] <author> P.C. </author> <title> Loizou and A.S. Spanias. High-Performance Alphabet Recognition. </title> <journal> IEEE Trans. on Speech and Audio Processing, </journal> <volume> 4(6):430445, </volume> <month> Nov </month> <year> 1996. </year>
Reference-contexts: Usually kN t n for large systems. Since the entities to cluster are Gaussians, we adopt as distance measure the classification-based Bhattacharyya distance which has been shown to perform well in speech-related tasks <ref> [5, 6] </ref>. The Bhattacharyya distance captures both the first and the second order statistics, and is expected to give better clustering results than the previous distortion measure. Algorithm 2 Modified K-means algorithm for clustering subspace Gaussians Goal: To derive a K-stream SDCHMM with N subspace Gaussian prototypes for each stream.
Reference: [6] <author> B. Mak and E. Barnard. </author> <title> Phone Clustering using the Bhattacharyya Distance. </title> <booktitle> In Proc. of ICSLP, </booktitle> <volume> volume 4, </volume> <pages> pages 20052008, </pages> <year> 1996. </year>
Reference-contexts: Usually kN t n for large systems. Since the entities to cluster are Gaussians, we adopt as distance measure the classification-based Bhattacharyya distance which has been shown to perform well in speech-related tasks <ref> [5, 6] </ref>. The Bhattacharyya distance captures both the first and the second order statistics, and is expected to give better clustering results than the previous distortion measure. Algorithm 2 Modified K-means algorithm for clustering subspace Gaussians Goal: To derive a K-stream SDCHMM with N subspace Gaussian prototypes for each stream.
Reference: [7] <author> S. Takahashi and S. Sagayama. </author> <title> Effects of Variance Tying for Four-Level Tied Structure Phone Models. </title> <booktitle> In Proc. of ASI Conference, volume 1-Q-23, </booktitle> <pages> pages 141142, </pages> <note> 1995 (in Japanese). </note>
Reference-contexts: From the perspective of quantization, SDCHMM approximates CDHMM and achieves great data compression by Gaussian distribution quantization. From the perspective of parameter tying, SDCHMM may allow acoustic modeling to a greater detail without requiring more training data. Finally, SDCHMM unifies the theory of CDHMM and feature-level tying HMM <ref> [7] </ref>. This is because when there is only one subspace, SDCHMM falls back to the conventional CDHMM, whereas if each subspace is one dimension of the feature space (scalar), it becomes the feature-level tying HMM. SDCHMM requires a definition of the disjoint subspaces (or streams) and a Gaussian clustering scheme.
References-found: 7

