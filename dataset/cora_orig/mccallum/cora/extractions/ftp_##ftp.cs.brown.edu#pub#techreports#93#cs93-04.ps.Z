URL: ftp://ftp.cs.brown.edu/pub/techreports/93/cs93-04.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-93-04.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [Bara89] <author> Baraff, David, </author> <title> "Analytical Methods for Dynamic Simulation of Non-Penetrating Rigid Bodies," </title> <booktitle> the Proceedings of SIGGRAPH '89, published as Computer Graphics, </booktitle> <volume> Vol. 23, No. 3, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 223-232. </pages>
Reference-contexts: For simplicity, we call these algorithms the detection algorithm and the response algorithm, respectively. Both parts of a collision-handling algorithm pose interesting problems, but we focus on detection algorithms in this paper. For a discussion of response algorithms, see the work of Baraff <ref> [Bara89, Bara90, Bara91, Bara92a, Bara92b] </ref> The disadvantage associated with the detection algorithms in the literature is their slowness. Slowness is undesirable in any application, but in applications that require interaction with users at real-time rates (e.g. virtual reality) slowness can be unacceptable.
Reference: [Bara90] <author> Baraff, David, </author> <title> "Curved Surfaces and Coherence for Non-Penetrating Rigid Body Simulation," </title> <booktitle> the Proceedings of SIGGRAPH '90, published as Computer Graphics, </booktitle> <volume> Vol. 24, No. 4, </volume> <month> August </month> <year> 1990, </year> <pages> pp. 19-29. </pages>
Reference-contexts: For simplicity, we call these algorithms the detection algorithm and the response algorithm, respectively. Both parts of a collision-handling algorithm pose interesting problems, but we focus on detection algorithms in this paper. For a discussion of response algorithms, see the work of Baraff <ref> [Bara89, Bara90, Bara91, Bara92a, Bara92b] </ref> The disadvantage associated with the detection algorithms in the literature is their slowness. Slowness is undesirable in any application, but in applications that require interaction with users at real-time rates (e.g. virtual reality) slowness can be unacceptable.
Reference: [Bara91] <author> Baraff, David, </author> <title> "Coping with Friction for Non-Penetrating Rigid Body Simulation," </title> <booktitle> the Proceedings of SIGGRAPH '91, published as Computer Graphics, </booktitle> <volume> Vol. 25, No. 4, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 31-40. </pages>
Reference-contexts: For simplicity, we call these algorithms the detection algorithm and the response algorithm, respectively. Both parts of a collision-handling algorithm pose interesting problems, but we focus on detection algorithms in this paper. For a discussion of response algorithms, see the work of Baraff <ref> [Bara89, Bara90, Bara91, Bara92a, Bara92b] </ref> The disadvantage associated with the detection algorithms in the literature is their slowness. Slowness is undesirable in any application, but in applications that require interaction with users at real-time rates (e.g. virtual reality) slowness can be unacceptable.
Reference: [Bara92a] <author> Baraff, David, </author> <title> Dynamic Simulation of Non-Penetrating Rigid Bodies, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Cornell University, </institution> <type> Technical Report 92-1275, </type> <month> March </month> <year> 1992. </year>
Reference-contexts: For simplicity, we call these algorithms the detection algorithm and the response algorithm, respectively. Both parts of a collision-handling algorithm pose interesting problems, but we focus on detection algorithms in this paper. For a discussion of response algorithms, see the work of Baraff <ref> [Bara89, Bara90, Bara91, Bara92a, Bara92b] </ref> The disadvantage associated with the detection algorithms in the literature is their slowness. Slowness is undesirable in any application, but in applications that require interaction with users at real-time rates (e.g. virtual reality) slowness can be unacceptable.
Reference: [Bara92b] <author> Baraff, David and Andrew Witkin, </author> <title> "Dynamic Simulation of Non-Penetrating Flexible Bodies," </title> <booktitle> The Proceedings of SIGGRAPH '92, published as Computer Graphics, </booktitle> <volume> Vol. 26, No. 2, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 303-308. </pages>
Reference-contexts: For simplicity, we call these algorithms the detection algorithm and the response algorithm, respectively. Both parts of a collision-handling algorithm pose interesting problems, but we focus on detection algorithms in this paper. For a discussion of response algorithms, see the work of Baraff <ref> [Bara89, Bara90, Bara91, Bara92a, Bara92b] </ref> The disadvantage associated with the detection algorithms in the literature is their slowness. Slowness is undesirable in any application, but in applications that require interaction with users at real-time rates (e.g. virtual reality) slowness can be unacceptable.
Reference: [Bent79] <author> Bentley, Jon L. and Thomas A. Ottmann, </author> <title> "Algorithms for Reporting and Counting Geometric Intersections," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-28, No. 9, </volume> <month> September </month> <year> 1979, </year> <pages> pp. 643-647. </pages>
Reference-contexts: We are left with the problem of efficiently finding intersections between 2D segments. One solution would be to test every pair of segments for intersections, but we want to avoid this version of the all-pairs weakness. Instead, we use the algorithm of Bentley and Ottmann <ref> [Bent79, Prep85] </ref>. In its full generality, this algorithm sweeps a line across the ff-t plane (from low t coordinates to high t coordinates), updating its data structures whenever a new segment starts or ends, and reporting every intersection it finds. We were able to simplify the algorithm in our application. <p> Finding this point is an example of a two-variable linear-programming problem. Preparata and Shamos [Prep85] describe a solution to this problem, but we chose not to implement their algorithm. Instead, we extended our implementation of the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to solve this problem. The necessary extensions are quite simple and straightforward. Whenever the Bentley-Ottmann algorithm reports that two segments have intersected, we must consider what happens to the two half-spaces bounded by those segments. <p> Our special test steps through the interval endpoints in ascending x-coordinate order, keeping track of those intervals that intersect and checking the corresponding space-time bounds for initial penetration. This process is quite efficient, especially because we already need to sort the interval endpoints before using the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to find face intersections. Even so, we want to avoid running this special test when it is unnecessary. <p> Section 2 identified the weaknesses of previous algorithms that we addressed in our work. We argued in Section 4.2 that our algorithm avoids the all-pairs weakness by using the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to find hyper-trapezoid intersections. In Section 5.1, we showed how our algorithm avoids the fixed-timestep weakness by tracking the intersections of space-time bounds. The timings from Section 6 indicate that our algorithm does indeed perform well in some test situations. <p> We now want the same change to occur in 1 time unit. Thus, we want to use the new acceleration bound M 0 = v = 1 = M ^ t. D Appendix: Round-Off Errors in the Bentley-Ottmann Algo rithm The Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> finds all the intersections within a set of 2D line segments. The foundation of the algorithm is a linear ordering on the segments at any abscissa t. To define the ordering, place a vertical line at t and rank the segments according to where they intersect the line. <p> Thus, the point with the lowest abscissa that lies within all the half-spaces must be the point at which some pair of bounding segments intersect. As we discuss in Appendix D, the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> finds the intersections between segments in order of increasing abscissa; the algorithm thus provides most of the functionality we need to solve our linear-programming problem. To get the remaining functionality, we must make the algorithm keep track of half-spaces as well as segments.
Reference: [Berg86] <author> Bergman, Larry, Henry Fuchs, Eric Grant and Susan Spach, </author> <title> "Image Rendering by Adaptive Refinement," </title> <booktitle> The Proceedings of SIGGRAPH '86, published as Computer Graphics, </booktitle> <volume> Vol. 20, No. 4, </volume> <month> August </month> <year> 1986, </year> <pages> pp. 29-37. </pages>
Reference-contexts: When the application discovers that its other tasks are taking too long, it should be able to tell the detection algorithm to spend less time computing a less-accurate, approximate result. Notice the analogy between this idea and progressive refinement in rendering <ref> [Berg86, Cohe88] </ref>; the success of that approach to rendering gives us hope that our idea will be useful. To support this sort of operation, a detection algorithm must have a special structure. We believe the algorithm should consist of a sequence of steps.
Reference: [Boys79] <author> Boyse, John W., </author> <title> "Interference Detection among Solids and Surfaces," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 22, No. 1, </volume> <month> January </month> <year> 1979, </year> <pages> pp. 3-9. </pages>
Reference-contexts: Lubachevsky [Luba91] notes that when agents move in straight lines in a plane it is simple to directly compute the time at which their paths cross. This approach does not generalize to more complicated situations, however. Boyse <ref> [Boys79] </ref> derives simple equations for the motion of polyhedral edges and faces over time; a root of any such equation corresponds to the time and location of a collision.
Reference: [Broo90] <author> Brooks, Frederick P., Jr., Ming Ouh-Young, James J. Batter and P. Jerome Kilpatrick, </author> <title> "Project GROPE: Haptic Displays for Scientific Visualization", </title> <booktitle> Proceedings of SIGGRAPH '90, published as Computer Graphics, </booktitle> <volume> Vol. 24, No. 4, </volume> <month> August </month> <year> 1990, </year> <pages> pp. 177-185. </pages>
Reference-contexts: For example, engineers need to know how pilots can bring spacecraft together in flight, and chemists need to understand the way drug molecules fit into receptor sites <ref> [Turk90, Broo90] </ref>. Computer simulations allow users to gain insight into docking tasks, and these simulations should incorporate collision detection to provide the most realism. In these simulations, users directly manipulate objects with a mouse or some other input device.
Reference: [Came85] <author> Cameron, Stephen A., </author> <title> "A Study of the Clash Detection Problem in Robotics," </title> <booktitle> Proceedings 1985 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 488-493. </pages>
Reference-contexts: Culley and Kempf describe an algorithm that uses this lower bound on collision time to adaptively vary its timestep. Cameron <ref> [Came85] </ref> sketches a similar idea in less detail. None of these authors present any efficient ways to make their algorithms handle non-linear motion or more than two agents. The spirit of this approach seems valid, however, and it served as an inspiration for our work on space-time bounds.
Reference: [Came89] <author> Cameron, Stephen A., </author> <title> "Efficient Intersection Tests for Objects Defined Constructively," </title> <journal> International Journal of Robotics Research, </journal> <volume> Vol. 8, No. 1, </volume> <month> February </month> <year> 1989, </year> <pages> pp. 3-25. </pages>
Reference-contexts: Hahn [Hahn88] suggests that he uses an octree in a manner similar to Moore and Wilhelms, but he provides no details of his algorithm. Cameron <ref> [Came89] </ref> presents an algorithm for a pair of CSG models. Viewing a CSG model as a tree of operations, he shows how bounds on the partial results at internal nodes allow collisions between trees to be found quickly. He reports speedups of "a factor of about 100" over brute-force techniques.
Reference: [Came90] <author> Cameron, Stephen A., </author> <title> "Collision Detection by Four-Dimensional Intersection Testing," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 6, No. 3, </volume> <month> June </month> <year> 1990, </year> <pages> pp. 291-302. </pages>
Reference-contexts: Viewing a CSG model as a tree of operations, he shows how bounds on the partial results at internal nodes allow collisions between trees to be found quickly. He reports speedups of "a factor of about 100" over brute-force techniques. In a later paper <ref> [Came90] </ref>, Cameron extends his bounds to work in four dimensions. The result is a solution to the fixed-timestep weakness. He assumes piecewise-linear motion, however, and this motion must be fully specified in advance. <p> The absence of a fully-satisfying detection algorithm motivated us to develop our own approach. 3 Space-Time Bounds Our detection algorithm is based on four-dimensional (4D) geometry, the fourth dimension representing time. The use of an explicit temporal dimension is not new; Canny [Cann86], Samet and Tamminen [Same85], Cameron <ref> [Came90] </ref> and Duff [Duff92] exploit this extra dimension, as we discuss in Section 2. Unlike these other researchers, we do not use 4D geometry to exactly represent each agent moving through time. Instead, our 4D structures are bounds that give conservative over-estimates on the space an agent could occupy.
Reference: [Cann86] <author> Canny, John, </author> <title> "Collision Detection for Moving Polyhedra," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 8, No. 2, </volume> <pages> pp. 200-209, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: He places fewer restrictions on motion than Lubachevsky, but he does assume one stationary and one moving polyhedron, with the moving polyhedron being limited to periods of only linear translation and periods of only rotation around a fixed axis. Canny <ref> [Cann86] </ref> uses more elaborate analysis to process more general motion. He obtains quintic polynomials whose roots, found with iterative numerical techniques, describe the collisions between convex polyhedra. The motion he can handle is still not completely general, however: agents must have constant linear velocity and "nearly-constant" angular velocity. <p> The absence of a fully-satisfying detection algorithm motivated us to develop our own approach. 3 Space-Time Bounds Our detection algorithm is based on four-dimensional (4D) geometry, the fourth dimension representing time. The use of an explicit temporal dimension is not new; Canny <ref> [Cann86] </ref>, Samet and Tamminen [Same85], Cameron [Came90] and Duff [Duff92] exploit this extra dimension, as we discuss in Section 2. Unlike these other researchers, we do not use 4D geometry to exactly represent each agent moving through time.
Reference: [Cohe88] <author> Cohen, Michael F., Shenchang Eric Chen, John R. Wallace and Donald P. Greenberg, </author> <title> "A Progressive Refinement Approach to Fast Radiosity Image Generation," </title> <booktitle> The Proceedings of SIG-GRAPH '88, published as Computer Graphics, </booktitle> <volume> Vol. 22, No. 4, </volume> <month> August </month> <year> 1988, </year> <pages> pp. 75-84. </pages>
Reference-contexts: When the application discovers that its other tasks are taking too long, it should be able to tell the detection algorithm to spend less time computing a less-accurate, approximate result. Notice the analogy between this idea and progressive refinement in rendering <ref> [Berg86, Cohe88] </ref>; the success of that approach to rendering gives us hope that our idea will be useful. To support this sort of operation, a detection algorithm must have a special structure. We believe the algorithm should consist of a sequence of steps.
Reference: [Cull86] <author> Culley, R. K., and K. G. Kempf, </author> <title> "A Collision Detection Algorithm Based on Velocity and Distance Bounds," </title> <booktitle> Proceedings of the 1986 IEEE International Conference on Robotics and Automation, </booktitle> <volume> Vol. 2, </volume> <year> 1986., </year> <pages> pp. 1064-1069. </pages>
Reference-contexts: Like Canny's algorithm, the algorithms of Von Herzen, Snyder and Duff all require that motion be prespecified. Culley and Kempf <ref> [Cull86] </ref> attack the fixed-timestep weakness by making the following observation: an upper bound on the relative velocity of two agents and a lower bound on their separation distance define a lower bound on the time before they can collide.
Reference: [Dobk83] <author> Dobkin, D. P. and D. G. Kirkpatrick, </author> <title> "Fast Detection of Polyhedral Intersection," </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 27, </volume> <year> 1983, </year> <pages> pp. 241-253. </pages>
Reference-contexts: The result is a solution to the fixed-timestep weakness. He assumes piecewise-linear motion, however, and this motion must be fully specified in advance. In this paper, Cameron also addresses the case of more than two CSG models, but he does not fully eliminate the all-pairs weakness. Dobkin and Kirkpatrick <ref> [Dobk83] </ref> describe another approach for a pair of convex polyhedra. By slicing the polyhedra into a particular form of cross-sections, the algorithm can applying two-dimensional techniques to find collisions.
Reference: [Duff92] <author> Duff, Tom, </author> <title> "Interval Arithmetic and Recursive Subdivision for Implicit Functions and Constructive Solid Geometry," </title> <booktitle> The Proceedings of SIGGRAPH '92, published as Computer Graphics, </booktitle> <volume> Vol. 26, No. 2, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 131-138. </pages>
Reference-contexts: The net effect is that the algorithm focuses on times when collisions are likely. The algorithm works only for agents represented by the Constructive Solid Geometry (CSG) modeling paradigm, though, and the authors are vague about how to handle agents whose motion is not linear. Duff <ref> [Duff92] </ref> uses the formalism of interval arithmetic as the basis of an algorithm similar to Samet and Tamminen's algorithm. Interval arithmetic allows Duff's algorithm to handle nonlinear motion, but it cannot handle motion defined by ordinary differential equations (e.g., physically-based motion). <p> The use of an explicit temporal dimension is not new; Canny [Cann86], Samet and Tamminen [Same85], Cameron [Came90] and Duff <ref> [Duff92] </ref> exploit this extra dimension, as we discuss in Section 2. Unlike these other researchers, we do not use 4D geometry to exactly represent each agent moving through time. Instead, our 4D structures are bounds that give conservative over-estimates on the space an agent could occupy.
Reference: [Elli78] <author> Ellis, Robert and Denny Gulick, </author> <title> Calculus with Analytic Geometry, </title> <publisher> Harcourt Brace Jovanovich (New York), </publisher> <year> 1978. </year>
Reference-contexts: Let the position of A in R 3 at time t be described by the function x (t); let _ x (t) denote the velocity and x (t) denote the acceleration of A at t. If we know x (0), _ x (0) and x (0) then Taylor's Theorem <ref> [Elli78] </ref> tells us that the position of A at time t 0 is x (t) = x (0) + _ x (0)t + x (0) 2 This result is not precise enough for our current purposes, but we can use this general idea to develop a more useful result.
Reference: [Fois90] <author> Foisy, Andre, Vincent Hayward and Stephane Aubry, </author> <title> "The Use of Awareness in Collision Prediction," </title> <booktitle> Proceedings of the 1990 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 338-343. </pages>
Reference-contexts: The idea of approximation is a promising one, however, and we will explore it further in Section 7. After we had developed our space-time bounds, we discovered a paper by Foisy et al. <ref> [Fois90] </ref> that contains some similar ideas. An important element of their work is a queueing scheme that allows the algorithm to compare only O (log N ) pairs of agents at each timestep. When the frequency and number of possible collisions meet certain conditions, this scheme solves the all-pairs weakness.
Reference: [Gold50] <author> Goldstein, Herbert, </author> <title> Classical Mechanics, </title> <publisher> Addison-Wesley (Reading, </publisher> <address> Massachusetts), </address> <year> 1950. </year>
Reference-contexts: At any moment in time, however, we can treat this motion as instantaneous rotation about a point y that is subject to only non-angular velocities and accelerations; if we are using Newtonian dynamics, y is A's center of mass <ref> [Gold50] </ref>. If A does not change its scale, there is a 3D sphere that, when centered at y (t), bounds the space swept by these rotations. Let r be the radius of this sphere; we call r the radius of A. <p> As a solution, the system could simulate attachment of the manipulated object to the user's cursor by means of a virtual spring. The virtual spring allows the system to convert cursor displacements into the needed acceleration bounds according to Hooke's law <ref> [Gold50] </ref>.
Reference: [Hahn88] <author> Hahn, James K., </author> <title> "Realistic Animation of Rigid Bodies," </title> <booktitle> The Proceedings of SIGGRAPH '88, published as Computer Graphics, </booktitle> <volume> Vol. 22, No. 4, </volume> <month> August </month> <year> 1988, </year> <pages> pp. 299-308. </pages>
Reference-contexts: Snyder and Von Herzen apply subdivision techniques to models made of parametric patches instead of polygons. Their algorithms seem to be an efficient way to find contacts between a single pair of patches, but they do not address models made from more than one patch. Hahn <ref> [Hahn88] </ref> suggests that he uses an octree in a manner similar to Moore and Wilhelms, but he provides no details of his algorithm. Cameron [Came89] presents an algorithm for a pair of CSG models.
Reference: [Herm86] <author> Herman, Martin, </author> <title> "Fast, Three-Dimensional, Collision-Free Motion Planning," </title> <booktitle> Proceedings of the 1986 IEEE International Conference on Robotics and Automation, </booktitle> <volume> Vol. 2, </volume> <year> 1986, </year> <pages> pp. 1056-1063. </pages>
Reference-contexts: The spirit of this approach seems valid, however, and it served as an inspiration for our work on space-time bounds. The all-pairs weakness has received less attention in the literature than the fixed-timestep weakness. Herman <ref> [Herm86] </ref> uses an octree to group agents according to their location in space, thus avoiding the processing of pairs of agents that are separated by large distances.
Reference: [Hubb91] <author> Hubbard, Philip M., Matthias M. Wloka and Robert C. Zeleznik, "UGA: </author> <title> A Unified Graphics Architecture," </title> <type> Technical Report CS-91-30, </type> <institution> Department of Computer Science, Brown University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The naive algorithm of Figure 2.1 suggests that the detection algorithm drives the system by repeatedly advancing the current position in "simulation time." A detection algorithm actually plays a more subservient role in the graphics system we have implemented <ref> [Zele91, Hubb91] </ref>. The current position in simulation time is controlled by another part of this system, which calls the detection algorithm whenever it needs to render a frame. When called, the detection algorithm determines if any collisions have occurred since the time of the previous frame.
Reference: [Luba91] <author> Lubachevsky, Boris D., </author> <title> "How to Simulate Billiards and Similar Systems," </title> <journal> Journal of Computational Physics, </journal> <volume> Vol. 94, No. 1, </volume> <month> May </month> <year> 1991, </year> <pages> pp. 255-283. </pages>
Reference-contexts: Furthermore, the algorithms that most nearly solve all three weaknesses impose 2 restrictions on the agents that make these algorithms less useful, particularly for interactive applications. The literature addressing the fixed-timestep weakness includes several attempts to dispense with the timestep t altogether. Lubachevsky <ref> [Luba91] </ref> notes that when agents move in straight lines in a plane it is simple to directly compute the time at which their paths cross. This approach does not generalize to more complicated situations, however.
Reference: [Mack90] <author> Mackinlay, Jock D., Stuart K. Card and George G. Robertson, </author> <title> "Rapid Controlled Movement Through a Virtual 3D Workspace," </title> <booktitle> Proceedings of SIGGRAPH '90, published as Computer Graphics, </booktitle> <volume> Vol. 24, No. 4, </volume> <month> August </month> <year> 1990, </year> <pages> pp. 171-176. </pages>
Reference-contexts: If the system is to detect collisions between the user-controlled viewpoint and the other agents, then it will need acceleration bounds for the user's position. Mackinlay et al. <ref> [Mack90] </ref> have published an appealing approach to navigation. The user points at a destination with a mouse cursor and the system moves the user's viewpoint to that destination; the speed with which the viewpoint moves decreases logarithmically as the destination draws nearer.
Reference: [Moor88] <author> Moore, Matthew and Jane Wilhelms, </author> <title> "Collision Detection and Response for Computer Animation," </title> <booktitle> The Proceedings of SIGGRAPH '88, published as Computer Graphics, </booktitle> <volume> Vol. 22, No. 4, </volume> <month> August </month> <year> 1988, </year> <pages> pp. 289-298. </pages>
Reference-contexts: Duff's subdivision algorithm has the properties of an octree algorithm, but also puts restrictions on the allowable motion as we mentioned earlier. Moore and Wil-helms <ref> [Moor88] </ref> describe another algorithm based on octrees. The only restrictions they place on an agent's motion is that it can be considered linear between timesteps, but they explicitly ignore a few special cases of motion that could produce collisions.
Reference: [Patt90] <author> Patterson, David A. and John. L. Hennessy, </author> <title> Computer Architecture: A Quantitative Approach, </title> <address> Morgan Kauffmann (San Mateo, California), </address> <year> 1990. </year>
Reference-contexts: In all three tests, we used the frame rate ffit = 0:02 time units (giving t = 0:01) and we used 3:0 as the end of simulation time. Figures 6.1, 6.2 and 6.3 graph the speedup of our algorithm for each test in each sequence. Following Patterson and Hennessy <ref> [Patt90] </ref>, we define the speedup of our algorithm to be the time spent by Turk's algorithm divided by the time spent by our algorithm. Thus, any speedup greater than 1 indicates an advantage for our algorithm, and the larger the speedup the better.
Reference: [Pent91] <author> Pentland, Alex and Stan Sclaroff, </author> <title> "Closed-Form Solutions for Physically Based Shape Modelling and Recognition," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 13, No. 7, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 715-729. </pages>
Reference-contexts: The authors prove that the algorithm has good asymptotic performance, but they admit that it has never been implemented and evaluated empiricly. Their algorithms addresses neither the fixed-timestep nor the all-pairs weaknesses. Sclaroff and Pentland <ref> [Scla91, Pent91] </ref> describe a novel solution to the pair-processing weakness. They suggest that polyhedral models be approximated by deformed superquadric ellipsoids whose surfaces have been modulated by a "displacement map"; these approximate models have the advantage that contacts between them are simpler to detect. <p> Ideally, the geometric approximations would eventually converge on the real geometry of the agents. We have not found this sort of geometric approximation discussed anywhere in the literature. The work of Sclaroff and Pentland <ref> [Scla91, Pent91] </ref> introduces one way to approximate an agent's geometry (recall Section 2), but the authors do not discuss how this approximation might be made more or less accurate to fit the available processing time.
Reference: [Prep85] <author> Preparata, Franco P. and Michael I. Shamos, </author> <title> Computational Geometry: An Introduction, </title> <publisher> Springer-Verlag (New York), </publisher> <year> 1985. </year>
Reference-contexts: The problem is the quadratic term, t 2 . The number of algorithms for processing non-linear geometry is small compared to the wealth of algorithms for linear geometry <ref> [Prep85] </ref>. We need an algorithm to efficiently determine which of the N bounds intersect, as we will discuss in Section 4. So that we can use a published algorithm for this intersection problem, we develop a linear approximation to parabolic horns. <p> We are left with the problem of efficiently finding intersections between 2D segments. One solution would be to test every pair of segments for intersections, but we want to avoid this version of the all-pairs weakness. Instead, we use the algorithm of Bentley and Ottmann <ref> [Bent79, Prep85] </ref>. In its full generality, this algorithm sweeps a line across the ff-t plane (from low t coordinates to high t coordinates), updating its data structures whenever a new segment starts or ends, and reporting every intersection it finds. We were able to simplify the algorithm in our application. <p> We want to find the earliest such point, i.e., the one with the lowest t coordinate. Finding this point is an example of a two-variable linear-programming problem. Preparata and Shamos <ref> [Prep85] </ref> describe a solution to this problem, but we chose not to implement their algorithm. Instead, we extended our implementation of the Bentley-Ottmann algorithm [Bent79, Prep85] to solve this problem. The necessary extensions are quite simple and straightforward. <p> Finding this point is an example of a two-variable linear-programming problem. Preparata and Shamos [Prep85] describe a solution to this problem, but we chose not to implement their algorithm. Instead, we extended our implementation of the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to solve this problem. The necessary extensions are quite simple and straightforward. Whenever the Bentley-Ottmann algorithm reports that two segments have intersected, we must consider what happens to the two half-spaces bounded by those segments. <p> Our special test steps through the interval endpoints in ascending x-coordinate order, keeping track of those intervals that intersect and checking the corresponding space-time bounds for initial penetration. This process is quite efficient, especially because we already need to sort the interval endpoints before using the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to find face intersections. Even so, we want to avoid running this special test when it is unnecessary. <p> Section 2 identified the weaknesses of previous algorithms that we addressed in our work. We argued in Section 4.2 that our algorithm avoids the all-pairs weakness by using the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> to find hyper-trapezoid intersections. In Section 5.1, we showed how our algorithm avoids the fixed-timestep weakness by tracking the intersections of space-time bounds. The timings from Section 6 indicate that our algorithm does indeed perform well in some test situations. <p> We now want the same change to occur in 1 time unit. Thus, we want to use the new acceleration bound M 0 = v = 1 = M ^ t. D Appendix: Round-Off Errors in the Bentley-Ottmann Algo rithm The Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> finds all the intersections within a set of 2D line segments. The foundation of the algorithm is a linear ordering on the segments at any abscissa t. To define the ordering, place a vertical line at t and rank the segments according to where they intersect the line. <p> Thus, the point with the lowest abscissa that lies within all the half-spaces must be the point at which some pair of bounding segments intersect. As we discuss in Appendix D, the Bentley-Ottmann algorithm <ref> [Bent79, Prep85] </ref> finds the intersections between segments in order of increasing abscissa; the algorithm thus provides most of the functionality we need to solve our linear-programming problem. To get the remaining functionality, we must make the algorithm keep track of half-spaces as well as segments.
Reference: [Pres88] <author> Press, William H., Brian P. Flannery, Saul A. Teukolsky and William T. Vetterling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press (Cambridge), </publisher> <year> 1988. </year>
Reference-contexts: In this case, when our algorithm asks the graphics system for an agent's x (t build ) and _ x (t build ), the system will invoke a numerical method to solve the ODEs. The common ODE solvers <ref> [Pres88] </ref> iterate through simulation time, building solutions at later times from solutions at earlier times. Knowing that ODE solvers work in this manner, one might be surprised that the computation of x (t build ) and _ x (t build ) adds any extra processing time. <p> In our tests, however, the latter is consistently faster. This speed probably comes from a characteristic of the ODE solver we use. This solver, a fourth-order Runge Kutta method <ref> [Pres88] </ref>, chooses the size of its iteration steps adaptively. When it is iterating over a longer interval of simulation time, it has more opportunity to stretch out its steps without losing accuracy. <p> One might be tempted to use the abscissa of the "new" intersection as follows: if it is less than the abscissa of the most-recently-processed intersection, one would conclude that the "new" intersection has already been processed. Unfortunately, this approach is prone to round-off errors <ref> [Pres88] </ref>. When the algorithm computes an intersection abscissa, it uses floating-point computations that are not guaranteed to be exactly correct. To see how these inaccuracies can cause problems, refer again to Figure D.1.
Reference: [Same85] <author> Samet, Hanan and Markku Tamminen, "Bintrees, </author> <title> CSG Trees, and Time," </title> <booktitle> The Proceedings of SIGGRAPH '85, published as Computer Graphics, </booktitle> <volume> Vol. 19, No. 3, </volume> <month> July </month> <year> 1985, </year> <pages> pp. 121-130. </pages>
Reference-contexts: Canny's algorithm also requires the motion of all agents to be completely specified in advance, making his algorithm unusable for applications in which a user interactively specifies the motion. Samet and Tamminen <ref> [Same85] </ref> address the fixed-timestep weakness by working in a four-dimensional space that includes a temporal dimension. Their algorithm recursively subdivides the space like an octree, making more subdivisions in regions that could contain a pair of agents. <p> The absence of a fully-satisfying detection algorithm motivated us to develop our own approach. 3 Space-Time Bounds Our detection algorithm is based on four-dimensional (4D) geometry, the fourth dimension representing time. The use of an explicit temporal dimension is not new; Canny [Cann86], Samet and Tamminen <ref> [Same85] </ref>, Cameron [Came90] and Duff [Duff92] exploit this extra dimension, as we discuss in Section 2. Unlike these other researchers, we do not use 4D geometry to exactly represent each agent moving through time.
Reference: [Scla91] <author> Sclaroff, Stan and Alex Pentland, </author> <title> "Generalized Implicit Functions for Computer Graphics," </title> <booktitle> The Proceedings of SIGGRAPH '91, published as Computer Graphics, </booktitle> <volume> Vol. 25, No. 4, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 247-250. </pages>
Reference-contexts: The authors prove that the algorithm has good asymptotic performance, but they admit that it has never been implemented and evaluated empiricly. Their algorithms addresses neither the fixed-timestep nor the all-pairs weaknesses. Sclaroff and Pentland <ref> [Scla91, Pent91] </ref> describe a novel solution to the pair-processing weakness. They suggest that polyhedral models be approximated by deformed superquadric ellipsoids whose surfaces have been modulated by a "displacement map"; these approximate models have the advantage that contacts between them are simpler to detect. <p> Ideally, the geometric approximations would eventually converge on the real geometry of the agents. We have not found this sort of geometric approximation discussed anywhere in the literature. The work of Sclaroff and Pentland <ref> [Scla91, Pent91] </ref> introduces one way to approximate an agent's geometry (recall Section 2), but the authors do not discuss how this approximation might be made more or less accurate to fit the available processing time.
Reference: [Snyd92] <author> Snyder, John M., </author> <title> "Interval Analysis for Computer Graphics," </title> <booktitle> The Proceedings of SIGGRAPH '92, published as Computer Graphics, </booktitle> <volume> Vol. 26, No. 2, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 121-130. </pages>
Reference-contexts: Duff [Duff92] uses the formalism of interval arithmetic as the basis of an algorithm similar to Samet and Tamminen's algorithm. Interval arithmetic allows Duff's algorithm to handle nonlinear motion, but it cannot handle motion defined by ordinary differential equations (e.g., physically-based motion). Snyder <ref> [Snyd92] </ref> presents another application of interval arithmetic to solving the fixed-timestep weakness. His approach is based on the approach of Von Herzen [VonH89, VonH90], which uses the Lipschitz condition.
Reference: [Turk90] <author> Turk, Greg, </author> <title> "Interactive Collision Detection for Molecular Graphics," </title> <type> Technical Report TR90-014, </type> <institution> Computer Science Department, The University of North Carolina at Chapel Hill, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Lubachevsky's approach to the all-pairs weakness is to subdivide space into equal-sized squares he calls "sectors." Turk <ref> [Turk90] </ref> uses a similar approach in his algorithm to detect collisions between molecules, with his sectors being cubes. The algorithm compares only pairs of agents that occupy the same sector; the goal is to obtain the advantages of an octree without the overhead of building it. <p> For example, engineers need to know how pilots can bring spacecraft together in flight, and chemists need to understand the way drug molecules fit into receptor sites <ref> [Turk90, Broo90] </ref>. Computer simulations allow users to gain insight into docking tasks, and these simulations should incorporate collision detection to provide the most realism. In these simulations, users directly manipulate objects with a mouse or some other input device. <p> We therefore believe we accurately simulated how a user might interactively control an agent by periodically applying control forces (accelerations). To put the performance of our algorithm in perspective, we compared it with the performance of Turk's algorithm <ref> [Turk90] </ref> on the same test configurations. We chose Turk's algorithm for several reasons. Recall from Section 2 that Turk's algorithm solves the all-pairs weakness by dividing space into fixed-sized sectors and comparing only pairs of agents within the same sector. Turk's empirical results suggest that this approach is quite efficient.
Reference: [VonH89] <author> Von Herzen, Brian, </author> <title> Applications of Surface Networks to Sampling Problems in Computer Graphics, </title> <type> Ph.D. Dissertation, </type> <institution> California Institute of Technology, Computer Science Department, Caltech-CS-TR-88-15, </institution> <year> 1989. </year>
Reference-contexts: Interval arithmetic allows Duff's algorithm to handle nonlinear motion, but it cannot handle motion defined by ordinary differential equations (e.g., physically-based motion). Snyder [Snyd92] presents another application of interval arithmetic to solving the fixed-timestep weakness. His approach is based on the approach of Von Herzen <ref> [VonH89, VonH90] </ref>, which uses the Lipschitz condition. The Lipschitz condition gives bounds on the space a surface can occupy over a particular interval of time; Von Herzen uses these bounds in a recursive-subdivision algorithm analogous to Samet and Tamminen's algorithm.
Reference: [VonH90] <author> Von Herzen, Brian, Alan H. Barr and Harold R. Zatz, </author> <title> "Geometric Collisions for Time-Dependent Parametric Surfaces," </title> <booktitle> The Proceedings of SIGGRAPH '90, published as Computer Graphics, </booktitle> <volume> Vol. 24, No. 4, </volume> <month> August </month> <year> 1990, </year> <pages> pp. 39-48. </pages>
Reference-contexts: Interval arithmetic allows Duff's algorithm to handle nonlinear motion, but it cannot handle motion defined by ordinary differential equations (e.g., physically-based motion). Snyder [Snyd92] presents another application of interval arithmetic to solving the fixed-timestep weakness. His approach is based on the approach of Von Herzen <ref> [VonH89, VonH90] </ref>, which uses the Lipschitz condition. The Lipschitz condition gives bounds on the space a surface can occupy over a particular interval of time; Von Herzen uses these bounds in a recursive-subdivision algorithm analogous to Samet and Tamminen's algorithm.
Reference: [Zele91] <author> Zeleznik, Robert C., D. Brookshire Conner, Matthias M. Wloka, Daniel G. Aliaga, Nathan T. Huang, Philip M. Hubbard, Brian Knep, Henry Kaufman, John F. Hughes and Andries van Dam, </author> <title> "An Object-Oriented Framework for the Integration of Interactive Animation Techniques," </title> <booktitle> The Proceedings of SIGGRAPH '91, published as Computer Graphics, </booktitle> <volume> Vol. 25, No. 4, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 105-112. </pages>
Reference-contexts: The naive algorithm of Figure 2.1 suggests that the detection algorithm drives the system by repeatedly advancing the current position in "simulation time." A detection algorithm actually plays a more subservient role in the graphics system we have implemented <ref> [Zele91, Hubb91] </ref>. The current position in simulation time is controlled by another part of this system, which calls the detection algorithm whenever it needs to render a frame. When called, the detection algorithm determines if any collisions have occurred since the time of the previous frame.
References-found: 37

