URL: http://www.cs.arizona.edu/people/will/papers/thesis.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/will/papers.html
Root-URL: http://www.cs.arizona.edu
Title: Information Theory and Noisy Computation  
Author: William S. Evans 
Date: November, 1994  
Pubnum: TR-94-057  
Abstract: The information carried by a signal unavoidably decays when the signal is corrupted by random noise. This occurs when a noisy channel transmits a message as well as when a noisy component performs computation. We first study this signal decay in the context of communication and obtain a tight bound on the decay of the information carried by a signal as it crosses a noisy channel. We then use this information theoretic result to obtain depth lower bounds in the noisy circuit model of computation defined by von Neumann. In this model, each component fails (produces 1 instead of 0 or vice-versa) independently with a fixed probability, and yet the output of the circuit should be correct with high probability. Von Neumann showed how to construct circuits in this model that reliably compute a function and are no more than a constant factor deeper than noiseless circuits for the function. Our result implies that such a multiplicative increase in depth is necessary for reliable computation. The result also indicates that above a certain level of component noise, reliable computation is impossible. We use a similar technique to lower bound the size of reliable circuits in terms of the noise and complexity of their components, and the sensitivity of the function they compute. Our 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Our first step in determining these parameters relies on a very general fact about maximizing the ratio between two discrete second derivatives. For any function f , any two values x; y in the domain of f , and any p 2 <ref> [0; 1] </ref>, let f 2 (x; y; p) = f (px + (1 p)y) pf (x) (1 p)f (y) denote the discrete second derivative of f . Lemma 2.1.1 For any strictly concave functions f , g on closed and bounded intervals and any p 2 [0; 1], the ratio r <p> and any p 2 <ref> [0; 1] </ref>, let f 2 (x; y; p) = f (px + (1 p)y) pf (x) (1 p)f (y) denote the discrete second derivative of f . Lemma 2.1.1 For any strictly concave functions f , g on closed and bounded intervals and any p 2 [0; 1], the ratio r (x; y) = g 2 (x; y; p)=f 2 (x; y; p) is maximized in the limit jx yj ! 0. CHAPTER 2. <p> The distance function is induced from the interval. Proof: Fix any weights p X (0) and p X (1). Then I (X ; Y ) and I (X ; Z) are the discrete second derivatives of the strictly concave entropy function on <ref> [0; 1] </ref>. (See [1] for proof of concavity of entropy.) 2 Observe also that unless the channel is either perfectly noiseless or perfectly noisy, that is unless the entries of A are all 0's and 1's, the corollary will hold strictly; which is to say that the maximum ratio is achieved <p> The distance function is induced from the interval. Proof: Fix any weights p X (0) and p X (1). Then I (X ; Y ) and I (X ; Z) are the discrete second derivatives of the strictly concave entropy function on [0; 1]. (See <ref> [1] </ref> for proof of concavity of entropy.) 2 Observe also that unless the channel is either perfectly noiseless or perfectly noisy, that is unless the entries of A are all 0's and 1's, the corollary will hold strictly; which is to say that the maximum ratio is achieved only in the <p> As in the proof of that corollary, for fixed weights p X (0) and p X (1), I (X; Y ) is the discrete second derivative of the entropy function on <ref> [0; 1] </ref>. Since Z is the output of channel A on input Y , the distributions p Zj0 and p Zj1 lie on the line in &lt; m between (0; 1) A and (1; 0) A. <p> If ! is too large, it may not be possible to find such a and in this case the *-noisy gate cannot be viewed as having !-noisy input wires. To calculate the error on a particular input, treat the function as a 2 k dimensional column vector in <ref> [0; 1] </ref> 2 k , and A (x; y) as a 2 k fi 2 k matrix. The error on input x is the xth component of the matrix, vector product A which we denote (A )(x). <p> The error on input x is the xth component of the matrix, vector product A which we denote (A )(x). With this notation, the condition that the gate be *-noisy is that, for all x 2 f0; 1g k (x) 2 <ref> [0; 1] </ref> and (A )(x) = 1 * if g (x) = 0 (4.1) 2 The Hamming distance between x and y is the number of bit positions in which x and y differ. CHAPTER 4. <p> If * &lt; 1=2, we show the existence of the function by treating condition 4.1 as a system of linear equations. We show that is uniquely defined by this system and that for ! 2 [0; 1 k p 2 ], (x) is a probability (i.e. lies in <ref> [0; 1] </ref>) for all x. The matrix A can be written as the tensor product of a 2 fi 2 matrix with itself k times. <p> of all 1=2's and 2 f+~=2; ~=2g 2 k (for ~ = 1 2*) corresponds to the Boolean function g: (x) = +~=2 if g (x) = 0 To complete the proof, we must show that for all Boolean functions g (i.e. all vectors ), the value (x) lies in <ref> [0; 1] </ref> for all x. Since 1 2 is an eigenvector of A 1 with eigenvalue 1, A 1 ( 2 + ) = 2 + A 1 . <p> NOISY CIRCUIT SIZE 30 2!) k ((1 !) + !) k = (1 2!) k . Since the magnitude of the entries in is ~=2, the maximum of j (A 1 )(x)j is ~(1 2!) k =2. Thus (x) 2 <ref> [0; 1] </ref> if ~(1 2!) k =2 1=2 or equivalently ! 2 4.2 Lower Bound on Noisy Circuit Size Assume that k is an upper bound on the number of inputs to a gate in the basis. <p> Since f k (fi k ; 0) = 0, to show m fi k ;k (a) &gt; a for a 2 [0; 1=2) it suffices to show f k (fi k ; ff) is an increasing function of ff 2 <ref> [0; 1] </ref> (i.e. df k (fi k ; ff)=dff 0), df k (*; ff) = 1=2 + (1 2*) dff Since, d k = 2 k k 1 2 (1 ff 2 ) 2 substituting into (5.3) yields, df k (*; ff) = 1=2 (1 2*) 2 k k 1 2 <p> THRESHOLD FOR NOISY COMPUTATION 40 For * = fi k , dff 1 k1 2 which is non-negative for ff 2 <ref> [0; 1] </ref>. We now show the second statement of the lemma. The second derivative of f k (*; ff) is d 2 f k (*; ff) k (k 1) k1 ! k3 which is non-negative for all ff; * 2 [0; 1=2]. <p> An *-noisy, k-input gate g takes as input Y 1 ; : : : ; Y k , described by points Y 1 ; : : : ; Y k , and outputs Y described by Y . Thus the gate g defines a mapping g : <ref> [0; 1] </ref> 2k ! [0; 1] 2 . Lemma 5.3.1 states that if * fi k then the union over all g of g (S (a) k ) is contained in S (m *;k (a)). <p> Thus the gate g defines a mapping g : <ref> [0; 1] </ref> 2k ! [0; 1] 2 . Lemma 5.3.1 states that if * fi k then the union over all g of g (S (a) k ) is contained in S (m *;k (a)). <p> purpose of the following two lemmas is to show that it suffices to prove that the union over all threshold gates g of g ((a; a) k ) is contained in S (m *;k (a)). (Note: (a; a) k is the point (a; a); (a; a); :::; (a; a) in <ref> [0; 1] </ref> 2k .) The method is to show that the set of image points has the same convex hull in both cases. Thus, since S (m *;k (a)) is convex, showing containment of either set implies containment of the other. <p> If C is the convex hull of the union over all g of g (S (a) k ) and C a is the convex hull of the union over all g of g ((a; a) k ) then C = C a Proof: The mapping from S (a) k to <ref> [0; 1] </ref> 2 defined by g is affine, [0; 1] 2 ! [0; 1] 2 , in each Y i when the others are fixed. <p> union over all g of g (S (a) k ) and C a is the convex hull of the union over all g of g ((a; a) k ) then C = C a Proof: The mapping from S (a) k to <ref> [0; 1] </ref> 2 defined by g is affine, [0; 1] 2 ! [0; 1] 2 , in each Y i when the others are fixed. Thus the image of S (a) k is contained in the convex hull of the image of the set of vertices of S (a) k . <p> of g (S (a) k ) and C a is the convex hull of the union over all g of g ((a; a) k ) then C = C a Proof: The mapping from S (a) k to <ref> [0; 1] </ref> 2 defined by g is affine, [0; 1] 2 ! [0; 1] 2 , in each Y i when the others are fixed. Thus the image of S (a) k is contained in the convex hull of the image of the set of vertices of S (a) k . <p> jX; Q) n X H (Y i jY 1 ; : : : ; Y i1 ; Q) i=1 = i=1 2 Non-negativity of Information A function f (x) is convex on the interval (a; b) if for all x 1 ; x 2 2 (a; b) and p 2 <ref> [0; 1] </ref>, f (px 1 + (1 p)x 2 ) pf (x 1 ) + (1 p)f (x 2 ) The function is strictly convex if, in addition, the preceding inequality is strict for p 2 (0; 1). A function f (x) is concave if f (x) is convex. <p> In this case, I (X ; Y ) H (X ) H (E) 1 + ffi log ffi + (1 ffi) log (1 ffi). For background on information theory, the texts of Gallager [10] and Cover and Thomas <ref> [1] </ref> as well as Shannon's original paper [23] are recommended. 57 Appendix B L c Distance In this appendix, we discuss the use of L c distance in place of mutual information for proving lower bounds on noisy formula depth.
Reference: [2] <author> R. L. Dobrushin and S. I. Ortyukov. </author> <title> Lower bound for the redundancy of self-correcting arrangements of unreliable functional elements. </title> <journal> Problems of Information Transmission, </journal> <volume> 13 </volume> <pages> 59-65, </pages> <year> 1977. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> This implication was made explicit in the work of Dobrushin and Ortyukov [3] and Pippenger [15]. In 1977, Dobrushin and Ortyukov <ref> [2] </ref> claimed a lower bound of W (s log s) on the reliable circuit size of any function with sensitivity 2 s. <p> We found it convenient to adopt this view of a noisy gate when proving bounds on noisy circuit depth. However, to bound noisy circuit size, we adopt a different view first proposed by Dobrushin and Ortyukov in <ref> [2] </ref>. This is only a conceptual aid. The model is still that of the *-noisy gate defined by property 4.1.1. In this new view, rather than the output wire failing with probability *, each input wire fails independently with probability !. <p> CHAPTER 4. NOISY CIRCUIT SIZE 29 Our goal is to find the values of ! for which there is a that satisfies 4.1. In other words, how noisy can we view the input wires of an *-noisy gate? The following lemma answers this question. It improves lemma 3.1 from <ref> [2] </ref> which required ! 2 [0; *=k]. As in chapter 3, let ~ be twice the bias of * from 1/2 (i.e. ~ = 1 2*).
Reference: [3] <author> R. L. Dobrushin and S. I. Ortyukov. </author> <title> Upper bound for the redundancy of self-correcting arrangements of unreliable functional elements. </title> <journal> Problems of Information Transmission, </journal> <volume> 13 </volume> <pages> 203-218, </pages> <year> 1977. </year>
Reference-contexts: Von Neumann's 1952 work implies that, for any function, there exists a reliable circuit for the function of size O (c log c) where c is the size of a noiseless circuit for the function. This implication was made explicit in the work of Dobrushin and Ortyukov <ref> [3] </ref> and Pippenger [15]. In 1977, Dobrushin and Ortyukov [2] claimed a lower bound of W (s log s) on the reliable circuit size of any function with sensitivity 2 s. <p> In 1977, Dobrushin and Ortyukov refined von Neumann's method to prove that for all functions there exist noisy circuits whose size is O (c log c) where c is the size of a noiseless circuit for the function <ref> [3] </ref>. Pippenger strengthened this result by exhibiting an explicit construction of the noisy circuit and extending the result to less powerful models of noisy computation [15]. Accompanying these positive upper bounds are lower bounds of the same general form.
Reference: [4] <author> M. Dyer, A. Frieze, and R. Kannan. </author> <title> A randomized polynomial time algorithm for approximating the volume of convex bodies. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Theory of Computing, </booktitle> <pages> pages 375-381, </pages> <year> 1989. </year>
Reference-contexts: In each case, proving a bound on the rate of convergence is the crucial CHAPTER 6. FUTURE WORK 49 step in analyzing the efficiency of the algorithm. See Dyer, Frieze, and Kannan <ref> [4] </ref> for a specific example; and Vazirani [27] and Sinclair [24] for a survey of results. Convergence is typically argued in terms of the L 1 or L 2 distance between the current distribution on the chain and the final stationary distribution.
Reference: [5] <author> P. Elias. </author> <title> Computation in the presence of noise. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 2(4) </volume> <pages> 346-353, </pages> <month> October </month> <year> 1958. </year>
Reference-contexts: channel with arbitrarily low error, is defined as the maximum mutual information between an input and output of the channel. (In an attempt to translate this result to noisy computation, Elias studied the capacity of noisy computation, but concluded that arbitrarily low error requires an arbitrarily low rate of computation <ref> [5] </ref>.) In this thesis, we view information as a measure of correlation. One of the fundamental results which support this view is the data processing inequality. Informally, it states that a function of Y cannot carry more information about X than Y itself.
Reference: [6] <author> W. Evans and L. J. Schulman. </author> <title> Signal propagation, with application to a lower bound on the depth of noisy formulas. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 594-603, </pages> <year> 1993. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> CHAPTER 3. NOISY CIRCUIT DEPTH 19 The first indication that information theoretic techniques were not extravagant and could provide extremely precise bounds on noisy computation, came in our work on lower bounding the depth of noisy formula <ref> [6] </ref>. In this chapter, we discuss our previous result and extend it to general circuits. We increase the lower bound on the factor of depth increase; and decrease the upper bound on the noise threshold.
Reference: [7] <author> T. Feder. </author> <title> Reliable computation by networks in the presence of noise. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(3) </volume> <pages> 569-571, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> Pippenger also showed that for * 1=2 1=2k, reliable computation 1 using noisy formulas is impossible. In 1989, Feder <ref> [7] </ref> extended Pippenger's result to general circuits by abandoning the information theoretic approach for a strictly probabilistic argument. <p> Finally, Pippenger showed through an elegant information-theoretic argument that both features were necessary, at least in noisy formulas (circuits in which the output of each gate is the input to at most one other gate) [16]. Shortly afterward Feder extended Pippenger's bound to general noisy circuits <ref> [7] </ref>. Surprisingly, Feder's extension to circuits (which obtained the same threshold and factor of depth increase bounds as Pippenger) did not argue in terms of mutual information. Thus, Pippenger's use of information theoretic techniques to attack this problem seemed extravagant. <p> He showed that if * 1=2 1=2k then reliable computation by formula is impossible using *-noisy, k-input gates. This implies that F k 1=2 1=2k (e.g. F 3 1=3). Soon after, Feder extended this result to general circuits, showing C k 1=2 1=2k <ref> [7] </ref>.
Reference: [8] <author> P. Gacs and A. Gal. </author> <title> Lower bounds for the complexity of reliable boolean circuits with noisy gates. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 40(2) </volume> <pages> 579-583, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> In 1991, this asymptotic behavior was proved by both Gal (for ffi &lt; 1=4) [9] and independently by Reischuk and Schmeltz [22]. Recently, Gacs and Gal extended Gal's result to all values ffi &lt; 1=2 <ref> [8] </ref>. The improvement presented in this thesis is in the dependence of the bound on the noise * introduced by each gate. In particular, we show that as * approaches 1=2, the size of the circuit increases unboundedly.
Reference: [9] <author> A. Gal. </author> <title> Lower bounds for the complexity of reliable boolean circuits with noisy gates. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 594-601, </pages> <year> 1991. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> INTRODUCTION 5 showed that this work contains serious flaws. By a different argument, they were able to prove an W (n log n) bound on the size of reliable circuits computing the parity of n inputs. In 1991, Gal <ref> [9] </ref> and Reischuk and Schmeltz [22] successfully reproved Dobrushin and Ortyukov's original claim, that functions with sensitivity s require reliable circuits of size W (s log s). Our lower bound is asymptotically equivalent to these previous bounds as a function of the sensitivity of the function being computed. <p> Pippenger et. al. showed that this work contains serious flaws [20]. By a different argument, they were able to prove that noisy circuits computing the parity of n bits require W (n log n) size. Gal <ref> [9] </ref> and Reischuk and Schmeltz [22] successfully reproved Dobrushin and Ortyukov's original claim, that all functions with sensitivity s require W (s log s) size noisy circuits. This statement treats as constants the noise of each component, the reliability requirement of the noisy circuit, and the complexity of the basis. <p> For fixed k, *, and ffi, theorem 4.2.1 implies that the size of a noisy circuit to compute a function with sensitivity s is W (s log s). In 1991, this asymptotic behavior was proved by both Gal (for ffi &lt; 1=4) <ref> [9] </ref> and independently by Reischuk and Schmeltz [22]. Recently, Gacs and Gal extended Gal's result to all values ffi &lt; 1=2 [8]. The improvement presented in this thesis is in the dependence of the bound on the noise * introduced by each gate. <p> Thus, for example, our result extends to the Static Noisy Decision Tree model just as in [22]. CHAPTER 4. NOISY CIRCUIT SIZE 34 4.2.2 Block Sensitivity As in <ref> [9] </ref>, theorem 4.2.1 also holds when the sensitivity of f is replaced by the block sensitivity of f .
Reference: [10] <author> R. G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: In this case, I (X ; Y ) H (X ) H (E) 1 + ffi log ffi + (1 ffi) log (1 ffi). For background on information theory, the texts of Gallager <ref> [10] </ref> and Cover and Thomas [1] as well as Shannon's original paper [23] are recommended. 57 Appendix B L c Distance In this appendix, we discuss the use of L c distance in place of mutual information for proving lower bounds on noisy formula depth.
Reference: [11] <author> B. Hajek and T. Weller. </author> <title> On the maximum tolerable noise for reliable computation by formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(2) </volume> <pages> 388-391, </pages> <month> March </month> <year> 1991. </year> <note> BIBLIOGRAPHY 65 </note>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> Strangely, our result requires the component noise to be precise. In other words, that each component must fail independently with precisely probability *. This extends work done by Hajek and Weller <ref> [11] </ref> who prove a tight threshold for computation by formulas using 3-input gates. 1.4 Information Theory In order to set the stage for our discussion, we must introduce the tools from information theory which we will be using. <p> In chapter 3, we used more precise bounds on the information decay caused by noisy components to show that C k 1=2 1=2 p In 1991, Hajek and Weller used a completely different technique to prove a tight threshold for reliable computation by formulas with noisy 3-input gates <ref> [11] </ref>, showing that F 3 = 1=6. In this chapter, we extend the work of Hajek and Weller to prove a tight threshold for reliable computation by formulas using noisy, k-input gates (k odd). <p> (k t; a)) The inequality holds term by term since i dk=2e implies a k k a) ki a i . 2 5.4 Reduction lemmas The above proof relies on two lemmas which are rather straight-forward extensions of similar lemmas for k = 3 given by Hajek and Weller in <ref> [11] </ref>. An *-noisy, k-input gate g takes as input Y 1 ; : : : ; Y k , described by points Y 1 ; : : : ; Y k , and outputs Y described by Y . <p> The output of XNAND on input (X; Y 1 ; Y 2 ) is intended to be a reliable version of NAND on input (x; y). The following lemma makes this connection precise. Lemma 5.5.1 (Lemma 3.1 from <ref> [11] </ref>) For *; - 2 [0; 1=2) there is a ffi &lt; 1=2 and an open interval I with - 2 I [0; 1=2] so that the following is true. <p> Note that this implies F k fi k . Proof: The proof is a simple extension of Proposition 3 from Hajek and Weller <ref> [11] </ref>. For k odd (k 3), an *-noisy XNAND gate can be implemented by an *-noisy, k-input gate which ignores all but three of its inputs.
Reference: [12] <author> R. W. </author> <title> Hamming. Error detecting and error correcting codes. </title> <journal> Bell Syst. Tech. J., </journal> <volume> 29 </volume> <pages> 147-160, </pages> <month> April </month> <year> 1950. </year> <title> Also in Key Papers in the Development of Coding Theory, </title> <editor> E. R. Berlekamp (Ed), </editor> <publisher> IEEE Press, N.Y., </publisher> <pages> pages 9-12, </pages> <year> 1974. </year>
Reference-contexts: The problem of signal decay is not restricted to communication: that it plagues long computations, as well, was all too apparent to the first users of electronic computers, and was for example the spur for Hamming's interest in coding theory <ref> [12] </ref>. Von Neumann recognized that, rather than being technological and passing, this signal decay was an essential difficulty for large-scale computations, which by their nature rely on the propagation of long chains of events [28].
Reference: [13] <author> G. Hardy, J. E. Littlewood, and G. Polya. </author> <title> Inequalities. </title> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1952. </year>
Reference-contexts: See theorem 25 in <ref> [13] </ref>. APPENDIX B.
Reference: [14] <author> W. S. McCulloch and W. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bull. of Math. Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: To make quite clear the mechanism of computation, we adopt a model of noisy computation, inspired by the work of Turing [25] and McCulloch and CHAPTER 1. INTRODUCTION 3 Pitts <ref> [14] </ref>, which was first proposed by von Neumann in 1952 [28]. The model of computation is the noisy circuit. A circuit takes n Boolean values as input and produces one Boolean output. It is composed of a collection of individual components called gates.
Reference: [15] <author> N. Pippenger. </author> <title> On networks of noisy gates. </title> <booktitle> In Proceedings of the 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 30-36, </pages> <year> 1985. </year>
Reference-contexts: This implication was made explicit in the work of Dobrushin and Ortyukov [3] and Pippenger <ref> [15] </ref>. In 1977, Dobrushin and Ortyukov [2] claimed a lower bound of W (s log s) on the reliable circuit size of any function with sensitivity 2 s. <p> Pippenger strengthened this result by exhibiting an explicit construction of the noisy circuit and extending the result to less powerful models of noisy computation <ref> [15] </ref>. Accompanying these positive upper bounds are lower bounds of the same general form. In particular, Dobrushin and Ortyukov claimed that W (s log s) noisy circuit size is necessary to compute functions reliably where s is the sensitivity 1 of the function.
Reference: [16] <author> N. Pippenger. </author> <title> Reliable computation by formulas in the presence of noise. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34(2) </volume> <pages> 194-197, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Surprisingly, it took over thirty-five years before the tools developed by Shannon to study information and communication were successfully applied to the problem of noisy computation, in the work of Pippenger <ref> [16] </ref>. During that time, Shannon's methods proved so useful for communication that an entire area devoted to the study of information developed. <p> So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> The data processing inequality establishes an upper bound of 1 on this fraction. In his paper which pioneered the use of information for studying noisy computation, Pippenger <ref> [16] </ref> determined an upper bound of ~ on the fraction for symmetric binary channels which have probability * = (1 ~)=2 of complementing their input. We determine an exact upper bound on the fraction for any binary channel. <p> The construction works as long as * (the noise at each gate) is less than some threshold. In 1988, Pippenger <ref> [16] </ref> proved the first lower bound on reliable circuit depth by using information theoretic techniques to prove that there exist functions whose reliable formula depth is at least 1= log k (k~) times their noiseless formula depth where k is the number of inputs to each gate and ~ = 1 <p> first inequality of this type on the ratio I (X; Z)=I (X; Y ) was derived by Pippenger (for binary symmetric channels) as a key step in his method for showing a lower bound on the depth, and an upper bound on the maximum tolerable component noise, of noisy formulas <ref> [16] </ref>. CHAPTER 2. SIGNAL DECAY 8 In this chapter, we obtain an exact upper bound on the maximum achievable signal strength ratio I (X; Z)=I (X; Y ), for every binary channel (two input values and two output values). <p> Finally, Pippenger showed through an elegant information-theoretic argument that both features were necessary, at least in noisy formulas (circuits in which the output of each gate is the input to at most one other gate) <ref> [16] </ref>. Shortly afterward Feder extended Pippenger's bound to general noisy circuits [7]. Surprisingly, Feder's extension to circuits (which obtained the same threshold and factor of depth increase bounds as Pippenger) did not argue in terms of mutual information. <p> See figure 3.2. CHAPTER 3. NOISY CIRCUIT DEPTH 25 Von Neumann's original construction implies a depth increase by a factor of 2 for * &lt; 0:0073 when computation is performed by 3-input majority gates. Pippenger <ref> [16] </ref> provides a more careful analysis of von Neumann's method and shows that, for computation by 3-input parity gates, as * ! 0, the factor is asymptotic to 1=(1 2= log 3 (1=*)). <p> This suggested to von Neumann that perhaps reliable computation is not possible by *-noisy, 3-input gates if * 1=6. The first proof of the impossibility of reliable computation by noisy components came in 1988 from Pippenger's work on formula depth bounds <ref> [16] </ref>. He showed that if * 1=2 1=2k then reliable computation by formula is impossible using *-noisy, k-input gates. This implies that F k 1=2 1=2k (e.g. F 3 1=3). Soon after, Feder extended this result to general circuits, showing C k 1=2 1=2k [7]. <p> Let X and Y be Boolean random variables. One measure of the correlation between X and Y is the mutual information I (X ; Y ). This is the measure used by Pippenger in proving the first lower bounds on noisy formula depth <ref> [16] </ref>. Mutual information is an attractive measure for this purpose because it possesses three properties: 1. If X and Y are equal with high probability then I (X ; Y ) is large. 2.
Reference: [17] <author> N. Pippenger. </author> <title> Analysis of error correction by majority voting. </title> <booktitle> Advances in Computing Research, </booktitle> <volume> 5 </volume> <pages> 171-198, </pages> <year> 1989. </year>
Reference-contexts: Our result implies that this factor is at least 1=(1 2 log 3 (1=~)) which is asymptotic to 4*= ln 3. Von Neumann's construction uses majority voting to reduce error. For a thorough discussion of error correction by majority voting, see Pippenger <ref> [17] </ref>. Our depth bounds can be easily extended to the case of asymmetric noise in which a component fails with probability that depends on its pre-noise output. The model allows the possibility that a gate may fail with a different probability when producing a 0 than when producing a 1.
Reference: [18] <author> N. Pippenger. </author> <title> Invariance of complexity measures for networks with unreliable gates. </title> <journal> J. ACM, </journal> <volume> 36 </volume> <pages> 531-539, </pages> <year> 1989. </year>
Reference-contexts: In effect, precise noise permits the construction of random bits, allowing one to implement randomized algorithms. This objection has been addressed by designing models which are more realistic. (See <ref> [18] </ref> for examples and [19] for a survey of results in the theory of reliable computation.) Most of the results in this thesis are negative results or lower bounds, saying that reliable, noisy computation cannot be done under various conditions.
Reference: [19] <author> N. Pippenger. </author> <title> Developments in "the synthesis of reliable organisms from unreliable components". </title> <booktitle> In Proceedings of Symposia in Pure Mathematics, </booktitle> <volume> volume 50, </volume> <pages> pages 311-324, </pages> <year> 1990. </year>
Reference-contexts: In effect, precise noise permits the construction of random bits, allowing one to implement randomized algorithms. This objection has been addressed by designing models which are more realistic. (See [18] for examples and <ref> [19] </ref> for a survey of results in the theory of reliable computation.) Most of the results in this thesis are negative results or lower bounds, saying that reliable, noisy computation cannot be done under various conditions.
Reference: [20] <author> N. Pippenger, G. D. Stamoulis, and J. N. Tsitsiklis. </author> <title> On a lower bound for the redundancy of reliable networks with noisy gates. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(3) </volume> <pages> 639-643, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This implication was made explicit in the work of Dobrushin and Ortyukov [3] and Pippenger [15]. In 1977, Dobrushin and Ortyukov [2] claimed a lower bound of W (s log s) on the reliable circuit size of any function with sensitivity 2 s. In 1991, Pippenger, Stamoulis, and Tsitsiklis <ref> [20] </ref> 1 Reliable computation refers to the ability to (1 ffi)-reliably compute all Boolean functions for some fixed ffi &lt; 1=2. 2 A function's sensitivity is the maximum (over all input vectors) of the number of bits which change the function CHAPTER 1. <p> In particular, Dobrushin and Ortyukov claimed that W (s log s) noisy circuit size is necessary to compute functions reliably where s is the sensitivity 1 of the function. Pippenger et. al. showed that this work contains serious flaws <ref> [20] </ref>. By a different argument, they were able to prove that noisy circuits computing the parity of n bits require W (n log n) size.
Reference: [21] <author> Y. Rabinovich, A. Sinclair, and A. Wigderson. </author> <title> Quadratic dynamical systems. </title> <booktitle> In Proceedings of the 33nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 304-313, </pages> <year> 1992. </year>
Reference-contexts: The system evolves by applying the function f to the current configuration of the system (a point in S) to obtain a new configuration in S (for a more complete discussion see <ref> [21] </ref>). Markov chains are one of the simplest examples of a dynamical system. For 1 Conductance is the minimum over all bipartitions of the chain of the probability of escaping the smaller partition in one time step (assuming initially the uniform distribution on the smaller partition). CHAPTER 6. <p> The hope is that a bound on this decay will translate into a bound on the rate of convergence. 2 An example of such a system is Maxwell's (space homogeneous) kinetic gas model. See <ref> [21] </ref>. 51 Appendix A Information Theory This appendix is meant as a quick review of some of the properties of entropy and information. The results presented here should provide the reader with some intuition for the quantities which play the major roles in information theory.
Reference: [22] <author> R. Reischuk and B. Schmeltz. </author> <title> Reliable computation with noisy circuits and decision trees a general n log n lower bound. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 602-611, </pages> <year> 1991. </year>
Reference-contexts: So assuming the most powerful model of computation leads to the widest applicability. Thus von Neumann's noise model is an appropriate choice as our model CHAPTER 1. INTRODUCTION 4 of noisy computation. It has been used to obtain virtually all lower bounds on noisy circuit complexity <ref> [2, 16, 7, 11, 9, 22, 6, 8] </ref>. 1.3 Outline of Results Chapter 2 presents a new information theoretic result which bounds the fraction of information that can cross a noisy channel. The data processing inequality establishes an upper bound of 1 on this fraction. <p> INTRODUCTION 5 showed that this work contains serious flaws. By a different argument, they were able to prove an W (n log n) bound on the size of reliable circuits computing the parity of n inputs. In 1991, Gal [9] and Reischuk and Schmeltz <ref> [22] </ref> successfully reproved Dobrushin and Ortyukov's original claim, that functions with sensitivity s require reliable circuits of size W (s log s). Our lower bound is asymptotically equivalent to these previous bounds as a function of the sensitivity of the function being computed. <p> Pippenger et. al. showed that this work contains serious flaws [20]. By a different argument, they were able to prove that noisy circuits computing the parity of n bits require W (n log n) size. Gal [9] and Reischuk and Schmeltz <ref> [22] </ref> successfully reproved Dobrushin and Ortyukov's original claim, that all functions with sensitivity s require W (s log s) size noisy circuits. This statement treats as constants the noise of each component, the reliability requirement of the noisy circuit, and the complexity of the basis. <p> For fixed k, *, and ffi, theorem 4.2.1 implies that the size of a noisy circuit to compute a function with sensitivity s is W (s log s). In 1991, this asymptotic behavior was proved by both Gal (for ffi &lt; 1=4) [9] and independently by Reischuk and Schmeltz <ref> [22] </ref>. Recently, Gacs and Gal extended Gal's result to all values ffi &lt; 1=2 [8]. The improvement presented in this thesis is in the dependence of the bound on the noise * introduced by each gate. <p> Thus, for example, our result extends to the Static Noisy Decision Tree model just as in <ref> [22] </ref>. CHAPTER 4. NOISY CIRCUIT SIZE 34 4.2.2 Block Sensitivity As in [9], theorem 4.2.1 also holds when the sensitivity of f is replaced by the block sensitivity of f .
Reference: [23] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Tech. J., </journal> <volume> 27 </volume> <pages> 379-423; 623-656, </pages> <year> 1948. </year>
Reference-contexts: Von Neumann's goal was to subject noisy computation to the same thermodynamical treatment as communication had received in the contemporary work of Shannon <ref> [23] </ref>. Surprisingly, it took over thirty-five years before the tools developed by Shannon to study information and communication were successfully applied to the problem of noisy computation, in the work of Pippenger [16]. <p> In this case, I (X ; Y ) H (X ) H (E) 1 + ffi log ffi + (1 ffi) log (1 ffi). For background on information theory, the texts of Gallager [10] and Cover and Thomas [1] as well as Shannon's original paper <ref> [23] </ref> are recommended. 57 Appendix B L c Distance In this appendix, we discuss the use of L c distance in place of mutual information for proving lower bounds on noisy formula depth.
Reference: [24] <author> A. Sinclair. </author> <title> Algorithms for Random Generation and Counting : A Markov Chain Approach. </title> <booktitle> Progress in Theoretical Computer Science. </booktitle> <publisher> Birkhauser, </publisher> <year> 1993. </year>
Reference-contexts: In each case, proving a bound on the rate of convergence is the crucial CHAPTER 6. FUTURE WORK 49 step in analyzing the efficiency of the algorithm. See Dyer, Frieze, and Kannan [4] for a specific example; and Vazirani [27] and Sinclair <ref> [24] </ref> for a survey of results. Convergence is typically argued in terms of the L 1 or L 2 distance between the current distribution on the chain and the final stationary distribution. Each step of the Markov chain is shown to reduce this distance by a multiplicative constant.
Reference: [25] <author> A. M. </author> <title> Turing. On the computable numbers, with an application to the Entscheidungsproblem. </title> <journal> Proc. London Math. Soc., </journal> <pages> pages 230-265, </pages> <year> 1936. </year> <note> BIBLIOGRAPHY 66 </note>
Reference-contexts: To make quite clear the mechanism of computation, we adopt a model of noisy computation, inspired by the work of Turing <ref> [25] </ref> and McCulloch and CHAPTER 1. INTRODUCTION 3 Pitts [14], which was first proposed by von Neumann in 1952 [28]. The model of computation is the noisy circuit. A circuit takes n Boolean values as input and produces one Boolean output.
Reference: [26] <author> L. G. Valiant. </author> <title> Short monotone formulae for the majority function. </title> <journal> Journal of Algorithms, </journal> <volume> 5 </volume> <pages> 363-366, </pages> <year> 1984. </year>
Reference-contexts: Given inputs with symmetric error, one introduces bias towards 1 and the other bias towards 0. Repeatedly using the same one skews the error probability more and more. In order to dampen the bias, Valiant <ref> [26] </ref> proposed alternating and and or gates level by level. This scheme decreases error if the gate noise is below a certain threshold. However, it is not clear that this is the best way to decrease error for noise values at and above that threshold.
Reference: [27] <author> U. Vazirani. </author> <title> Rapidly mixing Markov chains. </title> <booktitle> Proceedings of Symposia in Applied Mathematics, </booktitle> <volume> 44 </volume> <pages> 99-121, </pages> <year> 1991. </year>
Reference-contexts: In each case, proving a bound on the rate of convergence is the crucial CHAPTER 6. FUTURE WORK 49 step in analyzing the efficiency of the algorithm. See Dyer, Frieze, and Kannan [4] for a specific example; and Vazirani <ref> [27] </ref> and Sinclair [24] for a survey of results. Convergence is typically argued in terms of the L 1 or L 2 distance between the current distribution on the chain and the final stationary distribution.
Reference: [28] <author> J. von Neumann. </author> <title> Probabilistic logics and the synthesis of reliable organisms from unreliable components. </title> <editor> In C. E. Shannon and J. McCarthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <pages> pages 43-98. </pages> <publisher> Princeton University Press, </publisher> <year> 1956. </year>
Reference-contexts: Von Neumann recognized that, rather than being technological and passing, this signal decay was an essential difficulty for large-scale computations, which by their nature rely on the propagation of long chains of events <ref> [28] </ref>. Von Neumann's goal was to subject noisy computation to the same thermodynamical treatment as communication had received in the contemporary work of Shannon [23]. <p> To make quite clear the mechanism of computation, we adopt a model of noisy computation, inspired by the work of Turing [25] and McCulloch and CHAPTER 1. INTRODUCTION 3 Pitts [14], which was first proposed by von Neumann in 1952 <ref> [28] </ref>. The model of computation is the noisy circuit. A circuit takes n Boolean values as input and produces one Boolean output. It is composed of a collection of individual components called gates. Each gate in the circuit is one of a finite set of allowable gates called the basis. <p> It will be an essential part of the lower bound in chapter 3. The remaining chapters of the thesis are devoted to presenting several lower bounds on reliable, noisy circuit complexity. Chapter 3 concerns the depth of noisy circuits. In 1952, Von Neumann <ref> [28] </ref> provided the first upper bound on reliable circuit depth by showing, for any function, how to construct a reliable circuit for the function which is no more than a constant factor deeper than its noiseless circuit. <p> We will show that this is indeed the case for certain functions. Noisy circuit size has received somewhat more attention than noisy circuit depth. Von Neumann planted the seeds of this investigation by heuristically bounding the redundancy required for computation using noisy components <ref> [28] </ref>. In 1977, Dobrushin and Ortyukov refined von Neumann's method to prove that for all functions there exist noisy circuits whose size is O (c log c) where c is the size of a noiseless circuit for the function [3].
References-found: 28

