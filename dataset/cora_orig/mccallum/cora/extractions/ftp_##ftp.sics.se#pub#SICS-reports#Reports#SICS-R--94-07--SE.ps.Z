URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--94-07--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Email: psm@sics.se landin@sics.se erik.hagersten@eng.sun.com  
Title: Efficient Software Synchronization on Large Cache Coherent Multiprocessors  
Author: Peter Magnusson Anders Landin Erik Hagersten 
Date: February 18, 1994  
Address: Box 1263, S-164 28 Kista, SWEDEN  
Affiliation: Swedish Institute of Computer Science  
Pubnum: SICS Research Report T94:07  
Abstract: Large-scale shared-memory multiprocessors typically have long latencies for remote data accesses. A key issue for execution performance of many common applications is the synchronization cost. The communication scalability of synchronization has been improved by the introduction of queue-based spin-locks instead of Test&(Test&Set). For architectures with long access latencies for global data, attention should also be paid to the number of global accesses that are involved in synchronization. We present a method to characterize the performance of proposed queue lock algorithms, and apply it to previously published algorithms. We also present two new queue locks, the LH lock and the M lock. We compare the locks in terms of performance, memory requirements, code size, and required hardware support. The LH lock is the simplest of all the locks, yet requires only an atomic swap operation. The M lock is superior in terms of global accesses needed to perform synchronization and still competitive in all other criteria. We conclude that the M lock is the best overall queue lock for the class of architectures studied. fl Currently with Sun Microsystems, work done while at SICS.
Abstract-found: 1
Intro-found: 1
Reference: [And90] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: To remedy this, Goodman suggested the queue-on-sync bits [GVW89], which involves local spinning on a synchronization flag, thereby eliminating the O (N 2 ) network operations. This solution was meant to be implemented in hardware, but Graunke and Thakkar [GT90] and Andersson <ref> [And90] </ref> implemented similar concepts in software. The queueing principle addresses the scalability of a lock in terms of number of contending processors. Another scalability issue is performance|large shared memory architectures are today designed with some form of memory hierarchy. <p> next item to spin upon, P will refer to the (local or cached) item that the processor eventually spins upon, and I the item that the next processor in the queue spins upon (i.e., the next processor's P). 2.1 Anderson's queue lock Anderson describes an array-based lock for shared-memory multiprocessors <ref> [And90] </ref>. A similar approach was independently developed by Graunke and Thakkar (see section 2.2). An array of flags is used to allocate a unique flag for each processor that might attempt to take the lock. This flag is cached and the processor can thus read-spin locally, relieving the network.
Reference: [AT92] <author> R. Alur and G. Taubenfeld. </author> <title> Results about fast mutual exclusion. </title> <booktitle> In Proceedings of the 1992 Real-Time Symposium, </booktitle> <address> Phoenix, Arizona, </address> <month> December 2-4, </month> <pages> pages 12-21. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: The earliest work in the area assumed that the most powerful atomic primitive was a read or a write [Lam78, LL77]. For various reasons, we are not interested in such a restriction: to begin with, such an algorithm can never be bounded <ref> [AT92] </ref>. The inefficiency of these algorithms prompted development of more powerful hardware primitives. These primitives, such as atomic swap, were sufficiently powerful to implement trivial locking algorithms.
Reference: [Cra93] <author> T. S. Craig. </author> <title> Building FIFO and Priority-Queueing Spin Locks from Atomic Swap. </title> <type> Technical Report 93-02-02, </type> <institution> Department of Computer Science and Engineering, FR-35, University of Washington, </institution> <month> February </month> <year> 1993. </year> <note> Available via anonymous ftp from "cs.washington.edu" as "tr/1993/02/UW-CSE-93-02-02.PS.Z". </note>
Reference-contexts: The same lock has been developed independently by Craig <ref> [Cra93] </ref>. Each lock requires an initial global pointer, L, that points to a global memory space for a flag. The flag is initially set to free. The location of the pointer is statically known by all processors.
Reference: [Dig92] <institution> Digital Equipment Corp. Alpha Architecture Handbook, </institution> <year> 1992. </year>
Reference-contexts: The smaller the code, the less of an impact for in-lining the locks. The compare-and-clear can be just as easily implemented as atomic swap on current architectures, such as the R4000 and Alpha <ref> [MIP91, Dig92] </ref>. Hence we are not too concerned with the atomic operation requirements. Nevertheless, if only an atomic swap is available, only the LH and GT locks qualify. Memory requirements fall into two categories: O (LP ) vs O (L+P ).
Reference: [GT90] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: To remedy this, Goodman suggested the queue-on-sync bits [GVW89], which involves local spinning on a synchronization flag, thereby eliminating the O (N 2 ) network operations. This solution was meant to be implemented in hardware, but Graunke and Thakkar <ref> [GT90] </ref> and Andersson [And90] implemented similar concepts in software. The queueing principle addresses the scalability of a lock in terms of number of contending processors. Another scalability issue is performance|large shared memory architectures are today designed with some form of memory hierarchy. <p> His implementation on the Sequent Symmetry Model B, however, did not use an atomic fetch-and-increment, since the Symmetry's atomic add instruction does not return the previous value. Instead, Anderson added an outer lock to protect the enqueue operation. 2.2 Graunke and Thakkar's queue lock Graunke and Thakkar <ref> [GT90] </ref>, independently from Anderson, developed a queue lock with similar characteristics. The pseudo code for their lock is shown in figure 2. The "^=" operator inverts the previous flag value, thereby releasing the lock in the present "sense." This is necessary to avoid race conditions on usage of the lock.
Reference: [GVW89] <author> J. R. Goodman, M. K. Vernon, and P.J. Woest. </author> <title> Efficient Synchronization Primitives for Large-scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 3rd Architecture Symposium for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <year> 1989. </year>
Reference-contexts: In some cases these locks were a major contributor to network contention, including the problem of so-called "hot-spots" [PN85]. To remedy this, Goodman suggested the queue-on-sync bits <ref> [GVW89] </ref>, which involves local spinning on a synchronization flag, thereby eliminating the O (N 2 ) network operations. This solution was meant to be implemented in hardware, but Graunke and Thakkar [GT90] and Andersson [And90] implemented similar concepts in software.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, Clocks and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <year> 1978. </year>
Reference-contexts: The difficulties involved in designing synchronization primitives vary greatly with the nature of the underlying hardware. The earliest work in the area assumed that the most powerful atomic primitive was a read or a write <ref> [Lam78, LL77] </ref>. For various reasons, we are not interested in such a restriction: to begin with, such an algorithm can never be bounded [AT92]. The inefficiency of these algorithms prompted development of more powerful hardware primitives. These primitives, such as atomic swap, were sufficiently powerful to implement trivial locking algorithms.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Mul-tiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: The cache coherency protocol is presumed be of the write invalidate type, i.e., all other copies are invalidated upon writes and data is replicated in the caches upon multiple reads. Finally, we assume our memory to be sequentially consistent <ref> [Lam79] </ref>. We expect most of the results to hold for weaker memory models as well, but we do not explore the issue in this report. Since we're interested in large machines, we assume that the principal distinction in memory access times is that between cache hits and misses.
Reference: [LL77] <author> G. Le Lann. </author> <title> Distributed systems: towards a formal approach. </title> <booktitle> In IFIP Congress, </booktitle> <publisher> North Holland, </publisher> <pages> pages 155-160, </pages> <year> 1977. </year>
Reference-contexts: The difficulties involved in designing synchronization primitives vary greatly with the nature of the underlying hardware. The earliest work in the area assumed that the most powerful atomic primitive was a read or a write <ref> [Lam78, LL77] </ref>. For various reasons, we are not interested in such a restriction: to begin with, such an algorithm can never be bounded [AT92]. The inefficiency of these algorithms prompted development of more powerful hardware primitives. These primitives, such as atomic swap, were sufficiently powerful to implement trivial locking algorithms.
Reference: [MCS91a] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 21-65, </pages> <year> 1991. </year>
Reference-contexts: will contend for the lock, Anderson's lock requires O (1) space per lock, whereas Graunke and Thakkar's will require O (N ). 2.3 The MCS lock Mellor-Crummey and Scott have described a queue-based spin lock that spins on local data, requires O (N + P ) space, and guarantees FIFO <ref> [MCS91a, MCS91b] </ref>. Pseudo code for the MCS lock is shown in figures 3 and 4. A global pointer L is maintained for each lock. If there is a queue, L points to a record allocated by the enqueued processor.
Reference: [MCS91b] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Synchronization Without Contention. </title> <booktitle> In Proceedings of the 4th Annual Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <year> 1991. </year>
Reference-contexts: Anderson's lock requires an atomic fetch-and-increment operation. The pseudo code for Andersons lock is given in figure 1. % is the modulus operator. N is the number of processors that can conceivably compete for the lock. As pointed out by Mellor-Crummey and Scott <ref> [MCS91b] </ref>, Andersons lock is incorrect unless the number of processors is 3 struct lock f int flag [N]; int L; g gt acquire (Q) struct lock * Q; f f = Q-&gt;flag [myid]; atomic f /* swap */ P = Q-&gt;L; Q-&gt;L = myid; Q-&gt;f = f; while (Q-&gt;flag [P] == <p> This is easily corrected, and they present one such correction <ref> [MCS91b] </ref>. 1 Anderson compared his queue lock with spin locks with various back-off strategies. In all cases, Anderson's queue lock performed worse for 1-4 processors, and better for 7 or more. <p> will contend for the lock, Anderson's lock requires O (1) space per lock, whereas Graunke and Thakkar's will require O (N ). 2.3 The MCS lock Mellor-Crummey and Scott have described a queue-based spin lock that spins on local data, requires O (N + P ) space, and guarantees FIFO <ref> [MCS91a, MCS91b] </ref>. Pseudo code for the MCS lock is shown in figures 3 and 4. A global pointer L is maintained for each lock. If there is a queue, L points to a record allocated by the enqueued processor. <p> Subsequent processors that wish to queue on the lock update the L pointer to point to a new record, and subsequently update the record previously pointed to by L to complete the link. The compare-and-swap used by the MCS lock as described in <ref> [MCS91b] </ref> does not require the full semantics of a true compare-and-swap instruction. Instead, it is sufficient with a clear-if-equal that zeroes a memory location if the contents is equal to a parameter. Mellor-Crummey and Scott also describe a version of the MCS lock that only requires an atomic swap.
Reference: [MIP91] <institution> MIPS R4000 Microprocessor User's Manual, </institution> <year> 1991. </year>
Reference-contexts: The smaller the code, the less of an impact for in-lining the locks. The compare-and-clear can be just as easily implemented as atomic swap on current architectures, such as the R4000 and Alpha <ref> [MIP91, Dig92] </ref>. Hence we are not too concerned with the atomic operation requirements. Nevertheless, if only an atomic swap is available, only the LH and GT locks qualify. Memory requirements fall into two categories: O (LP ) vs O (L+P ).
Reference: [MLH94] <author> Peter Magnusson, Anders Landin, and Erik Hagersten. </author> <title> Efficient software synchronization on large cache coherent multiprocessors. </title> <type> Technical Report T94:03, </type> <institution> Swedish Institute of Computer Science, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: In the figure, this would be (8). Also, the time to release the lock when there is a queue, case (4). These distinctions are not important in pratice. In <ref> [MLH94] </ref> the distinction is made. The different times are summarized in table 3.
Reference: [PN85] <author> G. F. Pfister and A. Norton. </author> <title> "Hot Spot" Contention and Combining in Multistage Interconenction Networks. </title> <journal> IEEE Transactions on Computers, </journal> <month> October </month> <year> 1985. </year> <month> 31 </month>
Reference-contexts: In some cases these locks were a major contributor to network contention, including the problem of so-called "hot-spots" <ref> [PN85] </ref>. To remedy this, Goodman suggested the queue-on-sync bits [GVW89], which involves local spinning on a synchronization flag, thereby eliminating the O (N 2 ) network operations. This solution was meant to be implemented in hardware, but Graunke and Thakkar [GT90] and Andersson [And90] implemented similar concepts in software.
Reference: [SR84] <author> Z. Segall and L. Rudolph. </author> <title> Dynamic Decentralized Cache Schemes for MIMD Par--allel Processors. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <month> June </month> <year> 1984. </year> <month> 32 </month>
Reference-contexts: The inefficiency of these algorithms prompted development of more powerful hardware primitives. These primitives, such as atomic swap, were sufficiently powerful to implement trivial locking algorithms. The common problem with these locks, such as the Test&(Test&Set) lock <ref> [SR84] </ref>, is that they required O (N 2 ) network transactions to service N processors attempting to enter a critical section simultaneously. In some cases these locks were a major contributor to network contention, including the problem of so-called "hot-spots" [PN85].
References-found: 15

