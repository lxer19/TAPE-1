URL: http://www.cs.berkeley.edu/~alanm/CP/blumrich.isca21.94.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: Early Experience with Message-Passing on the SHRIMP Multicomputer  
Author: Edward W. Felten, Richard D. Alpert, Angelos Bilas, Matthias A. Blumrich, Douglas W. Clark, Stefanos N. Damianakis, Cezary Dubnicki, Liviu Iftode, and Kai Li 
Address: Princeton, NJ 08544 USA  
Affiliation: Department of Computer Science Princeton University  
Note: In Proceedings of the 23rd Annual International Symposium on Computer Architecture, May, 1996, pp. 296-307.  
Abstract: The SHRIMP multicomputer provides virtual memory-mapped communication (VMMC), which supports protected, user-level message passing, allows user programs to perform their own buffer management, and separates data transfers from control transfers so that a data transfer can be done without the intervention of the receiving node CPU. An important question is whether such a mechanism can indeed deliver all of the available hardware performance to applications which use conventional message-passing libraries. This paper reports our early experience with message-passing on a small, working SHRIMP multicomputer. We have implemented several user-level communication libraries on top of the VMMC mechanism, including the NX message-passing interface, Sun RPC, stream sockets, and specialized RPC. The first three are fully compatible with existing systems. Our experience shows that the VMMC mechanism supports these message-passing interfaces well. When zero-copy protocols are allowed by the semantics of the interface, VMMC can effectively deliver to applications almost all of the raw hardware's communication performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Kai Li. </author> <title> Design and Implementation of NX Message Passing Using SHRIMP Virtual Memory Mapped Communication. </title> <type> Technical Report TR-507-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested. Full discussions of the three libraries are found elsewhere <ref> [1, 6, 16] </ref>; here we give only brief descriptions of how the libraries operate in the common cases. 0 16 32 48 64 Message Size (bytes) 0 20 40 One-Way Latency (microseconds) AU-1copy AU-2copy DU-0copy DU-1copy 0K 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Message Size (bytes) 0 <p> A small "bump" is seen where the protocol changes. For in-depth analysis see <ref> [1] </ref>. 4.2 Remote Procedure Call VRPC is a fast implementation of remote procedure call (RPC) for SHRIMP, fully compatible with the SunRPC standard. In implementing VRPC we changed only the SunRPC runtime library; the stub generator and the operating system kernel are unchanged.
Reference: [2] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. A Distributed Implementation of the Shared Data-Object Model. </title> <booktitle> In USENIX Workshop on Experiences with Building Distributed and Multiprocessor Systems, </booktitle> <pages> pages 1-19, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call [8, 37, 4], ordered multicast [7], and object-oriented models <ref> [2, 12, 28] </ref>. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [3] <institution> BCPR Services Inc. </institution> <note> EISA Specification, Version 3.12, </note> <year> 1992. </year>
Reference-contexts: The Xpress memory bus has a maximum burst write bandwidth of 73 Mbytes/sec, and includes a memory expansion connector which carries the majority of the bus signals. Peripherals are connected to the system through the EISA expansion bus <ref> [3] </ref>, which has main memory mastering capability and a maximum burst bandwidth of 33 Mbytes/sec. Main memory data can be cached by the CPU as write-through or write-back on a per-virtual-page basis, as specified in process page tables.
Reference: [4] <author> B.N. Bershad, T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Lightweight Remote Procedure Call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call <ref> [8, 37, 4] </ref>, ordered multicast [7], and object-oriented models [2, 12, 28]. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [5] <author> Brian N. Bershad, Thomas E. Anderson, Edward D. La-zowska, and Henry M. Levy. </author> <title> User-Level Interprocess Communication for Shared Memory Multiprocessors. </title> <journal> ACM Trans. Comput. Sys., </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The stub generator and runtime library were designed with SHRIMP in mind, so we believe they come close to the best possible RPC performance on the SHRIMP hardware. Buffer Management The design of SHRIMP RPC is similar to Bershad's URPC <ref> [5] </ref>. Each RPC binding consists of one receive buffer on each side (client and server) with bidirectional import-export mappings between them. When a call occurs, the client-side stub marshals the arguments into its buffer, and then transmits them into the server's buffer.
Reference: [6] <author> Angelos Bilas and Edward W. Felten. </author> <title> Fast RPC on the SHRIMP Virtual Memory Mapped Network Interface. </title> <type> Technical Report TR-512-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested. Full discussions of the three libraries are found elsewhere <ref> [1, 6, 16] </ref>; here we give only brief descriptions of how the libraries operate in the common cases. 0 16 32 48 64 Message Size (bytes) 0 20 40 One-Way Latency (microseconds) AU-1copy AU-2copy DU-0copy DU-1copy 0K 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Message Size (bytes) 0 <p> About 7 secs are spent in preparing the header and making the call, 1-2 secs in returning from the call and the remaining 5-6 secs in processing the header. More details are found in <ref> [6] </ref>. Further optimizations It is also possible in principle to eliminate the receiver-side copy. This violates the initial constraint that the stub generator remain unchanged.
Reference: [7] <author> K.S. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight Causal and Atomic Group Multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call [8, 37, 4], ordered multicast <ref> [7] </ref>, and object-oriented models [2, 12, 28]. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [8] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call <ref> [8, 37, 4] </ref>, ordered multicast [7], and object-oriented models [2, 12, 28]. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [9] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin [32] and its successor SESAME [43], the Plus system <ref> [9] </ref>, and GalacticaNet [27]. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system [42] supports user-level message passing, but requires application programs to build packet headers. They have not tried to implement message-passing libraries using the underlying communication mechanism.
Reference: [10] <author> Matthias Blumrich, Cezary Dubnick, Edward Felten, and Kai Li. </author> <title> Protected, User-Level DMA for the SHRIMP Network Interface. </title> <booktitle> In IEEE 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 154-165, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: These accesses specify the source address, destination address, and size of a transfer. The ordinary virtual memory protection mechanisms (MMU and page tables) are used to maintain protection <ref> [10] </ref>. VMMC guarantees the in-order, reliable delivery of all data transfers, provided that the ordinary, blocking version of the deliberate-update send operation is used.
Reference: [11] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [11, 21, 13] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [30, 19].
Reference: [12] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call [8, 37, 4], ordered multicast [7], and object-oriented models <ref> [2, 12, 28] </ref>. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [13] <author> William J. Dally. </author> <title> The J-Machine System. </title> <editor> In P.H. Winston and S.A. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <pages> pages 550-580. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [11, 21, 13] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [30, 19].
Reference: [14] <author> William J. Dally and Charles L. Seitz. </author> <title> The Torus Routing Chip. </title> <journal> Distributed Computing, </journal> <volume> 1 </volume> <pages> 187-196, </pages> <year> 1986. </year>
Reference-contexts: The network connecting the nodes is an Intel routing backplane consisting of a two-dimensional mesh of Intel Mesh Routing Chips (iMRCs) [40], and is the same network used in the Paragon multicomputer [24]. The iMRC is essentially a wider, faster version of the Caltech Mesh Routing Chip <ref> [14] </ref>. The backplane supports deadlock-free, oblivious wormhole routing [15], and preserves the order of messages from each sender to each receiver. In addition to the fast backplane interconnect, the PC nodes are connected by a commodity Ethernet, which is used for diagnostics, booting, and exchange of low-priority messages.
Reference: [15] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock-Free Message Routing in Multiprocessor Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: The iMRC is essentially a wider, faster version of the Caltech Mesh Routing Chip [14]. The backplane supports deadlock-free, oblivious wormhole routing <ref> [15] </ref>, and preserves the order of messages from each sender to each receiver. In addition to the fast backplane interconnect, the PC nodes are connected by a commodity Ethernet, which is used for diagnostics, booting, and exchange of low-priority messages.
Reference: [16] <author> Stefanos Damianakis, Cezary Dubnicki, and Edward W. Felten. </author> <title> Stream Sockets on SHRIMP. </title> <type> Technical Report TR-513-96, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested. Full discussions of the three libraries are found elsewhere <ref> [1, 6, 16] </ref>; here we give only brief descriptions of how the libraries operate in the common cases. 0 16 32 48 64 Message Size (bytes) 0 20 40 One-Way Latency (microseconds) AU-1copy AU-2copy DU-0copy DU-1copy 0K 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Message Size (bytes) 0 <p> Our own one-way transfer microbenchmark obtained a bandwidth of 9.8 MB/s for 7 kbyte messages. Finally, ttcp measured a bandwidth of 1.3 MB/s, which is higher than Ethernet's peak bandwidth, at a message size of 70 bytes. For more analysis see <ref> [16] </ref>. 5 Specialized Libraries While our compatibility libraries have very good performance, their implementation was limited by the need to remain compatible with the existing standards. To explore the further performance gains that are possible, we implemented a non-compatible version of remote procedure call.
Reference: [17] <author> G. S. Delp, D. J. Farber, R. G. Minnich, J. M. Smith, and M. C. Tam. </author> <title> Memory as a Network Abstraction. </title> <journal> IEEE Network, </journal> <volume> 5(4) </volume> <pages> 34-41, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet <ref> [17] </ref>, Merlin [32] and its successor SESAME [43], the Plus system [9], and GalacticaNet [27]. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system [42] supports user-level message passing, but requires application programs to build packet headers.
Reference: [18] <author> T. Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Thirdly, it allows user programs to implement zero-copy data transfer protocols. Finally, it can separate data transfers from control transfers so that a data transfer can be done without the intervention of the receiving node CPU; the implementation of control transfer such as the active message mechanism <ref> [18] </ref> is an option. Our early experience with the SHRIMP multicomputer indicates that the mechanism is easy to use and that these features are very important for applications to achieve low-latency message passing and to obtain communication bandwidth and latency approaching the values provided by the raw hardware. <p> Unlike signals, however, notifications are queued when blocked. Our current implementation of notifications uses signals, but we expect to reimplement notifications in a way similar to active messages <ref> [18] </ref>, with performance much better than signals in the common case. 3 SHRIMP Prototype We have built and experimented with the SHRIMP multi-computer prototype in order to demonstrate the performance of hardware-supported virtual memory-mapped communication. Figure 1 shows the structure of our system. 3.1 Hardware Components . . <p> Most related work is in using remote memory reference models and implementing various message-passing libraries. Spector originated a remote memory reference model [38]; it has recently been revived by several later research efforts as a model to program message-passing primitives for high performance <ref> [39, 18, 41] </ref>. All of these implementations require explicit CPU action or new instructions to initiate communication and require CPU intervention to receive messages. <p> As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers <ref> [18] </ref>. Overhead is reduced to a few hundred instructions on stock hardware, but an interrupt is still required in most cases. 8 Conclusion This paper reports our early experience with message passing on our prototype SHRIMP multicomputer system.
Reference: [19] <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs <ref> [30, 19] </ref>. Protection is provided through the virtual memory system, which controls access to these FIFOs. However, there are a limited number of FIFOs, so they must be shared within a partition (subset of nodes), restricting the degree of multiprogramming.
Reference: [20] <author> John Heinlein, Kourosh Gharachorloo, Scott A. Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of 6th International Conference on Architectural S upport for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel [35, 34]. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing <ref> [33, 24, 23, 22, 20] </ref>. This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead.
Reference: [21] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectur al Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: They have not tried to implement message-passing libraries using the underlying communication mechanism. Several projects have tried to lower overhead by bringing the network all the way into the processor and mapping the network interface FIFOs to special processor registers <ref> [11, 21, 13] </ref>. While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs [30, 19].
Reference: [22] <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 Interconnect Elan Elite Design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel [35, 34]. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing <ref> [33, 24, 23, 22, 20] </ref>. This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead.
Reference: [23] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> A Message Passing Coprocessor for Distributed Memory Multicomputers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel [35, 34]. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing <ref> [33, 24, 23, 22, 20] </ref>. This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead.
Reference: [24] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: The network connecting the nodes is an Intel routing backplane consisting of a two-dimensional mesh of Intel Mesh Routing Chips (iMRCs) [40], and is the same network used in the Paragon multicomputer <ref> [24] </ref>. The iMRC is essentially a wider, faster version of the Caltech Mesh Routing Chip [14]. The backplane supports deadlock-free, oblivious wormhole routing [15], and preserves the order of messages from each sender to each receiver. <p> Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel [35, 34]. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing <ref> [33, 24, 23, 22, 20] </ref>. This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead.
Reference: [25] <author> Intel Corporation. </author> <title> Express Platforms Technical Product Summary: System Overview, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Routing Backplane Paragon Mesh Specialized Libraries Applications ... PC System PC System PC System Linux Interface Network LinuxLinux Network Interface Network Interface Virtual Memory Mapped Communication Compatibility Libraries The prototype system consists of four interconnected nodes. Each node is a DEC 560ST PC containing an Intel Pentium Xpress motherboard <ref> [25] </ref>. The motherboard has a 60 Mhz Pentium CPU [36] with an external 256 Kbyte second-level cache, and 40 Mbytes of main DRAM memory.
Reference: [26] <institution> Internet Network Working Group. </institution> <month> RPC: </month> <title> Remote Procedure Call Protocol Specification Version 2, </title> <month> June </month> <year> 1988. </year> <title> Internet Request For Comments RFC 1057. </title>
Reference-contexts: Our goal was to build compatibility libraries that support standard communication interfaces, without compromising performance. We built user-level compatibility libraries to support three common communication interfaces: the Intel NX multicom-puter message-passing system [35], Sun RPC <ref> [26] </ref>, and stream sockets [29]. All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested.
Reference: [27] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware Assist for Distributed Shared Memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin [32] and its successor SESAME [43], the Plus system [9], and GalacticaNet <ref> [27] </ref>. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system [42] supports user-level message passing, but requires application programs to build packet headers. They have not tried to implement message-passing libraries using the underlying communication mechanism.
Reference: [28] <author> Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black. </author> <title> Fine-Grained Mobility in the Emerald System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call [8, 37, 4], ordered multicast [7], and object-oriented models <ref> [2, 12, 28] </ref>. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [29] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Our goal was to build compatibility libraries that support standard communication interfaces, without compromising performance. We built user-level compatibility libraries to support three common communication interfaces: the Intel NX multicom-puter message-passing system [35], Sun RPC [26], and stream sockets <ref> [29] </ref>. All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested. <p> It is fully compatible 40 16 32 48 64 Message Size (bytes) 0 20 40 One-Way Latency (microseconds) AU-2copy DU-1copy DU-2copy 0K 1K 2K 3K 4K 5K 6K 7K 8K 9K 10K Message Size (bytes) 0 4 8 12 16 20 24 Bandwidth (MB/s) with the Unix stream sockets facility <ref> [29] </ref>. Connection Establishment During connection establishment, the implementation use a regular internet-domain socket, on the Ethernet, to exchange the data required to establish two VMMC mappings (one in each direction). The internet socket is held open, and is used to detect when the connection has been broken.
Reference: [30] <author> C.E. Leiserson, Z.S. Abuhamdeh, D.C. Douglas, C.R. Feyn-man, M.N. Ganmukhi, J.V. Hill, D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, D.S. Wells, M.C. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: While this is efficient for fine-grain, low-latency communication, it requires the use of a nonstandard CPU, and it does not support the protection of multiple contexts in a multiprogramming environment. The Connection Machine CM-5 implements user-level communication through memory-mapped network interface FIFOs <ref> [30, 19] </ref>. Protection is provided through the virtual memory system, which controls access to these FIFOs. However, there are a limited number of FIFOs, so they must be shared within a partition (subset of nodes), restricting the degree of multiprogramming.
Reference: [31] <author> Richard Lipton and Jonathan Sandberg. </author> <title> PRAM: A Scalable Shared Memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: All of these implementations require explicit CPU action or new instructions to initiate communication and require CPU intervention to receive messages. The idea of automatic-update data delivery in our virtual memory mapped communication is derived from the Pipelined RAM network interface <ref> [31] </ref>, which allows physical memory mapping only for a small amount of special memory. Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin [32] and its successor SESAME [43], the Plus system [9], and GalacticaNet [27].
Reference: [32] <author> Creve Maples. </author> <title> A High-Performance, Memory-Based Interconnection System For Multicomputer Environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin <ref> [32] </ref> and its successor SESAME [43], the Plus system [9], and GalacticaNet [27]. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system [42] supports user-level message passing, but requires application programs to build packet headers.
Reference: [33] <author> R.S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A Mul-tithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel [35, 34]. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing <ref> [33, 24, 23, 22, 20] </ref>. This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead.
Reference: [34] <author> John Palmer. </author> <title> The NCUBE Family of High-Performance Parallel Computer Systems. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 845-851, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Since the application must build packet headers, message passing overhead is still hundreds of CPU instructions. Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel <ref> [35, 34] </ref>. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing [33, 24, 23, 22, 20].
Reference: [35] <author> Paul Pierce. </author> <title> The NX/2 Operating System. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Our goal was to build compatibility libraries that support standard communication interfaces, without compromising performance. We built user-level compatibility libraries to support three common communication interfaces: the Intel NX multicom-puter message-passing system <ref> [35] </ref>, Sun RPC [26], and stream sockets [29]. All three of these libraries are fully compatible with existing systems, so that existing application code can be used without modification. Full compatibility means that the entire interface, including all of the inconvenient corner cases, has been implemented and tested. <p> Since the application must build packet headers, message passing overhead is still hundreds of CPU instructions. Old multicomputers have traditional network interfaces and thus their implementations of the NX message-passing library manage communication buffers in the kernel <ref> [35, 34] </ref>. Current machines like the Intel Paragon and Meiko CS-2 attack software overhead by adding a separate processor on every node just for message passing [33, 24, 23, 22, 20].
Reference: [36] <author> Avtar Saini. </author> <title> An Overview of the Intel Pentium Processor. </title> <booktitle> In Proceedings of the IEEE COMPCON'93 Conference, </booktitle> <pages> pages 60-62, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Each node is a DEC 560ST PC containing an Intel Pentium Xpress motherboard [25]. The motherboard has a 60 Mhz Pentium CPU <ref> [36] </ref> with an external 256 Kbyte second-level cache, and 40 Mbytes of main DRAM memory. The Xpress memory bus has a maximum burst write bandwidth of 73 Mbytes/sec, and includes a memory expansion connector which carries the majority of the bus signals.
Reference: [37] <author> Michael D. Schroeder and Mike Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still tens of microseconds in software overhead. Distributed systems offer a wider range of communication abstractions, including remote procedure call <ref> [8, 37, 4] </ref>, ordered multicast [7], and object-oriented models [2, 12, 28]. As above, the performance of these systems is limited by the hardware architecture. Active Messages is one well-known system that attempts to reduce communication overhead on multicomputers [18].
Reference: [38] <author> Alfred Z. Spector. </author> <title> Performing Remote Operations Efficiently on a Local Computer Network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-273, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Most related work is in using remote memory reference models and implementing various message-passing libraries. Spector originated a remote memory reference model <ref> [38] </ref>; it has recently been revived by several later research efforts as a model to program message-passing primitives for high performance [39, 18, 41]. All of these implementations require explicit CPU action or new instructions to initiate communication and require CPU intervention to receive messages.
Reference: [39] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for multicomputing on ATM Networks. </title> <type> Technical Report 93-04-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Most related work is in using remote memory reference models and implementing various message-passing libraries. Spector originated a remote memory reference model [38]; it has recently been revived by several later research efforts as a model to program message-passing primitives for high performance <ref> [39, 18, 41] </ref>. All of these implementations require explicit CPU action or new instructions to initiate communication and require CPU intervention to receive messages.
Reference: [40] <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The SHRIMP multicomputer is a network of commodity systems. Each node is a Pentium PC running the Linux operating system. The network is a multicomputer routing network <ref> [40] </ref> connected to the PC nodes via custom-designed network interfaces. The SHRIMP network interface closely cooperates with a thin layer of software to form a communication mechanism called virtual memory-mapped communication. This mechanism supports various message-passing packages and applications effectively, and delivers excellent performance. <p> The caches snoop DMA transactions and automatically invalidate corresponding cache lines, thereby keeping consistent with all main memory updates, including those from EISA bus masters. The network connecting the nodes is an Intel routing backplane consisting of a two-dimensional mesh of Intel Mesh Routing Chips (iMRCs) <ref> [40] </ref>, and is the same network used in the Paragon multicomputer [24]. The iMRC is essentially a wider, faster version of the Caltech Mesh Routing Chip [14]. The backplane supports deadlock-free, oblivious wormhole routing [15], and preserves the order of messages from each sender to each receiver.
Reference: [41] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: Most related work is in using remote memory reference models and implementing various message-passing libraries. Spector originated a remote memory reference model [38]; it has recently been revived by several later research efforts as a model to program message-passing primitives for high performance <ref> [39, 18, 41] </ref>. All of these implementations require explicit CPU action or new instructions to initiate communication and require CPU intervention to receive messages.
Reference: [42] <author> John Wilkes. </author> <title> Hamlyn An Interface for Sender-Based Communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin [32] and its successor SESAME [43], the Plus system [9], and GalacticaNet [27]. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system <ref> [42] </ref> supports user-level message passing, but requires application programs to build packet headers. They have not tried to implement message-passing libraries using the underlying communication mechanism.
Reference: [43] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager Sharing for Efficient Massive Parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Several shared memory architecture projects use the page-based, automatic-update approach to support shared memory, including Memnet [17], Merlin [32] and its successor SESAME <ref> [43] </ref>, the Plus system [9], and GalacticaNet [27]. These projects did not study the implementation of message-passing libraries. Wilkes's "sender-based" communication in the Hamlyn system [42] supports user-level message passing, but requires application programs to build packet headers.
References-found: 43

