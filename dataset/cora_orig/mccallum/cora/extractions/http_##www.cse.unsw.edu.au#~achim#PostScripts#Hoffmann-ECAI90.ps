URL: http://www.cse.unsw.edu.au/~achim/PostScripts/Hoffmann-ECAI90.ps
Refering-URL: http://www.cse.unsw.edu.au/~achim/index.html
Root-URL: http://www.cse.unsw.edu.au
Title: General Limitations on Machine Learning  
Author: Achim G. Hoffmann 
Keyword: Machine learning, algorithmic information theory, Kolmogorov complexity, knowledge acquisition.  
Address: Franklinstr. 28/29, D-1000 Berlin 10, Germany  
Affiliation: Technische Universitat Berlin Fachbereich Informatik  
Abstract: Machine learning is widely regarded as a tool for overcoming the bottleneck in knowledge acquisition. Especially in knowledge-intensive domains there is the hope for using machine learning techniques successfully. This paper prove the general inability of simple learning programs to learn complex concepts from few input data. This holds independently of the epistemological problems of inductive inference. These results are obtained by the use of algorithmic information theory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: This also means, that if a short program p is able to learn complex concepts, then C must also contain a large number of further concepts. At next membership queries <ref> [1] </ref> will be considered, where the learner L gives an arbitrary object x 2 X to an oracle. The answer of the oracle will be `yes' or `no' depending on whether or not x 2 c t . Corollary 1 Let C be a concept class.
Reference: [2] <author> J. G. Carbonell. </author> <title> Paradigms of machine learning. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 1-9, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Machine learning is often considered as a tool for increasing the performance of systems for coping with knowledge-intensive problem domains. For a recent overview of various approaches in machine learning see <ref> [2] </ref>. The present paper considers machine learning for classifying objects correctly. There are different approaches how the learner interacts with its environment. One approach is to provide the learning system with positive and/or negative examples. Another approach allows queries about specific objects.
Reference: [3] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <type> Technical Report UCSC-CRL-87-26, </type> <institution> University of California, Computer Research Laboratory, </institution> <address> Santa Cruz, CA, </address> <month> Jan-uary </month> <year> 1988. </year>
Reference-contexts: The small constant const only depends on the type of the universal Turing machine U . For the proof the Vapnik-Chervonenkis dimension well known in learning theory is used [9] along with a lower bound on the number of examples for pac-learning proved in <ref> [3] </ref>. For more details see [4]. Note: For most concept classes C on X, is K max (C) jXj. That means, only for concept classes which contain no complex concepts a reasonable number of examples is sufficient. 5 Discussion and Conclusion The results indicate a rather general point.
Reference: [4] <author> A. G. Hoffmann. </author> <title> General Limitations on Machine Learning. </title> <type> Technical Report 7/90, </type> <institution> Technische Uni-versitat Berlin, Fachbereich Informatik, Fachgebiet Wissensbasierte Systeme, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The small constant const only depends on the type of the universal Turing machine U . For the proof the Vapnik-Chervonenkis dimension well known in learning theory is used [9] along with a lower bound on the number of examples for pac-learning proved in [3]. For more details see <ref> [4] </ref>. Note: For most concept classes C on X, is K max (C) jXj. That means, only for concept classes which contain no complex concepts a reasonable number of examples is sufficient. 5 Discussion and Conclusion The results indicate a rather general point.
Reference: [5] <author> P. Langley. </author> <title> Toward a unified science of machine learning. </title> <journal> Machine Learning, </journal> <volume> 3(4), </volume> <year> 1989. </year>
Reference-contexts: The results do not mean, that machine learning is completely purposeless. But they clearly show that one cannot expect any magic from machine learning. Furthermore, the results raise the following question: What can be achieved at all by searching for general machine-learning techniques, as proposed severally, e.g. by Langley <ref> [5] </ref> ? Certainly, one cannot find methods which are capable of acquiring much knowledge by providing a learning system with a much smaller amount of knowledge. Different learning protocols provide a more or less indirect control of the process of knowledge formation.
Reference: [6] <author> M. Li and P. M. B. Vitanyi. </author> <title> Two decades of applied Kolmogorov complexity. </title> <booktitle> In Proceedings of the 3 rd Annual Conference on Structure in Complexity Theory, </booktitle> <pages> pages 80-101, </pages> <year> 1988. </year>
Reference-contexts: The length of a binary encoded program p is denoted by jpj. The length of the shortest program for constructing s is called its Kolmogorov European Conference on Artificial Intelligence 1990 pp. 345-347 complexity K (s). According to the Invariance The--orem (see e.g. <ref> [6] </ref>) the particular kind of considered universal Turing machine U makes only a difference of a certain constant c. It has been shown that most strings of length n have Kolmogorov complexity K (s) of magnitude n [6]. <p> According to the Invariance The--orem (see e.g. <ref> [6] </ref>) the particular kind of considered universal Turing machine U makes only a difference of a certain constant c. It has been shown that most strings of length n have Kolmogorov complexity K (s) of magnitude n [6]. Examples: Strings as '1111111111111111111111' or '00000000000' or '1010101010101010' etc. are simple strings, since their description by encoding a program which outputs these strings is short. In contrast to that strings like `101100100111011001011101010' are more complex, i.e. require a longer program for getting printed.
Reference: [7] <author> R. J. Solomonoff. </author> <title> Complexity-based induction systems: comparisons and convergence theorems. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22 and 224-254, </pages> <year> 1964. </year>
Reference-contexts: In any way learning involves gathering information from its environment for behaving more purposefully in a certain sense. Thus, the use of algorithmic information theory in machine learning research appears rather naturally. The presented approach for applying algorithmic information theory to learning contrasts the work of Solomonoff <ref> [7] </ref>, who was concerned with Occam's principle of choosing the simplest available explanation. However, the present paper is concerned with the relationship between the amount of a priori knowledge of a learner and the amount of information required by the learner for determining the desired target concept.
Reference: [8] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The organization of the paper is as follows. In section 2 the notation and basic assumptions concerning the learning tasks are given. Section 3 is concerned with the structure of concept classes. Section 4 considers learning within Valiant's framework <ref> [8] </ref>. Section 5 discusses the obtained results. 2 Preliminaries We consider a set of objects X. Each subset of X is called a concept. Thus, there are 2 jXj different concepts. A concept class C is a subset of the set of all concepts. <p> Thus, the corollary follows from theorem 1. 2 4 Learning complex concepts from examples Certainly, successful learning from examples depends on the particular examples provided to the learner. In probably approximately correctly learning (pac-learning) as introduced by Valiant 1984 in <ref> [8] </ref> an arbitrary unknown but fixed probability distribution D on X is assumed. Each x 2 X appears with a fixed proba bility determined by D. During the learning phase the objects in X appear along with an indication whether they belong to the target concept.
Reference: [9] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year> <month> 347 </month>
Reference-contexts: The small constant const only depends on the type of the universal Turing machine U . For the proof the Vapnik-Chervonenkis dimension well known in learning theory is used <ref> [9] </ref> along with a lower bound on the number of examples for pac-learning proved in [3]. For more details see [4]. Note: For most concept classes C on X, is K max (C) jXj.
References-found: 9

