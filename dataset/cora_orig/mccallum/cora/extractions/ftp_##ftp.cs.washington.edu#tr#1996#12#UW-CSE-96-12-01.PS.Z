URL: ftp://ftp.cs.washington.edu/tr/1996/12/UW-CSE-96-12-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: Interleaving sequences to maximize the minimum prefix sum Tracy Kimbrel Technical Report 96-12-01 Department of Computer Science and Engineering University of Washington 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> A study of Integrated Prefetching and Caching Strategies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 188-197. </pages>
Reference-contexts: 1 Introduction As processors get faster relative to memory speeds, file system I/O is increasingly a bottleneck to performance. Recent studies have shown that an application program's I/O overhead can be significantly reduced using integrated prefetching and caching policies <ref> [1, 2] </ref>. These techniques, augmented by mechanisms to allocate cache space among multiple processes, were shown empirically to improve the performance of multi-programmed workloads as well, assuming standard, fair scheduling mechanisms are used to arbitrate the processes' competing prefetch requests and processing demands. <p> The goal is to minimize the completion time of the last process to finish. 2 The general problem described above appears to be a difficult one, even in the case of a single process. In the single-process case, it can be shown <ref> [1] </ref> that any time a block must be evicted, an optimal choice is available: that block which is not needed for the longest time among all blocks present in the cache. (This rule is derived from Belady's optimal o*ine paging algorithm [4].) Cao et. al [1] also showed that it is <p> case, it can be shown <ref> [1] </ref> that any time a block must be evicted, an optimal choice is available: that block which is not needed for the longest time among all blocks present in the cache. (This rule is derived from Belady's optimal o*ine paging algorithm [4].) Cao et. al [1] also showed that it is always best to choose the first missing block when deciding which block to prefetch, and that a prefetch should never be initiated for a block that is not needed until after the next request of the block that would be evicted.
Reference: [2] <author> R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995, </year> <pages> pp. 79-95. </pages>
Reference-contexts: 1 Introduction As processors get faster relative to memory speeds, file system I/O is increasingly a bottleneck to performance. Recent studies have shown that an application program's I/O overhead can be significantly reduced using integrated prefetching and caching policies <ref> [1, 2] </ref>. These techniques, augmented by mechanisms to allocate cache space among multiple processes, were shown empirically to improve the performance of multi-programmed workloads as well, assuming standard, fair scheduling mechanisms are used to arbitrate the processes' competing prefetch requests and processing demands. <p> The algorithm requires O (n log m) arithmetic operations, where n is the total number of transactions in all of the m sequences. 2 Motivation and background Recent research into maximizing file system performance shows that many applications' I/O demands can be disclosed to the file system in advance <ref> [2, 3] </ref>. Suppose multiple processes share a cache backed by a storage device, and that the system has advance knowledge of the processes' sequences of requests for items residing on the backing store.
Reference: [3] <author> Todd C. Mowry, Angela K. Demke, and Orran Krieger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the 1996 Symposium on Operating System Design and Implementation, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 3-17. </pages>
Reference-contexts: The algorithm requires O (n log m) arithmetic operations, where n is the total number of transactions in all of the m sequences. 2 Motivation and background Recent research into maximizing file system performance shows that many applications' I/O demands can be disclosed to the file system in advance <ref> [2, 3] </ref>. Suppose multiple processes share a cache backed by a storage device, and that the system has advance knowledge of the processes' sequences of requests for items residing on the backing store.
Reference: [4] <author> L.A. Belady. </author> <title> A Study of Replacement Algorithms for Virtual Storage Computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: In the single-process case, it can be shown [1] that any time a block must be evicted, an optimal choice is available: that block which is not needed for the longest time among all blocks present in the cache. (This rule is derived from Belady's optimal o*ine paging algorithm <ref> [4] </ref>.) Cao et. al [1] also showed that it is always best to choose the first missing block when deciding which block to prefetch, and that a prefetch should never be initiated for a block that is not needed until after the next request of the block that would be evicted.
Reference: [5] <author> Tracy Kimbrel, Andrew Tomkins, R. Hugo Patterson, Brian Bershad, Pei Cao, Edward W. Felten, Garth A. Gibson, Anna Karlin, and Kai Li. </author> <title> A Trace-driven Comparison of Algorithms for Parallel Prefetching and Caching. </title> <booktitle> In Proceedings of the 1996 Symposium on Operating System Design and Implementation, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 19-34. </pages>
Reference-contexts: For typical systems, F=K :02, so this performance is very close to optimal. No polynomial-time exact solution is known. Kimbrel et. al <ref> [5] </ref> considered algorithms for the single-process case in a practical setting, simulating the algorithms' behaviors using file access sequences from real programs.
Reference: [6] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983, </year> <pages> pp. 135-145. 11 </pages>
Reference-contexts: The second loop can be handled similarly. Producing the output has a total cost of O (n). Thus, the most expensive operations are the O (n) insert and delete maximum (or minimum) operations on the priority queue, each with a cost of O (log m) (see, for example, <ref> [6] </ref>). Thus we have the following.
References-found: 6

