URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.TC.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.TC.html
Root-URL: 
Title: Clustering Learning Tasks and the Selective Cross-Task Transfer of Knowledge  
Author: Sebastian Thrun Joseph O'Sullivan 
Note: The first author is also affiliated with the  where part of this research was carried out.  
Address: Pittsburgh, PA 15213  Bonn, Germany,  
Affiliation: School of Computer Science Carnegie Mellon University  Computer Science Department III of the University of  
Date: November 1995  
Pubnum: CMU-CS-95-209  
Abstract: This research is sponsored in part by the National Science Foundation under award IRI-9313367, and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of NSF, Wright Laboratory or the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abu-Mostafa, Y. S. </author> <title> A Method for Learning from Hints. </title> <booktitle> in: Advances in Neural Information Processing Systems 5, </booktitle> <editor> edited by S. J. Hanson, J. Cowan, and C. L. Giles. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 73-80. </pages>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. <p> In other words, d is obtained by minimizing the following expression: E (d) = x;y ffi xy dist d (x ; y) (5) where ffi xy = 1 if f (x) = f (y) (6) Here each component d (i) is constrained to lie in <ref> [0:01; 1] </ref> for all i. 1 Let d fl = argmin d denote the parameter vector that minimizes E, henceforth called E-optimal or task-optimal. Let dist fl be the corresponding optimal distance metric. <p> The results were not sensitive to these learning parameters. We also did not observe over-fitting, neither for the tasks that the distance metrics were optimized for, nor for any other task (such as the test task 13). The latter observation crucially depended on the existence of the bounds <ref> [0:01; 1] </ref> for the distance parameters d (i) . In the absence of such bounds, the performance on the training task increased only slightly, while it usually decreased significantly for any other task.
Reference: [2] <author> Ahn, W.-K. and Brewer, W. F. </author> <title> Psychological Studies of Explanation-Based Learning. in: Investigating Explanation-Based Learning, edited by G. DeJong. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1993. </year>
Reference-contexts: Motivated by the observation that humans encounter more than one learning task during their lifetime and that they successfully improve their ability to learn <ref> [2, 20] </ref>, several researchers have proposed algorithms that are able to acquire domain-specific knowledge and re-use it in future learning tasks.
Reference: [3] <author> Atkeson, C. A. </author> <title> Using Locally Weighted Regression for Robot Learning. </title> <booktitle> in: Proceedings of the 1991 IEEE International Conference on Robotics and Automation, edited by . Sacramento, </booktitle> <address> CA, </address> <year> 1991, </year> <pages> pp. 958-962. </pages>
Reference-contexts: KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension.
Reference: [4] <author> Baxter, J. </author> <title> The Canonical Metric For Vector Quantization. </title> <institution> Flinders University, Department of Mathematics and Statistics, Australia, </institution> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., <ref> [4, 18, 33] </ref>, * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. <p> KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension. <p> Obviously, different vectors d will make KNN interpolate differently. Thus, the vector d determines how KNN generalizes. Learning d has been shown empirically to be an effective way to transfer knowledge across multiple learning tasks <ref> [4, 33] </ref>. 4 2.2 Determining the Optimal Distance Metric Each task (or each set of tasks, as discussed below) possesses an optimal distance metric.
Reference: [5] <author> Baxter, J. </author> <title> Learning Internal Representations. </title> <booktitle> in: Proceedings of the Con ference on Computation Learning Theory, edited by . 1995, </booktitle> <address> p. </address> . <note> To appear. </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [6] <author> Beymer, D., Shashua, A., and Poggio, T. </author> <title> Example Based Image Analysis and Synthesis. </title> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <month> November </month> <year> 1993. </year> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431. </pages>
Reference-contexts: For example, in the context of face recognition methods have been developed that improve the recognition accuracy significantly when learning to recognize a face, by learning and transferring face-specific invariances acquired through other face recognition tasks <ref> [6, 16] </ref>. Similar results in the context of object recognition, robot navigation and chess are reported in [31]. Technically speaking, the underlying learning problem can be stated as follows. <p> the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., <ref> [6, 16, 31] </ref>, * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2.
Reference: [7] <author> Buhmann, J. </author> <title> Data Clustering and Learning. in: Handbook of Brain Theory and Neural Networks, edited by M. Arbib. </title> <publisher> Bradfort Books/MIT Press, </publisher> <year> 1995, </year> <pages> pp. 278-282. </pages>
Reference-contexts: It remains to be discussed how to maximize J. In general, optimizing a functional J defined over a pairwise cost matrix is a well-understood combinatorial data clustering problem for which various algorithms exist (see for example <ref> [7, 13] </ref> 6 and references therein). In all our experiments, the set of support tasks N was sufficiently small so that the globally optimal solution could be computed explicitly. For larger N incomplete methods must be applied. 2.4 Selective Transfer Each task cluster defines an E-optimal distance metric.
Reference: [8] <author> Buhmann, J., Burgard, W., Cremers, A. B., Fox, D., Hofmann, T., Schneider, F., Strikos, J., and Thrun, S. </author> <title> The Mobile Robot Rhino. </title> <journal> AI Magazine, </journal> <volume> vol. </volume> <month> 16 </month> <year> (1995), </year> <note> p. </note> . 
Reference-contexts: Figure 14 shows how the XAVIER robot navigates using an occupancy map approach [19] that has originally been developed in the RHINO mobile robot project at the University of Bonn <ref> [32, 8] </ref>. Since occupancy maps assume the world is static, they cannot handle well failures that arise from the dynamics in real-world environments. A typical failure situation is shown in Figure 15. Here the door, which was previously open, is suddenly locked.
Reference: [9] <author> Caruana, R. </author> <title> Multitask Learning: A Knowledge-Based of Source of Inductive Bias. </title> <booktitle> in: Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <editor> edited by P. E. Utgoff. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 41-48. </pages>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [10] <author> Franke, R. </author> <title> Scattered Data Interpolation: Tests of Some Methods. </title> <journal> Mathe matics of Computation, </journal> <volume> vol. 38 (1982), </volume> <pages> pp. 181-200. </pages>
Reference-contexts: It then transfers knowledge only from this single clusterother task clusters are not employed. The TC implementation presented here instantiates the general scheme shown in Table 1. At the lowest level of learning, TC uses K-nearest neighbor (KNN) for generalization <ref> [10, 28] </ref>. KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also [3, 4, 11, 12, 18, 33]). <p> Empirical results obtained in a mobile robot domain are described in Section 3. Finally Section 4 discusses some of the strengths and weaknesses of the approach and outlines open questions. 3 2 The TC Algorithm 2.1 Nearest Neighbor K-nearest neighbor (KNN) is a well-known method for fitting functions <ref> [10, 28] </ref> we will therefore review it only briefly here. Suppose one would like to approximate the function f () based on a finite and potentially noisy set of input-output examples of f .
Reference: [11] <author> Friedman, J. H. </author> <title> Flexible Metric Nearest Neighbor Classification. </title> <institution> Department of Statistics and Linear Accelerator Center, Stanford University, Stanford. </institution> <address> CA 94305, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension.
Reference: [12] <author> Hastie, T. and Tibshirani, R. </author> <title> Discriminant Adaptive Nearest Neighbor Clas sification. </title> <institution> Dept. of Statistics and Biostatistics, Stanford University, Stanford, </institution> <address> CA, </address> <month> December </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension.
Reference: [13] <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: It remains to be discussed how to maximize J. In general, optimizing a functional J defined over a pairwise cost matrix is a well-understood combinatorial data clustering problem for which various algorithms exist (see for example <ref> [7, 13] </ref> 6 and references therein). In all our experiments, the set of support tasks N was sufficiently small so that the globally optimal solution could be computed explicitly. For larger N incomplete methods must be applied. 2.4 Selective Transfer Each task cluster defines an E-optimal distance metric.
Reference: [14] <author> Hild, H. and Waibel, A. </author> <title> Multi-Speaker/Speaker-Independent Architectures for the Multi-State Time Delay Neural Network. </title> <booktitle> in: Proceedings of the International Conference on Acoustics, Speech and Signal Processing, IEEE, edited by . 1993, </booktitle> <pages> pp. II 255-258. </pages>
Reference-contexts: there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., <ref> [14, 33] </ref>, * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2.
Reference: [15] <author> Jordan, M. I. and Rumelhart, D. E. </author> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> vol. 16 (1992), </volume> <pages> pp. 307-354. </pages>
Reference-contexts: If a new task arrives, determine the most similar task cluster. Selectively transfer knowledge from that task cluster only. Table 1: TC: The general scheme for task clustering. * learning domain models, e.g., <ref> [15, 21, 31] </ref>. Many of these approaches have been demonstrated to reduce the sample complexity, given that the number of available support tasks N is sufficiently large and, equally importantly, that they are appropriately related. What it means for tasks to be related is currently not very well understood.
Reference: [16] <author> Lando, M. and Edelman, S. </author> <title> Generalizing from a single view in face recogni tion. no. </title> <type> CS-TR 95-02, </type> <institution> Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: For example, in the context of face recognition methods have been developed that improve the recognition accuracy significantly when learning to recognize a face, by learning and transferring face-specific invariances acquired through other face recognition tasks <ref> [6, 16] </ref>. Similar results in the context of object recognition, robot navigation and chess are reported in [31]. Technically speaking, the underlying learning problem can be stated as follows. <p> the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., <ref> [6, 16, 31] </ref>, * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2.
Reference: [17] <author> Moore, A. W. </author> <title> Efficient Memory-based Learning for Robot Control. </title> <publisher> Trinity Hall, </publisher> <address> University of Cambridge, England, </address> <year> 1990. </year>
Reference-contexts: Nearest neighbor memorizes training examples, avoiding the need for long training times. For efficient retrieval, tree-based algorithms exist that facilitate fast access of the memorized data (see e.g., <ref> [17] </ref>). The particular testing task, the distinction of open and closed doors, is useful in the context of mobile robot navigation.
Reference: [18] <author> Moore, A. W., Hill, D. J., and Johnson, M. P. </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators. </title> <booktitle> in: Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3, </volume> <editor> edited by S. Hanson, S. Judd, and T. Petsche. </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., <ref> [4, 18, 33] </ref>, * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. <p> KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension.
Reference: [19] <author> Moravec, H. P. </author> <title> Sensor Fusion in Certainty Grids for Mobile Robots. </title> <journal> AI Magazine, </journal> <volume> vol. </volume> <year> (1988), </year> <pages> pp. 61-74. 30 </pages>
Reference-contexts: The particular testing task, the distinction of open and closed doors, is useful in the context of mobile robot navigation. Figure 14 shows how the XAVIER robot navigates using an occupancy map approach <ref> [19] </ref> that has originally been developed in the RHINO mobile robot project at the University of Bonn [32, 8]. Since occupancy maps assume the world is static, they cannot handle well failures that arise from the dynamics in real-world environments. A typical failure situation is shown in Figure 15.
Reference: [20] <author> Moses, Y., Ullman, S., and Edelman, S. </author> <title> Generalization across changes in illumination and viewing position in upright and inverted faces. no. </title> <type> CS-TR 93-14, </type> <institution> Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel, </institution> <year> 1993. </year>
Reference-contexts: Motivated by the observation that humans encounter more than one learning task during their lifetime and that they successfully improve their ability to learn <ref> [2, 20] </ref>, several researchers have proposed algorithms that are able to acquire domain-specific knowledge and re-use it in future learning tasks.
Reference: [21] <author> O'Sullivan, J., Mitchell, T. M., and Thrun, S. </author> <title> Explanation-Based Neural Network Learning from Mobile Robot Perception. in: Symbolic Visual Learning, edited by K. </title> <editor> Ikeuchi and M. Veloso. </editor> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: If a new task arrives, determine the most similar task cluster. Selectively transfer knowledge from that task cluster only. Table 1: TC: The general scheme for task clustering. * learning domain models, e.g., <ref> [15, 21, 31] </ref>. Many of these approaches have been demonstrated to reduce the sample complexity, given that the number of available support tasks N is sufficiently large and, equally importantly, that they are appropriately related. What it means for tasks to be related is currently not very well understood.
Reference: [22] <author> O'Sullivan, J. and Thrun, S. </author> <title> A Robot That Improves Its Ability To Learn. </title> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. <p> To recognize the upcoming plan failure much earlier, the navigation routines were augmented by the learned door status recognizer (along with the result of task 12 above: door/no door) <ref> [22] </ref>. Training examples for the door recognition task are easily constructed, since plan failures (the robot turns around) are easily detected post the fact. In [22], it has been shown that an artificial neural network is able to reliably (100% in more than 30 trials) detect closed doors, enabling the robot <p> plan failure much earlier, the navigation routines were augmented by the learned door status recognizer (along with the result of task 12 above: door/no door) <ref> [22] </ref>. Training examples for the door recognition task are easily constructed, since plan failures (the robot turns around) are easily detected post the fact. In [22], it has been shown that an artificial neural network is able to reliably (100% in more than 30 trials) detect closed doors, enabling the robot to change its path accordingly. An example is shown in Figure 16. To reduce the ratio 25 robot leaving a room.
Reference: [23] <author> Pratt, L. Y. </author> <title> Transferring Previously Learned Back-Propagation Neural Net works to New Learning Tasks. </title> <institution> Rutgers University, Department of Computer Science, </institution> <address> New Brunswick, NJ 08904, </address> <month> May </month> <year> 1993. </year> <note> also appeared as Technical Report ML-TR-37. </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [24] <author> Rendell, L., Seshu, R., and Tcheng, D. </author> <title> Layered Concept-Learning and Dynamically-Variable Bias Management. </title> <booktitle> in: Proceedings of IJCAI-87. </booktitle> <year> 1987, </year> <pages> pp. 308-314. </pages>
Reference-contexts: internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., <ref> [24, 30, 35] </ref>, and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2. Arrange all tasks into a small number of clusters by maximizing the perfor mance gain within each task cluster. 3.
Reference: [25] <author> Sejnowski, T. J. and Rosenberg, C. R. NETtalk: </author> <title> A Parallel Network that Learns to Read Aloud. no. </title> <institution> JHU/EECS-86/01, Johns Hopkins University, </institution> <year> 1986. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [26] <author> Sharkey, N. E. and Sharkey, A. J. C. </author> <title> Adaptive Generalization and the Transfer of Knowledge. </title> <booktitle> in: Proceedings of the Second Irish Neural Networks Conference, edited by . Belfast, 1992, </booktitle> <address> p. </address> . 
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [27] <author> Silver, D. and Mercer, R. </author> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge. </title> <booktitle> in: Proceedings of the INNS World Congress on Neural Networks, edited by . Washington, </booktitle> <address> DC, </address> <year> 1995, </year> <pages> pp. 164-169, </pages> <booktitle> Volume III. </booktitle>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [28] <author> Stanfill, C. and Waltz, D. </author> <title> Towards Memory-Based Reasoning. </title> <journal> Communica tions of the ACM, </journal> <volume> vol. 29 (1986), </volume> <pages> pp. 1213-1228. </pages>
Reference-contexts: It then transfers knowledge only from this single clusterother task clusters are not employed. The TC implementation presented here instantiates the general scheme shown in Table 1. At the lowest level of learning, TC uses K-nearest neighbor (KNN) for generalization <ref> [10, 28] </ref>. KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also [3, 4, 11, 12, 18, 33]). <p> Empirical results obtained in a mobile robot domain are described in Section 3. Finally Section 4 discusses some of the strengths and weaknesses of the approach and outlines open questions. 3 2 The TC Algorithm 2.1 Nearest Neighbor K-nearest neighbor (KNN) is a well-known method for fitting functions <ref> [10, 28] </ref> we will therefore review it only briefly here. Suppose one would like to approximate the function f () based on a finite and potentially noisy set of input-output examples of f .
Reference: [29] <author> Suddarth, S. C. and Holden, A. </author> <title> Symbolic neural systems and the use of hints for developing complex systems. </title> <journal> International Journal of Machine Studies, </journal> <volume> vol. </volume> <month> 35 </month> <year> (1991), </year> <note> p. . 31 </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., <ref> [1, 5, 9, 22, 23, 25, 26, 29, 27] </ref>, * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1.
Reference: [30] <author> Sutton, R. S. </author> <title> Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta. </title> <booktitle> in: Proceeding of Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <publisher> AAAI, edited by . AAAI Press/The MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1992, </year> <pages> pp. 171-176. </pages>
Reference-contexts: internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., <ref> [24, 30, 35] </ref>, and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2. Arrange all tasks into a small number of clusters by maximizing the perfor mance gain within each task cluster. 3.
Reference: [31] <author> Thrun, S. </author> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: Similar results in the context of object recognition, robot navigation and chess are reported in <ref> [31] </ref>. Technically speaking, the underlying learning problem can be stated as follows. Given 1. training data for the current learning task, 2. training data for N other, previous learning tasks, and 3. a performance measure find a hypothesis which maximizes the performance in the current (the N +1-th) learning task. <p> the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., <ref> [6, 16, 31] </ref>, * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2. <p> If a new task arrives, determine the most similar task cluster. Selectively transfer knowledge from that task cluster only. Table 1: TC: The general scheme for task clustering. * learning domain models, e.g., <ref> [15, 21, 31] </ref>. Many of these approaches have been demonstrated to reduce the sample complexity, given that the number of available support tasks N is sufficiently large and, equally importantly, that they are appropriately related. What it means for tasks to be related is currently not very well understood.
Reference: [32] <author> Thrun, S. </author> <title> Exploration and Model Building in Mobile Robot Domains. </title> <booktitle> in: Proceedings of the ICNN-93, IEEE Neural Network Council, </booktitle> <address> edited by . San Francisco, CA, </address> <year> 1993, </year> <pages> pp. 175-180. </pages>
Reference-contexts: Figure 14 shows how the XAVIER robot navigates using an occupancy map approach [19] that has originally been developed in the RHINO mobile robot project at the University of Bonn <ref> [32, 8] </ref>. Since occupancy maps assume the world is static, they cannot handle well failures that arise from the dynamics in real-world environments. A typical failure situation is shown in Figure 15. Here the door, which was previously open, is suddenly locked.
Reference: [33] <author> Thrun, S. </author> <title> Is Learning the n-th Thing Any Easier Than Learning the First? in: </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <editor> edited by D. Touretzky and M. Mozer. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <note> p. . to appear. </note>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., <ref> [4, 18, 33] </ref>, * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. <p> there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks: * learning internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., <ref> [14, 33] </ref>, * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., [24, 30, 35], and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2. <p> KNN memorizes all training data explicitly and interpolates them at query time. TC transfers knowledge across tasks by adjusting the distance metric that is used for determining the proximity of data points in KNN (see also <ref> [3, 4, 11, 12, 18, 33] </ref>). To determine task relatedness, TC inspects the effect on the generalization accuracy for each task when using the distance metric that is optimally adjusted for another task (Step 1). <p> Notice that the Euclidean metric weighs each data dimension equally. Following the ideas in <ref> [3, 4, 11, 12, 18, 33] </ref>, the TC algorithm uses a globally weighted version of dist Euclid dist d (x ; y) = s i 2 Here d (with d (i) 0 for all i) is a vector of parameters that determine the relative weight of each input dimension. <p> Obviously, different vectors d will make KNN interpolate differently. Thus, the vector d determines how KNN generalizes. Learning d has been shown empirically to be an effective way to transfer knowledge across multiple learning tasks <ref> [4, 33] </ref>. 4 2.2 Determining the Optimal Distance Metric Each task (or each set of tasks, as discussed below) possesses an optimal distance metric.
Reference: [34] <author> Thrun, S. </author> <title> Lifelong Learning: A Case Study. no. </title> <institution> CMU-CS-95-208, Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Applying the TC algorithm to more sophisticated image encodings, and applying algorithms that can discover more sophisticated commonalities between multiple tasks such as those surveyed in <ref> [34] </ref>, is subject to ongoing research. The reader may notice that the general scheme outlined in the very beginning of this paper (Table 1) may be applicable to other approaches that transfer knowledge across multiple learning tasks.
Reference: [35] <author> Utgoff, P. E. </author> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year> <month> 32 </month>
Reference-contexts: internal representations for artificial neural networks, e.g., [1, 5, 9, 22, 23, 25, 26, 29, 27], * learning distance metrics, e.g., [4, 18, 33], * learning to re-represent the data, e.g., [14, 33], * learning invariances in classification, e.g., [6, 16, 31], * learning algorithmic parameters and choosing algorithms, e.g., <ref> [24, 30, 35] </ref>, and 1. For each pair of support tasks n and m: Compute the performance gain for task n, if knowledge is transferred from task m. 2. Arrange all tasks into a small number of clusters by maximizing the perfor mance gain within each task cluster. 3.
References-found: 35

