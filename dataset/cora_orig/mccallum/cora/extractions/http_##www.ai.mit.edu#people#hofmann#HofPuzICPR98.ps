URL: http://www.ai.mit.edu/people/hofmann/HofPuzICPR98.ps
Refering-URL: http://www.ai.mit.edu/people/hofmann/
Root-URL: 
Email: hofmann@ai.mit.edu  jan@cs.uni-bonn.de  
Phone: 2  
Title: Mixture Models for Co-occurrence and Histogram Data  
Author: Thomas Hofmann and Jan Puzicha 
Affiliation: 1 Artificial Intelligence Laboratory, M.I.T.,  Institut fur Informatik III, University of  
Address: (ICPR'98), Brisbane, Australia  Cambridge, MA 02139,  Bonn, D-53177 Bonn, Germany,  
Note: Published in: Proceedings of the 14th International Conference on Pattern Recognition  
Abstract: Modeling and predicting co-occurrences of events is a fundamental problem of unsupervised learning. In this contribution, we develop a general statistical framework for analyzing co-occurrence data based on probabilistic clustering by mixture models. More specifically, we discuss three models which pursue different modeling goals and which differ in the way they define the probabilistic partitioning of the observations. Adopting the maximum likelihood principle, annealed EM algorithms are derived for parameter estimation. From the class of potential applications in pattern recognition and data analysis, we have chosen document retrieval, language modeling, and unsupervised texture segmentation to test and evaluate the proposed algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Brown, P. deSouza, R. Mercer, V. Della Pietra, and J. Lai. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computa tional Linguistics, </journal> <volume> 18(4):467479, </volume> <year> 1992. </year>
Reference-contexts: As a major advantage clustering based on COD does not require an external similarity measure, but exclusively relies on the occurrence statistics. Several clustering and mixture models for COD have recently been investigated <ref> [1, 6, 8] </ref>. Our approach provides a unifying framework for all of these models. 2. The Separable Mixture Model The first model we propose is the Separable Mixture Model (SMM). <p> Additional model variants as well as more details can be found in [4]. 5. Annealed EM Annealed EM is a method to deal with two problems of the standard EM approach: the sensitivity to local maxima 3 A similar (hard-clustering) objective function has been proposed by Brown et al. <ref> [1] </ref> in their (nonprobabilistic) classbased ngram model. T. Hofmann, J.
Reference: [2] <author> A. Dempster, N. Laird, and D. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39:138, </volume> <year> 1977. </year>
Reference-contexts: This yields the joint probability p ij P ff ff p ijff q jjff . The component distributions are separable, i.e., x i and y j are conditionally independent given the class C ff . We apply a standard maximum likelihood technique known as the ExpectationMaximization (EM) Algorithm <ref> [2] </ref> and introduce indicator variables R rff 2 f0; 1g to represent the unknown class C ff of the rth observation.
Reference: [3] <author> T. Hofmann and J. Buhmann. </author> <title> Pairwise data clustering by de terministic annealing. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(1), </volume> <year> 1997. </year>
Reference-contexts: Moreover, they offer a natural framework for unifying statistical inference and clustering. This is particularly important, since one is often interested in discovering structure, typically represented by groups of similar objects as in pairwise data clustering <ref> [3] </ref>. As a major advantage clustering based on COD does not require an external similarity measure, but exclusively relies on the occurrence statistics. Several clustering and mixture models for COD have recently been investigated [1, 6, 8]. Our approach provides a unifying framework for all of these models. 2. <p> Annealed EM is based on the idea of deterministic annealing which has been applied to many clustering problems, including vectorial clustering [7], pairwise clustering <ref> [3] </ref>, and for distributional clustering [6]. The key idea is to introduce an (inverse temperature) parameter fi, and to replace the negative (averaged) log likelihood by a substitute known as the free energy.
Reference: [4] <author> T. Hofmann and J. Puzicha. </author> <title> Statistical models for cooccur rence data. </title> <type> Technical report, </type> <institution> Artifical Intelligence Laboratory Memo 1625, M.I.T., </institution> <year> 1998. </year>
Reference-contexts: Other potential application domains are data mining, molecular biology, and preference analysis. fl This research has been supported by a M.I.T. Faculty Sponser's Discretionary Fund and by the German Research Foundation (DFG) under grant # BU 914/31. A detailed version of this paper can be found in <ref> [4] </ref>. It is a pleasure to thank Hans du Buf for providing aerial image data. The intrinsic problem of COD is data sparseness. For large object sets a majority of pairs (x i ; y j ) only has a small probability of being observed even for large sample sets. <p> Iterating the E and Mstep, the parameters converge to a local maximum of the likelihood. 1 1 In the special case of X = Y the SMM is equivalent to the word clustering model of Saul and Pereira [8] (proposed independently in <ref> [4] </ref>). T. Hofmann, J. Puzicha, Mixture Models for Co-occurrence and Histogram Data, ICPR'98, Brisbane, Australia 2 clusters by their most probable words. 3. The Asymmetric Clustering Model The grouping structure inferred by the SMM corresponds to a probabilistic partitioning of the observation space X fi Y. <p> In the SCM, the 2 The ACM is similar to the distributional clustering model proposed in [6], cf. the discussion in <ref> [4] </ref>. coupling of I and J makes the exact computation of posteriors in the Estep intractable and we have to recourse to a factorial approximation, hI i- J j i hI i- ihJ j i (mean field approximation), which results in the following approx imate Estep (analogously for hJ i- i) <p> Taking into account the normaliza tion constraint on p ij (cf. <ref> [4] </ref>), c - = - =( x - y ), where - = i;j hI i- ihJ j in ij =L and x P = - is obtained. The resulting approximate EM alternates the update of posterior marginals with an update of the continuous parameters. <p> The ACM intro duces an asymmetry by restricting only one of the distri butions p ijff / p i hI iff i. In the SCM both distributions are restricted. Additional model variants as well as more details can be found in <ref> [4] </ref>. 5. Annealed EM Annealed EM is a method to deal with two problems of the standard EM approach: the sensitivity to local maxima 3 A similar (hard-clustering) objective function has been proposed by Brown et al. [1] in their (nonprobabilistic) classbased ngram model. T. Hofmann, J.
Reference: [5] <author> T. Hofmann, J. Puzicha, and J. Buhmann. </author> <title> Deterministic an nealing for unsupervised texture segmentation. </title> <journal> IEEE Trans actions on Pattern Analysis and Machine Intelligence, </journal> <note> 1998 (to appear). </note>
Reference-contexts: experiments we have utilized data from three different domains: (i) Word occurrence data from the Cranfield document collection (CRAN), (ii) adjectivenoun co-occurrences from tagged versions of the Penn Treebank (PENN) and the LOB corpus (LOB), (iii) localized histogram data based on a Gabor multiscale image representation of aerial images (cf. <ref> [5] </ref>). In a series of experiments we have investigated how well different models perform in predicting occurrences, evaluating the perplexity P. 4 The results are summarized in Table 1.
Reference: [6] <author> F. Pereira, N. Tishby, and L. Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In 30th Annual Meeting of the ACL, </booktitle> <pages> pages 183190, </pages> <year> 1993. </year>
Reference-contexts: As a major advantage clustering based on COD does not require an external similarity measure, but exclusively relies on the occurrence statistics. Several clustering and mixture models for COD have recently been investigated <ref> [1, 6, 8] </ref>. Our approach provides a unifying framework for all of these models. 2. The Separable Mixture Model The first model we propose is the Separable Mixture Model (SMM). <p> In the SCM, the 2 The ACM is similar to the distributional clustering model proposed in <ref> [6] </ref>, cf. the discussion in [4]. coupling of I and J makes the exact computation of posteriors in the Estep intractable and we have to recourse to a factorial approximation, hI i- J j i hI i- ihJ j i (mean field approximation), which results in the following approx imate Estep <p> Annealed EM is based on the idea of deterministic annealing which has been applied to many clustering problems, including vectorial clustering [7], pairwise clustering [3], and for distributional clustering <ref> [6] </ref>. The key idea is to introduce an (inverse temperature) parameter fi, and to replace the negative (averaged) log likelihood by a substitute known as the free energy.
Reference: [7] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8):945948, </volume> <year> 1990. </year>
Reference-contexts: Annealed EM is based on the idea of deterministic annealing which has been applied to many clustering problems, including vectorial clustering <ref> [7] </ref>, pairwise clustering [3], and for distributional clustering [6]. The key idea is to introduce an (inverse temperature) parameter fi, and to replace the negative (averaged) log likelihood by a substitute known as the free energy.
Reference: [8] <author> L. Saul and F. Pereira. </author> <title> Aggregate and mixedorder Markov models for statistical language processing. </title> <booktitle> In Proceedings of the 2nd International Conference on Empirical Methods in Natural Language Processing, </booktitle> <year> 1997. </year> <title> 4 P is related to the average test set loglikelihood l by P = e l </title> . 
Reference-contexts: As a major advantage clustering based on COD does not require an external similarity measure, but exclusively relies on the occurrence statistics. Several clustering and mixture models for COD have recently been investigated <ref> [1, 6, 8] </ref>. Our approach provides a unifying framework for all of these models. 2. The Separable Mixture Model The first model we propose is the Separable Mixture Model (SMM). <p> Iterating the E and Mstep, the parameters converge to a local maximum of the likelihood. 1 1 In the special case of X = Y the SMM is equivalent to the word clustering model of Saul and Pereira <ref> [8] </ref> (proposed independently in [4]). T. Hofmann, J. Puzicha, Mixture Models for Co-occurrence and Histogram Data, ICPR'98, Brisbane, Australia 2 clusters by their most probable words. 3. The Asymmetric Clustering Model The grouping structure inferred by the SMM corresponds to a probabilistic partitioning of the observation space X fi Y.
References-found: 8

