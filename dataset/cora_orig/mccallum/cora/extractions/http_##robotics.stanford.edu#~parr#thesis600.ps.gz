URL: http://robotics.stanford.edu/~parr/thesis600.ps.gz
Refering-URL: http://robotics.stanford.edu/~parr/
Root-URL: http://www.robotics.stanford.edu
Title: Hierarchical Control and Learning for Markov Decision Processes  
Author: by Ronald Edward Parr 
Degree: 1990 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Stuart Russell, Chair Professor Jitendra Malik Professor Thomas Marschak  
Date: 1998  
Affiliation: A.B. (Princeton University)  
Abstract-found: 0
Intro-found: 1
Reference: <author> Andre, D., Friedman, N., & Parr, R. </author> <year> (1997). </year> <title> Generalized prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Baird, L. </author> <year> (1995). </year> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 30-37 Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical report TR-91-57, </type> <institution> University of Massachusetts Computer Science Department, Amherst, Massachusetts. </institution>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference: <author> Bertsekas, D. C., & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Bertsekas, D. C., & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts. </address>
Reference: <author> Blackwell, D. </author> <year> (1962). </year> <title> Discrete dynamic programming. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33, </volume> <pages> 719-726. </pages>
Reference: <author> Boutilier, C., Dean, T., & Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pp. </pages> <address> 1104-1111 Montreal, Canada. </address> <publisher> Morgan Kaufmann. 152 Boutilier, </publisher> <editor> C., & Dearden, R. </editor> <year> (1994). </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94) Seattle, </booktitle> <address> Washington. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Boyan, J. A., & Moore, A. W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Bradtke, S. J., & Duff, M. O. </author> <year> (1995). </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2, </volume> <pages> 14-23. </pages>
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pp. </pages> <address> 1023-1028 Seattle, Washington. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Crites, R. H., & Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 8: Proceedings of the 1995 Conference 1996. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Dantzig, G., & Wolfe, P. </author> <year> (1960). </year> <title> Decomposition principle for dynamic programs. </title> <journal> Operations Research, </journal> <volume> 8(1), </volume> <pages> 101-111. </pages>
Reference: <author> Dayan, P., & Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <editor> In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), </editor> <booktitle> Neural Information Processing Systems 5, </booktitle> <pages> pp. </pages> <address> 361-368 San Mateo, California. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: This approach was primarily focused on tasks of achievement that had specific decomposition properties, but it may be possible to combine these ideas with the HAMs. Feudal Reinforcement Learning <ref> (Dayan & Hinton, 1993) </ref> and MAXQ value function decomposition (Dietterich, 1997) use a task hierarchy to generate abstract actions for reinforcement learning automatically.
Reference: <author> Dean, T., & Givan, R. </author> <year> (1997). </year> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Aritificial Intelligence, </booktitle> <pages> pp. </pages> <address> 106-111 Providence, Rhode Island. </address> <publisher> MIT Press. </publisher>
Reference: <author> Dean, T., Givan, R., & Leach, S. </author> <year> (1997). </year> <title> Model reduction techniques for computing approximately optimal solutions for Markov decision processes. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97) Providence, </booktitle> <address> Rhode Island. </address> <publisher> Morgan Kaufmann. 153 Dean, </publisher> <editor> T., Kaelbling, L., Kirman, J., & Nicholson, A. </editor> <year> (1995). </year> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, </journal> <volume> 76 (1-2), </volume> <pages> 35-74. </pages>
Reference: <author> Dean, T., & Lin, S.-H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pp. </pages> <address> 1121-1127 Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> (1997). </year> <title> Hierarchical reinforcement learning with the MAXQ value function decomposition. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. </institution>
Reference-contexts: This approach was primarily focused on tasks of achievement that had specific decomposition properties, but it may be possible to combine these ideas with the HAMs. Feudal Reinforcement Learning (Dayan & Hinton, 1993) and MAXQ value function decomposition <ref> (Dietterich, 1997) </ref> use a task hierarchy to generate abstract actions for reinforcement learning automatically. These methods use a pre-specified task hierarchy to make guesses about the value of reaching certain states in a manner similar to the method used by Lin (1993) to generate robot subtasks.
Reference: <author> Forestier, J.-P., & Varaiya, P. </author> <year> (1978). </year> <title> Multilayer control of large Markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-23, </volume> <pages> 298-304. </pages>
Reference: <author> Gordon, G. J. </author> <year> (1995). </year> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 261-268 Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This stored, generalized experience could reduce the amount of trial and error necessary to learn in new environments. The danger of function approximation for HAMs is that unless a very conservative approximation method is used, such as Gordon's Averagers <ref> (Gordon, 1995) </ref>, all of the formal guarantees of the HAM method will be lost. Since function approximation is often used in practical problems in spite of the loss of formal guarantees, the combination of function approximation and HAMs may still be worthwhile.
Reference: <author> Hansen, E. </author> <year> (1997). </year> <title> An improved policy iteration algorithm for partially observable MDPs. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Wiering and Schmidhuber (1996) propose a method for learning complex state machines for POMDPs through reinforcement learning. This method uses some methods similar to those used in HAMs, but its formal properties are unclear. Recent advances in the manipulation of POMDP policies as state machines <ref> (Hansen, 1997) </ref> may give further insight into this issue.
Reference: <author> Harada, D. </author> <year> (1997). </year> <title> Reinforcement learning with time. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Aritificial Intelligence, </booktitle> <pages> pp. </pages> <address> 577-582 Providence, Rhode Island. </address> <publisher> MIT Press. </publisher>
Reference: <author> Hauskrecht, M. </author> <year> (1998). </year> <title> Planning with temporally abstract actions. </title> <type> Tech. rep. </type> <institution> CS-98-01, Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. </author> <year> (1998). </year> <title> Hierarchical solution of Markov decision processes using macro-actions. </title> <booktitle> In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98). To appear. </booktitle>
Reference: <author> Jaakkola, T., Jordan, M., & Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 (6), </volume> <pages> 1185-1201. </pages>
Reference: <author> Jaakola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. 154 Kaelbling, </publisher> <editor> L. P., Littman, M. L., & Moore, A. W. </editor> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference-contexts: Some methods do exist for tuning stochastic action choices in policies that map from states to actions <ref> (Jaakola, Singh, & Jordan, 1995) </ref>. The question of tuning a global parameter for a machine is important because it may help reduce the complexity of refining a machine for a new task. Suppose, for example, that a machine is intended for use in a symmetric room.
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylva-nia. </institution>
Reference: <author> Lin, S.-H. </author> <year> (1997). </year> <title> Exploiting Structure for Planning and Control. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Lipton, R. J., Rose, D. J., & Tarjan, R. E. </author> <year> (1979). </year> <title> Generalized nested dissection. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 16, </volume> <pages> 346-358. </pages>
Reference: <author> Littman, M., Dean, T. L., & Kaelbling, L. P. </author> <year> (1995). </year> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95) Montreal, </booktitle> <address> Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Littman, M. L. </author> <year> (1996). </year> <title> Algorithms for Sequential Decision Making. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. </author> <year> (1996). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 362-370 Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lovejoy, W. S. </author> <year> (1991). </year> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 (1-4), </volume> <pages> 47-66. </pages>
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <booktitle> Machine Learning, </booktitle> <pages> pp. 159-196. </pages>
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 (2-3), </volume> <pages> 311-365. </pages>
Reference: <author> Mahadevan, S., Marchalleck, N., Das, T., & Abhijit, G. </author> <year> (1997). </year> <title> Self improving factory simulation using continuous-time reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <pages> pp. </pages> <address> 202-210 Nashville, Tennessee. </address> <publisher> Morgan Kaufmann. 155 Marbach, </publisher> <editor> P., & Tsitsiklis, J. </editor> <year> (1998). </year> <title> Simulation-based optimization of Markov reward processes. </title> <type> Tech. rep., </type> <institution> Laboratory of Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, Massachusetts. </institution>
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping|reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages>
Reference: <author> Nilsson, N. J. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 139-158. </pages>
Reference: <author> Papadimitriou, C. H., & Tsitsiklis, J. N. </author> <year> (1987). </year> <title> The complexity of Markov decision pro-cessses. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 (3), </volume> <pages> 441-450. </pages>
Reference-contexts: Belief state representations are also problematic since planning with such representations is extremely difficult. POMDPs are known to be PSPACE-hard <ref> (Papadimitriou & Tsitsiklis, 1987) </ref>, so there is little hope of finding a silver-bullet algorithm that will make things any easier. One hope for handling POMDPs is the identification of special case algorithms that work with interesting subclasses of problems.
Reference: <author> Parr, R. </author> <year> (1996). </year> <title> Policy based clustering for Markov decision processes. </title> <booktitle> In Proceedings of the AAAI 96 Fall Symposium on Learning Complex Behaviors. </booktitle>
Reference: <author> Precup, D., & Sutton, R. S. </author> <year> (1997). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Ramadge, P. J., & Wonham, W. M. </author> <year> (1989). </year> <title> On the supervisory control of discrete event systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 (1), </volume> <pages> 81-98. </pages>
Reference: <author> Russell, S. J., & Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 323-340. </pages>
Reference: <author> Singh, S. P., & Gullapalli, V. </author> <year> (1993). </year> <title> Asynchronous modified policy iteration with single-side updates. </title> <type> Unpublished manuscript. </type>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference Austin, </booktitle> <address> Texas. </address> <publisher> Morgan Kaufmann. 156 Sutton, </publisher> <editor> R. S. </editor> <year> (1995). </year> <title> Temporal abstraction in reinforcement learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 531-539 Tahoe City, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S., Precup, D., & Singh, S. P. </author> <year> (1998). </year> <title> Between MDPs and semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales. </title> <booktitle> In prep. </booktitle>
Reference: <author> Szepesvari, C., & Littman, M. L. </author> <year> (1996). </year> <title> Generalized Markov decision processes: Dynamic programming and reinforcement-learning algorithms. </title> <type> Tech. rep., </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island. </address>
Reference: <author> Tate, A. </author> <year> (1977). </year> <title> Generating project networks. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence (IJCAI-77), </booktitle> <pages> pp. </pages> <address> 888-893 Cambridge, Mas-sachusetts. IJCAII. </address>
Reference: <author> Tesauro, G. </author> <year> (1989). </year> <journal> Neurogammon wins computer olympiad. Neural Computation, </journal> <volume> 1 (3), </volume> <pages> 321-323. </pages>
Reference: <author> Thrun, S., & Schwartz, A. </author> <year> (1995). </year> <title> Finding structure in reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference: <author> Watkins, C. J. </author> <year> (1989). </year> <title> Models of Delayed Reinforcement Learning. </title> <type> Ph.D. thesis, </type> <institution> Psychology Department, Cambridge University, </institution> <address> Cambridge, United Kingdom. </address>
Reference: <author> White, D. J. </author> <year> (1993). </year> <title> Markov Decision Processes. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Wiering, M., & Schmidhuber, J. </author> <year> (1996). </year> <title> HQ-learning: Discovering Markovian subgoals for non-Markovian reinforcement learning. </title> <type> Tech. rep., </type> <institution> Istituo Dalle Molle di Studi sull'Intelligenza Artificiale, Lugano, Switzerland. </institution>
Reference: <author> Williams, R. J., & Baird, L. C. I. </author> <year> (1993). </year> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Tech. rep. </type> <institution> NU-CCS-93-14, College of Computer Science, Northeastern University, Boston, Massachusetts. </institution>
Reference: <author> Zhang, N., & Liu, W. </author> <year> (1997). </year> <title> Region-based approximations for planning in stochastic domains. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97) Providence, </booktitle> <address> Rhode Island. </address> <note> Morgan Kaufmann. 157 Zhang, </note> <author> W., & Dietterich, T. </author> <year> (1995). </year> <title> High-performance job-shop scheduling with a time-delay TD() network. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference Denver, </booktitle> <address> Colorado. </address> <publisher> MIT Press. </publisher>
Reference-contexts: If the environment can generate confusing observations, then the HAM might wind up generating choice points for all states in the environment, eliminating any hopes of simplifying the problem. This problem may be avoidable using approximation methods that disregard low-probability, i.e. noisy, sensor information <ref> (Zhang & Liu, 1997) </ref>. The application of the decomposition methods of Chapter 6 to POMDPs would be very complicated. In general, there may exist strings of observations that could cause an 149 agent's belief state to become smeared out over the entire state space.
References-found: 59

