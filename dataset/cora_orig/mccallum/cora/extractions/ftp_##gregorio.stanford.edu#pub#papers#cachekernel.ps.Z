URL: ftp://gregorio.stanford.edu/pub/papers/cachekernel.ps.Z
Refering-URL: http://www.cs.washington.edu/homes/romer/590lobo/index.html
Root-URL: 
Email: fcheriton,kjdg@cs.stanford.edu  
Title: A Caching Model of Operating System Kernel Functionality  
Author: David R. Cheriton and Kenneth J. Duda 
Address: Stanford, CA 94025  
Affiliation: Computer Science Department Stanford University  
Date: November 1994.  
Note: Appears in "Proceedings of the First Symposium on Operating Systems Design and Implementation," Usenix Association,  
Abstract: Operating system research has endeavored to develop micro-kernels that provide modularity, reliability and security improvements over conventional monolithic kernels. However, the resulting kernels have been slower, larger and more error-prone than desired. These efforts have also failed to provide sufficient application control of resource management required by sophisticated applications. This paper describes a caching model of operating system functionality as implemented in the Cache Kernel, the supervisor-mode component of the V++ operating system. The Cache Kernel caches operating system objects such as threads and address spaces just as conventional hardware caches memory data. User-mode application kernels handle the loading and writeback of these objects, implementing application-specific management policies and mechanisms. Experience with implementing the Cache Kernel and measurements of its performance on a multiprocessor suggest that the caching model can provide competitive performance with conventional monolithic operating systems, yet provides application-level control of system resources, better modularity, better scalability, smaller size and a basis for fault containment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.N. Bershad, T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: This trap forwarding uses similar techniques to those described for UNIX binary emulation <ref> [8, 19, 1] </ref>. A trap executed by a thread execut ing in its application kernel (address space) is handled as a Cache Kernel call.
Reference: [2] <author> B.N. Bershad, C. Chambers, S. Eggers, C. Maeda, D. McNamee, P. Pardyak, S. Savage, and E. Gun Sirer. </author> <title> Spin an extensible microkernel for application-specific operating system services. </title> <institution> University of Washington Computer Science and Engineering Technical Report 94-03-03, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: The Cache Kernel trap forwarding facility most closely resembles the sort of same-CPU IPC found in L3, providing efficient transfer of control in the special case of an application communicating with its kernel. A different approach to application-specific customization is being explored by the SPIN micro-kernel effort <ref> [2] </ref>. In SPIN, untrusted users write kernel extensions in a pointer-safe language. The extensions are compiled by a trusted compiler and dynamically loaded into the micro-kernel, where they are activated by system events (such as context switch or page fault).
Reference: [3] <author> J.B. Chen, A. Borg, </author> <title> and N.P. Jouppi. A simulation-based study of TLB performance. </title> <booktitle> In Proc. 19th Annual Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 114-123. </pages> <publisher> ACM SIGARCH, IEEE Computer Society, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: Moreover, a program that has poorer page locality than we have hypothesized (i.e., less than four percent usage of pages) also suffers a significant performance penalty from TLB miss behavior on most architectures <ref> [3] </ref>. For example, we measured up to a 25 percent degradation in performance in the MP3D program mentioned above from processors accessing particles scattered across too many pages.
Reference: [4] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Comm. ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: The following sections argue that this caching model reduces supervisor-level complexity, provides application control of resource management and provides application control over exception conditions and recovery, addressing the problems with micro-kernel designs to date (including a micro-kernel that we developed previously <ref> [4] </ref>). The next section describes the Cache Kernel programming interface, illustrating its use by describing how an emulator application kernel would use this interface to implement standard UNIX-like services. Section 3 describes how sophisticated applications can use this interface directly by executing as part of their own application kernel. <p> The SRM communicates with other instances of itself on other MPMs using the RPC facility, coordinating to provide distributed scheduling using techniques developed for distributed operating systems. In this sense, the SRM corresponds to the first team in V <ref> [4] </ref>. The SRM is replicated on each MPM for failure autonomy between MPMs, to simplify the SRM management, and to limit the degree of parallelism, as was discussed with other application kernels above. <p> measured by Harty and Cheriton [16] indicates that performance of applications on the Cache Kernel will be comparable to that of conventional operating systems on the same hardware. 6 Related Work The Cache Kernel builds on the experience and insight gained in the previous work on micro-kernels, such as V <ref> [4] </ref> and Mach [21]. As demonstrated in Mach, a dominant contributor to the complexity of these kernels is the virtual memory mechanisms for recording shared mappings, such as shadow object chains.
Reference: [5] <author> D.R. Cheriton, H. Goosen, and P. Boyle. </author> <title> ParaDiGM: A highly scalable shared-memory multi-computer architecture. </title> <journal> IEEE Computer, </journal> <volume> 24(2), </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: This simulation library provides temporal synchronization, virtual space decomposition of processing, load balancing and cache-architecture-sensitive memory management. By allowing application control of resource management and exception handling, the Cache Kernel provides the basis for a highly scalable general-purpose parallel computer architecture that we have been developing in the ParaDiGM <ref> [5] </ref> project. The ParaDiGM architecture is illustrated in Figure 4. Each multiprocessor module (MPM) is a self-contained unit with a small number of processors, second-level cache and high-speed network interfaces, executing its own copy of the Cache Kernel out of its PROM and local memory.
Reference: [6] <author> D.R. Cheriton, H. Goosen, and P. Machanick. </author> <title> Restructuring a parallel simulation to improve shared memory multiprocessor cache behavior: A first experience. </title> <booktitle> In Shared Memory Multiprocessor Symposium, </booktitle> <pages> pages 23-31. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, we have experimented with a hypersonic wind tunnel simulator, MP3D <ref> [6] </ref>, implemented using the particle-in-cell technique. This program can use hundreds of megabytes of memory, parallel processing and significant communication bandwidth to move particles when executed across multiple nodes and can significantly benefit from careful management of its own resources. <p> We are currently developing application kernels and operating system emulators that exploit the Cache Kernel capabilities. In particular, we are developing a simulation kernel (running on the Cache Kernel) that supports applications such as the MP3D wind tunnel simulation <ref> [6] </ref>. The operating systems emulators, such as one for UNIX, allow simple applications to share the same hardware concurrently with these sophisticated applications. We are also exploring the use of the Cache Kernel and modular application kernels for fault-tolerant scalable parallel and distributed computing, as described in Section 3.
Reference: [7] <author> D.R. Cheriton and R. Kutter. </author> <title> Optimizing memory-based messaging for scalable shared memory multiprocessor architectures. </title> <institution> Stanford Computer Science Technical Report CS-93-123, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: of page mapping descriptor is minimized because space for these descriptors dominates the space requirements for the Cache Kernel (see Section 5). 2.2 Interprocess Communication All interprocess and device communication is provided in the caching model by implementing it as an extension of the virtual memory system using memory-based messaging <ref> [7] </ref>. With memory-based messaging, threads communicate through the memory system by mapping a shared region of physical memory into the sender and receiver address spaces, as illustrated in Figure 3. <p> In particular, the driver only needs to support memory mapping the special device 2 The ParaDiGM hardware <ref> [7] </ref> provides automatic signal-on-write to memory in message mode, delivering an address-valued signal to each processor managing a signal thread for the page when a thread writes a cache line in message mode. <p> These measurements are in agreement with (in fact slightly better than) those reported for a similar implementation of memory-based messaging <ref> [7] </ref>. Because the communication schemes are identical at the higher levels, and no Cache Kernel involvement occurs on data transfer, the communication using the Cache Kernel is as efficient as the communication in the V implementation of memory-based messaging. <p> A redesign of Multics [20] proposed the idea of virtual processes that were loaded and saved from a fixed number of real processes, similar to the thread caching mechanism in the Cache Kernel, but this proposal was never implemented. Finally, the Cache Kernel exploits memory-based messaging <ref> [7] </ref> and application-controlled physical memory [16] to minimize mechanism while providing performance and control to sophisticated applications that wish to provide their own operating system kernel. It also builds on experience in implementing binary UNIX emulation [8, 19].
Reference: [8] <author> D.R. Cheriton, G.R. Whitehead, </author> <title> and E.W. Sznyter. Binary emulation of UNIX using the V kernel. </title> <booktitle> In USENIX Summer Conference. USENIX, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This trap forwarding uses similar techniques to those described for UNIX binary emulation <ref> [8, 19, 1] </ref>. A trap executed by a thread execut ing in its application kernel (address space) is handled as a Cache Kernel call. <p> Finally, the Cache Kernel exploits memory-based messaging [7] and application-controlled physical memory [16] to minimize mechanism while providing performance and control to sophisticated applications that wish to provide their own operating system kernel. It also builds on experience in implementing binary UNIX emulation <ref> [8, 19] </ref>. In contrast to Chorus [14], which loads operating system emulator modules directly into the kernel, the Cache Kernel executes its emulators in a separate address space and in non-privileged mode. The lock-free implementation uses similar techniques to that used by Massalin and Pu [18].
Reference: [9] <author> D.R. Engler, M.F. Kaashoek, and J.W. O'Toole Jr. </author> <title> The operating system kernel as a secure programmable machine. </title> <booktitle> Proceedings of the ACM European SIGOPS Workshop, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: Moreover, the mechanisms in the user class libraries, such as the virtual memory support, are more readily user customizable using the C++ inheritance mechanism. Like the Cache Kernel, the Aegis exokernel <ref> [9] </ref> enables application-specific customization through a micro-kernel implementing a minimal machine-dependent interface to the underlying hardware. Like SPIN, Aegis allows un trusted users to extend the supervisor-level portion of the operating system using a variety of techniques to achieve saftey, including code examination, sandboxing, and the use of type-safe languages.
Reference: [10] <editor> A.C. Bomberger et al. </editor> <booktitle> The KeyKOS nanokernel architecture. In Proceedings of the USENIX Workshop on Micro-kernels and Other Kernel Architectures. USENIX, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: The Pmap interface also does not consider multi-mapping consistency support, as required for memory-based messaging. In contrast to the caching of operating system objects in the Cache Kernel, which writes back the objects to untrusted application kernels, KeyKOS <ref> [10] </ref> writes back objects to protected disk pages. That is, it is only caching the objects in the sense of paging those data structures. <p> They also provide application performance that is competitive with conventional monolithic operating systems. In contrast to the highly optimized, same-CPU and cross-address space IPC in L3 [12] and KeyKOS <ref> [10] </ref>, the Cache Kernel supports inter-CPU peer-to-peer horizontal communication through memory-based messaging. The Cache Kernel trap forwarding facility most closely resembles the sort of same-CPU IPC found in L3, providing efficient transfer of control in the special case of an application communicating with its kernel.
Reference: [11] <author> D. Black et al. </author> <title> Translation lookaside consistency: A software approach. </title> <booktitle> In Proc. 17th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 113-122, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: With the Cache Kernel, this complexity has been moved outside of the kernel, cleanly partitioned from the performance-critical mapping supported by the Cache Kernel. The Cache Kernel interface and facilities for virtual memory support bear some similarity to Mach's Pmap interface <ref> [11, 15] </ref>. However, the Cache Kernel includes additional support for deferred copy as well as page group protection, which was not needed in Mach because the Pmap interface was only an internal interface. The Pmap interface also does not consider multi-mapping consistency support, as required for memory-based messaging.
Reference: [12] <author> J. Liedtke et al. </author> <title> Two years of experience with a micro-kernel based os. </title> <journal> Operating Systems Review, </journal> <volume> 25(2) </volume> <pages> 57-62, </pages> <year> 1991. </year>
Reference-contexts: They also provide application performance that is competitive with conventional monolithic operating systems. In contrast to the highly optimized, same-CPU and cross-address space IPC in L3 <ref> [12] </ref> and KeyKOS [10], the Cache Kernel supports inter-CPU peer-to-peer horizontal communication through memory-based messaging. The Cache Kernel trap forwarding facility most closely resembles the sort of same-CPU IPC found in L3, providing efficient transfer of control in the special case of an application communicating with its kernel.
Reference: [13] <author> K. Anderson et al. </author> <title> Tools for the development of application-specific virtual memory management. </title> <booktitle> In OOPSLA, </booktitle> <year> 1993. </year>
Reference-contexts: The memory management library provides the abstraction of physical segments mapped into virtual memory regions, managed by a segment manager that assigns virtual addresses to physical memory, handling the loading of mapping descriptors on page faults. It bears some similarity to the library described by Anderson et al. <ref> [13] </ref>. The processing library is basically a thread library that schedules threads by loading them into the Cache Kernel rather than by using its own dispatcher and run queue.
Reference: [14] <author> M. Rozier et al. </author> <title> Overview of the CHORUS distributed operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-kernels and Other Kernel Architectures. USENIX, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Finally, the Cache Kernel exploits memory-based messaging [7] and application-controlled physical memory [16] to minimize mechanism while providing performance and control to sophisticated applications that wish to provide their own operating system kernel. It also builds on experience in implementing binary UNIX emulation [8, 19]. In contrast to Chorus <ref> [14] </ref>, which loads operating system emulator modules directly into the kernel, the Cache Kernel executes its emulators in a separate address space and in non-privileged mode. The lock-free implementation uses similar techniques to that used by Massalin and Pu [18].
Reference: [15] <author> R. Rashid et al. </author> <title> Machine-independent virtual memory management for paged unitprocessor and multiprocessor architectures. </title> <journal> IEEE Trans Comput., </journal> <volume> 37(8) </volume> <pages> 896-908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: With the Cache Kernel, this complexity has been moved outside of the kernel, cleanly partitioned from the performance-critical mapping supported by the Cache Kernel. The Cache Kernel interface and facilities for virtual memory support bear some similarity to Mach's Pmap interface <ref> [11, 15] </ref>. However, the Cache Kernel includes additional support for deferred copy as well as page group protection, which was not needed in Mach because the Pmap interface was only an internal interface. The Pmap interface also does not consider multi-mapping consistency support, as required for memory-based messaging.
Reference: [16] <author> K. Harty and D.R. Cheriton. </author> <title> Application-controlled physical memory using external page cache management. </title> <booktitle> In ASPLOS, </booktitle> <pages> pages 187-197. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: For example, a large-scale parallel scientific simulation can run directly on top of the Cache Kernel to allow application-specific management of physical memory <ref> [16] </ref> (to avoid random page faults), direct access to the memory-based messaging, and application-specific processor scheduling to match program parallelism to the number of available processors. For example, we have experimented with a hypersonic wind tunnel simulator, MP3D [6], implemented using the particle-in-cell technique. <p> This cost is comparable to the page fault cost of Ultrix and other conventional systems and also comparable to the cost of the comparable operation for external page cache management in the V kernel as described by Harty and Cheriton <ref> [16] </ref>. A page fault that entails page zeroing, page copying or page read from backing store incurs costs that make the Cache Kernel overhead insignificant. Extrapolating from the application-level performance measured by Harty and Cheriton [16] indicates that performance of applications on the Cache Kernel will be comparable to that of <p> for external page cache management in the V kernel as described by Harty and Cheriton <ref> [16] </ref>. A page fault that entails page zeroing, page copying or page read from backing store incurs costs that make the Cache Kernel overhead insignificant. Extrapolating from the application-level performance measured by Harty and Cheriton [16] indicates that performance of applications on the Cache Kernel will be comparable to that of conventional operating systems on the same hardware. 6 Related Work The Cache Kernel builds on the experience and insight gained in the previous work on micro-kernels, such as V [4] and Mach [21]. <p> Finally, the Cache Kernel exploits memory-based messaging [7] and application-controlled physical memory <ref> [16] </ref> to minimize mechanism while providing performance and control to sophisticated applications that wish to provide their own operating system kernel. It also builds on experience in implementing binary UNIX emulation [8, 19].
Reference: [17] <author> J. Kearns and S. DeFazio. </author> <title> Diversity in database reference behavior. Performance Evaluation Review, </title> <year> 1989. </year>
Reference-contexts: Second, micro-kernels do not support domain-specific resource allocation policies any better than monolithic kernels, an increasingly important issue with sophisticated applications and application systems. For example, the standard page-replacement policies of UNIX-like operating systems perform poorly for applications with random or sequential access <ref> [17] </ref>. Placement of conventional operating system kernel services in a micro-kernel-based server does not generally give the applications any more control because the server is a fixed protected system service.
Reference: [18] <author> H. Massalin and C. Pu. </author> <title> A lock-free multiprocessor OS kernel. </title> <type> Technical Report CUCS-005-91, </type> <institution> Computer Science Department, Columbia University, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: In contrast to Chorus [14], which loads operating system emulator modules directly into the kernel, the Cache Kernel executes its emulators in a separate address space and in non-privileged mode. The lock-free implementation uses similar techniques to that used by Massalin and Pu <ref> [18] </ref>. These advances together with the caching approach reduce the complexity of the Cache Kernel such that it can be integrated with the PROM for further stability and reliability. They also provide application performance that is competitive with conventional monolithic operating systems.
Reference: [19] <author> R. Rashid and D. Goluv. </author> <title> UNIX as an application process. </title> <booktitle> In USENIX Summer Conference. Usenix, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This trap forwarding uses similar techniques to those described for UNIX binary emulation <ref> [8, 19, 1] </ref>. A trap executed by a thread execut ing in its application kernel (address space) is handled as a Cache Kernel call. <p> Finally, the Cache Kernel exploits memory-based messaging [7] and application-controlled physical memory [16] to minimize mechanism while providing performance and control to sophisticated applications that wish to provide their own operating system kernel. It also builds on experience in implementing binary UNIX emulation <ref> [8, 19] </ref>. In contrast to Chorus [14], which loads operating system emulator modules directly into the kernel, the Cache Kernel executes its emulators in a separate address space and in non-privileged mode. The lock-free implementation uses similar techniques to that used by Massalin and Pu [18].
Reference: [20] <author> M. Schroeder, D. Clark, and J. Saltzer. </author> <title> The MUL-TICS kernel design project. </title> <booktitle> In Proceedings of the 6th Symposium on Operating Systems Principles, </booktitle> <pages> pages 43-56. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1977. </year>
Reference-contexts: That is, it is only caching the objects in the sense of paging those data structures. A redesign of Multics <ref> [20] </ref> proposed the idea of virtual processes that were loaded and saved from a fixed number of real processes, similar to the thread caching mechanism in the Cache Kernel, but this proposal was never implemented.
Reference: [21] <author> M. Young et al. </author> <title> The duality of memory and communication in the implementation of a multiprocessor operating system. </title> <booktitle> In 11th Symp. on Operating Systems Principles. ACM, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: and Cheriton [16] indicates that performance of applications on the Cache Kernel will be comparable to that of conventional operating systems on the same hardware. 6 Related Work The Cache Kernel builds on the experience and insight gained in the previous work on micro-kernels, such as V [4] and Mach <ref> [21] </ref>. As demonstrated in Mach, a dominant contributor to the complexity of these kernels is the virtual memory mechanisms for recording shared mappings, such as shadow object chains.
References-found: 21

