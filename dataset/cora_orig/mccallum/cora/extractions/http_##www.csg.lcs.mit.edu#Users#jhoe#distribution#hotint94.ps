URL: http://www.csg.lcs.mit.edu/Users/jhoe/distribution/hotint94.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/jhoe/
Root-URL: 
Email: jhoe@abp.lcs.mit.edu  
Title: Network Interface for Message-Passing Parallel Computation on a Workstation Cluster  
Author: James C. Hoe 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: As commercial microprocessors become increasingly popular in current MPP architectures, high-performance commercial workstations have also received increased attention as cost-effective building blocks for large parallel-processing systems. The Fast User-level Network (FUNet) project [10] is an attempt at constructing an inexpensive workstation-based parallel system capable of supporting efficient execution of message-passing parallel programs. Based on MIT's Arctic [1] network technology, FUNet connects stock-configured commodity workstations with a high-bandwidth packet-switched routing network. The Fast User-level Network Interface (FUNi) is the custom hardware network interface device that provides access to FUNet for both message passing and remote direct-memory-access (DMA) block transfers between parallel peer processes on FUNet-connected workstations. The FUNi hardware mechanisms allow direct low-overhead user-level accesses to FUNet while maintaining secure and transparent sharing of FUNet among multiple parallel applications. FUNi can be realized as SBus peripheral cards to allow compatibility with a variety of workstation platforms. The relaxed clock speed (25MHz max.) of SBus allows FUNi to be inexpensively implemented using FPGA parts that are synthesized from designs captured in Verilog Hardware Description Language [15]. SBus's Direct Virtual Memory Access (DVMA)[7] also assists FUNi in overcoming the performance limitations imposed by existing workstation designs. Simulation results have shown that FUNet with FUNi, when coupled with latency-hiding software techniques, is effective in supporting fine-grained parallel processing on a workstation cluster. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Boughton, et. al. </author> <title> Arctic User's Manual. Computation Structures Group, </title> <institution> Laboratory of Computer Science, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [2] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transaction on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The round-trip transit latency of the request and reply messages on the network is negligible by comparison. The software component of communication latency not only tends to be large but is also less predictable. Fortunately, split-phased transactions <ref> [2] </ref> can allow us to tolerate the effects of communication latency by overlapping the communication with useful computations. Instead of stalling for the requested data, a processor could perform other independent computations and continue the thread requesting the data only when the fetch has been satisfied.
Reference: [3] <author> A. Beguelin, J. Dongarra, G. Geist, R. Manchek, and V. Sunderam. </author> <title> A user's guide to PVM parallel virtual machine. </title> <type> Technical Report TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: In Figure 7 and an improvement in the linearity of speedup in all cases as problem size is increased. 7 Related Work PVM <ref> [3] </ref> and Linda [8] are examples of software systems that enable parallel processing on a cluster of workstations using existing LAN facilities. Interworkstation communication is accomplished through interface routines implemented over existing Unix interprocessor communication and networking facilities. These systems have the advantages of not requiring additional hardware.
Reference: [4] <author> E. A. Brewer. </author> <title> Aspects of a parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-527, </type> <institution> Laboratory of Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: We first describe the simulator to establish confidence in the results of the experiments. Next, we explain the two benchmark programs and analyze the results of the simulations. 5.1 The FUNi Simulator The simulator is based on the PROTEUS <ref> [4] </ref> simulation engine that allows rapid development of event-driven simulators of parallel architectures. The PROTEUS simulation engine is a collection of C source files for an abstracted core system of a simulator.
Reference: [5] <author> Chris Dalton, el. al. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993. </year>
Reference-contexts: These systems have the advantages of not requiring additional hardware. However, the scalability and granularity of parallel processing are restricted by the overhead of communication. The high communication overhead has been attributed to the poor implementations of transport and network protocols [6]. Afterburner <ref> [5] </ref> combines hardware and software techniques to improve the performance of TCP/IP communications. The Afterburner project investigated several software techniques for reducing redundant data movement in the TCP/IP protocol and implemented an accompanying network-independent network interface card for HP series 700 workstations.
Reference: [6] <author> D. D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communication Magazine, </journal> <month> June </month> <year> 1989. </year>
Reference-contexts: These systems have the advantages of not requiring additional hardware. However, the scalability and granularity of parallel processing are restricted by the overhead of communication. The high communication overhead has been attributed to the poor implementations of transport and network protocols <ref> [6] </ref>. Afterburner [5] combines hardware and software techniques to improve the performance of TCP/IP communications. The Afterburner project investigated several software techniques for reducing redundant data movement in the TCP/IP protocol and implemented an accompanying network-independent network interface card for HP series 700 workstations.
Reference: [7] <author> E. H. Frank and J. D. Lyle. </author> <title> SBus Specification B.0. Sun Microsystems, </title> <publisher> Inc., </publisher> <year> 1990. </year>
Reference-contexts: Thus, this mechanism also serves as an automatic rate control for throttling network activities of over-active sending processes, preventing them from swamping other processors with messages. 4 FUNi FUNi will be implemented as peripheral cards for SBus <ref> [7] </ref>. The SBus card is chosen as the target FUNi implementation primarily for the SBus's DVMA (Direct Virtual Memory Accesses) feature that is crucial to FUNi's programming interface. The ease of implementation is also a major consideration that stood in favor of the SBus.
Reference: [8] <author> D. Gelernter. </author> <title> Parallel programming in Linda. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: In Figure 7 and an improvement in the linearity of speedup in all cases as problem size is increased. 7 Related Work PVM [3] and Linda <ref> [8] </ref> are examples of software systems that enable parallel processing on a cluster of workstations using existing LAN facilities. Interworkstation communication is accomplished through interface routines implemented over existing Unix interprocessor communication and networking facilities. These systems have the advantages of not requiring additional hardware.
Reference: [9] <author> D. S. Henry and C. F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of ASP-LOS, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: A straightforward message-passing interface could be implemented as memory-mapped registers such as in CM-5 [14], or as a packet-sized array of memory-mapped registers as suggested by Joerg and Henry <ref> [9] </ref> (Figure 1). These interfaces are passive devices that only respond to the processor's direct manipulation through memory-mapped operations. A user program composes an outbound packet by writing the content of the packet, with its header, to the registers through memory-mapped writes. <p> The in-terprocessor communication in fine-grain parallel processing occurs in frequent and small-size messages. The communication overhead must be further minimized by giving the user processes direct control of the network interface. These low-overhead user-level network interface designs can be found in many contemporary MPP architectures <ref> [9, 14] </ref>. However, these designs typically involve the support of custom system or CPU design. In most contemporary workstation designs, the RISC microprocessors are optimized for cached accesses while the bus architectures are optimized for blocked transfers.
Reference: [10] <author> J. C. Hoe. </author> <title> Effective parallel computation on workstation cluster with user-level communication network. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: 1 Introduction In a cluster of workstations connected by a traditional LAN, the scalability and granularity of parallel processing are heavily restricted by the cost of interprocessor communication. The Fast User-level Network (FUNet) <ref> [10] </ref> is an attempt at constructing an efficient workstation-based message-passing parallel system by augmenting a LAN-connected cluster with an additional high-performance user-level communication network suitable for parallel computation. <p> The transfer bandwidth through an established path is two 16-bit halfwords per network cycle. The wire delay between the routers is one network cycle. 5.2 FUNi FUNi hardware events are accurately accounted for in terms of latency and resource utilization. The simulator supports the full programming interface defined in <ref> [10] </ref>. User processes access FUNi's internal control registers through simulated memory-mapped reads and writes. The simulated FUNi uses DVMA accesses in bursts of 1, 4 or 8 words to access the user memory. The DVMA bus transactions are sequentialized with bus transactions from the CPU.
Reference: [11] <institution> LSI Logic. </institution> <note> L64853A SBus DMA Controller Technical Manual, </note> <year> 1991. </year>
Reference-contexts: The FUNi card behaves as a slave device in response to memory-mapped accesses from the CPU. FUNi assumes the role of a master device to perform DVMA to access the user's packet queues. An off-the-shelf L64853A SBus DVMA Controller <ref> [11] </ref> from LSI Logic will provide both interfaces on the FUNi card. 5 4.2.2 FUNi Core Module The FUNi Core Module contains three finite-state-machine controllers that coordinate packet movements through FUNi. This module will be partitioned, according to the subunit boundary, for implementation as a 3-FPGA chip set.
Reference: [12] <author> M. A. Blumrich, et. al. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of 21st ISCA, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The network interface design must take these constraints into account and use the available features to its advantage. FUNi uses the DVMA feature of the SBus to replace the costly memory-mapped accesses. The SHRIMP multicomputer project <ref> [12] </ref> specifies another user-level network interface that involves the memory system to reduce the communication overhead in a constrained environment. The SHRIMP network interface is designed for a network of Pentium PC's with Xpress Bus and EISA bus.
Reference: [13] <author> A. S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Thus, a process is only able to communicate with its cooperating peer processes of the same application whom all share the same application GID. 3.2 Acknowledgment/Retry End-to-End Flow Control An Acknowledgment/Retry End-to-End Flow Control Protocol, a simplification of Selective Repeat Protocol <ref> [13] </ref>, is carried out by the FUNi hardware transparently from user programs. When FUNi absorbs an inbound packet from the network, it needs to return an acknowledgment to the originating FUNi. If FUNi accepts the packet, a positive acknowledgment is sent back to acknowledge the acceptance.
Reference: [14] <author> Thinking Machines Corporation. </author> <title> The Connection Machine: </title> <type> CM-5 Technical Summary, </type> <month> January </month> <year> 1992. </year>
Reference-contexts: A straightforward message-passing interface could be implemented as memory-mapped registers such as in CM-5 <ref> [14] </ref>, or as a packet-sized array of memory-mapped registers as suggested by Joerg and Henry [9] (Figure 1). These interfaces are passive devices that only respond to the processor's direct manipulation through memory-mapped operations. <p> The in-terprocessor communication in fine-grain parallel processing occurs in frequent and small-size messages. The communication overhead must be further minimized by giving the user processes direct control of the network interface. These low-overhead user-level network interface designs can be found in many contemporary MPP architectures <ref> [9, 14] </ref>. However, these designs typically involve the support of custom system or CPU design. In most contemporary workstation designs, the RISC microprocessors are optimized for cached accesses while the bus architectures are optimized for blocked transfers.
Reference: [15] <author> D. E. Thomas and P. Moorby. </author> <title> The Verilog Hardware Description Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference: [16] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of 19th ISCA, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: transaction overhead (not including the cycles to acquire the bus), and a transfer bandwidth of one 32-bit word per two cycles is used in the simulation. 6 Benchmark and Analysis Two benchmark programs based on University of Califor-nia at Berkeley's version of the Connection Machine Active Message (CMAM) communication library <ref> [16] </ref> were executed on a FUNet simulator to evaluate FUNet and FUNi. The CMAM library was ported to the FUNet cluster by rewriting the low-level primitives that dealt with the network interface directly. <p> be further reduced by hand crafting the primitives.) Furthermore, when coupled with the larger packet size and the DMA feature of FUNi, the extended CMAM primitives perform competitively with their CM-5 counterpart in all respects. 6.2 Matrix Multiply This particular version of matrix-multiply is taken from von Eiken et. al. <ref> [16] </ref>. The example is well suited for a FUNet cluster because the algorithm pipelines each remote fetch with computations on previously fetched data. The overlapping of communication delay with useful computations hides the effect of FUNi's relatively high communication latency. Two experiments were performed with the matrix multiply program. <p> by overlapping communication latency with useful computation, the lowered overhead of communication enables the FUNet cluster to achieve comparable processor utilization and scalability as a successful contemporary MPP system. 6.2.1 Latency Hiding and Overhead Amortization This experiment was performed by von Eiken, et. al. for CMAM on a 128-node CM-5 <ref> [16] </ref>. The experiment is scaled down for execution on a 32-node FUNet simulator.
Reference: [17] <author> Xilinx. </author> <title> The Programmable Logic Data Book, </title> <booktitle> 1993. </booktitle> <pages> 10 </pages>
Reference-contexts: Instead, designs will be entered in Verilog Hardware Description Language and compiled into the appropriate netlists by Synopsys HDL Compiler. The current plan calls for implementation using the Xilinx 4000 Family of Field Programmable Gate Arrays (FPGA) <ref> [17] </ref>. The reprogrammability of the FPGA firmware will allow rapid revisioning of the FUNi hardware during the hardware development and future studies.
References-found: 17

