URL: http://www.research.microsoft.com/~ruf/files/thesis.ps
Refering-URL: http://www.research.microsoft.com/~ruf/
Root-URL: http://www.research.microsoft.com
Title: Topics in Online Partial Evaluation  
Author: Erik Ruf 
Keyword: Key Words and Phrases: Partial Evaluation, Program Specialization, Online Specialization, Abstract Interpretation, Control Flow Analysis, Polyvariant Specialization, Re-use Analysis, Program Generation  
Note: This research was supported in part by NSF Contract No. MIP-8902764, by Advanced Research Projects Agency, Department of Defense, Contract No. N0039-91-K-0138, and by an AT&T Bell Laboratories Ph.D. Scholarship. (i)  
Address: Stanford, California 94305-4055  
Affiliation: Computer Systems Laboratory Departments of Electrical Engineering Computer Science Stanford University  
Date: March, 1993  
Pubnum: Technical Report: CSL-TR-93-563 (also FUSE Memo 93-14)  
Abstract: Partial evaluation is a performance optimization technique for computer programs. When a program is run repeatedly with only small variations in its input, we can profit by taking the program and a description of the constant portion of the input, and producing a "specialized" program that computes the same input/output relation as the original program, but only for inputs satisfying the description. This program runs faster because computations depending only on constant inputs are performed only once, when the specialized program is constructed. This technique has not only proven useful for speeding up "interpretive" programs such as simulators, language interpreters, and pattern matchers, but also encompasses many "traditional" compiler optimizations. Contemporary work in partial evaluation has concentrated on "o*ine" methods, where high-speed specialization is achieved at the cost of slower specialized programs by limiting the sorts of decision-making that can occur at specialization time. This dissertation investigates "online" methods, which impose no such restrictions, and demonstrates the benefits of specialization-time decision-making in FUSE, a partial evaluator for a functional subset of Scheme. We describe two new methods, return value analysis and parameter value analysis, for computing more accurate estimates of runtime values at specialization time, enabling more optimizations, and yielding better specialized programs with less "hand-tweaking" of the source. We develop a re-use analysis mechanism for avoiding redundancies in specialized code, improving the efficiency of both the specializer and the programs it produces. Finally, we show how to produce efficient, optimizing program generators by using our techniques to specialize an online program specializer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abelson, G. J. Sussman, and J. Sussman. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Their solution consists of a pre-processing phase which marks each program point with the names of the variables known to be live (in the sense of <ref> [2] </ref>) at that point; only the values of those variables are used in making re-use decisions. Compared with our mechanism, this scheme has two disadvantages. Because the analysis is static, it cannot handle conditional liveness based on specialization-time values.
Reference: [3] <author> A. Aiken and B. R. Murphy. </author> <title> Static type inference in a dynamically typed language. </title> <booktitle> In Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 279-290, </pages> <address> Orlando, </address> <year> 1991. </year>
Reference-contexts: In particular, the control-flow analyses of Shivers [105] and Harrison [50], and the type analysis of Aiken and Murphy <ref> [3, 84] </ref> must recompute the analysis of a procedure each time its abstract arguments move up in the lattice. If their abstract interpreters were to keep track of which information was actually used to perform abstract reductions during the analysis, they might be able to avoid some amount of recomputation. <p> The FL type inferencer of Aiken and Murphy <ref> [84, 3] </ref> treats types as sets of expressions rather than sets of values, avoiding some of the difficulties usually encountered when treating function types. <p> Aiken and Murphy's type analyzer <ref> [3, 84] </ref> appears to use a similar method. Mogensen's higher-order partially static polyvariant binding time analysis [83] is for a typed language, and uses (user- or inferencer-provided) declarations when deciding what recursive types to construct.
Reference: [4] <author> L. Andersen. </author> <title> Self-applicable C program specialization. </title> <booktitle> In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <pages> pages 54-61, </pages> <month> June </month> <year> 1992. </year>
Reference: [5] <author> L. Andersen and C. Gomard. </author> <title> Speedup analysis in partial evaluation (preliminary results). </title> <booktitle> In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <pages> pages 1-7, </pages> <year> 1992. </year>
Reference: [6] <author> W.-Y. Au, D. Weise, and S. Seligman. </author> <title> Generating compiled simulations using partial evaluation. </title> <booktitle> In Proceedings of the 28th Design Automation Conference, </booktitle> <pages> pages 205-210. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications <ref> [8, 10, 6, 117] </ref> rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [7] <author> L. Beckman et al. </author> <title> A partial evaluator and its use as a programming tool. </title> <journal> Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 291-357, </pages> <year> 1976. </year>
Reference-contexts: Work continues on all of these fronts; however, we do not believe that any current o*ine specializer is sufficiently powerful to produce an efficient program generator from TINY. 7.5. RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers <ref> [67, 7, 49] </ref> used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used.
Reference: [8] <author> A. </author> <title> Berlin. A compilation strategy for numerical programs based on partial evaluation. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> July </month> <year> 1989. </year> <note> Published as Artificial Intelligence Laboratory Technical Report TR-1144. 341 342 BIBLIOGRAPHY </note>
Reference-contexts: RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers <ref> [67, 49, 8, 101, 29] </ref> maintain type information at specialization time. REDFUN-2 [49], can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> In certain restricted cases, REDFUN-2 also reasons about values returned by specializations of non-recursive procedures, though it lacks a template mechanism, and thus must compute all return values explicitly. The online systems of Berlin <ref> [8] </ref> and Schooler [101] propagate information downward using placeholders and partials, respectively, both of which are similar to FUSE's symbolic values. <p> None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications <ref> [8, 10, 6, 117] </ref> rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [9] <author> A. </author> <title> Berlin. Partial evaluation applied to numerical computation. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <year> 1990. </year>
Reference: [10] <author> A. Berlin and D. Weise. </author> <title> Compiling scientific code using partial evaluation. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 25-37, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications <ref> [8, 10, 6, 117] </ref> rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application. <p> We believe that meta-object protocols [72, 75] and reflective language implementations [36, 106] will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies. For example, the work in <ref> [10] </ref> used a very simple partial evaluator that produced only straight-line code (i.e., had no procedure specialization or termination mechanism, and could not produce residual loops), yet obtained quite large speedups|the insight was to specialize only the body of an inner loop in a numeric computation.
Reference: [11] <author> A. Bondorf. </author> <title> Automatic autoprojection of higher order recursive equations. </title> <editor> In N. Jones, editor, </editor> <booktitle> Proceedings of the 3rd European Symposium on Programming, </booktitle> <pages> pages 70-87. </pages> <publisher> Springer-Verlag, LNCS 432, </publisher> <year> 1990. </year>
Reference-contexts: Of course, specializing all calls might require too much bookkeeping and caching, so some heuristic for deciding which calls are worth caching might be necessary (Similix-2 <ref> [11] </ref> chooses to specialize all if-expressions and closure bodies).
Reference: [12] <author> A. Bondorf. </author> <title> Self-Applicable Partial Evaluation. </title> <type> PhD thesis, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1990. </year> <note> Revised version: DIKU Report 90/17. </note>
Reference-contexts: We would expect that specializing an MP interpreter on this program would build two residual loops (i.e., specializations of the procedure mp-while), one for both of the two (while kn ...) loops, and one for the (while next ...) loop. Most o*ine specializers (such as those described in <ref> [12, 76, 83, 102] </ref> do exactly that. They are able to build one 6.5. <p> Since most loops in a program reference only a small fraction of the variables in the state, we would expect to see many redundant specializations based on the values of unreferenced variables in the state. However, the literature on specializing interpreters <ref> [12, 22, 33, 102, 64] </ref> fails to report such redundancies. We believe there are two reasons for this. First, most work to date on specializing interpreters has been performed with o*ine specializers. <p> Because we expect that the reader may wish to contrast our account of specializer specialization (Section 7.3) with other treatments of the subject (such as <ref> [15, 12, 83] </ref>), we adopt the standard o*ine terminology for this chapter only . <p> For brevity, our description will be couched in a denotational-semantics-like language similar to that used in <ref> [12] </ref>, with double brackets around Scheme syntactic objects. Injection and projection functions for sum domains will be omitted. Of course, the real specializer is written in Scheme and operates on (preprocessed) Scheme programs. <p> the form (cons (car names) (car values)); this might be part of a routine to construct an association list representation of the store to be used as the final output of the interpreter. (The expression (cons (car names) (car values)) is a standard example; we are following the treatment of <ref> [15, 12, 43] </ref>. <p> 0 let p 00 1 2 Val ! [[(quote p 0 1 )]], p 0 p 00 2 2 Val ! [[(quote p 0 2 )]], p 0 in [[(cons p 00 1 p 00 The program generator of Figure 7.6 is similar to that obtained by the DIKU researchers <ref> [15, 12] </ref> when self-applying a simple online specializer on an interpreter. <p> This is the major rationale behind the development of o*ine specialization techniques <ref> [64, 15, 12, 83] </ref>. O*ine specialization solves the generality problem with relatively little added mechanism in the specializer (indeed, o*ine specializers are usually smaller than their online counterparts, since specialization-time values no longer need be tagged). <p> The efficiency of this program generator is comparable to that of one produced by specializing an o*ine specializer on the same fragment <ref> [15, 12] </ref>, with the exception of an additional tagging operation to inject the returned residual code fragment into a dynamic pe-value. 7.3.3 Examples In this section, we give several examples of program generators constructed by specializing TINY on inputs with known binding times, and analyze their performance relative to "naive" program <p> The program generator was executed on two inputs: interpreter (1): program = comparison program (c.f. Page 225) interpreter (2): program = exponentiation program (c.f. <ref> [12] </ref>) 7.3.4 Results Before we continue, we should note that the specialized programs obtained by direct specialization, execution of an efficient program generator, and execution of a naive program 12 We mean, known to be static, but with value unknown until program generation time. 7.3. <p> A comparison of the speed of the specializer and our program generators is shown in reported by other work on program generator generation <ref> [64, 12, 83] </ref>. These figures describe benefits due to the use of a program generator, but do not indicate how much of this benefit is due to the use of an efficient program generator. <p> The size ratios are more striking: the naive program generators are 2-37 times larger than the efficient program generators. These numbers are larger than those reported in <ref> [12] </ref>, presumably because Similix factors out primitives into abstract data types (which results in operations like peval-car or car in the program generator), while FUSE beta-substitutes the entire bodies of TINY's primitives. <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions <ref> [83, 12, 23, 45] </ref>, global side effects [12], and issues of code duplication and termination [103, 12]. <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions [83, 12, 23, 45], global side effects <ref> [12] </ref>, and issues of code duplication and termination [103, 12]. The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA [22, 83], and facet analysis [29], which allows BTA to make use of known properties of unknown values. <p> Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions [83, 12, 23, 45], global side effects [12], and issues of code duplication and termination <ref> [103, 12] </ref>. The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA [22, 83], and facet analysis [29], which allows BTA to make use of known properties of unknown values. <p> Other accuracy improvements have been achieved via program transformation, both manually <ref> [12] </ref> and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in [15, 12, 62]. <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in <ref> [15, 12, 62] </ref>. Even in the o*ine world, efficiency methods other than self-application have been used, including the factoring out of computations depending on binding time information (as opposed to only factoring out the binding time computations themselves) [25] and handwriting a program generator generator [56]. <p> In particular, V-Mix uses a configuration analysis (described as a "BTA at specialization time") to make reduce/residualize decisions and to compute static/dynamic approximations of function results at specialization time (Bondorf outlines a similar approach in <ref> [12] </ref>, p. 34). <p> Thus, a system with an "accurate BTA" would be an 7.6. FUTURE WORK 257 expresses concerns over the pollution of entire expressions due to a single subexpression having an unknown binding time. Bondorf 's Treemix <ref> [12] </ref> uses such an analysis, but its effectiveness is not described. The motivation behind such methods is twofold.
Reference: [13] <author> A. Bondorf. </author> <title> Improving binding times without explicit CPS-conversion. </title> <booktitle> In 1992 ACM Conference in Lisp and Functional Programming, </booktitle> <address> San Francisco, California. </address> <booktitle> (Lisp Pointers, </booktitle> <volume> vol. V, no. 1, </volume> <year> 1992), </year> <pages> pages 1-10. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference: [14] <author> A. Bondorf and O. Danvy. </author> <title> Automatic autoprojection for recursive equations with global variables and abstract data types. </title> <type> DIKU Report 90/04, </type> <institution> University of Copen-hagen, Copenhagen, Denmark, </institution> <year> 1990. </year>
Reference-contexts: We believe that the need for such mechanisms will only increase in the future, as specializers and the languages they operate on become more powerful. * Side Effects: Existing specializers for languages with side effects make little use of information about values in the store; indeed, some, such as <ref> [14, 21] </ref> force all store 202 CHAPTER 6. AVOIDING REDUNDANT SPECIALIZATION operations to be residualized. <p> One simple way to do this is to specialize all calls, then, after specialization, post-unfold all calls with only one call site (such a 1-bit reference counting scheme is used in <ref> [14] </ref>). Of course, specializing all calls might require too much bookkeeping and caching, so some heuristic for deciding which calls are worth caching might be necessary (Similix-2 [11] chooses to specialize all if-expressions and closure bodies). <p> Schism [21] allows the definition of ML-like types in Scheme, allowing a minor amount of representation independence (e.g., the user needn't know if records are implemented as lists or vectors) but no higher degree of insulation (i.e., if Scheme supported record types, we'd get the same result). Similix <ref> [14] </ref> allows the user to hide the internals of an operator from the partial evaluator, executing the operator only if its inputs are completely known, and leaving it residual otherwise. This is useful for avoiding code size explosion, but unfortunately prevents any optimization that might take advantage of partial inputs.
Reference: [15] <author> A. Bondorf, N. Jones, T. Mogensen, and P. Sestoft. </author> <title> Binding time analysis and the taming of self-application. </title> <type> Draft, </type> <pages> 18 pages, </pages> <institution> DIKU, University of Copenhagen, Den-mark, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Because we expect that the reader may wish to contrast our account of specializer specialization (Section 7.3) with other treatments of the subject (such as <ref> [15, 12, 83] </ref>), we adopt the standard o*ine terminology for this chapter only . <p> The type PEVal is a disjoint union of Scheme values and expressions; one obvious way to implement it is with a tagged record, namely either (static &lt;value&gt;) or (dynamic &lt;expression&gt;). Many online specializers, such as the one of <ref> [15] </ref> and the simple online partial evaluation semantics of [29] capitalize on a relationship between expressions and values, namely, that constant expressions of the form (quote &lt;value&gt;) are capable of representing values. <p> the form (cons (car names) (car values)); this might be part of a routine to construct an association list representation of the store to be used as the final output of the interpreter. (The expression (cons (car names) (car values)) is a standard example; we are following the treatment of <ref> [15, 12, 43] </ref>. <p> 0 let p 00 1 2 Val ! [[(quote p 0 1 )]], p 0 p 00 2 2 Val ! [[(quote p 0 2 )]], p 0 in [[(cons p 00 1 p 00 The program generator of Figure 7.6 is similar to that obtained by the DIKU researchers <ref> [15, 12] </ref> when self-applying a simple online specializer on an interpreter. <p> This is the major rationale behind the development of o*ine specialization techniques <ref> [64, 15, 12, 83] </ref>. O*ine specialization solves the generality problem with relatively little added mechanism in the specializer (indeed, o*ine specializers are usually smaller than their online counterparts, since specialization-time values no longer need be tagged). <p> The efficiency of this program generator is comparable to that of one produced by specializing an o*ine specializer on the same fragment <ref> [15, 12] </ref>, with the exception of an additional tagging operation to inject the returned residual code fragment into a dynamic pe-value. 7.3.3 Examples In this section, we give several examples of program generators constructed by specializing TINY on inputs with known binding times, and analyze their performance relative to "naive" program <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in <ref> [15, 12, 62] </ref>. Even in the o*ine world, efficiency methods other than self-application have been used, including the factoring out of computations depending on binding time information (as opposed to only factoring out the binding time computations themselves) [25] and handwriting a program generator generator [56].
Reference: [16] <author> M. A. Bulyonkov. </author> <title> Polyvariant mixed computation for analyzer programs. </title> <journal> Acta In-formatica, </journal> <volume> 21 </volume> <pages> 473-484, </pages> <year> 1984. </year>
Reference-contexts: After the appropriate formal/actual bindings are added to the environment, the unfolded/specialized body can be constructed via a recursive invocation of the specializer (i.e., a call to the valuation function PE of Figure 7.2). Specialization TINY is a polyvariant program point specializer <ref> [16, 62] </ref>; that is, it constructs specializations of certain program points (in this case, user function applications) and caches them for potential re-use at other program points (function applications with equivalent argument vectors).
Reference: [17] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference: [18] <author> C. Chambers. </author> <title> The Design and Implementation of the Self Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1992. </year> <note> Published as technical report STAN-CS-92-1420. </note>
Reference: [19] <author> C. Chambers and D. Ungar. </author> <title> Iterative type analysis and extended message splitting: Optimizing dynamically-typed object-oriented programs. </title> <journal> LISP and Symbolic Computation, </journal> <volume> 4(3) </volume> <pages> 283-310, </pages> <month> July </month> <year> 1991. </year> <note> BIBLIOGRAPHY 343 </note>
Reference: [20] <author> D. R. Chase, M. Wegman, and F. K. Zadeck. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This is difficult because the "collapsing" operation can be done in different ways, with different results; such difficulties are standard when analyzing pointer data structures <ref> [20, 51] </ref>. Existing approaches to this problem in the pointer analysis community have used either k-bounded approximations, which limit the size of nonrecursive type descriptors, or methods based on limiting each program expression to returning a single type descriptor.
Reference: [21] <author> C. Consel. </author> <title> New insights into partial evaluation: the SCHISM experiment. </title> <booktitle> In Proceedings of the 2nd European Symposium on Programming, </booktitle> <pages> pages 236-246, </pages> <address> Nancy, France, 1988. </address> <publisher> Springer-Verlag, LNCS 300. </publisher>
Reference-contexts: We believe that the need for such mechanisms will only increase in the future, as specializers and the languages they operate on become more powerful. * Side Effects: Existing specializers for languages with side effects make little use of information about values in the store; indeed, some, such as <ref> [14, 21] </ref> force all store 202 CHAPTER 6. AVOIDING REDUNDANT SPECIALIZATION operations to be residualized. <p> Some partial evaluators have limited support for abstract data types. Schism <ref> [21] </ref> allows the definition of ML-like types in Scheme, allowing a minor amount of representation independence (e.g., the user needn't know if records are implemented as lists or vectors) but no higher degree of insulation (i.e., if Scheme supported record types, we'd get the same result).
Reference: [22] <author> C. Consel. </author> <title> Analyse de programmes, Evaluation partielle et Generation de compila-teurs. </title> <type> PhD thesis, </type> <institution> Universite de Paris 6, Paris, France, </institution> <month> June </month> <year> 1989. </year> <pages> 109 pages. </pages> <note> (In French). </note>
Reference-contexts: Since most loops in a program reference only a small fraction of the variables in the state, we would expect to see many redundant specializations based on the values of unreferenced variables in the state. However, the literature on specializing interpreters <ref> [12, 22, 33, 102, 64] </ref> fails to report such redundancies. We believe there are two reasons for this. First, most work to date on specializing interpreters has been performed with o*ine specializers. <p> As we move to more sophisticated binding time analyses, such as polyvariant BTA <ref> [83, 22, 76] </ref> and BTA which allows the use of known properties of otherwise unknown values [29], more information will be made available at specialization time. <p> Because of the complexity of such a specializer, we will not demonstrate full self-application; instead, we will show that a nontrivial online program specializer with power at least equivalent to that of MIX [64] and Schism <ref> [22] </ref> can yield an efficient program generator when specialized by FUSE. We will specialize our small online specializer on several programs, and will evaluate the efficiency of the results. <p> This latter approach is also used by the partially static binding time analyses of Mogensen (one binding time grammar production per program point in [83]) and Consel (one type descriptor per cons point in <ref> [22] </ref>), and in the monovariant type evaluator of [118]. Such approaches work well for analyzing an existing program, because the identity of the expressions in the program can be used as "anchor points" to perform least upper bounding and build recursions (as is done in [61, 82, 23]). <p> The problem, then, lies in deciding when and how to collapse, or generalize. Consel's polyvariant partially static BTA <ref> [22] </ref> operates by keeping all nonrecursive invocations of a procedure distinct, but collapsing recursive call sites together with initial call sites; this works fine for BTA, but will not work for online specialization, as it precludes unfolding of recursive procedures. <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures <ref> [83, 22] </ref>, higher-order functions [83, 12, 23, 45], global side effects [12], and issues of code duplication and termination [103, 12]. <p> The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA <ref> [22, 83] </ref>, and facet analysis [29], which allows BTA to make use of known properties of unknown values. Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. <p> "unknown binding time" rather than "dynamic." Only decisions involving expressions with "static" and "dynamic" binding time annotations would be performed at BTA time; decisions involving expressions annotated as "unknown binding time" would be delayed until specialization time, as in online specialization. 20 Consel describes such a binding time lattice in <ref> [22] </ref> but 20 The difference between this approach and traditional o*ine specialization lies in the fact that, in o*ine systems, all binding times must be computed at BTA time, and all reduce/residualize decisions must be completely dictated by the results of the BTA.
Reference: [23] <author> C. Consel. </author> <title> Binding time analysis for higher order untyped functional languages. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 264-272, </pages> <address> Nice, France, </address> <year> 1990. </year>
Reference-contexts: No implementation of this scheme has, as yet, been reported in the literature. The partially static binding time analyses of Mogensen [82] and Consel <ref> [23] </ref> reason about structured types, including recursive ones. Both operate by building finite tree representations of these data types. Consel's facet analysis [29] adds the ability to deduce that certain properties of unknown values will be known at specialization time. <p> Such approaches work well for analyzing an existing program, because the identity of the expressions in the program can be used as "anchor points" to perform least upper bounding and build recursions (as is done in <ref> [61, 82, 23] </ref>). <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions <ref> [83, 12, 23, 45] </ref>, global side effects [12], and issues of code duplication and termination [103, 12].
Reference: [24] <author> C. Consel and O. Danvy. </author> <title> Partial evaluation of pattern matching in strings. </title> <journal> Information Processing Letters, </journal> <volume> 30(2) </volume> <pages> 79-86, </pages> <year> 1989. </year>
Reference-contexts: Otherwise, the outer application will be residualized, and (under all existing higher-order specialization techniques) any inner applications will not be as fully optimized as they otherwise might be. Still other examples are more application-specific, and more difficult to address by automatic means. For example, in <ref> [24] </ref>, a naive pattern matcher was manually rewritten to explicitly maintain (essentially redundant) information so that, when specialized, the resulting program would run faster. 4 An ideal partial evaluator might have been able to deduce 3 Turchin's supercompiler [111, 112] can optimize the consumer loop even if the producer loop's iteration <p> Thus, Turchin's solution, though highly effective in some cases, is not as general as having recursive type descriptions. 4 This example is too large to describe in detail here; please refer to <ref> [24] </ref> for listings of the original and rewritten matchers. 266 CHAPTER 8. CONCLUSION the static information from history of the computation, without the need to make it explicit in the input program.
Reference: [25] <author> C. Consel and O. Danvy. </author> <title> From interpreting to compiling binding times. </title> <editor> In N. Jones, editor, </editor> <booktitle> Proceedings of the 3rd European Symposium on Programming, </booktitle> <pages> pages 88-105. </pages> <publisher> Springer-Verlag, LNCS 432, </publisher> <year> 1990. </year>
Reference-contexts: This autoprojection property, although sufficient to allow self-application to be performed, is insufficient to guarantee efficient results. That is, when the specialized specializer runs, it 1 Several other approaches include handwriting a specializer generator [49, 56] and performing more operations statically prior to specialization time <ref> [25] </ref>. This chapter addresses only the specializer specialization technique. 2 Of course, we know more about the specializer's inputs than this. <p> Even in the o*ine world, efficiency methods other than self-application have been used, including the factoring out of computations depending on binding time information (as opposed to only factoring out the binding time computations themselves) <ref> [25] </ref> and handwriting a program generator generator [56]. Work continues on all of these fronts; however, we do not believe that any current o*ine specializer is sufficiently powerful to produce an efficient program generator from TINY. 7.5. <p> evaluators require some amount of help from the user, either in the form of explicit annotations or in the form of binding time transformations 5 This isn't actually all that useful unless self-application or some other form of static precomputation based on binding times (such as the "action trees" of <ref> [25] </ref>) is used. 268 CHAPTER 8. CONCLUSION performed on the source program. FUSE has made some progress on this front; the termination mechanisms of [115] provide termination without being excessively conservative, and the mechanisms of Chapters 4 and 5 reduce the need for binding time transformations.
Reference: [26] <author> C. Consel and O. Danvy. </author> <title> For a better support of static data flow. </title> <editor> In J. Hughes, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <volume> (LNCS 523), </volume> <pages> pages 496-519, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1991. </year> <title> ACM, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Similarly, 208 CHAPTER 6. AVOIDING REDUNDANT SPECIALIZATION binding time analysis can propagate information out of conditionals only when the test is static, whereas FUSE can do this in both the static and dynamic cases. Several program transformations <ref> [83, 26, 55] </ref> have been developed to address this problem of o*ine systems. The techniques used by partially static binding time analyses to represent specialization-time structures at BTA time may have interesting applications in online specialization. <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically <ref> [83, 34, 26, 55] </ref>. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in [15, 12, 62].
Reference: [27] <author> C. Consel and O. Danvy. </author> <title> Static and dynamic semantics processing. </title> <booktitle> In Eighteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> Orlando, Florida, </address> <pages> pages 14-24. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1991. </year>
Reference-contexts: Consider a statically typed language with parametric polymorphism. An interpreter for such a language would either factor the type descriptors into a separate type environment which is completely known at specialization time (as is done by the Algol interpreter in <ref> [27] </ref>), or would leave them attached to the values in the state (which would still allow a sufficiently precise Binding Time Analysis to deduce that they are static). <p> In the case of specializing the specializer on an interpreter, the resultant program generator bears a similarity to a compiler, in that it makes reductions based on the "static semantics" (syntactic dispatch, static environment lookup, static typing) <ref> [27] </ref> of the program and constructs residual code to implement the "dynamic semantics" (dynamic typing, store operations, primitive reductions) of the program.
Reference: [28] <author> C. Consel and O. Danvy. </author> <title> Partial evaluation: </title> <booktitle> Principles and perspectives. In Proceedings of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1993. </year> <note> (tutorial presented at the conference, to appear). </note>
Reference-contexts: We believe there is still much to be done in both of these areas, but also in the areas of automation, integration, and applications. This section describes our impressions of open problems and potential solutions in each of these areas. For another treatment of some these issues, we recommend <ref> [28] </ref>. 8.2.1 Strength By strength, we mean degree of optimization, or how much the program specializer is able to speed up the input program.
Reference: [29] <author> C. Consel and S. Khoo. </author> <title> Parameterized partial evaluation. </title> <booktitle> In SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991, </year> <title> Toronto, Canada. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 6, </volume> <month> June </month> <year> 1991), </year> <pages> pages 92-105. </pages> <note> ACM, 1991. 344 BIBLIOGRAPHY </note>
Reference-contexts: As we move to more sophisticated binding time analyses, such as polyvariant BTA [83, 22, 76] and BTA which allows the use of known properties of otherwise unknown values <ref> [29] </ref>, more information will be made available at specialization time. It is not clear that all of this information will indeed be useful for performing reductions; thus, we foresee an increased risk of redundant specialization. We also foresee several uses other than the re-use of specializations for our re-use mechanism. <p> Second, since partial evaluation can be considered to be a form of abstract interpretation <ref> [29] </ref> (though this interpretation is not necessarily over a finite-height domain), our mechanism might be useful in other abstract interpretation settings. <p> RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers <ref> [67, 49, 8, 101, 29] </ref> maintain type information at specialization time. REDFUN-2 [49], can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> The online systems of Berlin [8] and Schooler [101] propagate information downward using placeholders and partials, respectively, both of which are similar to FUSE's symbolic values. The parameterized partial evaluation (PPE) framework of Consel and Khoo <ref> [29] </ref> is a user-extensible type system for program specialization which can infer and maintain "static information" drawn from finite semantic algebras. <p> No implementation of this scheme has, as yet, been reported in the literature. The partially static binding time analyses of Mogensen [82] and Consel [23] reason about structured types, including recursive ones. Both operate by building finite tree representations of these data types. Consel's facet analysis <ref> [29] </ref> adds the ability to deduce that certain properties of unknown values will be known at specialization time. Launch-bury's projection-based binding time analysis [76] also models recursive types; it assumes a statically typed language, and constructs a finite domain of approximations from the type declarations. <p> The type PEVal is a disjoint union of Scheme values and expressions; one obvious way to implement it is with a tagged record, namely either (static &lt;value&gt;) or (dynamic &lt;expression&gt;). Many online specializers, such as the one of [15] and the simple online partial evaluation semantics of <ref> [29] </ref> capitalize on a relationship between expressions and values, namely, that constant expressions of the form (quote &lt;value&gt;) are capable of representing values. <p> This is not the case, as adding online termination mechanisms (at least simple ones) does not appreciably increase the difficulty in obtaining an efficient program generator. 5 For a more formal description of a single-threaded cache, see <ref> [29] </ref>. 7.2. TINY: A SMALL ONLINE SPECIALIZER 221 (lambda (names values) (cons (car names) (car values))) In Section 7.4.1, we will show that is the case. 7.2.3 Example In this section, we show two examples of TINY in action. <p> The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA [22, 83], and facet analysis <ref> [29] </ref>, which allows BTA to make use of known properties of unknown values. Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. <p> However, this approach does have some difficulties. For a specializer like FUSE, which can represent typed unknown values, the choice of binding time domain may be difficult; it may be possible to adapt the facet analysis of <ref> [29] </ref> for this purpose. A highly polyvariant analysis would be required; otherwise, the binding times of several variants would be collapsed into one, forcing greater numbers of binding-time-related reductions to be delayed until program generation time.
Reference: [30] <author> C. Consel, C. Pu, and J. Walpole. </author> <title> Incremental specialization: The key to high performance, modularity and portability in operating systems. </title> <type> Research report, </type> <institution> Pacific Software Research Center, Oregon Graduate Institute of Science and Technology, Beaverton, Oregon, USA, </institution> <year> 1992. </year> <note> (draft, to appear). </note>
Reference-contexts: First, if partial evaluation is to be popularized, it will be important to demonstrate its applicability to a wide range of problems, not just interpreters and their close relatives. Luckily, contemporary research is moving into newer domains, such as incremental computation [110] and operating systems <ref> [30] </ref>. We believe that meta-object protocols [72, 75] and reflective language implementations [36, 106] will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [31] <author> K. D. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In IEEE International Conference on Computer Languages, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: Because the analysis is static, it cannot handle conditional liveness based on specialization-time values. Second, since it operates at the granularity of variables rather than values, it cannot detect redundancy due to partially live structures such as the store in an interpreter. In <ref> [31] </ref>, Cooper, Hall, and Kennedy define an optimization called procedure cloning which has strong similarities to program specialization, and describe a re-use criterion for clones. <p> 2 Env defined on all free variables in e, cache c 2 Cache, and "enclosing specialization environment" h 2 Env, the value specification (value-projection s) of the symbolic value s returned by specializing e on r, c, p, and h 22 This is akin to performing inlining and procedure cloning <ref> [31] </ref>, where duplication is performed in the hope that it will enable delta-reduction optimizations (constant folding/propagation, dead code elimination, etc) in some later optimization pass. 23 This complication is due to the fact that the specializer performs type inference and constructs residual code simultaneously.
Reference: [32] <author> O. Danvy. </author> <type> Personal communication. </type> <month> March </month> <year> 1991. </year>
Reference-contexts: 18 If we could decide which expressions were "interesting," we could make the evaluation function explicit in the structure of the source by hoisting "interesting" expressions such as (+ x y) of their enclosing lambda; then, the parameter 3 would be identical in both cases, and reuse could take place <ref> [32] </ref>. Of course, this merely delegates the problem to each call site; taken to extremes, this would require hoisting all statically reducible expressions as high as possible. 6.6. RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known.
Reference: [33] <author> A. De Niel. </author> <title> Partial evaluation: Its application to compiler generator* generation. </title> <type> Technical Report CMU-CS-88-166, </type> <institution> Department of Computer Science, Carnegie Mel-lon University, Pittsburgh, Pennsylvania, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Since most loops in a program reference only a small fraction of the variables in the state, we would expect to see many redundant specializations based on the values of unreferenced variables in the state. However, the literature on specializing interpreters <ref> [12, 22, 33, 102, 64] </ref> fails to report such redundancies. We believe there are two reasons for this. First, most work to date on specializing interpreters has been performed with o*ine specializers.
Reference: [34] <author> A. De Niel, E. Bevers, and K. De Vlaminck. </author> <title> Program bifurcation for a polymorphi-cally typed functional language. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> Septem-ber </month> <year> 1991), </year> <pages> pages 142-153. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: In some cases, partially static structures can be "teased apart" into static and dynamic components either manually or automatically <ref> [83, 34] </ref>, but this is not always possible. Thus, TINY's lack of partially static structures is a limitation. 242 CHAPTER 7. <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically <ref> [83, 34, 26, 55] </ref>. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in [15, 12, 62].
Reference: [35] <author> G. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: SUMMARY 209 This similarity is easier to see in the alternate formulation of <ref> [35] </ref>, called Explanation-Based Learning (EBL), which avoids goal regression by maintaining two substitutions, SPECIFIC and GENERAL. In the case of FUSE, these substitutions correspond to the value and domain specifications of the index, respectively.
Reference: [36] <author> J. des Rivieres and B. C. Smith. </author> <booktitle> The implementation of procedurally reflective languages. In ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 331-347. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: Luckily, contemporary research is moving into newer domains, such as incremental computation [110] and operating systems [30]. We believe that meta-object protocols [72, 75] and reflective language implementations <ref> [36, 106] </ref> will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [37] <author> A. Ershov. </author> <title> On the essence of compilation. </title> <editor> In E. Neuhold, editor, </editor> <booktitle> Formal Description of Programming Concepts, </booktitle> <pages> pages 391-420. </pages> <publisher> North-Holland, </publisher> <year> 1978. </year>
Reference: [38] <author> A. Ershov, D. Bjtrner, Y. Futamura, K. Furukawa, A. Haraldson, and W. Scherlis, </author> <title> editors. Special Issue: Selected Papers from the Workshop on Partial Evaluation and Mixed Computation, </title> <journal> 1987 (New Generation Computing, </journal> <volume> vol. 6, </volume> <pages> nos. </pages> <address> 2,3). </address> <publisher> OHMSHA Ltd. and Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Finally, our result may be of interest to the logic programming community, where, in contrast to the functional programming community, most program specializers use online methods <ref> [38, 100, 74] </ref>. Nonetheless, this result is unlikely to lead to the widespread proliferation of online-specializer-based program generators. The most obvious reason is that, although we can 7.7.
Reference: [39] <author> A. P. Ershov. </author> <title> On the partial computation principle. </title> <journal> Information Processing Letters, </journal> <volume> 6(2) </volume> <pages> 38-41, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: quality of specialization has been improved by various techniques, while the efficiency of specialization has been addressed primarily by means of program generator generation; that is, specializing the specializer itself. 1 The idea of improving the efficiency of specialization by specializing the specializer, independently discovered by Futamura [40] and Ershov <ref> [39] </ref>, is based on the same observation we made above. That is, if a program is executed repeatedly on a constant input, we can benefit by specializing the program on that constant input, and executing the specialized program instead. <p> Self-application is important if we wish to speed up the process of program generator generation via specialization (i.e., if we specialize the specializer specializing itself, producing a program generator generator, often called a "compiler compiler" in the literature <ref> [40, 39, 64] </ref>), or if we wish to perform multiple self-application [43] to achieve several levels of currying.
Reference: [40] <author> Y. Futamura. </author> <title> Partial evaluation of computation process|an approach to a compiler-compiler. </title> <journal> Systems, Computers, Controls, </journal> <volume> 2(5) </volume> <pages> 45-50, </pages> <year> 1971. </year> <note> BIBLIOGRAPHY 345 </note>
Reference-contexts: The quality of specialization has been improved by various techniques, while the efficiency of specialization has been addressed primarily by means of program generator generation; that is, specializing the specializer itself. 1 The idea of improving the efficiency of specialization by specializing the specializer, independently discovered by Futamura <ref> [40] </ref> and Ershov [39], is based on the same observation we made above. That is, if a program is executed repeatedly on a constant input, we can benefit by specializing the program on that constant input, and executing the specialized program instead. <p> Self-application is important if we wish to speed up the process of program generator generation via specialization (i.e., if we specialize the specializer specializing itself, producing a program generator generator, often called a "compiler compiler" in the literature <ref> [40, 39, 64] </ref>), or if we wish to perform multiple self-application [43] to achieve several levels of currying.
Reference: [41] <author> Y. Futamura and K. Nogi. </author> <title> Generalized partial computation. </title> <editor> In D. Bjtrner, A. Ershov, and N. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 133-151. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Unfortunately, SIS is not automatic: it lacks a theorem prover, and thus leaves all reasoning about generalization and folding to the user. Futamura's "Generalized Partial Computation" <ref> [41] </ref> also advocates the use of predicates as specifications, and the use of a theorem prover to further specialize the arms of conditionals based on knowledge of the test. No implementation of this scheme has, as yet, been reported in the literature.
Reference: [42] <author> C. Ghezzi, D. Mandrioli, and A. Tecchio. </author> <title> Program simplification via symbolic interpretation. </title> <editor> In S. Maheshwari, editor, </editor> <booktitle> Foundations of Software Technology and Theoretical Computer Science. Fifth Conference, </booktitle> <address> New Delhi, India. </address> <booktitle> (Lecture Notes in Computer Science, </booktitle> <volume> Vol. 206), </volume> <pages> pages 116-128. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The online variant of PPE performs generalization to compute return values for if expressions, but its behavior with respect to return values of residual calls and parameters to specializations of higher-order procedures is unspecified. The SIS <ref> [42] </ref> system uses predicates as specifications, giving finer-grained specifications than FUSE's type specifications, and offers the possibility of using theorem proving at folding time to show that re-use of specializations was proper.
Reference: [43] <author> R. Gluck. </author> <title> Towards multiple self-application. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 309-320. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: the form (cons (car names) (car values)); this might be part of a routine to construct an association list representation of the store to be used as the final output of the interpreter. (The expression (cons (car names) (car values)) is a standard example; we are following the treatment of <ref> [15, 12, 43] </ref>. <p> PROGRAM GENERATOR GENERATION 231 dynamic arm, values produced by online generalization, values returned from primitives which perform algebraic optimizations, etc; see Chapter 2 for more detailed examples), some of the program generator generation-time representations of pe-values will indeed have dynamic binding time tags. Gluck's "online BTA" strategy <ref> [43] </ref> avoids this problem by calculating binding time tags only from other binding time tags. Since all such calculations are, by definition, static, and thus performable at program generator generation time, the resultant program generator will contain no residual binding time reductions. <p> Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix <ref> [43] </ref>, has been used to generate efficient program generators via self-application. <p> Self-application is important if we wish to speed up the process of program generator generation via specialization (i.e., if we specialize the specializer specializing itself, producing a program generator generator, often called a "compiler compiler" in the literature [40, 39, 64]), or if we wish to perform multiple self-application <ref> [43] </ref> to achieve several levels of currying. It appears as though self-application is achievable only with specializers at particular levels of complexity, where the specializer is simultaneously sufficiently powerful to specialize itself, while being sufficiently simple to be specialized by itself. <p> This has large costs in both space and time due to the quadratic explosion in the size of the representations of values; i.e., if a specializer executes k instructions per instruction in the program being specialized, then program generator generation takes time k 2 . Gluck <ref> [43] </ref> also notes such growth.
Reference: [44] <author> C. Gomard and N. Jones. </author> <title> Compiler generation by partial evaluation: A case study. </title> <journal> Structured Programming, </journal> <volume> 12 </volume> <pages> 123-144, </pages> <year> 1991. </year>
Reference-contexts: Some work on imperative languages has encountered this problem; both <ref> [44] </ref> and [80] limit the use of the store in making re-use decisions. * Typed Languages: The specialization of languages with types, particularly those with polymorphism and subtyping, will lead to more opportunities for redundant specialization. <p> Such a mechanism could be considered a static approximation of FUSE's; its static nature would prevent it from exploiting information that is not available until specialization time. However, from an efficiency standpoint, static analyses are attractive; we are actively investigating a combination of the two. Gomard and Jones <ref> [44] </ref> present a specializer for an imperative language, and discuss the problem of redundant specialization due to "dead" variables assuming multiple static values.
Reference: [45] <author> C. Gomard and N. Jones. </author> <title> A partial evaluator for the untyped lambda-calculus. </title> <journal> Journal of Functional Programming, </journal> <volume> 1(1) </volume> <pages> 21-69, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions <ref> [83, 12, 23, 45] </ref>, global side effects [12], and issues of code duplication and termination [103, 12].
Reference: [46] <author> M. A. Guzowski. </author> <title> Towards developing a reflexive partial evaluator for an interesting subset of LISP. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Engineering and Science, Case Western Reserve University, Cleveland, Ohio, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [47] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference: [48] <author> T. Hansen. </author> <title> Properties of unfolding-based meta-level systems. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 243-254. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference: [49] <author> A. Haraldsson. </author> <title> A Program Manipulation System Based on Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Linkoping University, </institution> <year> 1977. </year> <note> Published as Linkoping Studies in Science and Technology Dissertation No. 14. </note>
Reference-contexts: RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers <ref> [67, 49, 8, 101, 29] </ref> maintain type information at specialization time. REDFUN-2 [49], can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers [67, 49, 8, 101, 29] maintain type information at specialization time. REDFUN-2 <ref> [49] </ref>, can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> This autoprojection property, although sufficient to allow self-application to be performed, is insufficient to guarantee efficient results. That is, when the specialized specializer runs, it 1 Several other approaches include handwriting a specializer generator <ref> [49, 56] </ref> and performing more operations statically prior to specialization time [25]. This chapter addresses only the specializer specialization technique. 2 Of course, we know more about the specializer's inputs than this. <p> Work continues on all of these fronts; however, we do not believe that any current o*ine specializer is sufficiently powerful to produce an efficient program generator from TINY. 7.5. RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers <ref> [67, 7, 49] </ref> used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE <ref> [49] </ref> were used. Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications [8, 10, 6, 117] rather than on efficiency.
Reference: [50] <author> W. L. Harrison III. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. Lisp and Symbolic Computation: </title> <journal> An International Journal 2:3/4:, </journal> <pages> pages 179-396, </pages> <year> 1989. </year> <note> 346 BIBLIOGRAPHY </note>
Reference-contexts: Second, since partial evaluation can be considered to be a form of abstract interpretation [29] (though this interpretation is not necessarily over a finite-height domain), our mechanism might be useful in other abstract interpretation settings. In particular, the control-flow analyses of Shivers [105] and Harrison <ref> [50] </ref>, and the type analysis of Aiken and Murphy [3, 84] must recompute the analysis of a procedure each time its abstract arguments move up in the lattice. <p> FORMALISMS This is just a standard control flow analysis function as in <ref> [105, 50, 104] </ref>; for readers seeking formal definitions, we suggest [105]. We require that control flow analysis be correct in the usual sense (i.e., it only overestimates, never underestimates, the set of call sites). Third, we change the way closures are specialized.
Reference: [51] <author> L. J. Hendren and A. Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 35-47, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This is difficult because the "collapsing" operation can be done in different ways, with different results; such difficulties are standard when analyzing pointer data structures <ref> [20, 51] </ref>. Existing approaches to this problem in the pointer analysis community have used either k-bounded approximations, which limit the size of nonrecursive type descriptors, or methods based on limiting each program expression to returning a single type descriptor.
Reference: [52] <author> F. Henglein. </author> <title> Efficient type inference for higher-order binding-time analysis. </title> <editor> In J. Hughes, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, Massachusetts, </address> <month> August </month> <year> 1991. </year> <booktitle> (Lecture Notes in Computer Science, </booktitle> <volume> vol. 523), </volume> <pages> pages 448-472. </pages> <publisher> ACM, Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [53] <author> F. Henglein. </author> <title> Global tagging optimization by type inference. </title> <booktitle> In Proceedings of the 1992 ACM Conference on Lisp and Functional Programming (LISP Pointers vol. </booktitle> <volume> 5, no. 1, </volume> <month> January-March </month> <year> 1992), </year> <pages> pages 205-215, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: It is likely that tag optimization techniques for dynamically typed languages <ref> [53, 86] </ref> could provide significant improvements here, not only for the specialization of specializers, but the specialization of interpreters for dynamically typed languages as well.
Reference: [54] <author> C. K. Holst. </author> <title> Finiteness analysis. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, Massachusetts, </address> <month> August </month> <year> 1991 </year> <month> (LNCS 523), </month> <pages> pages 473-495. </pages> <publisher> ACM, Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: We extended TINY to perform online generalization as follows. Each formal parameter of each user function definition is annotated according to whether it is guaranteed to assume only a finite number of values at specialization time (such annotations can be computed o*ine, as in <ref> [54] </ref>). The specializer maintains a stack of active procedure invocations; if it detects a recursive call with identical finite arguments, it builds a specialization on the generalization of the argument vectors of the initial and recursive calls; otherwise, it unfolds the call. <p> Given that a good partial evaluator must transcend both these restrictions to achieve good performance, termination becomes a very hard problem. Recent work on termination, in static and dynamic contexts, can be found in <ref> [54] </ref> and [70], respectively. Given the existence of the halting problem, users will always have to choose between guaranteed termination and optimal performance.
Reference: [55] <author> C. K. Holst and J. Hughes. </author> <title> Towards binding time improvement for free. </title> <editor> In S. Pey-ton Jones, G. Hutton, and C. Kehler Holst, editors, </editor> <booktitle> Functional Programming, Glasgow 1990, </booktitle> <pages> pages 83-100. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Similarly, 208 CHAPTER 6. AVOIDING REDUNDANT SPECIALIZATION binding time analysis can propagate information out of conditionals only when the test is static, whereas FUSE can do this in both the static and dynamic cases. Several program transformations <ref> [83, 26, 55] </ref> have been developed to address this problem of o*ine systems. The techniques used by partially static binding time analyses to represent specialization-time structures at BTA time may have interesting applications in online specialization. <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically <ref> [83, 34, 26, 55] </ref>. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in [15, 12, 62].
Reference: [56] <author> C. K. Holst and J. Launchbury. </author> <title> Handwriting cogen to avoid problems with static typing. </title> <booktitle> In Draft Proceedings, Fourth Annual Glasgow Workshop on Functional Programming, Skye, Scotland, </booktitle> <pages> pages 210-218. </pages> <address> Glasgow University, </address> <year> 1991. </year>
Reference-contexts: This autoprojection property, although sufficient to allow self-application to be performed, is insufficient to guarantee efficient results. That is, when the specialized specializer runs, it 1 Several other approaches include handwriting a specializer generator <ref> [49, 56] </ref> and performing more operations statically prior to specialization time [25]. This chapter addresses only the specializer specialization technique. 2 Of course, we know more about the specializer's inputs than this. <p> Even in the o*ine world, efficiency methods other than self-application have been used, including the factoring out of computations depending on binding time information (as opposed to only factoring out the binding time computations themselves) [25] and handwriting a program generator generator <ref> [56] </ref>. Work continues on all of these fronts; however, we do not believe that any current o*ine specializer is sufficiently powerful to produce an efficient program generator from TINY. 7.5. RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods.
Reference: [57] <author> U. Holzle, C. Chambers, and D. Ungar. </author> <title> Optimizing dynamically-typed object-oriented languages with polymorphic inline caches. </title> <booktitle> In ECOOP '91 Conference Proceedings, </booktitle> <pages> pages 21-38. </pages> <publisher> Springer-Verlag LNCS, </publisher> <month> July </month> <year> 1991. </year>
Reference: [58] <author> P. Hudak. </author> <title> A semantic model of reference counting and its abstraction (detailed summary). </title> <booktitle> In Proceedings of the 1986 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 351-363, </pages> <month> August </month> <year> 1986. </year>
Reference: [59] <author> S. Hunt and D. Sands. </author> <title> Binding time analysis: A new PERspective. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 154-165. </pages> <note> ACM, 1991. BIBLIOGRAPHY 347 </note>
Reference: [60] <author> T. Johnsson. </author> <title> Lambda lifting: Transforming programs to recursive equations. </title> <editor> In Jouannaud, editor, </editor> <booktitle> Conference on Functional Programming and Computer Architecture, Nancy. </booktitle> <publisher> Springer-Verlag (LNCS 201), </publisher> <year> 1985. </year>
Reference: [61] <author> N. Jones and A. Mycroft. </author> <title> Data flow analysis of applicative programs using minimal function graphs. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Programming Languages, </booktitle> <address> St. Petersburg, Florida, </address> <pages> pages 296-306. </pages> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: Such approaches work well for analyzing an existing program, because the identity of the expressions in the program can be used as "anchor points" to perform least upper bounding and build recursions (as is done in <ref> [61, 82, 23] </ref>).
Reference: [62] <author> N. D. Jones. </author> <title> Automatic program specialization: A re-examination from basic principles. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 225-282. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: No unnecessary comparisons are performed; the program generator "knows" that the information necessary to reduce the static semantics will be present, and that the information needed to reduce the dynamic semantics will not be present, because that information (the "program division" of <ref> [62] </ref>) was explicitly available when the program generator was constructed. There is a tradeoff here between the efficiency of the program generator and the the quality of the programs it produces. <p> After the appropriate formal/actual bindings are added to the environment, the unfolded/specialized body can be constructed via a recursive invocation of the specializer (i.e., a call to the valuation function PE of Figure 7.2). Specialization TINY is a polyvariant program point specializer <ref> [16, 62] </ref>; that is, it constructs specializations of certain program points (in this case, user function applications) and caches them for potential re-use at other program points (function applications with equivalent argument vectors). <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in <ref> [15, 12, 62] </ref>. Even in the o*ine world, efficiency methods other than self-application have been used, including the factoring out of computations depending on binding time information (as opposed to only factoring out the binding time computations themselves) [25] and handwriting a program generator generator [56].
Reference: [63] <author> N. D. Jones, C. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <booktitle> 1993. (in progress). </booktitle>
Reference: [64] <author> N. D. Jones, P. Sestoft, and H. Stndergaard. </author> <title> An experiment in partial evaluation: The generation of a compiler generator. </title> <booktitle> In Rewriting Techniques and Applications, </booktitle> <pages> pages 124-140. </pages> <publisher> Springer-Verlag, LNCS 202, </publisher> <year> 1985. </year>
Reference-contexts: Since most loops in a program reference only a small fraction of the variables in the state, we would expect to see many redundant specializations based on the values of unreferenced variables in the state. However, the literature on specializing interpreters <ref> [12, 22, 33, 102, 64] </ref> fails to report such redundancies. We believe there are two reasons for this. First, most work to date on specializing interpreters has been performed with o*ine specializers. <p> PROGRAM GENERATOR GENERATION may perform reductions which instead could have been performed once, when the specializer was specialized. This desire to construct efficient program generators motivated the invention of o*ine program specialization <ref> [64] </ref>, in which all of the specializer's reduce/residualize decisions are made prior to specialization time, usually via an automatic prepass called Binding Time Analysis (BTA). The results of these decisions are made available to the specializer as annotations on the source program. <p> Because of the complexity of such a specializer, we will not demonstrate full self-application; instead, we will show that a nontrivial online program specializer with power at least equivalent to that of MIX <ref> [64] </ref> and Schism [22] can yield an efficient program generator when specialized by FUSE. We will specialize our small online specializer on several programs, and will evaluate the efficiency of the results. <p> This is the major rationale behind the development of o*ine specialization techniques <ref> [64, 15, 12, 83] </ref>. O*ine specialization solves the generality problem with relatively little added mechanism in the specializer (indeed, o*ine specializers are usually smaller than their online counterparts, since specialization-time values no longer need be tagged). <p> A comparison of the speed of the specializer and our program generators is shown in reported by other work on program generator generation <ref> [64, 12, 83] </ref>. These figures describe benefits due to the use of a program generator, but do not indicate how much of this benefit is due to the use of an efficient program generator. <p> The first o*ine specializer, MIX <ref> [64, 65] </ref> did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. <p> Self-application is important if we wish to speed up the process of program generator generation via specialization (i.e., if we specialize the specializer specializing itself, producing a program generator generator, often called a "compiler compiler" in the literature <ref> [40, 39, 64] </ref>), or if we wish to perform multiple self-application [43] to achieve several levels of currying.
Reference: [65] <author> N. D. Jones, P. Sestoft, and H. Stndergaard. </author> <title> Mix: A self-applicable partial evaluator for experiments in compiler generation. </title> <journal> Lisp and Symbolic Computation, </journal> 1(3/4):9-50, 1988. 
Reference-contexts: The first o*ine specializer, MIX <ref> [64, 65] </ref> did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable.
Reference: [66] <author> U. Jtrring and W. L. Scherlis. </author> <title> Compilers and staging transformations. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 86-96, </pages> <address> St. Petersburg, Florida, </address> <year> 1986. </year>
Reference-contexts: If, in addition, the specializer had some description of the degree to which an implementation of an abstract data type depended upon its representations, it might attempt to get the best of both worlds. Staging Another strength issue lies in the sensitivity of partial evaluators to staging <ref> [66] </ref> decisions made in the source program. We give a few example here. Most existing specializers miss optimizations when static information must be propagated between loops, because their type systems only describe structured values (pairs and closures) whose size is known at specialization time.
Reference: [67] <author> K. M. Kahn. </author> <title> A partial evaluator of Lisp programs written in Prolog. </title> <editor> In M. V. Caneghem, editor, </editor> <booktitle> First International Logic Programming Conference, </booktitle> <pages> pages 19-25, </pages> <address> Marseille, France, </address> <year> 1982. </year>
Reference-contexts: RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers <ref> [67, 49, 8, 101, 29] </ref> maintain type information at specialization time. REDFUN-2 [49], can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> Work continues on all of these fronts; however, we do not believe that any current o*ine specializer is sufficiently powerful to produce an efficient program generator from TINY. 7.5. RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers <ref> [67, 7, 49] </ref> used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used.
Reference: [68] <author> A. Kanamori and D. Weise. </author> <title> An empirical study of an abstract interpretation of Scheme programs. </title> <type> Unpublished manuscript, </type> <year> 1991. </year>
Reference: [69] <author> M. Katz. </author> <type> Personal communication. </type> <month> September </month> <year> 1991. </year>
Reference: [70] <author> M. Katz and D. Weise. </author> <title> Towards a new perspective on partial evaluation. </title> <booktitle> In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program 348 BIBLIOGRAPHY Manipulation, </booktitle> <pages> pages 29-37, </pages> <address> San Francisco, CA, </address> <year> 1992. </year> <note> Proceedings available as YALEU/DCS/RR-909. </note>
Reference-contexts: However, since z is true, the computation of t is dead code, and does not affect the building of the specialization. Similarly, any computations performed in computing a return value are really only necessary if the specialization of the caller makes use of the return value. Katz <ref> [70] </ref> has proposed performing specialization-time reductions (and corresponding MGI calculations) lazily to avoid this problem. 6.6.2 Types We divide existing work on type systems for specializers into two categories based on when the type analysis is performed: systems that perform type analysis at specialization time, 18 If we could decide which <p> Mogensen's higher-order partially static polyvariant binding time analysis [83] is for a typed language, and uses (user- or inferencer-provided) declarations when deciding what recursive types to construct. One promising approach is the two-stage partial evaluation framework of Katz <ref> [70] </ref>, in 17 By "size," we mean "number of cons cells contained." Even numeric types can become arbitrarily large at runtime, but they are still scalars; there is no need for recursive type descriptors to describe bignums. 7.4. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application. <p> Given that a good partial evaluator must transcend both these restrictions to achieve good performance, termination becomes a very hard problem. Recent work on termination, in static and dynamic contexts, can be found in [54] and <ref> [70] </ref>, respectively. Given the existence of the halting problem, users will always have to choose between guaranteed termination and optimal performance. <p> of information to assure termination (i.e., by discarding statically known values that vary from iteration to iteration, but are not themselves induction variables 7 ), it should not be the case that, given less information, we should have to discard more. 7 These are often called "counters" in the literature <ref> [70] </ref>. A.2. METHODS OF ARGUMENT 279 A.2 Methods of Argument Before going on to describe the implementation of the specializer and its properties, we take a moment to describe the basic structure of the arguments we will be making, and the assumptions underlying them. <p> Not only is this inefficient (since all code constructed on the first k 1 iterations of a fixed point loop gets discarded), but it adds conceptual complexity. We now believe that performing a polyvariant type analysis, followed by a separate code construction pass <ref> [70] </ref>, is a cleaner method. A.4.
Reference: [71] <author> S. Khoo and R. Sundaresh. </author> <title> Compiling inheritance using partial evaluation. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 211-222. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference: [72] <author> G. Kiczales, J. des Rivieres, and D. G. Bobrow. </author> <title> The Art of the Metaobject Protocol. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Luckily, contemporary research is moving into newer domains, such as incremental computation [110] and operating systems [30]. We believe that meta-object protocols <ref> [72, 75] </ref> and reflective language implementations [36, 106] will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [73] <author> S. Kleene. </author> <title> Introduction to Metamathematics. </title> <address> D. </address> <publisher> van Nostrand, </publisher> <address> Princeton, New Jersey, </address> <year> 1952. </year>
Reference: [74] <author> A. Lakhotia and L. Sterling. ProMiX: </author> <title> A Prolog partial evaluation system. </title> <editor> In L. Sterling, editor, </editor> <booktitle> The Practice of Prolog, chapter 5, </booktitle> <pages> pages 137-179. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Finally, our result may be of interest to the logic programming community, where, in contrast to the functional programming community, most program specializers use online methods <ref> [38, 100, 74] </ref>. Nonetheless, this result is unlikely to lead to the widespread proliferation of online-specializer-based program generators. The most obvious reason is that, although we can 7.7.
Reference: [75] <author> J. Lamping, G. Kiczales, L. Rodriguez, and E. Ruf. </author> <title> An architecture for an open compiler. </title> <booktitle> In IMSA Workshop on Meta-Level Architectures and Reflection, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Thus, we believe that it would be worthwhile to investigate both (1) implementing a partial evaluator at some intermediate language level in a "real" compiler, and (2) implementing a partial evaluator that can cooperate with an "open" compiler using explicit pragmas or even a meta-level protocol <ref> [75, 89] </ref>. 8.2.5 Applications This work has concentrated largely on methods rather than on applications. We believe that work on applications is of great importance, for two reasons. <p> Luckily, contemporary research is moving into newer domains, such as incremental computation [110] and operating systems [30]. We believe that meta-object protocols <ref> [72, 75] </ref> and reflective language implementations [36, 106] will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [76] <author> J. Launchbury. </author> <title> Projection factorisations in partial evaluation. </title> <booktitle> Distinguished Dissertations in Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: We would expect that specializing an MP interpreter on this program would build two residual loops (i.e., specializations of the procedure mp-while), one for both of the two (while kn ...) loops, and one for the (while next ...) loop. Most o*ine specializers (such as those described in <ref> [12, 76, 83, 102] </ref> do exactly that. They are able to build one 6.5. <p> In either case, the known values used by the interpreter to implement types would not be abstracted by the binding time analysis and could thus lead to redundant specialization. O*ine program specializers for typed languages can treat polymorphism in programs explicitly <ref> [76] </ref>, but even such specializers would need a re-use mechanism for interpreters for polymorphic languages, due to the need to represent values in the interpreted program using a single universal type. <p> As we move to more sophisticated binding time analyses, such as polyvariant BTA <ref> [83, 22, 76] </ref> and BTA which allows the use of known properties of otherwise unknown values [29], more information will be made available at specialization time. <p> Thus, we obtain the desired result without the need for assumptions about the type system. Launchbury also suggests (Section 8.2.5 of <ref> [76] </ref>) using projection-based strictness analysis to deduce that certain information in the arguments to a function will not be used at runtime, and having the specializer treat that function as though it had a smaller argument 6.6. RELATED WORK 205 domain (i.e. without the useless information). <p> Both operate by building finite tree representations of these data types. Consel's facet analysis [29] adds the ability to deduce that certain properties of unknown values will be known at specialization time. Launch-bury's projection-based binding time analysis <ref> [76] </ref> also models recursive types; it assumes a statically typed language, and constructs a finite domain of approximations from the type declarations.
Reference: [77] <author> J. Launchbury. </author> <title> Self-applicable partial evaluation without s-expressions. </title> <booktitle> In Functional Programming Languages and Computer Architecture, </booktitle> <address> Cambridge, Massachusetts, </address> <month> August </month> <year> 1991 </year> <month> (LNCS 523), </month> <pages> pages 145-164. </pages> <publisher> ACM, Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: O*ine specializers for untyped languages can avoid this need for encoding because they have no need to encode dynamic values, and can thus inherit the representation of static values from the underlying virtual machine. Launchbury's lazy encoding technique for typed languages <ref> [77] </ref> reduces encoding overhead for completely static values, but appears to be of less use for partially static values, because, under online methods, the entire spine from the root of a partially static structure to each of its dynamic leaves must be fully encoded at the time the structure is created
Reference: [78] <author> L. A. Lombardi and B. Raphael. </author> <title> Lisp as the language for an incremental computer. </title> <editor> In Berkeley and Bobrow, editors, </editor> <booktitle> The Programming Language Lisp, </booktitle> <pages> pages 204-219. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1964. </year>
Reference: [79] <author> K. Malmkjr. </author> <title> Predicting properties of residual programs. </title> <booktitle> In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <pages> pages 8-13, </pages> <year> 1992. </year> <note> Proceedings available as YALEU/DCS/RR-909. </note>
Reference: [80] <author> M. Marquard and B. Steensgaard. </author> <title> Partial evaluation of an object-oriented imperative language. </title> <type> Master's thesis, </type> <institution> DIKU, University of Copenhagen, Denmark, </institution> <year> 1992. </year> <note> BIBLIOGRAPHY 349 </note>
Reference-contexts: Some work on imperative languages has encountered this problem; both [44] and <ref> [80] </ref> limit the use of the store in making re-use decisions. * Typed Languages: The specialization of languages with types, particularly those with polymorphism and subtyping, will lead to more opportunities for redundant specialization.
Reference: [81] <author> T. Mitchell, M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: In its usual formulation <ref> [81] </ref>, EBG consists of taking an example and an explanation of the construction of the example, and performing goal regression through the explanation, producing a more general rule.
Reference: [82] <author> T. Mogensen. </author> <title> Partially static structures. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 325-347. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: No implementation of this scheme has, as yet, been reported in the literature. The partially static binding time analyses of Mogensen <ref> [82] </ref> and Consel [23] reason about structured types, including recursive ones. Both operate by building finite tree representations of these data types. Consel's facet analysis [29] adds the ability to deduce that certain properties of unknown values will be known at specialization time. <p> Such approaches work well for analyzing an existing program, because the identity of the expressions in the program can be used as "anchor points" to perform least upper bounding and build recursions (as is done in <ref> [61, 82, 23] </ref>).
Reference: [83] <author> T. Mogensen. </author> <title> Binding Time Aspects of Partial Evaluation. </title> <type> PhD thesis, </type> <institution> DIKU, University of Copenhangen, Copenhagen, Denmark, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: We would expect that specializing an MP interpreter on this program would build two residual loops (i.e., specializations of the procedure mp-while), one for both of the two (while kn ...) loops, and one for the (while next ...) loop. Most o*ine specializers (such as those described in <ref> [12, 76, 83, 102] </ref> do exactly that. They are able to build one 6.5. <p> As we move to more sophisticated binding time analyses, such as polyvariant BTA <ref> [83, 22, 76] </ref> and BTA which allows the use of known properties of otherwise unknown values [29], more information will be made available at specialization time. <p> Similarly, 208 CHAPTER 6. AVOIDING REDUNDANT SPECIALIZATION binding time analysis can propagate information out of conditionals only when the test is static, whereas FUSE can do this in both the static and dynamic cases. Several program transformations <ref> [83, 26, 55] </ref> have been developed to address this problem of o*ine systems. The techniques used by partially static binding time analyses to represent specialization-time structures at BTA time may have interesting applications in online specialization. <p> Because we expect that the reader may wish to contrast our account of specializer specialization (Section 7.3) with other treatments of the subject (such as <ref> [15, 12, 83] </ref>), we adopt the standard o*ine terminology for this chapter only . <p> This is the major rationale behind the development of o*ine specialization techniques <ref> [64, 15, 12, 83] </ref>. O*ine specialization solves the generality problem with relatively little added mechanism in the specializer (indeed, o*ine specializers are usually smaller than their online counterparts, since specialization-time values no longer need be tagged). <p> A comparison of the speed of the specializer and our program generators is shown in reported by other work on program generator generation <ref> [64, 12, 83] </ref>. These figures describe benefits due to the use of a program generator, but do not indicate how much of this benefit is due to the use of an efficient program generator. <p> In some cases, partially static structures can be "teased apart" into static and dynamic components either manually or automatically <ref> [83, 34] </ref>, but this is not always possible. Thus, TINY's lack of partially static structures is a limitation. 242 CHAPTER 7. <p> This latter approach is also used by the partially static binding time analyses of Mogensen (one binding time grammar production per program point in <ref> [83] </ref>) and Consel (one type descriptor per cons point in [22]), and in the monovariant type evaluator of [118]. <p> Aiken and Murphy's type analyzer [3, 84] appears to use a similar method. Mogensen's higher-order partially static polyvariant binding time analysis <ref> [83] </ref> is for a typed language, and uses (user- or inferencer-provided) declarations when deciding what recursive types to construct. <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures <ref> [83, 22] </ref>, higher-order functions [83, 12, 23, 45], global side effects [12], and issues of code duplication and termination [103, 12]. <p> The first o*ine specializer, MIX [64, 65] did not handle partially static structures and used explicit unfolding annotations, but was efficiently self-applicable. Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions <ref> [83, 12, 23, 45] </ref>, global side effects [12], and issues of code duplication and termination [103, 12]. <p> The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA <ref> [22, 83] </ref>, and facet analysis [29], which allows BTA to make use of known properties of unknown values. Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically [83, 34, 26, 55]. In all of the above, efficiency was realized by self-application of the specializer. <p> Other accuracy improvements have been achieved via program transformation, both manually [12] and automatically <ref> [83, 34, 26, 55] </ref>. In all of the above, efficiency was realized by self-application of the specializer. The usefulness of explicit binding time computations in realizing self-application is described in [15, 12, 62].
Reference: [84] <author> B. R. Murphy. </author> <title> A type inference system for FL. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: In particular, the control-flow analyses of Shivers [105] and Harrison [50], and the type analysis of Aiken and Murphy <ref> [3, 84] </ref> must recompute the analysis of a procedure each time its abstract arguments move up in the lattice. If their abstract interpreters were to keep track of which information was actually used to perform abstract reductions during the analysis, they might be able to avoid some amount of recomputation. <p> The FL type inferencer of Aiken and Murphy <ref> [84, 3] </ref> treats types as sets of expressions rather than sets of values, avoiding some of the difficulties usually encountered when treating function types. <p> Aiken and Murphy's type analyzer <ref> [3, 84] </ref> appears to use a similar method. Mogensen's higher-order partially static polyvariant binding time analysis [83] is for a typed language, and uses (user- or inferencer-provided) declarations when deciding what recursive types to construct.
Reference: [85] <author> M. Perlin. </author> <title> Call-graph caching: Transforming programs into networks. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 122-128, </pages> <year> 1989. </year>
Reference: [86] <author> J. Peterson. </author> <title> Untagged data in tagged languages: choosing optimal representations at compile time. </title> <booktitle> In Proceedings of the 4th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 89-99, </pages> <year> 1989. </year>
Reference-contexts: It is likely that tag optimization techniques for dynamically typed languages <ref> [53, 86] </ref> could provide significant improvements here, not only for the specialization of specializers, but the specialization of interpreters for dynamically typed languages as well.
Reference: [87] <editor> S. L. Peyton Jones. </editor> <booktitle> The implementation of functional programming languages. Prentice-Hall International, </booktitle> <year> 1987. </year>
Reference: [88] <editor> J. Rees, W. Clinger, et al. </editor> <title> Revised 4 report on the algorithmic language Scheme. </title> <journal> LISP Pointers, </journal> <volume> 4(3) </volume> <pages> 1-55, </pages> <year> 1991. </year>
Reference-contexts: That is, a value is "static" if it is known at specialization time, and "dynamic" otherwise, without any connotation of approximation. 7.2 TINY: A Small Online Specializer This section describes TINY, a small but nontrivial online program specializer for a functional subset of Scheme <ref> [88] </ref> which we will use to demonstrate the construction of online program generators. For reasons of clarity, we will first describe a "watered-down" version of TINY using a denotational-semantics-like language, then describe the actual Scheme implementation. <p> We will not describe this evaluator formally; any interpreter obeying the commonly-understood semantics of functional Scheme (i.e., a subset of the language described in <ref> [88] </ref>) will do. Evaluation of graphical programs is less obvious.
Reference: [89] <author> L. H. Rodriguez Jr. </author> <title> Coarse-grained parallelism using metaobject protocols. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <year> 1991. </year> <note> Published as Xerox PARC Technical Report SSL-91-06. </note>
Reference-contexts: Thus, we believe that it would be worthwhile to investigate both (1) implementing a partial evaluator at some intermediate language level in a "real" compiler, and (2) implementing a partial evaluator that can cooperate with an "open" compiler using explicit pragmas or even a meta-level protocol <ref> [75, 89] </ref>. 8.2.5 Applications This work has concentrated largely on methods rather than on applications. We believe that work on applications is of great importance, for two reasons.
Reference: [90] <author> S. Romanenko. </author> <title> Arity raiser and its use in program specialization. </title> <editor> In N. Jones, editor, </editor> <booktitle> ESOP '90. 3rd European Symposium on Programming, </booktitle> <address> Copenhagen, Denmark, </address> <month> May </month> <year> 1990. </year> <booktitle> (Lecture Notes in Computer Science, </booktitle> <volume> vol. 432), </volume> <pages> pages 341-360. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Of course, any value which might reach a binding time test must be tagged, but current arity raising <ref> [90] </ref> techniques are insufficiently powerful to remove all "dead" tags. In particular, current arity raisers eliminate only static portions of parameter values, without removing static portions of returned values. <p> In this case, it might well be able to, but in other more specialized cases, it might not. Second, even though a particular transformation might be expressible in source code, the decision about whether to perform it may require additional knowledge about the underlying virtual machine. Consider arity raising <ref> [90] </ref>, in which tuple-valued parameters are "spread" 8.2. FUTURE WORK 269 into multiple distinct parameters.
Reference: [91] <author> E. Ruf. </author> <title> Errata for "Using types to avoid redundant specialization". </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 27(1) </volume> <pages> 37-39, </pages> <year> 1992. </year> <note> 350 BIBLIOGRAPHY </note>
Reference: [92] <author> E. Ruf and D. Weise. </author> <title> Using types to avoid redundant specialization. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 321-333. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: It is not clear how to extend this technique to program specialization. 18 The re-use analysis described in <ref> [92] </ref> and this chapter is eager, in that it treats a value as "used" (i.e., reducing its domain specification) whenever a specialization-time reduction is performed which is limited to that particular value. However, not all computed values affect the building of the specialization.
Reference: [93] <author> E. Ruf and D. Weise. </author> <title> Avoiding redundant specialization during partial evaluation. </title> <type> Technical Report CSL-TR-92-518, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [94] <author> E. Ruf and D. Weise. </author> <title> Improving the accuracy of higher-order specialization using control flow analysis. </title> <booktitle> In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <pages> pages 67-74, </pages> <address> San Francisco, CA, </address> <year> 1992. </year> <note> Proceedings available as YALEU/DCS/RR-909. </note>
Reference-contexts: See Chapter 5 and <ref> [97, 94] </ref> for examples. 11 What's important here is that TINY is online; it doesn't matter whether the specializer used to specialize it is online or o*ine, as long as it is sufficiently powerful.
Reference: [95] <author> E. Ruf and D. Weise. </author> <title> On the specialization of online program specializers. </title> <type> Technical report, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1992. </year>
Reference: [96] <author> E. Ruf and D. Weise. </author> <title> Opportunities for online partial evaluation. </title> <type> Technical Report CSL-TR-92-516, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: In FUSE, this arises because of the fundamentally iterative nature of the specializer. First, FUSE uses pairwise generalization <ref> [112, 100, 96] </ref> to compute what actual arguments to use when building a specialization. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [97] <author> E. Ruf and D. Weise. </author> <title> Preserving information during online partial evaluation. </title> <type> Technical Report CSL-TR-92-517, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: See Chapter 5 and <ref> [97, 94] </ref> for examples. 11 What's important here is that TINY is online; it doesn't matter whether the specializer used to specialize it is online or o*ine, as long as it is sufficiently powerful. <p> Chapter 4 and <ref> [116, 97] </ref>). These operations, not present in TINY but present in some versions of FUSE, significantly complicate the task of producing efficient program generators. In this section, we will briefly describe these mechanisms, and explain why present specialization technology cannot handle them. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [98] <author> B. Rytz and M. Gengler. </author> <title> A polyvariant binding time analysis. </title> <booktitle> In ACM SIG-PLAN Workshop on Partial Evaluation and Semantics-Directed Program Manipulation, </booktitle> <pages> pages 21-28, </pages> <year> 1992. </year>
Reference: [99] <author> D. Sahlin. </author> <title> The Mixtus approach to automatic partial evaluation of full Prolog. </title> <editor> In S. Debray and M. Hermenegildo, editors, </editor> <booktitle> Logic Programming: Proceedings of the 1990 North American Conference, </booktitle> <address> Austin, Texas, </address> <month> October </month> <year> 1990, </year> <pages> pages 377-398. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In this section, we will briefly describe these mechanisms, and explain why present specialization technology cannot handle them. Induction Detection Some online specializers such as FUSE and Mixtus <ref> [99, 100] </ref> make unfold/specialize decisions automatically during specialization. This is usually a two-step process: (1) a recursive call is detected (using a specialization-time call stack), then (2) the specializer decides whether the recursive call poses a risk of nontermination.
Reference: [100] <author> D. Sahlin. </author> <title> An Automatic Partial Evaluator for Full Prolog. </title> <type> PhD thesis, </type> <institution> Kungliga Tekniska Hogskolan, Stockholm, Sweden, </institution> <month> March </month> <year> 1991. </year> <type> Report TRITA-TCS-9101, </type> <pages> 170 pages. BIBLIOGRAPHY 351 </pages>
Reference-contexts: In FUSE, this arises because of the fundamentally iterative nature of the specializer. First, FUSE uses pairwise generalization <ref> [112, 100, 96] </ref> to compute what actual arguments to use when building a specialization. <p> In this section, we will briefly describe these mechanisms, and explain why present specialization technology cannot handle them. Induction Detection Some online specializers such as FUSE and Mixtus <ref> [99, 100] </ref> make unfold/specialize decisions automatically during specialization. This is usually a two-step process: (1) a recursive call is detected (using a specialization-time call stack), then (2) the specializer decides whether the recursive call poses a risk of nontermination. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application. <p> Finally, our result may be of interest to the logic programming community, where, in contrast to the functional programming community, most program specializers use online methods <ref> [38, 100, 74] </ref>. Nonetheless, this result is unlikely to lead to the widespread proliferation of online-specializer-based program generators. The most obvious reason is that, although we can 7.7.
Reference: [101] <author> R. Schooler. </author> <title> Partial evaluation as a means of language extensibility. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1984. </year> <note> Published as MIT/LCS/TR-324. </note>
Reference-contexts: RELATED WORK 207 and systems that perform it in a pre-pass, at which time only the binding times of values are known. Several online specializers <ref> [67, 49, 8, 101, 29] </ref> maintain type information at specialization time. REDFUN-2 [49], can also propagate information out of conditionals and from the test of a conditional into its branches, but handles only scalar types (though it does compute disjoint unions, and a limited form of negation, which FUSE doesn't). <p> In certain restricted cases, REDFUN-2 also reasons about values returned by specializations of non-recursive procedures, though it lacks a template mechanism, and thus must compute all return values explicitly. The online systems of Berlin [8] and Schooler <ref> [101] </ref> propagate information downward using placeholders and partials, respectively, both of which are similar to FUSE's symbolic values. The parameterized partial evaluation (PPE) framework of Consel and Khoo [29] is a user-extensible type system for program specialization which can infer and maintain "static information" drawn from finite semantic algebras. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [102] <author> P. Sestoft. </author> <title> The structure of a self-applicable partial evaluator. </title> <editor> In H. Ganzinger and N. Jones, editors, </editor> <title> Programs as Data Objects, </title> <booktitle> Copenhagen, Denmark, 1985. (Lecture Notes in Computer Science, </booktitle> <volume> vol. 217), </volume> <pages> pages 236-256. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Interpreters Our first example is an interpreter for a small imperative language. The "MP" language, first used as an example by Sestoft <ref> [102] </ref>, has if, while, and assignment statements, as well as a variety of expressions. A fragment of a direct-style interpreter for this language is given in Figure 6.8. <p> We would expect that specializing an MP interpreter on this program would build two residual loops (i.e., specializations of the procedure mp-while), one for both of the two (while kn ...) loops, and one for the (while next ...) loop. Most o*ine specializers (such as those described in <ref> [12, 76, 83, 102] </ref> do exactly that. They are able to build one 6.5. <p> Since most loops in a program reference only a small fraction of the variables in the state, we would expect to see many redundant specializations based on the values of unreferenced variables in the state. However, the literature on specializing interpreters <ref> [12, 22, 33, 102, 64] </ref> fails to report such redundancies. We believe there are two reasons for this. First, most work to date on specializing interpreters has been performed with o*ine specializers. <p> However, we would like to show that TINY is indeed a realistic program specializer. To this end, we will show the operation of TINY on an interpreter for a small imperative language. The MP language <ref> [102] </ref> is a small imperative language with if and while control structures, which has become the "canonical" interpreter example for specializers.
Reference: [103] <author> P. Sestoft. </author> <title> Automatic call unfolding in a partial evaluator. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 485-506. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: It is possible that o*ine induction analyses such as that of Sestoft <ref> [103] </ref> could be adapted for this purpose. However, using this information to make TINY's unfold/specialize decisions at program generator generation time could be difficult; just because specialize knows that one list 7.4. EXTENSIONS 249 is shorter than another doesn't mean that it can fully evaluate TINY's decision procedure. <p> Subsequent research has produced increasingly powerful o*ine specializers, which handle partially static structures [83, 22], higher-order functions [83, 12, 23, 45], global side effects [12], and issues of code duplication and termination <ref> [103, 12] </ref>. The accuracy of o*ine specializers has been improved through the development of more accurate binding time analyses, such as the polyvariant BTA [22, 83], and facet analysis [29], which allows BTA to make use of known properties of unknown values.
Reference: [104] <author> P. Sestoft. </author> <title> Replacing function parameters by global variables. </title> <type> Master's thesis, </type> <institution> DIKU, University of Copenhagen, </institution> <year> 1988. </year> <note> Published as DIKU Student Report 88-7-2. </note>
Reference-contexts: FORMALISMS This is just a standard control flow analysis function as in <ref> [105, 50, 104] </ref>; for readers seeking formal definitions, we suggest [105]. We require that control flow analysis be correct in the usual sense (i.e., it only overestimates, never underestimates, the set of call sites). Third, we change the way closures are specialized.
Reference: [105] <author> O. Shivers. </author> <title> Control Flow Analysis of Higher-Order Languages. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1991. </year> <note> Published as technical report CMU-CS-91-145. </note>
Reference-contexts: Second, since partial evaluation can be considered to be a form of abstract interpretation [29] (though this interpretation is not necessarily over a finite-height domain), our mechanism might be useful in other abstract interpretation settings. In particular, the control-flow analyses of Shivers <ref> [105] </ref> and Harrison [50], and the type analysis of Aiken and Murphy [3, 84] must recompute the analysis of a procedure each time its abstract arguments move up in the lattice. <p> For an example of a proof in this style, see <ref> [105] </ref>. 312 APPENDIX A. FORMALISMS body. <p> FORMALISMS This is just a standard control flow analysis function as in <ref> [105, 50, 104] </ref>; for readers seeking formal definitions, we suggest [105]. We require that control flow analysis be correct in the usual sense (i.e., it only overestimates, never underestimates, the set of call sites). Third, we change the way closures are specialized. <p> FORMALISMS This is just a standard control flow analysis function as in [105, 50, 104]; for readers seeking formal definitions, we suggest <ref> [105] </ref>. We require that control flow analysis be correct in the usual sense (i.e., it only overestimates, never underestimates, the set of call sites). Third, we change the way closures are specialized.
Reference: [106] <author> B. C. Smith. </author> <title> Reflection and semantics in Lisp. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 23-35, </pages> <year> 1984. </year>
Reference-contexts: A simpler solution relies on explicit reflection <ref> [106] </ref> in the form of an upcall (i.e., instead of explicitly implementing the "shorter" predicate on lists in TINY, make it a primitive in specialize, which simply checks to see if one of the lists is derived from the other). <p> Luckily, contemporary research is moving into newer domains, such as incremental computation [110] and operating systems [30]. We believe that meta-object protocols [72, 75] and reflective language implementations <ref> [36, 106] </ref> will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [107] <author> D. Smith and T. Hickey. </author> <title> Partial evaluation of a CLP language. </title> <booktitle> In Logic Programming: Proceedings of the 1990 North American Conference, </booktitle> <pages> pages 119-138. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference: [108] <author> G. L. Steele Jr. Rabbit: </author> <title> A compiler for Scheme. </title> <type> Technical Report AI-TR-474, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: Of course, any value which might reach a binding time test must be tagged, but current arity raising [90] techniques are insufficiently powerful to remove all "dead" tags. In particular, current arity raisers eliminate only static portions of parameter values, without removing static portions of returned values. CPS-converting <ref> [108] </ref> programs will take care of the problem of returned values, but may require a fairly sophisticated dead-code analysis in addition to any complexities added by the higher-order nature of CPS code. <p> We describe the changes made to the specializer, some basic monotonicity and termination properties, and give a proof sketch of a correctness property. A.5.1 Changes to the Specializer Basics First, we treat only CPS-converted source programs. 26 . By "CPS-conversion," we do not mean the full CPS transformation of <ref> [108] </ref>; we require only that all function definitions (i.e., top-level definitions and LAMBDA forms not representing continuations) accept continuation parameters, and that IF expressions be CPS-converted. Expressions consisting of primitive calls are permitted, as this saves us from having to rewrite the existing specializer's code for primitives.
Reference: [109] <author> B. Steensgaard. </author> <type> Personal communication. </type> <month> October </month> <year> 1991. </year>
Reference: [110] <author> R. Sundaresh. </author> <title> Building incremental programs using partial evaluation. In Partial Evaluation and Semantics-Based Program Manipulation, New Haven, Connecticut. </title> <journal> (SIGPLAN Notices, </journal> <volume> vol. 26, no. 9, </volume> <month> September </month> <year> 1991), </year> <pages> pages 83-93. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: First, if partial evaluation is to be popularized, it will be important to demonstrate its applicability to a wide range of problems, not just interpreters and their close relatives. Luckily, contemporary research is moving into newer domains, such as incremental computation <ref> [110] </ref> and operating systems [30]. We believe that meta-object protocols [72, 75] and reflective language implementations [36, 106] will also provide significant opportunities for partial evaluation. Second, in some domains, partial evaluation can provide large performance benefits using very simple specialization strategies.
Reference: [111] <author> V. Turchin. </author> <title> The concept of a supercompiler. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(3) </volume> <pages> 292-325, </pages> <year> 1986. </year>
Reference-contexts: RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application. <p> His formulation of self-application (using the "metasystem transition" formalism of Turchin <ref> [111] </ref>) is very similar to the formulation we gave in Choice 2 on page 228. Despite these similarities, however, V-Mix and this work differ appreciably in the encoding and preservation of binding time information. <p> For example, in [24], a naive pattern matcher was manually rewritten to explicitly maintain (essentially redundant) information so that, when specialized, the resulting program would run faster. 4 An ideal partial evaluator might have been able to deduce 3 Turchin's supercompiler <ref> [111, 112] </ref> can optimize the consumer loop even if the producer loop's iteration count is unknown, by combining the two loops into a single residual loop. <p> Thus, we believe that type system and type inference mechanisms, such as recursive types, the use of predicate information in arms of irreducible conditionals, constraints between types, will be useful, as will transformation techniques such as driving <ref> [111] </ref> and currying/uncurrying. Still, we doubt that such issues can be addressed adequately by automatic means. 8.2.2 Efficiency We see several opportunities for improving the efficiency of online program specializers, that is, the speed with which the specialized program is produced.
Reference: [112] <author> V. Turchin. </author> <title> The algorithm of generalization in the supercompiler. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 531-549. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <note> 352 BIBLIOGRAPHY </note>
Reference-contexts: In FUSE, this arises because of the fundamentally iterative nature of the specializer. First, FUSE uses pairwise generalization <ref> [112, 100, 96] </ref> to compute what actual arguments to use when building a specialization. <p> For example, in [24], a naive pattern matcher was manually rewritten to explicitly maintain (essentially redundant) information so that, when specialized, the resulting program would run faster. 4 An ideal partial evaluator might have been able to deduce 3 Turchin's supercompiler <ref> [111, 112] </ref> can optimize the consumer loop even if the producer loop's iteration count is unknown, by combining the two loops into a single residual loop.
Reference: [113] <author> F. van Harmelen and A. Bundy. </author> <title> Explanation-based generalisation = partial evaluation. </title> <journal> Artificial Intelligence, </journal> <volume> 36(3) </volume> <pages> 401-412, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In the case of FUSE, these substitutions correspond to the value and domain specifications of the index, respectively. This link between specialization and EBG has been noted before by van Harmelen and Bundy <ref> [113] </ref>, who showed how to convert a partial evaluator into an EBG system by leaving out unifications due to operational predicates (primitive reductions) run by the partial evaluator.
Reference: [114] <author> D. Weise. </author> <title> Graphs as an intermediate representation for partial evaluation. </title> <type> Technical Report CSL-TR-90-421, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <address> Stan-ford, CA, </address> <year> 1990. </year>
Reference-contexts: use it. (We're still being a bit dishonest here: TINY doesn't actually implement a pe-value as a tag followed by a value or a residual code expression, it implements it as a tag followed by a value and a residual code expression, similar to the symbolic value objects of FUSE <ref> [114, 115] </ref>. This is necessary if partially static values are to be allowed, since the description of the value (e.g., a pair whose car is 4 and and whose cdr is unknown) may not be deducible from its residual code (e.g., (cdr (foo x))).
Reference: [115] <author> D. Weise, R. Conybeare, E. Ruf, and S. Seligman. </author> <title> Automatic online partial evaluation. </title> <editor> In J. Hughes, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture (LNCS 523), </booktitle> <pages> pages 165-191, </pages> <address> Cambridge, MA, </address> <month> August </month> <year> 1991. </year> <title> ACM, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: use it. (We're still being a bit dishonest here: TINY doesn't actually implement a pe-value as a tag followed by a value or a residual code expression, it implements it as a tag followed by a value and a residual code expression, similar to the symbolic value objects of FUSE <ref> [114, 115] </ref>. This is necessary if partially static values are to be allowed, since the description of the value (e.g., a pair whose car is 4 and and whose cdr is unknown) may not be deducible from its residual code (e.g., (cdr (foo x))). <p> this case, having TINY's cons primitive consult the binding time tags of its arguments to determine the tag for its result) is performable at specialization time. 7.4.3 Other Online Mechanisms In addition to binding time tests in primitives, and generalization, some specializers perform other operations online, such as induction detection <ref> [115] </ref> and fixpoint iteration (c.f. Chapter 4 and [116, 97]). These operations, not present in TINY but present in some versions of FUSE, significantly complicate the task of producing efficient program generators. In this section, we will briefly describe these mechanisms, and explain why present specialization technology cannot handle them. <p> RELATED WORK 253 7.5.2 Online Specialization The earliest program specializers [67, 7, 49] used online methods. None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy <ref> [101, 46, 115, 96, 97, 93, 70, 100, 111] </ref> and applications [8, 10, 6, 117] rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application. <p> CONCLUSION performed on the source program. FUSE has made some progress on this front; the termination mechanisms of <ref> [115] </ref> provide termination without being excessively conservative, and the mechanisms of Chapters 4 and 5 reduce the need for binding time transformations. However, there is much work still to be done. <p> See Section 3.3.4 or <ref> [115] </ref> for more details. 278 APPENDIX A. FORMALISMS a boolean value which is true if the function call should be unfolded, and false if it should be residualized.
Reference: [116] <author> D. Weise and E. Ruf. </author> <title> Computing types during program specialization. </title> <type> Technical Report CSL-TR-90-441, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1990. </year> <note> Updated version available as FUSE-MEMO-90-3-revised. </note>
Reference-contexts: Chapter 4 and <ref> [116, 97] </ref>). These operations, not present in TINY but present in some versions of FUSE, significantly complicate the task of producing efficient program generators. In this section, we will briefly describe these mechanisms, and explain why present specialization technology cannot handle them.
Reference: [117] <author> D. Weise and S. Seligman. </author> <title> Accelerating object-oriented simulation via automatic program specialization. </title> <type> Technical Report CSL-TR-92-519, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: None were suitable for program generator generation, though handwritten program generator generators such as REDCOMPILE [49] were used. Subsequent work on online specialization has focused primarily on accuracy [101, 46, 115, 96, 97, 93, 70, 100, 111] and applications <ref> [8, 10, 6, 117] </ref> rather than on efficiency. A notable exception to this is the work of Gluck, whose online specializer, V-Mix [43], has been used to generate efficient program generators via self-application.
Reference: [118] <author> J. Young and P. O'Keefe. </author> <title> Experience with a type evaluator. </title> <editor> In D. Bjtrner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 573-581. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Modifying these techniques to run at specialization time (to describe runtime values),would give FUSE the ability to describe structures such as a list of unknown length that contains only integers. Young and O'Keefe's type evaluator <ref> [118] </ref> is very similar to FUSE, but cannot be considered to be a program specializer because it doesn't build specializations. The type evaluator discovers types (including recursive types) using a variety of techniques, including fixpoint-ing and generalization as used in FUSE. <p> This latter approach is also used by the partially static binding time analyses of Mogensen (one binding time grammar production per program point in [83]) and Consel (one type descriptor per cons point in [22]), and in the monovariant type evaluator of <ref> [118] </ref>. Such approaches work well for analyzing an existing program, because the identity of the expressions in the program can be used as "anchor points" to perform least upper bounding and build recursions (as is done in [61, 82, 23]).
References-found: 118

