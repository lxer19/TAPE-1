URL: http://vismod.www.media.mit.edu/~jdavis/OldPapers/eccv.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/~jdavis/
Root-URL: http://www.media.mit.edu
Title: Recognizing Hand Gestures  
Author: James Davis and Mubarak Shah 
Address: Orlando FL 32816, USA  
Affiliation: Computer Vision Laboratory, University of Central Florida,  
Date: May 2-6, 1994.  
Note: To appear in ECCV-94, Stockholm, Sweden,  
Abstract: This paper presents a method for recognizing human-hand gestures using a model-based approach. A Finite State Machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories, which are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of Left, Right, Up, Down, Grab, Rotate, and Stop.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Baudel T., Beaudouin-Lafon M.: Charade: </author> <title> Remote control of objects using free-hand gestures. </title> <journal> CACM. </journal> <month> July </month> <year> (1993) </year> <month> 28-35 </month>
Reference-contexts: Second, a simple vision glove is employed, i.e., no mechanical glove with LEDs or bulky wires. Current gesture input devices require the user to be linked to the computer, reducing autonomy <ref> [1] </ref>. Vision input overcomes this problem. Third, a duration parameter for gestures is incorporated. <p> Finally, due to Finite State Machine (FSM) implementation of a generic gesture, no warping of the image sequences is necessary. That is, the number of frames in each gesture can be variable. 2 Related Work Baudel and Beaudouin-Lafon <ref> [1] </ref> implemented a system for the remote control of computer-aided presentations using hand gestures. In this system, the user wears a VPL DataGlove which is linked to the computer. The glove can measure the bending of fingers and the position and orientation of the hand in 3-D space. <p> The system is robust in that it is able to detect the pointing regardless of the operator's pointing style. Applications of this system can be similar to the gesture controlled computer-aided presentations of Baudel and Beaudouin-Lafon <ref> [1] </ref> and also can be used in a video browser with a VCR. Darrell and Pentland [4] have also proposed a glove-free environment approach for gesture recognition. Objects are represented using sets of view models, and then are matched to stored gesture patterns using dynamic time warping.
Reference: 2. <author> Cipolla R., Okamoto Y., Kuno Y.: </author> <title> Robust structure from motion using motion parallax. </title> <address> ICCV. </address> <year> (1993) </year> <month> 374-382 </month>
Reference-contexts: The user must begin in the designated start position and is able to make gestures until the termination gesture (Stop) is recognized. There are several advantages of this system over other methods. First, it uses inexpensive black-and-white video. Incorporating color markers on a glove as interest points <ref> [2] </ref> requires costly color imaging, whereas a binary marked glove, as used in this research, can be detected in low-cost black-and-white imaging. Second, a simple vision glove is employed, i.e., no mechanical glove with LEDs or bulky wires. <p> With trained users, the recognition rate was 90- to 98%. This system does not use vision to recognize gestures, but instead uses a linked hardware system to track the hand and arm movements, which makes movement less natural for the user. Cipolla, Okamoto, and Kuno <ref> [2] </ref> present a real-time structure-from-motion (SFM) method in which the 3-D visual interpretation of hand gestures is used in a man-machine interface. A glove with colored markers attached is used as input to the vision system.
Reference: 3. <author> Costello E.: </author> <title> Signing: How to speak with your hands. </title> <publisher> Bantam Books, </publisher> <address> New York (1993) </address>
Reference-contexts: The best example of communication through gestures is given by sign language. American Sign Language (ASL) incorporates the entire English alphabet along with many gestures representing words and phrases <ref> [3] </ref>, which permits people to exchange information in a non-verbal manner. Currently, the human-computer interface is through a keyboard and/or mouse. Physically challenged people may have difficulties with such input devices and may require a new means of entering commands or data into the computer.
Reference: 4. <author> Darrell T., Pentland A.: </author> <title> Space-time gestures. </title> <address> CVPR. </address> <year> (1993) </year> <month> 335-340 </month>
Reference-contexts: Applications of this system can be similar to the gesture controlled computer-aided presentations of Baudel and Beaudouin-Lafon [1] and also can be used in a video browser with a VCR. Darrell and Pentland <ref> [4] </ref> have also proposed a glove-free environment approach for gesture recognition. Objects are represented using sets of view models, and then are matched to stored gesture patterns using dynamic time warping. Each gesture is dynamically time-warped to make it of the same length as the longest model.
Reference: 5. <author> Fukumoto, M., Mase, K., Suenaga, Y.: </author> <title> Real-time detection of pointing actions for a glove-free interface. </title> <booktitle> IAPR Workshop on Machine Vision Applications. </booktitle> <month> Dec. 7-9 </month> <year> (1992) </year> <month> 473-476 </month>
Reference-contexts: The SFM method used here assumes rigid objects, which is not true in the case of hand gestures. Fukumoto, Mase, and Suenaga <ref> [5] </ref> present a system called Finger-Pointer which recognizes pointing actions and simple hand forms in real-time. The system uses stereo image sequences and does not require the operator to wear any special glove. It also requires no special image processing hardware.
Reference: 6. <author> Rangarajan, K., Shah, M.: </author> <title> Establishing motion correspondence. CVGIP: </title> <booktitle> Image Understanding. </booktitle> <month> 54 July </month> <year> (1991) </year> <month> 56-73 </month>
Reference-contexts: A path, known as a trajectory, is generated for each of the m points, starting with the points in the first image and ending with the points in the nth image. Rangarajan and Shah's <ref> [6] </ref> motion correspondence algorithm was chosen for its exploitation of a proximal uniformity constraint, which says objects follow smooth paths and cover a small (proximal) distance in a small time. It was stated previously, in the Phase 2 gesture constraint, that the fingers must move smoothly to the gesture position. <p> image k and image k + 1, 1 p; q; r m; 2 k m 1; q = k1 (p); X k r is the vector from point q in image k to point r in image k + 1, and k X k denotes the magnitude of vector X <ref> [6] </ref>.
References-found: 6

