URL: http://www.cs.umd.edu/users/keleher/papers/wusenix94.ps.gz
Refering-URL: http://www.cs.umd.edu/users/keleher/syllabus.818.html
Root-URL: 
Title: TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems  
Author: Pete Keleher, Alan L. Cox, Sandhya Dwarkadas and Willy Zwaenepoel 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: TreadMarks is a distributed shared memory (DSM) system for standard Unix systems such as SunOS and Ultrix. This paper presents a performance evaluation of TreadMarks running on Ultrix using DECstation-5000/240's that are connected by a 100-Mbps switch-based ATM LAN and a 10-Mbps Ethernet. Our objective is to determine the efficiency of a user-level DSM implementation on commercially available workstations and operating systems. We achieved good speedups on the 8-processor ATM network for Jacobi (7.4), TSP (7.2), Quick-sort (6.3), and ILINK (5.7). For a slightly modified version of Water from the SPLASH benchmark suite, we achieved only moderate speedups (4.0) due to the high communication and synchronization rate. Speedups decline on the 10-Mbps Ethernet (5.5 for Jacobi, 6.5 for TSP, 4.2 for Quicksort, 5.1 for ILINK, and 2.1 for Water), reflecting the bandwidth limitations of the Ethernet. These results support the contention that, with suitable networking technology, DSM is a viable technique for parallel computation on clusters of workstations. To achieve these speedups, TreadMarks goes to great lengths to reduce the amount of communication performed to maintain memory consistency. It uses a lazy implementation of release consistency, and it allows multiple concurrent writers to modify a page, reducing the impact of false sharing. Great care was taken to minimize communication overhead. In particular, on the ATM network, we used a standard low-level protocol, AAL3/4, bypassing the TCP/IP protocol stack. Unix communication overhead, however, remains the main obstacle in the way of better performance for programs like Water. Compared to the Unix communication overhead, memory management cost (both kernel and user level) is small and wire time is negligible.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To do so, LRC divides the execution of each process into intervals, each denoted by an interval index. Every time a process executes a release or an acquire, a new interval begins and the interval index is incremented. Intervals of different processes are partially ordered <ref> [1] </ref>: (i) intervals on a single processor are totally ordered by program order, and (ii) an interval on processor p precedes an interval on processor q if the interval of q begins with the acquire corresponding to the release that concluded the interval of p.
Reference: [2] <author> S. Ahuja, N. Carreiro, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19(8) </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces <ref> [2] </ref>, distributed shared memory [18], and message passing [23]. TreadMarks is a distributed shared memory (DSM) system [18]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 1).
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. A distributed implementation of the shared data-object model. </title> <booktitle> Distributed Systems and Multiprocessor Workshop, </booktitle> <pages> pages 1-19, </pages> <year> 1989. </year>
Reference-contexts: Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [4] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: An interesting alternative is entry consistency (EC) <ref> [4] </ref>. EC differs from RC in that it requires all shared data to be explicitly associated with some synchronization variable. On a lock acquisition EC only propagates the shared data associated with that lock. <p> The overhead of a page fault (without the actual page transfer) is approximately 1 milliseconds, half of which is attributed to process switching overhead in the exception-based implementation. The time to transfer a page (11 milliseconds) dominates all other overheads in the remote page fault time. Bershad et al. <ref> [4] </ref> use a different strategy to implement EC in the Midway DSM system, running on DECStation-500/200s connected by an ATM LAN and running Mach 3.0. Instead of relying on the VM system to detect shared memory updates, they modify the compiler to update a software dirty bit.
Reference: [5] <author> D. Black, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> The Mach exception handling facility. </title> <journal> SigPlan Notices, </journal> <volume> 24(1) </volume> <pages> 45-56, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Bryant et al. [7] implemented SSVM (Structured Shared Virtual Memory) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface [24], and one using the Mach exception interface <ref> [5] </ref>. They report that the latter implementation|which is very similar to ours|is more efficient, because of the inability of Mach's external pager interface to asynchronously update a page in the user's address space.
Reference: [6] <author> M.L. Blount and M. Butrico. DSVM6K: </author> <title> Distributed shared virtual memory on the Risc System/6000. </title> <booktitle> In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 491-500, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Instead of relying on the VM system to detect shared memory updates, they modify the compiler to update a software dirty bit. Our results show that, at least in Ultrix and we suspect in Mach as well, the software communication overhead dominates the memory management overhead. DSVM6K <ref> [6] </ref> is a sequentially consistent DSM system running on IBM RS/6000s connected by 220-Mbps fiber optic links and a nonblocking crossbar switch. The system is implemented inside the AIX v3 kernel and uses a low-overhead protocol for communication over the fiber optic links (IMCS).
Reference: [7] <author> R. Bryant, P. Carini, H.-Y. Chang, and B. Rosenburg. </author> <title> Supporting structured shared virtual memory under Mach. </title> <booktitle> In Proceedings of the 2nd Mach Usenix Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency. Bryant et al. <ref> [7] </ref> implemented SSVM (Structured Shared Virtual Memory) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface [24], and one using the Mach exception interface [5].
Reference: [8] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In order to address the performance problems with earlier DSM systems, the TreadMarks implementation focuses on reducing the amount of communication necessary to keep the distributed memories consistent. It uses a lazy implementation [14] of release consistency [13] and multiple-writer protocols to reduce the impact of false sharing <ref> [8] </ref>. On the 100-Mbps ATM LAN, good speedups were achieved for Jacobi, TSP, Quicksort, and ILINK (a program from the genetic LINKAGE package [16]). <p> Release consistency requires less communication than conventional, sequentially consistent [15] shared memory, but provides a very similar programming interface. The lazy implementation of release consistency in TreadMarks further reduces the number of messages and the amount of data compared to earlier, eager implementations <ref> [8] </ref>. False sharing is another source of frequent communication in DSM systems. TreadMarks uses multiple-writer protocols to address this problem. Multiple-writer protocols require the creation of diffs, data structures that record updates to parts of a page. <p> While this strategy masks latency, LRC sends far fewer messages, an important consideration in a software implementation on a general-purpose network because of the high per message cost. In an eager software implementation of RC <ref> [8] </ref>, a processor propagates its modifications of shared data when it executes a release. <p> This problem occurs in snoopy-cache multiprocessors as well, but it is more prevalent in software DSM because the consistency protocol operates on pages rather than smaller cache blocks. To address this problem, Munin introduced a multiple-writer protocol <ref> [8] </ref>. With multiple-writer protocols two or more processors can simultaneously modify their local copy of a shared page. <p> Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency. <p> Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems [3, 8, 11, 12, 18, 21] difficult. We have however borrowed from Munin <ref> [8] </ref> the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency. Bryant et al. [7] implemented SSVM (Structured Shared Virtual Memory) on a star network of IBM RS-6000s running Mach 2.5.
Reference: [9] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A write notice is an indication that a page has been modified in a particular interval, but it does not contain the actual modifications. The timing of the actual data movement depends on whether an invalidate, an update, or a hybrid protocol is used (see <ref> [9] </ref>). TreadMarks currently uses an invalidate protocol: the arrival of a write notice for a page causes the processor to invalidate its copy of that page. A subsequent access to that page causes an access miss, at which time the modifications are propagated to the local copy. <p> Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs <ref> [9] </ref>, and makes comparisons with older systems [3, 8, 11, 12, 18, 21] difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [10] <author> S. Dwarkadas, A. A. Schaffer, R. W. Cottingham Jr., A. L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <note> To appear in Journal of Human Heredity, </note> <year> 1993. </year>
Reference-contexts: TSP uses a branch-and-bound algorithm to solve the traveling salesman problem for a 19-city tour. Quicksort sorts an array of 256K integers, using a bubblesort to sort subarrays of less than 1K elements. ILINK, from the LINKAGE package [16], performs genetic linkage analysis (see <ref> [10] </ref> for more details). ILINK's input consists of data on 12 families with autosomal dominant nonsyndromic cleft lip and palate (CLP). 4.4 Results obtained by running the applications without TreadMarks. Figure 4 provides execution statistics for each of the five applications when using 8 processors. <p> As a result, speedup for Quicksort is higher than for Water. ILINK achieves less than linear speedup on TreadMarks because of a load balancing problem inherent to the nature of the algorithm <ref> [10] </ref>.
Reference: [11] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [12] <author> A. Forin, J. Barrera, and R. Sanzi. </author> <title> The shared memory server. </title> <booktitle> In Proceedings of the 1989 Winter Usenix Conference, </booktitle> <pages> pages 229-243, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [13] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In order to address the performance problems with earlier DSM systems, the TreadMarks implementation focuses on reducing the amount of communication necessary to keep the distributed memories consistent. It uses a lazy implementation [14] of release consistency <ref> [13] </ref> and multiple-writer protocols to reduce the impact of false sharing [8]. On the 100-Mbps ATM LAN, good speedups were achieved for Jacobi, TSP, Quicksort, and ILINK (a program from the genetic LINKAGE package [16]). <p> We discuss related work in Section 6, and conclude in Section 7. 2 Design TreadMarks' design focuses on reducing the amount of communication necessary to maintain memory consistency. To this end, it presents a release consistent memory model <ref> [13] </ref> to the user. Release consistency requires less communication than conventional, sequentially consistent [15] shared memory, but provides a very similar programming interface. The lazy implementation of release consistency in TreadMarks further reduces the number of messages and the amount of data compared to earlier, eager implementations [8]. <p> Multiple-writer protocols require the creation of diffs, data structures that record updates to parts of a page. With lazy release consistency, diff creation can often be postponed or avoided, a technique we refer to as lazy diff creation. 2.1 Release Consistency Release consistency (RC) <ref> [13] </ref> is a relaxed memory consistency model that permits a processor to delay making its changes to shared data visible to other processors until certain synchronization accesses occur. <p> Programs written for SC memory produce the same results on an RC memory, provided that (i) all synchronization operations use system-supplied primitives, and (ii) there is a release-acquire pair between conflicting ordinary accesses to the same memory location on different processors <ref> [13] </ref>. In practice, most shared memory programs require little or no modifications to meet these requirements. Although execution on an RC memory produces the same results as on a SC memory for the overwhelming majority of the programs, RC can be implemented more efficiently than SC. <p> For ILINK, performance is comparable under both schemes. For TSP, ERC results in better performance than LRC. TSP is implemented using a branch-and-bound algorithm that uses a current minimum to prune searching. The performance on LRC suffers from the fact that TSP is not a properly labeled <ref> [13] </ref> program. Although updates to the current minimum tour length are synchronized, read accesses are not. Since LRC updates cached values only on an acquire, a processor may read an old value of the current minimum. <p> Adding synchronization around the read accesses would deteriorate performance, given the very large number of such accesses. 6 Related Work Among the many proposed relaxed memory consistency models, we have chosen release consistency <ref> [13] </ref>, because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC) [4]. EC differs from RC in that it requires all shared data to be explicitly associated with some synchronization variable.
Reference: [14] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In order to address the performance problems with earlier DSM systems, the TreadMarks implementation focuses on reducing the amount of communication necessary to keep the distributed memories consistent. It uses a lazy implementation <ref> [14] </ref> of release consistency [13] and multiple-writer protocols to reduce the impact of false sharing [8]. On the 100-Mbps ATM LAN, good speedups were achieved for Jacobi, TSP, Quicksort, and ILINK (a program from the genetic LINKAGE package [16]). <p> No such requirement exists under RC. The propagation of the modifications can be postponed until the next synchronization operation takes effect. 2.2 Lazy Release Consistency In lazy release consistency (LRC) <ref> [14] </ref>, the propagation of modifications is postponed until the time of the acquire. At this time, the acquiring processor determines which modifications it needs to see according to the definition of RC. To do so, LRC divides the execution of each process into intervals, each denoted by an interval index. <p> At the time of a release, ERC creates diffs of modified pages, and distributes each diff to all processors that cache the corresponding page. Our implementation of ERC uses an update protocol. Eager invalidate protocols have been shown to result in inferior performance for DSM systems <ref> [14] </ref>. We are thus comparing LRC against the best protocol available for ERC. With an eager invalidate protocol, the diffs cause a large number of invalidations, which trigger a large number of access misses.
Reference: [15] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: To this end, it presents a release consistent memory model [13] to the user. Release consistency requires less communication than conventional, sequentially consistent <ref> [15] </ref> shared memory, but provides a very similar programming interface. The lazy implementation of release consistency in TreadMarks further reduces the number of messages and the amount of data compared to earlier, eager implementations [8]. False sharing is another source of frequent communication in DSM systems. <p> Essentially, RC requires ordinary shared memory updates by a processor p to become visible at another processor q, only when a subsequent release by p becomes visible at q. In contrast, in sequentially consistent (SC) memory <ref> [15] </ref>, the conventional model implemented by most snoopy-cache, bus-based multiprocessors, modifications to shared memory must become visible to other processors immediately [15]. <p> In contrast, in sequentially consistent (SC) memory <ref> [15] </ref>, the conventional model implemented by most snoopy-cache, bus-based multiprocessors, modifications to shared memory must become visible to other processors immediately [15]. Programs written for SC memory produce the same results on an RC memory, provided that (i) all synchronization operations use system-supplied primitives, and (ii) there is a release-acquire pair between conflicting ordinary accesses to the same memory location on different processors [13].
Reference: [16] <author> G.M. Lathrop, J.M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for multilocus linkage analysis in humans. </title> <booktitle> Proceedings of National Academy of Science, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: It uses a lazy implementation [14] of release consistency [13] and multiple-writer protocols to reduce the impact of false sharing [8]. On the 100-Mbps ATM LAN, good speedups were achieved for Jacobi, TSP, Quicksort, and ILINK (a program from the genetic LINKAGE package <ref> [16] </ref>). TreadMarks achieved only a moderate speedup for a slightly modified version of the Water program from the SPLASH benchmark suite [22], because of the high synchronization and communication rates. We present a detailed decomposition of the overheads. <p> TSP uses a branch-and-bound algorithm to solve the traveling salesman problem for a 19-city tour. Quicksort sorts an array of 256K integers, using a bubblesort to sort subarrays of less than 1K elements. ILINK, from the LINKAGE package <ref> [16] </ref>, performs genetic linkage analysis (see [10] for more details). ILINK's input consists of data on 12 families with autosomal dominant nonsyndromic cleft lip and palate (CLP). 4.4 Results obtained by running the applications without TreadMarks.
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A subsequent access to that page causes an access miss, at which time the modifications are propagated to the local copy. Alternative implementations of RC generally cause more communication than LRC. For example, the DASH shared-memory multiprocessor <ref> [17] </ref> implements RC in hardware, buffering writes to avoid blocking the processor until the write has been performed with respect to main memory and remote caches.
Reference: [18] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces [2], distributed shared memory <ref> [18] </ref>, and message passing [23]. TreadMarks is a distributed shared memory (DSM) system [18]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 1). <p> Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces [2], distributed shared memory <ref> [18] </ref>, and message passing [23]. TreadMarks is a distributed shared memory (DSM) system [18]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 1). This approach is attractive since most programmers find it easier to use than a message passing paradigm, which requires them to explicitly partition data and manage communication. <p> Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [19] <author> K. Li and R. Schaefer. </author> <title> A hypercube shared virtual memory system. </title> <booktitle> 1989 International Conference on Parallel Processing, </booktitle> <volume> 1 </volume> <pages> 125-131, </pages> <year> 1989. </year>
Reference-contexts: A remote page fault takes 1.75 milliseconds when using IMCS, and is estimated to take 3.25 milliseconds when using TCP/IP. The breakdown of the 1.75 milliseconds page fault time is: 1.05 milliseconds for DSVM6K overhead, 0.47 milliseconds for IMCS overhead and 0.23 milliseconds of wire time. Shiva <ref> [19] </ref> is an implementation of sequentially consistent DSM on an Intel IPSC/2. Shiva is implemented outside the kernel. A remote page fault takes 3.82 milliseconds, and the authors estimate that time could be reduced by 23 percent by a kernel implementation.
Reference: [20] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: With a global address space, the programmer can focus on algorithmic development rather than on managing partitioned data sets and communicating values. Many DSM implementations have been reported in the literature (see <ref> [20] </ref> for an overview). Unfortunately, none of these implementations are widely available. Many run on in-house research platforms, rather than on generally available operating systems, or require kernel modifications that make them unappealing. Early DSM systems also suffered from performance problems.
Reference: [21] <author> U. Ramachandran and M.Y.A. Khalidi. </author> <title> An implementation of distributed shared memory. </title> <journal> Software: Practice and Experience, </journal> <volume> 21(5) </volume> <pages> 443-464, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Typically, RC does not require additional synchronization. Eager Speedups In terms of comparisons with other systems, we restrict ourselves to implementations on comparable processor and networking technology. Differences in processor and network speed and their ratio lead to different tradeoffs [9], and makes comparisons with older systems <ref> [3, 8, 11, 12, 18, 21] </ref> difficult. We have however borrowed from Munin [8] the concept of multiple-writer protocols. Munin also implements eager release consistency, which moves more messages and data than lazy release consistency.
Reference: [22] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: On the 100-Mbps ATM LAN, good speedups were achieved for Jacobi, TSP, Quicksort, and ILINK (a program from the genetic LINKAGE package [16]). TreadMarks achieved only a moderate speedup for a slightly modified version of the Water program from the SPLASH benchmark suite <ref> [22] </ref>, because of the high synchronization and communication rates. We present a detailed decomposition of the overheads. For the applications measured, the software communication overhead is the bottleneck in achieving high performance for finer grained applications like Water. <p> The minimum time to perform an 8 processor barrier is 2186 seconds. A remote page fault, to obtain a 4096 byte page from another processor takes 2792 seconds. 4.3 Applications We used five programs in this study: Water, Jacobi, TSP, Quicksort, and ILINK. Water, obtained from SPLASH <ref> [22] </ref>, is a molecular dynamics simulation. We made one simple modification to the original program to reduce the number of lock accesses. We simulated 343 molecules for 5 steps. Jacobi implements a form of Successive Over-Relaxation (SOR) with a grid of 2000 by 1000 elements.
Reference: [23] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency:Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Various software systems have been proposed and built to support parallel computation on workstation networks, e.g., tuple spaces [2], distributed shared memory [18], and message passing <ref> [23] </ref>. TreadMarks is a distributed shared memory (DSM) system [18]. DSM enables processes on different machines to share memory, even though the machines physically do not share memory (see Figure 1).
Reference: [24] <author> M. Young, A. Tevanian, R. Rashid, D. Golub, J. Eppinger, J. Chew, W. Bolosky, D. Black, and R. Baron. </author> <title> The duality of memory and communication in the implementation of a multiprocessor operating system. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 63-76, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Munin also implements eager release consistency, which moves more messages and data than lazy release consistency. Bryant et al. [7] implemented SSVM (Structured Shared Virtual Memory) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface <ref> [24] </ref>, and one using the Mach exception interface [5]. They report that the latter implementation|which is very similar to ours|is more efficient, because of the inability of Mach's external pager interface to asynchronously update a page in the user's address space.
References-found: 24

