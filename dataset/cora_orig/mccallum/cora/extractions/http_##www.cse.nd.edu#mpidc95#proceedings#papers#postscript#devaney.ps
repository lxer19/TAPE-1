URL: http://www.cse.nd.edu/mpidc95/proceedings/papers/postscript/devaney.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jdevaney@nist.gov  
Title: Experience with MPI: 'Converting pvmmake to mpimake under LAM' and 'MPI and Parallel Genetic Programming'  
Author: Judith Ellen Devaney NIST 
Date: June 1995  
Abstract: This looks at the issues which arose in porting the pvmmake utility from PVM to MPI. Pvmmake is a PVM application which allows a user to send files, execute commands, and receive results from a single machine on any machine in the virtual machine. Its actions are controlled by the contents of a configuration file. Its most common use is to enable management of the development of a parallel program in a heterogeneous environment. A utility with the same features, mpimake, was coded up to run under LAM. Genetic programming is an algorithm which evolves an algorithm in the form of a program to solve your input problem. The implementation under MPI requires the transfer of dynamic data structures such as lists and trees. This paper discusses the match between the requirements of this algorithm and the datatype feature in MPI. A new library, MPI DataStruct is being developed which can transfer dynamic data structures, created with pointers, without intervention by the user. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mpi: </author> <title> A message passing interface standard. HTML document, </title> <year> 1994. </year>
Reference-contexts: 1 Introduction Parallel computing has been dominated by applications whose core work has been some numerical technique. This has influenced much of what has been developed to date. It is the purpose of this paper to examine how well the functionality of MPI <ref> [1] </ref> [7] matches the needs of two non-traditional applications. The first application is a parallel make utility [5] designed to run under LAM [2].
Reference: [2] <author> D. Burns and R. B. Daoud. Lam: </author> <title> An open cluster environment for mpi. </title> <booktitle> In Supercomputing Symposium '94, </booktitle> <month> June </month> <year> 1994. </year> <institution> Toronto, Canada. </institution>
Reference-contexts: This has influenced much of what has been developed to date. It is the purpose of this paper to examine how well the functionality of MPI [1] [7] matches the needs of two non-traditional applications. The first application is a parallel make utility [5] designed to run under LAM <ref> [2] </ref>. The second is a parallelization of the Genetic Programming Algorithm [8] [9] for evolving a model of a protein domain [4]. 2 Converting pvmmake to mpimake Maintaining program coherence is always a critical issue in any parallel program.
Reference: [3] <author> R. B. </author> <title> Daoud. </title> <type> Personal Communication. </type>
Reference-contexts: So the string datatype is appropriate. The whole file can be broadcast with a single call this way. mcast () is the PVM utility which broadcasts to a subset. In PVM, it is actually implemented by a loop of sends <ref> [3] </ref>. It was the method used to broadcast files. In MPI, there is no string datatype. However a character buffer can be allocated. So this is not a limitation. There is no limit to the size of the messages which can be broadcast under LAM [3] also. <p> by a loop of sends <ref> [3] </ref>. It was the method used to broadcast files. In MPI, there is no string datatype. However a character buffer can be allocated. So this is not a limitation. There is no limit to the size of the messages which can be broadcast under LAM [3] also. So the exact functionality is there. There is no equivalent to mcast () in MPI. So this was implemented as a loop of sends.
Reference: [4] <author> J. E. Devaney. </author> <title> Evolving a model of a protein domain using genetic programming. </title> <type> unpublished, </type> <year> 1994. </year>
Reference-contexts: The first application is a parallel make utility [5] designed to run under LAM [2]. The second is a parallelization of the Genetic Programming Algorithm [8] [9] for evolving a model of a protein domain <ref> [4] </ref>. 2 Converting pvmmake to mpimake Maintaining program coherence is always a critical issue in any parallel program. It is even more so when the program is being developed in a multi-platform, heterogeneous environment. <p> Then the next iteration is begun. This continues until the fitness function indicates that the goal has been achieved or until an iteration limit has been reached. This algorithm is usually implemented in lisp but it can be implemented in the C language <ref> [4] </ref>. A way to do this is to store the programs in trees. The inner nodes are functions and the leaves are data. The nodes of the trees need a slot to indicate whether they are a function or data. <p> Execution of the program is then accomplished by a depth first traversal of the tree. Genetic programming is well suited to parallel processing. Most of the time is spent running the population programs. See figure 5 for some timings of individual iterations of a recent implementation <ref> [4] </ref>. In addition a recent study [10] has found that small amounts of immigration and emigration of programs, every so often can produce linear speed-up. The main issue in this is whether MPI can easily transfer trees which have been created dynamically. Anything can always be hacked.
Reference: [5] <author> J. E. Devaney and M. Edwards. Pvmmake: </author> <title> A utility for remote transfer and compilation of pvm programs using pvm. 1994. </title> <institution> Oak Ridge Tennessee, </institution> <month> May 19-20. </month>
Reference-contexts: This has influenced much of what has been developed to date. It is the purpose of this paper to examine how well the functionality of MPI [1] [7] matches the needs of two non-traditional applications. The first application is a parallel make utility <ref> [5] </ref> designed to run under LAM [2]. The second is a parallelization of the Genetic Programming Algorithm [8] [9] for evolving a model of a protein domain [4]. 2 Converting pvmmake to mpimake Maintaining program coherence is always a critical issue in any parallel program. <p> See figure 2. The compilation commands are generated on the target machines using the popen feature. See figure 3. And it must be possible to get error messages regarding compilation on various machines back to the developer in some orderly form. See figure 4. Pvmmake <ref> [5] </ref> was developed to make all this as trivial an operation as possible. The input to this program is a configuration file. This file contains two types of commands. One type specifies what files are to be broadcast to what machines.
Reference: [6] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sun-deram. </author> <title> PVM 3 USER'S GUIDE AND REFERENCE MANUAL. </title> <institution> Oak Ridge, Tennessee, </institution> <year> 1994. </year>
Reference-contexts: The user can't broadcast a file or command to a machine which is not part of the virtual machine. It is also needed for the program to figure out what node corresponds to what machine. In PVM <ref> [6] </ref>, there is a configuration command. This does not exist in MPI. However, it is easy to construct especially under LAM. Lmake.c is the host. It broadcasts it's rank to the 'Ltargets'. They each get their rank and machine name and send them to the host (Lmake.c).
Reference: [7] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Parallel computing has been dominated by applications whose core work has been some numerical technique. This has influenced much of what has been developed to date. It is the purpose of this paper to examine how well the functionality of MPI [1] <ref> [7] </ref> matches the needs of two non-traditional applications. The first application is a parallel make utility [5] designed to run under LAM [2].
Reference: [8] <editor> J. R. Koza. </editor> <booktitle> Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year> <month> 7 </month>
Reference-contexts: It is the purpose of this paper to examine how well the functionality of MPI [1] [7] matches the needs of two non-traditional applications. The first application is a parallel make utility [5] designed to run under LAM [2]. The second is a parallelization of the Genetic Programming Algorithm <ref> [8] </ref> [9] for evolving a model of a protein domain [4]. 2 Converting pvmmake to mpimake Maintaining program coherence is always a critical issue in any parallel program. It is even more so when the program is being developed in a multi-platform, heterogeneous environment. <p> After this, the message is sent back to the host. After that, everything is simple. MPI Get count () is used to determine the size of the incoming message. Then the space is allocated for the message and it is received. 3 Parallel Genetic Programming The genetic programming algorithm <ref> [8] </ref> is a technique to evolve a computer program to solve a given problem. The algorithm starts off with a population of randomly generated computer programs. Data is run through each of these programs and the programs are ranked by means of a fitness function.
Reference: [9] <editor> J. R. Koza. </editor> <booktitle> Genetic Programming II. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: The first application is a parallel make utility [5] designed to run under LAM [2]. The second is a parallelization of the Genetic Programming Algorithm [8] <ref> [9] </ref> for evolving a model of a protein domain [4]. 2 Converting pvmmake to mpimake Maintaining program coherence is always a critical issue in any parallel program. It is even more so when the program is being developed in a multi-platform, heterogeneous environment.
Reference: [10] <author> J. R. Koza and D. Andre. </author> <title> Parallel genetic programming on a network of transputers. </title> <type> Technical report, </type> <institution> Computer Science Department, Stanford University, Stanford, California, </institution> <month> January </month> <year> 1995. </year> <type> 8 9 10 11 12 13 </type>
Reference-contexts: Genetic programming is well suited to parallel processing. Most of the time is spent running the population programs. See figure 5 for some timings of individual iterations of a recent implementation [4]. In addition a recent study <ref> [10] </ref> has found that small amounts of immigration and emigration of programs, every so often can produce linear speed-up. The main issue in this is whether MPI can easily transfer trees which have been created dynamically. Anything can always be hacked.
References-found: 10

