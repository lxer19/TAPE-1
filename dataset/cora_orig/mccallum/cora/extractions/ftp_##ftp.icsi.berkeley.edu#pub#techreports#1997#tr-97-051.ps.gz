URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-051.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Note: Contents  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Jont B. Allen. </author> <title> How do humans process and recognize speech? IEEE Transactions on Speech and Audio Processing, </title> <booktitle> 2(4) </booktitle> <pages> 567-577, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: not a solved problem, and with the recent debates on the incremental nature of advancement in the field [6], new and exploratory paradigms are most welcome. 1.2 Our Goal Recently there has been much excitement generated in the ASR community on the topic of multi-band paradigm, mainly by Jont Allen's <ref> [1] </ref> cogent retelling Har-vey Fletcher's [19] psycho-acoustic studies of the 1950's. Multi-band ASR method [18, 42, 7, 25] is a special case of the multi-stream paradigm. <p> option of both developing feature extraction methods and employing variable sized temporal windows tuned to the dynamic characteristics of each frequency region. * Some evidence suggests that human speech perception is based on narrow frequency channel analysis, and that the recombination of these features is performed at higher processing levels <ref> [19, 1, 41] </ref>. We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized [8, 67, 68] that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> They showed significantly better classification results with the multi-band system. 10.2 Psycho-acoustic Studies The research into multi-band approaches is first and foremost motivated by the work of Harvey Fletcher, as summarized and re-reported by Jont Allen <ref> [19, 1] </ref>. The underlying hypothesis of their work is that human speech perception is based on narrow frequency channels. Fletcher performed human listening experiments using nonsense CVC (consonant-vowel-consonant) sets, and based on the data, he proposed a model for human speech processing with five layers.
Reference: [2] <author> L. E. Baum. </author> <title> An inequality and associated maximization techniques in statistical estimation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: A bigram grammar is specified by a list of words that can follow a particular word, along with associated probabilities. 2 lexicon 2 are used in a dynamic programming-based Viterbi search [70], a simplified version of the Forward algorithm <ref> [2, 3] </ref>, to find the best strings of words corresponding to the acoustic data. 1.4 What is Multi-band Processing? Figures 2 and 3 highlight the basic elements of a system based on the multi-band paradigm.
Reference: [3] <author> R. Baum and J. A. Eagon. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model of ecology. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 73 </volume> <pages> 360-363, </pages> <year> 1967. </year>
Reference-contexts: A bigram grammar is specified by a list of words that can follow a particular word, along with associated probabilities. 2 lexicon 2 are used in a dynamic programming-based Viterbi search [70], a simplified version of the Forward algorithm <ref> [2, 3] </ref>, to find the best strings of words corresponding to the acoustic data. 1.4 What is Multi-band Processing? Figures 2 and 3 highlight the basic elements of a system based on the multi-band paradigm.
Reference: [4] <author> Jeff Bilmes. </author> <title> Maximum mutual information based reduction strategies for cross-correlation based joint distributional modeling. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1998. </year> <note> To Appear. </note>
Reference-contexts: The formants of male speakers for example, are lower in frequency range than that of the female speakers. We could use this method to perform vocal tract normalization. 5 the voiced stops before various vowels. (after Delattre, Liberman, and Cooper 1955) * Bilmes' <ref> [4] </ref> calculation of the time-frequency information density 3 for conversational speech (see Figure 5) shows that most of the information is within a sub-band, suggesting that extracting information in a narrow band region is justified in terms of the information content. section of Switchboard, in bits per unit area (From Bilmes
Reference: [5] <author> M. Blomberg. </author> <title> Modelling articulatory inter-timing variation in a speech recognition system based on synthetic references. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 789-792, </pages> <address> Genova, Italy, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: In Section 4.3 we focus our attention on the following: some multi-band researchers <ref> [68, 8, 67, 43, 5] </ref> have postulated that transitions in sub-bands occur asynchronously, and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit.
Reference: [6] <author> Herve Bourlard. </author> <title> Towards increasing speech recognition error rates. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <pages> pages 883-894, </pages> <address> Madrid, Spain, </address> <year> 1995. </year> <note> Keynote Paper. </note>
Reference-contexts: Clearly, ASR is not a solved problem, and with the recent debates on the incremental nature of advancement in the field <ref> [6] </ref>, new and exploratory paradigms are most welcome. 1.2 Our Goal Recently there has been much excitement generated in the ASR community on the topic of multi-band paradigm, mainly by Jont Allen's [1] cogent retelling Har-vey Fletcher's [19] psycho-acoustic studies of the 1950's.
Reference: [7] <author> Herve Bourlard and Stephane Dupont. </author> <title> A new ASR approach based on independent processing and recombination of partial frequency bands. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, PA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Multi-band ASR method <ref> [18, 42, 7, 25] </ref> is a special case of the multi-stream paradigm. The goal of the multi-stream model is the incorporation of different information streams [9], as the streams may be audio and visual information, or different sets of features derived from speech 1 data. <p> Some motivations for the multi-band paradigm are signal processing advantages, psycho-acoustic studies, robustness to noise, and taking advantage of parallel processing architectures, among others. In some experiments multi-band has had a lower word error rate in both normal and band-limited noise conditions <ref> [7, 25] </ref> compared to a traditional full-band system. Our goal in this work is to design and implement a multi-band system, analyze and explore the advantages and disadvantages of this paradigm, and extend the state-of-the-art by implementing extensions to multi-band, more specifically, explore multi-band ASR with merged sub-band classes. <p> If full-band features are extracted from the noisy speech, the full vector would be corrupted, whereas, in the multi-band processing case, only the features pertaining to the noisy channel would be corrupted. It was shown <ref> [7, 25] </ref>, that the multi-band paradigm is more robust to such noise, and shows more graceful degradation. * It has been suggested [21] that different frequency regions have different dynamic characteristics. <p> Bourlard and Dupont <ref> [7] </ref> experimented with three, four, and six sub-bands, and observed that four (or perhaps five) was the optimal number of bands for their experiments. They used [17-778Hz], [707-1631Hz], [1506-2709Hz], and [2121-3769Hz] for their four band experiment, grouping critical bands [1-6], [7-10], [11-13], and [13-15]. <p> Determining the optimal number of bands is not the focus of the work reported here. Based on previous experiments and psycho-acoustic observations, we choose four bands, with cutoffs based on the RASTA-PLP filter cutoffs (see Table 1), similar to that of <ref> [7] </ref>, except that we discard the first two critical bands because telephone speech is band-passed above roughly 300Hz. Our experiments show that the first two bands do not help word recognition. <p> 541.89 628.53 7 707.14 837.63 948.84 9 1050.92 1222.34 1369.93 11 1506.32 1736.88 1936.52 13 2121.72 2435.90 2708.80 15 2962.48 3393.65 3768.80 Table 1: The half-power low and high frequency cutoffs for the RASTA-PLP filters when the sampling frequency is 8kHz. on band-limited critical band values, as also observed by <ref> [7, 67] </ref>. In this work, we use RASTA-PLP processing [24] in each sub-band. <p> Bourlard and Dupont <ref> [7] </ref> 10 have run preliminary experiments with state, phone, and syllable combination levels and their results were inconclusive. Cooke et. al. [14] have claimed that asynchrony is helpful, though open questions regarding the generalizability of their experiments remain. <p> Traditionally, non-linear methods have had higher accuracy compared to linear combination strategies. Non-linear merging (using an MLP) produce lower word error rates than linear merging (e.g., multiplying the probabilities) <ref> [7, 67] </ref> when multi-band information is merged on the frame level. We explore different ways of combining the probabilities (e.g., adding or multiplying the probabilities, adding their logs, etc.). Another question is whether the merging should be done in a posterior or likelihood domain. <p> The former method (i.e., using limited narrow band information for discriminating between all classes for each sub-band) has been used in multi-band 11 approaches <ref> [42, 7, 25, 18] </ref>, and the latter (using all available features for distinguishing between all classes) is the norm in the traditional ASR systems. In order to determine the merged phonetic classes for each band, we performed band-limited phone recognition and created confusion matrices. <p> More recently, work by us [42] and our collaborators Bourlard and Dupont <ref> [7, 8] </ref> and Hermansky and Tibrewala [25, 67] has focused on multi-band for continuous speech recognition. Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. <p> This is of note as it shows the inherent noise robustness of multi-band approach (as also observed by <ref> [7] </ref>). One does not need to running 127 (or a similar number of MLP merging units) to reap the noise robustness benefits, as for a large task the needed computational power would make this approach infeasible. <p> The error rate of the HTK system with the same amount of training data was similarly around 60%. 43 system. In their experiments Bourlard and Dupont <ref> [8, 7] </ref> tested critical band energy features, LPC-cepstra computed on band-limited critical band values for clean and narrow-band noise, and J-RASTA-PLP features [24] for wide-band noise. They also experimented with the choice of three, four, or six sub-bands.
Reference: [8] <author> Herve Bourlard and Stephane Dupont. </author> <title> Subband-based speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 125-128, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized <ref> [8, 67, 68] </ref> that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> In Section 4.3 we focus our attention on the following: some multi-band researchers <ref> [68, 8, 67, 43, 5] </ref> have postulated that transitions in sub-bands occur asynchronously, and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit. <p> More recently, work by us [42] and our collaborators Bourlard and Dupont <ref> [7, 8] </ref> and Hermansky and Tibrewala [25, 67] has focused on multi-band for continuous speech recognition. Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. <p> The error rate of the HTK system with the same amount of training data was similarly around 60%. 43 system. In their experiments Bourlard and Dupont <ref> [8, 7] </ref> tested critical band energy features, LPC-cepstra computed on band-limited critical band values for clean and narrow-band noise, and J-RASTA-PLP features [24] for wide-band noise. They also experimented with the choice of three, four, or six sub-bands. <p> However, if the neighboring spectral regions are removed, or if the removal is according to local levels of SNR, the performance deteriorates gravely. Multi-band appears to be more robust in these situations <ref> [8, 66] </ref>. 11 Appendix B Confusion Matrices Figures 18, 19, 20, 21, 22, and 23 show confusion matrices generated for sub-bands 1 through 4, the full-band, and multi-band, respectively. The data has been generated on NUMBERS95 development set.
Reference: [9] <author> Herve Bourlard, Stephane Dupont, and Christophe Ris. </author> <title> Multi-stream speech recognition. </title> <type> Technical Report IDIAP-RR 96-07, </type> <institution> IDIAP, Martigny, Valais, Switzerland, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: Multi-band ASR method [18, 42, 7, 25] is a special case of the multi-stream paradigm. The goal of the multi-stream model is the incorporation of different information streams <ref> [9] </ref>, as the streams may be audio and visual information, or different sets of features derived from speech 1 data. <p> Merging in the posterior and likelihood domains has been compared mathematically, not practically, in <ref> [9] </ref>. In this work, we will experiment with both methods. 3.6 Merged Phone Classes Part of this work focuses on defining merged phonetic classes for each sub-band. The main hypothesis is that some frequency bands contain more information for distinguishing particular acoustic classes.
Reference: [10] <author> Herve Bourlard and Nelson Morgan. </author> <title> Connectionist Speech Recognition A Hybrid Approach. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1994. </year>
Reference-contexts: Finally, the Appendices include a detailed summary of previous work, as well as confusion matrices and merged sub-band classes. 1.3 ICSI's HMM/ANN Hybrid Speech Recognition System This work is performed within the established framework of ICSI's hybrid hidden Markov model/artificial neural network (HMM/ANN) speech recognition system <ref> [10] </ref>. The main components of our speech recognizer are highlighted in Figure 1. <p> Without such evidence, we could not justify consideration of longer-term merging units for multi-band ASR. In Section 4.3, we examine this assumption by analyzing the transition lags in each sub-band to see if sub-band transitions occur asynchronously. 4.1 System Description First, a few words on our HMM/MLP based <ref> [10] </ref> full-band baseline system: We train the MLP phonetic probability estimator on a 9 frame window of 8th-order 13 RASTA-PLP [24], energy, and delta-RASTA-PLP features for every 25 ms window, stepped every 10 ms.
Reference: [11] <author> John Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman 56 Soulie and J. Herault, editors, Neurocomputing: </editor> <booktitle> Algorithms, Architectures, and Applications, </booktitle> <pages> pages 227-236. </pages> <booktitle> NATO ASI Series, </booktitle> <year> 1990. </year>
Reference-contexts: We usually use RASTA-PLP processing [24]. The next element in our system is the phonetic probability estimator, which is a fully connected multi-layer perceptron trained using the backpropagation algorithm [56] with softmax normalization <ref> [11] </ref> on the output layer and relative entropy error criterion [61] to estimate the probability of each phoneme corresponding to (multiple) frames of speech. Next, the phonetic probabilities, along with a grammar 1 and a 1 We often use a bigram grammar. <p> The MLP is fully connected and has 153 inputs (9 frames with 17 features per frame), 1000 hidden units, and 56 outputs (one output for each phone 4 ), and is trained using backpropagation [56], with softmax normalization <ref> [11] </ref>, and relative entropy error criterion [61] at the output layer. The system is trained on hand-transcribed phone labels (without embedded realignment). Using a multiple pronunciation lexicon (derived from hand transcriptions), and a bigram language model, the WERR of this baseline system on the test set is 7.9%.
Reference: [12] <editor> Numbers corpus, release 1.0, </editor> <year> 1995. </year>
Reference-contexts: Based on our observation from the training set, we can prune down the number of transition classes. The remaining research issues and design decisions are similar to that of the phone-based system. 3.8 Choosing a Database NUMBERS <ref> [12] </ref> is a continuous speech database recorded over the telephone and sampled at 8kHz. It has been prepared at Oregon Graduate Institute from census information (phone numbers, birth-dates, zip-codes, etc.), and includes noise, nonspeech sounds, and cut-off speech. A portion of the database is phonetically hand-transcribed.
Reference: [13] <author> Jordan R. Cohen. </author> <title> The summers of our discontent. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <pages> pages 9-10, </pages> <year> 1996. </year> <booktitle> Proceedings Addendum. </booktitle>
Reference-contexts: In a recent DARPA evaluation, for example, a word error rate of 6% was achieved on a speaker-independent unlimited vocabulary read-speech task [62]. Although impressive, the ASR problem is not solved by any means, as the state of the art is nowhere close to human speech recognition capabilities <ref> [37, 52, 13] </ref>. The error rates of automatic speech recognizers are one or two orders of magnitude higher than those of humans for many speech recognition tasks, ranging from a 10-word digit to a 65,000-word spontaneous continuous speech recognition task.
Reference: [14] <author> Martin Cooke, Andrew Morris, and Phil Green. </author> <title> Missing data techniques for robust speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 863-866, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Bourlard and Dupont [7] 10 have run preliminary experiments with state, phone, and syllable combination levels and their results were inconclusive. Cooke et. al. <ref> [14] </ref> have claimed that asynchrony is helpful, though open questions regarding the generalizability of their experiments remain. Merging the sub-band information on the state level is fairly simple, and it involves estimating the overall phonetic probabilities for each frame given the posterior probability estimates of all narrow-band estimators. <p> The results, summarized in Figure 17, further re-confirm the ability of humans for narrow-band speech recognition, and suggests the redundancy of information in speech spectrum. 10.3 Work on ASR with Missing Features The work of <ref> [14] </ref> and [39] show that speech recognition with incomplete features may be done using missing feature theory. The main idea is to reconstruct the missing 47 hi-pass filters with 0 SNR (from [French and Steinberg 1947]). pieces of information using means and variances of the existing data. [14] has shown that <p> The work of <ref> [14] </ref> and [39] show that speech recognition with incomplete features may be done using missing feature theory. The main idea is to reconstruct the missing 47 hi-pass filters with 0 SNR (from [French and Steinberg 1947]). pieces of information using means and variances of the existing data. [14] has shown that up to 80% of the spectro-temporal regions may be randomly removed without much degradation of the recognition. However, if the neighboring spectral regions are removed, or if the removal is according to local levels of SNR, the performance deteriorates gravely.
Reference: [15] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley-Interscience, </publisher> <year> 1991. </year>
Reference-contexts: In order to determine the merged phonetic classes for each band, we performed band-limited phone recognition and created confusion matrices. We merged the highly-confusable categories in each band according to winner-takes-all, mutual information <ref> [15] </ref>, and a simple greedy estimator. The details are discussed in Section 7. 3.7 Using Transition-based Classes One of the most immediate extensions of the multi-band paradigm is the use of transition based phone classes, similar to those of Stochastic Perceptual Auditory-event Models (SPAM) [47]. <p> In Table 3, for example, 93 instances of /s/ are perceived as /eh/. We used frame level phonetic classification 6 on the test set for generating phone CMs. We merged the highly-confusable categories in each band according to winner-takes-all, mutual information <ref> [15] </ref>, and a simple greedy estimator. Our hope is that the pattern of class merges will be different in each band so that we will be able to distinguish the identity of the phone class when the sub-band information is merged.
Reference: [16] <author> K. H. Davis, R. Biddulph, and S. Balashek. </author> <title> Automatic recognition of spoken digits. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 24(6), </volume> <month> November </month> <year> 1952. </year>
Reference-contexts: Arguably, the first simplified speech recognition system was Radio Rex, a celluloid toy dog which would bounce up when its name was called, built in the 1920's [51]. The first "real" speech recognizer, came along next in 1952 at Bell Labs built by Davis, Biddulph, and Balashek <ref> [16] </ref>. This system was a speaker-dependent digit recognizer which achieved 2% error rate as long as the speaker did not move his head. ASR research has advanced greatly since then, creating more general systems with larger vocabulary and increased robustness to environmental and speaker variation.
Reference: [17] <author> P. C. Delattre, A. M. Liberman, and F. S. Cooper. </author> <title> Acoustic loci and transitional cues for consonants. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 27 </volume> <pages> 769-773, </pages> <year> 1955. </year>
Reference: [18] <author> Paul Duchnowski. </author> <title> A New Structure for Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Multi-band ASR method <ref> [18, 42, 7, 25] </ref> is a special case of the multi-stream paradigm. The goal of the multi-stream model is the incorporation of different information streams [9], as the streams may be audio and visual information, or different sets of features derived from speech 1 data. <p> discuss the ones we choose to address, as well as the ones on which we have made design decisions. 7 calculated on the NUMBERS95 development set. calculated on the NUMBERS95 development set. 8 3.1 Selecting Sub-Bands What is the optimal number of sub-bands and what should the cutoffs be? Duch-nowski <ref> [18] </ref> chose four non-overlapping bands [100-700Hz], [700-1500Hz], [1500-3000Hz], [3000-4500Hz] roughly based on the formant regions. Bourlard and Dupont [7] experimented with three, four, and six sub-bands, and observed that four (or perhaps five) was the optimal number of bands for their experiments. <p> The former method (i.e., using limited narrow band information for discriminating between all classes for each sub-band) has been used in multi-band 11 approaches <ref> [42, 7, 25, 18] </ref>, and the latter (using all available features for distinguishing between all classes) is the norm in the traditional ASR systems. In order to determine the merged phonetic classes for each band, we performed band-limited phone recognition and created confusion matrices. <p> Research Fellowship by the University of California, European Community Basic Research grant (Project Sprach), and the International Computer Science Institute. 40 10 Appendix A Related Work 10.1 Previous Work on Multi-band ASR In this section, we will discuss some related work on multi-band approach: the PhD thesis of Paul Duchnowski <ref> [18] </ref>, our earlier work, and the work of our collaborators, Bourlard and Hermansky and their students. The first work published on the multi-band ASR has been by Paul Duchnowski [18] at MIT as his PhD thesis. <p> on Multi-band ASR In this section, we will discuss some related work on multi-band approach: the PhD thesis of Paul Duchnowski <ref> [18] </ref>, our earlier work, and the work of our collaborators, Bourlard and Hermansky and their students. The first work published on the multi-band ASR has been by Paul Duchnowski [18] at MIT as his PhD thesis. His goal was to apply multi-band processing to the task of phonetic, speaker-independent, phone recognition as cue-ing aid for the deaf. He divided the frequency band into four non-overlapping bands [100-700Hz], [700-1500Hz], [1500-3000Hz], [3000-4500Hz] loosely based on the formant regions.
Reference: [19] <author> Harvey Fletcher. </author> <title> Speech and Hearing in Communication. </title> <publisher> Krieger, </publisher> <address> New York, </address> <year> 1953. </year>
Reference-contexts: with the recent debates on the incremental nature of advancement in the field [6], new and exploratory paradigms are most welcome. 1.2 Our Goal Recently there has been much excitement generated in the ASR community on the topic of multi-band paradigm, mainly by Jont Allen's [1] cogent retelling Har-vey Fletcher's <ref> [19] </ref> psycho-acoustic studies of the 1950's. Multi-band ASR method [18, 42, 7, 25] is a special case of the multi-stream paradigm. <p> option of both developing feature extraction methods and employing variable sized temporal windows tuned to the dynamic characteristics of each frequency region. * Some evidence suggests that human speech perception is based on narrow frequency channel analysis, and that the recombination of these features is performed at higher processing levels <ref> [19, 1, 41] </ref>. We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized [8, 67, 68] that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> They showed significantly better classification results with the multi-band system. 10.2 Psycho-acoustic Studies The research into multi-band approaches is first and foremost motivated by the work of Harvey Fletcher, as summarized and re-reported by Jont Allen <ref> [19, 1] </ref>. The underlying hypothesis of their work is that human speech perception is based on narrow frequency channels. Fletcher performed human listening experiments using nonsense CVC (consonant-vowel-consonant) sets, and based on the data, he proposed a model for human speech processing with five layers.
Reference: [20] <author> N. R. French and J. C. Steinberg. </author> <title> Factors governing the intelligibility of speech sounds. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 19(1) </volume> <pages> 90-119, </pages> <month> January </month> <year> 1947. </year>
Reference-contexts: Clearly, there must be much redundancy in the information content in speech to make recognition of narrow-band speech possible [63]. Finally, French & Steinberg <ref> [20] </ref> have also performed human speech recognition experiments with non-sense CVC's with high-pass and low-pass speech.
Reference: [21] <author> Oded Ghitza. </author> <title> Auditory models and human performance in tasks related to speech coding and speech recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(1) </volume> <pages> 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: It was shown [7, 25], that the multi-band paradigm is more robust to such noise, and shows more graceful degradation. * It has been suggested <ref> [21] </ref> that different frequency regions have different dynamic characteristics. <p> Another interesting psycho-acoustic study that involves the division of the frequency band is Oded Ghitza's tiling experiments <ref> [21] </ref> with the Diagnostic Rhyme Test (DRT). He tested the discrimination between various phonetic qualities, such as sibilation (chair vs. care), voicing (veal vs. feel), and nasality (meat vs. beat).
Reference: [22] <author> Steven Greenberg. </author> <type> Personal Communications, </type> <month> December </month> <year> 1996. </year>
Reference-contexts: By dividing up the space of patterns into smaller units, we hope to increase our ability to find relevant structures in data sub-spaces, and to ultimately, increase our generalization power. * It has been asserted <ref> [22] </ref> that spontaneous conversational speech has more temporal asynchrony than read speech.
Reference: [23] <author> Hynek Hermansky. </author> <title> Perceptual linear predictive (PLP) analysis of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87(4) </volume> <pages> 1738-1752, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Though still in an inchoate stage, spectral sub-band centroids as features have also recently been proposed in [50]. In our previous work [42], we used power spectrum values obtained after PLP critical band filter analysis, cube-root compression, and equal loudness equalization <ref> [23] </ref>. <p> For features we chose a vector of 15 power spectrum values obtained after PLP critical band filter analysis, cube-root compression, and equal loudness equalization <ref> [23] </ref>. To 41 keep the turn-around time of our experiments short, we chose the Bellcore Digits database for testing.
Reference: [24] <author> Hynek Hermansky and Nelson Morgan. </author> <title> RASTA processing of speech. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(4) </volume> <pages> 578-589, </pages> <month> October </month> <year> 1994. </year> <month> 57 </month>
Reference-contexts: The first component is the signal processing element, where each frame of speech (e.g., each 20 msec segment, overlapped every 10 msecs) is processed and relevant speech features (e.g., spectral formants, energy) are derived and non-relevant features (e.g., voice quality parameters) are de-emphasized. We usually use RASTA-PLP processing <ref> [24] </ref>. <p> In this work, we use RASTA-PLP processing <ref> [24] </ref> in each sub-band. Since RASTA-PLP processing emphasizes transitions (and because it has channel robustness characteristics), it is a particularly good candidate for this task. 3.3 DSP Window Size One can imagine using a different feature extraction window size for each band. <p> we examine this assumption by analyzing the transition lags in each sub-band to see if sub-band transitions occur asynchronously. 4.1 System Description First, a few words on our HMM/MLP based [10] full-band baseline system: We train the MLP phonetic probability estimator on a 9 frame window of 8th-order 13 RASTA-PLP <ref> [24] </ref>, energy, and delta-RASTA-PLP features for every 25 ms window, stepped every 10 ms. <p> The error rate of the HTK system with the same amount of training data was similarly around 60%. 43 system. In their experiments Bourlard and Dupont [8, 7] tested critical band energy features, LPC-cepstra computed on band-limited critical band values for clean and narrow-band noise, and J-RASTA-PLP features <ref> [24] </ref> for wide-band noise. They also experimented with the choice of three, four, or six sub-bands. The recombination of the sub-band log likelihoods was performed on either HMM-state, phone, or syllable level.
Reference: [25] <author> Hynek Hermansky, Sangita Tibrewala, and Misha Pavel. </author> <title> Towards ASR on par-tially corrupted speech. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, PA, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Multi-band ASR method <ref> [18, 42, 7, 25] </ref> is a special case of the multi-stream paradigm. The goal of the multi-stream model is the incorporation of different information streams [9], as the streams may be audio and visual information, or different sets of features derived from speech 1 data. <p> Some motivations for the multi-band paradigm are signal processing advantages, psycho-acoustic studies, robustness to noise, and taking advantage of parallel processing architectures, among others. In some experiments multi-band has had a lower word error rate in both normal and band-limited noise conditions <ref> [7, 25] </ref> compared to a traditional full-band system. Our goal in this work is to design and implement a multi-band system, analyze and explore the advantages and disadvantages of this paradigm, and extend the state-of-the-art by implementing extensions to multi-band, more specifically, explore multi-band ASR with merged sub-band classes. <p> If full-band features are extracted from the noisy speech, the full vector would be corrupted, whereas, in the multi-band processing case, only the features pertaining to the noisy channel would be corrupted. It was shown <ref> [7, 25] </ref>, that the multi-band paradigm is more robust to such noise, and shows more graceful degradation. * It has been suggested [21] that different frequency regions have different dynamic characteristics. <p> The former method (i.e., using limited narrow band information for discriminating between all classes for each sub-band) has been used in multi-band 11 approaches <ref> [42, 7, 25, 18] </ref>, and the latter (using all available features for distinguishing between all classes) is the norm in the traditional ASR systems. In order to determine the merged phonetic classes for each band, we performed band-limited phone recognition and created confusion matrices. <p> More recently, work by us [42] and our collaborators Bourlard and Dupont [7, 8] and Hermansky and Tibrewala <ref> [25, 67] </ref> has focused on multi-band for continuous speech recognition. Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. <p> The merging was simply done on the word level by training an MLP on the normalized log likelihoods obtained from the Viterbi decoding distances. The word error rate of the full-band was 4.6% and the two-band system's was 4.3%. The improvement was not statistically significant. corpus. Hermansky and Tibrewala <ref> [25, 65, 67, 66] </ref> tested 2-band and 7-band multi-band systems on clean and noisy speech on the DIGITS database.
Reference: [26] <author> H. G. Hirsch. </author> <title> Estimation of noise spectrum and its applications to SNR estimation and speech enhancement. </title> <type> Technical Report TR-93-012, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <year> 1993. </year>
Reference-contexts: They trained 127 MLPs (that is: 1 + 7 7 ) for every possible configuration of the 7 bands. An merging MLP configuration was chosen based on various techniques, such as SNR thresholding (i.e., leaving out the sub-bands which yielded SNR estimates <ref> [26] </ref> below a certain threshold), majority vote, and adaptation. For noise conditions with local frequency degradations (e.g., additive sinusoid, pink, and babble) the performance of the multi-band system was better than the full-band system.
Reference: [27] <author> Tammo Houtgast and Herman J. M. Steeneken. </author> <title> The modulation transfer function in room acoustics. </title> <journal> Bruel and Kjaer Technical Review, </journal> <volume> 3 </volume> <pages> 3-12, </pages> <year> 1985. </year>
Reference-contexts: The smearing happens in a frequency dependent way, such that typically the high frequencies get smeared less and the low frequencies get smeared more <ref> [27] </ref>. An intuitive explanation is that the high frequencies get absorbed by the air and most wall materials more readily than low frequency energies, whereas low frequency energy reflects to a greater degree and dissipates more gradually.
Reference: [28] <author> Tammo Houtgast and Jan A. Verhave. </author> <title> A physical approach to speech quality assessment: Correlation patterns in the speech spectrogram. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 1, </volume> <pages> pages 285-288. </pages> <booktitle> European Speech Communication Association, Istituto Int. </booktitle> <address> Co-municazioni, </address> <year> 1991. </year>
Reference-contexts: Tibrewala and Hermansky [67] observed that the error rates for a two- and four-band system were lower than that of a seven-band system. One study which may be of interest in the sub-band boundary decision is that of Houtgast and Verhave <ref> [28] </ref>. They performed experiments to determine the amount of information overlap in the speech signal and observed that there is more information overlap for higher frequencies on a linear domain.
Reference: [29] <author> X. D. Huang. </author> <title> Phoneme classification using semicontinuous hidden Markov models. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40(5) </volume> <pages> 1062-1067, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The highest phoneme recognition accuracy achieved on the TIMIT test set was 58.5%, which was within the range of the performance achieved by the established phonetic recognizers <ref> [34, 29, 74] </ref>. Our work on multi-band processing has more differences than similarities with that of Duchnowski.
Reference: [30] <author> Fred Jelinek. </author> <title> Personal Communications, </title> <booktitle> Johns Hopkins Summer Workshop on ASR, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: We chose 300 hidden units and a window of one as the parameters of our merger MLP. As a side-note, it has been suggested by Fred Jelinek <ref> [30] </ref> that training the merger MLP on the same training data which the sub-band MLPs were trained may lead to over-training. As an alternative, we trained a 50 hidden unit merger MLP with a 22 Word Err.
Reference: [31] <author> Brian E. D. Kingsbury and Nelson Morgan. </author> <title> Recognizing reverberant speech with RASTA-PLP. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 2, </volume> <pages> pages 1259-1262, </pages> <address> Munich, Germany, </address> <month> April </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: Variables such as speaking style, speaking rate, accent, variable vocal effort, background noise, room reverberation, and channel effects degrade ASR accuracy dramatically, whereas they affect human speech recognition much less <ref> [44, 35, 31] </ref>.
Reference: [32] <author> Karl D. Kryter. </author> <title> Methods for the calculation and use of the articulation index. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 34(11) </volume> <pages> 1689-1697, </pages> <month> November </month> <year> 1962. </year>
Reference-contexts: There is no statistical model for the error in the full-band being equal to the product of the errors in the narrow bands, as this requires knowledge of the reliability of a band with respect to the others. 2. The work of Kryter and Lippmann <ref> [32, 36] </ref> point out the shortcomings of the AI theory in explaining human speech perception (see discussion of [36] below). 45 Fletcher's multiple-band model is interesting and warrants simulation and study, however, it is not clear that using AI theory for the combination of information from the narrow-band channels is correct.
Reference: [33] <author> Karl D. Kryter. </author> <title> Validation of the articulation index. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 34(11) </volume> <pages> 1698-1702, </pages> <month> November </month> <year> 1962. </year>
Reference-contexts: These results are particularly interesting in that they bring some aspects of the AI theory under question, since one of the most popular methods for calculating AI does not even take speech energy above 6.4 kHz into account <ref> [33] </ref>.
Reference: [34] <author> Kai-Fu Lee and Hsiao-Wuen Hon. </author> <title> Speaker independent phone recognition using hidden Markov models. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37(11) </volume> <pages> 1641-1648, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The highest phoneme recognition accuracy achieved on the TIMIT test set was 58.5%, which was within the range of the performance achieved by the established phonetic recognizers <ref> [34, 29, 74] </ref>. Our work on multi-band processing has more differences than similarities with that of Duchnowski.
Reference: [35] <author> A. M. Liberman and I. G. Mattingly. </author> <title> The motor theory of speech perception revised. </title> <journal> Cognition, </journal> <volume> 21(1) </volume> <pages> 1-36, </pages> <year> 1985. </year>
Reference-contexts: Variables such as speaking style, speaking rate, accent, variable vocal effort, background noise, room reverberation, and channel effects degrade ASR accuracy dramatically, whereas they affect human speech recognition much less <ref> [44, 35, 31] </ref>.
Reference: [36] <author> Richard P. Lippmann. </author> <title> Accurate consonant perception without mid-frequency speech energy. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 4(1) </volume> <pages> 66-69, </pages> <year> 1996. </year>
Reference-contexts: There is no statistical model for the error in the full-band being equal to the product of the errors in the narrow bands, as this requires knowledge of the reliability of a band with respect to the others. 2. The work of Kryter and Lippmann <ref> [32, 36] </ref> point out the shortcomings of the AI theory in explaining human speech perception (see discussion of [36] below). 45 Fletcher's multiple-band model is interesting and warrants simulation and study, however, it is not clear that using AI theory for the combination of information from the narrow-band channels is correct. <p> The work of Kryter and Lippmann [32, 36] point out the shortcomings of the AI theory in explaining human speech perception (see discussion of <ref> [36] </ref> below). 45 Fletcher's multiple-band model is interesting and warrants simulation and study, however, it is not clear that using AI theory for the combination of information from the narrow-band channels is correct. Richard Lippmann [36] reports on human perception experiments using low and high frequencies. <p> out the shortcomings of the AI theory in explaining human speech perception (see discussion of <ref> [36] </ref> below). 45 Fletcher's multiple-band model is interesting and warrants simulation and study, however, it is not clear that using AI theory for the combination of information from the narrow-band channels is correct. Richard Lippmann [36] reports on human perception experiments using low and high frequencies. The common belief has been that high-frequency speech energy above 4 kHz contains inadequate information for speech perception.
Reference: [37] <author> Richard P. Lippmann. </author> <title> Speech perception by humans and machines. </title> <booktitle> In Proceedings of the Workshop on the Auditory Basis of Speech Perception, </booktitle> <pages> pages 309-316, </pages> <address> Keele University, UK, </address> <month> July </month> <year> 1996. </year> <month> 58 </month>
Reference-contexts: In a recent DARPA evaluation, for example, a word error rate of 6% was achieved on a speaker-independent unlimited vocabulary read-speech task [62]. Although impressive, the ASR problem is not solved by any means, as the state of the art is nowhere close to human speech recognition capabilities <ref> [37, 52, 13] </ref>. The error rates of automatic speech recognizers are one or two orders of magnitude higher than those of humans for many speech recognition tasks, ranging from a 10-word digit to a 65,000-word spontaneous continuous speech recognition task.
Reference: [38] <author> Richard P. Lippmann. </author> <title> Speech recognition by machines and humans. </title> <journal> Speech Communication, </journal> <volume> 22(1) </volume> <pages> 1-15, </pages> <year> 1997. </year>
Reference-contexts: For example, band 2 transmits 87% of the frontness features that are transmitted by the full-band system. 4. There is much redundancy in phonetic information content in the sub-bands, as the sum of information transmission over all bands far exceeds 100%. Lippmann <ref> [38] </ref> has highlighted this redundancy as a source of human robustness to speech degradations.
Reference: [39] <author> Richard P. Lippmann and Beth A. Carlson. </author> <title> Robust speech recognition with time-varying filtering, interruptions, and noise. </title> <editor> In Sadaoki Furui, B.-H. Juang, and Wu Chou, editors, </editor> <booktitle> Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 365-372, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: The results, summarized in Figure 17, further re-confirm the ability of humans for narrow-band speech recognition, and suggests the redundancy of information in speech spectrum. 10.3 Work on ASR with Missing Features The work of [14] and <ref> [39] </ref> show that speech recognition with incomplete features may be done using missing feature theory.
Reference: [40] <author> Alvin Martin, Jon Fiscus, Bill Fisher, Dave Pallett, and Mark Przybocki. </author> <title> 1997 LVCSR/HUB-5E workshop: System descriptions & performance summary. </title> <booktitle> In Proceedings of the Conversational Speech Recognition Workshop on DARPA Hub-5E Evaluation, </booktitle> <address> Baltimore, MD, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: ASR systems are particularly poor in recognizing spontaneous continuous speech, as the error rate on the best system for such a task is roughly 35% and around 54% if the conversation parties are familiar with one another <ref> [40] </ref>. Variables such as speaking style, speaking rate, accent, variable vocal effort, background noise, room reverberation, and channel effects degrade ASR accuracy dramatically, whereas they affect human speech recognition much less [44, 35, 31].
Reference: [41] <author> George A. Miller and Patricia E. </author> <title> Nicely. An analysis of perceptual confusions among some English consonants. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 27(2) </volume> <pages> 338-352, </pages> <month> March </month> <year> 1955. </year>
Reference-contexts: option of both developing feature extraction methods and employing variable sized temporal windows tuned to the dynamic characteristics of each frequency region. * Some evidence suggests that human speech perception is based on narrow frequency channel analysis, and that the recombination of these features is performed at higher processing levels <ref> [19, 1, 41] </ref>. We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized [8, 67, 68] that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> In particular, we analyze the phonetic feature transmission pattern in each sub-band, the merged multi-band, and full-band probability streams. As discussed in Section 4.2, we use methods similar to those of Miller & Nicely <ref> [41] </ref> and calculate confusion matrices for phone and feature classes, and use mutual information as a measure of information transmission in a channel. <p> For this analysis we use phone and broad category confusion matrices, as in the seminal studies of Miller and Nicely <ref> [41] </ref> on human speech recognition. A confusion matrix (CM) is simply an extended matrix of hits and misses for all classes, as in Table 3. The column headings represent the features we intend to transmit, and the row headings correspond to the received features. <p> Table 4: An example of binary acoustic features for CV classification. vowel consonant silence vowel 74393 6962 1816 consonant 6738 61030 5055 silence 2321 8922 49281 Table 5: An example of a feature-based confusion matrix. To summarize the confusion matrix, we calculate mutual information (MI) for each CM <ref> [41] </ref> as P p ij p i p j , where i is the feature we would like to transmit, and j is the feature that is perceived. <p> The results are consistent with our knowledge of acoustic phonetics, as, for example, we would expect the low frequency band to contain the most information about voicing. Comparing our results with <ref> [41] </ref>, we observe similar patterns also for fricatives and nasals. 3. Low and sometimes mid frequency bands (often band 1 and sometimes band 2) transmit most of the feature information alone. For example, band 2 transmits 87% of the frontness features that are transmitted by the full-band system. 4. <p> The intersection point may be considered as the frequency threshold above and below which the same amount of information for speech recognition is available. These curves are similar in shape and point of intersection to the ones reported by by Miller and Nicely <ref> [41] </ref> in their psycho-acoustic experiments. We trained a two-band and a full-band system using the spectral features described above. The training set consisted of 1720 utterances, cross validation of 230 utterances, and the final test set had 650 utterances. <p> Ghitza's findings serve as motivations for developing specialized phone-like classes for each sub-band, and for exploring transition-based classes [47], which similar to diphones, emphasize the transition regions. Finally, we wish to briefly discuss the the seminal work of Miller and Nicely <ref> [41] </ref> on low frequency and high frequency masking. They compared the intelligibility of sixteen consonants in a C-/a/ context in various conditions of low-pass and high-pass filtering and with random masking noise as presented to five listeners (800 syllables in each condition).
Reference: [42] <author> Nikki Mirghafori. </author> <title> Automatic speech recognition using multiple frequency bands. </title> <note> Unpublished Draft (http://www.icsi.berkeley.edu/~nikki/papers/Multiband asr 1995.ps), </note> <month> May </month> <year> 1995. </year>
Reference-contexts: Multi-band ASR method <ref> [18, 42, 7, 25] </ref> is a special case of the multi-stream paradigm. The goal of the multi-stream model is the incorporation of different information streams [9], as the streams may be audio and visual information, or different sets of features derived from speech 1 data. <p> Though still in an inchoate stage, spectral sub-band centroids as features have also recently been proposed in [50]. In our previous work <ref> [42] </ref>, we used power spectrum values obtained after PLP critical band filter analysis, cube-root compression, and equal loudness equalization [23]. <p> The former method (i.e., using limited narrow band information for discriminating between all classes for each sub-band) has been used in multi-band 11 approaches <ref> [42, 7, 25, 18] </ref>, and the latter (using all available features for distinguishing between all classes) is the norm in the traditional ASR systems. In order to determine the merged phonetic classes for each band, we performed band-limited phone recognition and created confusion matrices. <p> timing deviations from the full-band alignments; thus, we would expect that there is a potential for improvements in acoustic modeling if longer time-scale information stream merging (i.e., phone or syllable) is used. 5 Designing a Multi-band System As will be described later in Section 10.1, we performed some preliminary experiments <ref> [42] </ref> with the Bellcore Digits database in 1995, which have served as proof of concept experiments for multi-band ASR. As discussed in Section 3.8, in the work reported here we have used NUMBERS95, a continuous spontaneous speech database. <p> The WERR on this systems was 7.9%, compared to the WERR for the baseline full-band system trained on RASTA-PLP order 8 features (plus deltas and delta energy) of also 7.9%. In our preliminary experiments in 1995 <ref> [42] </ref>, we had expected spectral features to be good for multi-band recognition. However, we observed that they were outperformed by narrow-band cepstral features. For sake of completeness, we extracted spectral features and trained a full-band recognition on these 15 spectral features and their deltas. <p> More recently, work by us <ref> [42] </ref> and our collaborators Bourlard and Dupont [7, 8] and Hermansky and Tibrewala [25, 67] has focused on multi-band for continuous speech recognition. Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. <p> Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. The goal of our first set of experiments (in 1995) <ref> [42] </ref> was to establish a proof of concept by developing a two-band multi-band system for word recognition. For features we chose a vector of 15 power spectrum values obtained after PLP critical band filter analysis, cube-root compression, and equal loudness equalization [23].
Reference: [43] <author> Nikki Mirghafori. </author> <title> An alternative approach to automatic speech recognition using sub-band linguistic categories. </title> <note> Thesis Proposal (http://www.icsi.berkeley.edu/~nikki/papers/thesis prop.ps), </note> <month> December </month> <year> 1996. </year>
Reference-contexts: In Section 4.3 we focus our attention on the following: some multi-band researchers <ref> [68, 8, 67, 43, 5] </ref> have postulated that transitions in sub-bands occur asynchronously, and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit.
Reference: [44] <author> Nikki Mirghafori, Eric Fosler, and Nelson Morgan. </author> <title> Fast speakers in large vocabulary continuous speech recognition: Analysis and antidotes. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <volume> volume 1, </volume> <pages> pages 491-494, </pages> <address> Madrid, Spain, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Variables such as speaking style, speaking rate, accent, variable vocal effort, background noise, room reverberation, and channel effects degrade ASR accuracy dramatically, whereas they affect human speech recognition much less <ref> [44, 35, 31] </ref>.
Reference: [45] <author> Nikki Mirghafori, Eric Fosler, and Nelson Morgan. </author> <title> Why is ASR harder for fast speech and what can we do about it? In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </title> <address> pages 179-183, Snowbird, Utah, </address> <month> December </month> <year> 1995. </year>
Reference: [46] <author> Nikki Mirghafori, Eric Fosler, and Nelson Morgan. </author> <title> Towards robustness to fast speech in ASR. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 335-338, </pages> <address> Atlanta, Georgia, </address> <month> May </month> <year> 1996. </year>
Reference: [47] <author> Nelson Morgan, Herve Bourlard, Steven Greenberg, and Hynek Hermansky. </author> <title> Stochastic perceptual auditory-event-based models for speech recognition. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <pages> pages 1943-1946, </pages> <address> Yokohama, Japan, </address> <month> September </month> <year> 1994. </year> <month> 59 </month>
Reference-contexts: The details are discussed in Section 7. 3.7 Using Transition-based Classes One of the most immediate extensions of the multi-band paradigm is the use of transition based phone classes, similar to those of Stochastic Perceptual Auditory-event Models (SPAM) <ref> [47] </ref>. The main idea is that as we divide the full-band into sub-bands, we better isolate the spectral transition regions. The mathematical modeling power of the recognizer should be more focused on distinguishing between states representing change than on states corresponding to little change (steady states). <p> We have started experimenting with these algorithms, though it is too early for us to make any conclusive statements about the results. 38 9.3 Transition-Based Phone Models Transition-Based (TB) Phone Modeling, a simplified version of the Stochastic Perceptual Auditory-event Model (SPAM) <ref> [47] </ref>, is one of the most immediate extensions of the multi-band paradigm. The main idea of TB models is that the modeling power of the recognizer should be more focused on distinguishing between states representing change than on states corresponding to little change. <p> Ghitza's findings serve as motivations for developing specialized phone-like classes for each sub-band, and for exploring transition-based classes <ref> [47] </ref>, which similar to diphones, emphasize the transition regions. Finally, we wish to briefly discuss the the seminal work of Miller and Nicely [41] on low frequency and high frequency masking.
Reference: [48] <author> Nelson Morgan, Su-Lin Wu, and Herve Bourlard. </author> <title> Digit recognition with stochas-tic perceptual speech models. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <pages> pages 771-774, </pages> <address> Madrid, Spain, </address> <year> 1995. </year>
Reference-contexts: The WERR of the multi-band system increased to 15.2% (from the baseline 8.3%) on NUMBERS95 development set. 5.6 Combining the Probabilities of Full- and Multi-band Systems Research at ICSI on merging streams of information <ref> [72, 48] </ref> has shown that often merging the probabilities of the conventional full-band system with another experimental system with different error characteristics leads to improvements in the overall system performance.
Reference: [49] <author> S. Okawa, E. Bocchieri, and A. Potamianos. </author> <title> Multi-band speech recognition in noisy environments. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1998. </year> <note> To Appear. </note>
Reference-contexts: All previous experiments cited performed stream merges in the likelihood domain. We explore the above areas in this work. Another question is in what domain should the merging occur: phonetic posteriors, phonetic likelihoods, or as recently explored in <ref> [49] </ref>, feature vectors? Although there is some advantages to merging the feature vectors (e.g., the simplicity of having just one probability estimator), one can not take advantage of asynchronous merging on that level. Merging in the posterior and likelihood domains has been compared mathematically, not practically, in [9].
Reference: [50] <author> Kuldip K. Paliwal. </author> <title> Spectral subband centroids as features for speech recognition. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 124-130, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: Though still in an inchoate stage, spectral sub-band centroids as features have also recently been proposed in <ref> [50] </ref>. In our previous work [42], we used power spectrum values obtained after PLP critical band filter analysis, cube-root compression, and equal loudness equalization [23].
Reference: [51] <author> John R. Pierce. </author> <title> Whither speech recognition. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 46 </volume> <pages> 1049-1051, </pages> <year> 1969. </year>
Reference-contexts: There have been great advances in ASR technology since its inception in 1950's. Arguably, the first simplified speech recognition system was Radio Rex, a celluloid toy dog which would bounce up when its name was called, built in the 1920's <ref> [51] </ref>. The first "real" speech recognizer, came along next in 1952 at Bell Labs built by Davis, Biddulph, and Balashek [16]. This system was a speaker-dependent digit recognizer which achieved 2% error rate as long as the speaker did not move his head.
Reference: [52] <author> Louis C. W. Pols. </author> <title> Flexile human speech recognition. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 273-283, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: In a recent DARPA evaluation, for example, a word error rate of 6% was achieved on a speaker-independent unlimited vocabulary read-speech task [62]. Although impressive, the ASR problem is not solved by any means, as the state of the art is nowhere close to human speech recognition capabilities <ref> [37, 52, 13] </ref>. The error rates of automatic speech recognizers are one or two orders of magnitude higher than those of humans for many speech recognition tasks, ranging from a 10-word digit to a 65,000-word spontaneous continuous speech recognition task.
Reference: [53] <author> Lawrence Rabiner and Biing-Hwang Juang. </author> <title> Fundamentals of Speech Recognition, </title> <booktitle> chapter 7.3, </booktitle> <pages> pages 395-400. </pages> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: Merging on phone or syllable levels pose a bigger challenge, as it may be implemented using an algorithm such as HMM decomposition [69] or two-level dynamic programming <ref> [53] </ref>. Increased space and search time requirements may be a problem with these algorithms. We have started to look into their application in asynchronous merging of the bands. 3.5 Method of Combination An important research issue is finding an optimal strategy for combining the sub-bands. <p> Allowing the streams in each band to merge asynchronously on a longer time scale seems promising. The HMM decomposition algorithm [69] and the two-level dynamic programming <ref> [53] </ref> are viable algorithms for implementing asynchronous merging.
Reference: [54] <author> Sudhakar Rao and Wiliam A. Pearlman. </author> <title> Analysis of linear prediction, coding, and spectral estimation from subbands. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(4) </volume> <pages> 1160-1178, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: we will present reasons for choosing spontaneous continuous speech tasks as our test-bed. 2.1 Motivation for the Multi-Band Paradigm The following are motivations for the multi-band approach: 2 A lexicon is made up of the phonetic transcription of each word in an HMM format. 3 * Recently Rao and Pearlman <ref> [54] </ref> have proven theoretically, and shown with simulations, that auto-regressive spectral estimation from sub-bands offers a gain over full-band auto-regressive spectral estimation. <p> We train four MLPs on these acoustic features, that is, one on each sub-band. The input layer to each MLP has a context window of nine frames, for total input layer sizes of <ref> [72, 72, 54, 54] </ref> respectively. We choose hidden layer sizes of [497, 497, 372, 372], respectively, so that the total number of parameters in the four MLPs and the full-band system are roughly equal. There are 56 output units, one for every phone, as in the full-band MLP 1 . <p> We trained 20 four MLPs on these acoustic features, that is, one on each sub-band. The input layer to each MLP has a context window of nine frames. The total number of inputs is <ref> [72, 72, 54, 54] </ref> for each MLP, respectively.
Reference: [55] <author> Christopher Ris. </author> <title> Four-band multi-band results on Switchboard database. </title> <note> Reported at the Johns Hopkins 96 Workshop (http://www.clsp.jhu.edu/ws96/ris/results-report.html), August 1996. </note>
Reference-contexts: Dividing the streams into four narrow-bands seemed better than either two or six bands. The experiment on merging level (HMM-state vs. phone, vs. syllable) was inconclusive. Bourlard et. al. 's results on the Switchboard continuous speech corpus <ref> [55] </ref> are also noteworthy, as they show applicability of multi-band paradigm to a large vocabulary task (similar to the work of Tibrewala and Hermansky discussed above). The training and testing sets used in their experiment were similar to the ones used in [65] (as reported above).
Reference: [56] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: We usually use RASTA-PLP processing [24]. The next element in our system is the phonetic probability estimator, which is a fully connected multi-layer perceptron trained using the backpropagation algorithm <ref> [56] </ref> with softmax normalization [11] on the output layer and relative entropy error criterion [61] to estimate the probability of each phoneme corresponding to (multiple) frames of speech. Next, the phonetic probabilities, along with a grammar 1 and a 1 We often use a bigram grammar. <p> The MLP is fully connected and has 153 inputs (9 frames with 17 features per frame), 1000 hidden units, and 56 outputs (one output for each phone 4 ), and is trained using backpropagation <ref> [56] </ref>, with softmax normalization [11], and relative entropy error criterion [61] at the output layer. The system is trained on hand-transcribed phone labels (without embedded realignment).
Reference: [57] <author> M. J. Russell, K. M. Ponting, S. M. Peeling, S. R. Browning, J. S. Bridle, and R. K. Moore. </author> <title> The ARM continuous speech recognition system. </title> <booktitle> In ICASSP, </booktitle> <address> Albuquerque, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Tomlinson et. al. [68] devised a two-band system with asynchrony between a high-and a low-pass component of the speech spectrum through a variant of HMM decomposition. Their experiments were performed on an in-house speaker-dependent 500 word ARM (Airborne Reconnaissance Mission) task <ref> [57] </ref>.
Reference: [58] <author> Martin Russell. </author> <title> Progress towards speech models that model speech. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 115-122, </pages> <address> Santa Barbara, CA, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: One main advantage of the multi-band paradigm is the ability to combine the streams asynchronously. We note that this facet of multi-band highlighted in the ASR community as a potential method for overcoming fundamental limitations of current HMM-based systems <ref> [58] </ref>. 2 Motivation In what follows, we first present motivations for the multi-band paradigm. Next, we explain why using merged linguistic classes seems promising.
Reference: [59] <author> Bulent Sankur, Yasemin P. Kahya, E. C~ agatay Guler, and Tanju Engin. </author> <title> Feature extraction and classification of nonstationary signals based on the multiresolu-tion. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <year> 1994. </year> <month> 60 </month>
Reference-contexts: Furthermore, it is unclear whether the performance gain was purely from asynchronous merging, as the results for two vs. three band systems were not consistent. A related multi-band work is that of Sankur et. al. <ref> [59] </ref> with application to the classification of respiratory sound signals into healthy and pathological cases.
Reference: [60] <author> Robert V. Shannon, Fan-Gang Zeng, Vivek Kamath, John Wygonski, and Michael Ekelid. </author> <title> Speech recognition with primarily temporal cues. </title> <journal> Science, </journal> <volume> 270 </volume> <pages> 303-304, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: They used [17-778Hz], [707-1631Hz], [1506-2709Hz], and [2121-3769Hz] for their four band experiment, grouping critical bands [1-6], [7-10], [11-13], and [13-15]. This division is similar to what Shan-non used in his psychoacoustic experiments on amplitude modulated noise in four frequency bands <ref> [60] </ref>. Tibrewala and Hermansky [67] observed that the error rates for a two- and four-band system were lower than that of a seven-band system. One study which may be of interest in the sub-band boundary decision is that of Houtgast and Verhave [28].
Reference: [61] <author> S. A. Solla, E. Levin, and M. Fleisher. </author> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 625-640, </pages> <year> 1988. </year>
Reference-contexts: We usually use RASTA-PLP processing [24]. The next element in our system is the phonetic probability estimator, which is a fully connected multi-layer perceptron trained using the backpropagation algorithm [56] with softmax normalization [11] on the output layer and relative entropy error criterion <ref> [61] </ref> to estimate the probability of each phoneme corresponding to (multiple) frames of speech. Next, the phonetic probabilities, along with a grammar 1 and a 1 We often use a bigram grammar. <p> The MLP is fully connected and has 153 inputs (9 frames with 17 features per frame), 1000 hidden units, and 56 outputs (one output for each phone 4 ), and is trained using backpropagation [56], with softmax normalization [11], and relative entropy error criterion <ref> [61] </ref> at the output layer. The system is trained on hand-transcribed phone labels (without embedded realignment). Using a multiple pronunciation lexicon (derived from hand transcriptions), and a bigram language model, the WERR of this baseline system on the test set is 7.9%.
Reference: [62] <author> Richard M. Stern. </author> <title> Specification of the 1995 ARPA HUB 3 evaluation: Unlimited vocabulary NAB news baseline. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 5-7, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: In a recent DARPA evaluation, for example, a word error rate of 6% was achieved on a speaker-independent unlimited vocabulary read-speech task <ref> [62] </ref>. Although impressive, the ASR problem is not solved by any means, as the state of the art is nowhere close to human speech recognition capabilities [37, 52, 13].
Reference: [63] <author> K. Stevens, S. Keyser, and H. Kawasaki. </author> <title> Toward a phonetic and phonological theory of redundant features. </title> <editor> In J. Perkell and D. Klatt, editors, </editor> <booktitle> Invariance & Variability in Speech Processes. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, N.J., </address> <year> 1986. </year>
Reference-contexts: The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. We will examine this supposition in Section 4.3. * Signal processing approaches may attempt to merge the redundancy <ref> [63] </ref> in the information inherent in the speech signal. <p> For continuous speech recognition, our informal listening experiments on TIMIT sentences suggest that band-limited speech recognition is surprisingly good, perhaps due to the presence of contextual information. Clearly, there must be much redundancy in the information content in speech to make recognition of narrow-band speech possible <ref> [63] </ref>. Finally, French & Steinberg [20] have also performed human speech recognition experiments with non-sense CVC's with high-pass and low-pass speech.
Reference: [64] <author> Sangita Tibrewala. </author> <type> Personal Communication, </type> <month> October </month> <year> 1996. </year>
Reference-contexts: Our four chosen bands are [216-778Hz], [707-1631Hz], [1506-2709Hz], and [2121-3769Hz]. 3.2 Acoustic Features Designing an optimal feature set for each sub-band is another research issue that warrants investigation. This is one of the topics addressed in Tibrewala's work <ref> [64] </ref>, where she uses high and low modulation spectrum for capturing the long term variations (e.g., syllable level) and short term variations (e.g., phone level). Though still in an inchoate stage, spectral sub-band centroids as features have also recently been proposed in [50]. <p> For normal conversational speech, different window sizes may allow capturing different dynamics of speech such as short term and long term variations <ref> [64] </ref>. <p> To our surprise, the word error of this full-band system increased to 9.0% (up from 7.9%). Perhaps training on the same data will result in over-training. We note that Tibrewala <ref> [64] </ref> observed an improvement when she trained a similar probability estimator on an data set different from the training set. Another parameter we tested was the difference between using likelihoods versus posterior probabilities. Up to now our colleagues have merged the likelihoods only.
Reference: [65] <author> Sangita Tibrewala. </author> <title> Seven-band multi-band results on Switchboard database. </title> <note> Reported at the Johns Hopkins 96 Workshop (http://www.clsp.jhu.edu/ws96/ris/results-report.html), August 1996. </note>
Reference-contexts: The merging was simply done on the word level by training an MLP on the normalized log likelihoods obtained from the Viterbi decoding distances. The word error rate of the full-band was 4.6% and the two-band system's was 4.3%. The improvement was not statistically significant. corpus. Hermansky and Tibrewala <ref> [25, 65, 67, 66] </ref> tested 2-band and 7-band multi-band systems on clean and noisy speech on the DIGITS database. <p> The training and testing sets used in their experiment were similar to the ones used in <ref> [65] </ref> (as reported above). Each of the four sub-bands had 162-234 input units, 500 hidden units, and 56 output units (a total of roughly 510K parameters).
Reference: [66] <author> Sangita Tibrewala and Hynek Hermansky. </author> <title> Multi-band and adaptation approaches to robust speech recognition. </title> <booktitle> In Proccedings of the European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2619-2622, </pages> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: The merging was simply done on the word level by training an MLP on the normalized log likelihoods obtained from the Viterbi decoding distances. The word error rate of the full-band was 4.6% and the two-band system's was 4.3%. The improvement was not statistically significant. corpus. Hermansky and Tibrewala <ref> [25, 65, 67, 66] </ref> tested 2-band and 7-band multi-band systems on clean and noisy speech on the DIGITS database. <p> However, if the neighboring spectral regions are removed, or if the removal is according to local levels of SNR, the performance deteriorates gravely. Multi-band appears to be more robust in these situations <ref> [8, 66] </ref>. 11 Appendix B Confusion Matrices Figures 18, 19, 20, 21, 22, and 23 show confusion matrices generated for sub-bands 1 through 4, the full-band, and multi-band, respectively. The data has been generated on NUMBERS95 development set.
Reference: [67] <author> Sangita Tibrewala and Hynek Hermansky. </author> <title> Sub-band based recognition of noisy speech. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1255-1258, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized <ref> [8, 67, 68] </ref> that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> They used [17-778Hz], [707-1631Hz], [1506-2709Hz], and [2121-3769Hz] for their four band experiment, grouping critical bands [1-6], [7-10], [11-13], and [13-15]. This division is similar to what Shan-non used in his psychoacoustic experiments on amplitude modulated noise in four frequency bands [60]. Tibrewala and Hermansky <ref> [67] </ref> observed that the error rates for a two- and four-band system were lower than that of a seven-band system. One study which may be of interest in the sub-band boundary decision is that of Houtgast and Verhave [28]. <p> 541.89 628.53 7 707.14 837.63 948.84 9 1050.92 1222.34 1369.93 11 1506.32 1736.88 1936.52 13 2121.72 2435.90 2708.80 15 2962.48 3393.65 3768.80 Table 1: The half-power low and high frequency cutoffs for the RASTA-PLP filters when the sampling frequency is 8kHz. on band-limited critical band values, as also observed by <ref> [7, 67] </ref>. In this work, we use RASTA-PLP processing [24] in each sub-band. <p> Traditionally, non-linear methods have had higher accuracy compared to linear combination strategies. Non-linear merging (using an MLP) produce lower word error rates than linear merging (e.g., multiplying the probabilities) <ref> [7, 67] </ref> when multi-band information is merged on the frame level. We explore different ways of combining the probabilities (e.g., adding or multiplying the probabilities, adding their logs, etc.). Another question is whether the merging should be done in a posterior or likelihood domain. <p> In Section 4.3 we focus our attention on the following: some multi-band researchers <ref> [68, 8, 67, 43, 5] </ref> have postulated that transitions in sub-bands occur asynchronously, and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit. <p> More recently, work by us [42] and our collaborators Bourlard and Dupont [7, 8] and Hermansky and Tibrewala <ref> [25, 67] </ref> has focused on multi-band for continuous speech recognition. Comparable or better performance for normal speech, and superior performance for band-limited noisy speech were demonstrated. We briefly summarize these results below. <p> The merging was simply done on the word level by training an MLP on the normalized log likelihoods obtained from the Viterbi decoding distances. The word error rate of the full-band was 4.6% and the two-band system's was 4.3%. The improvement was not statistically significant. corpus. Hermansky and Tibrewala <ref> [25, 65, 67, 66] </ref> tested 2-band and 7-band multi-band systems on clean and noisy speech on the DIGITS database. <p> One does not need to running 127 (or a similar number of MLP merging units) to reap the noise robustness benefits, as for a large task the needed computational power would make this approach infeasible. Another notable result of <ref> [67] </ref> is that merging on the frame level is as good as on the word level, again, making their approach scalable to tasks with larger vocabulary sizes. <p> Their results showed that for clean speech, the multi-band paradigm was either as good or better than the full-band system; and for noisy speech, the performance of the multi-band system was superior. Choosing LPC-cepstral features was superior to critical band energies (also observed by <ref> [67] </ref>). Dividing the streams into four narrow-bands seemed better than either two or six bands. The experiment on merging level (HMM-state vs. phone, vs. syllable) was inconclusive.
Reference: [68] <author> M. J. Tomlinson, M. J. Russell, R. K. Moore, A. P. Buckland, and M. A. Fawley. </author> <title> Modelling asynchrony in speech using elementary single-signal decomposition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1247-1250, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: We discuss the psycho-acoustic motivation in more detail in Chapter 10.2. * Researchers have hypothesized <ref> [8, 67, 68] </ref> that phone transitions occur at different times in different bands (see Figure 4). The multi-band recognition allows us to conveniently allow for asynchronous combination of the sub-band frequency information. <p> In Section 4.3 we focus our attention on the following: some multi-band researchers <ref> [68, 8, 67, 43, 5] </ref> have postulated that transitions in sub-bands occur asynchronously, and that a phone or syllable level merging of multi-band streams is necessary to permit independent alignment for each band within the merged unit. <p> The word error rate decreased to 59.7%. What distinguishes the work in this TR from that of our collaborators, as cited above, is that we have focused our attention on developing merged classes, transition based narrow-band classes, as well as on the analysis of multi-band. Tomlinson et. al. <ref> [68] </ref> devised a two-band system with asynchrony between a high-and a low-pass component of the speech spectrum through a variant of HMM decomposition. Their experiments were performed on an in-house speaker-dependent 500 word ARM (Airborne Reconnaissance Mission) task [57]. <p> Again, it is encouraging that multi-band has been shown to perform as well or better than a full-band system, however, the generalizability of the approach in <ref> [68] </ref> is uncertain. Furthermore, it is unclear whether the performance gain was purely from asynchronous merging, as the results for two vs. three band systems were not consistent.
Reference: [69] <author> A. P. Varga and R. K. Moore. </author> <title> Hidden Markov model decomposition of speech and noise. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 845-848, </pages> <year> 1990. </year>
Reference-contexts: Merging on phone or syllable levels pose a bigger challenge, as it may be implemented using an algorithm such as HMM decomposition <ref> [69] </ref> or two-level dynamic programming [53]. Increased space and search time requirements may be a problem with these algorithms. We have started to look into their application in asynchronous merging of the bands. 3.5 Method of Combination An important research issue is finding an optimal strategy for combining the sub-bands. <p> Allowing the streams in each band to merge asynchronously on a longer time scale seems promising. The HMM decomposition algorithm <ref> [69] </ref> and the two-level dynamic programming [53] are viable algorithms for implementing asynchronous merging. <p> The recombination of the sub-band log likelihoods was performed on either HMM-state, phone, or syllable level. A multi-layer perceptron (MLP) was used as merging unit in the frame-level combination experiments, and in syllable- and word-level combination experiments, HMM decomposition <ref> [69] </ref> was applied to force synchronization of the sub-band streams at particular points. They used one of three databases (i.e., a database of German command words, OGI Numbers '93, and Switchboard) for each experiment.
Reference: [70] <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13(2) </volume> <pages> 260-269, </pages> <year> 1967. </year>
Reference-contexts: Next, the phonetic probabilities, along with a grammar 1 and a 1 We often use a bigram grammar. A bigram grammar is specified by a list of words that can follow a particular word, along with associated probabilities. 2 lexicon 2 are used in a dynamic programming-based Viterbi search <ref> [70] </ref>, a simplified version of the Forward algorithm [2, 3], to find the best strings of words corresponding to the acoustic data. 1.4 What is Multi-band Processing? Figures 2 and 3 highlight the basic elements of a system based on the multi-band paradigm.
Reference: [71] <author> Charles Clayton Wooters. </author> <title> Lexical Modelling in a Speaker Independent Speech Understanding System. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <month> November </month> <year> 1993. </year> <type> ICSI Technical Report TR-93-068. 61 </type>
Reference-contexts: Although the differences are not significant for this dataset, for a larger database we may actually see significant differences. Merging posteriors actually may make more sense, as dividing posteriors by priors may exclude prior information. 5.4 Performing Embedded Alignment We wondered about the effects of embedded training <ref> [71] </ref> on our multi-band system.
Reference: [72] <author> Su-Lin Wu, Brian E. D. Kingsbury, Nelson Morgan, and Steven Greenberg. </author> <title> In--corporating information from syllable-length time scales into automatic speech recognition. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, & Signal Processing, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: We train four MLPs on these acoustic features, that is, one on each sub-band. The input layer to each MLP has a context window of nine frames, for total input layer sizes of <ref> [72, 72, 54, 54] </ref> respectively. We choose hidden layer sizes of [497, 497, 372, 372], respectively, so that the total number of parameters in the four MLPs and the full-band system are roughly equal. There are 56 output units, one for every phone, as in the full-band MLP 1 . <p> We trained 20 four MLPs on these acoustic features, that is, one on each sub-band. The input layer to each MLP has a context window of nine frames. The total number of inputs is <ref> [72, 72, 54, 54] </ref> for each MLP, respectively. <p> The WERR of the multi-band system increased to 15.2% (from the baseline 8.3%) on NUMBERS95 development set. 5.6 Combining the Probabilities of Full- and Multi-band Systems Research at ICSI on merging streams of information <ref> [72, 48] </ref> has shown that often merging the probabilities of the conventional full-band system with another experimental system with different error characteristics leads to improvements in the overall system performance.
Reference: [73] <author> Steve Young. </author> <title> Large vocabulary continuous speech recognition: A review. </title> <booktitle> In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, </booktitle> <pages> pages 29-44, </pages> <address> Snowbird, Utah, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Word error rates between 7% and 8% are not uncommon using context-dependent phoneme level hidden Markov models (HMMs), N-gram language models and huge quantities of training data for vocabularies of up to 65,000 words <ref> [73] </ref>. In a recent DARPA evaluation, for example, a word error rate of 6% was achieved on a speaker-independent unlimited vocabulary read-speech task [62].
Reference: [74] <author> Victor Zue and et. al. </author> <title> Recent progress on SUMMIT system. </title> <booktitle> In Proceedings of the Third DARPA Workshop on Speech and Natural Language, </booktitle> <pages> pages 380-384, </pages> <month> June </month> <year> 1990. </year> <month> 62 </month>
Reference-contexts: The highest phoneme recognition accuracy achieved on the TIMIT test set was 58.5%, which was within the range of the performance achieved by the established phonetic recognizers <ref> [34, 29, 74] </ref>. Our work on multi-band processing has more differences than similarities with that of Duchnowski.
References-found: 74

