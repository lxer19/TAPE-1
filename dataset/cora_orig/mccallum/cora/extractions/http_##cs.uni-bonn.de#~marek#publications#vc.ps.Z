URL: http://cs.uni-bonn.de/~marek/publications/vc.ps.Z
Refering-URL: http://cs.uni-bonn.de/~marek/index-en.html
Root-URL: http://cs.uni-bonn.de
Title: Bounding VC-Dimension for Neural Networks: Progress and Prospects  
Author: Marek Karpinski (Bonn) Angus Macintyre 
Note: Research partially supported by the DFG Grant KA 673/4-1, and by ESPRIT BR Grants 7097 and ECUS 030. Mathematical Institute, University of Oxford, Oxford OX1 3LB. Research supported in part by a Senior Research Fellowship of the SERC.  
Address: Bonn, 53117 Bonn.  
Affiliation: (Oxford) Dept. of Computer Science, University of  
Abstract: Techniques from differential topology are used to give polynomial bounds for the VC-dimension of sigmoidal neural networks. The bounds are quadratic in w, the dimension of the space of weights. Similar results are obtained for a wide class of Pfaffian activation functions. The obstruction (in differential topology) to improving the bound to an optimal bound O (w log w) is discussed, and attention is paid to the role of other parameters involved in the network architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [AB92] <author> M. Anthony, N. Biggs, </author> <title> Computational Learning Theory: An Introduction, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction We refer to Macintyre-Sontag [MS93](cf, e.g., also <ref> [AB92] </ref> and [GJ93]) for all notions required from the theory of neural architectures, and to Hirsch for necessary notions from differential topology [H76].
Reference: [AS93] <author> M. Anthony, J. Shawe-Taylor, </author> <title> A Result of Vapnik with Applications, </title> <journal> Discrete Applied Math. </journal> <volume> 47 (1993), </volume> <pages> pp. 207-217. </pages>
Reference: [BT90] <author> A. Borodin, P. Tiwari, </author> <title> On the Decidability of Sparse Univariate Polynomial Interpolation, </title> <booktitle> Proc. 22nd ACM STOC (1990), </booktitle> <pages> pp. 535-545. </pages>
Reference: [D92] <author> L. van den Dries, </author> <title> Tame Topology and 0-minimal Structures, </title> <type> preprint, </type> <institution> University of Illinois, Urbana, </institution> <note> 1992; to appear as a book. </note>
Reference: [DMM94] <author> L. van den Dries, A.Macintyre and D.Marker, </author> <title> The Elementary Theory of Restricted Analytic Fields with Exponentation, </title> <booktitle> Annuals of Mathematics 140 (1994), </booktitle> <pages> pp 183-205. </pages>
Reference: [GJ93] <author> P.Goldberg and M.Jerrum, </author> <title> Bounding the Vapnik Chervonenkis Dimension of Concept Classes Parametrized by Real Numbers. </title> <journal> Machine Learning, </journal> <note> 1994 (to appear). A preliminary version appeared in Proc. 6th ACM Workshop on Computational Learning Theory, pp. 361-369, </note> <year> 1993. </year>
Reference-contexts: 1 Introduction We refer to Macintyre-Sontag [MS93](cf, e.g., also [AB92] and <ref> [GJ93] </ref>) for all notions required from the theory of neural architectures, and to Hirsch for necessary notions from differential topology [H76]. <p> The method applies to a huge variety of other activation functions, and the polynomials involved in computing could be replaced by much more general functions. The method is, however, inadequate to give interesting bounds. Taking a hint from Goldberg and Jerrum works <ref> [GJ93] </ref>, and a reference to Warren's [W68], we have found a method, not appealing to logic but using rather more differential topology, for giving very good polynomial bounds when the activation function satisfies a special kind of Pfaffian differential equation. <p> This is true, for example, for the fi corresponding to sigmoidal neural networks [KM94]. Our general result, based essentially on Warren's ideas [W68], is: Theorem 1 VC-Dim (C ) 2 log B + 16 l (log s + 1). Note: Goldberg and Jerrum <ref> [GJ93] </ref> have a result of this type when the o are polynomial and apply it to get a bound of order l log l for the VC-dimension of neural netwoks with semi-algebraic activation functions. <p> B was estimated via a result of Warren [W68] (or Milnor [M64] can be used) but Warren's method was irrelevant in <ref> [GJ93] </ref>. l log l appears not from log B, but from the l log s, by crude estimates. 2.1 Application to Neural Nets In this case (v; ~y) is o (v; ~y) &gt; 0; where o is composed out of polynomials and activation functions according to the structure of the underlying
Reference: [H12] <author> G.H. Hardy, </author> <title> Properties of Logarithmic-Exponential Functions, </title> <journal> Proc. London Math. Soc. </journal> <volume> 10 (1912), </volume> <pages> pp. 54-90. </pages>
Reference: [H92] <author> D. Haussler, </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Nets and other Learning Applications, Information an Computation 100, </title> <booktitle> (1992), </booktitle> <pages> pp. 78-150. </pages>
Reference: [HKP91] <author> J. Hertz, A. Krogh and R. G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [H76] <author> M. W. Hirsch, </author> <title> Differential Topology, </title> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: 1 Introduction We refer to Macintyre-Sontag [MS93](cf, e.g., also [AB92] and [GJ93]) for all notions required from the theory of neural architectures, and to Hirsch for necessary notions from differential topology <ref> [H76] </ref>. There [MS93], some general (but profound) results from logic were used to show that for feedforward neural architectures with activation the standard sigmoid oe (y) = 1=1 + e x the VC-dimension is finite.
Reference: [KM94] <author> M. Karpinski and A. Macintyre, </author> <title> Quadratic Bounds for VC Dimension at Sig-moidal Neural Networks, </title> <note> Research Report No. 85116-CS, Universitat Bonn, 1994; to be submitted. </note>
Reference-contexts: Let C be the family of all ~ fi as ~ fi varies through R ` . We give good bounds of the VC-dimension of C , under some assumptions (some are necessary!) about the o i . In <ref> [KM94] </ref> it is shown how the VC-dimension of a neural architecture with k inputs and l weights is a special case of a VC-Dim (C ). 2.2. We maintain the notation of 2.1. <p> This is true, for example, for the fi corresponding to sigmoidal neural networks <ref> [KM94] </ref>. Our general result, based essentially on Warren's ideas [W68], is: Theorem 1 VC-Dim (C ) 2 log B + 16 l (log s + 1). <p> For w = 1, we can use another method, going back to Hardy [H12],allowing us to replace mw (mw 1) (= m (m 1)) by a term linear in m. For the proof of Theorem 2, see <ref> [KM94] </ref>. 3 Generalization and Prospects 3.1. Theorem 2, with a quadratic dominant term, does not depend too strongly on the form of the activation function. To see this, one can appeal to Khovanskii's book [K91](p. 91).
Reference: [KW93] <author> M.Karpinski and T.Werther, </author> <title> VC Dimension and Uniform Learnability of Sparse Polynomials and Rational Functions, </title> <journal> SIAM J. Computing 22 (1993), </journal> <pages> pp 1276-1285. </pages>
Reference: [K91] <editor> A.G.Khovanski, Fewnomials, </editor> <publisher> American Mathematical Society, </publisher> <address> Providence, R.I., </address> <year> 1991. </year>
Reference-contexts: The restriction to Pfaffian is because we use a method of Khovanski <ref> [K91] </ref>. 2 Formulation and Main Results 2.1. The method is by no means restricted to neural architectures, and we choose to present it in greater generality. Fix integers k; l and C 1 (infinitely differentiable) functions o 1 ; : : : ; o s from R k+` to R. <p> The problem then is to bound log B explicitly. For sigmoidal activation functions, one can appeal to a method, and results, of Khovanski <ref> [K91] </ref>. In order to avoid a clash of notation (cf. [M93]), we now write w for `. w is the dimension of the weight space. The other relevant parameters are: i) d = a bound for the degrees of all polynomials involved; ii) m = number of computation nodes. <p> V C Dim (A) (mw)(mw 1) + 8mw log (2md + 1) Note: The only troublesome term is (mw)(mw 1) . Its presence is easily traced to the term 2 q (q1)=2 in Khovanskii's basic estimate <ref> [K91] </ref>, where q is the number of exponentials involved in a problem. For w = 1, we can use another method, going back to Hardy [H12],allowing us to replace mw (mw 1) (= m (m 1)) by a term linear in m. <p> There is an obvious way to consider networks with multivariate activation functions. If these are Pfaffian, we still get a quadratic dominant term. We will elaborate this in a future publication. 3.2. How to remove the quadratic term? One sees easily that it occurs because Khovanski <ref> [K91] </ref> removes one exponential at a time in his basic inductive method. We are hard at work on a method for removing all at once, and we expect to replace mw (mw 1) by mw. 2
Reference: [KPS86] <author> J.Knight, A.Pillay and C.Steinhorn, </author> <title> Definable Sets and Ordered Structures II, </title> <journal> Trans. American Mathematical Society 295 (1986), pp.593-605. </journal>
Reference: [L92] <author> M.C.Laskowsky, </author> <title> Vapnik-Chervonenkis Classes od Definable Sets, </title> <journal> J.London Math. Society 45 (1992), </journal> <pages> pp 377-384. </pages>
Reference: [M93] <author> W.Maass, </author> <title> On the Complexity of Learning on Feedforward Neural Nets, </title> <booktitle> in Proc. EATCS Advanced School on Computational Learning and Cryptography, Vietri sul Mare, </booktitle> <year> 1993. </year>
Reference-contexts: The problem then is to bound log B explicitly. For sigmoidal activation functions, one can appeal to a method, and results, of Khovanski [K91]. In order to avoid a clash of notation (cf. <ref> [M93] </ref>), we now write w for `. w is the dimension of the weight space. The other relevant parameters are: i) d = a bound for the degrees of all polynomials involved; ii) m = number of computation nodes.
Reference: [MSS91] <author> W. Maass, G. Schnitger and E. D. Sontag, </author> <title> On the Computational Power of Sigmoidal versus Boolean Threshold Circuits, </title> <booktitle> Proc. 32nd IEEE FOCS (1991), </booktitle> <pages> pp. 767-776. </pages>
Reference: [MS93] <author> A.J.Macintyre and E.D.Sontag, </author> <title> Finiteness results for Sigmoidal Neural Networks, </title> <booktitle> Proc. 25th ACM STOC (1993), </booktitle> <address> pp.325-334. </address>
Reference-contexts: 1 Introduction We refer to Macintyre-Sontag <ref> [MS93] </ref>(cf, e.g., also [AB92] and [GJ93]) for all notions required from the theory of neural architectures, and to Hirsch for necessary notions from differential topology [H76]. There [MS93], some general (but profound) results from logic were used to show that for feedforward neural architectures with activation the standard sigmoid oe (y) = 1=1 + e x the VC-dimension is finite.
Reference: [M64] <author> J.Milnor, </author> <title> On the Betti Numbers of Real Varieties, </title> <booktitle> Proc. of the American Mathematical Society 15 (1964), </booktitle> <pages> pp 275-280. </pages>
Reference-contexts: Note: Goldberg and Jerrum [GJ93] have a result of this type when the o are polynomial and apply it to get a bound of order l log l for the VC-dimension of neural netwoks with semi-algebraic activation functions. B was estimated via a result of Warren [W68] (or Milnor <ref> [M64] </ref> can be used) but Warren's method was irrelevant in [GJ93]. l log l appears not from log B, but from the l log s, by crude estimates. 2.1 Application to Neural Nets In this case (v; ~y) is o (v; ~y) &gt; 0; where o is composed out of polynomials
Reference: [M65] <author> J.Milnor, </author> <title> Topology from the Differentiable Viewpoint, </title> <address> Univ.Press, Virginia, </address> <year> 1965. </year>
Reference: [S-T94] <author> J. Shawe-Taylor, </author> <title> Sample Sizes for Sigmoidal Neural Networks, </title> <type> Preprint, </type> <institution> University of London, </institution> <year> 1994. </year>
Reference: [TV94] <author> G. Turan and F. Vatan, </author> <title> On the Computation of Boolean Functions by Analog Circuits of Bounded Fan-in, </title> <booktitle> Proc. 35th IEEE FOCS (1994), </booktitle> <pages> pp. 553-564. </pages>
Reference: [W68] <author> H.E.Warren, </author> <title> Lower Bounds for Approximation by Non-linear Manifolds, Trans. </title> <booktitle> of the AMS 133 (1968), </booktitle> <pages> pp. 167-178. </pages>
Reference-contexts: The method applies to a huge variety of other activation functions, and the polynomials involved in computing could be replaced by much more general functions. The method is, however, inadequate to give interesting bounds. Taking a hint from Goldberg and Jerrum works [GJ93], and a reference to Warren's <ref> [W68] </ref>, we have found a method, not appealing to logic but using rather more differential topology, for giving very good polynomial bounds when the activation function satisfies a special kind of Pfaffian differential equation. <p> This is true, for example, for the fi corresponding to sigmoidal neural networks [KM94]. Our general result, based essentially on Warren's ideas <ref> [W68] </ref>, is: Theorem 1 VC-Dim (C ) 2 log B + 16 l (log s + 1). <p> Note: Goldberg and Jerrum [GJ93] have a result of this type when the o are polynomial and apply it to get a bound of order l log l for the VC-dimension of neural netwoks with semi-algebraic activation functions. B was estimated via a result of Warren <ref> [W68] </ref> (or Milnor [M64] can be used) but Warren's method was irrelevant in [GJ93]. l log l appears not from log B, but from the l log s, by crude estimates. 2.1 Application to Neural Nets In this case (v; ~y) is o (v; ~y) &gt; 0; where o is composed
Reference: [W94] <author> A.J.Wilkie, </author> <title> Model Completeness Results of Restricted Pfaffian Functions and the Exponential Function, </title> <note> to appear in Journal of the AMS, </note> <year> 1994. </year>
References-found: 24

