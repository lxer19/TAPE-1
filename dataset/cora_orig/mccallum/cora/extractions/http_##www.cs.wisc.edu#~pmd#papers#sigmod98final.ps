URL: http://www.cs.wisc.edu/~pmd/papers/sigmod98final.ps
Refering-URL: http://www.cs.wisc.edu/~pmd/pmd.html
Root-URL: http://www.cs.wisc.edu
Email: pmd@cs.wisc.edu  karthik@cs.wisc.edu  samit@cs.wisc.edu  naughton@cs.wisc.edu  
Title: Caching Multidimensional Queries Using Chunks  
Author: Prasad M. Deshpande Karthikeyan Ramasamy Amit Shukla Jeffrey F. Naughton 
Address: Wisconsin, Madison  Wisconsin, Madison  Wisconsin, Madison  Wisconsin, Madison  
Affiliation: University of  University of  University of  University of  
Abstract: Caching has been proposed (and implemented) by OLAP systems in order to reduce response times for multidimensional queries. Previous work on such caching has considered table level caching and query level caching. Table level caching is more suitable for static schemes. On the other hand, query level caching can be used in dynamic schemes, but is too coarse for "large" query results. Query level caching has the further drawback for small query results in that it is only effective when a new query is subsumed by a previously cached query. In this paper, we propose caching small regions of the multidimensional space called "chunks". Chunk-based caching allows fine granularity caching, and allows queries to partially reuse the results of previous queries with which they overlap. To facilitate the computation of chunks required by a query but missing from the cache, we propose a new organization for relational tables, which we call a "chunked file." Our experiments show that for workloads that exhibit query locality, chun-ked caching combined with the chunked file organization performs better than query level caching. An unexpected benefit of the chunked file organization is that, due to its multidimensional clustering properties, it can significantly improve the performance of queries that "miss" the cache entirely as compared to traditional file organizations. 
Abstract-found: 1
Intro-found: 1
Reference: [AAD+96] <author> S. Agarwal, R. Agrawal, P.M. Deshpande, A. Gupta, J.F. Naughton, R. Ramakrishnan, S. Sarawagi. </author> <title> On the Computation of Multidimensional Aggregates, </title> <booktitle> Proc. of the 22nd Int. VLDB Conf., </booktitle> <pages> 506-521, </pages> <year> 1996. </year>
Reference: [BPT97] <author> E. Baralis, S. Paraboschi, E. Teniente. </author> <title> Materialized View Selection in a Multidimensional Database, </title> <booktitle> Proc. of the 23rd Int. VLDB Conf., </booktitle> <year> 1997. </year>
Reference: [DFJST] <author> S. Dar, M. J. Franklin, B. T. Jonsson, D. Srivas-tava, M. </author> <title> Tan Semantic Data Caching and Replacement Proc. </title> <booktitle> of the 22nd Int. VLDB Conf., </booktitle> <year> 1996. </year>
Reference-contexts: We use similar ideas in our replacement policies. The idea of using a profit metric is relevant for OLAP, since highly aggregated results are expensive to compute and should be given preference while caching. Caching at a granularity smaller than a query has been proposed in <ref> [DFJST] </ref>. They describe a semantic scheme of caching for client-server systems, where they cache semantic regions. The semantic approach is highly suitable to the OLAP domain where data is multi-dimensional and the notion of semantic data regions is natural.
Reference: [DKLP+94] <author> D. DeWitt, N. Kabra, J. Luo, J. M. Patel, J. Yu. </author> <title> Client-Server Paradise. </title> <booktitle> Proc. of the 1994 VLDB Conf., </booktitle> <year> 1994. </year>
Reference-contexts: We implemented the chunked file in the Paradise <ref> [DKLP+94] </ref> Database System using option 1 above. The chunked file was implemented by using a B-Tree as a chunk index on a fact file.
Reference: [Fell57] <author> William Feller, </author> <title> An Introduction to Probability Theory and Its Applications, Vol. I, </title> <publisher> John Wiley & Sons, </publisher> <pages> pp 241; 1957. </pages>
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, H. Pirahesh. </author> <title> Data Cube: A Relational Aggregation Operator Generalizing Group-By, </title> <booktitle> Cross-Tab, and Sub-Totals, Proc. of the 12th Int. Conf. on Data Engg., </booktitle> <pages> pp 152-159, </pages> <year> 1996. </year>
Reference: [GHRU97] <author> H. Gupta, V. Harinarayan, A. Rajaraman, J.D. Ullman. </author> <title> Index Selection for OLAP. </title> <booktitle> Proc. of the 13th ICDE, </booktitle> <pages> 208-219, </pages> <year> 1997. </year>
Reference-contexts: A static approach is specifically suited for the backend where one can precom-pute aggregated tables. A set of group-bys is chosen and the corresponding tables are materialized. This refers to the problem of selecting what aggregates to precom-pute <ref> [HRU96, GHRU97, SDN] </ref> using a given amount of space. On the other hand, in dynamic caching schemes, the cache contents vary dynamically, since new items may be inserted and old items may be removed from the cache.
Reference: [Gupt97] <author> H. Gupta. </author> <title> Selection of Views to Materialize in a Data Warehouse. </title> <booktitle> Proc. of the Sixth ICDT, </booktitle> <pages> 98-112, </pages> <year> 1997. </year>
Reference: [HRU96] <author> V. Harinarayanan, A. Rajaraman, J.D. Ullman. </author> <title> Implementing Data Cubes Efficiently, </title> <booktitle> Proc. ACM SIG-MOD Int. Conf. on Management of Data, </booktitle> <pages> 205-227, </pages> <year> 1996. </year>
Reference-contexts: A static approach is specifically suited for the backend where one can precom-pute aggregated tables. A set of group-bys is chosen and the corresponding tables are materialized. This refers to the problem of selecting what aggregates to precom-pute <ref> [HRU96, GHRU97, SDN] </ref> using a given amount of space. On the other hand, in dynamic caching schemes, the cache contents vary dynamically, since new items may be inserted and old items may be removed from the cache. <p> Schemes which make use of a profit metric consisting of the size and execution cost of a query are considered in [SSV96]. We use a similar replacement scheme which considers the "benefit" of a chunk. The notion of benefit of a group-by was introduced in <ref> [HRU96] </ref>. Since we do not perform aggregations in the middle tier, a group-by benefits only itself. The benefit of a chunk is measured by the fraction of the base table that it represents.
Reference: [OG95] <author> P. O'Neil, G. Graefe, </author> <title> Multi-Table Joins Through Bitmapped Join Indices. </title> <booktitle> SIGMOD Record, </booktitle> <pages> 8-11, </pages> <month> September </month> <year> 1995. </year>
Reference: [OQ97] <author> P. O'Neil, D. Quass, </author> <title> Improved Query Performance with Variant Indexes. </title> <booktitle> Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> 38-49, </pages> <year> 1997. </year>
Reference: [RJZN97] <author> K. Ramasamy, Q. Jin, Y. Zhao and J. F. Naughton. </author> <title> Bit-Map Indices: Implementation Issues and Performance Results. </title> <note> Working Paper. </note>
Reference-contexts: We implemented the chunked file in the Paradise [DKLP+94] Database System using option 1 above. The chunked file was implemented by using a B-Tree as a chunk index on a fact file. A Fact file <ref> [RJZN97] </ref> is a relational file optimized for storing and accessing the records in a fact table, which exploits the fixed length property of fact table records. It eliminates the slot overhead in the pages and provides a fast path for "skipped" sequential access.
Reference: [RK96] <author> R. Kimball. </author> <title> The Data Warehouse Toolkit, </title> <publisher> John Wi-ley & Sons, </publisher> <year> 1996. </year>
Reference: [SDJL96] <author> D. Srivastava, S. Dar, H. V. Jagadish and A. Y. Levy. </author> <title> Answering Queries with Aggregation Using Views Proc. </title> <booktitle> of the 22nd Int. VLDB Conf., </booktitle> <year> 1996. </year>
Reference-contexts: Some of them have significant applicability to the domain of data warehousing and OLAP. The problem of answering queries using views in the presence of aggregations has been studied extensively in <ref> [SDJL96] </ref>. They describe the conditions under which a new query can be answered using cached aggregate views. A query need not be answered from a single view, but can be decomposed into multiple queries each of which is answered from a different view.
Reference: [SDN] <author> A. Shukla, P.M. Deshpande, J.F. Naughton, </author> <note> Submitted for VLDB 1998. </note>
Reference-contexts: A static approach is specifically suited for the backend where one can precom-pute aggregated tables. A set of group-bys is chosen and the corresponding tables are materialized. This refers to the problem of selecting what aggregates to precom-pute <ref> [HRU96, GHRU97, SDN] </ref> using a given amount of space. On the other hand, in dynamic caching schemes, the cache contents vary dynamically, since new items may be inserted and old items may be removed from the cache.
Reference: [SDNR96] <author> A. Shukla, P.M. Deshpande, J.F. Naughton, K. Ramasamy, </author> <title> Storage Estimation for Multidimensional Aggregates in the Presence of Hierarchies, </title> <booktitle> Proc. of the 22nd Int. VLDB Conf., </booktitle> <pages> 522-531, </pages> <year> 1996. </year>
Reference: [SS94] <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient Organization of Large Multidimensional Arrays. </title> <booktitle> Proc. of the 11th Int. Conf. on Data Engg., </booktitle> <year> 1994. </year>
Reference-contexts: Instead of storing a large array in simple row major or column major order, they are broken down into chunks and stored in a chunked format <ref> [SS94, ZDN97] </ref>. The distinct values for each dimension are divided into ranges, and chunks are created based on this division. Figure 1 shows how the multidimensional space can be broken up into chunks. <p> This suggests that the chunk range at any level in the hierarchy should be a proportional to the number of distinct values of the dimension at that level. This agrees with a similar observation made by <ref> [SS94] </ref> in the context of multi-dimensional arrays. The missing chunks are computed by scanning the corresponding chunks at the backend. Since disk I/O is in units of pages, the average chunk size should be a multiple of the page size to reduce the overhead of extra I/O.
Reference: [SSV96] <author> P. Scheuermann, J. Shim and R. </author> <title> Vingralek WATCHMAN : A Data Warehouse Intelligent Cache Manager Proc. </title> <booktitle> of the 22nd Int. VLDB Conf., </booktitle> <year> 1996. </year>
Reference-contexts: However, their approach is limited by query containment, since the new query has to be computable from the set of available views. Cache replacement and admission schemes specific to warehousing have been studied in <ref> [SSV96] </ref>. In their work, they use a profit metric for queries to decide whether a query should be cached or not. We use similar ideas in our replacement policies. <p> This cost has to be incorporated into any cache replacement policy. Schemes which make use of a profit metric consisting of the size and execution cost of a query are considered in <ref> [SSV96] </ref>. We use a similar replacement scheme which considers the "benefit" of a chunk. The notion of benefit of a group-by was introduced in [HRU96]. Since we do not perform aggregations in the middle tier, a group-by benefits only itself. <p> The average execution time of the last 100 queries in the query stream. 2. Cost Saving Ratio This metric was defined in <ref> [SSV96] </ref>, and is a measure of the percentage of the total cost of the queries saved due to hits in the cache. It uses an estimate of the cost of executing a query at the backend to compute the savings in cost due to a cache hit.
Reference: [Ull96] <author> J.D. Ullman, </author> <title> Efficient Implementation of Data Cubes Via Materialized Views A survey of the field for the 1996 KDD conference. </title>
Reference: [ZDN97] <author> Y. Zhao, P.M. Deshpande, J.F. Naughton. </author> <title> An Array-Based Algorithm for Simultaneous Multidimensional Aggregates, </title> <booktitle> Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> 159-170, </pages> <year> 1997. </year>
Reference-contexts: Instead of storing a large array in simple row major or column major order, they are broken down into chunks and stored in a chunked format <ref> [SS94, ZDN97] </ref>. The distinct values for each dimension are divided into ranges, and chunks are created based on this division. Figure 1 shows how the multidimensional space can be broken up into chunks.
References-found: 20

