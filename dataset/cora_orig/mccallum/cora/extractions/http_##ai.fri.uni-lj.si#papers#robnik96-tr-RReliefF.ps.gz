URL: http://ai.fri.uni-lj.si/papers/robnik96-tr-RReliefF.ps.gz
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: fMarko.Robnik, Igor.Kononenkog@fer.uni-lj.si  
Phone: tel.: +386-61-1768386, fax: +386-61-1768386,  
Title: Non-myopic attribute estimation in regression  
Author: Marko Robnik- Sikonja, Igor Kononenko 
Keyword: regression, attribute estimation, regression trees  
Address: Trzaska 25, 61001 Ljubljana, Slovenia,  
Affiliation: University of Ljubljana, Faculty of Computer and Information Science,  
Abstract: One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. Heuristic measures mostly assume independence of attributes and therefore cannot be successfully used in domains with strong dependencies between attributes. Relief and its extension ReliefF are statistical methods capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. Following the analysis of ReliefF we have extended it to continuous class problems. Regressional ReliefF (RReliefF) and ReliefF provide a unified view on estimation of quality of attributes. The experiments show that RReliefF successfully estimates the quality of attributes and can be used for non-myopic learning of regression trees. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, L., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <publisher> Wadsworth Inc., </publisher> <address> Belmont, California. </address>
Reference-contexts: The majority of the current propositional inductive learning systems predict discrete class. They can solve also continuous class problems by discretiz-ing the class in advance. This approach is often inappropriate. Regression learning systems eg. CART <ref> (Breiman et al., 1984) </ref>, Retis (Karalic, 1992), M5 (Quinlan, 1992), predict continuous class directly. One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. <p> One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. Heuristic measures for estimating attributes' quality mostly assume independence of attributes (eg. information gain (Hunt et al., 1966), gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989) and j-measure (Smyth and Goodman, 1990) for discrete class and mean squared error (Breiman et al., 1984) for continuous class) and therefore cannot be successfully used in domains with strong dependencies between attributes. <p> Heuristic measures for estimating attributes' quality mostly assume independence of attributes (eg. information gain (Hunt et al., 1966), gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989) and j-measure (Smyth and Goodman, 1990) for discrete class and mean squared error (Breiman et al., 1984) for continuous class) and therefore cannot be successfully used in domains with strong dependencies between attributes. <p> Firstly we have examined the ability of RReliefF to recognize and rank important attributes, and then we have tested it in regression tree building. We compare the estimates of RReliefF with the mean squared error as the measure of the attribute's quality <ref> (Breiman et al., 1984) </ref>. This measure is standard in regression tree systems. <p> Estimator A 1 A 2 A 3 R best ? RReliefF-e 0.023 0.008 0.014 0.003 p RReliefF-k 0.022 0.003 0.006 -0.001 p mean squared error (equation 9). If the discrete attribute is selected it is discretized optimally according to the purity of the split <ref> (Breiman et al., 1984) </ref> and if continuous attribute is chosen, the best split point is found with the same criterion. For simplicity reason our system has no pruning capabilities.
Reference: <author> Elomaa, T. and Ukkonen, E. </author> <year> (1994). </year> <title> A geometric approach to feature selection. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Proceedings of Euro-pean Conference on Machine Learning, </booktitle> <pages> pages 351-354. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Relief (Kira and Rendell, 1992a; Kira and Rendell, 1992b) and its extended version ReliefF (Kononenko, 1994) are capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are contextual merit (Hong, 1994) and geometrical approach <ref> (Elomaa and Ukkonen, 1994) </ref>. Following the analysis of ReliefF we have extended it to continuous class problems. 1 In the next Section we present and analyze the novel RReliefF (Regres--sional ReliefF) algorithm.
Reference: <author> Hong, S. J. </author> <year> (1994). </year> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report RC19664, </type> <institution> IBM. </institution> <note> to appear in IEEE Trans. on Knowledge and Data Engineering. </note>
Reference-contexts: Relief (Kira and Rendell, 1992a; Kira and Rendell, 1992b) and its extended version ReliefF (Kononenko, 1994) are capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are contextual merit <ref> (Hong, 1994) </ref> and geometrical approach (Elomaa and Ukkonen, 1994). Following the analysis of ReliefF we have extended it to continuous class problems. 1 In the next Section we present and analyze the novel RReliefF (Regres--sional ReliefF) algorithm. <p> If A i is the continuous attribute, dif f (A i ; 2; 5) = 7 0:43. So, with this form of dif f function continuous attributes are underestimated. We can overcome this problem with the ramp function as proposed by <ref> (Hong, 1994) </ref>.
Reference: <author> Hunt, E., Martin, J., and Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. Heuristic measures for estimating attributes' quality mostly assume independence of attributes (eg. information gain <ref> (Hunt et al., 1966) </ref>, gini index (Breiman et al., 1984), distance measure (Mantaras, 1989) and j-measure (Smyth and Goodman, 1990) for discrete class and mean squared error (Breiman et al., 1984) for continuous class) and therefore cannot be successfully used in domains with strong dependencies between attributes.
Reference: <author> Karalic, A. </author> <year> (1992). </year> <title> Employing linear regression in regression tree leaves. </title> <editor> In Neumann, B., editor, </editor> <booktitle> Proceedings of ECAI'92, </booktitle> <pages> pages 440-441. </pages> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The majority of the current propositional inductive learning systems predict discrete class. They can solve also continuous class problems by discretiz-ing the class in advance. This approach is often inappropriate. Regression learning systems eg. CART (Breiman et al., 1984), Retis <ref> (Karalic, 1992) </ref>, M5 (Quinlan, 1992), predict continuous class directly. One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes.
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992a). </year> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> In Proc. AAAI'92. </booktitle>
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title> <editor> In D.Sleeman and P.Edwards, editors, </editor> <booktitle> Proc. Intern. Conf. on Machine Learning, </booktitle> <pages> pages 249-256. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: analysis and extensions of Relief. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Proceedings of European Conference on Machine Learning, </booktitle> <pages> pages 171-182. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Relief (Kira and Rendell, 1992a; Kira and Rendell, 1992b) and its extended version ReliefF <ref> (Kononenko, 1994) </ref> are capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are contextual merit (Hong, 1994) and geometrical approach (Elomaa and Ukkonen, 1994). <p> When the number of instances is too small for ReliefF one should switch to estimating the quality of attributes with ordinary impurity measures. Further study shall develop techniques to detect the appropriate point in the tree building to make such a switch. Analogously to extensions in ReliefF <ref> (Kononenko, 1994) </ref> we have extended RReliefF with handling of noisy and incomplete data and preliminary results show promising robustness. Both, RReliefF in regression and ReliefF in classification are estimators of equation 3, which gives a unified view on the estimation of quality of attributes for classification and regression. 12
Reference: <author> Kononenko, I., Simec, E., and Robnik Sikonja, M. </author> <year> (1996). </year> <title> Overcoming the myopia of inductive learning algorithms with ReliefF. </title> <journal> Applied Intelligence. </journal> <note> (in press). </note>
Reference-contexts: Its use in learning of regression trees seems promising. The RReliefF's estimates as well as the ReliefF's estimates in classification <ref> (Kononenko et al., 1996) </ref> become unreliable with small number of examples. It seems that in such situation both variants of ReliefF tend to overfit the data. When the number of instances is too small for ReliefF one should switch to estimating the quality of attributes with ordinary impurity measures.
Reference: <author> Mantaras, R. </author> <year> (1989). </year> <title> ID3 revisited: A distance based criterion for attribute selection. </title> <booktitle> In Proceedings of Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, USA. </address>
Reference-contexts: Heuristic measures for estimating attributes' quality mostly assume independence of attributes (eg. information gain (Hunt et al., 1966), gini index (Breiman et al., 1984), distance measure <ref> (Mantaras, 1989) </ref> and j-measure (Smyth and Goodman, 1990) for discrete class and mean squared error (Breiman et al., 1984) for continuous class) and therefore cannot be successfully used in domains with strong dependencies between attributes.
Reference: <author> Quinlan, J. </author> <year> (1992). </year> <title> Learning with continuous classes. </title> <booktitle> In Proceedings of AI'92. </booktitle> <publisher> World Scientific. </publisher>
Reference-contexts: The majority of the current propositional inductive learning systems predict discrete class. They can solve also continuous class problems by discretiz-ing the class in advance. This approach is often inappropriate. Regression learning systems eg. CART (Breiman et al., 1984), Retis (Karalic, 1992), M5 <ref> (Quinlan, 1992) </ref>, predict continuous class directly. One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes.
Reference: <author> Smyth, P. and Goodman, R. </author> <year> (1990). </year> <title> Rule induction using information theory. </title> <editor> In Piatetsky-Shapiro, G. and Frawley, W., editors, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Heuristic measures for estimating attributes' quality mostly assume independence of attributes (eg. information gain (Hunt et al., 1966), gini index (Breiman et al., 1984), distance measure (Mantaras, 1989) and j-measure <ref> (Smyth and Goodman, 1990) </ref> for discrete class and mean squared error (Breiman et al., 1984) for continuous class) and therefore cannot be successfully used in domains with strong dependencies between attributes.
References-found: 12

