URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/CSL-TR-94-601.ps.z
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/
Root-URL: http://www.cs.panam.edu
Title: EFFICIENT SCHEDULING ON MULTIPROGRAMMED SHARED-MEMORY MULTIPROCESSORS  
Author: Andrew Tucker 
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By  
Date: December 1993  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. Sched-uler activations: </author> <title> Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 95-109, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> Executions of parallel workloads using process control, on a 4-processor Silicon Graphics workstation, show an 8-22% performance improvement over the performance on a standard UNIX scheduler. An approach similar to process control was developed concurrently with it at the University of Washington. This approach, scheduler activations <ref> [1] </ref>, will be discussed in detail in Section 6.5.1, along with its differences from process control. 1.4 Thesis Contributions This thesis addresses a number of issues concerning the problem of multiprogramming on multiprocessors. <p> For example, one can find several programming languages with runtime systems based on the task-queue model [6, 16, 19, 22], and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages <ref> [1, 7, 11] </ref> follow this model, as do many independently written applications [17, 32, 39]. In these cases, process control can be added to the library or runtime system without changing application programs at all. <p> Processor partitioning mechanisms have also been developed for use in allocating applications fixed numbers of processors for fixed periods of time. 6.5. RELATED WORK 63 6.5.1 Scheduler Activations Anderson et al. at the University of Washington <ref> [1] </ref> have developed a system that is similar in concept and motivation to the process control and processor partitioning approach proposed in our previously published paper [45] and further developed in later work [18] and this thesis.
Reference: [2] <author> Maurice J. Bach. </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: Processes are ranked by priority, and priority is derived from a base value plus the recent CPU usage of the process. Similar approaches are used in most UNIX systems <ref> [2, 23] </ref>. The IRIX system also includes some additional functionality, including an implementation of gang scheduling that will be discussed in Chapter 5. 2.2 Applications Once we have a machine and operating system on which to implement our scheduling approaches, we need workloads to execute.
Reference: [3] <author> Forest Baskett, Tom Jermoluk, and Doug Solomon. </author> <title> The 4D-MP graphics superworkstation: Computing + graphics = 40 MIPS + 40 MFLOPS and 100,000 lighted polygons per second. </title> <booktitle> In Proceedings of COMPCON '88, </booktitle> <pages> pages 468-471, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The quantitative results presented in this thesis are based on a Silicon Graphics (SGI) PowerStation 4D/340 multiprocessor <ref> [3] </ref>. The system consists of four 33 MHz MIPS R3000/R3010 processors on a shared bus and provides peak computing power of about 100 MIPS and 35 MFLOPS. Each processor has a 64 Kbyte instruction cache, a 64 Kbyte first-level (primary) data cache and a 256 Kbyte second-level (secondary) data cache.
Reference: [4] <author> David L. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: This form of scheduling was called coscheduling. ("Coscheduling" and "gang scheduling" are often used interchangeably in the literature. This thesis uses gang scheduling as that seems to be the preference of more recent authors. This should not be confused the use of gang scheduling by other authors <ref> [4] </ref> to refer to dedicating a "gang" of processors to a single application until that application completes.) Ousterhout proposed several methods for implementing gang scheduling on a multipro-grammed system. <p> Each processor set consists of a local run-queue and other related data structures. A high-level policy module is responsible for assigning both resources (processors) and tasks (application processes) to it. Each processor executes processes that have been assigned to its processor set in a regular time-sliced manner <ref> [4] </ref>, though this can be changed on a per-partition basis. It is possible to have a processor set with no processors assigned to it, in which case the processes assigned to it will simply be waiting in the run queue. <p> The runtime system could then employ an avoidance-based approach to process suspension, just as in scheduler activations, at the user level. 6.5.2 Processor Partitioning The processor partitioning kernel interface we use owes much of its structure to the implementation of processor sets developed for Mach <ref> [4] </ref>. In Mach, users may flexibly create and destroy processor sets, assign processes to different processor sets, and allocate processors to processor sets (security restrictions notwithstanding). This allows a variety of scheduling policies to be implemented by user-level servers, though initially only a simple batch-based scheduling policy was implemented.
Reference: [5] <author> James Boyle, Ewing Lusk, et al. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1987. </year>
Reference-contexts: In the standard SPLASH release of these applications (available by anonymous ftp from mojave.stanford.edu), parallelism is implemented using a collection of macros designed at the Argonne National Laboratory (ANL) <ref> [5] </ref>. However, in this thesis, alternate versions of the applications written in the COOL (Concurrent Object-Oriented Language) [6] parallel language are used. COOL provides basic tasking and synchronization mechanisms and an overall object-oriented framework for parallel programming. <p> The contents of this chapter are based on previously published work done jointly by the author along with Josep Torellas and Anoop Gupta [44]. The older work developed a workable implementation of affinity scheduling and applied it to a number of workloads consisting of parallel applications using ANL macros <ref> [5] </ref> to provide explicit parallelism. This chapter re-applies affinity scheduling to the workloads selected for this thesis so that its performance can be compared with that of gang scheduling and process control. This chapter is organized as follows. Section 4.1 characterizes the workloads from the standpoint of cache affinity.
Reference: [6] <author> Rohit Chandra, Anoop Gupta, and John Hennessy. </author> <title> COOL: A Language for Parallel Programming. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> In the standard SPLASH release of these applications (available by anonymous ftp from mojave.stanford.edu), parallelism is implemented using a collection of macros designed at the Argonne National Laboratory (ANL) [5]. However, in this thesis, alternate versions of the applications written in the COOL (Concurrent Object-Oriented Language) <ref> [6] </ref> parallel language are used. COOL provides basic tasking and synchronization mechanisms and an overall object-oriented framework for parallel programming. The basic mechanism is a collection of task queues containing the work to be performed in parallel. <p> Since task-queue based models are widely used to implement parallel applications on shared-memory architectures, the applicability of process control is quite large. For example, one can find several programming languages with runtime systems based on the task-queue model <ref> [6, 16, 19, 22] </ref>, and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications [17, 32, 39]. <p> Like process suspension, this can be encapsulated within a library or runtime 6.2. KERNEL-APPLICATION INTERACTION 51 system, transparently to the application programmer. For our implementation, we have added process control to the runtime system of COOL <ref> [6] </ref>, a task-queue based object-oriented programming language. To ensure safe suspension, we suspend a server process only when its task has finished or when its task blocks on a blocking synchronization primitive. 1 Processes can be created or resumed asynchronously without restriction.
Reference: [7] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> COOL was chosen over other task-queue-based systems primarily because of familiarity with the internals of its runtime system and because several large parallel applications have been implemented using it; 2.2. APPLICATIONS 9 other task-based parallel languages (such as Jade [22]) or even general-purpose user-level thread packages (such as C-threads <ref> [7] </ref>) would work as well. One goal of this research was to test the effect of workloads including "serial" and I/O-bound applications on scheduling performance. <p> For example, one can find several programming languages with runtime systems based on the task-queue model [6, 16, 19, 22], and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages <ref> [1, 7, 11] </ref> follow this model, as do many independently written applications [17, 32, 39]. In these cases, process control can be added to the library or runtime system without changing application programs at all.
Reference: [8] <author> Mark Crovella. </author> <title> The costs and benefits of coscheduling. </title> <type> Technical report, </type> <institution> University of Rochester Computer Science Department, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The performance of a number of multiprogrammed workloads improved by up to 10%, a small but significant increase. 1.2.2 Gang Scheduling Researchers have also proposed the use of gang scheduling strategies <ref> [31, 13, 14, 8] </ref> that ensure that all processes belonging to an application execute at the same time. The idea is that by scheduling all processes simultaneously, synchronization performance will be improved. <p> Ousterhout proposed several means of scheduling processes to achieve the desired goal. These approaches were later expanded on and used in several other systems <ref> [13, 14, 8, 35] </ref>. We will first discuss these approaches, then describe our approach to the problem and 41 42 CHAPTER 5. <p> The simplicity and effectiveness of the matrix method have made it the most commonly used 5.2. PREVIOUS APPROACHES 43 approach, both in Medusa and in modified forms in later systems. 5.2.2 Psyche The Psyche project at the University of Rochester also looked into implementing gang scheduling <ref> [8, 9] </ref>. The Psyche implementation was based on Ousterhout's matrix method, but made several changes to make it more effective on real machines. The foremost variation was the method for coercing processors to select processes from the same application.
Reference: [9] <author> Mark Crovella, Prakash Das, Cezary Dubnicki, Tom LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Computing, </booktitle> <pages> pages 590-597, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The simplicity and effectiveness of the matrix method have made it the most commonly used 5.2. PREVIOUS APPROACHES 43 approach, both in Medusa and in modified forms in later systems. 5.2.2 Psyche The Psyche project at the University of Rochester also looked into implementing gang scheduling <ref> [8, 9] </ref>. The Psyche implementation was based on Ousterhout's matrix method, but made several changes to make it more effective on real machines. The foremost variation was the method for coercing processors to select processes from the same application. <p> In sum, the advent of scalable shared-memory machines creates a great opportunity for improving the performance of multiprogrammed workloads with intelligent scheduling policies. Several researchers have begun to work on these problems in the context of NUMA machines <ref> [9, 14, 34, 49] </ref>. 8.3 Final Remarks In conclusion, process control has been shown to be a simple but effective approach to improving the performance of parallel applications on multiprogrammed systems, one that may be cleanly integrated into a traditional operating system.
Reference: [10] <author> Murthy Devarakonda and Arup Mukherjee. </author> <title> Issues in implementation of cache-affinity scheduling. </title> <booktitle> In Proceedings Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: This technique is called cache affinity scheduling. Cache affinity scheduling has been the subject of several previous studies. However, many of these studies were based on analytical modeling [41], simulations [18], or synthetic applications <ref> [10] </ref>, missing the subtle effects of real applications running on a real machine. Others studied uniprocessors [30], single applications within a narrow domain [42], or systems with unusual (space-sharing) scheduling policies [46]. <p> Trace-driven simulations of real applications on a multiprocessor by the author along with Gupta and Urushibara [18] similarly point to small benefits of affinity, but again a small time quantum was used (10 ms). Also, I/O and other load variations were not considered. A study by Devarakonda and Mukherjee <ref> [10] </ref> evaluated the performance of a real implementation of cache affinity on a multiprocessor, but used synthetic workloads.
Reference: [11] <author> Thomas W. Doeppner, Jr. </author> <title> Threads: A system for the support of concurrent programming. </title> <type> Technical Report CS-87-11, </type> <institution> Brown University, </institution> <year> 1987. </year> <note> 82 BIBLIOGRAPHY 83 </note>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> For example, one can find several programming languages with runtime systems based on the task-queue model [6, 16, 19, 22], and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages <ref> [1, 7, 11] </ref> follow this model, as do many independently written applications [17, 32, 39]. In these cases, process control can be added to the library or runtime system without changing application programs at all.
Reference: [12] <author> Iain S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: A typical parallel task consists of one panel updating another panel in the matrix. The primary data structure is the matrix itself, stored by columns with arrays of non-zero elements and their corresponding rows. The experiments in this thesis used bcsttk18, a standard matrix from the Boeing/Harwell test set <ref> [12] </ref>. The matrix is fairly large, 11948 fi 11948, with 68571 non-zero elements. The unfactored matrix takes about 1 megabyte; the factored matrix uses 5.7 megabytes. There are 7438 supernodes in the input matrix, of sizes up to 208 columns.
Reference: [13] <author> Jan Edler, Jim Lipkis, and Edith Schonberg. </author> <title> Process management for highly parallel UNIX systems. </title> <booktitle> In Proceedings of the USENIX Workshop on UNIX and Supercomputers, </booktitle> <pages> pages 1-17, </pages> <year> 1988. </year>
Reference-contexts: Other researchers have proposed keeping spinning synchronization but using intelligent schedulers that are given enough knowledge about the applications to avoid preempting processes while they are inside a critical section <ref> [13, 47] </ref>. 1.2.1 Cache Affinity The above approaches, while helping the problem of processes wasting time waiting for locks held by preempted processes, do not address the problem of cache behavior in a multiprogrammed system. <p> The performance of a number of multiprogrammed workloads improved by up to 10%, a small but significant increase. 1.2.2 Gang Scheduling Researchers have also proposed the use of gang scheduling strategies <ref> [31, 13, 14, 8] </ref> that ensure that all processes belonging to an application execute at the same time. The idea is that by scheduling all processes simultaneously, synchronization performance will be improved. <p> Ousterhout proposed several means of scheduling processes to achieve the desired goal. These approaches were later expanded on and used in several other systems <ref> [13, 14, 8, 35] </ref>. We will first discuss these approaches, then describe our approach to the problem and 41 42 CHAPTER 5.
Reference: [14] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The performance of a number of multiprogrammed workloads improved by up to 10%, a small but significant increase. 1.2.2 Gang Scheduling Researchers have also proposed the use of gang scheduling strategies <ref> [31, 13, 14, 8] </ref> that ensure that all processes belonging to an application execute at the same time. The idea is that by scheduling all processes simultaneously, synchronization performance will be improved. <p> Ousterhout proposed several means of scheduling processes to achieve the desired goal. These approaches were later expanded on and used in several other systems <ref> [13, 14, 8, 35] </ref>. We will first discuss these approaches, then describe our approach to the problem and 41 42 CHAPTER 5. <p> Also, the excessive number of interrupts can add substantial overhead to the system. We will look further into the performance of the SGI implementation in Section 5.4. Feitelson and Rudolph <ref> [14] </ref> also investigated gang scheduling, in a somewhat different environment. They were interested in using gang scheduling in a hierarchical fashion on distributed machines. They point out the possibility of contention with a centralized gang scheduling system, and organize their gang scheduler in a distributed fashion. <p> System-wide gang scheduling may be unreasonable when the cost of migrating data between clusters is high; however, a hierarchical gang scheduler (as suggested by Feitelson and Rudolph <ref> [14] </ref>) where applications are gang-scheduled within clusters or small groups of clusters might work well. In sum, the advent of scalable shared-memory machines creates a great opportunity for improving the performance of multiprogrammed workloads with intelligent scheduling policies. <p> In sum, the advent of scalable shared-memory machines creates a great opportunity for improving the performance of multiprogrammed workloads with intelligent scheduling policies. Several researchers have begun to work on these problems in the context of NUMA machines <ref> [9, 14, 34, 49] </ref>. 8.3 Final Remarks In conclusion, process control has been shown to be a simple but effective approach to improving the performance of parallel applications on multiprogrammed systems, one that may be cleanly integrated into a traditional operating system.
Reference: [15] <author> Stuart I. Feldman. </author> <title> Make: A program for maintaining computer programs. </title> <type> Technical Report 57, </type> <institution> Computing Science, Bell Laboratories, </institution> <year> 1977. </year>
Reference-contexts: With these parameters, the array of records takes 200 Kbytes of memory; if we include per-process private data structures, the entire application takes 230 Kbytes. 2.2.6 Pmake Pmake is a parallel version of the standard UNIX make command <ref> [15] </ref>. It spawns multiple simultaneous compilation threads up to a predefined limit. The result is a number of simultaneously running I/O-dependent processes with very little intercommunication and synchronization.
Reference: [16] <author> Richard P. Gabriel and John McCarthy. </author> <booktitle> Queue-based multi-processing Lisp. In ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 25-43, </pages> <year> 1984. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> Since task-queue based models are widely used to implement parallel applications on shared-memory architectures, the applicability of process control is quite large. For example, one can find several programming languages with runtime systems based on the task-queue model <ref> [6, 16, 19, 22] </ref>, and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications [17, 32, 39].
Reference: [17] <author> Anoop Gupta, Charles Forgy, Dirk Kalp, Allen Newell, and Milind Tambe. </author> <title> Parallel implementation of OPS5 on the Encore multiprocessor: Results and analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(2), </volume> <year> 1988. </year>
Reference-contexts: Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications <ref> [17, 32, 39] </ref>. In these cases, process control can be added to the library or runtime system without changing application programs at all.
Reference: [18] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of SIGMETRICS '91, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: One approach that has been used to address the problem of processes being preempted while inside critical sections is to use blocking or two-phase synchronization primitives instead of busy-waiting primitives <ref> [31, 18] </ref>. With blocking synchronization, processes yield the processor if they are unable to acquire a lock, allowing other processes to run and reducing the time until the process inside the critical section is rescheduled (assuming it is currently unscheduled). <p> If the process holding the lock is unscheduled, the waiting process will block before wasting much processor time. Previous studies of this approach <ref> [18, 21] </ref> found it fairly effective and suggest that if the duration of spinning before blocking is fixed, the duration should be close to the duration of a context switch. <p> To address the cache hit-rate problem, 4 CHAPTER 1. INTRODUCTION scheduling strategies that use cache affinity (the amount of relevant data a process has in some processor's cache) to determine which process to schedule on a processor have been proposed <ref> [41, 18] </ref>. Unfortunately, these techniques have had only limited success in real implementations [46]. One problem with cache affinity scheduling is that it is difficult to integrate into a general-purpose system where response time and fair allocation of processors are important. <p> Simulation studies, done jointly by the author along with Anoop Gupta and Shigeru Urushibara, found that parallel applications using spinning synchronization in a multiprogrammed environment often spend over 50% of their execution time waiting for preempted processes <ref> [18] </ref>. Another earlier work by the author and Anoop Gupta found that reducing context switching by using process control improved the performance of multiprogrammed applications using spinning synchronization by over 200%, largely by reducing the amount of busy-waiting. <p> This technique is called cache affinity scheduling. Cache affinity scheduling has been the subject of several previous studies. However, many of these studies were based on analytical modeling [41], simulations <ref> [18] </ref>, or synthetic applications [10], missing the subtle effects of real applications running on a real machine. Others studied uniprocessors [30], single applications within a narrow domain [42], or systems with unusual (space-sharing) scheduling policies [46]. <p> In contrast to our studies, however, their study was confined to process switching on a single processor. Their conclusions were also based on the use of a fairly short 16 ms time quantum. Trace-driven simulations of real applications on a multiprocessor by the author along with Gupta and Urushibara <ref> [18] </ref> similarly point to small benefits of affinity, but again a small time quantum was used (10 ms). Also, I/O and other load variations were not considered. <p> Although it results in excess processes for a brief time, the processes can be simply scheduled onto the processors in a round-robin manner, and continue to do useful work. This is especially true for applications using two-phase rather than spinning synchronization primitives <ref> [18] </ref>. If the number of runnable processes is less than the number of processors, however, one or more processors will sit idle. Although this may be reasonable for very short periods to avoid excess context switching, any long idle time will result in performance loss. <p> In our implementation, we minimize such idle time by always using blocking synchronization primitives with a small amount of spin time before blocking. By making the spin time before blocking equal to the context switching time, we can ensure good performance for both short and long critical sections <ref> [18] </ref>. However, there still remains the disadvantage of worse cache behavior when there are excess processes. <p> RELATED WORK 63 6.5.1 Scheduler Activations Anderson et al. at the University of Washington [1] have developed a system that is similar in concept and motivation to the process control and processor partitioning approach proposed in our previously published paper [45] and further developed in later work <ref> [18] </ref> and this thesis.
Reference: [19] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> Since task-queue based models are widely used to implement parallel applications on shared-memory architectures, the applicability of process control is quite large. For example, one can find several programming languages with runtime systems based on the task-queue model <ref> [6, 16, 19, 22] </ref>, and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications [17, 32, 39].
Reference: [20] <author> Kieran Harty and David R. Cheriton. </author> <title> Application-controlled physical memory using external page-cache management. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 187-197, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: CONCLUSIONS becomes increasingly important to inform them about kernel-level events. Process control is only one example of the use that can be made of information about resource management. Others have considered informing applications of the state of the virtual memory system <ref> [20] </ref>; an application might try to reduce its memory usage in an attempt to keep its working set in physical memory. In a NUMA machine that allowing process migration, it can be important to give applications information on memory and process placement.
Reference: [21] <author> Anna R. Karlin, Kai Li, Mark S. Manasse, and Susan Owicki. </author> <title> Empirical studies of competitive spinning for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: If the process holding the lock is unscheduled, the waiting process will block before wasting much processor time. Previous studies of this approach <ref> [18, 21] </ref> found it fairly effective and suggest that if the duration of spinning before blocking is fixed, the duration should be close to the duration of a context switch. <p> More complex solutions have also been proposed involving adapting the spin duration to the specific lock in question based on previous history <ref> [21] </ref>.
Reference: [22] <author> Monica S. Lam and Martin Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: By dynamically maintaining a match, context switches are largely eliminated and good cache and synchronization behavior can be ensured. The process control approach is most easily applied to the wide variety of parallel applications that are written using the task-queue or threads 1.4. THESIS CONTRIBUTIONS 5 style <ref> [1, 6, 7, 11, 16, 19, 22] </ref>, where user-level tasks (threads) are scheduled onto a number of kernel-scheduled server processes. In such an environment, the number of server processes can be safely changed without affecting the running of user-level tasks. <p> COOL was chosen over other task-queue-based systems primarily because of familiarity with the internals of its runtime system and because several large parallel applications have been implemented using it; 2.2. APPLICATIONS 9 other task-based parallel languages (such as Jade <ref> [22] </ref>) or even general-purpose user-level thread packages (such as C-threads [7]) would work as well. One goal of this research was to test the effect of workloads including "serial" and I/O-bound applications on scheduling performance. <p> Since task-queue based models are widely used to implement parallel applications on shared-memory architectures, the applicability of process control is quite large. For example, one can find several programming languages with runtime systems based on the task-queue model <ref> [6, 16, 19, 22] </ref>, and consequently all programs written in these languages follow the model. Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications [17, 32, 39].
Reference: [23] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Processes are ranked by priority, and priority is derived from a base value plus the recent CPU usage of the process. Similar approaches are used in most UNIX systems <ref> [2, 23] </ref>. The IRIX system also includes some additional functionality, including an implementation of gang scheduling that will be discussed in Chapter 5. 2.2 Applications Once we have a machine and operating system on which to implement our scheduling approaches, we need workloads to execute.
Reference: [24] <author> Dan Lenoski, Kourosh Gharachorloo, James Laudon, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> Design of scalable shared-memory multiprocessors: The DASH approach. </title> <booktitle> In Proceedings of COMPCON '90, </booktitle> <pages> pages 62-67, </pages> <month> February </month> <year> 1990. </year> <note> 84 BIBLIOGRAPHY </note>
Reference-contexts: All caches are direct-mapped and have 16-byte blocks. (The machine has been slightly modified for use as a 4-processor cluster of the DASH distributed-memory multiprocessor <ref> [24] </ref>. An unmodified SGI 4D/340 has a second-level cache with a fetch size of 64 bytes and a miss cost of 50 cycles.) As a result of the large miss penalties, it is very important to have high cache-hit rates to get high processor utilizations. <p> This is particularly true of the new generation of NUMA machines, where the miss latency varies and can be very long for remote misses. For example, on the cluster-based DASH machine developed at Stanford <ref> [24] </ref>, a miss that is not filled by the secondary cache can take as long as 130 processor cycles. As the latency increases, the time taken to fill the cache increases, and thus affinity becomes more important.
Reference: [25] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Rather, we have found that the slightly sluggish response of our approach helps prevent many short duration suspensions and resumptions of processes, which if done immediately would have degraded system performance. This would be even more true in large-scale multiprocessors <ref> [25] </ref> where the cost of cache misses can be very high, with each miss costing tens or hundreds of processor cycles. <p> The alternative is to build a scalable system, using a point-to-point network connecting clusters of one or more processors on a bus. Shared-memory mechanisms can still be supported in hardware. The Stanford DASH machine <ref> [25] </ref> is such a machine, with 4-processor clusters. 1 On many scalable machines, remote memory accesses (accessing memory in another cluster) take much longer than local memory accesses (accessing memory in the same cluster). 2 These are also called NUMA (Non-Uniform Memory Access) machines because of this effect.
Reference: [26] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of SIGMETRICS '90, </booktitle> <pages> pages 226-236, </pages> <year> 1990. </year>
Reference-contexts: Other researchers have similarly found that allocating CPU time fairly to each job, regardless of type, is the best way to minimize the mean running time of all jobs in the system <ref> [26] </ref>. 7.2 Response Time Issues for Interactive Applications Another interesting class of serial applications is interactive programs, where the applications sleep for long periods of time broken by short periods of CPU use. Applications of this type abound on timesharing systems, from network servers to editors.
Reference: [27] <author> Shikharesh Majumdar, Derek L. Eager, and Richard B. Bunt. </author> <title> Scheduling in multipro-grammed parallel systems. </title> <booktitle> In Proceedings of SIGMETRICS '88, </booktitle> <pages> pages 104-113, </pages> <year> 1988. </year>
Reference-contexts: The importance of using such information in scheduling decisions has previously been emphasized by Majumdar et al. <ref> [27] </ref> and Zahorjan and McCann [48]. In a recent report, McCann et al. [28] empirically evaluate the use of such a "dynamic" processor allocation approach and the results show noticeable benefits when running applications with highly variable parallelism.
Reference: [28] <author> Cathy McCann, Raj Vaswani, and John Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed, shared memory multiprocessors. </title> <type> Technical Report 90-03-02, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: The importance of using such information in scheduling decisions has previously been emphasized by Majumdar et al. [27] and Zahorjan and McCann [48]. In a recent report, McCann et al. <ref> [28] </ref> empirically evaluate the use of such a "dynamic" processor allocation approach and the results show noticeable benefits when running applications with highly variable parallelism.
Reference: [29] <author> Jeffrey D. McDonald and Donald Baganoff. </author> <title> Vectorization of a particle simulation method for hypersonic rarified flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: These parameters force a number of time steps resulting in a highly accurate simulation 10 CHAPTER 2. EXPERIMENTAL ENVIRONMENT for problems of mesoscale (one to a few hundred kilometers) resolution. The active data set size with this grid size is about 3.2 Mbytes. 2.2.3 MP3D MP3D <ref> [29] </ref> is a particle-based simulator used to study the pressure and temperature profiles created as an object flies at high speed through the upper atmosphere.
Reference: [30] <author> Jeffrey C. Mogul and Anita Borg. </author> <title> The effect of context switches on cache performance. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This problem can also also occur on a serial machine, though without the process migration component. Previous researchers have investigated the effect of context switching on the performance of applications on a serial machine, both using analytical techniques and using memory access traces of real executions <ref> [30, 43] </ref>. They found that context switching can have a significant effect on performance even in serial environments. The above data on individual applications can be used to estimate the effect of cache data loss on multiprogrammed workloads. <p> Cache affinity scheduling has been the subject of several previous studies. However, many of these studies were based on analytical modeling [41], simulations [18], or synthetic applications [10], missing the subtle effects of real applications running on a real machine. Others studied uniprocessors <ref> [30] </ref>, single applications within a narrow domain [42], or systems with unusual (space-sharing) scheduling policies [46]. This chapter makes a more thorough attempt to understand the effect of cache affinity scheduling on a real system running a variety of applications. <p> They conclude that unconditionally fixing processes onto processors, while allowing processes to reuse more cache state, causes too much load imbalance which results in fairness problems and idle time. Mogul and Borg <ref> [30] </ref> used address traces of a variety of real applications to study the potential for cache reuse. They found cache reload overheads of up to 8%, depending on the workload. In contrast to our studies, however, their study was confined to process switching on a single processor.
Reference: [31] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: One approach that has been used to address the problem of processes being preempted while inside critical sections is to use blocking or two-phase synchronization primitives instead of busy-waiting primitives <ref> [31, 18] </ref>. With blocking synchronization, processes yield the processor if they are unable to acquire a lock, allowing other processes to run and reducing the time until the process inside the critical section is rescheduled (assuming it is currently unscheduled). <p> The performance of a number of multiprogrammed workloads improved by up to 10%, a small but significant increase. 1.2.2 Gang Scheduling Researchers have also proposed the use of gang scheduling strategies <ref> [31, 13, 14, 8] </ref> that ensure that all processes belonging to an application execute at the same time. The idea is that by scheduling all processes simultaneously, synchronization performance will be improved. <p> As in Chapter 4, we wish to minimize changes to the standard UNIX scheduler, and preserve fair allocation of processor time and reasonable response time. 5.2 Previous Approaches Gang scheduling was first proposed by Ousterhout as part of the Medusa project at Carnegie-Mellon University <ref> [31] </ref>. Ousterhout proposed several means of scheduling processes to achieve the desired goal. These approaches were later expanded on and used in several other systems [13, 14, 8, 35]. We will first discuss these approaches, then describe our approach to the problem and 41 42 CHAPTER 5.
Reference: [32] <author> Jonathan Rose. LocusRoute: </author> <title> A parallel global router for standard cells. </title> <booktitle> In Proceedings of the 25th ACM/IEEE Design Automation Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The data is gathered from a Silicon Graphics 4D/340 multiprocessor with 4 processors. The graph shows the finish time for two parallel applications, LocusRoute and Ocean, when the two are started at the same time and as the number of processes is varied. LocusRoute <ref> [32] </ref> is a VLSI standard-cell router, and Ocean [37] is an ocean basin current simulator. (These applications, and the machine on which they are run, are discussed in detail in Chapter 2.) Each application breaks its problem into a number of tasks, which are scheduled onto the processes executing that application. <p> Pmake is a component of the IRIX operating system used on Silicon Graphics machines. We will now look at the purpose, structure, and memory requirements of these applications in a little more detail. 2.2.1 LocusRoute LocusRoute <ref> [32] </ref> is a commercial-quality global router for VLSI standard cells. It uses an iterative algorithm to route wires with the goal of minimizing overall chip area. <p> Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications <ref> [17, 32, 39] </ref>. In these cases, process control can be added to the library or runtime system without changing application programs at all.
Reference: [33] <author> Edward Rothberg and Anoop Gupta. </author> <title> Techniques for improving the performance of sparse matrix factorization on multiprocessor workstations. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: With these parameters, the particles takes 3.4 Mbytes of memory, and the space array takes 160 Kbytes. Larger problems should behave similarly. 2.2.4 Cholesky The Cholesky <ref> [33] </ref> application is used for factoring sparse, symmetric, positive definite matrices. Factorization of positive definite matrices is important in a number of domains including structural analysis and device and process simulation.
Reference: [34] <author> Sanjeev K. Setia, Mark S. Squillante, and Satish K. Tripathi. </author> <title> Processor scheduling on multiprogrammed, distributed memory multiprocessors. </title> <booktitle> In Proceedings of SIGMETRICS '93, </booktitle> <pages> pages 158-170, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In sum, the advent of scalable shared-memory machines creates a great opportunity for improving the performance of multiprogrammed workloads with intelligent scheduling policies. Several researchers have begun to work on these problems in the context of NUMA machines <ref> [9, 14, 34, 49] </ref>. 8.3 Final Remarks In conclusion, process control has been shown to be a simple but effective approach to improving the performance of parallel applications on multiprogrammed systems, one that may be cleanly integrated into a traditional operating system.
Reference: [35] <institution> Silicon Graphics Inc. </institution> <note> IRIX 4.0 Programmer's Reference Manual, </note> <month> August </month> <year> 1991. </year>
Reference-contexts: Ousterhout proposed several means of scheduling processes to achieve the desired goal. These approaches were later expanded on and used in several other systems <ref> [13, 14, 8, 35] </ref>. We will first discuss these approaches, then describe our approach to the problem and 41 42 CHAPTER 5. <p> Also, if a gang-scheduled process blocks, the scheduler automatically schedules the next-highest priority runnable process on the local queue. This approach also involves no changes to the low-level process scheduler. 5.2.3 Other Approaches The Silicon Graphics IRIX operating system <ref> [35] </ref> used on the machine used for experiments in this thesis provides another, simpler, approach to gang scheduling. Under IRIX, applications can either be gang-scheduled or non-gang-scheduled.
Reference: [36] <author> Jaswinder Pal Singh and John L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The unfactored matrix takes about 1 megabyte; the factored matrix uses 5.7 megabytes. There are 7438 supernodes in the input matrix, of sizes up to 208 columns. The maximum panel size used in the computation contains 3240 non-zeros. 2.2.5 Water Water <ref> [36] </ref> is an N-body molecular dynamics application that evaluates forces and potentials in a 2.3. APPLICATION PERFORMANCE 11 system of water molecules in the liquid state. The computation performs a user-specified number of time steps, hopefully allowing the system to reach a steady state.
Reference: [37] <author> Jaswinder Pal Singh and John L. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experience, results, and implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1) </volume> <pages> 27-48, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The graph shows the finish time for two parallel applications, LocusRoute and Ocean, when the two are started at the same time and as the number of processes is varied. LocusRoute [32] is a VLSI standard-cell router, and Ocean <ref> [37] </ref> is an ocean basin current simulator. (These applications, and the machine on which they are run, are discussed in detail in Chapter 2.) Each application breaks its problem into a number of tasks, which are scheduled onto the processes executing that application. <p> The cost array for Primary2 contains 25800 routing cells (1290 in each of 20 channels), taking a total of 200 Kbytes of memory. The pin positions consume roughly 1 Mbyte. The routes are progressively stored as the application computes, finally amounting to about 600 Kbytes. 2.2.2 Ocean Ocean <ref> [37] </ref> simulates eddy currents in an ocean basin using a discretized quasi-geostrophic circulation model. The simulation is performed for many time steps until the eddies and ocean flow attain a mutual balance.
Reference: [38] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year> <note> BIBLIOGRAPHY 85 </note>
Reference-contexts: These include both closely communicating parallel applications and essentially serial applications. For the scheduling analysis, we primarily use workloads composed of five parallel applications selected from the SPLASH (Stanford ParalleL Applications for Shared Memory) suite of parallel benchmarks <ref> [38] </ref>. The SPLASH applications used in this thesis are LocusRoute, Ocean, MP3D, Cholesky, and Water. They are complex scientific and engineering programs representing the fields of VLSI design, oceanography, aeronautical simulation, numerical analysis, and molecular dynamics (described briefly below; more details can be found in the SPLASH report).
Reference: [39] <author> Larry Soule and Anoop Gupta. </author> <title> Characterization of parallelism and deadlocks in distributed digital logic simulation. </title> <booktitle> In Proceedings of the 26th ACM/IEEE Design Automation Conference, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Similarly, all applications written using threads packages [1, 7, 11] follow this model, as do many independently written applications <ref> [17, 32, 39] </ref>. In these cases, process control can be added to the library or runtime system without changing application programs at all.
Reference: [40] <author> M. Squillante and R. Nelson. </author> <title> Analysis of task migration in shared-memory multiprocessor scheduling. </title> <booktitle> In Proceedings of SIGMETRICS '91, </booktitle> <pages> pages 143-155, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Squillante and Lazows-ka [41] measure response time under different affinity-based scheduling policies. Their results suggest that affinity scheduling provides substantial benefits. However, their results are based on analytical calculations with simple machine and application models, rather than a real implementation. Another analytical study was done by Squillante and Nelson <ref> [40] </ref> studying the effects of migrating processes between processors. They conclude that unconditionally fixing processes onto processors, while allowing processes to reuse more cache state, causes too much load imbalance which results in fairness problems and idle time.
Reference: [41] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor-cache affinity in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: To address the cache hit-rate problem, 4 CHAPTER 1. INTRODUCTION scheduling strategies that use cache affinity (the amount of relevant data a process has in some processor's cache) to determine which process to schedule on a processor have been proposed <ref> [41, 18] </ref>. Unfortunately, these techniques have had only limited success in real implementations [46]. One problem with cache affinity scheduling is that it is difficult to integrate into a general-purpose system where response time and fair allocation of processors are important. <p> This technique is called cache affinity scheduling. Cache affinity scheduling has been the subject of several previous studies. However, many of these studies were based on analytical modeling <ref> [41] </ref>, simulations [18], or synthetic applications [10], missing the subtle effects of real applications running on a real machine. Others studied uniprocessors [30], single applications within a narrow domain [42], or systems with unusual (space-sharing) scheduling policies [46]. <p> As a consequence, while their results are more broad-based, they do not consider many of the subtle issues and complex interactions of application, architecture, and scheduler discussed here. Squillante and Lazows-ka <ref> [41] </ref> measure response time under different affinity-based scheduling policies. Their results suggest that affinity scheduling provides substantial benefits. However, their results are based on analytical calculations with simple machine and application models, rather than a real implementation.
Reference: [42] <author> Shreekant S. Thakkar and Mark Sweiger. </author> <title> Performance of an OLTP application on Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 228-238, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, many of these studies were based on analytical modeling [41], simulations [18], or synthetic applications [10], missing the subtle effects of real applications running on a real machine. Others studied uniprocessors [30], single applications within a narrow domain <ref> [42] </ref>, or systems with unusual (space-sharing) scheduling policies [46]. This chapter makes a more thorough attempt to understand the effect of cache affinity scheduling on a real system running a variety of applications. It first investigates the ways application and workload characteristics affect their ability to benefit from affinity scheduling. <p> One study that did measure real applications on a real system, by Thakkar and Sweiger <ref> [42] </ref>, looked only at the performance of a database system under an extreme form of affinity. They studied a database application with 12 to 24 processors, running with attached scheduling.
Reference: [43] <author> Dominique Thiebaut and Harold S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: This problem can also also occur on a serial machine, though without the process migration component. Previous researchers have investigated the effect of context switching on the performance of applications on a serial machine, both using analytical techniques and using memory access traces of real executions <ref> [30, 43] </ref>. They found that context switching can have a significant effect on performance even in serial environments. The above data on individual applications can be used to estimate the effect of cache data loss on multiprogrammed workloads.
Reference: [44] <author> Josep Torrellas, Andrew Tucker, and Anoop Gupta. </author> <title> Evaluating the benefits of cache-affinity scheduling in shared-memory multiprocessors. </title> <type> Technical Report CSL-TR-92-536, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> August </month> <year> 1992. </year> <booktitle> Published in short form in the Proceedings of SIGMETRICS '93, </booktitle> <pages> pages 272-274, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The contents of this chapter are based on previously published work done jointly by the author along with Josep Torellas and Anoop Gupta <ref> [44] </ref>. The older work developed a workable implementation of affinity scheduling and applied it to a number of workloads consisting of parallel applications using ANL macros [5] to provide explicit parallelism.
Reference: [45] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multipro-grammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <year> 1989. </year>
Reference-contexts: Second, many parallel applications use synchronization that requires busy-waiting on a variable. If the process that will set the variable is preempted, other processes may end-up wasting processor time waiting for that variable to be set <ref> [45] </ref>. Third, frequent context 1.2. PREVIOUS APPROACHES 3 switching can indirectly affect processor cache behavior. When a context switch is performed, the preempted process may be rescheduled onto another processor, without the cache data that had been loaded into the cache of the original processor. <p> In this thesis, I propose and fully investigate one such approach called the process control technique <ref> [45] </ref>. It requires process control from the applications and processor partitioning and interface support from the operating system. <p> The role of the kernel is obvious in providing quick response time to events, since the kernel knows exactly when processor assignments are changed, or when a process blocks on some kernel semaphore. It is not possible for application-based polling to be competitive in this regard <ref> [45] </ref>. The primary drawback of the kernel asynchronously signaling the application is the high overhead associated with signal handling. In contrast, polling can be made quite cheap by having a shared data area between the application and the operating system where relevant events are recorded. <p> RELATED WORK 63 6.5.1 Scheduler Activations Anderson et al. at the University of Washington [1] have developed a system that is similar in concept and motivation to the process control and processor partitioning approach proposed in our previously published paper <ref> [45] </ref> and further developed in later work [18] and this thesis. <p> The benefits are relative to the performance of an "equi-partition" policy that is based on the concept of process control <ref> [45] </ref>, but unfortunately lacks some important elements of our process control approach. The equi-partition policy described requires that a processor being moved between applications be "released" by the application to which it was allocated before the reallocation can take place.
Reference: [46] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: INTRODUCTION scheduling strategies that use cache affinity (the amount of relevant data a process has in some processor's cache) to determine which process to schedule on a processor have been proposed [41, 18]. Unfortunately, these techniques have had only limited success in real implementations <ref> [46] </ref>. One problem with cache affinity scheduling is that it is difficult to integrate into a general-purpose system where response time and fair allocation of processors are important. This thesis looks again at cache affinity scheduling, using a new implementation that avoids problems with response time and fairness. <p> However, many of these studies were based on analytical modeling [41], simulations [18], or synthetic applications [10], missing the subtle effects of real applications running on a real machine. Others studied uniprocessors [30], single applications within a narrow domain [42], or systems with unusual (space-sharing) scheduling policies <ref> [46] </ref>. This chapter makes a more thorough attempt to understand the effect of cache affinity scheduling on a real system running a variety of applications. It first investigates the ways application and workload characteristics affect their ability to benefit from affinity scheduling. <p> While our results disagree on the effectiveness of attached scheduling, the difference may be due to the larger number of processors they use, which cause problems with bus contention. The bus contention increases miss latency, increasing the potential for gains from affinity. Vaswani and Zahorjan <ref> [46] </ref> also used real applications and a real implementation of cache affinity. They found that due to the relative 40 CHAPTER 4. CACHE AFFINITY infrequency of process preemption, benefits were minimal.
Reference: [47] <author> John Zahorjan, Edward D. Lazowska, and Derek L. Eager. </author> <title> Spinning versus blocking in parallel systems with uncertainty. </title> <booktitle> In Proceedings of the International Seminar on Performance of Distributed and Parallel Systems, </booktitle> <pages> pages 455-472, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Other researchers have proposed keeping spinning synchronization but using intelligent schedulers that are given enough knowledge about the applications to avoid preempting processes while they are inside a critical section <ref> [13, 47] </ref>. 1.2.1 Cache Affinity The above approaches, while helping the problem of processes wasting time waiting for locks held by preempted processes, do not address the problem of cache behavior in a multiprogrammed system.
Reference: [48] <author> John Zahorjan and Cathy McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of SIGMETRICS '90, </booktitle> <pages> pages 214-225, </pages> <year> 1990. </year>
Reference-contexts: The importance of using such information in scheduling decisions has previously been emphasized by Majumdar et al. [27] and Zahorjan and McCann <ref> [48] </ref>. In a recent report, McCann et al. [28] empirically evaluate the use of such a "dynamic" processor allocation approach and the results show noticeable benefits when running applications with highly variable parallelism.
Reference: [49] <author> Songnian Zhou and Timothy Brecht. </author> <title> Processor pool-based scheduling for large-scale NUMA multiprocessors. </title> <booktitle> In Proceedings of SIGMETRICS '91, </booktitle> <pages> pages 133-142, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In sum, the advent of scalable shared-memory machines creates a great opportunity for improving the performance of multiprogrammed workloads with intelligent scheduling policies. Several researchers have begun to work on these problems in the context of NUMA machines <ref> [9, 14, 34, 49] </ref>. 8.3 Final Remarks In conclusion, process control has been shown to be a simple but effective approach to improving the performance of parallel applications on multiprogrammed systems, one that may be cleanly integrated into a traditional operating system.
References-found: 49

