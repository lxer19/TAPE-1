URL: ftp://ftp.fas.sfu.ca/pub/cs/han/kdd/vldb94.ps
Refering-URL: http://fas.sfu.ca/cs/research/groups/DB/sections/publication/kdd/kdd.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rng@cs.ubc.ca  han@cs.sfu.ca  
Title: Efficient and Effective Clustering Methods for Spatial Data Mining  
Author: Raymond T. Ng Jiawei Han 
Address: Vancouver, B.C., V6T 1Z4, Canada  Burnaby, B.C., V5A 1S6, Canada  
Affiliation: Department of Computer Science University of British Columbia  School of Computing Sciences Simon Fraser University  
Abstract: Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also develop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms. Furthermore, experiments conducted to compare the performance of CLARANS with that of existing clustering methods show that CLARANS is the most efficient.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> (1992) An Interval Classifier for Database Mining Applications, </title> <booktitle> Proc. 18th VLDB, </booktitle> <pages> pp 560-573. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> Many excellent studies on data mining have been conducted, such as those reported in [1, 2, 4, 7, 11, 13, 16]. <ref> [1] </ref> considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] includes many interesting studies on various issues
Reference: [2] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> (1993) Mining Association Rules between Sets of Items in Large Databases, </title> <booktitle> Proc. 1993 SIGMOD, </booktitle> <pages> pp 207-216. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> Many excellent studies on data mining have been conducted, such as those reported in [1, 2, 4, 7, 11, 13, 16]. [1] considers the problem of inferring classification functions from samples; <ref> [2] </ref> studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] includes many interesting studies on various issues in knowledge discovery such as finding functional dependencies between attributes.
Reference: [3] <author> W. G. Aref and H. Samet. </author> <title> (1991) Optimization Strategies for Spatial Query Processing, </title> <booktitle> Proc. 17th VLDB, </booktitle> <pages> pp. 81-90. </pages>
Reference-contexts: The kind of spatial data mining considered in this paper assumes that a spatial database consists of both spatial and non-spatial attributes, and that nonspatial attributes are stored in relations <ref> [3, 12, 17] </ref>. The general approach here is to use clustering algorithms to deal with the spatial attributes, and use other learning tools to take care of the non-spatial counterparts. DBLEARN is the tool we have chosen for mining non-spatial attributes [7].
Reference: [4] <author> A. Borgida and R. J. Brachman. </author> <title> (1993) Loading Data into Description Reasoners, </title> <booktitle> Proc. 1993 SIGMOD, </booktitle> <pages> pp 217-226. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16]
Reference: [5] <author> T. Brinkhoff and H.-P. Kriegel and B. Seeger. </author> <title> (1993) Efficient Processing of Spatial Joins Using R-trees, </title> <booktitle> Proc. 1993 SIGMOD, </booktitle> <pages> pp 237-246. </pages>
Reference: [6] <author> O. Gunther. </author> <title> (1993) Efficient Computation of Spatial Joins, </title> <booktitle> Proc. 9th Data Engineering, </booktitle> <pages> pp 50-60. </pages>
Reference: [7] <author> J. Han, Y. Cai and N. Cercone. </author> <title> (1992) Knowledge Discovery in Databases: an Attribute-Oriented Approach, </title> <booktitle> Proc. 18th VLDB, </booktitle> <pages> pp. 547-559. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> Many excellent studies on data mining have been conducted, such as those reported in [1, 2, 4, 7, 11, 13, 16]. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; <ref> [7] </ref> proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] includes many interesting studies on various issues in knowledge discovery such as finding functional dependencies between attributes. <p> The general approach here is to use clustering algorithms to deal with the spatial attributes, and use other learning tools to take care of the non-spatial counterparts. DBLEARN is the tool we have chosen for mining non-spatial attributes <ref> [7] </ref>. It takes as inputs relational data, generalization hierarchies for attributes, and a learning query specifying the focus of the mining task to be carried out. From a learning request, DBLEARN first extracts a set of relevant tuples via SQL queries. <p> Then a generalization operation on the attribute ethnicgroup causes all tuples of the form hm; Indiani and hm; Chinesei to be merged to the tuple hm; Asiansi. This merging has the effect of reducing the number of remaining (generalized) tuples. As described in <ref> [7] </ref>, each tuple has a system-defined attribute called count which keeps track of the number of original tuples (as stored in the relational database) that are represented by the current (generalized) tuple.
Reference: [8] <author> Y. Ioannidis and Y. Kang. </author> <title> (1990) Randomized Algorithms for Optimizing Large Join Queries, </title> <booktitle> Proc. 1990 SIGMOD, </booktitle> <pages> pp. 312-321. </pages>
Reference-contexts: The higher the value of maxneighbor, the closer is CLARANS to PAM, and the longer is each search of a local minima. But the quality of such a local minima is higher, and fewer local minima needs to be obtained. Like many applications of randomized search <ref> [8, 9] </ref>, we rely on experiments to determine the appropriate values of these parameters.
Reference: [9] <author> Y. Ioannidis and E. Wong. </author> <title> (1987) Query Optimization by Simulated Annealing, </title> <booktitle> Proc. 1987 SIGMOD, </booktitle> <pages> pp. 9-22. </pages>
Reference-contexts: The higher the value of maxneighbor, the closer is CLARANS to PAM, and the longer is each search of a local minima. But the quality of such a local minima is higher, and fewer local minima needs to be obtained. Like many applications of randomized search <ref> [8, 9] </ref>, we rely on experiments to determine the appropriate values of these parameters.
Reference: [10] <author> L. Kaufman and P.J. Rousseeuw. </author> <title> (1990) Finding Groups in Data: an Introduction to Cluster Analysis, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Its main goal is to identify structures or clusters present in the data. While there is no general definition of a cluster, algorithms have been developed to find several kinds of clusters: spherical, linear, drawn-out, etc. See <ref> [10, 18] </ref> for more detailed discussions and analyses of these issues. Among all the existing clustering algorithms, we have chosen the k-medoid methods as the basis of our algorithm for the following reasons. <p> Furthermore, they are invariant with respect to translations and orthogonal transformations of data points. Last but not least, experiments have shown that the k-medoid methods described below can handle very large data sets quite efficiently. See <ref> [10] </ref> for a more detailed comparison of k-medoid methods with other partitioning methods. In this section, we present the two best-known k-medoid methods on which our algorithm is based. PAM (Partitioning Around Medoids) was developed by Kaufman and Rousseeuw [10]. <p> See <ref> [10] </ref> for a more detailed comparison of k-medoid methods with other partitioning methods. In this section, we present the two best-known k-medoid methods on which our algorithm is based. PAM (Partitioning Around Medoids) was developed by Kaufman and Rousseeuw [10]. To find k clusters, PAM's approach is to determine a representative object for each cluster. This representative object, called a medoid, is meant to be the most centrally located object within the cluster. <p> Otherwise, for each non-selected object, find the most similar representative object. Halt. 2 Experimental results show that PAM works satisfactorily for small data sets (e.g. 100 objects in 5 clusters <ref> [10] </ref>). But it is not efficient in dealing with medium and large data sets. This is not too surprising if we perform a complexity analysis on PAM. In Steps (2) and (3), there are altogether k (n k) pairs of O i ; O h . <p> Thus, it is obvious that PAM becomes too costly for large values of n and k. This analysis motivates the development of CLARA. 2.2 CLARA Designed by Kaufman and Rousseeuw to handle large data sets, CLARA (Clustering LARge Applications) relies on sampling <ref> [10] </ref>. Instead of finding representative objects for the entire data set, CLARA draws a sample of the data set, applies PAM on the sample, and finds the medoids of the sample. <p> Here, for accuracy, the quality of a clustering is measured based on the average dissimilarity of all objects in the entire data set, and not only of those objects in the samples. Experiments reported in <ref> [10] </ref> indicate that 5 samples of size 40 + 2k give satisfactory results. Algorithm CLARA 1. For i = 1 to 5, repeat the following steps: 2. <p> The set of nodes in the graph is the set f fO m 1 ; : : : ; O m k g j O m 1 ; : : : ; O m k are objects in the data setg. 1 <ref> [10] </ref> reports a useful heuristic to draw samples. Apart from the first sample, subsequent samples include the best set of medoids found so far. In other words, apart from the first iteration, subsequent iterations draw 40 + k objects to add on to the best k medoids. <p> The unfortunate answer is no. In fact, determining k nat is one of the most difficult problems in cluster analysis, for which no unique solution exists. For SD (CLARANS), we adopt the heuristics of computing the silhouette coefficients, first developed by Kaufman and Rousseeuw <ref> [10] </ref>. (For a survey of alternative criteria, see [14].) For space considerations, we do not include the formulas for computing silhouettes, and will only concentrate on how we use silhouettes in our algorithms. <p> The closer the value is to 1, the higher the degree O j belongs to its cluster. The silhouette width of a cluster is the average silhouette of all objects in the cluster. Based on extensive experimentation, <ref> [10] </ref> proposes the following interpretation of the silhouette width of a cluster: silhouette width interpretation 0.71 - 1 strong cluster 0.51 - 0.7 reasonable cluster 0.26 - 0.5 weak or artificial cluster 0:25 no cluster found For a given number k 2 of clusters, the silhouette coefficient for k is the
Reference: [11] <author> D. Keim and H. Kriegel and T. Seidl. </author> <title> (1994) Supporting Data Mining of Large Databases by Visual Feedback Queries, </title> <booktitle> Proc. 10th Data Engineering, </booktitle> <pages> pp 302-313. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> studies on data mining have been conducted, such as those reported in [1, 2, 4, 7, 11, 13, 16]. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; <ref> [11] </ref> develops a visual feedback querying system to support data mining; and [16] includes many interesting studies on various issues in knowledge discovery such as finding functional dependencies between attributes.
Reference: [12] <author> R. Laurini and D. Thompson. </author> <title> (1992) Fundamentals of Spatial Information Systems, </title> <publisher> Academic Press. </publisher>
Reference-contexts: The kind of spatial data mining considered in this paper assumes that a spatial database consists of both spatial and non-spatial attributes, and that nonspatial attributes are stored in relations <ref> [3, 12, 17] </ref>. The general approach here is to use clustering algorithms to deal with the spatial attributes, and use other learning tools to take care of the non-spatial counterparts. DBLEARN is the tool we have chosen for mining non-spatial attributes [7].
Reference: [13] <author> W. Lu, J. Han and B. C. Ooi. </author> <title> (1993) Discovery of General Knowledge in Large Spatial Databases, </title> <booktitle> Proc. Far East Workshop on Geographic Information Systems, Singapore, </booktitle> <pages> pp. 275-289. </pages>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> However, most of these studies are concerned with knowledge discovery on non-spatial data, and the study most relevant to our focus here is <ref> [13] </ref> which studies spatial data mining. More specifically, [13] proposes a spatial data-dominant knowledge-extraction algorithm and a nonspatial data-dominant one, both of which aim to extract high-level relationships between spatial and nonspatial data. However, both algorithms suffer from the following problems. <p> However, most of these studies are concerned with knowledge discovery on non-spatial data, and the study most relevant to our focus here is <ref> [13] </ref> which studies spatial data mining. More specifically, [13] proposes a spatial data-dominant knowledge-extraction algorithm and a nonspatial data-dominant one, both of which aim to extract high-level relationships between spatial and nonspatial data. However, both algorithms suffer from the following problems. <p> Having described SD (CLARANS), we are now in a position to compare SD (CLARANS) with an earlier approach reported in <ref> [13] </ref> whose goal is to enhance DBLEARN with spatial learning capabilities. One of the two proposed approaches there is to first perform spatial generalizations, and then to use DBLEARN to conduct non-spatial generalizations. The fundamental difference between SD (CLARANS) and that algorithm in [13] is that a user of the latter <p> (CLARANS) with an earlier approach reported in <ref> [13] </ref> whose goal is to enhance DBLEARN with spatial learning capabilities. One of the two proposed approaches there is to first perform spatial generalizations, and then to use DBLEARN to conduct non-spatial generalizations. The fundamental difference between SD (CLARANS) and that algorithm in [13] is that a user of the latter must give a priori as input generalization hierarchies for spatial attributes. <p> Far extending the capability of the algorithm in <ref> [13] </ref>, SD (CLARANS) finds the clusters directly from the given data. To a certain extent, the clustering algorithm, CLARANS in this case, can be viewed as computing the spatial generalization hierarchy dynamically.
Reference: [14] <author> G. Milligan and M. Cooper. </author> <title> (1985) An Examination of Procedures for Determining the Number of Clusters in a Data Set, </title> <journal> Psychometrika, </journal> <volume> 50, </volume> <pages> pp. 159-179. </pages>
Reference-contexts: In fact, determining k nat is one of the most difficult problems in cluster analysis, for which no unique solution exists. For SD (CLARANS), we adopt the heuristics of computing the silhouette coefficients, first developed by Kaufman and Rousseeuw [10]. (For a survey of alternative criteria, see <ref> [14] </ref>.) For space considerations, we do not include the formulas for computing silhouettes, and will only concentrate on how we use silhouettes in our algorithms.
Reference: [15] <author> R. Ng and J. Han. </author> <title> (1994) Effective and Effective Clustering Methods for Spatial Data Mining, </title> <type> Technical Report 94-13, </type> <institution> University of British Columbia. </institution>
Reference-contexts: All the performance results of CLARANS quoted in the remainder of this paper are based on the version of CLARANS that set numlocal to 2 and maxneighbor to be the larger value between 1.25% of k (n k) and 250. See <ref> [15] </ref> for more information on how and why these specific values are chosen. 3.3 Experimental Results: CLARANS vs PAM In the following we present experimental results comparing CLARANS with PAM. <p> As discussed in Section 2.2, CLARA is not designed for small data sets. Thus, we ran this set of experiments on data sets whose number of objects exceeds 100. And the objects were organized in different number of clusters, as well as in different types of clusters <ref> [15] </ref>. When we conducted this series of experiments run and CLARA ning CLARA and CLARANS as presented earlier, CLARANS is always able to find clusterings of better quality than those found by CLARA. However, in some cases, CLARA may take much less time than CLARANS.
Reference: [16] <author> G. Piatetsky-Shapiro and W. J. Frawley. </author> <title> (1991) Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Many excellent studies on data mining have been conducted, such as those reported in <ref> [1, 2, 4, 7, 11, 13, 16] </ref>. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and [16] <p> [1, 2, 4, 7, 11, 13, 16]. [1] considers the problem of inferring classification functions from samples; [2] studies the problem of mining association rules between sets of data items; [7] proposes an attribute-oriented approach to knowledge discovery; [11] develops a visual feedback querying system to support data mining; and <ref> [16] </ref> includes many interesting studies on various issues in knowledge discovery such as finding functional dependencies between attributes. However, most of these studies are concerned with knowledge discovery on non-spatial data, and the study most relevant to our focus here is [13] which studies spatial data mining.
Reference: [17] <author> H. Samet. </author> <title> (1990) The Design and Analysis of Spatial Data Structures, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The kind of spatial data mining considered in this paper assumes that a spatial database consists of both spatial and non-spatial attributes, and that nonspatial attributes are stored in relations <ref> [3, 12, 17] </ref>. The general approach here is to use clustering algorithms to deal with the spatial attributes, and use other learning tools to take care of the non-spatial counterparts. DBLEARN is the tool we have chosen for mining non-spatial attributes [7].
Reference: [18] <author> H. Spath. </author> <title> (1985) Cluster Dissection and Analysis: Theory, FORTRAN programs, Examples, </title> <publisher> Ellis Horwood Ltd. </publisher>
Reference-contexts: Its main goal is to identify structures or clusters present in the data. While there is no general definition of a cluster, algorithms have been developed to find several kinds of clusters: spherical, linear, drawn-out, etc. See <ref> [10, 18] </ref> for more detailed discussions and analyses of these issues. Among all the existing clustering algorithms, we have chosen the k-medoid methods as the basis of our algorithm for the following reasons.
References-found: 18

