URL: http://sls-www.lcs.mit.edu/~hmmeng/thesis.ps
Refering-URL: http://www.sls.lcs.mit.edu/SLSPublications.html
Root-URL: 
Title: Phonological Parsing for Bi-directional Letter-to-Sound Sound-to-Letter Generation  
Author: by Helen Mei-Ling Meng Stephanie Seneff Victor W. Zue 
Degree: (1989) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering and Computer Science at the  c 1995 Helen Mei-Ling Meng. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Signature of Author  Certified by  Thesis Supervisor Certified by  Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: June 1995  February 14, 1995  
Address: (1991)  
Affiliation: S.M., Massachusetts Institute of Technology  S.B., Massachusetts Institute of Technology  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adams, M., </author> <title> "What Good is Orthographic Redundancy?," Perception of Print, </title> <editor> H. Singer and O. </editor> <booktitle> Tzend (Eds.), </booktitle> <pages> pp. 197-221, </pages> <address> Hillsdale, NJ:Erlbaum, </address> <year> 1981. </year>
Reference-contexts: This will be examined in the following subsection. 1.3.1 Orthographic-phonological Correspondences in English The English writing system is built from the 26 letters in the alphabet. However, only certain letter sequences are found in English words. Adams <ref> [1] </ref> [53] noted that, From an alphabet of 26 letters, we could generate over 475,254 unique strings of 4 letters or less, or 12,376,630 of 5 letters or less. <p> Other lexical structures like the syllable are derived from phonotactic constraints specific to the language, so if written English largely corresponds to spoken English, then syllabic structures should be found in the orthography as well <ref> [1] </ref> [53]. The way that English orthography corresponds to morphology, syllabification and phonology is fairly systematic, but it also admits many irregularities. Therefore, English has been described as a quasi-regular system [53]. To illustrate correspondences in morphology, consider the words "preview" and "decode," which contain CHAPTER 1.
Reference: [2] <author> Allen, J., </author> <title> "Synthesis of speech from unrestricted text," </title> <journal> Proc. IEEE, </journal> <volume> vol. 64, </volume> <pages> pp. 422-433, </pages> <year> 1976. </year>
Reference-contexts: CONCLUSIONS AND FUTURE WORK 159 among words which should lower storage requirements. Words in a highly inflected language like English can be collapsed together according to similar morphological compositions. Allen has estimated a savings factor of 10 if a lexicon stores morphemes instead of all possible forms of words <ref> [2] </ref> [13]. Suhm et al [82] performed a study on the orthographic transcriptions in the Wall Street Journal (WSJ) domain.
Reference: [3] <author> Allen, J., S. Hunnicutt and D. Klatt, </author> <title> From Text to Speech: The MITalk System, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1987. </year>
Reference-contexts: turn dependent on the part of speech of a word, e.g., homographs which can take on two parts-of-speech often have a stress-unstress pattern if they are nouns, and an unstress-stress pattern if they are verbs, as in "record" and "permit." Another interesting class of exceptional pronunciations arises from high-frequency words <ref> [3] </ref>. Initial "th" is pronounced as /T/ (a voiceless fricative) in many words (such as "thin," "thesis," "thimble"), but for very frequent words such as the short function words ("the," "this," "there," "those"), "th" is pronounced as /D/ (a voiced fricative). <p> Due to the above reasons, we should be careful when comparing different systems based on the quoted performance values. The following is a sketch of the various approaches with a few illustrative examples. 1. Rule-based Approaches The classic examples of rule-based approaches include MITalk <ref> [3] </ref>, the NRL system [21], and DECtalk [17]. These use a set of hand-engineered, ordered rules for transliteration. Transformation rules may also be applied in multiple passes in order to process linguistic units larger than the phoneme/grapheme, e.g., morphs. <p> Function words are omitted because they tend to have different letter-to-sound mappings from other English words <ref> [3] </ref>. 84 CHAPTER 4. EXPERIMENTAL RESULTS 85 popped off the stack reaches 330, whichever happens first. These numbers are empirically chosen as a limit on the depth of the search. <p> Alternatively, the layered bigrams can be viewed as an inexpensive means for generating multiple plausible hypotheses, and refinement processes can follow. The refinement can exploit various kinds of "future context" | letters, phonemes, syllables, suffixes, etc., which is considered by some to be more important than the left context <ref> [3] </ref> 1 , [36]. The combination of left and right contexts is vital for tracing stress dependencies, which are known to spread over a long range of several syllables [12] and thus cannot be determined locally.
Reference: [4] <author> Alleva, F. and K. F. Lee, </author> <title> "Automatic New Word Acquisition: Spelling from Acoustics," </title> <booktitle> Proc. of the DARPA Speech and Natural Language Workshop, </booktitle> <month> Octo-ber </month> <year> 1989. </year>
Reference-contexts: These rules are manually written with reference to the lexicons. Accuracies rose to 84.5% and 62.8% for the small and large lexicons respectively. 2. Hidden Markov Models HMMs have also been used by Alleva and Lee <ref> [4] </ref> for acoustics-to-spelling generation. The problem is formulated roughly as an inverse of the previous application of HMMs on spelling-to-pronunciation generation | the surface form is the acoustic signal, and the underlying form is the orthography. Therefore the HMMs model the relationship between the acoustics and orthography CHAPTER 1.
Reference: [5] <author> Antworth, E., </author> <title> PC KIMMO: A Two-Level Processor for Morphological Analysis, </title> <booktitle> Summer Institute of Linguistics, </booktitle> <publisher> Inc., </publisher> <year> 1990. </year>
Reference-contexts: The probabilistic parsing paradigm is preferred for four reasons: First, the probabilities serve to augment the text-to-speech synthesis, and the two-level rules found in the pc-kimmo system for morphological analysis <ref> [5] </ref>. CHAPTER 1. INTRODUCTION 39 known structural regularities that can be encoded in simple rules with other structural regularities which may be automatically discovered from a large body of training data. <p> Another example concerns the vowel in "wash." It can be realized as [O], [U], [^], <ref> [5] </ref>, or [a]-[r], with retroflexion inserted in the last two cases. Similarly, we find much variation in the "sa-2" sentences, as illustrated in Figure 7-2. <p> In the word "carry," the phoneme /k/ illustrates the case where one phoneme is mapped to multiple phones ([k] 4 and [k]), and the phone <ref> [5] </ref> 4 k-closures may be denoted as [k] or [kcl]. CHAPTER 7. EXTENDING THE HIERARCHY 131 "Don't ask me to carry..." | with a terminal phonetic layer. CHAPTER 7. EXTENDING THE HIERARCHY 132 | "...an oily rag like that." | with a terminal phonetic layer. CHAPTER 7. <p> J- d d a r k k s u F | 4 g g r i s iw a S w O F 5 O l y | 5] The problematic word here is "year," realized as the phone sequence [|5], and the probability going from the phone [|] to <ref> [5] </ref> is 0.
Reference: [6] <author> Asadi, A., </author> <title> Automatic Detection and Modeling of New Words in a Large-Vocabulary Continuous Speech Recognition System, </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, Northeastern University, </institution> <address> Boston, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Speech recognizers are now capable of speaker-independent, large-vocabulary, continuous speech recognition. The speech input may either be read or spontaneous. 1 Vocabulary sizes can range from a few thousand words to tens of thousands of words [63] and efforts to handle out-of-vocabulary words are under way <ref> [6] </ref>, [35]. Natural language understanding systems can analyze a recognized sentence to obtain a meaning representation [73]. The semantics are then channelled to the appropriate locations to perform specific actions (such as sav 1 Read speech tends to be "cleaner" than spontaneous speech.
Reference: [7] <author> Bernstein, J. and D. Pisoni, </author> <title> "Unlimited Text-to-speech System: Description and Evaluation of a Microprocessor based device," </title> <booktitle> Proc. ICASSP-80, </booktitle> <pages> pp. 576-579, </pages> <address> Denver, </address> <year> 1984. </year> <note> 185 BIBLIOGRAPHY 186 </note>
Reference: [8] <author> Bernstein, J. and D. Rtishchev, </author> <title> "A Voice Interactive Language Instruction System," </title> <booktitle> Proc. Eurospeech-91, </booktitle> <pages> pp. 981-984, </pages> <address> Italy, </address> <year> 1991. </year>
Reference-contexts: The systems accept spontaneous speech as input and respond with synthesized speech as output. They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation [94], or learn to read <ref> [8] </ref> [33] [54]. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge [14] [96].
Reference: [9] <author> Brill, E., </author> <title> "A simple rule-based part-of-speech tagger," </title> <booktitle> Proc. of the Third Conference on Applied Natural Language Processing, Association of Computational Linguistics, </booktitle> <address> Trento, Italy, </address> <year> 1992. </year>
Reference-contexts: Alternatively, we can eliminate systematic errors and refine generation outputs by post-processing with additional contextual information. A pilot experiment is conducted along these lines, using a technique known as transformational error-driven learning <ref> [9] </ref>. The study will be described in Chapter 8. The words that belong to the portion of the test set lying above the asymptote appear intractable | a correct pronunciation/spelling did not emerge as one of the 30 complete theories. <p> We have made a preliminary trial attempt to design a post-process of such ilk using a typical induction technique known as "transformation-based error-driven learning." This is a learning algorithm previously used for part-of-speech tagging <ref> [9] </ref>. In this work the learning algorithm is used for the automatic inference of refinement rules for generated pronunciations.
Reference: [10] <author> Byrd R. and M. Chodorow, </author> <title> "Using an On-line Dictionary to Find Rhyming words and Pronunciations for Unknown Words," </title> <booktitle> Proc. ACL, </booktitle> <pages> pp. 277-283, </pages> <address> Chicago, </address> <year> 1985. </year>
Reference-contexts: The system was evaluated on a set of 70 nonsense monosyllabic words, and was found to disagree with CHAPTER 1. INTRODUCTION 31 human subjects on 9% of the set. Another system modelled after Glushko's theory can be found in <ref> [10] </ref>. Sullivan and Damper [83] developed a system based on the dual-route theory [68], where the duality refers to a set of context-free rules conjoined with lexical analogies. Therefore, Sullivan and Damper's system draws phonemic analogies in addition to orthographic analogies.
Reference: [11] <author> Chomsky, N. and M. Halle, </author> <title> Sound Pattern of English, </title> <address> New York, </address> <publisher> Harper & Row. </publisher>
Reference-contexts: The alphabetic principle [69] refers to the occurrence of systematic correspondences between the spoken and written forms of words | the letters and letter patterns found in written English map somewhat consistently to the speech units such as phonemes in spoken English. Chomsky and Halle <ref> [11] </ref> pointed out that English phonology and morphology are simultaneously represented in the orthography. This suggests that the orthography should exhibit cues which reflect lexical structures like the morpheme.
Reference: [12] <author> Church, K., </author> <title> "Stress Assignment in Letter to Sound Rules for Speech Synthesis," </title> <booktitle> Proc. ACL, </booktitle> <pages> pp. 246-253, </pages> <address> Chicago, </address> <year> 1985. </year>
Reference-contexts: The combination of left and right contexts is vital for tracing stress dependencies, which are known to spread over a long range of several syllables <ref> [12] </ref> and thus cannot be determined locally. Therefore post-processes can use additional contextual information to filter the hypotheses, select the most desirable one from the pool, or correct systematic generation errors.
Reference: [13] <author> Church, K., </author> <title> "Morphological Decomposition and Stress Assignment for Speech Synthesis," </title> <booktitle> Proc. ACL, </booktitle> <pages> pp. 156-164, </pages> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Words in a highly inflected language like English can be collapsed together according to similar morphological compositions. Allen has estimated a savings factor of 10 if a lexicon stores morphemes instead of all possible forms of words [2] <ref> [13] </ref>. Suhm et al [82] performed a study on the orthographic transcriptions in the Wall Street Journal (WSJ) domain.
Reference: [14] <author> Church, K., </author> <title> Phonological Parsing in Speech Recognition, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge <ref> [14] </ref> [96]. Amongst these are: * Signal processing | the transformation of a continuously-varying acoustic speech signal into a discrete form. * Phonology and acoustic-phonetics | the study of speech sounds, their variabilities as a result of coarticulation, as well as their acoustic characteristics.
Reference: [15] <author> Cohen, M., G. Baldwin, J. Bernstein, H. Murveit and M. Weintraub, </author> <title> "Studies for an Adaptive Recognition Lexicon," </title> <booktitle> Proc. DARPA Speech Recognition Workshop, </booktitle> <volume> Report No. SAIC-87/1644, </volume> <pages> pp. 49-55, </pages> <month> February </month> <year> 1987. </year>
Reference: [16] <author> Coker, C., K. Church and M. Liberman, </author> <title> "Morphology and Rhyming: Two Powerful Alternatives to Letter-to-Sound Rules for Speech Synthesis," </title> <booktitle> Proc. of the Conference on Speech Synthesis, European Speech Communication Association, </booktitle> <year> 1990. </year>
Reference-contexts: The outputs from the orthographic and phonemic analogisers are eventually combined to generate the result. 6. Case-based Reasoning and Hybrid Approaches Case-based approaches generate a pronunciation of an input word based on similar exemplars in the training corpus. The TTS system <ref> [16] </ref> developed at Bell Labs adopts this approach for generating name pronunciations. <p> This approach was evaluated on a name pronunciation task, with a case-library of 5000 names, and a separate set of 400 names for testing. The percentage of acceptable pronunciations was measured and compared with NETtalk and other commercial systems (from Bellcore [77], Bell Labs <ref> [16] </ref>, and DEC [17]). ANAPRON performed significantly better than NETtalk in this task, yielding a word accuracy of 86%, which is very close to the performance of the commercial systems. Van den Bosch et al. [88] experimented with two data-oriented methods for gra CHAPTER 1.
Reference: [17] <author> Conroy, D., T. Vitale and D. Klatt, </author> <title> DECtalk DTC03 Text-to-Speech System Owner's Manual, </title> <institution> Educational Services of Digital Equitpment Corporation, </institution> <address> P.O. Box CS2008, Nashua, NH 03061, </address> <year> 1986. </year> <title> Document number EK-DTC03-OM-001. BIBLIOGRAPHY 187 </title>
Reference-contexts: The different types of information, or subsets of them, are often incorporated independently, and with ad hoc methodologies, into the components of existing conversational systems. Phonological rules are applied in letter-to-sound generation in speech synthesis <ref> [17] </ref>. <p> The following is a sketch of the various approaches with a few illustrative examples. 1. Rule-based Approaches The classic examples of rule-based approaches include MITalk [3], the NRL system [21], and DECtalk <ref> [17] </ref>. These use a set of hand-engineered, ordered rules for transliteration. Transformation rules may also be applied in multiple passes in order to process linguistic units larger than the phoneme/grapheme, e.g., morphs. The rule-based approaches have by far given the best generation CHAPTER 1. INTRODUCTION 27 performance. <p> This approach was evaluated on a name pronunciation task, with a case-library of 5000 names, and a separate set of 400 names for testing. The percentage of acceptable pronunciations was measured and compared with NETtalk and other commercial systems (from Bellcore [77], Bell Labs [16], and DEC <ref> [17] </ref>). ANAPRON performed significantly better than NETtalk in this task, yielding a word accuracy of 86%, which is very close to the performance of the commercial systems. Van den Bosch et al. [88] experimented with two data-oriented methods for gra CHAPTER 1. INTRODUCTION 33 pheme-to-phoneme conversion in Dutch.
Reference: [18] <author> Damper, R., </author> <title> "Self-Learning and Connectionist Approaches to Text-Phoneme Conversion," </title> <booktitle> Proc. of 2nd Neural Computation and Psychology Workshop, edited by J. Levy, </booktitle> <publisher> Forthcoming. </publisher>
Reference-contexts: Excellent reviews can be found in <ref> [18] </ref>, [29] and [41]. The various approaches have given rise to a wide range of letter-to-sound generation accuracies. Many of these accuracies are based on different corpora, and some corpora may be more difficult than others.
Reference: [19] <author> Dedina, M. and H. Nusbaum, </author> <title> "PRONOUNCE: A Program for Pronunciation by Analogy," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol. 5, No. 1, </volume> <pages> pp. 55-64, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The output symbol corresponding to the node with the highest activation is selected as the generated translation. In text-to-phonemics conversion, training and testing on two disjoint 2000-word corpora gave a 66% phoneme accuracy and 26% word accuracy. 5. Psychological Approaches Dedina and Nusbaum <ref> [19] </ref> developed the system PRONOUNCE to demonstrate the computational feasibility of the analogical model. This model is proposed by Glushko [26] in psychology literature, which suggests that humans use a process of analogy to derive the pronunciation for a spelling pattern, as an alternative to the pronunciation-by-rule theory.
Reference: [20] <author> Denes, P. and E. Pinson, </author> <title> The Speech Chain: The Physics and Biology of Spoken Language, </title> <editor> W. H. </editor> <publisher> Freeman and Company. </publisher>
Reference-contexts: This integration is best exemplified by the human communication system. Our framework is therefore designed to mirror the chain of events underlying the communication between a speaker and a listener, a sequence which has been described as the speech chain <ref> [20] </ref>. When a speaker wants to convey a spoken message to a listener, he first gathers his thoughts, which constitutes the semantics of his speech. The semantics is generally coherent with the context of the dialogue, which involves discourse and pragmatics.
Reference: [21] <author> Elovitz, H., R. Johnson, A. McHugh and J. Shore, </author> <title> Automatic Translation of English Text to Phonetics by means of Letter-to-Sound Rules, </title> <type> Naval Research Laboratory Technical Report 7949, </type> <month> January </month> <year> 1976. </year>
Reference-contexts: Coar-ticulatory effects in different phonetic contexts and across word boundaries are expressed as phonological rules [62]. Examples include the flapping of the /t/ in "water" 3 This example is borrowed from <ref> [21] </ref>. CHAPTER 1. INTRODUCTION 19 (/w O 5/) and the palatalization of the /d/ before the word boundary in "did you" (/d I J y u/). Naturally, these rules are found in both synthesizers and recogniz-ers. <p> Due to the above reasons, we should be careful when comparing different systems based on the quoted performance values. The following is a sketch of the various approaches with a few illustrative examples. 1. Rule-based Approaches The classic examples of rule-based approaches include MITalk [3], the NRL system <ref> [21] </ref>, and DECtalk [17]. These use a set of hand-engineered, ordered rules for transliteration. Transformation rules may also be applied in multiple passes in order to process linguistic units larger than the phoneme/grapheme, e.g., morphs. The rule-based approaches have by far given the best generation CHAPTER 1. INTRODUCTION 27 performance.
Reference: [22] <author> Fant, G., </author> <title> "Analysis and Synthesis of Speech Processes," Manural of Phonetics, </title> <editor> ed. B. Malmberg, </editor> <volume> Chapter 8, </volume> <pages> pp. 173-277, </pages> <publisher> North-Holland Publishing Co., </publisher> <year> 1970. </year>
Reference-contexts: This is shown in The fourth layer, syllable parts, also provides tactics for the two successive layers of distinctive features <ref> [22] </ref> [81]. The sequence of broad classes (manner features) in the fifth layer bears the Sonority Sequencing Constraint.
Reference: [23] <author> Flammia, G., J. Glass, M. Phillips, J. Polifroni, S. Seneff and V. Zue, </author> <title> "Porting the Bilingual Voyager System to Italian," </title> <booktitle> Proc. ICSLP-94, </booktitle> <pages> pp. 911-914, </pages> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: Interactive information retrieval via speech also requires a language generation component for response generation [25]. The combined applications of these four branches of technology, namely, speech synthesis, speech recognition, language understanding and language generation, has brought about the recent advent of conversational systems <ref> [23] </ref> [100] . These systems can carry out a conversational dialogue with the user concerning topics in a restricted domain (or multiple restricted domains [28]). The systems accept spontaneous speech as input and respond with synthesized speech as output.
Reference: [24] <author> Glass, J., D. Goodine, M. Phillips, S. Sakai, S. Seneff and V. Zue, </author> <title> "A Bilingual Voyager System," </title> <booktitle> Proc. Europeech-93, </booktitle> <pages> pp. 2063-2066, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The systems accept spontaneous speech as input and respond with synthesized speech as output. They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) <ref> [24] </ref> [28], convey a spoken message with another language via machine translation [94], or learn to read [8] [33] [54]. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output.
Reference: [25] <author> Glass, J., J. Polifroni and S. Seneff, </author> <title> "Multilingual Language Generation Across Multiple Domains," </title> <booktitle> Proc. ICSLP-94, </booktitle> <pages> pp. 983-986, </pages> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: INTRODUCTION 17 ing or deleting a file), or to retrieve information (such as airline reservations and city navigation). Interactive information retrieval via speech also requires a language generation component for response generation <ref> [25] </ref>. The combined applications of these four branches of technology, namely, speech synthesis, speech recognition, language understanding and language generation, has brought about the recent advent of conversational systems [23] [100] .
Reference: [26] <author> Glushko, R., </author> <title> "The organization and activation of orthographic knowledge in reading aloud," </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> Vol. 5, </volume> <pages> pp. 674-691, </pages> <year> 1979. </year>
Reference-contexts: In text-to-phonemics conversion, training and testing on two disjoint 2000-word corpora gave a 66% phoneme accuracy and 26% word accuracy. 5. Psychological Approaches Dedina and Nusbaum [19] developed the system PRONOUNCE to demonstrate the computational feasibility of the analogical model. This model is proposed by Glushko <ref> [26] </ref> in psychology literature, which suggests that humans use a process of analogy to derive the pronunciation for a spelling pattern, as an alternative to the pronunciation-by-rule theory. PRONOUNCE uses a lexical database of approximately 20,000 words. It does not have a training phase.
Reference: [27] <author> Goddeau, D., </author> <type> personal communication. BIBLIOGRAPHY 188 </type>
Reference-contexts: standard bigram language model (perplexity = 11.3), and comparable with a standard trigram language model (perplexity = 8.3). 8.5 Multilingual Applications It should be possible to apply our system methodologies to multilingual systems whenever the letter-sound correspondences and context interact in the same way 6 as our current monolingual system <ref> [27] </ref>, e.g. generating English name pronunciations in terms of the Japanese Katakana pronunciation alphabet. Some resemblances can be found between our formalism and the Speech Maker Formalism for Dutch [89]. Speech 6 This is, of course, dependent on the language.
Reference: [28] <author> Goddeau, D., E. Brill, J. Glass, C. Pao, M. Phillips, J. Polifroni, S. Seneff and V. Zue, </author> <title> "Galaxy: A Human-Language Interface to On-line Travel Information," </title> <booktitle> Proc. ICSLP-94, </booktitle> <pages> pp. 707-710, </pages> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: These systems can carry out a conversational dialogue with the user concerning topics in a restricted domain (or multiple restricted domains <ref> [28] </ref>). The systems accept spontaneous speech as input and respond with synthesized speech as output. They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation [94], or learn to read [8] <p> a conversational dialogue with the user concerning topics in a restricted domain (or multiple restricted domains <ref> [28] </ref>). The systems accept spontaneous speech as input and respond with synthesized speech as output. They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation [94], or learn to read [8] [33] [54]. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output.
Reference: [29] <author> Golding, A., </author> <title> Pronouncing Names by a Combination of Case-based and Rule-based Reasoning, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Excellent reviews can be found in [18], <ref> [29] </ref> and [41]. The various approaches have given rise to a wide range of letter-to-sound generation accuracies. Many of these accuracies are based on different corpora, and some corpora may be more difficult than others. <p> Evaluation by six human subjects gave a word accuracy between 47% and 68%. An extension of this work is found in [80]. Another approach using case-based reasoning can be found in [46]. Golding <ref> [29] </ref> proposed a hybrid approach based on the interaction of rule-based and case-based reasoning and developed the system ANAPRON. Rules are used to implement broad trends and the cases are for pockets of exceptions. The set of rules is adapted from MITalk and foreign-language textbooks.
Reference: [30] <author> Golding, A. and P. Rosenbloom, </author> <title> "A Comparison of ANAPRON with Seven Other Name-pronunciation Systems," </title> <journal> Journal of the Americal Voice I/O Society, </journal> <month> August </month> <year> 1993, </year> <month> pp.1-21. </month>
Reference-contexts: Errors in the generated stress pattern and/or phoneme insertion errors may be neglected 3 A careful study comparing the performance of eight name-pronunciation system can be found in <ref> [30] </ref>. CHAPTER 4. EXPERIMENTAL RESULTS 87 in some cases. However, the phoneme accuracy measurement which we use above includes insertion penalties. To a certain extent, stress errors are also accounted for, since some of our vowel phonemes are stress-loaded, i.e. we distinguish between their stressed and unstressed realizations.
Reference: [31] <author> Goodine, D. and V. Zue, "Romaine: </author> <title> A Lattice Based Approach to Lexical Access," </title> <booktitle> Proc. Eurospeech-93, </booktitle> <pages> pp., </pages> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: The different types of information, or subsets of them, are often incorporated independently, and with ad hoc methodologies, into the components of existing conversational systems. Phonological rules are applied in letter-to-sound generation in speech synthesis [17]. They are also embedded in pronunciation models and networks in speech recognizers <ref> [31] </ref>. n-gram language models [39] are popular for guiding the search in speech recognizers, because they can be automatically acquired for different tasks with a wide range of perplexities, and are thus more adaptable than finite-state grammars [47] [49].
Reference: [32] <author> Groner, G., J. Bernstein, E. Ingber, J. Perlman and T. Toal, </author> <title> "A Real-time Text-to-speech Converter," </title> <journal> Speech Technology, </journal> <volume> 1, </volume> <year> 1982. </year>
Reference-contexts: The rule-based approaches have by far given the best generation CHAPTER 1. INTRODUCTION 27 performance. MITalk rules have attained word accuracies ranging from 66% to 76.5% [38] (all phonemes and stress pattern correct). The system Speech Plus Prose 2000 <ref> [32] </ref> has achieved a performance of 85% word accuracy using only its letter-to-sound rules. Adding exceptions dictionaries helps improve the overall performance noticeably | a 3000-word dictionary with rules gave a 97% word accuracy. In general, rules operate on one-dimensional data structures.
Reference: [33] <author> Hauptmann, A., J. Mostow, S. Roth, M. Kane, A. Swift, </author> <title> "A Prototype Reading Coach that Listens: Summary of Project LISTEN," </title> <booktitle> Proc. of the ARPA Human Language Technology Workshop, </booktitle> <pages> pp. 237-238, </pages> <address> New Jersey, </address> <year> 1994. </year>
Reference-contexts: They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation [94], or learn to read [8] <ref> [33] </ref> [54]. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge [14] [96].
Reference: [34] <author> Hertz, S., J. Kadin and K. Karplus, </author> <title> "The Delta rule development system for speech synthesis from text," </title> <booktitle> Proc. of the IEEE Vol. </booktitle> <volume> 73, No. 11, </volume> <pages> pp. 1589-1601, </pages> <year> 1985. </year>
Reference-contexts: In general, rules operate on one-dimensional data structures. There are also rules that operate on two-dimensional data structures, e.g., the Speech Maker formalism [89] developed for Dutch. The two-dimensional rules in the Speech Maker are modelled after the delta system <ref> [34] </ref>. The rules manipulate the contents of a data structure known as the grid, which contains streams of linguistic representations synchronized by markers. Writing rule sets is an arduous process. <p> Generation in the Speech Maker is achieved by complex, two-dimensional rules which can operate on more than one stream at a time, modelled after the delta language <ref> [34] </ref>. An example is shown in Figure 8-2, which is a rule stating that an "A", followed by any sequence of characters pronounced as a single consonant, followed by an "E", which is root final, should be pronounced as /e/.
Reference: [35] <author> Hetherington, Lee, </author> <title> A Characterization of the Problem of New, Out-of-Vocabulary Words in Continuous Speech Recognition and Understanding, </title> <type> Ph.D. thesis, </type> <institution> MIT, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Speech recognizers are now capable of speaker-independent, large-vocabulary, continuous speech recognition. The speech input may either be read or spontaneous. 1 Vocabulary sizes can range from a few thousand words to tens of thousands of words [63] and efforts to handle out-of-vocabulary words are under way [6], <ref> [35] </ref>. Natural language understanding systems can analyze a recognized sentence to obtain a meaning representation [73]. The semantics are then channelled to the appropriate locations to perform specific actions (such as sav 1 Read speech tends to be "cleaner" than spontaneous speech.
Reference: [36] <author> Hochberg, J., S. M. Mniszewski, T. Calleja and G. J. Papcun, </author> <title> "A Default Hierarchy for Pronouncing English," </title> <journal> IEEE Transactions on Pattern Matching and Machine Intelligence, </journal> <volume> Vol. 13, No. 9, </volume> <pages> pp. 957-964, </pages> <month> September </month> <year> 1991. </year> <note> BIBLIOGRAPHY 189 </note>
Reference-contexts: Induction Approaches Induction approaches attempt to infer letter-to-sound rules from a body of training data. The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in <ref> [36] </ref>, [40], [51], [61], [71] and [87]. The following briefly recounts a few of them. <p> A decision tree is constructed based on a 50,000 word lexicon, where at each step, the tree includes the context feature with the maximum conditional mutual information. 8 They reported a performance of 94% accuracy per letter on a test set of 5,000 words. Hochberg et al. <ref> [36] </ref> devised a default hierarchy of rules, ranging from the most general rule set at the bottom to the most specific rule set on top. <p> The refinement can exploit various kinds of "future context" | letters, phonemes, syllables, suffixes, etc., which is considered by some to be more important than the left context [3] 1 , <ref> [36] </ref>. The combination of left and right contexts is vital for tracing stress dependencies, which are known to spread over a long range of several syllables [12] and thus cannot be determined locally.
Reference: [37] <author> Huang, C., M. Son-Bell and D. Baggett, </author> <title> "Generation of Pronunciations from Orthographies using Transformation-based Error-driven Learning," </title> <booktitle> Proc. ICSLP, </booktitle> <pages> pp. 411-414, </pages> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: O-/a/ C-/k/ K-/k/ E-/t/ D-/d/ Application of the second rule: Change P 0 from /d/ to /null/ if P 2 = /k/ and P 1 = /t/. gives the correct pronunciation in the alignment: B-/b/ L-/l/ O-/a/ C-/k/ K-/k/ E-/t/ D-/null/ Similar results have been obtained by Huang et al. <ref> [37] </ref>. Their experiments adopted a similar learning algorithm (with similar rule templates) for letter-to-sound generation with 3,600 training words and 425 testing words. The experimental corpora consist of the CMU Pronunciation Dictionary [84] and the high-frequency words in the Brown Corpus.
Reference: [38] <author> Hunnicutt, S., </author> <title> "Phonological Rules for a Text-to-speech System," </title> <journal> American Journal of Computational Linguistics, </journal> <volume> AJCL Microfiche 57, </volume> <year> 1976. </year>
Reference-contexts: Transformation rules may also be applied in multiple passes in order to process linguistic units larger than the phoneme/grapheme, e.g., morphs. The rule-based approaches have by far given the best generation CHAPTER 1. INTRODUCTION 27 performance. MITalk rules have attained word accuracies ranging from 66% to 76.5% <ref> [38] </ref> (all phonemes and stress pattern correct). The system Speech Plus Prose 2000 [32] has achieved a performance of 85% word accuracy using only its letter-to-sound rules. Adding exceptions dictionaries helps improve the overall performance noticeably | a 3000-word dictionary with rules gave a 97% word accuracy.
Reference: [39] <author> Jelinek, F. </author> <title> "Up from Trigrams! The Struggle for Improved Language Model," </title> <booktitle> Proc. </booktitle> <address> Eurospeech-91, pp.1037-1041, Genova, Italy, </address> <year> 1991. </year>
Reference-contexts: Phonological rules are applied in letter-to-sound generation in speech synthesis [17]. They are also embedded in pronunciation models and networks in speech recognizers [31]. n-gram language models <ref> [39] </ref> are popular for guiding the search in speech recognizers, because they can be automatically acquired for different tasks with a wide range of perplexities, and are thus more adaptable than finite-state grammars [47] [49].
Reference: [40] <author> Klatt, D. and D. Shipman, </author> <title> "Letter-to-Phoneme Rules: A Semi-automatic Discovery Procedure," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 82, pp.737-793, </volume> <year> 1982. </year>
Reference-contexts: Induction Approaches Induction approaches attempt to infer letter-to-sound rules from a body of training data. The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in [36], <ref> [40] </ref>, [51], [61], [71] and [87]. The following briefly recounts a few of them. Klatt and Shipman [40] used a 20,000 word phonemic dictionary to create letter-to-sound rules of the form A![b]/CD EF, i.e., the letter "A" goes to the phoneme [b] in the letter environment consisting of 2 letters on <p> The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in [36], <ref> [40] </ref>, [51], [61], [71] and [87]. The following briefly recounts a few of them. Klatt and Shipman [40] used a 20,000 word phonemic dictionary to create letter-to-sound rules of the form A![b]/CD EF, i.e., the letter "A" goes to the phoneme [b] in the letter environment consisting of 2 letters on each side.
Reference: [41] <author> Klatt, D., </author> <title> "Review of Text-to-speech Conversion for English," </title> <journal> JASA 82 (3), Acoustic Society of America, </journal> <pages> pp. 737-793, </pages> <year> 1987. </year>
Reference-contexts: Great strides have been made in many areas of speech research over the past few decades. Speech synthesizers <ref> [41] </ref> have achieved a reasonable degree of clarity and naturalness, and are striving to cover unlimited vocabularies. Speech recognizers are now capable of speaker-independent, large-vocabulary, continuous speech recognition. <p> Excellent reviews can be found in [18], [29] and <ref> [41] </ref>. The various approaches have given rise to a wide range of letter-to-sound generation accuracies. Many of these accuracies are based on different corpora, and some corpora may be more difficult than others.
Reference: [42] <author> Kompe, R. et al, </author> <title> "Prosody Takes Over: Towards a Prosodically Guided Dialog System," </title> <journal> Speech Communication, </journal> <volume> Vol. 15, </volume> <pages> pp. 153-167, </pages> <year> 1994. </year>
Reference-contexts: A lower search complexity should help avoid search errors and maintain high recognition performance. Discourse and prosody have also been used in dialogue management <ref> [42] </ref>. 4 There also exists systems which attempt to obtain semantics without involving syntactic analysis, see [65] [92]. 5 Perplexity is an information-theoretic measure for the average uncertainty at the word boundary for the next possible words to follow. Later in the thesis we will show how it is computed.
Reference: [43] <author> Kucera, H. and W. N. Francis, </author> <title> Computational Analysis of Present-Day America English, </title> <publisher> Brown University Press, </publisher> <year> 1967. </year>
Reference-contexts: A small set of context-free rules are written by hand, and are used in accordance with a natural language parser to produce training parse trees from the labelled training corpus | a subset of the 10,000 most frequent words in the Brown corpus <ref> [43] </ref>. <p> In the next chapter, we will report on the generation performance of this parser. Chapter 4 Experimental Results This chapter reports on the performance of our parser for both letter-to-sound and sound-to-letter generation. Our experimental corpus consists of the 10,000 most frequent words appearing in the Brown Corpus <ref> [43] </ref>, and each lexical entry contains a spelling 1 and a single phoneme string as its pronunciation.
Reference: [44] <author> Lamel, L. F., R. H. Kassel and S. Seneff, </author> <title> "Speech Database Development: Design and Analysis of the Acoustic-Phonetic Corpus," </title> <booktitle> Proc. DARPA Speech Recognition Workshop, </booktitle> <volume> Report No. SAIC-86/1546, </volume> <pages> pp. 100-109, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: These sentences are designed especially for the study of dialectal variations. They also provide phoneme-to-phone alignments for generating our training parse trees, and the phonetic transcriptions are labelled manually through listening tests along with visual aids of the spectrogram and waveform <ref> [44] </ref>. The two "sa" sentences are: CHAPTER 7.
Reference: [45] <author> Lee, K. F., </author> <title> Automatic Speech Recognition: The development of the SPHINX system, </title> <publisher> Kluwer Publishers, </publisher> <year> 1989. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 29 3. Hidden Markov Models Parfitt and Sharman [64] have cast the problem of spelling-to-pronunciation generation in an HMM framework, which has been popular in speech recognition sytems [67] <ref> [45] </ref>. For the generation task, the HMM has phonemes as its hidden states, with trained transition and observation probabilities, and the orthographic letters as its observed outputs. <p> Phonetic transcription is totally bypassed, which makes the problem more difficult. Quad-letter models are used to represent the letter under consideration, two left letters and one right letter. These are used in conjunction with a five-gram letter language model in the Sphinx recognition system <ref> [45] </ref>. Testing on a disjoint corpus of 30 embedded and end-point detected words gave a 72.7% letter accuracy, 39.3% letter error rate and 21.2% string accuracy.
Reference: [46] <author> Lehnert, W., </author> <title> "Case-based problem solving with a Large Knowledge Based of Learned Cases," </title> <booktitle> Proc. AAAI-87, </booktitle> <pages> pp. 301-306, </pages> <address> Seattle, </address> <year> 1987. </year>
Reference-contexts: Training on 4438 words and testing on 100 novel words gave a performance accuracy of 86% per phoneme. Evaluation by six human subjects gave a word accuracy between 47% and 68%. An extension of this work is found in [80]. Another approach using case-based reasoning can be found in <ref> [46] </ref>. Golding [29] proposed a hybrid approach based on the interaction of rule-based and case-based reasoning and developed the system ANAPRON. Rules are used to implement broad trends and the cases are for pockets of exceptions. The set of rules is adapted from MITalk and foreign-language textbooks.
Reference: [47] <author> Lesser, V., R. Fennell, L. Erman and R. Reddy, </author> <title> "The Hearsay II Speech Understanding System," </title> <journal> IEEE Transactions on Acoustics Speech and Signal Processing, </journal> <volume> ASSP-23(1), </volume> <pages> pp. 11-24, </pages> <month> February </month> <year> 1975. </year> <note> BIBLIOGRAPHY 190 </note>
Reference-contexts: They are also embedded in pronunciation models and networks in speech recognizers [31]. n-gram language models [39] are popular for guiding the search in speech recognizers, because they can be automatically acquired for different tasks with a wide range of perplexities, and are thus more adaptable than finite-state grammars <ref> [47] </ref> [49]. The recognition outputs may be further re-processed using natural language parsers to provide syntactic analysis and derive meaning.
Reference: [48] <author> Lesser, V., F. Hayes-Roth, M. Birnbaum and R. Cronk, </author> <title> "Selection of Word Islands in the Hearsay-II Speech Understanding System," </title> <booktitle> Proc. ICASSP-77, </booktitle> <pages> pp. 791-794, </pages> <address> Hartford, </address> <year> 1977. </year>
Reference-contexts: These anchor points, or "islands of reliability," are then extended in a best-first manner into larger islands, until the islands culminate the speech utterance. This approach was adopted in the Hearsay-II system <ref> [48] </ref>. An island-driven approach may be better than left-to-right processing because the latter is often CHAPTER 3.
Reference: [49] <author> Lowerre, B. and R. Reddy, </author> <title> "The Harpy Speech Understanding System," Trends in Speech Recognition, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: They are also embedded in pronunciation models and networks in speech recognizers [31]. n-gram language models [39] are popular for guiding the search in speech recognizers, because they can be automatically acquired for different tasks with a wide range of perplexities, and are thus more adaptable than finite-state grammars [47] <ref> [49] </ref>. The recognition outputs may be further re-processed using natural language parsers to provide syntactic analysis and derive meaning.
Reference: [50] <author> Lucas, S. and R. Damper, </author> <title> "Syntactic Neural Networks for Bi-directional Text-phonetics Translation," in Talking Machines, theories, models and designs, </title> <editor> G. Bailly and C. </editor> <booktitle> Benoit (Eds.), </booktitle> <pages> pp. 127-142, </pages> <publisher> North-Holland Publishers, </publisher> <year> 1992. </year>
Reference-contexts: NETtalk was also re-implemented by McCulloch et al. [55] to become NETspeak, in order to examine the effects of different input and output encodings in the architecture, and of the word frequencies on network performance. Lucas and Damper <ref> [50] </ref> developed a system for bi-directional text-phonetics 9 The dot products between the output vector and the code vector of every phoneme are computed. The phoneme that has the smallest product is the "best guess" output. CHAPTER 1. <p> Since the letter accuracy and error rate add up to more than 100%, it is assured that insertion errors were omitted for letter accuracy. 3. Connectionism The aforementioned Syntactic Neural Network system <ref> [50] </ref>, which is the only reversible system we have found in the literature, gave a 71% letter accuracy and 23% word accuracy when trained and tested on two disjoint 2000-word corpora. 1.4.3 Summary of Previous Approaches Tables 1.4.3 and 1.4.3 summarize the two previous subsections. 1.5 Thesis Goals In essence, the
Reference: [51] <author> Lucassen, J. and R. Mercer, </author> <title> "An Information Theoretic Approach to the Automatic Determination of Phonemic Baseforms," </title> <booktitle> Proc. ICASSP-84, </booktitle> <pages> pp. </pages> <address> 42.5.1-42.5.3, San Diego, </address> <year> 1984. </year>
Reference-contexts: Induction Approaches Induction approaches attempt to infer letter-to-sound rules from a body of training data. The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in [36], [40], <ref> [51] </ref>, [61], [71] and [87]. The following briefly recounts a few of them. <p> If there are rule conflicts, the most popular rule in the conflicting set is used. The computer program organizes the rules into a tree for run-time efficiency, and CHAPTER 1. INTRODUCTION 28 the system achieved an accuracy of 93% correct by letter. Lucassen and Mercer <ref> [51] </ref> designed another letter-pattern learner using an information-theoretic approach. The phonemic pronunciation is viewed as being generated from the spelling via a noisy channel. The channel context consists of 4 letters to the left and right of the current letter, and the 3 phonemes to the left.
Reference: [52] <author> Luk, R. and R. Damper, </author> <title> "Inference of Letter-Phoneme Correspondences with Pre-defined Consonant and Vowel Patterns," </title> <booktitle> Proc. ICASSP-93, </booktitle> <pages> pp. 203-206, </pages> <address> Minneapolis, </address> <year> 1993. </year>
Reference-contexts: Aside from this work, HMMs have also been used for the alignment of orthography and phonemics prior to an inductive learning transliteration procedure for Dutch [86]. Another approach related to HMMs can be found in <ref> [52] </ref>. 4. Connectionist Approach A well-known example of this approach is NETtalk developed by Sejnowski and Rosenberg [72]. NETtalk is a neural network that learns the pronunciations of letters.
Reference: [53] <author> McClelland, J. L. and M. S. Seidenberg, </author> <title> "A Distributed Developmental Model of Word Recognition and Naming," </title> <journal> Psychological Review, </journal> <volume> Vol. 96, No. 4, </volume> <pages> pp. 523-568, </pages> <year> 1989. </year>
Reference-contexts: This will be examined in the following subsection. 1.3.1 Orthographic-phonological Correspondences in English The English writing system is built from the 26 letters in the alphabet. However, only certain letter sequences are found in English words. Adams [1] <ref> [53] </ref> noted that, From an alphabet of 26 letters, we could generate over 475,254 unique strings of 4 letters or less, or 12,376,630 of 5 letters or less. Alternatively, we could represent 823,543 unique strings with an alphabet of only 7 letters, or 16,777,216 with an alphabet of only 8. <p> Other lexical structures like the syllable are derived from phonotactic constraints specific to the language, so if written English largely corresponds to spoken English, then syllabic structures should be found in the orthography as well [1] <ref> [53] </ref>. The way that English orthography corresponds to morphology, syllabification and phonology is fairly systematic, but it also admits many irregularities. Therefore, English has been described as a quasi-regular system [53]. To illustrate correspondences in morphology, consider the words "preview" and "decode," which contain CHAPTER 1. <p> if written English largely corresponds to spoken English, then syllabic structures should be found in the orthography as well [1] <ref> [53] </ref>. The way that English orthography corresponds to morphology, syllabification and phonology is fairly systematic, but it also admits many irregularities. Therefore, English has been described as a quasi-regular system [53]. To illustrate correspondences in morphology, consider the words "preview" and "decode," which contain CHAPTER 1. INTRODUCTION 25 the prefix morphs "pre-" and "de-" respectively. <p> Irregularities arise due to the stress pattern of a word, different dialects (e.g. British and American English), lexical borrowings from other languages and spelling reforms, to name a few reasons <ref> [53] </ref>. Since English is quasi-regular in nature, it seems that a possible way to tackle the spelling-to-pronunciation or pronunciation-to-spelling conversion problems is to capture regularities using rules 7 These examples are borrowed from [53]. CHAPTER 1. INTRODUCTION 26 and statistics, while accommodating irregularities using exception dictionaries. <p> British and American English), lexical borrowings from other languages and spelling reforms, to name a few reasons <ref> [53] </ref>. Since English is quasi-regular in nature, it seems that a possible way to tackle the spelling-to-pronunciation or pronunciation-to-spelling conversion problems is to capture regularities using rules 7 These examples are borrowed from [53]. CHAPTER 1. INTRODUCTION 26 and statistics, while accommodating irregularities using exception dictionaries. Any attempt to determine the orthographic-phonological regularities in English must consider the two important areas of representing and deriving such regularities.
Reference: [54] <author> McCandless, M., </author> <title> Word Rejection for a Literacy Tutor, </title> <editor> S. B. </editor> <booktitle> Thesis, </booktitle> <publisher> M.I.T., </publisher> <year> 1992. </year>
Reference-contexts: They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation [94], or learn to read [8] [33] <ref> [54] </ref>. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge [14] [96].
Reference: [55] <author> McCulloch, N., M. Bedworth and J. Bridle, </author> <title> "NETspeak | a reimplementation of NETtalk," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 289-301, </pages> <year> 1987. </year>
Reference-contexts: The network was trained for 5 passes on 1,000 words and tested on a non-disjoint dictionary of 20,012 words. The "best guess" 9 performance was found to be 90% correct by letter. NETtalk was also re-implemented by McCulloch et al. <ref> [55] </ref> to become NETspeak, in order to examine the effects of different input and output encodings in the architecture, and of the word frequencies on network performance.
Reference: [56] <author> Meng, H., S. Seneff and V. Zue, </author> <title> "Phonological Parsing for Bi-directional Letter-to-Sound/Sound-to-Letter Generation," </title> <booktitle> Proc. ARPA HLT-94, </booktitle> <address> New Jersey, </address> <year> 1994. </year>
Reference: [57] <author> Meng, H., S. Seneff and V. Zue, </author> <title> "The Use of Higher Level Linguistic Knowledge for Spelling-to-Pronunciation Generation," </title> <booktitle> Proc. ISSIPNN-94, </booktitle> <pages> pp. 670-673, </pages> <address> Hong Kong, </address> <year> 1994. </year> <note> BIBLIOGRAPHY 191 </note>
Reference: [58] <author> Meng, H., S. Seneff and V. Zue, </author> <title> "Phonological Parsing for Reversible Letter-to-Sound/Sound-to-Letter Generation," </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. </pages> <address> II-1 to II-4, Ade-laide, Australia, </address> <year> 1994. </year>
Reference: [59] <author> Meng, H. M., S. Hunnicutt, S. Seneff, V. W. Zue, </author> <title> "Phonological Parsing for Bi-directional Letter-to-Sound / Sound-to-Letter Generation," </title> <journal> submitted to the Journal of Speech Communication, </journal> <month> October </month> <year> 1994. </year>
Reference-contexts: Another similar example is provided by the word "penthouse," where the letter sequence "th" is not realized as a medial fricative, due to the presence of a morph boundary in between the two letters. Morph composition also brings about spelling changes <ref> [59] </ref>. 3 For instance, the final "e" in the suffix "ize" of the word "baptized" is redundant with the "e" of the inflectional suffix "ed," and so one of the redundant letters is dropped. <p> 5 The Maximal Onset Principle states that the number of consonants in the onset position should be maximized when phonotactic and morphological constraints permit, and Stress Resyllabification refers to maximizing the number of consonants in the stressed syllables. 6 The morphological decomposition of our data is provided by Sheri Hunnicutt <ref> [59] </ref>. CHAPTER 2. THE LEXICAL REPRESENTATION 46 tree format. CHAPTER 2. THE LEXICAL REPRESENTATION 47 syllable nucleus towards the syllable margins. <p> This proved to be adequate for parsing sentences in the previous applications, where the probabilities are augmented with a semantic grammar and syntactic features to ensure agreement. Such elements are absent for our current task, and TINA's formalism led to a great deal of overgeneration while parsing words <ref> [59] </ref>. We can compensate somewhat by CHAPTER 3. THE PARSING ALGORITHM 72 writing a large number of explicit rules to filter the generated hypotheses, but this does not alleviate the heavy computational load in exploring partial theories that would later fail.
Reference: [60] <author> Nilsson, J., </author> <booktitle> Problem-solving methods in artificial intelligence, Computer Science Series, </booktitle> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Therefore, it is important to have an estimate of the future score of the path from the current column to the end. It can be shown <ref> [60] </ref> that if ^ h h, then the search algorithm is admissible, i.e., the first complete path delivered by the search will be the path with maximum log-likelihood, but admissibility is not guaranteed otherwise.
Reference: [61] <author> Oakey, S. and R. Cawthorne, </author> <title> "Inductive Learning of Pronunciation Rules by Hypothesis Testing and Correction," </title> <booktitle> Proc. IJCAI-81, </booktitle> <pages> pp. 109-114, </pages> <address> Vancouver, Canada, </address> <year> 1981. </year>
Reference-contexts: Induction Approaches Induction approaches attempt to infer letter-to-sound rules from a body of training data. The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in [36], [40], [51], <ref> [61] </ref>, [71] and [87]. The following briefly recounts a few of them.
Reference: [62] <author> Oshika, B., V. Zue, R. Weeks, H. Nue and J. </author> <title> Auerbach, "The Role of Phonological Rules in Speech Understanding Research," </title> <journal> IEEE Transactions on ASSP, </journal> <volume> Vol. ASSP-23, </volume> <pages> pp. 104-112, </pages> <year> 1975. </year>
Reference-contexts: Syntax leads to the different pronunciations between the noun and verb forms of "conduct" (/k a n d - k t/ and /k - n d ^ k t/). Coar-ticulatory effects in different phonetic contexts and across word boundaries are expressed as phonological rules <ref> [62] </ref>. Examples include the flapping of the /t/ in "water" 3 This example is borrowed from [21]. CHAPTER 1. INTRODUCTION 19 (/w O 5/) and the palatalization of the /d/ before the word boundary in "did you" (/d I J y u/). <p> EXTENDING THE HIERARCHY 126 7.1 Background In the past, handling phonological variations in the development of speech recognition sytems has been accomplished mainly by phoneticians with a set of rewrite rules <ref> [62] </ref> [99] that explains context dependencies. These rules transform phonemes into phones, and phones into other phones. The identity of the phoneme/phone sequence prior to a transformation is often not preserved, rendering the transformation irreversible. These transformations create alternate word pronunciations in the process.
Reference: [63] <author> Pallet, D., J. Fiscus, W. Fisher, J. Garofolo, B. Lund and M. Przbocki, </author> <title> "1993 Benchmark Tests for the ARPA Spoken Language Program," </title> <booktitle> Proc. of the ARPA Workshop on Human Language Technologyc, </booktitle> <pages> pp. 49-54, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Speech recognizers are now capable of speaker-independent, large-vocabulary, continuous speech recognition. The speech input may either be read or spontaneous. 1 Vocabulary sizes can range from a few thousand words to tens of thousands of words <ref> [63] </ref> and efforts to handle out-of-vocabulary words are under way [6], [35]. Natural language understanding systems can analyze a recognized sentence to obtain a meaning representation [73].
Reference: [64] <author> Parfitt, S. and R. Sharman, </author> <title> "A Bi-directional Model of English Pronuciation," </title> <booktitle> Proc. Eurospeech, </booktitle> <pages> pp. 801-804, </pages> <year> 1991. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 29 3. Hidden Markov Models Parfitt and Sharman <ref> [64] </ref> have cast the problem of spelling-to-pronunciation generation in an HMM framework, which has been popular in speech recognition sytems [67] [45]. For the generation task, the HMM has phonemes as its hidden states, with trained transition and observation probabilities, and the orthographic letters as its observed outputs. <p> For the generation task, the HMM has phonemes as its hidden states, with trained transition and observation probabilities, and the orthographic letters as its observed outputs. Based on disjoint training and test sets totalling 50,000 words, the system developed at IBM, UK <ref> [64] </ref> reported a performance of 85% accuracy per phoneme. Aside from this work, HMMs have also been used for the alignment of orthography and phonemics prior to an inductive learning transliteration procedure for Dutch [86]. Another approach related to HMMs can be found in [52]. 4.
Reference: [65] <author> Pierraccini, R., Z. Gorelov, E. Levin and E. Tzoukermann, </author> <title> "Automatic Learning in Spoken Language Understanding," </title> <booktitle> Proc. ICSLP-92, </booktitle> <pages> pp. 405-408, </pages> <address> Banff, Canada, </address> <year> 1992. </year>
Reference-contexts: A lower search complexity should help avoid search errors and maintain high recognition performance. Discourse and prosody have also been used in dialogue management [42]. 4 There also exists systems which attempt to obtain semantics without involving syntactic analysis, see <ref> [65] </ref> [92]. 5 Perplexity is an information-theoretic measure for the average uncertainty at the word boundary for the next possible words to follow. Later in the thesis we will show how it is computed. A high perplexity signifies a large search space, and a more difficult recognition problem. CHAPTER 1.
Reference: [66] <author> Price, P., </author> <title> "Evaluation of Spoken Language Systems: the ATIS Domain," </title> <booktitle> Proc. of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 91-95, </pages> <address> Hidden Valley, Pennsylvania, </address> <year> 1990. </year> <note> BIBLIOGRAPHY 192 </note>
Reference-contexts: Therefore, the advantage of the hybrid approach is to trade-off the expensive efforts in providing an elaborate set of letter-to-sound rules from linguistic 1 The layered bigrams have previously been used to parse sentences in the atis domain <ref> [66] </ref>. In this thesis, a modified version is developed for the subword parsing application. 68 CHAPTER 3. THE PARSING ALGORITHM 69 expertise, with a small set of simple rules augmented by constraints automatically discovered from a body of training data. <p> The procedure involves labelling a training corpus, writing a set of context-free rules, and boot-strapping with the natural language parser TINA [73]. TINA has previously been used with the summit recognizer [97] to parse sentences in the voyager domain [98] for navigation, and the atis domain <ref> [66] </ref> for retrieving air-travel information. The formalism of TINA derives a network from a context-free grammar, and the connecting arcs in the network are associated with probabilities.
Reference: [67] <author> Rabiner, L., </author> <title> "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition," Readings in Speech Recognition, </title> <editor> A. Waibel and K. F. Lee (Eds.), </editor> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1990. </year>
Reference-contexts: CHAPTER 1. INTRODUCTION 29 3. Hidden Markov Models Parfitt and Sharman [64] have cast the problem of spelling-to-pronunciation generation in an HMM framework, which has been popular in speech recognition sytems <ref> [67] </ref> [45]. For the generation task, the HMM has phonemes as its hidden states, with trained transition and observation probabilities, and the orthographic letters as its observed outputs.
Reference: [68] <author> Rosson, M., </author> <title> "The Interaction of Pronunciation Rules and Lexical Representation in Reading Aloud," </title> <journal> Memory and Cognition, </journal> <volume> Vol. 13, </volume> <pages> pp. 90-99, </pages> <year> 1985. </year>
Reference-contexts: INTRODUCTION 31 human subjects on 9% of the set. Another system modelled after Glushko's theory can be found in [10]. Sullivan and Damper [83] developed a system based on the dual-route theory <ref> [68] </ref>, where the duality refers to a set of context-free rules conjoined with lexical analogies. Therefore, Sullivan and Damper's system draws phonemic analogies in addition to orthographic analogies.
Reference: [69] <author> Rozin, P. and L. Gleitman, </author> <title> "The structure and acquisition of reading II: The reading process and the acquisition of the alphabetic principle," Towards a psychology of reading, </title> <editor> A. Reber & D. </editor> <booktitle> Scarborough (Eds.), </booktitle> <pages> (pp. 55-141). </pages> <address> Hillsdale, NJ:Erlbaum, </address> <year> 1977. </year>
Reference-contexts: For comparison, the total number of entries in Webster's New Collegiate Dictionary is only 150,000. Such a limited set of letters and letter patterns, however, encodes a vast body of knowledge. In fact, the graphemic constraints may very well be a consequence of this. The alphabetic principle <ref> [69] </ref> refers to the occurrence of systematic correspondences between the spoken and written forms of words | the letters and letter patterns found in written English map somewhat consistently to the speech units such as phonemes in spoken English.
Reference: [70] <author> Russell, N. H., </author> <type> personal communication. </type>
Reference-contexts: Rule "conflicts" are reconciled according to rule strengths and a "majority rules" principle. The system was trained and tested on disjoint sets of 18,000 and 2000 words respectively, and achieved an accuracy of 90% by phoneme. A similar approach was also adopted at Martin Marietta Laboratories <ref> [70] </ref>. 8 The conditional mutual information between u 1 , u 2 and u 3 is defined as log P (u 1 j u 2 ; u 3 ) P (u 1 j u 3 ) . CHAPTER 1. INTRODUCTION 29 3.
Reference: [71] <author> Segre, A., Sherwood, B. and W. Dickerson, </author> <title> "An Expert System for the Production of Phoneme Strings from Unmarked English Text using Machine-induced Rules," </title> <booktitle> Proc. First European ACL, Pisa, Italy, </booktitle> <pages> pp. 35-42, </pages> <year> 1983. </year>
Reference-contexts: Induction Approaches Induction approaches attempt to infer letter-to-sound rules from a body of training data. The rules follow the form of generative phonology, which gives a letter and its transcription under a specified spelling context. Examples of this approach can be found in [36], [40], [51], [61], <ref> [71] </ref> and [87]. The following briefly recounts a few of them. Klatt and Shipman [40] used a 20,000 word phonemic dictionary to create letter-to-sound rules of the form A![b]/CD EF, i.e., the letter "A" goes to the phoneme [b] in the letter environment consisting of 2 letters on each side.
Reference: [72] <author> Sejnowski, T. J. and C. R. Rosenberg, "NETtalk: </author> <title> parallel networks that learn to pronounce English text," </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> pp. 145-168, </pages> <year> 1987. </year>
Reference-contexts: Another approach related to HMMs can be found in [52]. 4. Connectionist Approach A well-known example of this approach is NETtalk developed by Sejnowski and Rosenberg <ref> [72] </ref>. NETtalk is a neural network that learns the pronunciations of letters.
Reference: [73] <author> Seneff, S., "TINA: </author> <title> A Natural Language System for Spoken Language Applications," </title> <journal> Computational Linguistics, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 61-86, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Natural language understanding systems can analyze a recognized sentence to obtain a meaning representation <ref> [73] </ref>. The semantics are then channelled to the appropriate locations to perform specific actions (such as sav 1 Read speech tends to be "cleaner" than spontaneous speech. <p> The procedure involves labelling a training corpus, writing a set of context-free rules, and boot-strapping with the natural language parser TINA <ref> [73] </ref>. TINA has previously been used with the summit recognizer [97] to parse sentences in the voyager domain [98] for navigation, and the atis domain [66] for retrieving air-travel information.
Reference: [74] <author> Seneff, S., H. Meng and V.Zue, </author> <title> "Language Modeling using Layered Bigrams," </title> <booktitle> Proc. ICSLP-92, </booktitle> <pages> pp. 317-320, </pages> <address> Banff, Calgary, Canada. </address>
Reference-contexts: The parse tree produced serves as training data for a probabilistic parsing algorithm based on the "layered bigrams" <ref> [74] </ref>. 1 The straightforward constraints specified by the rules, and other more subtle regularities embodied in the training parse trees, are all converted by the training procedure into a set of probabilities. <p> parsing is possible in this subword domain because the parse trees have exactly seven layers everywhere. 6 Left-to-right processing is inspired by its success in the development of speech recognition systems. 7 It allows computa 6 This was not the case when the layered bigrams were used to parse sentences <ref> [74] </ref>, where some columns in the parse tree have fewer layers than others. <p> CHAPTER 8. CONCLUSIONS AND FUTURE WORK 163 8.6 Speech Generation, Understanding and Learn ing in a Single Framework As was previously mentioned, the dimensions of the hierarchical representation can potentially be extended upwards to encompass natural language constraints <ref> [74] </ref>, prosody, discourse and even perhaps dialogue constraints, and augmented downwards to include a layer capturing phonetics and acoustics. The full speech hierarchy can provide a common framework for speech generation, understanding and learning. This thesis has mainly covered the facet of generation.
Reference: [75] <author> Shipman, D. and V. W. Zue, </author> <title> "Properties of large lexicons: Implications for advanced isolated word recognition systems," </title> <booktitle> Proc. ICASSP-82, </booktitle> <pages> pp. 546-549, </pages> <year> 1982. </year>
Reference-contexts: The layer of broad manner classes can be used for rapid lexical access (fast match) by narrowing down possible word candidates to a short list which belong to the same cohort <ref> [75] </ref>. Probabilistic phonological rules captured between the phoneme layer and the phone layer can offer alternate word pronunciations for decoding, and this should be intrinsic because the probabilities belong to part of a coherent whole in the layered bigrams framework.
Reference: [76] <author> Silverman, K., E. Blaauw, J. Spitz and J. Pitrelli, </author> <title> "A Prosodic Comparison of Spontaneous Speech and Read Speech," </title> <booktitle> Proc. ICSLP-92, </booktitle> <pages> pp. 1299-1302, </pages> <address> Banff, Canada, </address> <year> 1992. </year> <note> BIBLIOGRAPHY 193 </note>
Reference-contexts: CHAPTER 1. INTRODUCTION 19 (/w O 5/) and the palatalization of the /d/ before the word boundary in "did you" (/d I J y u/). Naturally, these rules are found in both synthesizers and recogniz-ers. Prosodics are essentials for conveying <ref> [76] </ref> and deducing the correct meaning in spoken language; and so is discourse, as can be observed from the examples above. Natural language understanding often involves parsing sentences. 4 The semantic and syntactic information extracted in the process can also be used to trim the recog-nizer's search space.
Reference: [77] <author> Spiegel, M. and M. Macchi, </author> <title> "Synthesis of Names by a Demisyllable-based Speech Synthesizer (Orator)," </title> <journal> Journal of the American Voice Input/Output Society, </journal> <volume> 7, </volume> <year> 1990. </year> <note> Special RHC/RBOC issue. </note>
Reference-contexts: This approach was evaluated on a name pronunciation task, with a case-library of 5000 names, and a separate set of 400 names for testing. The percentage of acceptable pronunciations was measured and compared with NETtalk and other commercial systems (from Bellcore <ref> [77] </ref>, Bell Labs [16], and DEC [17]). ANAPRON performed significantly better than NETtalk in this task, yielding a word accuracy of 86%, which is very close to the performance of the commercial systems. Van den Bosch et al. [88] experimented with two data-oriented methods for gra CHAPTER 1.
Reference: [78] <author> Stanfill, C. and D. Waltz, </author> <title> "Toward Memory-Based Reasoning," </title> <journal> Communications of the ACM, </journal> <volume> 12(12), </volume> <pages> pp. 1213-1228, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: If everything fails, then TTS will fall back on a rule-based system named NAMSA for prefix and suffix analysis and stress reassignment. MBRtalk <ref> [78] </ref> [79] is a pronunciation system operating within the memory-based reasoning paradigm. The primary inference mechanism is a best-match recall from memory. A data record is generated for every letter in a training word.
Reference: [79] <author> Stanfill, C., </author> <title> "Memory-Based Reasoning Applied to English Pronunciation," </title> <booktitle> Proc. AAAI-87, </booktitle> <pages> pp. 577-581, </pages> <year> 1987. </year>
Reference-contexts: If everything fails, then TTS will fall back on a rule-based system named NAMSA for prefix and suffix analysis and stress reassignment. MBRtalk [78] <ref> [79] </ref> is a pronunciation system operating within the memory-based reasoning paradigm. The primary inference mechanism is a best-match recall from memory. A data record is generated for every letter in a training word.
Reference: [80] <author> Stanfill, C., </author> <title> "Learning to Read: A Memory-based Model," </title> <booktitle> Proc. of the Case-based Reasoning Workshop, </booktitle> <address> Clearwater Beach, FL, </address> <pages> pp. 406-413, </pages> <year> 1988. </year>
Reference-contexts: Training on 4438 words and testing on 100 novel words gave a performance accuracy of 86% per phoneme. Evaluation by six human subjects gave a word accuracy between 47% and 68%. An extension of this work is found in <ref> [80] </ref>. Another approach using case-based reasoning can be found in [46]. Golding [29] proposed a hybrid approach based on the interaction of rule-based and case-based reasoning and developed the system ANAPRON. Rules are used to implement broad trends and the cases are for pockets of exceptions.
Reference: [81] <author> Stevens, K., </author> <title> Unpublished course notes for Speech Communication, </title> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Spring term, </institution> <year> 1989. </year>
Reference-contexts: This is shown in The fourth layer, syllable parts, also provides tactics for the two successive layers of distinctive features [22] <ref> [81] </ref>. The sequence of broad classes (manner features) in the fifth layer bears the Sonority Sequencing Constraint.
Reference: [82] <author> B. Suhm, M. Woszczyna, and A. Waibel, </author> <title> "Detection and transcription of new words," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 2179-2182, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Words in a highly inflected language like English can be collapsed together according to similar morphological compositions. Allen has estimated a savings factor of 10 if a lexicon stores morphemes instead of all possible forms of words [2] [13]. Suhm et al <ref> [82] </ref> performed a study on the orthographic transcriptions in the Wall Street Journal (WSJ) domain. They classified more than 1,000 words that lie outside a known vocabulary of approximately 14,000 words, and found that 45% were inflections of in-vocabulary words, and 6% were concatenations of in-vocabulary words.
Reference: [83] <author> Sullivan, K. and R. Damper, </author> <title> "Novel-word Pronunciation Within a Text-to-speech System," in Talking Machines, theories, models and designs, </title> <editor> G. Bailly and C. </editor> <booktitle> Benoit (Eds.), </booktitle> <pages> pp. 183-195, </pages> <publisher> North Holland Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The system was evaluated on a set of 70 nonsense monosyllabic words, and was found to disagree with CHAPTER 1. INTRODUCTION 31 human subjects on 9% of the set. Another system modelled after Glushko's theory can be found in [10]. Sullivan and Damper <ref> [83] </ref> developed a system based on the dual-route theory [68], where the duality refers to a set of context-free rules conjoined with lexical analogies. Therefore, Sullivan and Damper's system draws phonemic analogies in addition to orthographic analogies. <p> The primary inference mechanism is a best-match recall from memory. A data record is generated for every letter in a training word. Each record contains the current letter, the previous three letters, the 10 This terminology is adopted from the reference <ref> [83] </ref>. CHAPTER 1. INTRODUCTION 32 next three letters, and the phoneme and stress assigned to the current letter. For each letter in the test word, the system retrieves the 10 data records that are most "similar" to the letter under consideration. A special dissimilarity metric is used for the retrieval.

Reference: [90] <author> Vitale, T., </author> <title> "Foreign Language Speech Synthesis: </title> <booktitle> Linguistics and Speech Technology," Proc. of the Voice I/O System Applications Conference, </booktitle> <pages> pp. 363-370, </pages> <address> San Francisco, </address> <year> 1985. </year>
Reference-contexts: Some languages, such as Chinese, do not lend themselves easily to translating graphemes into phonemes. Some other languages may have a close fit between its graphemic and phonemic forms, e.g. Spanish can be thoroughly characterized by approximately 35 to 40 letter-to-sound rules <ref> [90] </ref>. English, in comparison, is much more complicated, due to numerous loan words that are not anglicized. CHAPTER 8. CONCLUSIONS AND FUTURE WORK 161 Maker is used for text-to-speech synthesis, and supports a multi-level synchronized data structure called a grid.
Reference: [91] <author> Wang, W. J., W. N. Campbell, N. Iwahashi & Y Sagisaka, </author> <title> "Tree-based Unit Selection for English Speech Synthesis," </title> <booktitle> Proc. ICASSP-93, </booktitle> <pages> pp. </pages> <note> II-191 to II-194. </note>
Reference-contexts: terminal layer is used as a dual representation of letters and phones, this hierarchical lexical representation can be used to capture phonological rules between the preterminal and terminal layers of phonemes and phones. 7 In some recently published experiments on tree-based unit selection for English speech synthesis, Sagisaka et al. <ref> [91] </ref> confirmed that syllabic stress, as well as the place and manner of voicing of the preceding phonemes, are important context variables. CHAPTER 2.
Reference: [92] <author> Ward W., and S. Issar, </author> <title> "Recent Improvements in the CMU Spoken Language Understanding System," </title> <booktitle> Proc. of the ARPA Workshop on Human Language Technology, </booktitle> <pages> pp. 213-216, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: A lower search complexity should help avoid search errors and maintain high recognition performance. Discourse and prosody have also been used in dialogue management [42]. 4 There also exists systems which attempt to obtain semantics without involving syntactic analysis, see [65] <ref> [92] </ref>. 5 Perplexity is an information-theoretic measure for the average uncertainty at the word boundary for the next possible words to follow. Later in the thesis we will show how it is computed. A high perplexity signifies a large search space, and a more difficult recognition problem. CHAPTER 1.
Reference: [93] <author> Weintraub, M., and J. Bernstein, </author> <title> "RULE: A System for Constructing Recognition Lexicons," </title> <booktitle> Proc. DARPA Speech Recognition Workshop, </booktitle> <volume> Report No. SAIC-87/1644, </volume> <pages> pp. 44-48, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: This involves following through successive rule applications, which is a difficult task because the rule transformations are irreversible and this makes back-tracing either tedious or impossible. A previous attempt has also been made which attaches probabilities to the rules productions <ref> [93] </ref>. The probability of a given rule reflects the number of times the rule is actually applied, normalized over the number of times the rule can be applied. Each rule is compiled to form a series of rule clauses specifying the rule's transformations on a phone lattice under different contexts.
Reference: [94] <author> Woszczyna, M. et al, </author> <title> "Towards Spontaneous Speech Translation," </title> <booktitle> Proc. ICASSP-94, </booktitle> <pages> pp. 345-348, </pages> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: The systems accept spontaneous speech as input and respond with synthesized speech as output. They enable the user to solve problems within the designated domain (such as trip planning, weather inquiries, etc.) [24] [28], convey a spoken message with another language via machine translation <ref> [94] </ref>, or learn to read [8] [33] [54]. The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge [14] [96].
Reference: [95] <author> Yannakoudakis, E. and P. Hutton, </author> <title> "Generation of spelling rules from phonemes and their implications for large dictionary speech recognition," </title> <journal> Speech Communication, </journal> <volume> Vol. 10, </volume> <pages> pp. 381-394, </pages> <year> 1991. </year>
Reference-contexts: We know of three approaches that have previously been adopted: 1. A Combined Rule-based and Inductive Approach The rule formalism in generative phonology is also used in generating spelling rules <ref> [95] </ref>. Two lexicons of respective sizes 96,939 and 11,638 were transcribed with one-to-one phoneme-to-grapheme matches, using the /null/ phoneme and "null" letter when necessary. Upon analysis of the lexicons, it was felt that there was insufficient consistency for a rule-based system.
Reference: [96] <author> Zue, V., </author> <title> "The Use of Speech Knowledge in Automatic Speech Recognition," </title> <journal> IEEE Proceedings, </journal> <volume> Vol. 73, No. 11, </volume> <pages> pp. 1062-1615, </pages> <month> November </month> <year> 1985. </year> <note> BIBLIOGRAPHY 195 </note>
Reference-contexts: The development of conversational systems necessitates correct interpretation of spoken input, and accurate generation of spoken output. Decoding the semantics embedded in an acoustic signal, or encoding a message in synthesized speech, involve diverse sources of linguistic knowledge [14] <ref> [96] </ref>. Amongst these are: * Signal processing | the transformation of a continuously-varying acoustic speech signal into a discrete form. * Phonology and acoustic-phonetics | the study of speech sounds, their variabilities as a result of coarticulation, as well as their acoustic characteristics.
Reference: [97] <author> Zue, V., J. Glass, M. Phillips and S. Seneff, </author> <title> "The MIT summit Speech Recognition System: a Progress Report," </title> <booktitle> Proc. ARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 21-23, </pages> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: The procedure involves labelling a training corpus, writing a set of context-free rules, and boot-strapping with the natural language parser TINA [73]. TINA has previously been used with the summit recognizer <ref> [97] </ref> to parse sentences in the voyager domain [98] for navigation, and the atis domain [66] for retrieving air-travel information. The formalism of TINA derives a network from a context-free grammar, and the connecting arcs in the network are associated with probabilities.
Reference: [98] <author> Zue, V., J. Glass, D. Goodine, H. Leung, M. Phillips and S. Seneff, </author> <title> "The voyager Speech Understanding system: Preliminary Development and Evaluation," </title> <booktitle> in Proc. ICASSP-90, </booktitle> <pages> pp. 73-76, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The procedure involves labelling a training corpus, writing a set of context-free rules, and boot-strapping with the natural language parser TINA [73]. TINA has previously been used with the summit recognizer [97] to parse sentences in the voyager domain <ref> [98] </ref> for navigation, and the atis domain [66] for retrieving air-travel information. The formalism of TINA derives a network from a context-free grammar, and the connecting arcs in the network are associated with probabilities.
Reference: [99] <author> Zue, V., J. Glass, D. Goodine, M. Phillips and S. Seneff, </author> <title> "The summit Speech Recognition System: Phonological Modelling and Lexical Access," </title> <booktitle> Proc. ICASSP, </booktitle> <pages> pp. 49-52, </pages> <address> Albuquerque, NM, </address> <year> 1990. </year>
Reference-contexts: EXTENDING THE HIERARCHY 126 7.1 Background In the past, handling phonological variations in the development of speech recognition sytems has been accomplished mainly by phoneticians with a set of rewrite rules [62] <ref> [99] </ref> that explains context dependencies. These rules transform phonemes into phones, and phones into other phones. The identity of the phoneme/phone sequence prior to a transformation is often not preserved, rendering the transformation irreversible. These transformations create alternate word pronunciations in the process.
Reference: [100] <author> Zue, V., J. Glass, D. Goddeau, D. Goodine, C. Pao, M. Phillips, J. Polifroni and S. Seneff, </author> <title> "Pegasus: A Spoken Dialogue Interface for On-line Air Travel Planning," </title> <journal> Speech Communication, </journal> <volume> Vol. 15, </volume> <pages> pp. 331-340, </pages> <year> 1995. </year>
Reference-contexts: Interactive information retrieval via speech also requires a language generation component for response generation [25]. The combined applications of these four branches of technology, namely, speech synthesis, speech recognition, language understanding and language generation, has brought about the recent advent of conversational systems [23] <ref> [100] </ref> . These systems can carry out a conversational dialogue with the user concerning topics in a restricted domain (or multiple restricted domains [28]). The systems accept spontaneous speech as input and respond with synthesized speech as output.
References-found: 94

