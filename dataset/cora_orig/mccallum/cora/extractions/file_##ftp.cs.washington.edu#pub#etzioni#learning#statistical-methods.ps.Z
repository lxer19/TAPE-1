URL: file://ftp.cs.washington.edu/pub/etzioni/learning/statistical-methods.ps.Z
Refering-URL: http://www.cs.washington.edu/research/projects/softbots/www/bib.html
Root-URL: 
Email: etzioni@cs.washington.edu  
Title: Statistical Methods for Analyzing Speedup Learning Experiments  
Author: Oren Etzioni Ruth Etzioni Fred Hutchinson 
Date: June 8, 1993  
Note: To appear in machine learning  
Address: Seattle, WA 98195  Seattle, WA 98104  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  Cancer Research Center Division of Public Health Sciences  
Abstract: Speedup learning systems are typically evaluated by comparing their impact on a problem solver's performance. The impact is measured by running the problem solver, before and after learning, on a sample of problems randomly drawn from some distribution. Often, the experimenter imposes a bound on the CPU time the problem solver is allowed to spend on any individual problem. Segre et al. [ 1991 ] argue that the experimenter's choice of time bound can bias the results of the experiment. To address this problem, we present statistical hypothesis tests specifically designed to analyze speedup data and eliminate this bias. We apply the tests to the data reported in [ Etzioni, 1990a ] , and show that most (but not all) of the speedups observed are statistically significant. y The statistical tests described in this paper are encoded as Common Lisp routines. The routines, and the data analyzed in the paper, are available by sending mail to etzioni@cs.washington.edu. We hope that other researchers will use the routines to validate their own speedup learning experiments. z Affiliate Assistant Professor, Department of Biostatistics, University of Washington. 
Abstract-found: 1
Intro-found: 1
Reference: [ Brown and Hollander, 1977 ] <author> Brown, Jr. B.W. and Hollander, </author> <title> M 1977. Statistics: A Biomedical Introduction. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: If several tests are being performed, each at significance level 0.05, then the chance that at least one null hypothesis will be rejected in error is substantially larger than 0.05; this is called the multiple comparisons problem <ref> [ Brown and Hollander, 1977 ] </ref> . For k independent tests, each conducted at level ff, the probability of at least one incorrect rejection of the null is 1 (1 ff) k .
Reference: [ Cohen and Kim, 1993 ] <author> Cohen, Paul R. and Kim, John B. </author> <year> 1993. </year> <title> A Bootstrap Test for Comparing Performance of Programs When Data are Censored, and Comparisons to Etzioni's Test. </title> <type> Unpublished, </type> <institution> University of Massachussetts, Amherst. </institution>
Reference-contexts: tradeoff is appropriate because we can compensate for decreased sensitivity by increased sample size and, in speedup experiments, large samples 2 Data point 5 in Table 1 is an example of a doubly censored data point. 3 We thank Charles Elkan and Craig Knoblock for making this point. 4 See <ref> [ Cohen and Kim, 1993 ] </ref> for a more sensitive statistical test, which is contrasted with our own. 6 are easy and inexpensive to generate.
Reference: [ DeGroot, 1986 ] <author> DeGroot, Morris H. </author> <year> 1986. </year> <title> Probability and Statistics. </title> <publisher> Addison Wesley, second edition. </publisher>
Reference-contexts: Unfortunately, the distributional assumption severely restricts the generality of the test. In fact, the actual distributions observed in many speedup learning have no straight-forward mathematical characterization|they are certainly not normal. Thus, using a test that presupposes a normal distribution, such as the matched-pair t-test <ref> [ DeGroot, 1986 ] </ref> , is inappropriate. The ideal statistical test would test for "average speedup," which is the intuitive notion employed in the machine-learning literature, without making distributional assumptions. It is easy to see that, given censored data, no such test exists. <p> This proportion can be computed using the binomial formula as follows: p-value = n X n! 2 : The normal approximation to the binomial distribution can be used if n is larger than 25 <ref> [ DeGroot, 1986 ] </ref> . In many experiments the significance level, ff, is taken to be 0.05, but there is no reason that this value should be adopted in all situations. It is up to the experimenter to decide what significance level is appropriate.
Reference: [ Etzioni, 1990a ] <author> Etzioni, </author> <title> Oren 1990a. A Structural Theory of Explanation-Based Learning. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution> <note> Available as technical report CMU-CS-90-185. </note>
Reference-contexts: Section 2 reviews the statistical background necessary to understand the paper, describes standard statistical methods for analyzing censored data, and considers their strengths and weaknesses. Section 3 describes the data set we use to illustrate our approach. The data set, taken from <ref> [ Etzioni, 1990a ] </ref> , compares the performance of prodigy, ebl, static, and human experts on prodigy's benchmark tasks. Section 4 introduces the statistical tests we propose. <p> If we fail to reject the null hypothesis, our tests are inconclusive; we can never conclude that the null hypothesis is true. 3 Speedup Learning Data We demonstrate the value of our approach by applying it to speedup learning data taken from <ref> [ Etzioni, 1990a ] </ref> . The data set compares the performance of the prodigy problem solver in the absence of control knowledge, to the performance of prodigy guided by the control rules generated by ebl, static, and by human experts. <p> The tests interpret truncated or censored data in a maximally conservative manner, eliminating bias due to the experimenter's choice of resource bound. We applied both tests to the speedup learning data set taken from <ref> [ Etzioni, 1990a ] </ref> and have shown that most of the differences observed are statistically significant (see, in particular, the results of our extended signed rank test in Table 6).
Reference: [ Etzioni, 1990b ] <author> Etzioni, </author> <title> Oren 1990b. Why Prodigy/EBL works. </title> <booktitle> In Proceedings of AAAI-90. </booktitle>
Reference: [ Gibbons, 1971 ] <author> Gibbons, Jean Dickinson 1971. </author> <title> Nonparametric Statistical Inference. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: We provide a brief description of the basic concepts and procedure below. See <ref> [ Gibbons, 1971, Wilks, 1962 ] </ref> , or any standard statistics textbook for a detailed exposition. Broadly speaking, the goal is to estimate, using the data, a "state of nature", or an underlying data-generating mechanism from a finite space of possibilities.
Reference: [ Hajek and Sidak, 1967 ] <author> Hajek, J. and Sidak, Z. </author> <year> 1967. </year> <title> Theory of Rank Tests. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: To address this problem, we turn to the more sensitive signed rank test. This test is a member of the class of linear rank methods, methods based on the rank of the observed pair differences <ref> [ Hajek and Sidak, 1967 ] </ref> . 4.1 The sign test The sign test is based on the sign of the difference between the observations in a pair. Suppose that systems s and f are being compared.
Reference: [ Hemelryk, 1952 ] <author> Hemelryk, J. </author> <year> 1952. </year> <title> A theorem on the sign test when ties are present. </title> <journal> Indagationes Math. </journal> <volume> 14 </volume> <pages> 322-326. </pages>
Reference-contexts: One solution to this problem is to discard the tied pairs and to perform the sign test on the remaining data. A second solution is to count half the tied pairs as positive differences and half as negative differences. See <ref> [ Hemelryk, 1952, Lehmann, 1975 ] </ref> for analyses of the two solutions. We choose the second solution here because it is more conservative.
Reference: [ Holt and Prentice, 1974 ] <author> Holt, J. D. and Prentice, R. L. </author> <year> 1974. </year> <title> Survival analysis in twin studies and matched pair experiments. </title> <journal> Biometrika 61 </journal> <pages> 17-30. </pages>
Reference-contexts: Consider, for example, doubly censored data in which the problem-solving time (or the survival time) is truncated for both systems being studied. 2 It is standard statistical practice to discard such data and only analyze the singly censored and uncensored pairs in the sample <ref> [ Holt and Prentice, 1974, Woolson and Lachenbruch, 1980 ] </ref> . However, this practice amounts to assuming that the relative performance of the two systems as observed in the uncensored and singly censored data extrapolates to the doubly censored data.
Reference: [ Kalbfleisch and Prentice, 1980 ] <author> Kalbfleisch, J.D. and Prentice, R.L. </author> <year> 1980. </year> <title> The Statistical Analysis of Failure Time Data. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Experiment: speedup learning trial clinical trial Elements compared: problem solvers treatments Termination criterion: problem solved death of patient Data: solution time survival time Censoring due to: resource bound end of follow-up A large body of statistical theory has been developed for survival analysis; <ref> [ Kalbfleisch and Prentice, 1980 ] </ref> is a classic reference. However, we have found that the theory relies on stronger assumptions than are warranted in the analysis of speedup learning data.
Reference: [ Kambhampati and Chen, 1993 ] <author> Kambhampati, Subbarao and Chen, </author> <month> Jengchin </month> <year> 1993. </year> <title> Relative utility of ebg based plan reuse in partial ordering vs. total ordering planning. </title> <booktitle> In Proceedings of 11th Natl. Conf. on Artifical Intelligence (AAAI-93). </booktitle> <publisher> MIT Press(AAAI). </publisher>
Reference-contexts: However, 13 both of our tests can be used as indirect evidence for an average speedup hypothesis. While we hope that other researchers will use our tests to validate their own speedup experiments (see, for example, <ref> [ Kambhampati and Chen, 1993, Knoblock, 1993, Minton, 1993 ] </ref> ) we offer three final caveats. First, as with any statistical test, failure to reject the null hypothesis is inconclusive; it is not a basis for concluding that system s is at least as fast as system f .
Reference: [ Knoblock, 1990 ] <author> Knoblock, Craig A. </author> <year> 1990. </year> <title> Learning abstraction hierarchies for problem solving. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference: [ Knoblock, 1993 ] <author> Knoblock, Craig A. </author> <year> 1993. </year> <title> Automatically generating abstractions for planning. </title> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: However, 13 both of our tests can be used as indirect evidence for an average speedup hypothesis. While we hope that other researchers will use our tests to validate their own speedup experiments (see, for example, <ref> [ Kambhampati and Chen, 1993, Knoblock, 1993, Minton, 1993 ] </ref> ) we offer three final caveats. First, as with any statistical test, failure to reject the null hypothesis is inconclusive; it is not a basis for concluding that system s is at least as fast as system f .
Reference: [ Lehmann, 1975 ] <author> Lehmann, E. L. </author> <year> 1975. </year> <title> Nonparametrics: Statistical Methods Based on Ranks. Holden Day, </title> <address> San Fransisco. </address> <month> 15 </month>
Reference-contexts: One solution to this problem is to discard the tied pairs and to perform the sign test on the remaining data. A second solution is to count half the tied pairs as positive differences and half as negative differences. See <ref> [ Hemelryk, 1952, Lehmann, 1975 ] </ref> for analyses of the two solutions. We choose the second solution here because it is more conservative. <p> The 6 Ten of the remaining eleven problems are ties, and one problem is doubly censored. 10 alternate hypothesis is that the pair differences are slanted towards positive (or negative) values, in a sense made precise by Lehmann <ref> [ Lehmann, 1975, page 157 ] </ref> . Under the null hypothesis, we expect the sum of the ranks corresponding to the positive differences to be at least as large as the sum of the ranks corresponding to the negative differences.
Reference: [ Minton, 1988a ] <author> Minton, S. </author> <year> 1988a. </year> <title> Quantitative Results Concerning the Utility of Explanation-Based Learning. </title> <booktitle> In Proceedings of AAAI-88. </booktitle> <pages> 564-569. </pages>
Reference: [ Minton, 1988b ] <author> Minton, </author> <title> Steven 1988b. Learning Effective Search Control Knolwedge: An Explanation-Based Approach. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution> <note> Available as technical report CMU-CS-88-133. </note>
Reference-contexts: Specifically, we analyze the pairwise comparisons prodigy versus static, static versus ebl, and ebl versus the human experts, on each of prodigy's benchmark tasks (the Blocksworld, Extended-Stripsworld, and Schedworld problem spaces). The problem sets, the human control rules, and the problem space definitions are taken from <ref> [ Minton, 1988b ] </ref> . prodigy's total problem-solving time and the number of censored data points, in each experimental setting, are summarized in Table 4.
Reference: [ Minton, 1993 ] <author> Minton, S. </author> <year> 1993. </year> <title> Integrating heuristics for constraint satisfaction problems: A case study. </title> <booktitle> In AAAI-93 Proceedings. </booktitle>
Reference-contexts: However, 13 both of our tests can be used as indirect evidence for an average speedup hypothesis. While we hope that other researchers will use our tests to validate their own speedup experiments (see, for example, <ref> [ Kambhampati and Chen, 1993, Knoblock, 1993, Minton, 1993 ] </ref> ) we offer three final caveats. First, as with any statistical test, failure to reject the null hypothesis is inconclusive; it is not a basis for concluding that system s is at least as fast as system f .
Reference: [ Mooney, 1989 ] <author> Mooney, Raymond J. </author> <year> 1989. </year> <title> The effect of rule use on the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <pages> 725-730. </pages>
Reference: [ O'Rorke, 1989 ] <author> O'Rorke, P. </author> <year> 1989. </year> <title> LT revisited: Explanation-based learning and the logic of Principia Mathematica. </title> <booktitle> Machine Learning 4(2) </booktitle> <pages> 117-160. </pages>
Reference: [ Segre et al., 1991 ] <author> Segre, Alberto; Elkan, Charles; and Russell, </author> <title> Alexander 1991. A critical look at experimental evaluations of EBL. </title> <booktitle> Machine Learning 6(2). </booktitle>
Reference: [ Shavlik, 1990 ] <author> Shavlik, Jude W. </author> <year> 1990. </year> <title> Acquiring recursive concepts and iterative concepts with explanation-based learning. </title> <booktitle> Machine Learning 5(1). </booktitle>
Reference: [ Wilks, 1962 ] <author> Wilks, Samuel S. </author> <year> 1962. </year> <title> Mathematical Statistics. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: We provide a brief description of the basic concepts and procedure below. See <ref> [ Gibbons, 1971, Wilks, 1962 ] </ref> , or any standard statistics textbook for a detailed exposition. Broadly speaking, the goal is to estimate, using the data, a "state of nature", or an underlying data-generating mechanism from a finite space of possibilities.
Reference: [ Woolson and Lachenbruch, 1980 ] <editor> Woolson, R.F. and Lachenbruch, </editor> <address> P.A. </address> <year> 1980. </year> <title> Rank tests for censored matched pairs. </title> <journal> Biometrika 67 </journal> <pages> 597-606. 16 </pages>
Reference-contexts: Consider, for example, doubly censored data in which the problem-solving time (or the survival time) is truncated for both systems being studied. 2 It is standard statistical practice to discard such data and only analyze the singly censored and uncensored pairs in the sample <ref> [ Holt and Prentice, 1974, Woolson and Lachenbruch, 1980 ] </ref> . However, this practice amounts to assuming that the relative performance of the two systems as observed in the uncensored and singly censored data extrapolates to the doubly censored data. <p> Thus, ties have less impact on the result of the signed rank test than on that of the sign test. 4.2.1 Censored Data The standard censored-data extension of the signed rank test is quite technical, so we omit its description here (see <ref> [ Woolson and Lachenbruch, 1980 ] </ref> ). We note, however, that the standard extension makes two important assumptions. First, although the procedure is rank-based, p-values are in fact computed under a distributional assumption about the pair differences.
References-found: 23

