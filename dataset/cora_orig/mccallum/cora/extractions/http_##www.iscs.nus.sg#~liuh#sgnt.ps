URL: http://www.iscs.nus.sg/~liuh/sgnt.ps
Refering-URL: 
Root-URL: 
Email: w.wen@trl.oz.au liuh@iscs.nus.sg  
Title: Supervised Learning for Self-Generating Neural Networks  
Author: Wilson X. Wen Huan Liu 
Address: Clayton, Victoria 3168 Kent Ridge Australia Singapore 0511  
Affiliation: AI Systems Dept. of ISCS Telecom Research Laboratories National University of Singapore  
Abstract: In this paper, supervised learning for Self-Generating Neural Networks (SGNN) method, which was originally developed for the purpose of unsupervised learning, is discussed. An information analytical method is proposed to assign weights to attributes in the training examples if class information is available. This significantly improves the learning speed and the accuracy of the SGNN classifier. The performance of the supervised version of SGNN is analyzed and compared with those of other well-known supervised learning methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algo rithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: This significantly improves learning time and the predictive accuracy of the SGNN classifiers. The method is different from the conventional supervised learning methods, such as ID3 [5, 7], CN2 <ref> [1] </ref> and Backpropagation (BP) [8, 4], and can find its analogy with self-learning or learning from textbook. Class information is only used in assigning weights and not used in training for reinforcement.
Reference: [2] <author> L. Fang, A.Jennings, W.Wen, K.Li, and T.Li. </author> <title> Un supervised learning in a self-organizing tree. </title> <booktitle> In Proc. IJCNN'91 (International Joint Conf. on Neural Networks), </booktitle> <address> Singapore, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: It is fairly tricky to choose the right structure of the neural network suitable for a particular application at hand. In this section, we briefly discuss the SGNT method proposed in [11] to generate neural trees <ref> [2] </ref> automatically from the training examples without human intervention. For this kind of networks, not only the weights of the network connections but also the structure of the whole network are learned from the training examples directly.
Reference: [3] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Since the SGNN/SGNT classifier does not make use of the class information even when this information is 1 These two are the variations of Fisher's COBWEB <ref> [3] </ref>. available, it cannot compete with the classifiers developed by some well known supervised methods (see Table 2 starting from the second row). Therefore, it seems necessary to find a way for the SGNN/SGNT method to make use of the available class information. <p> The most difficult one is M 2 , one has to examine all the six attributes of any example to tell whether it belongs to the right class. Methods like SGNT/SGNN or COBWEB <ref> [3] </ref>, will not perform well for these examples. The reason is twofold: firstly, these methods are unsupervised; secondly, these methods treat all attributes equally important, but in some (M 1 and M 3 ) of these problems some attributes may be much more important than the others.
Reference: [4] <editor> J. McClelland and D. Rumelhart. </editor> <booktitle> Parallel Dis tributed Processing, </booktitle> <volume> volume 2. </volume> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This significantly improves learning time and the predictive accuracy of the SGNN classifiers. The method is different from the conventional supervised learning methods, such as ID3 [5, 7], CN2 [1] and Backpropagation (BP) <ref> [8, 4] </ref>, and can find its analogy with self-learning or learning from textbook. Class information is only used in assigning weights and not used in training for reinforcement.
Reference: [5] <author> J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In this paper, an information analytical method is proposed to assign weights to attributes in the training examples using class information. This significantly improves learning time and the predictive accuracy of the SGNN classifiers. The method is different from the conventional supervised learning methods, such as ID3 <ref> [5, 7] </ref>, CN2 [1] and Backpropagation (BP) [8, 4], and can find its analogy with self-learning or learning from textbook. Class information is only used in assigning weights and not used in training for reinforcement. <p> In effect, A 3 , A 4 , and A 6 are completely irrelevant in M 1 , and A 1 ,A 3 , and A 6 in M 3 to the classification problems. Methods such ID3 <ref> [5] </ref> and the like should perform better than the above mentioned methods not only because they are supervised, but also because they discriminate some minor attributes from the others in one way or another.
Reference: [6] <author> J. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> Interna tional Journal of Man-Machine Studies, 27:221 -234, </journal> <year> 1987. </year>
Reference-contexts: An SGNT branch is called dead if the number of examples covered by its root does not increase during the repeated training. Finally, it is possible to simplify an SGNT by a method similar to Quin-lan's method of decision tree simplification <ref> [6] </ref>. For more detailed information about the SGNT algorithm, optimization, pruning, and simplification, please refer to [11].
Reference: [7] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, an information analytical method is proposed to assign weights to attributes in the training examples using class information. This significantly improves learning time and the predictive accuracy of the SGNN classifiers. The method is different from the conventional supervised learning methods, such as ID3 <ref> [5, 7] </ref>, CN2 [1] and Backpropagation (BP) [8, 4], and can find its analogy with self-learning or learning from textbook. Class information is only used in assigning weights and not used in training for reinforcement.
Reference: [8] <editor> D. Rumelhart, J. McClelland, </editor> <booktitle> and the PDP Rearch Group. Parallel Distributed Processing, volume 1. </booktitle> <address> Cambridge, Mass. </address> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This significantly improves learning time and the predictive accuracy of the SGNN classifiers. The method is different from the conventional supervised learning methods, such as ID3 [5, 7], CN2 [1] and Backpropagation (BP) <ref> [8, 4] </ref>, and can find its analogy with self-learning or learning from textbook. Class information is only used in assigning weights and not used in training for reinforcement.

Reference: [10] <author> W. Wen, A. Jennings, and H. Liu. </author> <title> Self-generating neural networks and their applications to telecommunications. </title> <booktitle> In Proc. of ICCT'92 (International Conference on Communication Technology, </booktitle> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION The SGNN (Self-Generating Neural Network) method proposed in [11] is an unsupervised learning method. It has been applied to different application areas such as image coding, diagnostic expert system and docu-ment/information retrieval system <ref> [10] </ref>. Although the SGNN method performs quite satisfactorily comparing with other unsupervised learning methods, it cannot compete with other supervised learning methods in some cases since the class information has not been used.
Reference: [11] <author> W. Wen, H. Liu, and A. Jennings. </author> <title> Self-generating neural networks. </title> <booktitle> In Proc. of IJCNN'92 Baltimore, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION The SGNN (Self-Generating Neural Network) method proposed in <ref> [11] </ref> is an unsupervised learning method. It has been applied to different application areas such as image coding, diagnostic expert system and docu-ment/information retrieval system [10]. <p> It is fairly tricky to choose the right structure of the neural network suitable for a particular application at hand. In this section, we briefly discuss the SGNT method proposed in <ref> [11] </ref> to generate neural trees [2] automatically from the training examples without human intervention. For this kind of networks, not only the weights of the network connections but also the structure of the whole network are learned from the training examples directly. <p> Finally, it is possible to simplify an SGNT by a method similar to Quin-lan's method of decision tree simplification [6]. For more detailed information about the SGNT algorithm, optimization, pruning, and simplification, please refer to <ref> [11] </ref>. The performance of the SGNT method described early is quite satisfactory in both accuracy and speed aspects comparing with some well-known unsupervised learning methods such as CLASSWEB and ECOBWEB 1 (see Table 1, where the unit of time is second).
References-found: 10

