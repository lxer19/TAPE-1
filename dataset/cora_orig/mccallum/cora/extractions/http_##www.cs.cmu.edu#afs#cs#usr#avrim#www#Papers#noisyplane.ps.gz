URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/noisyplane.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/People/kannan/Papers/pubs.html
Root-URL: 
Title: A Polynomial-time Algorithm for Learning Noisy Linear Threshold Functions  
Author: Avrim Blum Alan Frieze Ravi Kannan Santosh Vempala 
Abstract: In this paper we consider the problem of learning a linear threshold function (a half-space in n dimensions, also called a "perceptron"). Methods for solving this problem generally fall into two categories. In the absence of noise, this problem can be formulated as a Linear Program and solved in polynomial time with the Ellipsoid Algorithm or Interior Point methods. Alternatively, simple greedy algorithms such as the Perceptron Algorithm are often used in practice and have certain provable noise-tolerance properties; but, their running time depends on a separation parameter, which quantifies the amount of "wiggle room" available for a solution, and can be exponential in the description length of the input. In this paper, we show how simple greedy methods can be used to find weak hypotheses (hypotheses that correctly classify noticeably more than half of the examples) in polynomial time, without dependence on any separation parameter. Suitably combining these hypotheses results in a polynomial-time algorithm for learning linear threshold functions in the PAC model in the presence of random classification noise. (Also, a polynomial-time algorithm for learning linear threshold functions in the Statistical Query model of Kearns.) Our algorithm is based on a new method for removing outliers in data. Specifically, for any set S of points in R n , each given to b bits of precision, we show that one can remove only a small fraction of S so that in the remaining set T , for every vector v, max x2T (v x) 2 poly(n; b)E x2T (v x) 2 ; i.e., for any hyperplane through the origin, the maximum distance (squared) from a point in T to the plane is at most polynomially larger than the average. After removing these outliers, we are able to show that a modified version of the Perceptron Algorithm finds a weak hypothesis in polynomial time, even in the presence of random classification noise. 
Abstract-found: 1
Intro-found: 1
Reference: [ABFR91] <author> J. Aspnes, R. Beigel, M. Furst, and S. Rudich. </author> <title> The expressive power of voting polynomials. </title> <booktitle> In Proceedings of the 23rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 402-409, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: the label given to a new example drawn from D with probability at least 1=2 + 1=poly (n; b)? Known reductions show that a positive answer to this question would imply an n polylog (n) -time algorithm for learning DNF formulas, and more generally, AC 0 circuits, over arbitrary distributions <ref> [ABFR91] </ref>.
Reference: [AD93] <author> J. A. Aslam and S. E. Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Alternatively, Aslam and Decatur <ref> [AD93] </ref> have shown that Statistical Query (SQ) algorithms can, in fact, be boosted in the presence of noise.
Reference: [AD94] <author> J. A. Aslam and S. E. Decatur. </author> <title> Improved noise-tolerant learning and generalized statistical queries. </title> <type> Technical Report TR-17-94, </type> <institution> Harvard University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Finally, we describe how the algorithm can be adjusted to the noisy case using known techniques <ref> [Byl94, Kea93, AD94] </ref>. 1.2 Notation, definitions, and preliminaries In this paper, we consider the problem of learning linear threshold functions in the PAC model in the presence of random classification noise [KV94]. The problem can be stated as follows. <p> We present two ways of doing this. The first is to recast the algorithm in the Statistical Query (SQ) model of Kearns [Kea93] as extended by Aslam and Decatur <ref> [AD94] </ref>, and to use the fact that any SQ algorithm can be made tolerant of random classification noise. The second is a direct argument along the lines of Bylander [Byl94], who describes how the standard Perceptron Algorithm can be modified to work in this noise model. <p> Kearns [Kea93] and Aslam and Decatur <ref> [AD94] </ref> prove that one can similarly perform such an estimation even in the presence of random classification noise. 4 Specifically, for any noise rate &lt; 1=2 and any accuracy (or tolerance) parameter t , the desired expectation can be estimated with confidence 1 ffi in time (and sample size) poly ( <p> We can find such an approximation via statistical queries. Specifically, to approximate the ith coordinate of w;S , we ask for E x2D [`(x)x i j cos (w; x)`(x) ]. This conditional expectation can be approximated 4 Kearns [Kea93] considers queries with range f0; 1g. Aslam and Decatur <ref> [AD94] </ref> extends these arguments (among other things) to queries with range [0; 1], which is more convenient for our purposes. 9 from statistical queries since we are guaranteed from Step 2 that Pr (cos (w; x)`(x) ) is reasonably large.
Reference: [Agm54] <author> S. Agmon. </author> <title> The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6(3) </volume> <pages> 382-392, </pages> <year> 1954. </year>
Reference-contexts: Supported in part by NSF grant CCR9528973. Email: kannan+@cs.cmu.edu. School of Computer Science, Carnegie Mellon University, Pittsburgh PA 15213. Supported in part by NSF National Young Investigator grant CCR-9357793. Email: svempala@cs.cmu.edu. 1 instance, one commonly-used greedy algorithm for this task is the Perceptron Algorithm <ref> [Ros62, Agm54] </ref>, described below in Section 3. These algorithms have running times that depend on the amount of "wiggle room" available to a solution. In particular, the Perceptron Algorithm has the following guarantee [MP69]. <p> Otherwise, we repeat. The difficult issue is proving that this algorithm will in fact halt before removing too many points from S. The proof of this fact is deferred to Section 5. 3 The Perceptron Algorithm The Perceptron Algorithm <ref> [Ros62, Agm54] </ref> operates on a set S of labeled data points in n dimensional space. Its goal is to find a vector w such that w x &gt; 0 for all positive points x and w x &lt; 0 for all negative points x.
Reference: [Ama94] <author> E. Amaldi. </author> <title> From finding maximum feasible subsystems of linear systems to feed-forward neural network design. </title> <type> PhD thesis, </type> <institution> Swiss Federal Institute of Technology at Lausanne (EPFL), </institution> <month> October </month> <year> 1994. </year> <type> (Ph.D. dissertation No. 1282, </type> <institution> Department of Mathematics). </institution>
Reference-contexts: And, even though finding a vector w that minimizes the number of misclassified points is NP-hard, variants on the Perceptron Algorithm typically do well in practice <ref> [Gal90, Ama94] </ref>.
Reference: [AR88] <editor> J. A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: This function has a linear threshold representation, but it requires exponentially large weights and can cause the Perceptron Algorithm to take exponential time. (In practice, though, the Perceptron Algorithm and its variants tend to do fairly well; e.g., see <ref> [AR88] </ref>.) Given this difficulty, one might propose instead to use a polynomial-time linear programming algorithm to find the desired vector w. Each example provides one linear constraint and one could simply apply an LP solver to solve them [Kha79, Kar84, MT89].
Reference: [Byl93] <author> T. Bylander. </author> <title> Polynomial learnability of linear threshold approximations. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year> <month> 17 </month>
Reference-contexts: In fact, it is possible to provide guarantees for variations on the Perceptron Algorithm in the presence of inconsistent data (e.g., see <ref> [Byl93, Byl94, Kea93] </ref> 2 ), under models in which the inconsistency is produced by a sufficiently "benign" process, such as the random classification noise model discussed below. In this paper, we present a version of the Perceptron Algorithm that maintains its properties of noise-tolerance, while providing polynomial-time guarantees. <p> the input distribution restricted to the "don't know" region) to achieve a hypothesis of arbitrarily low error. 3 This yields the following theorem. 1 If a non-zero threshold is desired, this can be achieved by adding one extra dimension to the space. 2 The word "polynomial" in the title of <ref> [Byl93] </ref> means polynomial in the inverse of the separation param eter, which as noted above can be exponential in n even when points are chosen from f0; 1g n . 3 Thanks to Rob Schapire for pointing out that standard Boosting results [Sch90, Fre92] do not apply in the 2 Theorem
Reference: [Byl94] <author> T. Bylander. </author> <title> Learning linear threshold functions in the presence of classifica-tion noise. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 340-347. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: In fact, it is possible to provide guarantees for variations on the Perceptron Algorithm in the presence of inconsistent data (e.g., see <ref> [Byl93, Byl94, Kea93] </ref> 2 ), under models in which the inconsistency is produced by a sufficiently "benign" process, such as the random classification noise model discussed below. In this paper, we present a version of the Perceptron Algorithm that maintains its properties of noise-tolerance, while providing polynomial-time guarantees. <p> Finally, we describe how the algorithm can be adjusted to the noisy case using known techniques <ref> [Byl94, Kea93, AD94] </ref>. 1.2 Notation, definitions, and preliminaries In this paper, we consider the problem of learning linear threshold functions in the PAC model in the presence of random classification noise [KV94]. The problem can be stated as follows. <p> The reason for considering the = 0 case first is that we will be modifying algorithms that have already been proven tolerant to random classification noise (e.g., <ref> [Byl94] </ref>), and the key issue is getting the polynomial time guarantee. <p> The second is a direct argument along the lines of Bylander <ref> [Byl94] </ref>, who describes how the standard Perceptron Algorithm can be modified to work in this noise model. We begin with some observations needed for both approaches. <p> Note that examples are also used in the algorithm for the Outlier Removal Lemma. However, since this algorithm ignores the labels, it is unaffected by random classification noise. 4.2 A direct analysis We now describe a direct method for making the algorithm noise tolerant, along the lines of Bylander <ref> [Byl94] </ref>. First, for simplicity, we will reflect negative examples through the origin, and view every example as having a positive label. Thus we can ignore the "`(x)" term in equations (1)-(4) and in the definition of w;S at the beginning of this section.
Reference: [Coh97] <author> E. Cohen. </author> <title> Learning noisy perceptrons by a perceptron in polynomial time. </title> <booktitle> In Proceedings of the 38th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 514-523, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: VC-dimension arguments then imply that this new hypothesis likely has low true error. An alternative approach is described by Cohen <ref> [Coh97] </ref>. A big open question is whether weak-learning is possible in the presence of adversarial noise.
Reference: [Fre92] <author> Y. Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: 2 The word "polynomial" in the title of [Byl93] means polynomial in the inverse of the separation param eter, which as noted above can be exponential in n even when points are chosen from f0; 1g n . 3 Thanks to Rob Schapire for pointing out that standard Boosting results <ref> [Sch90, Fre92] </ref> do not apply in the 2 Theorem 1 The class of linear threshold functions in R n can be learned in polynomial time in the PAC prediction model in the presence of random classification noise.
Reference: [Gal90] <author> S. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 179-191, </pages> <year> 1990. </year>
Reference-contexts: And, even though finding a vector w that minimizes the number of misclassified points is NP-hard, variants on the Perceptron Algorithm typically do well in practice <ref> [Gal90, Ama94] </ref>.
Reference: [Kar84] <author> N. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Com-binatorica, </journal> <volume> 4(4) </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: Each example provides one linear constraint and one could simply apply an LP solver to solve them <ref> [Kha79, Kar84, MT89] </ref>. In practice, however, this approach is less often used in machine learning applications. One of the main reasons is that the data often is not consistent with any vector w and one's goal is simply to do as well as one can.
Reference: [Kea93] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: In fact, it is possible to provide guarantees for variations on the Perceptron Algorithm in the presence of inconsistent data (e.g., see <ref> [Byl93, Byl94, Kea93] </ref> 2 ), under models in which the inconsistency is produced by a sufficiently "benign" process, such as the random classification noise model discussed below. In this paper, we present a version of the Perceptron Algorithm that maintains its properties of noise-tolerance, while providing polynomial-time guarantees. <p> Remark: The learning algorithm can be made to fit the Statistical Query learning model <ref> [Kea93] </ref>. The main idea of our result is as follows. First, we modify the standard Perceptron Algorithm to produce an algorithm that succeeds in weak learning unless an overwhelming fraction of the data points lie on or very near to some hyperplane through the origin. <p> Finally, we describe how the algorithm can be adjusted to the noisy case using known techniques <ref> [Byl94, Kea93, AD94] </ref>. 1.2 Notation, definitions, and preliminaries In this paper, we consider the problem of learning linear threshold functions in the PAC model in the presence of random classification noise [KV94]. The problem can be stated as follows. <p> We present two ways of doing this. The first is to recast the algorithm in the Statistical Query (SQ) model of Kearns <ref> [Kea93] </ref> as extended by Aslam and Decatur [AD94], and to use the fact that any SQ algorithm can be made tolerant of random classification noise. <p> Kearns <ref> [Kea93] </ref> and Aslam and Decatur [AD94] prove that one can similarly perform such an estimation even in the presence of random classification noise. 4 Specifically, for any noise rate &lt; 1=2 and any accuracy (or tolerance) parameter t , the desired expectation can be estimated with confidence 1 ffi in time <p> We can find such an approximation via statistical queries. Specifically, to approximate the ith coordinate of w;S , we ask for E x2D [`(x)x i j cos (w; x)`(x) ]. This conditional expectation can be approximated 4 Kearns <ref> [Kea93] </ref> considers queries with range f0; 1g. Aslam and Decatur [AD94] extends these arguments (among other things) to queries with range [0; 1], which is more convenient for our purposes. 9 from statistical queries since we are guaranteed from Step 2 that Pr (cos (w; x)`(x) ) is reasonably large.
Reference: [Kha79] <author> L. G. Khachiyan. </author> <title> A polynomial algorithm in linear programming. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 20 </volume> <pages> 191-194, </pages> <year> 1979. </year>
Reference-contexts: Each example provides one linear constraint and one could simply apply an LP solver to solve them <ref> [Kha79, Kar84, MT89] </ref>. In practice, however, this approach is less often used in machine learning applications. One of the main reasons is that the data often is not consistent with any vector w and one's goal is simply to do as well as one can.
Reference: [KV94] <author> M. Kearns and U. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Finally, we describe how the algorithm can be adjusted to the noisy case using known techniques [Byl94, Kea93, AD94]. 1.2 Notation, definitions, and preliminaries In this paper, we consider the problem of learning linear threshold functions in the PAC model in the presence of random classification noise <ref> [KV94] </ref>. The problem can be stated as follows. We are given access to examples (points) drawn from some distribution D over R n . Each example is labeled as positive or negative.
Reference: [MP69] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> The MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: Email: svempala@cs.cmu.edu. 1 instance, one commonly-used greedy algorithm for this task is the Perceptron Algorithm [Ros62, Agm54], described below in Section 3. These algorithms have running times that depend on the amount of "wiggle room" available to a solution. In particular, the Perceptron Algorithm has the following guarantee <ref> [MP69] </ref>. <p> We begin with w = ~ 0. We then perform the following operation until all examples are correctly classified: Pick some arbitrary misclassified example x 2 S and let w w + `(x)^x. A classic theorem (see <ref> [MP69] </ref>) describes the convergence properties of this algorithm. Theorem 2 [MP69] Suppose the data set S can be correctly classified by some unit vector w fl . Then, the Perceptron Algorithm converges in at most 1= 2 iterations, where = min x2S jw fl ^xj. Proof. <p> We begin with w = ~ 0. We then perform the following operation until all examples are correctly classified: Pick some arbitrary misclassified example x 2 S and let w w + `(x)^x. A classic theorem (see <ref> [MP69] </ref>) describes the convergence properties of this algorithm. Theorem 2 [MP69] Suppose the data set S can be correctly classified by some unit vector w fl . Then, the Perceptron Algorithm converges in at most 1= 2 iterations, where = min x2S jw fl ^xj. Proof.
Reference: [MT89] <author> W. Maass and G. Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Each example provides one linear constraint and one could simply apply an LP solver to solve them <ref> [Kha79, Kar84, MT89] </ref>. In practice, however, this approach is less often used in machine learning applications. One of the main reasons is that the data often is not consistent with any vector w and one's goal is simply to do as well as one can.
Reference: [Ros62] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <year> 1962. </year>
Reference-contexts: Supported in part by NSF grant CCR9528973. Email: kannan+@cs.cmu.edu. School of Computer Science, Carnegie Mellon University, Pittsburgh PA 15213. Supported in part by NSF National Young Investigator grant CCR-9357793. Email: svempala@cs.cmu.edu. 1 instance, one commonly-used greedy algorithm for this task is the Perceptron Algorithm <ref> [Ros62, Agm54] </ref>, described below in Section 3. These algorithms have running times that depend on the amount of "wiggle room" available to a solution. In particular, the Perceptron Algorithm has the following guarantee [MP69]. <p> Otherwise, we repeat. The difficult issue is proving that this algorithm will in fact halt before removing too many points from S. The proof of this fact is deferred to Section 5. 3 The Perceptron Algorithm The Perceptron Algorithm <ref> [Ros62, Agm54] </ref> operates on a set S of labeled data points in n dimensional space. Its goal is to find a vector w such that w x &gt; 0 for all positive points x and w x &lt; 0 for all negative points x.
Reference: [Sch90] <author> R. E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: 2 The word "polynomial" in the title of [Byl93] means polynomial in the inverse of the separation param eter, which as noted above can be exponential in n even when points are chosen from f0; 1g n . 3 Thanks to Rob Schapire for pointing out that standard Boosting results <ref> [Sch90, Fre92] </ref> do not apply in the 2 Theorem 1 The class of linear threshold functions in R n can be learned in polynomial time in the PAC prediction model in the presence of random classification noise.
Reference: [VC71] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year> <month> 18 </month>
Reference-contexts: We can apply the algorithms to the PAC setting by running them on a sufficiently large sample of data drawn according to the above model, and then applying standard VC-dimension arguments to the result <ref> [VC71] </ref>. For most of this paper, we will consider the above problem for the case of zero noise ( = 0), which we extend to the general case in Section 4. <p> Strong (PAC) Learning The algorithm presented above splits the input space into a classification region fx : x 2 L and j cos (w; A 1 x)j g and a don't-know region fx : x 62 L or j cos (w; A 1 x)j &lt; g: By standard VC-dimension arguments <ref> [VC71] </ref>, if the sample S is drawn from distribution D, then for any *; ffi &gt; 0, if S is sufficiently (polynomially) large, then with high probability ( 1 ffi), the true error of the hypothesis inside the classification region is less than *.
References-found: 20

