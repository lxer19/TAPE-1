URL: http://www.cs.ucla.edu/~stott/mca/CSD-970002.ps.gz
Refering-URL: http://www.cs.ucla.edu/~stott/mca/
Root-URL: http://www.cs.ucla.edu
Title: Monte Carlo Arithmetic: exploiting randomness in floating-point arithmetic promise as a tool in numerical computation.
Author: D. Stott Parker 
Keyword: floating-point arithmetic, floating-point rounding, roundoff error, random rounding, ANSI/IEEE floating-point standards, significance arithmetic, Monte Carlo methods  
Note: This work summarizes ways in which MCA has  models.  AMS(MOS) subject classifications: 65C05, 65C20, 65G05, 65G10, 65J05, 68M07, 62P99  
Address: Los Angeles, CA 90095-1596  
Affiliation: Computer Science Department University of California  
Email: stott@cs.ucla.edu  
Date: March 30, 1997  
Abstract: Monte Carlo Arithmetic (MCA) is an extension of standard floating-point arithmetic that exploits randomness in basic floating-point operations. MCA includes random rounding | which forces roundoff errors to be randomly distributed | and precision bounding | which limits the number of significant digits in a given value by random perturbation. Random rounding can be used to produce roundoff errors that are truly random and uncorrelated, and that have zero expected bias. Precision bounding can be used to vary precision dynamically, to implement inexact values (values known to only a few significant digits), and most importantly to detect catastrophic cancellation, which is the primary way that significant digits are lost in numerical computation. Randomization has both theoretical and practical benefits. It has the effect of transforming any floating-point computation into a Monte Carlo computation, and roundoff analysis into statistical analysis. Unlike much previous work in this area, MCA makes no assumptions about the resulting roundoff error distributions, such as that they are normal. By running a program multiple times, one directly measures the sensitivity of particular outputs to random perturbations of particular inputs. MCA thus gives a way to implement roundoff analysis, using random variables for roundoff errors, so that the roundoff distributions can be studied explicitly. It encourages an empirical approach to evaluating numerical quality, and gives a way to exploit the Monte Carlo method in numerical computation. MCA also generally gives a different perspective on the study of error. For example, while floating-point summation is not associative, Monte Carlo summation is "statistically associative" up to the standard error of the sum. A statistical approach avoids anomalies of floating-point arithmetic. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> American National Standards Institute, </institution> <month> ANSI/IEEE Std </month> <year> 754-1985: </year> <title> IEEE standard for binary floating-point arithmetic, </title> <address> New York, 12 Aug. </address> <year> 1985. </year> <title> See also: "An American National Standard, IEEE standard for binary floating-point arithmetic", </title> <journal> SIGPLAN Notices, </journal> <volume> 22:2, </volume> <pages> 9-25, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi); where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 25, 62] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. <p> For example, in nonextended IEEE 754 floating-point arithmetic with denormalized numbers, the minimum and maximum positive floating-point numbers are f min = 2 E min b min and f max = 2 E max b max , with values derived from <ref> [1, x3.2] </ref> in the following table: floating-point system E min E max b min b max IEEE single precision 126 +127 2 23 2 2 23 IEEE double precision 1022 +1023 2 52 2 2 52 Relative to a given F , we can define a subset of the real numbers <p> 52 2 2 52 Relative to a given F , we can define a subset of the real numbers &lt; &lt; F = f0g [ f x 2 &lt; j r min &lt; jxj &lt; r max g: In IEEE single precision floating-point and the default rounding (round to nearest) <ref> [1, x4.1] </ref>, the first real value not in &lt; F is r max = 2 127 fi (2 2 24 ) ' 3:4 fi 10 38 (which rounds to +1 with an overflow). <p> Similarly <ref> [1] </ref> permits denormalized values above r min = 2 126 fi 2 24 ' 1:4 fi 10 44 (which can round to 0 with an underflow). <p> + y ] = (x + y) fi (1 + ffi) implicitly requires x and y to be floating-point values. 3.2 Fundamental problems with floating-point arithmetic Despite fifty years of development, the great contributions of Wilkinson, Knuth, Kahan and others, and the great advance made by the IEEE floating-point standards <ref> [1, 25] </ref>, floating-point arithmetic is still an arcane and labyrinthine subject. In general, finding good error bounds for numerical algorithms is difficult, and determining whether a numerical computation is `stable' is an art. <p> b L b U is computed with MCA, this theorem is borne out: the results are all correct to full precision, and the magnitudes of the standard deviations produced are considerably smaller than the worst-case error bounds stated in the theorem (the largest was 6 fi 10 8 for the <ref> [1; 1] </ref>-entry). This example underscores the difference between the loss of significance and backward error perspectives. b A (k) diverges completely from A (k) for the Hilbert matrix, yet the product b L b U will be very close to A.
Reference: [2] <author> R. Alt, </author> <title> "The use of the CESTAC Method in the Parallel Computation of Roots of Polynomials", </title> <note> pp. 3-9 in Numerical Mathematics and Applications, </note> <editor> R. Vichnevetsky and J. Vignes (eds.), Elsevier/North-Holland, </editor> <year> 1986. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker 52 8 Example Applications of Monte Carlo Arithmetic Several examples illustrate what MCA can do. A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding <ref> [2] </ref>, finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., [100, 103, 105, 106]) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [3] <author> R. Alt, J. Vignes, </author> <title> "Validation of results of collocation methods for ODEs with the CADNA library", </title> <journal> Appl. Numer. Math. </journal> <volume> 21:2, </volume> <pages> 119-139, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Open-endedness is vital. 7.7 Limitations of CESTAC and CADNA are avoided MCA generalizes and improves upon existing approaches for random rounding or random perturbation of numerical results. Specifically, in a number of ways it improves on the stochastic arithmetic of CADNA and CESTAC ([106], updated recently in <ref> [3] </ref>). 11 The CADNA environment includes: * randomization with PER (x), which is ad hoc but can be implemented very efficiently on almost all machines; * type declarations of stochastic program variables, causing 2 or 3 samples of that variable to be saved; * stochastic arithmetic operators, one execution of which <p> Copyright c fl1996, 1997 D. Stott Parker 50 are the true values, that 2 or 3 samples are sufficient, and that the resulting Student t values give meaningful confidence intervals that can be used in comparison operations ("hypotheses which generally hold in real problems" <ref> [3, p.131] </ref>). Chatelin gives a critical review of these assumptions in [20], and reworks them into rigorous conditions under which the results of CESTAC will be right 95% of the time. (Under assumptions that Chatelin disputes, taking only 2 or 3 samples gives a confidence level of 95%. <p> of these occurrences in MCA are zero if t digit precision (x) is implemented correctly. * In MCA, we recommend defining comparisons in terms of the underlying arithmetic opera tions, as programmers often assume this [43]: x &lt;fl y (x y) &lt; 0: In CADNA, comparisons are based on statistics <ref> [3, p.133] </ref>: x &lt;fl y b x b y &lt; ff b 2 y where b x is the computed average of x, b x is its computed standard deviation, and ff is half the Student t confidence interval width depending on the desired confidence level 1 ff. <p> Stott Parker 52 8 Example Applications of Monte Carlo Arithmetic Several examples illustrate what MCA can do. A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues [21], solving ODEs <ref> [3] </ref>, etc. The survey papers by Vignes (e.g., [100, 103, 105, 106]) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective. <p> Again, however, it is important to bear in mind that Chaitin-Chatelin and Fraysse [23, p.101] showed that Hull and Swensen's results hold only for regular points. 8.3.2 Chaotic differential equations A recent paper by Alt and Vignes <ref> [3] </ref> shows dramatic results of using CESTAC on standard example differential equations. Rather than consider such problems further, we elected to explore the effects of MCA in chaotic differential equations.
Reference: [4] <author> R.L. Ashenhurst, N. </author> <title> Metropolis, "Unnormalized Floating Point Arithmetic", </title> <journal> J. ACM 6:3, </journal> <pages> 415-428, </pages> <month> July </month> <year> 1959. </year>
Reference-contexts: See for example Knuth [68, x4.2.2.B], the fixed-point arithmetic analyses of Wilkinson in [109], and the arguments for unnormalized arithmetic of Ashenhurst and Metropolis <ref> [4, 5] </ref>, who show that it retains important information about the number of significant digits in the computed results. The basic idea is that unnormalized numbers can, by convention, represent their significant digits (although in practice this approach tends to over- or understate significance somewhat [68]). <p> Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. They wrote a number of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [4, 5, 11, 75] </ref>. Significance arithmetic faced several difficult problems, which it did not really overcome. First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" [92, x3.1]. Second, there are serious problems of implementation.
Reference: [5] <author> R.L. Ashenhurst, N. </author> <title> Metropolis, "Error Estimation in Computer Calculation", in Computers and Computing, </title> <journal> AMM Slaught Memorial Papers, American Mathematical Monthly 72:2, </journal> <pages> 47-48, </pages> <month> February </month> <year> 1965. </year>
Reference-contexts: See for example Knuth [68, x4.2.2.B], the fixed-point arithmetic analyses of Wilkinson in [109], and the arguments for unnormalized arithmetic of Ashenhurst and Metropolis <ref> [4, 5] </ref>, who show that it retains important information about the number of significant digits in the computed results. The basic idea is that unnormalized numbers can, by convention, represent their significant digits (although in practice this approach tends to over- or understate significance somewhat [68]). <p> Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. They wrote a number of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [4, 5, 11, 75] </ref>. Significance arithmetic faced several difficult problems, which it did not really overcome. First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" [92, x3.1]. Second, there are serious problems of implementation.
Reference: [6] <author> J.-C. Bajard, D. Michelucci, J.-M. Moreau, J.-M. Muller, </author> <title> Introduction to the Special Issue on "Real Numbers and Computers", </title> <journal> Journal of Universal Computer Science 1:7, </journal> <pages> page 438, </pages> <month> Jul 28, </month> <year> 1995. </year> <title> Available on the internet at many sites. </title>
Reference-contexts: Here the standard error suggests the computed average for r2 has one significant digit. Copyright c fl1996, 1997 D. Stott Parker 7 2.2 Two startling examples Consider the following two marvelous examples adapted from <ref> [6] </ref>. First, define the sequence (x k ) x 0 = 1:5100050721319 k 20x 3 k 24) = (4x 3 k + 70x k 50): As demonstrated in Table 5, depending on the precision of one's machine, the sequence converges to either 1, 2, 3, or 4.
Reference: [7] <author> J.L. Barlow, </author> <title> E.H. Bareiss, "On Roundoff Error Distributions in Floating Point and Logarithmic Arithmetic", </title> <booktitle> Computing 34:4, </booktitle> <pages> 325-347, </pages> <year> 1985. </year>
Reference-contexts: He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include <ref> [7, 13, 32, 70, 81] </ref>. In [81], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions. <p> When rounding is used in t-digit base-fi arithmetic, then <ref> [7, p.336] </ref>: E [ " ] = 48 12 (1 + fi) ln (fi) + O (fi 3t ) fi 2t + O (fi 3t ): Barlow and Bareiss go on to derive 99.5% confidence bounds for jEj = P fi fi k e ij fi fi for the Hilbert matrix
Reference: [8] <author> J.L. Barlow, </author> <title> E.H. Bareiss, "Probabilistic analysis of Gaussian elimination in Floating Point and Logarithmic Arithmetic", </title> <booktitle> Computing 34:4, </booktitle> <pages> 349-364, </pages> <year> 1985. </year>
Reference-contexts: For a good introduction see [39]; [59] gives excellent perspective on roundoff analyses for this problem. Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss <ref> [8] </ref>, Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. <p> Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss <ref> [8] </ref>, Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. These examples show that, at least for small linear systems, MCA can detect ill-conditioning. <p> If b (n) = c = c (1) , the second phase then solves the triangular system b U b x = c by back substitution. D.2 Error analysis We follow the error analysis of Barlow and Bareiss <ref> [8] </ref>, which differs from that of Wilkinson [108] to support statistical analysis. <p> Barlow and Bareiss <ref> [8, pp.357-360] </ref> give explicit formulas for the means E [ " ] and variances V [ " ], where " is any of the error variables e ik , e ij , e i , f i , as they have been constructed so as to be independent random variables.
Reference: [9] <author> F.L. Bauer, K. </author> <title> Samelson, "Optimale Rechengenauigkeit bei Rechenanlagen mit gleitendem Komma", </title> <journal> Zeitschrift fur angewandte Mathematik und Physik 4, </journal> <pages> 312-316, </pages> <year> 1953. </year>
Reference-contexts: The user is given responsibility for interpreting the significance of all floating-point results. Since the very early work by Bauer and Samelson in 1953 <ref> [9] </ref>, various authors have stressed that unnormalized arithmetic can improve on normalized arithmetic in some cases for retaining information about significance.
Reference: [10] <author> C. Beck, </author> <title> "Effects of roundoff errors on chaotic dynamics", </title> <booktitle> in: Signal Processing VI Theories and Applications: Proc. EUSIPCO-92, Sixth European Signal Processing Conference (Brussels, </booktitle> <address> Belgium, 24-27 Aug. </address> <year> 1992), </year> <editor> vol.1, p. 183-186. J. Vandewalle, R. Boite, M. Moonen, A. Oosterlinck (eds.), </editor> <publisher> Amsterdam: Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: Recently some researchers have shown that roundoff errors in differential equations corresponding to chaotic dynamical systems can have surprising and highly significant effects <ref> [10, 12, 27, 94] </ref>. These results are sometimes sensationalized, and predict disaster. In actual experience, on the other hand, numerical integration of these equations generally seems to do well, and give results that are consistent with physical experiments.
Reference: [11] <author> R.L. Bivins, N. </author> <title> Metropolis, "Significance Arithmetic: Application to a Partial Differential Equation", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-26:7, </volume> <pages> 639-642, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. They wrote a number of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [4, 5, 11, 75] </ref>. Significance arithmetic faced several difficult problems, which it did not really overcome. First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" [92, x3.1]. Second, there are serious problems of implementation.
Reference: [12] <author> M. Blank, </author> <title> "Pathologies generated by round-off in dynamical systems", </title> <journal> Physica D 78 </journal> <pages> 1-2, 93-114, </pages> <month> 1 Nov. </month> <year> 1994. </year>
Reference-contexts: Recently some researchers have shown that roundoff errors in differential equations corresponding to chaotic dynamical systems can have surprising and highly significant effects <ref> [10, 12, 27, 94] </ref>. These results are sometimes sensationalized, and predict disaster. In actual experience, on the other hand, numerical integration of these equations generally seems to do well, and give results that are consistent with physical experiments.
Reference: [13] <author> R.P. Brent, </author> <title> "Static and Dynamic Numerical Characteristics of Floating-Point Arithmetic", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-22:6, </volume> <pages> 598-607, </pages> <month> June </month> <year> 1973. </year>
Reference-contexts: He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include <ref> [7, 13, 32, 70, 81] </ref>. In [81], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [14] <author> M.-C. Brunet, F. Chatelin, "CESTAC, </author> <title> a tool for a stochastic round-off error analysis in scientific computing", </title> <note> pp. 11-20 in Numerical Mathematics and Applications, </note> <editor> R. Vichnevetsky and J. Vignes (eds.), Elsevier/North-Holland, </editor> <year> 1986. </year>
Reference-contexts: The retrospective survey [103] reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method. As described in <ref> [14] </ref>, CESTAC works on numerical programs in which each floating-point expression `X *Y ' has been preprocessed to PER (X *Y ), so for example X +Y +Z becomes PER (PER (X +Y )+Z), where PER is a function that implements random perturbation. <p> Vignes' survey [105] accompanies an entire journal issue with papers describing applications of CESTAC. Both PER and the perturbation mode of PEPER work identically <ref> [14, 105] </ref>: when the underlying machine arithmetic works with ordinary rounding, PER adds a least significant bit 1 with probability 1 4 , adds 0 with probability 1 2 , and subtracts 1 with probability 1 4 .
Reference: [15] <author> W. Buchholz, </author> <title> Planning a Computer System: Project Stretch, </title> <publisher> NY: McGraw-Hill, </publisher> <year> 1962. </year> <title> Copyright c fl1996, </title> <address> 1997 D. </address> <note> Stott Parker 81 </note>
Reference-contexts: The IEEE 754 standard follows the second convention. Copyright c fl1996, 1997 D. Stott Parker 21 conventional floating-point format, and imprecise values with unnormalized format. The Stretch supercomputer <ref> [15] </ref> also maintained this distinction. The design was inspired in part by the work of Ashenhurst and Metropolis, and was implemented in the IBM 7030 delivered to Los Alamos National Labs in April 1961. <p> In ways the Stretch floating-point system was more flexible than the IEEE standards: it had both normalized and unnormalized arithmetic, both rounding and chopping, support for extended precision, and representation of singular values (1, infinitesimals, and order-of-magnitude zero). An order-of-magnitude zero <ref> [15, p.108-111] </ref> is a value 0 fi fi e , which we say has base-fi order of magnitude e (has fioom e), and represent it by a floating-point value with fraction 0 and exponent e. Thus Stretch distinguished between exact and inexact zero. <p> Running the same problem twice, first in the normal mode and then in the noisy mode, gives an estimate of the significance of the results. || <ref> [15, p.25] </ref> After an extensive search, the most effective technique [for monitoring significance loss] turned out to be both elegant and remarkably simple. ... <p> The true value, if there had been no significance loss, should lie between these two extremes. Hence, two runs, one made without and one made with the noisy mode, should show differences in result that indicate which digits may have been affected by significance loss. || <ref> [15, p.102] </ref> Copyright c fl1996, 1997 D. <p> Furthermore, significance loss is relatively rare, so that running a problem twice when significance loss is suspected does not pose a serious problem. What is serious is the possibility of unsuspected significance loss. || <ref> [15, p.102] </ref> Although noisy mode is very clever, it is not difficult to come up with examples that subvert its intent. Moreover, it can require some care in programming; for example, in noisy mode apparently the difference of two successive values in any iteration will never be zero. <p> The ability to vary the virtual precision can be very useful, so we allow t to differ from p. 7 Because the order of magnitude of an exact 0 is unspecified, we define inexact (0; s; ~) = 0: On computers like Stretch <ref> [15] </ref> with an order-of-magnitude zero (0 fi fi e ), define inexact ((0 fi fi e ); s; ~) = fi es ~. Copyright c fl1996, 1997 D.
Reference: [16] <author> A.C. Callahan, </author> <title> "Random rounding: Some principles and applications", </title> <booktitle> Proc. 1976 IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Philadelphia, PA, USA, </address> <month> 12-14 April </month> <year> 1976, </year> <pages> 501-504, </pages> <year> 1976. </year>
Reference-contexts: Henrici was intrigued by this experimental approach, and showed their results could be predicted analytically to within an error of 10% [52]. We have also found the idea of random rounding in a paper of Callahan <ref> [16] </ref>, who demonstrated its use in signal processing applications (perhaps the most well-developed domain for roundoff analysis). He also proposes round_random_nearness. Callahan generally investigated the uses of randomness in signal processing, but like Forsythe, in this paper he stresses the usefulness of truly random roundoff errors. <p> often-used simple recursive integrator whose filter is implemented by the difference equation y n = (1 1=M ) y n1 + x n =M where the x n are an input time series, M = 2 m , can have "dramatic correlated quantization error effects unless random rounding is employed" <ref> [16, p.501] </ref>. Finally, a twist on the idea of random rounding was developed by Yoshida, Goto, and Ichikawa [115], permitting them to implement floating-point multipliers with many fewer gates than usual. <p> achieved if ~ is a uniform_absolute variable, i.e., is uniform on 2 ; 1 : Then ~ has mean 0 and standard deviation 1= p This method was implemented and used successfully by Forsythe in 1959 [38], Hull and Swenson (with coaching by Kahan) in 1966 [57], Callahan in 1976 <ref> [16] </ref>, and probably others. The final two random rounding methods above satisfy the worst-case forward error bound j random round (x) x j fi e p when x is a nonzero floating-point value, e = fioom (x), and again t = p.
Reference: [17] <author> S.L. Campbell, C.D. Meyer, Jr., </author> <title> Generalized Inverses of Linear Transformations, </title> <publisher> NY: Pitman, 1979; reprinted by Dover Publications, </publisher> <year> 1991. </year>
Reference-contexts: This was in keeping with Gauss' philosophy to publish nothing but well polished work of lasting significance. || Campbell & Meyer <ref> [17, pp.39-40] </ref> Goldstine, in his history of numerical analysis [45], translates the remarks of Gauss on how having more samples can increase the accuracy of an estimated value: But in such a case, if it is proposed to aim at the greatest precision, we shall take care to collect and employ
Reference: [18] <author> T.F. Chan, J.G. Lewis, </author> <title> "Computing standard deviations: Accuracy", </title> <journal> Comm. ACM 22:9, </journal> <pages> 526-531, </pages> <year> 1979. </year>
Reference-contexts: For example, the formula for computing the sample standard deviation from P i and x i v u t n 1 @ i=1 i n i=1 ! 2 A is in widespread use. Yet it routinely produces results of terrible quality <ref> [18, 19] </ref>.
Reference: [19] <author> T.F. Chan, G.H. Golub, </author> <title> R.J. LeVeque, "Algorithms for computing the sample variance: Analysis and recommendations", </title> <journal> Amer. Statist. </journal> <volume> 37:3, </volume> <pages> 242-247, </pages> <year> 1983. </year>
Reference-contexts: For example, the formula for computing the sample standard deviation from P i and x i v u t n 1 @ i=1 i n i=1 ! 2 A is in widespread use. Yet it routinely produces results of terrible quality <ref> [18, 19] </ref>.
Reference: [20] <author> F. </author> <type> Chatelin, </type> <institution> "Sur la taux de fiabilite general de la methode CESTAC", Comptes Rendus de l'Academie des Sciences, Serie I (Mathematique) 307, </institution> <month> 851-854, </month> <year> 1988. </year>
Reference-contexts: Chatelin gives a critical review of these assumptions in <ref> [20] </ref>, and reworks them into rigorous conditions under which the results of CESTAC will be right 95% of the time. (Under assumptions that Chatelin disputes, taking only 2 or 3 samples gives a confidence level of 95%.
Reference: [21] <author> F. Chatelin, M.-C. Brunet, </author> <title> "A probabilistic round-off error propagation model. Application to the eigenvalue problem", </title> <booktitle> in [29], </booktitle> <pages> 139-160, </pages> <year> 1990. </year>
Reference-contexts: Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., <ref> [21, 22] </ref>, and very recently [23]). Her work is exceptionally clear and rigorous, and careful about its assumptions. <p> Hull and Swenson [57] found that, for a Runge-Kutta code with probabilistic rounding, when several test differential equations were integrated the errors followed a normal distribution. Chatelin and Brunet <ref> [21] </ref> discuss the situation for eigenvalue computations with the QR algorithm and present a clear and well-reasoned set of assumptions about the distributions of errors in `sufficiently long' numeric programs in which the output is an analytic function of a perturbation of the problem. <p> When the operands are inexact, on the other hand, both operations can give inexact results. Chatelin and Brunet warn <ref> [21] </ref> that perturbation of the algorithm itself (affecting its branching structure, and not just perturbing its input) probably makes the function computed by the program nonanalytic, and requires qualitatively different analysis. We agree. Inexact real arithmetic has both an intuitive appeal and a practical usefulness that inexact boolean algebra lacks. <p> In a sense, statistical inference is tied to the number zero itself, and this inference rests on assumptions about the distribution of errors that may not hold (e.g., as they are proved not to hold for eigenvalue computations in <ref> [21, Theorem 8.2; cf. also p.158] </ref>). The only way that MCA treats zero specially is that it does not have an order of magnitude. Summarizing, CADNA is a compromise between theory and practice. <p> Copyright c fl1996, 1997 D. Stott Parker 52 8 Example Applications of Monte Carlo Arithmetic Several examples illustrate what MCA can do. A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues <ref> [21] </ref>, solving ODEs [3], etc. The survey papers by Vignes (e.g., [100, 103, 105, 106]) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [22] <author> F. Chatelin, V. Fraysse, </author> <title> "Elements of a Condition Theory for the Computational Analysis of Algorithms", in Iterative Methods in Linear Algebra, </title> <editor> R. Beauwens and P. de Groen (eds.), </editor> <publisher> Elsevier North-Holland, </publisher> <pages> 15-25, </pages> <year> 1992. </year>
Reference-contexts: Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., <ref> [21, 22] </ref>, and very recently [23]). Her work is exceptionally clear and rigorous, and careful about its assumptions.
Reference: [23] <author> F. Chaitin-Chatelin, V. Fraysse, </author> <title> Lectures on Finite Precision Computations, </title> <address> Philadelphia: </address> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: One feature common to such packages is that they strive to be comprehensible to ordinary users, allowing them to visualize the sensitivity (uncertainty, inexactness) of the results in an intuitive way. Techniques useful in visualizing sensitivity of matrix computations are described in <ref> [23] </ref> and [113]. Copyright c fl1996, 1997 D. <p> Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [21, 22], and very recently <ref> [23] </ref>). Her work is exceptionally clear and rigorous, and careful about its assumptions. <p> Since 1988 Chaitin-Chatelin has developed a MATLAB toolbox called PRECISE that allows users to perform statistical backward error analysis and sensitivity analysis experiments, with emphasis on linear system solution, eigencomputations, polynomial root finding, and general nonlinear (matrix or polynomial) equation solving under componentwise or normwise perturbations <ref> [23] </ref>. 5.4 Random rounding Random rounding arises from allowing floating-point rounding to be nondeterministic, rounding up or down with some probability that can depend on the value to be rounded. Although it has not been developed fully, random rounding has been used a number of times. <p> They then experimentally verify the normality assumption using the CESTAC perturbation method, and use this in finding t-test confidence intervals for specific eigenvalues. Both Hull and Swensen [57, pp.110-111] and Chatelin and Brunet used the 2 method to verify normality of the distribution. Recently Chaitin-Chatelin and Fraysse <ref> [23, p.101] </ref> showed that Hull and Swensen's results hold only for regular points; normality does not hold in neighborhoods of singular points. MCA is not committed to any assumption of normality. Sampling is an art, and the right approach to sampling can settle very tricky problems [26]. <p> Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss [8], Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse <ref> [23] </ref>. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. These examples show that, at least for small linear systems, MCA can detect ill-conditioning. <p> Again, however, it is important to bear in mind that Chaitin-Chatelin and Fraysse <ref> [23, p.101] </ref> showed that Hull and Swensen's results hold only for regular points. 8.3.2 Chaotic differential equations A recent paper by Alt and Vignes [3] shows dramatic results of using CESTAC on standard example differential equations.
Reference: [24] <author> J.-M. Chesneaux, J. </author> <type> Vignes, </type> <institution> "Sur la robustesse de la methode CESTAC", Comptes Rendus de l'Academie des Sciences, Serie I (Mathematique) 307, </institution> <month> 855-860, </month> <year> 1988. </year>
Reference-contexts: It is illuminating to think of CESTAC as being incorrect about 5% of the time.) These criticisms led to refinements by Vignes in <ref> [24] </ref> and [106, pp.241-242]. Unfortunately the paper [24], given as a formal basis for these assumptions, makes further assumptions and dismisses the possibility of exception to them in a cavalier way as "pratiquement jamais observe" (translated as "quasi never satisfied"). <p> It is illuminating to think of CESTAC as being incorrect about 5% of the time.) These criticisms led to refinements by Vignes in <ref> [24] </ref> and [106, pp.241-242]. Unfortunately the paper [24], given as a formal basis for these assumptions, makes further assumptions and dismisses the possibility of exception to them in a cavalier way as "pratiquement jamais observe" (translated as "quasi never satisfied").
Reference: [25] <author> W.J. Cody, J.T. Coonen, D.M. Gay, K. Hanson, et al. </author> <title> "A proposed radix- and word-length-independent standard for floating-point arithmetic", </title> <journal> SIGNUM Newsletter 20:1, </journal> <pages> 37-51, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi); where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 25, 62] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. <p> + y ] = (x + y) fi (1 + ffi) implicitly requires x and y to be floating-point values. 3.2 Fundamental problems with floating-point arithmetic Despite fifty years of development, the great contributions of Wilkinson, Knuth, Kahan and others, and the great advance made by the IEEE floating-point standards <ref> [1, 25] </ref>, floating-point arithmetic is still an arcane and labyrinthine subject. In general, finding good error bounds for numerical algorithms is difficult, and determining whether a numerical computation is `stable' is an art.
Reference: [26] <author> W.G. Cochran, </author> <title> Sampling Techniques, </title> <booktitle> 3rd edition, </booktitle> <address> NY: </address> <publisher> J. Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: MCA is not committed to any assumption of normality. Sampling is an art, and the right approach to sampling can settle very tricky problems <ref> [26] </ref>. The book by Fishman [34] surveys many sampling techniques for Monte Carlo analysis. Nonparametric statistical analysis is also useful: one need not assume any particular sample distribution, but can instead develop and test hypotheses using other tools, particularly for order statistics [107].
Reference: [27] <author> J.-F. Colonna, </author> <title> "The Subjectivity of Computers", </title> <journal> Comm. ACM 36:8, </journal> <month> August </month> <year> 1993. </year>
Reference-contexts: Recently some researchers have shown that roundoff errors in differential equations corresponding to chaotic dynamical systems can have surprising and highly significant effects <ref> [10, 12, 27, 94] </ref>. These results are sometimes sensationalized, and predict disaster. In actual experience, on the other hand, numerical integration of these equations generally seems to do well, and give results that are consistent with physical experiments.
Reference: [28] <author> R.M. Corless, </author> <title> "Error Backward", </title> <booktitle> in [67], </booktitle> <pages> 31-62, </pages> <year> 1994. </year>
Reference-contexts: Parker and Chua explain that solutions one obtains often are quite good because they approach an attractor (attracting limit set), and even if they are not in precise agreement with the intended trajectory, they at least follow the attractor. The paper <ref> [28] </ref> by Corless (in the edited volume [67], which gives a good overview of numerical effects in chaotic dynamics) argues that the situation is similar to what arises in Gaussian elimination, and that the backwards error perspective is useful for chaotic systems (although the forward perspective is needed for stiff systems).
Reference: [29] <author> M.G. Cox, S. Hammarling, </author> <title> Reliable Numerical Computation, </title> <publisher> NY: Oxford University Press, </publisher> <year> 1990. </year>
Reference-contexts: Fox <ref> [29, Epilogue p.333] </ref> We suspect that many users see loss of significance as the most natural error perspective, but this is a sociological question, and will not be resolved here.
Reference: [30] <author> H. Cramer, </author> <title> Mathematical Methods of Statistics, </title> <publisher> Princeton University Press, </publisher> <year> 1946. </year>
Reference-contexts: Stott Parker 23 4.5 Determining the number of significant digits via statistics Adapting standard statistical notation, we write E [ ~ ] to denote the expected value (mean) of ~, V [ ~ ] to denote its variance, and S [ ~ ] to denote its standard deviation. Then <ref> [30, x15] </ref>: E [ g (~) ] = g (t) dF (t) (where F is the distribution underlying ~) V [ ~ ] = E ( ~ E [ ~ ] ) 2 = E ~ 2 E [ ~ ] 2 q We can then prove the following important result: <p> Except for truly pathological x, enough random samples will make b normally distributed. We then have the confidence interval Pr j b j t n1 (ff=2) b = n = 1 ff where t n1 is the Student t distribution with n 1 degrees of freedom <ref> [30] </ref>. Copyright c fl1996, 1997 D. Stott Parker 24 5 Previous Work relating Statistics with Numeric Computation 5.1 The statistical theory of error The statistical nature of error has been important to scientists and numerical analysts for centuries, with a particularly strong early emphasis in the field of astronomy [93]. <p> Two-sided analysis is also inherent in statistical hypothesis testing. In statistics, most questions are reduced to statistical inequalities relating different parameters <ref> [30, x34] </ref>, of a form like Pr fi y y 0 fi fl fi fi Copyright c fl1996, 1997 D. Stott Parker 46 where * is an arbitrary threshold function and ffi is a function producing a confidence level. <p> These tests are not difficult to implement, but require maintaining a histogram for each sampled variable. A simpler test, which we implemented and seems to do fairly well, checks for large coefficients of skewness and excess (kurtosis) (cf. <ref> [30, x29.3] </ref> and/or [107, pp.234-235]). These measure higher moments of the samples, and moments can be computed incrementally (on-line) [91]. Skewness measures asymmetry of a distribution, while excess measures its flatness or peakedness. <p> Proof of this theorem is dependent of course on the conditions surrounding the summand variables and the type of convergence used. An excellent survey may be found in [33]; see also <ref> [30, 107] </ref> for formal treatments. For a history of early developements, see [93]. Fishman [34] lists a number of recently-derived variants of the Central Limit Theorem for Monte Carlo applications.
Reference: [31] <author> J.W. Demmel, </author> <title> "Underflow and the reliability of numerical software", </title> <journal> SIAM J. Sci. Stat Comput. </journal> <volume> 5:4, </volume> <pages> 887-919, </pages> <month> December </month> <year> 1984. </year>
Reference: [32] <author> J.W. Demmel, </author> <title> "The probability that a numerical analysis problem is difficult", </title> <booktitle> Mathematics of Computation 50:182, </booktitle> <pages> 449-480, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include <ref> [7, 13, 32, 70, 81] </ref>. In [81], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [33] <author> W. Feller, </author> <title> An Introduction to Probability Theory and its Applications, </title> <publisher> NY: J. Wiley & Sons, </publisher> <year> 1968. </year>
Reference-contexts: This strong statement ignores the mode's modest intent in Stretch, but Kahan's basic point is right. There is a better way to represent inexactness and track numeric significance. 4.4 Modeling inexactness with randomness Finally, we model inexact values as random variables | the fundamental idea of statistical probability theory <ref> [33] </ref>. <p> Proof of this theorem is dependent of course on the conditions surrounding the summand variables and the type of convergence used. An excellent survey may be found in <ref> [33] </ref>; see also [30, 107] for formal treatments. For a history of early developements, see [93]. Fishman [34] lists a number of recently-derived variants of the Central Limit Theorem for Monte Carlo applications.
Reference: [34] <author> G.S. Fishman, </author> <title> Monte Carlo: Concepts, Algorithms, and Applications, </title> <publisher> NY: Springer-Verlag, </publisher> <year> 1996. </year> <title> Copyright c fl1996, </title> <address> 1997 D. </address> <note> Stott Parker 82 </note>
Reference-contexts: Here s is a real value (typically a positive integer), and ~ is a random variable that can be discrete or continuous, and can depend on x, generating values from the interval ( 1 2 ; 1 The basic Monte Carlo approach is well-understood <ref> [34, 49, 50] </ref>, so implementing a Monte Carlo arithmetic on these values is straightforward. The real challenge comes from implementing this real arithmetic with floating-point hardware. <p> An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 5.2 Monte Carlo methods Monte Carlo methods <ref> [34, 49, 50] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes. <p> Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. Today Monte Carlo analysis is enjoying a resurgence of interest <ref> [34, 79] </ref>. It is worth mentioning that a number of available commercial Operations Research software packages for linear and nonlinear programming include Monte Carlo computation as a basic feature for sensitivity analysis. Among others, Paragon Decision Technology's AIMMS, Palisade's @RISK, and Sunset Software Technology's XA'llence rely on Monte Carlo analysis. <p> Monte Carlo Arithmetic seems quite close in spirit to what Metropolis had in mind throughout his career. As one of the founders of the Monte Carlo school [74] (see also <ref> [34, 49, 50] </ref>), it seems very likely that Metropolis thought of MCA-like schemes at some point. Copyright c fl1996, 1997 D. Stott Parker 43 7 Benefits of Monte Carlo Arithmetic MCA makes computer arithmetic more like real arithmetic. <p> Stott Parker 46 where * is an arbitrary threshold function and ffi is a function producing a confidence level. For example, the two-sided relative error condition is Pr fi y y 0 fi fl where * &gt; 0 and 0 &lt; ffi &lt; 1 are constants; see e.g. Fishman <ref> [34, x2.5] </ref>. <p> MCA is not committed to any assumption of normality. Sampling is an art, and the right approach to sampling can settle very tricky problems [26]. The book by Fishman <ref> [34] </ref> surveys many sampling techniques for Monte Carlo analysis. Nonparametric statistical analysis is also useful: one need not assume any particular sample distribution, but can instead develop and test hypotheses using other tools, particularly for order statistics [107]. <p> Third, one of the original uses of Monte Carlo methods was in the integration of differential equations. Whenever a problem can be phrased in terms of random walks or Markov chains, Monte Carlo techniques are sophisticated tools for finding qualitative solutions; see <ref> [34, Ch.5] </ref>. In the numerical solution of differential equations, rounding error often plays a significant and complex role. Numerical instability can arise in multiple ways: unstable solutions, unstable methods, and stiff problems. <p> We are skeptical the hardware cost or computation time would be excessive; in particular, fast random number generators can be implemented efficiently with feedback shift registers (e.g., [66]; a nice recent survey is in <ref> [34, x7.15-16] </ref>). Actually, the reverse can be argued! Though we are not endorsing their method, Yoshida, Goto, and Ichikawa [115] have shown that floating-point multipliers with random rounding can be implemented more efficiently than they can without it. <p> Another is that MCA is the adaptation Copyright c fl1996, 1997 D. Stott Parker 70 of Monte Carlo methods to floating-point arithmetic (where Monte Carlo is a well-developed branch of statistics <ref> [34] </ref>). This adaptation has various theoretical advantages, actually, such as resolution of floating-point anomalies. Another answer is to appeal to authority | C.F. Gauss, surely one of the most accomplished numerical analysts of all time. <p> Proof of this theorem is dependent of course on the conditions surrounding the summand variables and the type of convergence used. An excellent survey may be found in [33]; see also [30, 107] for formal treatments. For a history of early developements, see [93]. Fishman <ref> [34] </ref> lists a number of recently-derived variants of the Central Limit Theorem for Monte Carlo applications. For the special case of sums of roundoff errors, whose densities are convolutions of roundoff densities, the convergence to a normal distribution is strikingly rapid.
Reference: [35] <author> G.E. Forsythe, R.A. Leibler, </author> <title> "Matrix inversion by a Monte Carlo method.", Mathematical Tables and Other Aids to Computation 4, </title> <type> 127-129, </type> <year> 1950. </year>
Reference-contexts: Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler <ref> [35] </ref> (elaborating an idea of von Neumann and Ulam), and in Householder's book [56]. Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals.
Reference: [36] <author> G.E. </author> <title> Forsythe, "Round-off errors in numerical integration on automatic machinery. Preliminary report", </title> <journal> Bull. AMS 56, </journal> <volume> 61, </volume> <year> 1950. </year>
Reference-contexts: Although it has not been developed fully, random rounding has been used a number of times. Random rounding was considered in the early 1950s by George Forsythe <ref> [36, 37] </ref>. <p> Copyright c fl1996, 1997 D. Stott Parker 29 It seems clear that in the integration of smooth functions the ordinary rounding-off errors will frequently not be distributed like independent random variables. To circumvent [the problems of correlated errors * noted by Huskey], the present writer <ref> [36] </ref> has proposed a random rounding-off procedure which make * a true random variable. Suppose, for example, that a real number u is to be rounded off to an integer. Let [u] be the greatest integer not exceeding u, and let u [u] = v. <p> Forsythe [38] In concluding <ref> [36] </ref>, Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Prompted by the same results of Huskey for numerical integration that led to Forsythe's random rounding proposal, Henrici [51, ch.5] gives a good review of early <p> With MCA, this argument does not work. This is demonstrated with the equivalent plot, Randomization forces the roundoff errors to be random. Forsythe predicted randomization would have this effect <ref> [36] </ref>, but because this problem has considerable cancellation, we have used precision bounding, and not just the random rounding proposed by Forsythe. Figure 8 also shows the Copyright c fl1996, 1997 D. <p> Random rounding as proposed by Forsythe <ref> [36] </ref>, and implemented in MCA, avoids Huskey's phenomenon, because identical values are rounded randomly. Hull and Swensen [57] demonstrated this explicitly, and our full MCA implementation did too. Also, MCA avoided the high errors incurred when h = 2 12 .
Reference: [37] <author> G.E. </author> <title> Forsythe, "Note on rounding-off errors" (review by J. Todd), </title> <journal> Math. Rev. </journal> <volume> 12, 208, </volume> <year> 1951. </year>
Reference-contexts: Although it has not been developed fully, random rounding has been used a number of times. Random rounding was considered in the early 1950s by George Forsythe <ref> [36, 37] </ref>.
Reference: [38] <author> G.E. </author> <title> Forsythe, "Reprint of a note on rounding-off errors", </title> <journal> SIAM Review 1:1, </journal> <pages> 66-67, </pages> <year> 1959. </year> <title> Originally written June 1950 at the National Bureau of Standards, </title> <address> Los Angeles, CA, </address> <note> and abstracted in [37]. </note>
Reference-contexts: Although it has not been developed fully, random rounding has been used a number of times. Random rounding was considered in the early 1950s by George Forsythe [36, 37]. Inspired by the Monte Carlo approach, and the work of von Neumann and Goldstine, he noted <ref> [38] </ref> that roundoff errors in some problems (specifically citing Huskey's results for numerical integration [58]) are not distributed like independent random variables (a point stressed by Kahan [63] and Higham [54, x1.17,x2.6]), better error estimates could be obtained if they were. <p> Forsythe <ref> [38] </ref> In concluding [36], Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Prompted by the same results of Huskey for numerical integration that led to Forsythe's random rounding proposal, Henrici [51, ch.5] gives a good <p> The desired effect is achieved if ~ is a uniform_absolute variable, i.e., is uniform on 2 ; 1 : Then ~ has mean 0 and standard deviation 1= p This method was implemented and used successfully by Forsythe in 1959 <ref> [38] </ref>, Hull and Swenson (with coaching by Kahan) in 1966 [57], Callahan in 1976 [16], and probably others. <p> Proof We prove first the case t = p, which was essentially done by Forsythe <ref> [38] </ref> (see the reproduced paragraph on p.29). The case t &lt; p is a straightforward extension. Again, let (x) = fi pe x fi pe x Copyright c fl1996, 1997 D. Stott Parker 35 so that 0 j (x) j 1.
Reference: [39] <author> G.E. Forsythe, C.B. Moler, </author> <title> Computer Solution of Linear Algebraic Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1967. </year>
Reference-contexts: The computation reduces a general linear problem Ax = b to a pair of back substitution problems Ly = b, U x = y. For a good introduction see <ref> [39] </ref>; [59] gives excellent perspective on roundoff analyses for this problem.
Reference: [40] <author> G. </author> <title> Forsythe, "Solving a quadratic equation on a computer", </title> <booktitle> The Mathematical Sciences, </booktitle> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times. 2.1 A simple example Kahan (e.g., <ref> [40, 60, 61] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [41] <author> C.F. </author> <title> Gauss, Theoria Combinationis Observationum Erroribus Minimis Obnoxiae (Theory of the Combination of Observations Least Subject to Errors), translated by G.W. Stewart, </title> <address> Philadelphia, PA: </address> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: The probability that x has a value that lies within standard deviations of its mean is given by the error function Pr [ jx j ] = erf ( = 2 ) = p Z e 1 dz: For example, in <ref> [41, x9] </ref>, Gauss noted that the probabilities of an error lying within = 1, = 2:57, and = 3:89 standard deviations are, respectively, 0.6827, 0.99, and 0.9999. <p> In the 1820s Gauss revised the theory of errors and the method of least squares in Theoria Combinationis Observationum Erroribus Minimis Obnoxiae, of which an excellent recent edition by Stewart is now available <ref> [41] </ref>. This work is remarkable in many ways, and mainstream scientific and statistical discipline have so thoroughly adopted its ideas that much of it today seems like common sense. <p> Several noteworthy advances of this work were that Gauss removed his earlier assumption that the error distributions be normal (proposing instead to measure the central value and precision of a distribution by its mean and standard deviation , which he formulated in terms of moments <ref> [41, x5-8] </ref>), considered the problem of estimating the precision of the samples from their standard deviation, and criticized the use of the `biased' standard deviation estimate b naive = v u n i=1 where b is the average of the samples. <p> Gauss argued that it tends to overstate the precision, and that it should be replaced <ref> [41, x38] </ref> by the unbiased estimate b = u t 1 n X ( x i b ) 2 where (typically 1) is the number of parameters involved. This estimate is called unbiased since the expected value of ( b 2 2 ) is zero.
Reference: [42] <author> P.E. Gill, W. Murray, M.H. Wright, </author> <title> Numerical Linear Algebra and Optimization, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: By stability of a program here we mean the insensitivity of the computed values to perturbation of the particular input data given (estimated by their standard deviations as compared with the machine precision). This is much like forward stability of the program <ref> [42, p.47] </ref>, and less like backward stability. In this sense instability of the algorithm is essentially the same as ill-conditioning of the problem, i.e., high sensitivity of some computed values to minor changes in the input values. <p> These perspectives give fundamentally different interpretations of `quality of a solution' and can differ from the intuitive sense of solution quality entertained by numerical program users. A nice example illustrating this point is the ill-conditioned problem adapted from <ref> [42, p.50] </ref> of computing a root of (x 1) 4 = 0 (i.e., solving x 4 4x 3 + 6x 2 4x + 1 = 0), When working with eight-digit precision, an algorithm that produces the solution x = 1:01 has terrible forward error (10 6 times the machine precision).
Reference: [43] <author> D. Goldberg, </author> <title> "What every computer scientist should know about floating-point arithmetic", </title> <journal> ACM Computing Surveys 23:1, </journal> <pages> 5-48, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker 13 3 Problematic Issues in Floating-Point Arithmetic We review basic issues in floating-point arithmetic, and highlight some of the most important problems. Goldberg's tutorial <ref> [43] </ref> and Higham's encyclopedia [54, chs.1-5] are great references; here we develop only what is essential. 3.1 Floating-point arithmetic Any floating-point arithmetic system defines a certain set F of possible floating-point numbers. <p> Under this assumption, then, floating-point arithmetic is not to blame for the loss of significance. It has produced exact results, with zero error. Second, some texts point out that "cancellation is not always a bad thing" [54, p.12] (cf. also <ref> [43, Thm.11] </ref>). If the operands are exact, then the result is also. If the cancelled value is insignificant relative to the rest of the expression it appears it, then the cancellation affects nothing. <p> With this outlook, catastrophic cancellation is often viewed as a consequence of poor choice in algorithms, and it is the programmer who usually gets the blame. An excellent example is offered by Goldberg <ref> [43, pp.10-11] </ref>, who explains that computing x 2 y 2 with the expression (x x) (y y) can yield terrible results because of catastrophic cancellation, while the `equivalent' expression (x y) (x y) gives better results when x 2 y 2 , assuming x and y are exact, since then (x <p> The probabilities of these occurrences in MCA are zero if t digit precision (x) is implemented correctly. * In MCA, we recommend defining comparisons in terms of the underlying arithmetic opera tions, as programmers often assume this <ref> [43] </ref>: x &lt;fl y (x y) &lt; 0: In CADNA, comparisons are based on statistics [3, p.133]: x &lt;fl y b x b y &lt; ff b 2 y where b x is the computed average of x, b x is its computed standard deviation, and ff is half the Student
Reference: [44] <author> H.H. Goldstine, J. von Neumann, </author> <title> "Numerical inverting of matrices of high order. II", </title> <journal> Proc. Amer. Math. Soc. </journal> <volume> 2, </volume> <pages> 188-202, </pages> <year> 1951. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker 28 5.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [80] and 1951 <ref> [44] </ref>, who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [55, p.7] were not reflective of practice. <p> In the sequel paper <ref> [44] </ref>, following the development of Monte Carlo, Goldstine and von Neumann gave statistical bounds on the results of Gaussian elimination using the norms of a random matrix. <p> For a good introduction see [39]; [59] gives excellent perspective on roundoff analyses for this problem. Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann <ref> [44] </ref> (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss [8], Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23].
Reference: [45] <author> H.H. Goldstine, </author> <title> A History of Numerical Analysis from the 16th Through the 19th Century, NY: Springer-Verlag, </title> <booktitle> Studies in the History of Mathematics and Physical Sciences 2, </booktitle> <year> 1977. </year>
Reference-contexts: Gauss became the target of scornful allegations by Legendre, who had published his treatise on least squares in 1805 (cf. <ref> [45, p.210] </ref>, [93, pp.145-146]). Copyright c fl1996, 1997 D. Stott Parker 25 supported by his hypothesis of elementary errors, i.e., that the total error in these observations is a sum of individual, independent errors of small variance. <p> This was in keeping with Gauss' philosophy to publish nothing but well polished work of lasting significance. || Campbell & Meyer [17, pp.39-40] Goldstine, in his history of numerical analysis <ref> [45] </ref>, translates the remarks of Gauss on how having more samples can increase the accuracy of an estimated value: But in such a case, if it is proposed to aim at the greatest precision, we shall take care to collect and employ the greatest possible number of accurate places. <p> Nevertheless, an investigation of the connection between this law and the most probable orbit, which we will undertake in its utmost generality, is not to be regarded by any means a barren speculation. || C.F. Gauss, Theoria Motus, p.253; in <ref> [45, p.213] </ref>. Copyright c fl1996, 1997 D. Stott Parker 26 Gauss, according to Goldstein, was also first to systematically discuss the effect of rounding errors on the accuracy of numerical computation. Moreover, these discussions in Theoria Motus (excerpted and translated by Goldstine [45, pp.258-260]) are disconcertingly modern. <p> Gauss, Theoria Motus, p.253; in [45, p.213]. Copyright c fl1996, 1997 D. Stott Parker 26 Gauss, according to Goldstein, was also first to systematically discuss the effect of rounding errors on the accuracy of numerical computation. Moreover, these discussions in Theoria Motus (excerpted and translated by Goldstine <ref> [45, pp.258-260] </ref>) are disconcertingly modern. He also first described Gaussian elimination in the Theoria Motus. The impact of the method of least squares was immense; it was widely adopted in astronomy and geodesy [93, pp.39-40].
Reference: [46] <author> H.H. Goldstine, </author> <title> The Computer: From Pascal to von Neumann, </title> <publisher> Princeton University Press, </publisher> <year> 1993. </year>
Reference-contexts: We sensed that at least for positive definite matrices the Gaussian procedure could be shown to be quite stable. || Goldstine <ref> [46, p.290] </ref> In [80, p.1036], von Neumann and Goldstine argued that modeling roundoff errors with independent probabilistic estimates is natural since we know their average and worst-case values, and are ignorant of their exact distribution.
Reference: [47] <author> G.H. Golub, C.F. Van Loan, </author> <title> Matrix Computations: Second Edition, </title> <publisher> Baltimore: Johns Hop-kins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Having said all this, it is also worth saying that the use of stochastic or statistical methods in computer arithmetic is seldom discussed in contemporary numerical analysis literature. For example, the reference text <ref> [47] </ref> by Golub and Van Loan does not mention Vignes, and the only text we know of that mentions Vignes is Higham's encyclopedia [54, p.53], which allots two sentences to CESTAC. Generally, the coverage of significance arithmetic is similar. Interval arithmetic is mentioned in most texts, but is dismissed quickly.
Reference: [48] <author> G.H. Golub, J.M. Ortega, </author> <title> Scientific computing and differential equations: an introduction to numerical methods, </title> <address> Boston: </address> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: In the numerical solution of differential equations, rounding error often plays a significant and complex role. Numerical instability can arise in multiple ways: unstable solutions, unstable methods, and stiff problems. The texts by Golub and Ortega <ref> [48] </ref> or Parker and Chua [84] have excellent tutorials. 8.3.1 Random rounding can reduce global error in numerical integration Experimenting on ENIAC in 1948, Huskey [58] solved the simple system x 0 (t) = y (t); y 0 (t) = x (t) (having exact solution x (t) = sin (t), y
Reference: [49] <author> J.H. Halton, </author> <title> "A Retrospective and Prospective Survey of the Monte Carlo Method", </title> <journal> SIAM Review 12:1, </journal> <pages> 1-63, </pages> <year> 1970. </year>
Reference-contexts: Here s is a real value (typically a positive integer), and ~ is a random variable that can be discrete or continuous, and can depend on x, generating values from the interval ( 1 2 ; 1 The basic Monte Carlo approach is well-understood <ref> [34, 49, 50] </ref>, so implementing a Monte Carlo arithmetic on these values is straightforward. The real challenge comes from implementing this real arithmetic with floating-point hardware. <p> An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 5.2 Monte Carlo methods Monte Carlo methods <ref> [34, 49, 50] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes. <p> Monte Carlo Arithmetic seems quite close in spirit to what Metropolis had in mind throughout his career. As one of the founders of the Monte Carlo school [74] (see also <ref> [34, 49, 50] </ref>), it seems very likely that Metropolis thought of MCA-like schemes at some point. Copyright c fl1996, 1997 D. Stott Parker 43 7 Benefits of Monte Carlo Arithmetic MCA makes computer arithmetic more like real arithmetic.
Reference: [50] <author> J.M. </author> <title> Hammersley, D.C. Handscomb, Monte Carlo Methods, </title> <publisher> NY: Chapman and Hall, </publisher> <year> 1965. </year>
Reference-contexts: Here s is a real value (typically a positive integer), and ~ is a random variable that can be discrete or continuous, and can depend on x, generating values from the interval ( 1 2 ; 1 The basic Monte Carlo approach is well-understood <ref> [34, 49, 50] </ref>, so implementing a Monte Carlo arithmetic on these values is straightforward. The real challenge comes from implementing this real arithmetic with floating-point hardware. <p> An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 5.2 Monte Carlo methods Monte Carlo methods <ref> [34, 49, 50] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes. <p> A review of ENIAC in 1946 brought together Enrico Fermi, Stanislaw Ulam, and John von Neumann. In order to simulate random neutron diffusion in fissile material (extending the atomic bomb development of World War II), Ulam and von Neumann developed the Monte Carlo approach throughout 1947 <ref> [50, 76] </ref>. By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results [74]. <p> himself, he revealed that his "guesses" were really derived from the statistical sampling techniques that he used to calculate with whenever insomnia struck in the wee morning hours! And so it was that nearly fifteen years earlier Fermi had independently developed the Monte Carlo method. || Metropolis [76, p.128] In <ref> [50, ch.1] </ref>, Lord Kelvin is also credited with developing and using modern Monte Carlo techniques in his 1901 paper [65]. <p> Extraction of the medium significant bits from a sequence of squared values | the `midsquare' method of Metropolis and von Neumann [68, x3.1] | is how random numbers were produced in the original Monte Carlo computations on ENIAC, but this method was quickly discarded in favor of congruential methods <ref> [50, p.27] </ref>. 5.5 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 [99]. <p> the fact that exact values are degenerate cases of the inexact values just as non-random values are degenerate cases of the random values, which underlies a basic Monte Carlo principle: replacing any estimate by an exact value in a Monte Carlo computation reduces the sampling error in the final result <ref> [50, p.54] </ref>. Monte Carlo Arithmetic seems quite close in spirit to what Metropolis had in mind throughout his career. As one of the founders of the Monte Carlo school [74] (see also [34, 49, 50]), it seems very likely that Metropolis thought of MCA-like schemes at some point. <p> Monte Carlo Arithmetic seems quite close in spirit to what Metropolis had in mind throughout his career. As one of the founders of the Monte Carlo school [74] (see also <ref> [34, 49, 50] </ref>), it seems very likely that Metropolis thought of MCA-like schemes at some point. Copyright c fl1996, 1997 D. Stott Parker 43 7 Benefits of Monte Carlo Arithmetic MCA makes computer arithmetic more like real arithmetic.
Reference: [51] <author> P. Henrici, </author> <title> Error Propagation for Difference Methods, </title> <publisher> NY: J. Wiley & Sons, </publisher> <year> 1963. </year>
Reference-contexts: Forsythe [38] In concluding [36], Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Prompted by the same results of Huskey for numerical integration that led to Forsythe's random rounding proposal, Henrici <ref> [51, ch.5] </ref> gives a good review of early statistical analyses of roundoff error in numerical integration, and argues that it can be analyzed statistically. `Probabilistic rounding' (round_random_nearness again) was used by Hull and Swenson [57] in order to test the hypothesis that ordinary rounding can be modeled statistically as a random
Reference: [52] <author> P. Henrici, </author> <title> "Tests of Probabilistic Models for the Propagation of Roundoff Errors", </title> <journal> Comm. ACM 9:6, </journal> <pages> 409-410, </pages> <month> June </month> <year> 1966. </year>
Reference-contexts: Henrici was intrigued by this experimental approach, and showed their results could be predicted analytically to within an error of 10% <ref> [52] </ref>. We have also found the idea of random rounding in a paper of Callahan [16], who demonstrated its use in signal processing applications (perhaps the most well-developed domain for roundoff analysis). He also proposes round_random_nearness.
Reference: [53] <author> N.J. Higham, </author> <title> "The accuracy of floating point summation", </title> <journal> SIAM Journal on Scientific Computing 14:4, </journal> <pages> 783-799, </pages> <month> July </month> <year> 1993. </year> <title> Copyright c fl1996, </title> <address> 1997 D. </address> <note> Stott Parker 83 </note>
Reference-contexts: Even computing ordinary sums in floating-point arithmetic is problematic because of the possibility of catastrophic cancellation. See <ref> [53] </ref> or [54, ch.4] for an illuminating survey of the many tricks and techniques for dealing with summation. 3.2.3 Catastrophic cancellation produces major floating-point anomalies Floating-point arithmetic is widely (and correctly) viewed as a rat's nest of anomalies, i.e., important properties of real arithmetic it fails to satisfy, and which render
Reference: [54] <author> N.J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <address> Philadelphia, PA: </address> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: analysis is usually focused on basic algorithms, and not on large-scale models; it is significant that Wilkinson concludes his 1971 John von Neumann speech [111] by remarking that he expects error analysis will expand beyond its preoccupation with basic linear algebra problems, while Higham's comprehensive 1996 book on error analysis <ref> [54] </ref> goes no further. With this in mind, Pierce [85, x7.1] likens the problem of formal error analysis to program verification, and argues that proofs of numeric quality of programs of any reasonable scale are unachievable in practice. <p> Copyright c fl1996, 1997 D. Stott Parker 13 3 Problematic Issues in Floating-Point Arithmetic We review basic issues in floating-point arithmetic, and highlight some of the most important problems. Goldberg's tutorial [43] and Higham's encyclopedia <ref> [54, chs.1-5] </ref> are great references; here we develop only what is essential. 3.1 Floating-point arithmetic Any floating-point arithmetic system defines a certain set F of possible floating-point numbers. <p> In fact, the case is often made that floating-point arithmetic is blameless! First, many texts note that when catastrophic cancellation arises, the result of the floating-point operation is exact, under the nontrivial assumption that the subtraction operands are also exact (e.g., Higham <ref> [54, p.11] </ref>). That is, whenever the operands have zero error, the result also has zero error. Under this assumption, then, floating-point arithmetic is not to blame for the loss of significance. It has produced exact results, with zero error. <p> Under this assumption, then, floating-point arithmetic is not to blame for the loss of significance. It has produced exact results, with zero error. Second, some texts point out that "cancellation is not always a bad thing" <ref> [54, p.12] </ref> (cf. also [43, Thm.11]). If the operands are exact, then the result is also. If the cancelled value is insignificant relative to the rest of the expression it appears it, then the cancellation affects nothing. <p> Even computing ordinary sums in floating-point arithmetic is problematic because of the possibility of catastrophic cancellation. See [53] or <ref> [54, ch.4] </ref> for an illuminating survey of the many tricks and techniques for dealing with summation. 3.2.3 Catastrophic cancellation produces major floating-point anomalies Floating-point arithmetic is widely (and correctly) viewed as a rat's nest of anomalies, i.e., important properties of real arithmetic it fails to satisfy, and which render it difficult <p> An excellent summary of these papers is given by Wilkinson [111], and also by Higham <ref> [54, x9.6] </ref>, who reproduces the following remark by Goldstine about this work with von Neumann: 5 We did not feel it reasonable that so skilled a computer as Gauss would have fallen into the trap that Hotelling thought he had noted ... <p> Inspired by the Monte Carlo approach, and the work of von Neumann and Goldstine, he noted [38] that roundoff errors in some problems (specifically citing Huskey's results for numerical integration [58]) are not distributed like independent random variables (a point stressed by Kahan [63] and Higham <ref> [54, x1.17,x2.6] </ref>), better error estimates could be obtained if they were. He suggested that they can be forced to be random using the method we call (see p.34) round_random_nearness: 5 This perspective of Von Neumann addresses only Gaussian elimination, and not, for example, iterative computations like those in Section 2.2. <p> For example, the reference text [47] by Golub and Van Loan does not mention Vignes, and the only text we know of that mentions Vignes is Higham's encyclopedia <ref> [54, p.53] </ref>, which allots two sentences to CESTAC. Generally, the coverage of significance arithmetic is similar. Interval arithmetic is mentioned in most texts, but is dismissed quickly. Copyright c fl1996, 1997 D. <p> Higham <ref> [54, x1.17,x2.6] </ref>) argue that statistical analyses of roundoff error are improperly founded because they assume roundoff errors are random. <p> When Kahan changes the input values a bit, ProSolveur complains about attempting to compute the square root of a negative number. Kahan derides this as ignorance of a basic theorem of floating-point arithmetic (that the result of catastrophic cancellation is `exact', e.g., <ref> [54, p.12] </ref>). However, this theorem again rests on the assumption that the arguments of the cancellation are `exact', and Kahan's argument depends crucially on assumptions about error in catastrophic cancellation. Copyright c fl1996, 1997 D. <p> For a good introduction see [39]; [59] gives excellent perspective on roundoff analyses for this problem. Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and <ref> [54, p.187] </ref>), Barlow and Bareiss [8], Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan.
Reference: [55] <author> H. Hotelling, </author> <title> "Some new methods in matrix inversion", </title> <journal> Ann. Math. Stat. </journal> <volume> 14, </volume> <pages> 1-34, </pages> <year> 1943. </year>
Reference-contexts: Stott Parker 28 5.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [80] and 1951 [44], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling <ref> [55, p.7] </ref> were not reflective of practice.
Reference: [56] <author> A.S. </author> <title> Householder, Principles of Numerical Analysis, </title> <publisher> NY: McGraw-Hill, </publisher> <address> 1953. </address> <publisher> Reprinted by Dover Publications, </publisher> <year> 1974. </year>
Reference-contexts: Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler [35] (elaborating an idea of von Neumann and Ulam), and in Householder's book <ref> [56] </ref>. Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. Today Monte Carlo analysis is enjoying a resurgence of interest [34, 79]. <p> Specific other points about MCA we have tried to argue in this work include: * MCA is a simple way to bring some of the benefits of the Monte Carlo method <ref> [56] </ref> to floating-point computation. The Monte Carlo method offers simplicity; it replaces exact computation with random sampling, and replaces exact analysis with statistical analysis. * MCA gives a way to maintain information about the number of significant digits in conventional floating-point values.
Reference: [57] <author> T.E. Hull, J.R. Swenson, </author> <title> "Tests of Probabilistic Models for Propagation of Roundoff Errors", </title> <journal> Comm. ACM 9:2, </journal> <pages> 108-113, </pages> <month> February </month> <year> 1966. </year>
Reference-contexts: the same results of Huskey for numerical integration that led to Forsythe's random rounding proposal, Henrici [51, ch.5] gives a good review of early statistical analyses of roundoff error in numerical integration, and argues that it can be analyzed statistically. `Probabilistic rounding' (round_random_nearness again) was used by Hull and Swenson <ref> [57] </ref> in order to test the hypothesis that ordinary rounding can be modeled statistically as a random process. <p> The desired effect is achieved if ~ is a uniform_absolute variable, i.e., is uniform on 2 ; 1 : Then ~ has mean 0 and standard deviation 1= p This method was implemented and used successfully by Forsythe in 1959 [38], Hull and Swenson (with coaching by Kahan) in 1966 <ref> [57] </ref>, Callahan in 1976 [16], and probably others. The final two random rounding methods above satisfy the worst-case forward error bound j random round (x) x j fi e p when x is a nonzero floating-point value, e = fioom (x), and again t = p. <p> A quick look at Appendix A explains why: there are 4 fixed points of the iteration, and a sample distribution will therefore have 4 peaks. Hull and Swenson <ref> [57] </ref> found that, for a Runge-Kutta code with probabilistic rounding, when several test differential equations were integrated the errors followed a normal distribution. <p> They then experimentally verify the normality assumption using the CESTAC perturbation method, and use this in finding t-test confidence intervals for specific eigenvalues. Both Hull and Swensen <ref> [57, pp.110-111] </ref> and Chatelin and Brunet used the 2 method to verify normality of the distribution. Recently Chaitin-Chatelin and Fraysse [23, p.101] showed that Hull and Swensen's results hold only for regular points; normality does not hold in neighborhoods of singular points. <p> Random rounding as proposed by Forsythe [36], and implemented in MCA, avoids Huskey's phenomenon, because identical values are rounded randomly. Hull and Swensen <ref> [57] </ref> demonstrated this explicitly, and our full MCA implementation did too. Also, MCA avoided the high errors incurred when h = 2 12 . <p> The results are shown in Figures 12 and 13. MCA results reflect 5 samples obtained with ordinary rounding (round_to_nearest) and uniform_absolute input precision bounding. They agree closely with the single and double precision solutions, much as was observed by Hull and Swensen <ref> [57] </ref>; overall the magnitude of the global error is often less than the standard deviation of the samples, and was smaller than we had anticipated. The perspective of Parker and Chua quoted above was borne out.
Reference: [58] <author> H.D. Huskey, </author> <title> "On the precision of a certain procedure of numerical integration", </title> <editor> J. </editor> <booktitle> Research National Bureau of Standards 42, </booktitle> <pages> 57-62, </pages> <year> 1949. </year> <title> Includes the appendix: D.R. Hartree, "Note on Systematic Rounding-off Errors in Numerical Integration", </title> <publisher> p.62. </publisher>
Reference-contexts: Random rounding was considered in the early 1950s by George Forsythe [36, 37]. Inspired by the Monte Carlo approach, and the work of von Neumann and Goldstine, he noted [38] that roundoff errors in some problems (specifically citing Huskey's results for numerical integration <ref> [58] </ref>) are not distributed like independent random variables (a point stressed by Kahan [63] and Higham [54, x1.17,x2.6]), better error estimates could be obtained if they were. <p> Numerical instability can arise in multiple ways: unstable solutions, unstable methods, and stiff problems. The texts by Golub and Ortega [48] or Parker and Chua [84] have excellent tutorials. 8.3.1 Random rounding can reduce global error in numerical integration Experimenting on ENIAC in 1948, Huskey <ref> [58] </ref> solved the simple system x 0 (t) = y (t); y 0 (t) = x (t) (having exact solution x (t) = sin (t), y (t) = cos (t)) with Heun's method: x fl 2 ( y fl y fl 2 ( x fl ENIAC provided 10-digit decimal arithmetic, and <p> Thus the roundoff errors were not random, contradicting earlier arguments made by Rademacher [86]. Repeated rounding down made the resulting x (t) values inaccurate. In the appendix to <ref> [58] </ref>, Hartree explains Huskey's phenomenon as a consequence of properties of y = cos (t) and the ENIAC word length k = 10. Copyright c fl1996, 1997 D. Stott Parker 61 As an initial test, we reimplemented Huskey's computation.
Reference: [59] <author> W. Kahan, </author> <title> "Numerical Linear Algebra", </title> <journal> Canadian Mathematical Bulletin 9:6, </journal> <pages> 756-801, </pages> <year> 1966. </year>
Reference-contexts: The computation reduces a general linear problem Ax = b to a pair of back substitution problems Ly = b, U x = y. For a good introduction see [39]; <ref> [59] </ref> gives excellent perspective on roundoff analyses for this problem. <p> The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. These examples show that, at least for small linear systems, MCA can detect ill-conditioning. For the linear system Ax = b given by <ref> [59, p.772] </ref> 1:2969 0:8648 x = 0:1440 ! the vector b x = 0:9911 ! has the smallest nonzero residual A b x b with four-digit floating-point arithmetic, yet contains no significant digits! The exact solution is x = 2 ! The problem here is that the matrix A is ill-conditioned, <p> In other words, MCA detects the ill-conditioning by finding the computed solution entries not to be significant. A similar story holds for the linear system Ax = b defined by <ref> [59, p.788] </ref> 0:4322 0:2882 x = 0:2885 ! 13 Here 2 (M) = jj M jj 2 jj M 1 jj 2 , the condition number of M using the spectral norm jj jj 2 . Copyright c fl1996, 1997 D. <p> Copyright c fl1996, 1997 D. Stott Parker 57 with condition 2 (A) 6:75 fi 10 7 . Kahan notes <ref> [59, p.788] </ref> the exact solution is x = 1 ! Kahan stresses that the only clue of ill-conditioning is the cancellation in the first Gauss transformation, but single precision full MCA (with 5 samples) detects the ill-conditioning by again finding the computed solution entries not to be significant. <p> Finally, if the value of a 22 is changed to be 0:2822 (as in the typo in <ref> [59, p.787] </ref>), MCA obtains the solution average ( b x) = 0:0000101273 ; standard error ( b x) = 0:00000157369 with first entry correct to five significant digits, and the second entry correctly detected not to be significant (the correct value is about 7:7 fi 10 6 ). 8.2.2 The Turing-Wilkinson
Reference: [60] <author> W. Kahan, </author> <title> "A survey of error analysis," invited paper, </title> <booktitle> Proc. IFIP Congress 1971, </booktitle> <pages> 200-206, </pages> <month> August </month> <year> 1971. </year>
Reference-contexts: The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times. 2.1 A simple example Kahan (e.g., <ref> [40, 60, 61] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [61] <author> W. Kahan, </author> <title> "The programming environment's contribution to program robustness," </title> <journal> SIGNUM Newsletter, </journal> <month> Oct. </month> <year> 1981, </year> <month> p.10. </month>
Reference-contexts: The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times. 2.1 A simple example Kahan (e.g., <ref> [40, 60, 61] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [62] <author> W.V. Kahan, </author> <title> "Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic" (work in progress), </title> <institution> Dept. of Elect. Eng. & Computer Science, UC Berkeley, </institution> <note> dated May 31, 1996. Currently available as: http://http.cs.berkeley.edu/~wkahan/ieee754status/ieee754.ps </note>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi); where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 25, 62] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. <p> However, the standard does not otherwise track inexactness of values. Including the Inexact flag in the floating-point format instead would cost an additional bit, but would permit tracking of inexactness, and would eliminate the expense of monitoring this flag, which is controversial and has resulted in abuse <ref> [62] </ref>. Copyright c fl1996, 1997 D. Stott Parker 32 6.2 Implementing inexact values with randomization Let x be a numeric value (exact or inexact), and ~ be a random variable.
Reference: [63] <author> W.V. Kahan, </author> <title> "The Improbability of PROBABILISTIC ERROR ANALYSES for Numerical Computations", </title> <booktitle> lecture notes prepared for the UC Berkeley Statistics Colloquium, </booktitle> <month> 28 Febru-ary </month> <year> 1996, </year> <note> and subsequently revised (4 March 1996). (An earlier version of this lecture was presented at the third ICIAM Congress, 3-7 July, 1995.) Currently available as: http://http.cs.berkeley.edu/~wkahan/improber.ps </note>
Reference-contexts: Moreover, it can require some care in programming; for example, in noisy mode apparently the difference of two successive values in any iteration will never be zero. Even more surprisingly, the result of computing x x will apparently never be zero. Kahan <ref> [63, p.19] </ref> declares noisy mode to be the "worst way" to randomly perturb rounding errors. This strong statement ignores the mode's modest intent in Stretch, but Kahan's basic point is right. <p> Inspired by the Monte Carlo approach, and the work of von Neumann and Goldstine, he noted [38] that roundoff errors in some problems (specifically citing Huskey's results for numerical integration [58]) are not distributed like independent random variables (a point stressed by Kahan <ref> [63] </ref> and Higham [54, x1.17,x2.6]), better error estimates could be obtained if they were. <p> A value that has no significant digits is treated as an `informational zero' [104], and when tested by a comparison operator these zeroes produce exceptions. Kahan <ref> [63] </ref> has raised strong objections to the CESTAC approach, taking special issue with its assumption that roundoff errors are normally distributed. Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy. <p> Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy. He takes a strong stand, saying at one point that "probabilistic estimates of error are probably useless or worse" <ref> [63, p.7] </ref>. He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include [7, 13, 32, 70, 81]. <p> He takes a strong stand, saying at one point that "probabilistic estimates of error are probably useless or worse" [63, p.7]. He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits <ref> [63, p.19] </ref>. Other statistical studies of computer arithmetic include [7, 13, 32, 70, 81]. <p> Then ~ has mean and standard deviation 1= p This method is easily implemented with the existing IEEE standard through random toggling of the directed rounding bits, as suggested by Kahan in 1996 <ref> [63, p.19] </ref>. * round_random_nearness: Round x up or down with probability proportional to the distance from x to either of these two alternatives. <p> Thus it is possible to get a qualitative sense of the accuracy of 15-bit or 10-bit computation, for example. 0 2 4 6 t value T 20 (0:75) computed with full MCA (100 samples), at the indicated binary virtual precision t. 7.2 Roundoff errors actually do become random Kahan <ref> [63] </ref> and others (e.g. Higham [54, x1.17,x2.6]) argue that statistical analyses of roundoff error are improperly founded because they assume roundoff errors are random. <p> Unfortunately the paper [24], given as a formal basis for these assumptions, makes further assumptions and dismisses the possibility of exception to them in a cavalier way as "pratiquement jamais observe" (translated as "quasi never satisfied"). Kahan <ref> [63] </ref> exhibits problems for which these assumptions are invalid, and on which a CESTAC-based program named ProSolveur gives incorrect answers. * MCA has precision bounding t digit precision (x), while CADNA and CESTAC have PER (x), a random perturbation by 1 in the least significant bit of x. <p> For example, the problem suite of Kulisch and Miranker [72] produces severe cancellation errors. Kahan <ref> [63, p.23] </ref> also gives the problem of computing the ratio of areas of two narrow triangles r = (x + y + z) (z (x y)) (z + (x y)) (x + (y z))) where x = 1234567, y = 1234567, and z = 1:043e 8. <p> In general it is best in algebraic computations to leave the use of interval arithmetic as late as possible so that it effectively becomes an a posteriori weapon. A new option for analyzing roundoff can only help. As Kahan remarks <ref> [63, p.34] </ref>: Competent engineers rightly distrust all numerical computations and seek corroboration from al ternative numerical methods, from scale models, from prototypes, from experience, ... . * Won't running the program more than once be too expensive? Running the program more than once is clearly overhead.
Reference: [64] <author> T. Kapitaniak (ed.), </author> <title> Chaotic Oscillators: Theory and Applications, </title> <address> River Edge, NJ: </address> <publisher> World Scientific, </publisher> <year> 1992. </year>
Reference-contexts: Except for the trivial Duffing oscillator, none of these has an analytic solution. For more on chaotic oscillators, see e.g. <ref> [64] </ref>.
Reference: [65] <author> Lord Kelvin, </author> <title> "Nineteenth century clouds over the dynamical theory of heat and light", Phil. Mag. </title> <booktitle> (6th series) 2, </booktitle> <pages> 1-40, </pages> <year> 1901. </year>
Reference-contexts: with whenever insomnia struck in the wee morning hours! And so it was that nearly fifteen years earlier Fermi had independently developed the Monte Carlo method. || Metropolis [76, p.128] In [50, ch.1], Lord Kelvin is also credited with developing and using modern Monte Carlo techniques in his 1901 paper <ref> [65] </ref>. Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler [35] (elaborating an idea of von Neumann and Ulam), and in Householder's book [56].
Reference: [66] <author> S. Kirkpatrick, </author> <title> E.P. Stoll, "A very fast shift-register sequence random number generator", </title> <journal> J. Comp. Phys. </journal> <volume> 40:2, </volume> <pages> 517-526, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: We are skeptical the hardware cost or computation time would be excessive; in particular, fast random number generators can be implemented efficiently with feedback shift registers (e.g., <ref> [66] </ref>; a nice recent survey is in [34, x7.15-16]). Actually, the reverse can be argued! Though we are not endorsing their method, Yoshida, Goto, and Ichikawa [115] have shown that floating-point multipliers with random rounding can be implemented more efficiently than they can without it.
Reference: [67] <author> P.E. Kloeden, K.J. Palmer, eds., </author> <title> Chaotic Numerics, </title> <booktitle> Contemporary Mathematics series #172, </booktitle> <address> Providence, RI: </address> <publisher> AMS, </publisher> <year> 1994. </year>
Reference-contexts: Parker and Chua explain that solutions one obtains often are quite good because they approach an attractor (attracting limit set), and even if they are not in precise agreement with the intended trajectory, they at least follow the attractor. The paper [28] by Corless (in the edited volume <ref> [67] </ref>, which gives a good overview of numerical effects in chaotic dynamics) argues that the situation is similar to what arises in Gaussian elimination, and that the backwards error perspective is useful for chaotic systems (although the forward perspective is needed for stiff systems).
Reference: [68] <author> D.E. Knuth, </author> <title> The Art of Computer Programming. Vol. II: Seminumerical Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1969. </year>
Reference-contexts: This is an important distinction, since for example most quantities in scientific computation are inexact: constants like Avogadro's number 6:0225 fi 10 23 and Planck's constant 1:0545 fi 10 19 are known only to a few digits <ref> [68, x4.2.2.B] </ref>. <p> The user is given responsibility for interpreting the significance of all floating-point results. Since the very early work by Bauer and Samelson in 1953 [9], various authors have stressed that unnormalized arithmetic can improve on normalized arithmetic in some cases for retaining information about significance. See for example Knuth <ref> [68, x4.2.2.B] </ref>, the fixed-point arithmetic analyses of Wilkinson in [109], and the arguments for unnormalized arithmetic of Ashenhurst and Metropolis [4, 5], who show that it retains important information about the number of significant digits in the computed results. <p> The basic idea is that unnormalized numbers can, by convention, represent their significant digits (although in practice this approach tends to over- or understate significance somewhat <ref> [68] </ref>). In later work, Metropolis [75] proposes maintaining a distinction between two kinds of numbers: precise and imprecise. Precise values are represented with complete accuracy, and their preciseness is monitored by Metropolis' significant digit arithmetic (SDA). <p> Knuth <ref> [68, x4.2.2] </ref> has made a careful enumeration of axioms that are satisfied by floating-point operations and shows that they make it possible to prove basic arithmetic properties. As an important application, he shows how to build extended-precision arithmetic using these operations. <p> A major anomaly is that floating-point addition is not associative. Knuth gives the following example for eight-digit decimal arithmetic <ref> [68, p.196] </ref>: ( 11111113: 11111111: ) 7:5111111 = 2:0000000 7:5111111 = 9:5111111; This anomaly is a direct manifestation of catastrophic cancellation. Associativity fails after the cancellation removes all but the least significant digit, which is affected by roundoff errors. <p> Associativity fails after the cancellation removes all but the least significant digit, which is affected by roundoff errors. It is important to realize that modeling the significance (number of significant digits) of the input operands can evade problems of catastrophic cancellation. Knuth <ref> [68, x4.2.2.B] </ref> shows that the unnormalized arithmetic discussed above obeys an approximate associative law. Catastrophic cancellation is at the root of another major anomaly in floating-point arithmetic. <p> This is an important distinction. In scientific computation most quantities are inexact; constants like Avogadro's number 6:0225 fi 10 23 are known only to a few digits <ref> [68, x4.2.2.B] </ref>. The inexactness can be due to ignorance, uncertainty, estimation, measurement or computational error, but the upshot is that we have only a few significant digits. The distinction between exact and inexact values has been important in the implementation of significance arithmetics. <p> First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" [92, x3.1]. Second, there are serious problems of implementation. Implementations based on unnormalized arithmetic make error analysis tricky and can overstate or understate accuracy <ref> [68, x4.2.2B] </ref>. Implementing numbers of significant digits as integers gives results that are crude. Finally, implementing numbers of significant digits as real values (such as negative base-fi logarithms of relative error) is expensive. <p> Extraction of the medium significant bits from a sequence of squared values | the `midsquare' method of Metropolis and von Neumann <ref> [68, x3.1] </ref> | is how random numbers were produced in the original Monte Carlo computations on ENIAC, but this method was quickly discarded in favor of congruential methods [50, p.27]. 5.5 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at <p> For example, normality can be verified with the 2 test, which compares the histogram of the samples with the histogram expected from a normal distribution, or with a Kolmogorov-Smirnov test, which compares the cumulative distribution of the samples with a normal distribution <ref> [68, x3.3] </ref>. These tests are not difficult to implement, but require maintaining a histogram for each sampled variable. A simpler test, which we implemented and seems to do fairly well, checks for large coefficients of skewness and excess (kurtosis) (cf. [30, x29.3] and/or [107, pp.234-235]).
Reference: [69] <author> D.E. Knuth, </author> <booktitle> The Art of Computer Programming. Vol. II: Seminumerical Algorithms, 2nd edition, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: such complete cancellation that it raises floating-point exceptions, because the argument of the square root goes negative. (This actually happened in the initial test programs for this work!) It seems no one-pass algorithm for computing standard deviations avoids cancellation, although with some ingenuity the problem of exceptions can be avoided <ref> [69] </ref>. Even computing ordinary sums in floating-point arithmetic is problematic because of the possibility of catastrophic cancellation. <p> Early statistical analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth <ref> [69] </ref> and Sterbenz [92, x3.1.2]. Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [21, 22], and very recently [23]).
Reference: [70] <author> D. Kuck, D.S. Parker, A.H. Sameh, </author> <title> "Analysis of Floating-Point Rounding Methods", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-26:7, </volume> <pages> 643-650, </pages> <month> July </month> <year> 1977. </year> <title> Copyright c fl1996, </title> <address> 1997 D. </address> <note> Stott Parker 84 </note>
Reference-contexts: He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include <ref> [7, 13, 32, 70, 81] </ref>. In [81], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [71] <author> U. Kulisch, </author> <title> "Mathematical Foundation of Computer Arithmetic", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-26:7, </volume> <pages> 610-621, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Knuth [68, x4.2.2] has made a careful enumeration of axioms that are satisfied by floating-point operations and shows that they make it possible to prove basic arithmetic properties. As an important application, he shows how to build extended-precision arithmetic using these operations. The work of Kulisch and Miranker <ref> [71, 72] </ref> has made the axiomatic approach more popular, but it has hardly been received with universal acclaim, and formal methods still have far to go in influencing implementations of computer arithmetic (e.g., [87]). A major anomaly is that floating-point addition is not associative.
Reference: [72] <author> U. Kulisch, W.L. Miranker, </author> <title> "The Arithmetic of the Digital Computer: A New Approach", </title> <journal> SIAM Review 28:1, </journal> <pages> 1-40, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Knuth [68, x4.2.2] has made a careful enumeration of axioms that are satisfied by floating-point operations and shows that they make it possible to prove basic arithmetic properties. As an important application, he shows how to build extended-precision arithmetic using these operations. The work of Kulisch and Miranker <ref> [71, 72] </ref> has made the axiomatic approach more popular, but it has hardly been received with universal acclaim, and formal methods still have far to go in influencing implementations of computer arithmetic (e.g., [87]). A major anomaly is that floating-point addition is not associative. <p> For example, the problem suite of Kulisch and Miranker <ref> [72] </ref> produces severe cancellation errors.
Reference: [73] <author> M. La Porte, J. Vignes, </author> <title> "Methode numerique de detection de la singularite d'une matrice", </title> <journal> Numer. Math 23:1, </journal> <pages> 73-81, </pages> <year> 1974. </year>
Reference-contexts: The paper <ref> [73] </ref> apparently spawned the `permutation-perturbation' method, performing Gaussian elimination on a matrix both with random initial permutation of the matrix columns, and with random perturbation of some of the matrix entries Copyright c fl1996, 1997 D. <p> Copyright c fl1996, 1997 D. Stott Parker 52 8 Example Applications of Monte Carlo Arithmetic Several examples illustrate what MCA can do. A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution <ref> [73] </ref>, polynomial root finding [2], finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., [100, 103, 105, 106]) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [74] <author> N. Metropolis, S. Ulam, </author> <title> "The Monte Carlo method", </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 44, 335, </volume> <year> 1949. </year>
Reference-contexts: By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results <ref> [74] </ref>. Metropolis remarks that he invented the name `Monte Carlo' for the method because Ulam had a gambling uncle who borrowed money from relatives "because he just had to go to Monte Carlo" [76, p.127]. <p> Monte Carlo Arithmetic seems quite close in spirit to what Metropolis had in mind throughout his career. As one of the founders of the Monte Carlo school <ref> [74] </ref> (see also [34, 49, 50]), it seems very likely that Metropolis thought of MCA-like schemes at some point. Copyright c fl1996, 1997 D. Stott Parker 43 7 Benefits of Monte Carlo Arithmetic MCA makes computer arithmetic more like real arithmetic.
Reference: [75] <author> N. </author> <title> Metropolis, "Analyzed Binary Computing", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-22:6, </volume> <pages> 573-576, </pages> <month> June </month> <year> 1973. </year>
Reference-contexts: The basic idea is that unnormalized numbers can, by convention, represent their significant digits (although in practice this approach tends to over- or understate significance somewhat [68]). In later work, Metropolis <ref> [75] </ref> proposes maintaining a distinction between two kinds of numbers: precise and imprecise. Precise values are represented with complete accuracy, and their preciseness is monitored by Metropolis' significant digit arithmetic (SDA). <p> The inexactness can be due to ignorance, uncertainty, estimation, measurement or computational error, but the upshot is that we have only a few significant digits. The distinction between exact and inexact values has been important in the implementation of significance arithmetics. In Analyzed Binary Computing <ref> [75] </ref>, for example, Metropolis maintained the distinction between precise and imprecise values, and represented precise values with 3 When x = 0, we define e = fioom (x) = 0. Note: e = fioom (x) can differ from the effective floating-point exponent E of x, depending on the normalization convention. <p> Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. They wrote a number of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [4, 5, 11, 75] </ref>. Significance arithmetic faced several difficult problems, which it did not really overcome. First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" [92, x3.1]. Second, there are serious problems of implementation. <p> Inexactness is `contagious': any arithmetic operation involving an inexact operand yields an inexact value, except for degenerate expressions like (0 fi z). Ideally the distinction between exact and inexact floating-point values would be implemented in hardware, e.g., by a bit in the representation of floating-point values <ref> [75, 85] </ref>. 6 This was done in the NORC calculator in the early 1950s [92, p.196].
Reference: [76] <author> N. </author> <title> Metropolis, "The Beginning of the Monte Carlo method", </title> <address> Los Alamos Science 15, 125-130, </address> <year> 1987. </year>
Reference-contexts: A review of ENIAC in 1946 brought together Enrico Fermi, Stanislaw Ulam, and John von Neumann. In order to simulate random neutron diffusion in fissile material (extending the atomic bomb development of World War II), Ulam and von Neumann developed the Monte Carlo approach throughout 1947 <ref> [50, 76] </ref>. By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results [74]. <p> Metropolis remarks that he invented the name `Monte Carlo' for the method because Ulam had a gambling uncle who borrowed money from relatives "because he just had to go to Monte Carlo" <ref> [76, p.127] </ref>. He credits Fermi with having first conceived the Monte Carlo idea circa 1932: Fermi took great delight in astonishing his Roman colleagues with his remarkably accurate, "too-good-to-believe" predictions of experimental results. <p> After indulging himself, he revealed that his "guesses" were really derived from the statistical sampling techniques that he used to calculate with whenever insomnia struck in the wee morning hours! And so it was that nearly fifteen years earlier Fermi had independently developed the Monte Carlo method. || Metropolis <ref> [76, p.128] </ref> In [50, ch.1], Lord Kelvin is also credited with developing and using modern Monte Carlo techniques in his 1901 paper [65]. <p> An experimental attitude definitely helps in getting high-quality numerical results. In the words of Nick Metropolis <ref> [76, p.130] </ref>: It is, in fact, the coupling of the subtleties of the human brain with rapid and reliable calculations, both arithmetical and logical, by the modern computer that has stimulated the development of experimental mathematics. This development will enable us to achieve Olympian heights. Copyright c fl1996, 1997 D.
Reference: [77] <author> R.E. Moore, </author> <title> Interval Analysis, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1966. </year>
Reference-contexts: Interval arithmetic and significance arithmetic, for example, were considered by Turing as early as 1946 [111, p.566]. Interval computation <ref> [77, 78] </ref> has never really caught on because of the pessimism of its error bounds. (See the commentary by Wilkinson in Appendix B, for example.) Significance arithmetic has also failed to gain a large following. Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. <p> Practical error bounds should usually be determined by some form of a posteriori error analysis, since this takes full advantage of the statistical distribution of rounding errors and of any special features, such as sparseness, in the matrix. Interval analysis Interval computation <ref> [77, 78] </ref> has failed to gain widespread popularity. The most commonly given reason for this is that the intervals it produces are typically very pessimistic, and therefore not really useful in assessing the quality of the results.
Reference: [78] <author> R.E. Moore, </author> <title> Methods and Applications of Interval Analysis, </title> <address> Philadelphia, PA: </address> <publisher> SIAM, </publisher> <year> 1979. </year>
Reference-contexts: Interval arithmetic and significance arithmetic, for example, were considered by Turing as early as 1946 [111, p.566]. Interval computation <ref> [77, 78] </ref> has never really caught on because of the pessimism of its error bounds. (See the commentary by Wilkinson in Appendix B, for example.) Significance arithmetic has also failed to gain a large following. Significance arithmetic was popularized by Ashenhurst and Metropolis from the late 1950s through the 1970s. <p> Practical error bounds should usually be determined by some form of a posteriori error analysis, since this takes full advantage of the statistical distribution of rounding errors and of any special features, such as sparseness, in the matrix. Interval analysis Interval computation <ref> [77, 78] </ref> has failed to gain widespread popularity. The most commonly given reason for this is that the intervals it produces are typically very pessimistic, and therefore not really useful in assessing the quality of the results.
Reference: [79] <author> H. Niederreiter, </author> <title> Random Number Generation and Quasi-Monte Carlo Methods, </title> <address> Philadelphia: </address> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. Today Monte Carlo analysis is enjoying a resurgence of interest <ref> [34, 79] </ref>. It is worth mentioning that a number of available commercial Operations Research software packages for linear and nonlinear programming include Monte Carlo computation as a basic feature for sensitivity analysis. Among others, Paragon Decision Technology's AIMMS, Palisade's @RISK, and Sunset Software Technology's XA'llence rely on Monte Carlo analysis.
Reference: [80] <author> J. von Neumann, H.H. Goldstine, </author> <title> "Numerical inverting of matrices of high order", </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 53, </volume> <pages> 1021-1099, </pages> <year> 1947. </year>
Reference-contexts: Techniques useful in visualizing sensitivity of matrix computations are described in [23] and [113]. Copyright c fl1996, 1997 D. Stott Parker 28 5.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 <ref> [80] </ref> and 1951 [44], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [55, p.7] were not reflective of practice. <p> We sensed that at least for positive definite matrices the Gaussian procedure could be shown to be quite stable. || Goldstine [46, p.290] In <ref> [80, p.1036] </ref>, von Neumann and Goldstine argued that modeling roundoff errors with independent probabilistic estimates is natural since we know their average and worst-case values, and are ignorant of their exact distribution.
Reference: [81] <author> D.S. Parker, </author> <title> "The Statistical Theory of Relative Errors in Floating-Point Computation", M.S. </title> <type> Thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois, Urbana, IL, </institution> <year> 1976. </year>
Reference-contexts: He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include <ref> [7, 13, 32, 70, 81] </ref>. In [81], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions. <p> He also remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits [63, p.19]. Other statistical studies of computer arithmetic include [7, 13, 32, 70, 81]. In <ref> [81] </ref>, Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions. <p> Using a relative error analysis, we can go further and get statistical confidence intervals on the standard deviations produced by MCA for specific expressions. See <ref> [81] </ref>. 7.6 Open-ended statistical analysis is supported The easiest way to make strong assertions about the accuracy of computed values is the parametric approach | first to verify that the sample distribution follows a given distribution (such as the normal distribution), and then to apply statistical tests available for that distribution,
Reference: [82] <author> D.S. Parker, </author> <title> "Random Butterfly Transformations with Applications in Computational Linear Algebra", </title> <institution> UCLA Computer Science Department, </institution> <type> Technical Report CSD-950023, </type> <month> July </month> <year> 1995. </year>
Reference-contexts: Also, reproducibility can be attained by providing user access to random number initialization (allowing user specification of a seed value at any point). Furthermore, the `jitter' introduced by MCA can actually be useful in computations like equation-solving where degeneracy causes prob lems <ref> [82] </ref>. So while MCA is not needed in some situations, it may not harm to use it there. * Won't MCA give incorrect answers? All finite-precision arithmetics give incorrect answers. It is important to be clear here about what one means by `correct'.
Reference: [83] <author> D.S. Parker, </author> <title> "Explicit Formulas for the Results of Gaussian Elimination", </title> <institution> UCLA Computer Science Department, </institution> <type> Technical Report CSD-950025, </type> <year> 1995. </year>
Reference-contexts: Although the Hilbert matrix is positive-definite, it is spectacularly ill-conditioned. The ill-conditioning is such that when Gaussian elimination is applied to the 8 fi 8 matrix A with single precision computation, all significant digits are eroded from some entries of the matrix. In <ref> [83] </ref> we discuss this erosion further. <p> Although Gaussian elimination is of great importance, there remain a number of mysteries about the quality of its results [96]. Many of these revolve around its average- and worst-case roundoff error behavior, which is more amenable to direct computation than to formal analysis <ref> [83, 97] </ref>. Despite textbook assurances to the effect that partial pivoting is almost always adequate, it is not expensive to perform a statistical analysis in order to evaluate the quality of Gaussian elimination's results.
Reference: [84] <author> T.S. Parker, L.O. Chua, </author> <title> Practical Numerical Algorithms for Chaotic Systems, </title> <publisher> NY: Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: In the numerical solution of differential equations, rounding error often plays a significant and complex role. Numerical instability can arise in multiple ways: unstable solutions, unstable methods, and stiff problems. The texts by Golub and Ortega [48] or Parker and Chua <ref> [84] </ref> have excellent tutorials. 8.3.1 Random rounding can reduce global error in numerical integration Experimenting on ENIAC in 1948, Huskey [58] solved the simple system x 0 (t) = y (t); y 0 (t) = x (t) (having exact solution x (t) = sin (t), y (t) = cos (t)) with <p> What is true is that simulations of chaotic systems require careful interpretation and should always be verified. || Parker & Chua <ref> [84, p.110] </ref> In order to explain why numerical integration gives suprisingly good results, numerical analysts have taken different strategies.
Reference: [85] <author> B.A. Pierce, </author> <title> Applications of randomization to floating-point arithmetic and to linear systems solution, </title> <type> Ph.D. dissertation, </type> <institution> UCLA Computer Science Department, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: With this in mind, Pierce <ref> [85, x7.1] </ref> likens the problem of formal error analysis to program verification, and argues that proofs of numeric quality of programs of any reasonable scale are unachievable in practice. <p> Inexactness is `contagious': any arithmetic operation involving an inexact operand yields an inexact value, except for degenerate expressions like (0 fi z). Ideally the distinction between exact and inexact floating-point values would be implemented in hardware, e.g., by a bit in the representation of floating-point values <ref> [75, 85] </ref>. 6 This was done in the NORC calculator in the early 1950s [92, p.196]. <p> Boxed values are floating-point values. Copyright c fl1996, 1997 D. Stott Parker 38 * Random unrounding An elegant extension of input precision bounding was developed by Pierce <ref> [85] </ref>. He showed that it can be explained naturally in terms of random unrounding, which is the random conversion of a floating-point value to an inexact real value.
Reference: [86] <author> H. Rademacher, </author> <title> "On the accumulation of errors in processes of integration", </title> <journal> The Annals of the Computation Laboratory of Harvard University 16, </journal> <pages> 176-185, </pages> <year> 1948. </year>
Reference-contexts: Thus the roundoff errors were not random, contradicting earlier arguments made by Rademacher <ref> [86] </ref>. Repeated rounding down made the resulting x (t) values inaccurate. In the appendix to [58], Hartree explains Huskey's phenomenon as a consequence of properties of y = cos (t) and the ENIAC word length k = 10. Copyright c fl1996, 1997 D.
Reference: [87] <author> F. Ris, E. Barkmeyer, C. Schaffert, P. Farkas, </author> <title> "When floating-point addition isn't commutative", </title> <journal> SIGNUM Newsletter 28:1, </journal> <pages> 8-13, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The work of Kulisch and Miranker [71, 72] has made the axiomatic approach more popular, but it has hardly been received with universal acclaim, and formal methods still have far to go in influencing implementations of computer arithmetic (e.g., <ref> [87] </ref>). A major anomaly is that floating-point addition is not associative. Knuth gives the following example for eight-digit decimal arithmetic [68, p.196]: ( 11111113: 11111111: ) 7:5111111 = 2:0000000 7:5111111 = 9:5111111; This anomaly is a direct manifestation of catastrophic cancellation.
Reference: [88] <author> S. M. Rump, </author> <title> Reliability in Computing: The Role of Interval Methods in Scientific Computing, </title> <publisher> NY: Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Another example often used in the interval analysis literature is Rump's problem of evaluating f (x; y) = 333:75y 6 + x 2 11 x 2 y 2 y 6 121 y 4 2 + 5:5y 8 + 2 y at (x; y) = (77617; 33096) <ref> [88] </ref>. The results at various precisions are terrifying: while the correct answer is about 0:8273960599, IEEE single and double precision obtain the values -6.33825e+29 and -1.18059162071741e+21, respectively.
Reference: [89] <author> L. Shampine, H. Watts, S. Davenport, </author> <title> "Solving Nonstiff Ordinary Differential Equations - the State of the Art", </title> <journal> SIAM Review 18:3, </journal> <pages> 376-411, </pages> <month> July </month> <year> 1976. </year>
Reference-contexts: We suspect that statistical analysis gives better explanations. To test this suspicion, we ran the popular workhorse Runge-Kutta-Fehlberg RKF45 program <ref> [89] </ref> with MCA on both linear and nonlinear differential equations defining oscillators. RKF45 estimates error by comparing the results 14 Hartree's explanation seems slightly incorrect. <p> Also, and importantly, numerical algorithms often differ from their implementations in significant ways that affect the quality of the results dramatically. The paper by Shampine et al. <ref> [89] </ref> stresses, for example, that the ways ODE methods are implemented can be more significant than the formal differences between methods. Also note the opinion of Wilkinson [111, p.567]: There is still a tendency to attach too much importance to the precise error bounds obtained by an a priori analysis.
Reference: [90] <author> T. Simpson, </author> <title> "A letter to the Right Honorable George Earl of Macclesfield, President of the Royal Society, on the advantage of taking the mean of a number of observations, in practical astronomy," </title> <journal> Proc. Royal Society of London 49, </journal> <pages> 82-93, 1755. </pages> <address> Copyright c fl1996, 1997 D. </address> <note> Stott Parker 85 </note>
Reference-contexts: And the more observations or experiments there are made, the less will the conclusion be liable to err, provided they admit of being repeated under the same circumstances. || Simpson <ref> [90, pp.93-94] </ref> This statement of a law of large numbers was unsupported by proof, and Simpson's advice was long anticipated by at least the better astronomers. Nevertheless, the idea of basing this law upon mathematical calculations was new.
Reference: [91] <author> C.C. Spicer, </author> <title> "Calculation of Power Sums of Deviations About the Mean", </title> <journal> Applied Statistics 21:2, </journal> <pages> 226-227, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: A simpler test, which we implemented and seems to do fairly well, checks for large coefficients of skewness and excess (kurtosis) (cf. [30, x29.3] and/or [107, pp.234-235]). These measure higher moments of the samples, and moments can be computed incrementally (on-line) <ref> [91] </ref>. Skewness measures asymmetry of a distribution, while excess measures its flatness or peakedness.
Reference: [92] <author> P.H. Sterbenz, </author> <title> Floating-Point Computation, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: In the 1960s it became appreciated that the resolution of ties among nearest floating-point approximations was significant, because it arises frequently in floating-point addition. `Unbiased rounding' was developed to avoid this problem <ref> [92, x6.4] </ref>. Round to Nearest, the default IEEE method, resolves ties by choosing the nearest value b x whose least significant bit is zero, except that the magnitude of any value out of range rounds to 1. <p> Loss of Significance Error occurs in both the operands and operators in numerical computation, and measures the erosion of significant digits in computed values. A p-digit floating-point value x is specified to some number s of significant digits (which need not be integral <ref> [92, x3.1] </ref>), and this specification can denote any number x + " where j"j 1 2 fi es , if e is the floating-point exponent of x. We permit s &lt; p, so the number of significant digits of a value is not necessarily the precision of the machine. <p> We also show how to use randomness to model inexactness in floating-point computation, and how to use statistics (specifically, standard deviations) to measure the number of significant digits in an inexact value. 4.1 Formalizing accuracy, error, and number of significant digits Sterbenz <ref> [92, x3.1] </ref> discusses why it can be difficult to define what is meant by "the number of significant digits of a value", i.e., "the number of digits to which a value is accurate". <p> Let us now compare two different definitions | via relative error and a human algorithm of our own devising | and prove that they are close. The number of significant digits is often taken as a simple measure of relative error <ref> [92, p.72] </ref>. <p> Why is this distinction missing in all major computer programming languages? There seem to be several reasons, but a primary reason is lack of hardware support for monitoring inexactness. 4.3 Monitoring significance of computed results Computer scientists have long sought to include some automatic monitoring of accuracy in numerical computations <ref> [92, ch.7] </ref>. Interval arithmetic and significance arithmetic, for example, were considered by Turing as early as 1946 [111, p.566]. <p> Significance arithmetic faced several difficult problems, which it did not really overcome. First, there is the basic problem just discussed of defining what is meant by the "number of significant digits" <ref> [92, x3.1] </ref>. Second, there are serious problems of implementation. Implementations based on unnormalized arithmetic make error analysis tricky and can overstate or understate accuracy [68, x4.2.2B]. Implementing numbers of significant digits as integers gives results that are crude. <p> Early statistical analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth [69] and Sterbenz <ref> [92, x3.1.2] </ref>. Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [21, 22], and very recently [23]). <p> Ideally the distinction between exact and inexact floating-point values would be implemented in hardware, e.g., by a bit in the representation of floating-point values [75, 85]. 6 This was done in the NORC calculator in the early 1950s <ref> [92, p.196] </ref>. The distinction can still be implemented in an approximate way in software, using techniques described below in Section 6.8.1. 6 The IEEE 754 standard defines an Inexact flag, which is raised whenever the result of a floating-point computation does not fit within the floating-point format. <p> Greater precision allows the iteration to hold on longer near 6, but there is a linear erosion of accuracy, and it always will eventually arrive at 100. Sterbenz <ref> [92, x7.5] </ref> gives other nice examples of anomalies that can arise when re-running programs in higher precision. Copyright c fl1996, 1997 D.
Reference: [93] <editor> S.M. Stigler, </editor> <booktitle> The History of Statistics: The Measurement of Uncertainty before 1900, </booktitle> <address> Cam-bridge, MA: Belknap/Harvard U. </address> <publisher> Press, </publisher> <year> 1986. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker 24 5 Previous Work relating Statistics with Numeric Computation 5.1 The statistical theory of error The statistical nature of error has been important to scientists and numerical analysts for centuries, with a particularly strong early emphasis in the field of astronomy <ref> [93] </ref>. <p> Nevertheless, the idea of basing this law upon mathematical calculations was new. Simpson had seen that the concept of error distributions permitted a back-door access to the measurement of uncertainty. || Stigler <ref> [93, pp.91-94] </ref>. Simpson built directly upon the work of De Moivre, who in 1738 had succeeded in showing that Bernoulli's binomial distribution tended asymptotically to the normal distribution [93, p.82]. <p> Simpson had seen that the concept of error distributions permitted a back-door access to the measurement of uncertainty. || Stigler [93, pp.91-94]. Simpson built directly upon the work of De Moivre, who in 1738 had succeeded in showing that Bernoulli's binomial distribution tended asymptotically to the normal distribution <ref> [93, p.82] </ref>. The normal distribution of a variable x, having mean and standard deviation , is Pr [ x t ] = (t) = p Z t e 1 ) dx: Many basic foundations of mathematical statistics were then developed by Laplace. <p> Remarkably, although he published works on the normal distribution, error distributions, and Laplace transforms (developed from De Moivre and Simpson's generating functions), Laplace at first missed the idea of adopting the normal distribution as an error distribution, and e x 2 as an error curve <ref> [93, p.143] </ref>. He spent much of the 1770s working on the idea of an error curve, and focused particularly on the probability density (x) = (m=2) e mjxj [93, p.111]. <p> He spent much of the 1770s working on the idea of an error curve, and focused particularly on the probability density (x) = (m=2) e mjxj <ref> [93, p.111] </ref>. <p> Gauss became the target of scornful allegations by Legendre, who had published his treatise on least squares in 1805 (cf. [45, p.210], <ref> [93, pp.145-146] </ref>). Copyright c fl1996, 1997 D. Stott Parker 25 supported by his hypothesis of elementary errors, i.e., that the total error in these observations is a sum of individual, independent errors of small variance. <p> to complete the principle of least squares, under which the solution to a system of linear equations with least squares from the observed values is also the solution maximizing the probability of the observed values, assuming that the observations are of the same degree of accuracy and are normally distributed <ref> [93, pp.140-141] </ref>. Gauss used his method to sensational effect: In January of 1801 an astronomer named G. Piazzi briefly observed and then lost a `new planet' (actually this `new planet' was the asteroid now known as Ceres). <p> Moreover, these discussions in Theoria Motus (excerpted and translated by Goldstine [45, pp.258-260]) are disconcertingly modern. He also first described Gaussian elimination in the Theoria Motus. The impact of the method of least squares was immense; it was widely adopted in astronomy and geodesy <ref> [93, pp.39-40] </ref>. The normal distribution is still commonly referred to as the `Gaussian' distribution, although it was developed much earlier by both DeMoivre and Laplace. Seizing on Gauss' work, Laplace produced the Central Limit Theorem (see Appendix C.3) in 1810 and his classic Theorie analytique des probabilites in 1812. <p> Proof of this theorem is dependent of course on the conditions surrounding the summand variables and the type of convergence used. An excellent survey may be found in [33]; see also [30, 107] for formal treatments. For a history of early developements, see <ref> [93] </ref>. Fishman [34] lists a number of recently-derived variants of the Central Limit Theorem for Monte Carlo applications. For the special case of sums of roundoff errors, whose densities are convolutions of roundoff densities, the convergence to a normal distribution is strikingly rapid.
Reference: [94] <author> G.G. Szpiro, </author> <title> "Cycles and circles in roundoff errors", </title> <journal> Physical Review E, </journal> <volume> 47:6, </volume> <pages> 4560-4563, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Recently some researchers have shown that roundoff errors in differential equations corresponding to chaotic dynamical systems can have surprising and highly significant effects <ref> [10, 12, 27, 94] </ref>. These results are sometimes sensationalized, and predict disaster. In actual experience, on the other hand, numerical integration of these equations generally seems to do well, and give results that are consistent with physical experiments.
Reference: [95] <author> J.R. Taylor, </author> <title> An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements, </title> <address> Mill Valley, CA: </address> <publisher> University Science Books (Oxford University Press), </publisher> <year> 1982. </year>
Reference-contexts: b = n i=1 v u n 1 i=1 The expected error in b is measured by the standard deviation of b , called the standard error: standard error = = p n; which is estimated by b = p Use of standard error is common in scientific work (e.g., <ref> [95, 107] </ref>), and it is traditional to write " = average estimated standard error" (" = b b = p This notation is misleading since it does not give hard bounds on the error, but instead gives `one standard deviation' bounds. <p> This distinction is still one of the first topics in texts on experimental error (cf. <ref> [95] </ref>), and is fundamental to experimental design. Random errors are much easier to analyze, both formally and experimentally. The central idea behind the approaches of Simpson and Gauss is the identification of random errors with random variables, which have underlying distributions and independence properties. <p> Thus statistical inference is embedded in the comparison operator, and stochastic arithmetic is similar to the classical theory of propagation of error (or uncertainty) <ref> [95] </ref>. Actually, every fixed-precision arithmetic system presents this problem.
Reference: [96] <author> L.N. Trefethen, </author> <title> "Three mysteries of Gaussian elimination", </title> <journal> ACM SIGNUM Newsletter 20:4, </journal> <pages> 2-5, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: For example, S.J. Wright [112] finds exponential error growth for a specific family of matrices arising in two-point boundary value problems. Although Gaussian elimination is of great importance, there remain a number of mysteries about the quality of its results <ref> [96] </ref>. Many of these revolve around its average- and worst-case roundoff error behavior, which is more amenable to direct computation than to formal analysis [83, 97].
Reference: [97] <author> L.N. Trefethen, </author> <title> R.S. Schreiber, "Average-Case Stability of Gaussian Elimination", </title> <journal> SIAM J. Matrix Anal. Appl. </journal> <volume> 11:3, </volume> <pages> 335-360, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: For a good introduction see [39]; [59] gives excellent perspective on roundoff analyses for this problem. Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss [8], Trefethen and Schreiber <ref> [97] </ref>, Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. These examples show that, at least for small linear systems, MCA can detect ill-conditioning. <p> Although Gaussian elimination is of great importance, there remain a number of mysteries about the quality of its results [96]. Many of these revolve around its average- and worst-case roundoff error behavior, which is more amenable to direct computation than to formal analysis <ref> [83, 97] </ref>. Despite textbook assurances to the effect that partial pivoting is almost always adequate, it is not expensive to perform a statistical analysis in order to evaluate the quality of Gaussian elimination's results.
Reference: [98] <author> A.M. </author> <title> Turing, "Rounding-Off Errors in Matrix Processes", </title> <journal> Quart. J. Mech. </journal> <volume> 1, </volume> <pages> 287-308, </pages> <year> 1948. </year>
Reference-contexts: Turing, in his 1948 analysis of Gaussian elimination that originated the idea of LDU matrix decomposition, had actually made an analysis for random matrices also, remarking that normally distributed random matrices have expected spectral norm on the order of p n, and that "random matrices are only slightly ill-conditioned" <ref> [98, p.299] </ref>. Early statistical analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth [69] and Sterbenz [92, x3.1.2]. <p> In <ref> [98, p.307] </ref>, Turing remarks that the matrices defined by T n = B B B B @ 1 1 0 0 0 . . . . . . . . . 1 1 1 1 0 1 C C C C ; T 1 0 B B B B 1 0
Reference: [99] <author> J. Vignes, M. La Porte, </author> <title> "Error Analysis in Computing", </title> <booktitle> Proc. IFIP 1974, </booktitle> <publisher> North-Holland, </publisher> <pages> 610-614, </pages> <year> 1974. </year>
Reference-contexts: numbers were produced in the original Monte Carlo computations on ENIAC, but this method was quickly discarded in favor of congruential methods [50, p.27]. 5.5 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 <ref> [99] </ref>. The paper [73] apparently spawned the `permutation-perturbation' method, performing Gaussian elimination on a matrix both with random initial permutation of the matrix columns, and with random perturbation of some of the matrix entries Copyright c fl1996, 1997 D. <p> Stott Parker 30 (setting their least significant bits to 0 or 1, i.e., perturbing by 0 or 2 p ). In <ref> [99] </ref> `permutation' has evolved to mean changing the order in which additions are performed, and `perturbation' is the addition of a random 0 or 1 value to the least significant bit of a floating-point fraction. These were both implemented in a single FORTRAN function P, later called PEPER [100].
Reference: [100] <author> J. Vignes, </author> <title> "New methods for evaluating the validity of the results of mathematical computations", </title> <booktitle> Mathematics and Computers in Simulation XX, </booktitle> <pages> 227-249, </pages> <year> 1978. </year>
Reference-contexts: In [99] `permutation' has evolved to mean changing the order in which additions are performed, and `perturbation' is the addition of a random 0 or 1 value to the least significant bit of a floating-point fraction. These were both implemented in a single FORTRAN function P, later called PEPER <ref> [100] </ref>. Vignes and Ung patented the idea in Europe in 1979 [101], and in the USA in 1983 [102]. The retrospective survey [103] reviews the results of a decade of research on the permutation-perturbation method. <p> A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., <ref> [100, 103, 105, 106] </ref>) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [101] <author> J. Vignes, V. </author> <type> Ung, </type> <institution> Procede et ensemble de calcul aleatoirement par defaut ou par exces, pour fournir des resultats de calcul avec le nombre de chiffres significatifs exacts, </institution> <note> European Patent No. 7902784 (1979). </note>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [100]. Vignes and Ung patented the idea in Europe in 1979 <ref> [101] </ref>, and in the USA in 1983 [102]. The retrospective survey [103] reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method.
Reference: [102] <author> J. Vignes, V. </author> <title> Ung, Arrangement for determining number of exact significant figures in calculated results, </title> <type> U.S. </type> <note> Patent 4,367,536 (1983). </note>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [100]. Vignes and Ung patented the idea in Europe in 1979 [101], and in the USA in 1983 <ref> [102] </ref>. The retrospective survey [103] reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method.
Reference: [103] <author> J. Vignes, R. Alt, </author> <title> "An Efficient Stochastic Method for Round-off Error Analysis", in Accurate Scientific Computations (LNCS #235), </title> <editor> W.L. Miranker and R.A. Toupin (eds.), </editor> <address> NY: </address> <publisher> Springer-Verlag, </publisher> <pages> 183-205, </pages> <year> 1985. </year>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [100]. Vignes and Ung patented the idea in Europe in 1979 [101], and in the USA in 1983 [102]. The retrospective survey <ref> [103] </ref> reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method. <p> A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., <ref> [100, 103, 105, 106] </ref>) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [104] <author> J. </author> <type> Vignes, </type> <institution> "Zero mathematique et zero informatique", Comptes Rendus de l'Academie des Sciences, Serie I (Mathematique) 303:20, </institution> <month> 997-1000, 21 Dec. </month> <year> 1986. </year>
Reference-contexts: The definition also permits modeling of `totally inexact' values | values with no significant digits | when we permit negative values to be chosen for s. Vignes calls such values informatical zeroes <ref> [104] </ref>. It is often easier to analyze relative errors than absolute errors. <p> It requires a different implementation, in which each arithmetic expression is treated like a CESTAC program: it is evaluated 2 or 3 times, then the results averaged (and tested statistically if desired). A value that has no significant digits is treated as an `informational zero' <ref> [104] </ref>, and when tested by a comparison operator these zeroes produce exceptions. Kahan [63] has raised strong objections to the CESTAC approach, taking special issue with its assumption that roundoff errors are normally distributed. Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy. <p> Copyright c fl1996, 1997 D. Stott Parker 51 * As the inequality above suggests, CADNA treats values whose standard deviation exceeds their computed mean as `zero' <ref> [104] </ref>. In a sense, statistical inference is tied to the number zero itself, and this inference rests on assumptions about the distribution of errors that may not hold (e.g., as they are proved not to hold for eigenvalue computations in [21, Theorem 8.2; cf. also p.158]).
Reference: [105] <author> J. Vignes, </author> <title> "Review on stochastic approach to round-off error analysis and its applications," </title> <booktitle> Mathematics and Computers in Simulation 30:6, </booktitle> <pages> 481-491, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Finally the mean value and number of significant digits of these results are computed and printed by a module that performs a Student t computation on their 2 or 3 values. Vignes' survey <ref> [105] </ref> accompanies an entire journal issue with papers describing applications of CESTAC. <p> Vignes' survey [105] accompanies an entire journal issue with papers describing applications of CESTAC. Both PER and the perturbation mode of PEPER work identically <ref> [14, 105] </ref>: when the underlying machine arithmetic works with ordinary rounding, PER adds a least significant bit 1 with probability 1 4 , adds 0 with probability 1 2 , and subtracts 1 with probability 1 4 . <p> A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., <ref> [100, 103, 105, 106] </ref>) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [106] <author> J. Vignes, </author> <title> "A stochastic arithmetic for reliable scientific computation", </title> <booktitle> Mathematics and Computers in Simulation 35, </booktitle> <pages> 233-261, </pages> <year> 1993. </year>
Reference-contexts: This method can be implemented efficiently, and without extended precision. More recently, CESTAC has been reformulated as stochastic arithmetic and incorporated in the CADNA (Control of Accuracy and Debugging for Numerical Applications) library, which implements stochastic arithmetic for FORTRAN and Ada programs <ref> [106] </ref>. Stochastic arithmetic includes not only the basic arithmetic operators with perturbation, but also operators for comparing stochastic values (one can statistically test the hypotheses x &lt; y or x = y, for stochastic values x and y). This is a significant change from CESTAC. <p> It is illuminating to think of CESTAC as being incorrect about 5% of the time.) These criticisms led to refinements by Vignes in [24] and <ref> [106, pp.241-242] </ref>. Unfortunately the paper [24], given as a formal basis for these assumptions, makes further assumptions and dismisses the possibility of exception to them in a cavalier way as "pratiquement jamais observe" (translated as "quasi never satisfied"). <p> A large number of basic examples have been developed by Vignes and his coworkers: rank computations and linear system solution [73], polynomial root finding [2], finding eigenvalues [21], solving ODEs [3], etc. The survey papers by Vignes (e.g., <ref> [100, 103, 105, 106] </ref>) have many references with both new and classical numerical analysis examples that show interesting results with randomization, and should be consulted for perspective.
Reference: [107] <editor> B.L. van der Waerden, </editor> <booktitle> Mathematical Statistics, 2nd edition, </booktitle> <address> NY: </address> <publisher> Springer-Verlag, </publisher> <year> 1969. </year>
Reference-contexts: b = n i=1 v u n 1 i=1 The expected error in b is measured by the standard deviation of b , called the standard error: standard error = = p n; which is estimated by b = p Use of standard error is common in scientific work (e.g., <ref> [95, 107] </ref>), and it is traditional to write " = average estimated standard error" (" = b b = p This notation is misleading since it does not give hard bounds on the error, but instead gives `one standard deviation' bounds. <p> The relationship between the normal distribution and error was established in 1795 when, at the age of eighteen, Gauss conceived the method of least squares. 4 Inspired by the work of Laplace, Gauss's initial theory of errors <ref> [107, Chs.6-7] </ref> took errors to be normally distributed, which was 4 Or so Gauss later claimed, since his work was not published until 1809. Gauss became the target of scornful allegations by Legendre, who had published his treatise on least squares in 1805 (cf. [45, p.210], [93, pp.145-146]). <p> These tests are not difficult to implement, but require maintaining a histogram for each sampled variable. A simpler test, which we implemented and seems to do fairly well, checks for large coefficients of skewness and excess (kurtosis) (cf. [30, x29.3] and/or <ref> [107, pp.234-235] </ref>). These measure higher moments of the samples, and moments can be computed incrementally (on-line) [91]. Skewness measures asymmetry of a distribution, while excess measures its flatness or peakedness. <p> The book by Fishman [34] surveys many sampling techniques for Monte Carlo analysis. Nonparametric statistical analysis is also useful: one need not assume any particular sample distribution, but can instead develop and test hypotheses using other tools, particularly for order statistics <ref> [107] </ref>. Open-endedness is vital. 7.7 Limitations of CESTAC and CADNA are avoided MCA generalizes and improves upon existing approaches for random rounding or random perturbation of numerical results. <p> Proof of this theorem is dependent of course on the conditions surrounding the summand variables and the type of convergence used. An excellent survey may be found in [33]; see also <ref> [30, 107] </ref> for formal treatments. For a history of early developements, see [93]. Fishman [34] lists a number of recently-derived variants of the Central Limit Theorem for Monte Carlo applications.
Reference: [108] <author> J.H. Wilkinson, </author> <title> "Error analysis of direct methods of matrix inversion", </title> <journal> J. ACM 8:3, </journal> <pages> 281-330, </pages> <month> July </month> <year> 1961. </year>
Reference-contexts: If b (n) = c = c (1) , the second phase then solves the triangular system b U b x = c by back substitution. D.2 Error analysis We follow the error analysis of Barlow and Bareiss [8], which differs from that of Wilkinson <ref> [108] </ref> to support statistical analysis.
Reference: [109] <author> J.H. Wilkinson, </author> <title> Rounding Errors in Algebraic Processes, </title> <address> NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1963. </year>
Reference-contexts: Since the very early work by Bauer and Samelson in 1953 [9], various authors have stressed that unnormalized arithmetic can improve on normalized arithmetic in some cases for retaining information about significance. See for example Knuth [68, x4.2.2.B], the fixed-point arithmetic analyses of Wilkinson in <ref> [109] </ref>, and the arguments for unnormalized arithmetic of Ashenhurst and Metropolis [4, 5], who show that it retains important information about the number of significant digits in the computed results. <p> Catastrophic cancellation is at the root of another major anomaly in floating-point arithmetic. Although floating-point values x, y with sum in &lt; F f0g always satisfy the relative error bound fi fi (x + y) (x y) fi fi Wilkinson <ref> [109, p.17] </ref> points out that there is no useful bound on the relative error fi fi (x + y + z) ((x y) z) fi fi when x, y, and z are floating-point values with sum in &lt; F f0g. <p> Parker 18 Certainly Wilkinson is right in stating for backward error analysis that "it seems highly improbable that any method of solving any problem which is restricted to the use of t-digit arithmetic will, in general, do better than give the solution to a t-digit approximation to the original problem" <ref> [109, p.32] </ref>. If we accept the limitations of floating-point arithmetic, and therefore ignore the notion of significance, backward error bounds are essentially the best one can hope for. However, interpreting backward error bounds correctly requires education. <p> Using standard errors instead of standard deviations can be important when the number of samples is nontrivial. Wilkinson <ref> [109, p.29] </ref> gives an interesting abstract development of condition numbers, pointing out that the problem being solved determines which aggregate condition number bounds one can establish. <p> terms needed for convergence (4 samples). 8.1.2 Tchebycheff polynomials For another example, consider the Tchebycheff polynomial studied by Wilkinson T 20 (z) = cos (20 cos 1 (z)) 2050048 z 10 + 549120 z 8 84480 z 6 + 6600 z 4 200 z 2 + 1 Wilkinson points out <ref> [109, pp.46-47] </ref> that the zeroes of this polynomial are at cos ((2k + 1)=40) for k = 0; 1; : : : ; 19, and that the polynomial is moderately ill-conditioned at the roots near 1, but is otherwise well-conditioned. <p> pivoting is to reduce the standard deviations somewhat (some by an order of magnitude), but the computation still ultimately loses all significant digits in some entries of b A (n) . 8.2.4 Perspective on error bounds Wilkinson established the following well-known backward error bound for partial pivoting: Theorem 6 (Wilkinson <ref> [109] </ref>) Suppose A is an n fi n nonsingular matrix. <p> On the contrary, it is easy to produce examples with worse behavior. This example has the benefit of having a simple exact solution. Of course, none of this would have surprised Wilkinson; for an eloquent discussion with great historical perspective see <ref> [109, p.118] </ref> and his John von Neumann lecture [111]. Gaussian elimination is just a very compelling example that has persuaded many numerical analysts to adopt the backward perspective, giving a way to account for its success in practice.
Reference: [110] <author> J.H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> London: Oxford University Press, </publisher> <year> 1965. </year> <title> Copyright c fl1996, </title> <address> 1997 D. </address> <note> Stott Parker 86 </note>
Reference-contexts: It is plain from the definition here that 1 ( T n ) = n 2 n1 where 1 (M ) = jj M jj 1 jj M 1 jj 1 is the condition number of M using the column-sum norm jj jj 1 . Turing's colleague Wilkinson <ref> [110, p.212] </ref> later adapted T n to give a matrix W n with worst-case error in LU decomposition with partial pivoting, where L = T n : W n = B B B @ 1 1 0 0 1 . . . . . . . . . 1 1 1
Reference: [111] <author> J.H. Wilkinson, </author> <title> "Modern Error Analysis", </title> <journal> SIAM Review 13:4, </journal> <pages> 548-568, </pages> <month> October </month> <year> 1971. </year>
Reference-contexts: Interval analysis, although given lip service, appears little used in practice; the oft-cited reason for this is that its bounds tend to be overly pessimistic <ref> [111] </ref>. Formal error analysis is usually focused on basic algorithms, and not on large-scale models; it is significant that Wilkinson concludes his 1971 John von Neumann speech [111] by remarking that he expects error analysis will expand beyond its preoccupation with basic linear algebra problems, while Higham's comprehensive 1996 book on <p> although given lip service, appears little used in practice; the oft-cited reason for this is that its bounds tend to be overly pessimistic <ref> [111] </ref>. Formal error analysis is usually focused on basic algorithms, and not on large-scale models; it is significant that Wilkinson concludes his 1971 John von Neumann speech [111] by remarking that he expects error analysis will expand beyond its preoccupation with basic linear algebra problems, while Higham's comprehensive 1996 book on error analysis [54] goes no further. <p> Interval arithmetic and significance arithmetic, for example, were considered by Turing as early as 1946 <ref> [111, p.566] </ref>. Interval computation [77, 78] has never really caught on because of the pessimism of its error bounds. (See the commentary by Wilkinson in Appendix B, for example.) Significance arithmetic has also failed to gain a large following. <p> An excellent summary of these papers is given by Wilkinson <ref> [111] </ref>, and also by Higham [54, x9.6], who reproduces the following remark by Goldstine about this work with von Neumann: 5 We did not feel it reasonable that so skilled a computer as Gauss would have fallen into the trap that Hotelling thought he had noted ... <p> For a good introduction see [39]; [59] gives excellent perspective on roundoff analyses for this problem. Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are <ref> [111] </ref> and [54, p.187]), Barlow and Bareiss [8], Trefethen and Schreiber [97], Yeung and Chan [114], and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. <p> On the contrary, it is easy to produce examples with worse behavior. This example has the benefit of having a simple exact solution. Of course, none of this would have surprised Wilkinson; for an eloquent discussion with great historical perspective see [109, p.118] and his John von Neumann lecture <ref> [111] </ref>. Gaussian elimination is just a very compelling example that has persuaded many numerical analysts to adopt the backward perspective, giving a way to account for its success in practice. We argue that statistical analysis gives a better way to appreciate Gaussian elimination. <p> The paper by Shampine et al. [89] stresses, for example, that the ways ODE methods are implemented can be more significant than the formal differences between methods. Also note the opinion of Wilkinson <ref> [111, p.567] </ref>: There is still a tendency to attach too much importance to the precise error bounds obtained by an a priori analysis. In my opinion, the bound itself is usually the least important part of it. <p> Interval analysis Interval computation [77, 78] has failed to gain widespread popularity. The most commonly given reason for this is that the intervals it produces are typically very pessimistic, and therefore not really useful in assessing the quality of the results. An excellent summary is offered by Wilkinson <ref> [111, pp.566-567] </ref>: If this mode of arithmetic is applied, for example, to the elimination method for solving linear equations, it is found that after a few steps the intervals become very wide, the intervals representing the solution being so wide that virtually no information is obtained.
Reference: [112] <author> S.J. Wright, </author> <title> "A Collection of Problems for which Gaussian Elimination with Partial Pivoting is Unstable", </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 14:1, </volume> <pages> 231-238, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The empirical attitude is very important. It pays to question the quality of numerical results. Recently a number of researchers have begun to question the stability of Gaussian elimination with partial pivoting in practice. For example, S.J. Wright <ref> [112] </ref> finds exponential error growth for a specific family of matrices arising in two-point boundary value problems. Although Gaussian elimination is of great importance, there remain a number of mysteries about the quality of its results [96].
Reference: [113] <author> C.M. Wittenbrink, A.T. Pang, S.K. Lodha, </author> <title> "Glyphs for Visualizing Uncertainty in Vector Fields", </title> <journal> IEEE Trans. Vis. Comput. Graph. </journal> <volume> 2:3, </volume> <pages> 266-279, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: One feature common to such packages is that they strive to be comprehensible to ordinary users, allowing them to visualize the sensitivity (uncertainty, inexactness) of the results in an intuitive way. Techniques useful in visualizing sensitivity of matrix computations are described in [23] and <ref> [113] </ref>. Copyright c fl1996, 1997 D.
Reference: [114] <author> M.-C. Yeung, T.F. Chan, </author> <title> "Probabilistic Analysis of Gaussian Elimination without Pivoting", </title> <type> Technical report CAM-95-29, </type> <institution> Group in Computational and Applied Mathematics, Department of Mathematics, University of California, </institution> <address> Los Angeles, Los Angeles,CA 90024-1555, </address> <year> 1995. </year>
Reference-contexts: Statistical analyses have been performed for Gaussian elimination by various authors, including Goldstine and von Neumann [44] (nice distillations of which are [111] and [54, p.187]), Barlow and Bareiss [8], Trefethen and Schreiber [97], Yeung and Chan <ref> [114] </ref>, and Chaitin-Chatelin and Fraysse [23]. The statistical analysis of Barlow and Bareiss [8] is reproduced in Appendix D. 8.2.1 Small examples Let us begin with some nice examples of Kahan. These examples show that, at least for small linear systems, MCA can detect ill-conditioning.
Reference: [115] <author> N. Yoshida, E. Goto, S. Ichikawa, </author> <title> "Pseudorandom Rounding for Truncated Multipliers", </title> <journal> IEEE Trans. Comput. </journal> <volume> 40:9, </volume> <pages> 1065-1067, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Finally, a twist on the idea of random rounding was developed by Yoshida, Goto, and Ichikawa <ref> [115] </ref>, permitting them to implement floating-point multipliers with many fewer gates than usual. <p> They omit computation of the less significant part of the product of two floating-point numbers, instead relying on the fact that "the average of that neglected part is compensated for by using the statistical properties of medium significant bits", since they "are regarded as random in pseudorandom number generators" <ref> [115, p.1065-1066] </ref>. <p> Actually, the reverse can be argued! Though we are not endorsing their method, Yoshida, Goto, and Ichikawa <ref> [115] </ref> have shown that floating-point multipliers with random rounding can be implemented more efficiently than they can without it.
References-found: 115

