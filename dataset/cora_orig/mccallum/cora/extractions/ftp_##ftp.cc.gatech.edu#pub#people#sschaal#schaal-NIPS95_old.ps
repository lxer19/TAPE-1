URL: ftp://ftp.cc.gatech.edu/pub/people/sschaal/schaal-NIPS95_old.ps
Refering-URL: ftp://ftp.cc.gatech.edu/pub/people/sschaal/schaal-NIPS95.html
Root-URL: 
Email: sschaal@cc.gatech.edu cga@cc.gatech.edu  
Title: From Isolation to Cooperation: An Alternative View of a System of Experts  
Author: Stefan Schaal Christopher C. Atkeson 
Note: ATR Human Information Processing Research Laboratories, 2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto  
Address: 801 Atlantic Drive, Atlanta, GA 30332-0280  
Affiliation: College of Computing, Georgia Institute of Technology,  
Abstract: We introduce a constructive, incremental learning system for regression problems that models data by means of locally linear experts. In contrast to other approaches, the experts are trained independently and do not compete for data during learning. Only when a prediction for a query is required do the experts cooperate by blen ding their individual predictions. Each expert is trained by minimizing a penalized local cross validation error using second order methods. In this way, an expert is able to adjust the size and shape of the receptive field in which its predictions are valid, and also to adjust its bias on the importance of individual input dimensions. The size and shape adjustment corresponds to finding a local distance metric, while the bias adjustment accomplishes local dimensio n-ality reduction. We derive asymptotic results for our method. In a variety of simulations we demonstrate the properties of the algorithm with respect to interference, learning speed, prediction accuracy, feature detection, and task or i-ented incremental learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G., Moore, A. W., & Schaal, S. </author> <title> (submitted). "Locally weighted learning." </title> <journal> Artificial Intelligence Review. </journal>
Reference: <author> Atkeson, C. G. </author> <year> (1992). </year> <title> "Memory-based approaches to approximating continuous functions." </title> <editor> In: Casdagli, M., & Eubank, S. (Eds.), </editor> <title> Nonlinear Modeling and Forecasting, </title> <address> pp.503-521. Redwood City, CA: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: The deriva tives in batch mode can be calculated exactly due to the Sherman-Morrison-Woodburry theorem (Belsley, Kuh, & Welsch, 1980), and correspond to taking the derivatives of a locally weighted regression <ref> (Atkeson, 1992) </ref>. A new expert is initialized with a default M def and all other variables set to zero, except the matrix P. P is initialized as a diagonal matrix with elements 1 2 / r i , where the r i are usually small quantities, e.g., 0.01.
Reference: <author> Belsley, D. A., Kuh, E., & Welsch, R. E. </author> <year> (1980). </year> <title> Regression diagnostics: Identifying influential data and sources of collinearity. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The major ingredient is to take the derivatives of the cost function as if it were a batch update, and then to reformulate the result as an iterative scheme. The deriva tives in batch mode can be calculated exactly due to the Sherman-Morrison-Woodburry theorem <ref> (Belsley, Kuh, & Welsch, 1980) </ref>, and correspond to taking the derivatives of a locally weighted regression (Atkeson, 1992). A new expert is initialized with a default M def and all other variables set to zero, except the matrix P.
Reference: <author> Cleveland, W. S. </author> <year> (1979). </year> <title> "Robust locally weighted regression and smoothing scatterplots." </title> <journal> Journal of the American Statistical Association, </journal> <volume> 74, </volume> <editor> pp.829-836. de Boor, C. </editor> <year> (1978). </year> <title> A practical guide to splines. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference-contexts: The topic of data fitting with structurally simple local models (or experts) has received a great deal of attention in nonparametric statistics (e.g., Scott, 1992, Hastie & Tibshirani, 1990). Research on instance-based methods such as kernel regression (Nadaraya, 1964) or locally weighted regression <ref> (Cleveland, 1979) </ref> have been pursued for more than fifteen years.
Reference: <author> Hastie, T. J., & Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized additive models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> "Adaptive mixtures of local experts." </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <month> pp.79-87. </month>
Reference-contexts: This classifier determines which expert models are used in which part of the input space. For incremental learning, co m-petitive learning methods are usually applied. Here the experts compete for data such that they change their domains of expertise until a stable configuration is achieved <ref> (e.g., Jacobs, Jordan, Nowlan, & Hinton, 1991) </ref>. The advantage of local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness (e.g., Jordan & Jacobs, 1994).
Reference: <author> Jordan, M. I., & Jacobs, R. </author> <year> (1994). </year> <title> "Hierarchical mixtures of experts and the EM algorithm." </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <month> pp.79-87. </month>
Reference-contexts: The advantage of local experts is that they can have simple parameterizations, such as locally constant or locally linear models. This offers benefits in terms of analyzability, learning speed, and robustness <ref> (e.g., Jordan & Jacobs, 1994) </ref>. For si m-ple experts, however, a large number of experts is necessary to model a function. As a result, the expert selection system has to be more complicated and, thus, has a higher risk of getting stuck in local minima and/or of learning rather slowly.
Reference: <author> Ljung, L., & S_derstr_m, T. </author> <year> (1986). </year> <title> Theory and practice of recursive identification. </title> <publisher> Cambridge, MIT Press. </publisher>
Reference-contexts: This will be analyzed further in Section 2.2. The update equations for the linear subnet are the standard weighted recursive least squares equation with fo r-getting factor l <ref> (Ljung & Sderstrm, 1986) </ref>: b b b cv n T n cv w e e y = + = - = -( ) P x P P x P x where and (1) (3) Inputs Output Linear Unit Weighted Average Receptive Field Unit centered at c Gating Connection w x 2
Reference: <author> McLachlan, G. J., & Basford, K. E. </author> <year> (1988). </year> <title> Mixture models. </title> <address> New York: </address> <publisher> Marcel Dekker. </publisher>
Reference-contexts: In incremental learning, another potential danger arises when the input distribution of the data changes. The expert selection system usually makes either implicit or explicit prior assumptions about the input data distribution. For example, in the classical mixture model <ref> (McLachlan & Basford, 1988) </ref> which was employed in several local expert approaches, the prior probabilities of each mixture model can be interpreted as the fraction of data points each expert expects to experience.
Reference: <author> Nadaraya, E. A. </author> <year> (1964). </year> <title> "On estimating regression." </title> <journal> Theor. Prob. Appl., </journal> <volume> 9, </volume> <month> pp.141-142. </month>
Reference-contexts: The topic of data fitting with structurally simple local models (or experts) has received a great deal of attention in nonparametric statistics (e.g., Scott, 1992, Hastie & Tibshirani, 1990). Research on instance-based methods such as kernel regression <ref> (Nadaraya, 1964) </ref> or locally weighted regression (Cleveland, 1979) have been pursued for more than fifteen years.
Reference: <author> Schaal, S., & Atkeson, C. G. </author> <year> (1994b). </year> <title> "Assessing the quality of learned local models." </title> <editor> In: Cowan, J. , Tesauro, G., & Alspector, J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The topic of data fitting with structurally simple local models (or experts) has received a great deal of attention in nonparametric statistics <ref> (e.g., Scott, 1992, Hastie & Tibshirani, 1990) </ref>. Research on instance-based methods such as kernel regression (Nadaraya, 1964) or locally weighted regression (Cleveland, 1979) have been pursued for more than fifteen years.
Reference: <author> Sutton, R. S. </author> <year> (1992). </year> <title> "Gain adaptation beats least squares." </title> <booktitle> In: Proceedings of Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <address> New Haven, CT. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1990). </year> <title> "Stacked generalization." </title> <type> Los Alamos Technical Report LA-UR-90-3460 </type> . 
Reference-contexts: 1. Introduction Distributing a learning task among a set of experts has become a popular method in com putational learning. One approach is to employ several experts, each with a global domain of expertise <ref> (e.g., Wolpert, 1990) </ref>. When an output for a given input is to be predicted, every expert gives a prediction together with a confidence measure. The indivi dual predictions are combined into a single result, for instance, based on a confidence weighted average.
References-found: 14

