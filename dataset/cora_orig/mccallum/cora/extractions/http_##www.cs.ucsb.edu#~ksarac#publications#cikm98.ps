URL: http://www.cs.ucsb.edu/~ksarac/publications/cikm98.ps
Refering-URL: http://www.cs.ucsb.edu/~ksarac/
Root-URL: http://www.cs.ucsb.edu
Email: e-mail: fksarac; omer; amrg@cs.ucsb.edu  
Title: Iterated DFT Based Techniques for Join Size Estimation  
Author: Kamil Sara~c Omer Egecioglu Amr El Abbadi 
Address: Santa Barbara  
Affiliation: Department of Computer Science University of California  
Note: To appear in Conference of Information and Knowledge Management 1998 (CIKM98), Bethesda, Maryland 1  
Abstract: Novel techniques based on the Discrete Fourier Transform are proposed to estimate the size of relations resulting from join operations. For the special case of self join the proposed algorithm gives the exact join size using logarithmic space. A generalization to compute the join of arbitrary relations is then used to develop two tree-based techniques that provide a spectrum of algorithms which interpolate storage requirements versus accuracy of the estimation obtained. Finally, we present experimental results to exhibit the effectiveness of our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal R., Faloutsos C., Swami A.: </author> <title> Efficient Similarity Search In Sequence Databases. </title> <address> FODO, </address> <year> 1993. </year>
Reference-contexts: DFT has been used in various estimation problems in database research such as dimension reduction for searching as well as indexing high dimensional data <ref> [1, 14] </ref>. Consider an N -dimensional real vector X = (x 1 ; x 2 ; : : : ; x N ). <p> This is in accordance with the expectation that since TAAT ignores the nonnegative contribution of AAV , its approximation should be closer to the exact value. The algorithms TAA l are not suitable for unimodal distributions with sharp peaks. In the Zipf distribution for 1 Let <ref> [0; 1] </ref> N denote the unit cube in N-dimensional Euclidean space. This asymptotic behavior of AAV would follow from the convergence of the integral RR &lt;AAV (X);AAV (Y )&gt; &lt;X;Y &gt; dXdY ! 1:333::. 7 example, there are only a few values which are quite high relative to the rest.
Reference: [2] <author> Christodoulakis S.: </author> <title> Implications of Certain Assumptions in Database Performance Evaluation ACM Transactions on Database Systems, </title> <year> 1984. </year>
Reference-contexts: An exact expression for S is the matrix product S = T 0 T 1 : : : T n ; (1) but the storage of the frequency matrices and exact computation of this product is too costly. We follow the common assumption of attribute independence formulated in <ref> [2] </ref> and adopted by most database systems. According to this assumption, the distribution of attribute values are independent of each other.
Reference: [3] <author> Davis P. J.: </author> <title> Interpolation and Approximation, pg 192. </title> <publisher> Dover Publ. </publisher> <address> N.Y. </address> <year> 1963. </year>
Reference-contexts: It is well known that the DFT of X can be computed using the Fast Fourier Transform in O (N log N ) arithmetic operations. The fundamental properties of the DFT that we make use of are summarized below <ref> [3, 12] </ref>. <p> The most important property of the DFT we use is Parseval's identity <ref> [3] </ref>: Theorem Suppose X and Y are two N -dimensional real vectors. Then &lt; X; Y &gt;=&lt; ^ X; ^ Y &gt; where &lt; ; &gt; denotes the standard complex inner product. We use Parseval's identity in the following expanded form for real vectors X and Y .
Reference: [4] <author> Haas P. J. and Swami A. N.: </author> <title> Sampling-Based Selectivity Estimation for Joins Using Augmented Frequency Value Statistics. </title> <booktitle> The International conference on Data Engineering, </booktitle> <year> 1995. </year>
Reference-contexts: These techniques present various trade-offs in terms of storage, precision, run-time overhead, and make varying assumptions about the distribution of the underlying data. Various sampling techniques that operate at run time and compute estimates based on random samples of data have been proposed <ref> [9, 4] </ref>. Parametric techniques [15] assume mathematical distributions such as normal, uniform, Poisson, or Zipf. The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques [11, 5] divide attribute value domains into buckets and then record bucket size information as a histogram.
Reference: [5] <author> Ioannidis Y.: </author> <title> Universality of Serial Histograms. </title> <booktitle> Proceedings of the 19th Conference on Very Large Databases, </booktitle> <address> Dublin, </address> <year> 1993. </year>
Reference-contexts: Parametric techniques [15] assume mathematical distributions such as normal, uniform, Poisson, or Zipf. The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques <ref> [11, 5] </ref> divide attribute value domains into buckets and then record bucket size information as a histogram. Ioannidis and Poosala [7] used serial histograms which are constructed by grouping attribute values that have similar frequencies. The construction of serial histograms in general is computationally expensive. <p> We evaluate the performance of the algorithms in Section 6 and conclude the paper in Section 7. 2 Problem Formulation We first describe the problem of join size estimation in databases. The development and motivation closely follows that of Ioannidis and Poosala <ref> [5, 7] </ref>. Consider a database with relations R 0 ; R 1 ; : : : ; R n . Let a i , a i+1 be attributes of relation R i . <p> The first set of experiments are designed for estimating a worse case upper bound on the size of a relation resulting from a join operation. This case is referred to as the maximal result size <ref> [5] </ref>. Given two frequency vectors X and Y , of the attributes involved in a join operation, the maximal result size can be obtained by sorting the two frequency vectors and assuming that the frequency values at the corresponding entries in the sorted vectors correspond to the same attribute values.
Reference: [6] <author> Ioannidis Y., Christodoulakis S.: </author> <title> On the propagation of errors in the size of join results. </title> <booktitle> Proc. of the 1991 ACM-SIGMOD Conf. </booktitle> <address> Denver CO, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The accuracy fl This research was partially supported by the NSF/NASA/ARPA under grant number IRI94-11330. of this approximation is crucial to the performance of the database systems. A small error introduced in the execution of a portion of the query can grow exponentially and affect the overall performance considerably <ref> [6] </ref>. Several techniques (sampling based, parametric and histogram based) have been proposed in the literature for estimating query result sizes. The survey by Mannino, Chu and Sager is a good reference on the early work on database size estimation [10].
Reference: [7] <author> Ioannidis Y., Poosala V.: </author> <title> Balancing histogram Optimality and Practicality for Query Result Size Estimation. </title> <address> SIGMOD'95, San Jose, CA USA, </address> <month> June </month> <year> 1995 </year>
Reference-contexts: The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques [11, 5] divide attribute value domains into buckets and then record bucket size information as a histogram. Ioannidis and Poosala <ref> [7] </ref> used serial histograms which are constructed by grouping attribute values that have similar frequencies. The construction of serial histograms in general is computationally expensive. A practical heuristic approximation is the end-biased histogram, where most frequent values are stored exactly and the rest averaged out [7]. <p> Ioannidis and Poosala <ref> [7] </ref> used serial histograms which are constructed by grouping attribute values that have similar frequencies. The construction of serial histograms in general is computationally expensive. A practical heuristic approximation is the end-biased histogram, where most frequent values are stored exactly and the rest averaged out [7]. In this paper, we propose novel techniques based on the Discrete Fourier Transform (DFT) to estimate the size of relations resulting from join operations. <p> We evaluate the performance of the algorithms in Section 6 and conclude the paper in Section 7. 2 Problem Formulation We first describe the problem of join size estimation in databases. The development and motivation closely follows that of Ioannidis and Poosala <ref> [5, 7] </ref>. Consider a database with relations R 0 ; R 1 ; : : : ; R n . Let a i , a i+1 be attributes of relation R i . <p> For the purpose of comparison, we compare our algorithms with the end-biased v-optimal histograms (EB) of Ioannidis and Poosala <ref> [7] </ref>. This approach is practical in terms of computation time and gives very good approximations [7, 13] (unlike the serial method, which gives even better approximations, but is computation-ally impractical for large vector sizes [7]). <p> For the purpose of comparison, we compare our algorithms with the end-biased v-optimal histograms (EB) of Ioannidis and Poosala [7]. This approach is practical in terms of computation time and gives very good approximations <ref> [7, 13] </ref> (unlike the serial method, which gives even better approximations, but is computation-ally impractical for large vector sizes [7]). The first set of experiments are designed for estimating a worse case upper bound on the size of a relation resulting from a join operation. <p> of comparison, we compare our algorithms with the end-biased v-optimal histograms (EB) of Ioannidis and Poosala <ref> [7] </ref>. This approach is practical in terms of computation time and gives very good approximations [7, 13] (unlike the serial method, which gives even better approximations, but is computation-ally impractical for large vector sizes [7]). The first set of experiments are designed for estimating a worse case upper bound on the size of a relation resulting from a join operation. This case is referred to as the maximal result size [5]. <p> The experiments were run for vectors of sizes 2 k 1 for k = 5; 6; : : : ; 12, and hence the number of values stored per vector (or number of buckets in the standard terminology <ref> [7] </ref>) ranged from 5 to 12. In the figures, the x-axis refers to the number of values stored, while the y-axis refers to the relative errors as a percentage of the exact size.
Reference: [8] <author> Korn F., Jagadish H. V., Faloutsos C.: </author> <title> Efficiently Supporting Ad Hoc Queries in Large Datasets of Time Sequences. </title> <address> SIGMOD'97, AZ, USA, </address> <month> May </month> <year> 1997 </year>
Reference: [9] <author> Lipton R. J., Naughton J. F., and Schneider D. A.: </author> <title> Practical Selectivity Estimation through Adaptive Sampling. </title> <booktitle> Proceedings of ACM-SIGMOD, </booktitle> <year> 1990. </year>
Reference-contexts: These techniques present various trade-offs in terms of storage, precision, run-time overhead, and make varying assumptions about the distribution of the underlying data. Various sampling techniques that operate at run time and compute estimates based on random samples of data have been proposed <ref> [9, 4] </ref>. Parametric techniques [15] assume mathematical distributions such as normal, uniform, Poisson, or Zipf. The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques [11, 5] divide attribute value domains into buckets and then record bucket size information as a histogram.
Reference: [10] <author> Mannino M. V., Chu P., Sager T. </author> : <title> Statistical Profile Estimation in Database Systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 20(3) </volume> <pages> 192-221, </pages> <month> Sept </month> <year> 1988. </year>
Reference-contexts: Several techniques (sampling based, parametric and histogram based) have been proposed in the literature for estimating query result sizes. The survey by Mannino, Chu and Sager is a good reference on the early work on database size estimation <ref> [10] </ref>. These techniques present various trade-offs in terms of storage, precision, run-time overhead, and make varying assumptions about the distribution of the underlying data. Various sampling techniques that operate at run time and compute estimates based on random samples of data have been proposed [9, 4].
Reference: [11] <author> Muralikrishma M, and DeWitt D.: </author> <title> Equi-depth Histograms for Estimating Selectivity Factors for Multi-Dimensional Queries. </title> <booktitle> Proceedings of the ACM-SIGMOD Conference on Management of Data, </booktitle> <year> 1988. </year>
Reference-contexts: Parametric techniques [15] assume mathematical distributions such as normal, uniform, Poisson, or Zipf. The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques <ref> [11, 5] </ref> divide attribute value domains into buckets and then record bucket size information as a histogram. Ioannidis and Poosala [7] used serial histograms which are constructed by grouping attribute values that have similar frequencies. The construction of serial histograms in general is computationally expensive.
Reference: [12] <author> Oppenheim A. V., Schafer R. W.: </author> <title> Discrete-time Signal Processing. </title> <publisher> Prentice Hall Signal Processing Series, </publisher> <year> 1989. </year>
Reference-contexts: It is well known that the DFT of X can be computed using the Fast Fourier Transform in O (N log N ) arithmetic operations. The fundamental properties of the DFT that we make use of are summarized below <ref> [3, 12] </ref>.
Reference: [13] <author> Poosala V.: </author> <title> Histogram-based Estimation Techniques in Database Systems. </title> <type> Ph.D. Thesis, </type> <institution> University of Wiscon-sin Madison, </institution> <year> 1997. </year>
Reference-contexts: For the purpose of comparison, we compare our algorithms with the end-biased v-optimal histograms (EB) of Ioannidis and Poosala [7]. This approach is practical in terms of computation time and gives very good approximations <ref> [7, 13] </ref> (unlike the serial method, which gives even better approximations, but is computation-ally impractical for large vector sizes [7]). The first set of experiments are designed for estimating a worse case upper bound on the size of a relation resulting from a join operation.
Reference: [14] <author> Rafiei D., Mendelzon A.: </author> <title> Similarity-Based Queries for Time Series Data. </title> <booktitle> Proceedings of the ACM-SIGMOD Conference on Management of Data, </booktitle> <year> 1997. </year>
Reference-contexts: DFT has been used in various estimation problems in database research such as dimension reduction for searching as well as indexing high dimensional data <ref> [1, 14] </ref>. Consider an N -dimensional real vector X = (x 1 ; x 2 ; : : : ; x N ).
Reference: [15] <author> Selinger P., Astrahan M., Chamberlin D., Lorie R., Price T.: </author> <title> Access Path Selection in a Relational Database Management System. </title> <booktitle> Proceedings of the ACM-SIGMOD Conference on Management of Data, </booktitle> <year> 1979. </year>
Reference-contexts: These techniques present various trade-offs in terms of storage, precision, run-time overhead, and make varying assumptions about the distribution of the underlying data. Various sampling techniques that operate at run time and compute estimates based on random samples of data have been proposed [9, 4]. Parametric techniques <ref> [15] </ref> assume mathematical distributions such as normal, uniform, Poisson, or Zipf. The underlying assumption is that the data follows the characteristics of the used parametric distribution. Histogram based techniques [11, 5] divide attribute value domains into buckets and then record bucket size information as a histogram.
References-found: 15

