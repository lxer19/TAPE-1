URL: http://www.ri.cmu.edu/afs/cs/academic/class/15882-s98/www/hinton-wake-sleep.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs/academic/class/15882-s98/www/syllabus.html
Root-URL: 
Title: The wake-sleep algorithm for unsupervised neural networks  
Author: Geoffrey E Hinton Peter Dayan Brendan J Frey Radford M Neal 
Date: 27 th November 1994  
Address: 6 King's College Road Toronto M5S 1A4, Canada  
Affiliation: Department of Computer Science University of Toronto  
Abstract: We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons. Bottom-up recognition connections convert the input into representations in successive hidden layers and top-down generative connections reconstruct the representation in one layer from the representation in the layer above. In the wake phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the sleep phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections. The wake-sleep algorithm finesses both these problems. When there is no teaching signal to be matched, some other goal is required to force the hidden units to extract underlying structure. In the wake-sleep algorithm the goal is to learn representations that are economical to describe but allow the input to be reconstructed accurately. Each input vector could be communicated to a receiver by first sending its hidden representation and then sending the difference between the input vector and its top-down reconstruction from the hidden representation. The aim of learning is to minimize the description length which is the total number of bits that would be required to communicate the input vectors in this way [1]. No communication actually takes place, but minimizing the description length that would be required forces the network to learn economical representations that capture the underlying regularities in the data [2]. the correct activity vector in the layer above.
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> J Rissanen, </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <publisher> (World Scientific, </publisher> <address> Singa pore, </address> <year> 1989). </year>
Reference: [2] <institution> The description length can be viewed as an upper bound on the negative log probability of the data given the network's generative model, </institution> <note> so this approach is closely related to maximum likelihood methods of fitting models to data. </note>
Reference: [3] <institution> The number of bits required to specify the generative weights should also be included in the description length [1], </institution> <type> but we currently ignore it. </type>
Reference: [4] <author> GE Hinton and RS Zemel, </author> <booktitle> in Advances in Neural Information Processing Systems 6., </booktitle> <editor> JD Cowan, G Tesauro and J Alspector, editors, </editor> <publisher> (Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1994), </year> <pages> 3-10. </pages>
Reference: [5] <editor> An unbiased estimate of the exact gradient is easy to obtain, </editor> <title> but the noise in this estimate increases with the size of the network. Alternatively, a mean-field approximation can be made to the stochastic recognition model and the error derivatives can then be computed by a backpropagation process (P Dayan, GE Hinton, RM Neal and RS Zemel, Neural Computation, </title> <note> submitted). </note>
Reference: [6] <institution> This performs stochastic steepest descent in the Kullback-Leibler divergences X X p j log fl fl fl j ) log fl fl The cost function in equation 5 contains the same terms but with p and q interchanged leading to an approximation error equal to the asymmetry of the Kullback-Leibler divergences. </institution>

Reference: [8] <author> G Carpenter and S Grossberg, </author> <title> Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 37, </volume> <month> 54 </month> <year> (1987). </year>
Reference: [9] <author> S Ullman, </author> <title> in Large-Scale Theories of the Cortex, </title> <editor> C Koch and J Davis, editors, </editor> <publisher> (MIT Press: </publisher> <address> Cambridge, MA, </address> <year> 1994). </year>
Reference: [10] <author> M Kawato, H Hayakama and T Inui, </author> <title> Network, </title> <type> 4, 415, </type> <year> 1993. </year>
Reference: [11] <author> D Mumford, </author> <title> in Large-Scale Theories of the Cortex, </title> <editor> C Koch and J Davis, editors, </editor> <publisher> (MIT Press: </publisher> <address> Cambridge, MA, </address> <year> 1994). </year>
Reference: [12] <author> MI Jordan and DE Rumelhart, </author> <booktitle> Cognitive Science, </booktitle> <volume> 16, 307, </volume> <year> 1992. </year>
Reference: [13] <author> HB Barlow, </author> <booktitle> Neural Computation, </booktitle> <volume> 1, 295, </volume> <year> 1989. </year>
Reference: [14] <author> U Grenander, </author> <title> Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern Synthesis and Regular Structures, </title> <publisher> (Springer-Verlag, Berlin, </publisher> <pages> 1976-1981). </pages>
Reference: [15] <editor> The learning rates were 0:2 for the generative and recognition weights to and from the input units and 0:001 for the other weights. </editor> <title> The generative biases of the first hidden layer started at 3:00 and all other weights started at 0.0. The final asymmetric divergence between the network's generative model and the real data was 0:10 bits. The penalty term in Eq. </title> <note> 8 was 0:08 bits. </note>
Reference: [16] <institution> The training data was 700 examples of each digit from the CEDAR CDROM 1 made available by the US Postal Service Office of Advanced Technology. </institution> <note> Starting with the input layer, </note> <editor> each network had a 64-16-16-4 architecture. </editor> <title> All weights were started at 0 and the learning rate on all connections was 0:01. Training involved 500 sweeps through the 700 examples. For testing, each net was run 10 times to estimate the expected description length of the image. </title>

References-found: 15

