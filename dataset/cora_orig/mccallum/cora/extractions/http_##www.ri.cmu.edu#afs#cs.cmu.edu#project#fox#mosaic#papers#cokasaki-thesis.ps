URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/fox/mosaic/papers/cokasaki-thesis.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/fox/mosaic/papers.html
Root-URL: 
Title: Purely Functional Data Structures  
Author: Chris Okasaki Daniel Sleator Robert Tarjan, 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee: Peter Lee, Chair Robert Harper  
Note: Princeton University Copyright c 1996 Chris Okasaki This research was sponsored by the Advanced Research Projects Agency (ARPA) under Contract No. F19628-95-C-0050. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of ARPA or the U.S. Government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: September 1996  
Pubnum: CMU-CS-96-177  
Abstract-found: 0
Intro-found: 1
Reference: [Ada93] <author> Stephen Adams. </author> <title> Efficient setsa balancing act. Journal of Functional Program ming, </title> <address> 3(4):553561, </address> <month> October </month> <year> 1993. </year> <note> (p. 17) </note>
Reference-contexts: A cleaner way to write these functions is to consolidate the invariant-maintenance duties of snoc and tail into a single pseudo-constructor. Pseudo-constructors, sometimes called smart constructors <ref> [Ada93] </ref>, are functions that replace ordinary constructors in the construction of data, but that check and enforce an invariant.
Reference: [AFM + 95] <author> Zena M. Ariola, Matthias Felleisen, John Maraist, Martin Odersky, and Philip Wadler. </author> <title> A call-by-need lambda calculus. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 233246, </pages> <month> January </month> <year> 1995. </year> <note> (p. 10) </note>
Reference-contexts: Vuillemin [Vui74] later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy. The formal semantics of lazy evaluation has been studied extensively <ref> [Jos89, Lau93, OLT94, AFM + 95] </ref>. 2.2 Historical Notes 11 signature STREAM = sig datatype ff StreamCell = Nil j Cons of ff fi ff Stream withtype ff Stream = ff StreamCell susp val ++ : ff Stream fi ff Stream ! ff Stream (fl stream append fl) val take :
Reference: [AVL62] <author> G. M. Adel'son-Vel'skiff and E. M. Landis. </author> <title> An algorithm for the organization of information. </title> <journal> Soviet MathematicsDoklady, </journal> <volume> 3(5):12591263, </volume> <month> September </month> <year> 1962. </year> <journal> English translation of Russian orginal appearing in Doklady Akademia Nauk SSSR, </journal> <volume> 146 </volume> <pages> 263-266. </pages> <address> (p. </address> <month> 49) </month>
Reference-contexts: However, since it is usually too expensive to restore perfect balance after every update, most implementations settle for approximations of perfect balance that are at most a constant factor slower. Examples of this approach include AVL trees <ref> [AVL62] </ref> and red-black trees [GS78]. However, provided no update disturbs the balance too drastically, an attractive alternative is to postpone rebalancing until after a sequence of updates, and then to rebalance the entire structure, restoring it to perfect balance. We call this approach batched rebuilding.
Reference: [Bac78] <author> John Backus. </author> <title> Can programming be liberated from the von Neumann style? A functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8):613641, </volume> <month> August </month> <year> 1978. </year> <note> (p. 1) </note>
Reference-contexts: This thesis addresses this imbalance by specifically considering the design and analysis of functional data structures. 1.1 Functional vs. Imperative Data Structures The methodological benefits of functional languages are well known <ref> [Bac78, Hug89, HJ94] </ref>, but still the vast majority of programs are written in imperative languages such as C. This apparent contradiction is easily explained by the fact that functional languages have historically been slower than their more traditional cousins, but this gap is narrowing.
Reference: [BAG92] <author> Amir M. Ben-Amram and Zvi Galil. </author> <title> On pointers versus addresses. </title> <journal> Journal of the ACM, </journal> <volume> 39(3):617648, </volume> <month> July </month> <year> 1992. </year> <note> (p. 2) </note>
Reference-contexts: Furthermore, theoreticians have established lower bounds suggesting that functional programming languages may be fundamentally less efficient than imperative languages in some situations <ref> [BAG92, Pip96] </ref>. In spite of all these points, this thesis shows that it is often possible to devise functional data structures that are asymptotically as efficient as the best imperative solutions. 1.2 Strict vs.
Reference: [BC93] <author> F. Warren Burton and Robert D. Cameron. </author> <title> Pattern matching with abstract data types. </title> <journal> Journal of Functional Programming, </journal> <volume> 3(2):171190, </volume> <month> April </month> <year> 1993. </year> <note> (p. 129) </note>
Reference-contexts: The seductive allure of pattern matching leads many functional programmers to abandon sophisticated data structures in favor of simple, known representations such as lists, even when doing so causes an otherwise linear algorithm to explode to quadratic or even exponential time. Views [Wad87] and their successors <ref> [BC93, PPN96] </ref> offer one way of reconciling the convenience of pattern matching with the desirability of data abstraction. In fact, $-patterns are just a special case of views.
Reference: [Bel57] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year> <note> (p. 12) </note>
Reference-contexts: The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues [ST86b, FT87], and search trees [DSST89]. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming <ref> [Bel57] </ref> and path compression [HU73, TvL84]. Syntax for Lazy Evaluation Early versions of CAML [W + 90], a close cousin of Standard ML, offered support for lazy evaluation similar to the $-notation proposed here.
Reference: [BH89] <author> Bror Bjerner and Soren Holmstrom. </author> <title> A compositional approach to time analysis of first order lazy functional programs. </title> <booktitle> In Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 157165, </pages> <month> September </month> <year> 1989. </year> <note> (p. 38) </note>
Reference-contexts: In fact, a better name for these debits might be anticredits. 38 Amortization and Persistence via Lazy Evaluation Time-Analysis of Lazy Programs Several researchers have developed theoretical frameworks for analyzing the time complexity of lazy programs <ref> [BH89, San90, San95, Wad88] </ref>. However, these frameworks are not yet mature enough to be useful in practice. One difficulty is that these frameworks are, in some ways, too general.
Reference: [BJdM96] <author> Richard S. Bird, Geraint Jones, and Oege de Moor. </author> <title> A lazy pure language versus impure Lisp. </title> <note> http://www.comlab.ox.ac.uk/oucl/users/ geraint.jones/publications/FP-1-96.html, 1996. (p. 128) 138 BIBLIOGRAPHY </note>
Reference-contexts: However, when persistence is used heavily, memoization more than pays for itself and our implementations fly. In a followup to [Pip96], Bird, Jones, and de Moor <ref> [BJdM96] </ref> have recently exhibited a problem for which a lazy solution exists that is asymptotically superior to any possible strict solution. However, this result depends on several extremely restrictive assumptions. Our work suggests a promising approach towards removing these restrictions.
Reference: [Blu86] <author> Norbert Blum. </author> <title> On the single-operation worst-case time complexity of the disjoint set union problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(4):10211024, </volume> <month> November </month> <year> 1986. </year> <note> (p. 14) </note>
Reference-contexts: In fact, for some problems, such as the union-find problem [TvL84], there are amortized solutions that are asymptotically faster than any possible worst-case solution (assuming certain modest restrictions) <ref> [Blu86] </ref>.
Reference: [BO96] <author> Gerth Stlting Brodal and Chris Okasaki. </author> <title> Optimal purely functional priority queues. </title> <journal> Journal of Functional Programming, </journal> <volume> 6(6), </volume> <month> December </month> <year> 1996. </year> <note> To appear. (pp. 82, 84, 103, 129) </note>
Reference-contexts: This behavior is also possible using the catenable lists of Hughes [Hug86], which are the functional counterpart of difference lists. 1 As another example, Brodal and Okasaki <ref> [BO96] </ref> support a delete function on heaps using two primitive heaps, one containing positive elements and one containing negative elements. The negative elements are ones that have been deleted, but that have not yet been physically removed from the positive heap. <p> Varying the choices for each D i allows a tradeoff between the costs of insert and meld , and the cost of deleteMin . Skew binomial heaps were originally presented, in a slightly different form, in <ref> [BO96] </ref>. Chapter 7 Data-Structural Bootstrapping The term bootstrapping refers to pulling yourself up by your bootstraps. This seemingly nonsensical image is representative of a common situation in computer science: problems whose solutions require solutions to (simpler) instances of the same problem. <p> However, such an implementation would be both complicated and slow. Brodal and Okasaki simplify this implementation in <ref> [BO96] </ref>, using skew binomial heaps (Section 6.4.2) and structural abstraction (Section 7.2.2). Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as [Myc84, Hen93, KTU93]. <p> All in all, we believe the compromise taken by Haskell 1.3 [P + 96] is best: allow polymorphic recursion in those cases where the programmer explicitly provides a type signature, and disallow it everywhere else. Higher-order, Recursive Modules The bootstrapped heaps of Section 7.2.2 (see also <ref> [BO96] </ref>) demonstrate the usefulness of higher-order, recursive modules. In languages such as Standard ML that do not support higher-order, recursive modules, we can often sidestep this restriction by manually inlining the desired definitions for each instance of bootstrapping.
Reference: [Bro78] <author> Mark R. Brown. </author> <title> Implementation and analysis of binomial queue algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 7(3):298319, </volume> <month> August </month> <year> 1978. </year> <pages> (pp. 64, 68, 73, 84) </pages>
Reference-contexts: Assuming that elements are not rearranged, each of the three kinds of trees can be linked or unlinked in O (1) time. We next describe two existing data structures in terms of this framework: the one-sided flexible arrays of Kaldewaij and Dielissen [KD96], and the binomial queues of Vuillemin <ref> [Vui78, Bro78] </ref>. 6.2 Binary Representations 65 @ @ @ A A A A C C C C C C C C fl fl fl fl fl fl fl fl (a) s s s s s A A A C C C C fl fl fl fl (c) pennant. <p> O (log n) worst-case time. lookup and update take at most O (log n) time to find the right tree, and then at most O (log n) time to find the right element in that tree, for a total of O (log n) worst-case time. 6.2.2 Binomial Heaps Binomial queues <ref> [Vui78, Bro78] </ref> are a classical implementation of mergeable priority queues. To avoid confusion with FIFO queues, we will henceforth refer to priority queues as heaps and binomial queues as binomial heaps. <p> Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown <ref> [Bro78] </ref>. King [Kin94] showed that binomial heaps could be implemented elegantly in a purely functional language (in his case, Haskell).
Reference: [Bro95] <author> Gerth Stlting Brodal. </author> <title> Fast meldable priority queues. </title> <booktitle> In Workshop on Algorithms and Data Structures, volume 955 of LNCS, </booktitle> <pages> pages 282290. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year> <pages> (pp. 103, 124) </pages>
Reference-contexts: However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal <ref> [Bro95, Bro96] </ref> achieves equivalent worst-case bounds. His original data structure [Bro95] can be implemented purely functionally (and thus made persistent) by combining the recursive-slowdown technique of Kaplan and Tarjan [KT95] with a purely functional implementation of real-time deques, such as the real-time deques of Section 5.4.3. <p> However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal [Bro95, Bro96] achieves equivalent worst-case bounds. His original data structure <ref> [Bro95] </ref> can be implemented purely functionally (and thus made persistent) by combining the recursive-slowdown technique of Kaplan and Tarjan [KT95] with a purely functional implementation of real-time deques, such as the real-time deques of Section 5.4.3. However, such an implementation would be both complicated and slow. <p> Brodal <ref> [Bro95] </ref> used a similar technique to implement heaps. 8.6 Related Work 125 Implicit Recursive Slowdown and Binomial Heaps Lazy implementations of binomial heaps [Kin94, Oka96b] can be viewed as using implicit recursive slowdown.
Reference: [Bro96] <author> Gerth Stlting Brodal. </author> <title> Worst-case priority queues. </title> <booktitle> In ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 5258, </pages> <month> January </month> <year> 1996. </year> <note> (p. 103) </note>
Reference-contexts: However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal <ref> [Bro95, Bro96] </ref> achieves equivalent worst-case bounds. His original data structure [Bro95] can be implemented purely functionally (and thus made persistent) by combining the recursive-slowdown technique of Kaplan and Tarjan [KT95] with a purely functional implementation of real-time deques, such as the real-time deques of Section 5.4.3.
Reference: [BST95] <author> Adam L. Buchsbaum, Rajamani Sundar, and Robert E. Tarjan. </author> <title> Data-structural bootstrapping, linear path compression, and catenable heap-ordered double-ended queues. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(6):11901206, </volume> <month> December </month> <year> 1995. </year> <note> (p. 101) </note>
Reference-contexts: The recursion on structures then reduces to recursion on datatypes, which is supported by Standard ML. 3 7.3 Related Work Data-Structural Bootstrapping Buchsbaum et al. identified data-structural bootstrapping as a general data structure design technique in <ref> [Buc93, BT95, BST95] </ref>. Structural decomposition and structural abstraction had previously been used in [Die82] and [DST94], respectively.
Reference: [BT95] <author> Adam L. Buchsbaum and Robert E. Tarjan. </author> <title> Confluently persistent deques via data structural bootstrapping. </title> <journal> Journal of Algorithms, </journal> <volume> 18(3):513547, </volume> <month> May </month> <year> 1995. </year> <pages> (pp. 58, 101, 125) </pages>
Reference-contexts: The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques <ref> [BT95] </ref>, and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> The recursion on structures then reduces to recursion on datatypes, which is supported by Standard ML. 3 7.3 Related Work Data-Structural Bootstrapping Buchsbaum et al. identified data-structural bootstrapping as a general data structure design technique in <ref> [Buc93, BT95, BST95] </ref>. Structural decomposition and structural abstraction had previously been used in [Die82] and [DST94], respectively. <p> The resulting implementation supports catenation in O (log log k) worst-case time, where k is the number of list operations (note that k may be much smaller than n), and all other functions in O (1) worst-case time. Buchsbaum and Tarjan <ref> [BT95] </ref> use structural decomposition to recursively decompose catenable deques of size n into catenable deques of size O (log n). <p> Such implementations support insert in O (1) amortized time and all other operations in O (log n) amortized time. [Oka96b] extends a lazy implementation of binomial heaps with scheduling to improve these bounds to worst-case. Catenable Deques Buchsbaum and Tarjan <ref> [BT95] </ref> presented a purely functional implementation of catenable deques that supports tail and init in O (log fl n) worst-case time and all other operations in O (1) worst-case time. Our implementation improves that bound to O (1) for all operations, although in the amortized rather than worst-case sense.
Reference: [Buc93] <author> Adam L. Buchsbaum. </author> <title> Data-structural bootstrapping and catenable deques. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> June </month> <year> 1993. </year> <pages> (pp. 6, 85, 101) </pages>
Reference-contexts: Chapter 6 explores numerical representations, implementations designed in analogy to representations of numbers (typically binary numbers). In this model, designing efficient insertion and deletion routines corresponds to choosing variants of binary numbers in which adding or subtracting one take constant time. Chapter 7 examines data-structural bootstrapping <ref> [Buc93] </ref>. Data-structural bootstrapping comes in two flavors: structural decomposition, in which unbounded solutions are bootstrapped from bounded solutions, and structural abstraction, in which efficient solutions are bootstrapped from inefficient solutions. Chapter 8 describes implicit recursive slowdown, a lazy variant of the recursive-slowdown technique of Kaplan and Tarjan [KT95]. <p> Then, using the interpreter, you can execute the compiler on itself, thereby obtaining an efficient, compiled executable for the compiler. This can be viewed as an instance of bootstrapping an efficient solution from an inefficient solution. In his thesis <ref> [Buc93] </ref>, Adam Buchsbaum describes two algorithmic design techniques he collectively calls data-structural bootstrapping. The first technique, structural decomposition, involves bootstrapping complete data structures from incomplete data structures. The second technique, structural abstraction, involves bootstrapping efficient data structures from inefficient data structures. <p> The recursion on structures then reduces to recursion on datatypes, which is supported by Standard ML. 3 7.3 Related Work Data-Structural Bootstrapping Buchsbaum et al. identified data-structural bootstrapping as a general data structure design technique in <ref> [Buc93, BT95, BST95] </ref>. Structural decomposition and structural abstraction had previously been used in [Die82] and [DST94], respectively.
Reference: [Bur82] <author> F. Warren Burton. </author> <title> An efficient functional implementation of FIFO queues. </title> <journal> Infor mation Processing Letters, </journal> <volume> 14(5):205206, </volume> <month> July </month> <year> 1982. </year> <pages> (pp. 16, 18, 37) </pages>
Reference-contexts: : ff Queue ! ff (fl raises EMPTY if queue is empty fl) val tail : ff Queue ! ff Queue (fl raises EMPTY if queue is empty fl) end (Etymological note: snoc is cons spelled backward and means cons on the right.) A common representation for purely functional queues <ref> [Gri81, HM81, Bur82] </ref> is as a pair of lists, F and R, where F contains the front elements of the queue in the correct order and R contains the rear elements of the queue in reverse order. <p> He avoids the problems of persistence by insisting that data structures only be used in a single-threaded fashion. Queues Gries [Gri81, pages 250251] and Hood and Melville [HM81] first proposed the queues in Section 3.1.1. Burton <ref> [Bur82] </ref> proposed a similar implementation, but without the restriction that the front list be non-empty whenever the queue is non-empty. (Burton combines head and tail into a single operation, and so does not require this restriction to support head efficiently.) The queues in Section 3.4.2 first appeared in [Oka96b]. 2 There
Reference: [But83] <author> T. W. Butler. </author> <title> Computer response time and user performance. </title> <booktitle> In Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 5862, </pages> <month> December </month> <year> 1983. </year> <note> (p. 39) </note>
Reference-contexts: systems: If one processor in a synchronous system executes an expensive oper ation while the other processors execute cheap operations, then the other processors may sit idle until the slow processor finishes. * Interactive systems: Interactive systems are similar to real-time systems users often value consistency more than raw speed <ref> [But83] </ref>. For instance, users might prefer 100 1-second response times to 99 0.25-second response times and 1 25-second response time, even though the latter scenario is twice as fast. Remark: Raman also identified a fourth application area persistent data structures.
Reference: [CG93] <author> Tyng-Ruey Chuang and Benjamin Goldberg. </author> <title> Real-time deques, multihead Tur ing machines, and purely functional programming. </title> <booktitle> In Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 289298, </pages> <month> June </month> <year> 1993. </year> <pages> (pp. 55, 58) </pages>
Reference-contexts: swapping the roles of F and R. fun reverse (Queue fF = f , LenF = lenF , R = r , LenR = lenRg) = Queue fF = r , LenF = lenR, R = f , LenR = lenF g Many other implementations of deques share this property <ref> [Hoo92b, CG93] </ref>. Rather than essentially duplicating the code for the functions on the front element and the functions on the rear element, we could define the functions on the rear element in terms of reverse and the corresponding functions on the front element. <p> The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques <ref> [Hoo82, GT86, Sar86, CG93] </ref>, catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work <ref> [GT86, Sar86, CG93] </ref>. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently.
Reference: [CMP88] <author> Svante Carlsson, J. Ian Munro, and Patricio V. Poblete. </author> <title> An implicit binomial queue with constant insertion time. </title> <booktitle> In Scandinavian Workshop on Algorithm Theory, volume 318 of LNCS, </booktitle> <pages> pages 113. </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1988. </year> <pages> (pp. 48, 82) BIBLIOGRAPHY 139 </pages>
Reference-contexts: Others have used ad hoc techniques similar to scheduling to eliminate amortization from specific data structures such as relaxed heaps [DGST88] and implicit binomial queues <ref> [CMP88] </ref>. The form of scheduling described here was first applied to queues in [Oka95c] and later generalized in [Oka96b]. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. <p> For instance, might a numerical representation based on floating point numbers be useful in approximation algorithms? 6.6 Related Work Numerical Representations Data structures that can be cast as numerical representations are surprisingly common, but only rarely is the connection to a variant number system noted explicitly <ref> [GMPR77, Mye83, CMP88, KT96b] </ref>. 1 Thanks to Phil Wadler for this observation. 6.6 Related Work 83 functor SkewBinomialHeap (structure E : ORDERED) : HEAP = struct structure Elem = E datatype Tree = Node of int fi Elem.T fi Elem.T list fi Tree list type Heap = Tree list exception EMPTY
Reference: [DGST88] <author> James R. Driscoll, Harold N. Gabow, Ruth Shrairman, and Robert E. Tarjan. </author> <title> Relaxed heaps: An alternative to Fibonacci heaps with applications to parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 31(11):13431354, </volume> <month> November </month> <year> 1988. </year> <pages> (pp. 48, 103) </pages>
Reference-contexts: Others have used ad hoc techniques similar to scheduling to eliminate amortization from specific data structures such as relaxed heaps <ref> [DGST88] </ref> and implicit binomial queues [CMP88]. The form of scheduling described here was first applied to queues in [Oka95c] and later generalized in [Oka96b]. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. <p> It is much simpler than Kaplan and Tarjan's, but yields amortized bounds rather than worst-case bounds. Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues [KL93], Fibonacci heaps [FT87], relaxed heaps <ref> [DGST88] </ref>, V-heaps [Pet87], bottom-up skew heaps [ST86b], and pairing heaps [FSST86]. However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved.
Reference: [Die82] <author> Paul F. Dietz. </author> <title> Maintaining order in a linked list. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 122127, </pages> <month> May </month> <year> 1982. </year> <note> (p. 101) </note>
Reference-contexts: The recursion on structures then reduces to recursion on datatypes, which is supported by Standard ML. 3 7.3 Related Work Data-Structural Bootstrapping Buchsbaum et al. identified data-structural bootstrapping as a general data structure design technique in [Buc93, BT95, BST95]. Structural decomposition and structural abstraction had previously been used in <ref> [Die82] </ref> and [DST94], respectively. Catenable Lists Although it is relatively easy to design alternative representations of persistent lists that support efficient catenation (see, for example, [Hug86]), such alternative representations seem almost inevitably to sacrifice efficiency on the head and/or tail functions.
Reference: [Die89] <author> Paul F. Dietz. </author> <title> Fully persistent arrays. </title> <booktitle> In Workshop on Algorithms and Data Struc tures, volume 382 of LNCS, </booktitle> <pages> pages 6774. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1989. </year> <pages> (pp. 37, 128) </pages>
Reference-contexts: Amortization and Persistence Until this work, amortization and persistence were thought to be incompatible. Several researchers [DST94, Ram92] had noted that amortized data structures could not be made efficiently persistent using existing techniques for adding persistence to ephemeral data structures, such as <ref> [DSST89, Die89] </ref>, for reasons similar to those cited in Section 3.2. Ironically, these techniques produce persistent data structures with amortized bounds, but the underlying data structure must be worst-case. (These techniques have other limitations as well. <p> It is trivial to implement most functional data structures in an imperative language such as C, and such implementations suffer few of the complications and overheads associated with other methods for implementing persistent data structures, such as [DSST89] or <ref> [Die89] </ref>. Furthermore, unlike these other methods, functional programming has no problems with data structures that support combining functions such as list catenation.
Reference: [DR91] <author> Paul F. Dietz and Rajeev Raman. </author> <title> Persistence, amortization and randomization. </title> <booktitle> In ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 7888, </pages> <month> January </month> <year> 1991. </year> <note> (p. 48) </note>
Reference-contexts: Note the similarity to the potential function from the physicist's analysis in Section 3.5.2. Since this total is bounded by 2n, the collection as a whole contains only O (n) unevaluated suspensions, and therefore sort takes only O (n) worst-case time. 4.4 Related Work Eliminating Amortization Dietz and Raman <ref> [DR91, DR93, Ram92] </ref> have devised a framework for eliminating amortization based on pebble games, where the derived worst-case algorithms correspond to winning strategies in some game.
Reference: [DR93] <author> Paul F. Dietz and Rajeev Raman. </author> <title> Persistence, randomization and parallelization: On some combinatorial games and their applications. </title> <booktitle> In Workshop on Algorithms and Data Structures, volume 709 of LNCS, </booktitle> <pages> pages 289301. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> (p. 48) </note>
Reference-contexts: Note the similarity to the potential function from the physicist's analysis in Section 3.5.2. Since this total is bounded by 2n, the collection as a whole contains only O (n) unevaluated suspensions, and therefore sort takes only O (n) worst-case time. 4.4 Related Work Eliminating Amortization Dietz and Raman <ref> [DR91, DR93, Ram92] </ref> have devised a framework for eliminating amortization based on pebble games, where the derived worst-case algorithms correspond to winning strategies in some game.
Reference: [DS87] <author> Paul F. Dietz and Daniel D. Sleator. </author> <title> Two algorithms for maintaining order in a list. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 365372, </pages> <month> May </month> <year> 1987. </year> <note> (p. 58) </note>
Reference-contexts: The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques [BT95], and the order maintenance problem <ref> [DS87] </ref>. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81].
Reference: [DSST89] <author> James R. Driscoll, Neil Sarnak, Daniel D. K. Sleator, and Robert E. Tarjan. </author> <title> Mak ing data structures persistent. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38(1):86 124, </volume> <month> February </month> <year> 1989. </year> <journal> (pp. </journal> <volume> 2, 12, 20, 37, </volume> <pages> 128) </pages>
Reference-contexts: A data structure that supports multiple versions is called persistent while a data structure that allows only a single version at a time is called ephemeral <ref> [DSST89] </ref>. Functional programming languages have the curious property that all data structures are automatically persistent. <p> The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues [ST86b, FT87], and search trees <ref> [DSST89] </ref>. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression [HU73, TvL84]. Syntax for Lazy Evaluation Early versions of CAML [W + 90], a close cousin of Standard ML, offered support for lazy evaluation similar to the $-notation proposed here. <p> We will sometimes refer to the logical history or logical future of an object, meaning the logical history or logical future of the operation that created the object. Execution traces generalize the notion of version graphs <ref> [DSST89] </ref>, which are often used to model the histories of persistent data structures. In a version graph, nodes represent the various versions of a single persistent identity and edges represent dependencies between versions. Thus, version graphs model the results of operations and execution traces model the operations themselves. <p> Amortization and Persistence Until this work, amortization and persistence were thought to be incompatible. Several researchers [DST94, Ram92] had noted that amortized data structures could not be made efficiently persistent using existing techniques for adding persistence to ephemeral data structures, such as <ref> [DSST89, Die89] </ref>, for reasons similar to those cited in Section 3.2. Ironically, these techniques produce persistent data structures with amortized bounds, but the underlying data structure must be worst-case. (These techniques have other limitations as well. <p> It is trivial to implement most functional data structures in an imperative language such as C, and such implementations suffer few of the complications and overheads associated with other methods for implementing persistent data structures, such as <ref> [DSST89] </ref> or [Die89]. Furthermore, unlike these other methods, functional programming has no problems with data structures that support combining functions such as list catenation.
Reference: [DST94] <author> James R. Driscoll, Daniel D. K. Sleator, and Robert E. Tarjan. </author> <title> Fully persistent lists with catenation. </title> <journal> Journal of the ACM, </journal> <volume> 41(5):943959, </volume> <month> September </month> <year> 1994. </year> <pages> (pp. 4, 37, 101) </pages>
Reference-contexts: Until this research, it was widely believed that amortization was incompatible with persistence <ref> [DST94, Ram92] </ref>. However, we show that memoization, in the form of lazy evaluation, is the key to reconciling the two. <p> It is interesting that debits arise in Tarjan's analysis of path compression since path compression is essentially an application of memoization to the find function. Amortization and Persistence Until this work, amortization and persistence were thought to be incompatible. Several researchers <ref> [DST94, Ram92] </ref> had noted that amortized data structures could not be made efficiently persistent using existing techniques for adding persistence to ephemeral data structures, such as [DSST89, Die89], for reasons similar to those cited in Section 3.2. <p> Structural decomposition and structural abstraction had previously been used in [Die82] and <ref> [DST94] </ref>, respectively. Catenable Lists Although it is relatively easy to design alternative representations of persistent lists that support efficient catenation (see, for example, [Hug86]), such alternative representations seem almost inevitably to sacrifice efficiency on the head and/or tail functions. <p> Myers [Mye84] described a representation based on AVL trees that supports all relevant list functions in O (log n) time. Driscoll, Sleator, and Tarjan achieved the first sub-logarithmic implementation in <ref> [DST94] </ref>. They represent catenable lists as n-ary trees with the elements at the leaves. To keep the leftmost leaves near the root, they use a restructuring operation known as pull that removes the first grandchild of the root and reattaches it directly to the root.
Reference: [Fag96] <author> Rolf Fagerberg. </author> <title> A generalization of binomial queues. </title> <journal> Information Processing Letters, </journal> <note> 57(2):109114, January 1996. (p. 84) </note>
Reference-contexts: Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown [Bro78]. King [Kin94] showed that binomial heaps could be implemented elegantly in a purely functional language (in his case, Haskell). Fagerberg <ref> [Fag96] </ref> describes a generalization of binomial heaps in which the set D i of allowable digits at position i in a sequence of digits can be different for each i.
Reference: [FMR72] <author> Patrick C. Fischer, Albert R. Meyer, and Arnold L. Rosenberg. </author> <title> Real-time simula tion of multihead tape units. </title> <journal> Journal of the ACM, </journal> <volume> 19(4):590607, </volume> <month> October </month> <year> 1972. </year> <note> (p. 58) </note>
Reference-contexts: Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines <ref> [Sto70, FMR72, LS81] </ref>. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently. The real-time deques in Figure 5.3 first appeared in [Oka95c].
Reference: [FSST86] <author> Michael L. Fredman, Robert Sedgewick, Daniel D. K. Sleator, and Robert E. Tar jan. </author> <title> The pairing heap: A new form of self-adjusting heap. </title> <journal> Algorithmica, </journal> <volume> 1(1):111 129, </volume> <year> 1986. </year> <note> (p. 103) 140 BIBLIOGRAPHY </note>
Reference-contexts: Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues [KL93], Fibonacci heaps [FT87], relaxed heaps [DGST88], V-heaps [Pet87], bottom-up skew heaps [ST86b], and pairing heaps <ref> [FSST86] </ref>. However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal [Bro95, Bro96] achieves equivalent worst-case bounds.
Reference: [FT87] <author> Michael L. Fredman and Robert E. Tarjan. </author> <title> Fibonacci heaps and their uses in improved network optimization algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 34(3):596615, </volume> <month> July </month> <year> 1987. </year> <pages> (pp. 12, 103) </pages>
Reference-contexts: Algorithmics Both components of lazy evaluation delaying computations and memoizing the results have a long history in algorithm design, although not always in combination. The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues <ref> [ST86b, FT87] </ref>, and search trees [DSST89]. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression [HU73, TvL84]. <p> It is much simpler than Kaplan and Tarjan's, but yields amortized bounds rather than worst-case bounds. Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues [KL93], Fibonacci heaps <ref> [FT87] </ref>, relaxed heaps [DGST88], V-heaps [Pet87], bottom-up skew heaps [ST86b], and pairing heaps [FSST86]. However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved.
Reference: [FW76] <author> Daniel P. Friedman and David S. Wise. </author> <title> CONS should not evaluate its arguments. </title> <booktitle> In Automata, Languages and Programming, </booktitle> <pages> pages 257281, </pages> <month> July </month> <year> 1976. </year> <note> (p. 11) </note>
Reference-contexts: Friedman and Wise <ref> [FW76] </ref> and Henderson and Morris [HM76] extended Landin's streams with memoization.
Reference: [FWFD88] <author> Matthias Felleisen, Mitchell Wand, Daniel P. Friedman, and Bruce F. Duba. </author> <title> Ab stract continuations: a mathematical semantics for handling full functional jumps. </title> <booktitle> In ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 5262, </pages> <month> July </month> <year> 1988. </year> <note> (p. 130) </note>
Reference-contexts: In fact, $-patterns are just a special case of views. Unfortunately, views are not supported by any major functional programming language. 130 Conclusions Implementation Finally, we note that functional catenable lists are an essential ingredient in the implementation of certain sophisticated control structures <ref> [FWFD88] </ref>.
Reference: [GMPR77] <author> Leo J. Guibas, Edward M. McCreight, Michael F. Plass, and Janet R. Roberts. </author> <title> A new representation for linear lists. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 4960, </pages> <month> May </month> <year> 1977. </year> <pages> (pp. 82, 84, 124) </pages>
Reference-contexts: For instance, might a numerical representation based on floating point numbers be useful in approximation algorithms? 6.6 Related Work Numerical Representations Data structures that can be cast as numerical representations are surprisingly common, but only rarely is the connection to a variant number system noted explicitly <ref> [GMPR77, Mye83, CMP88, KT96b] </ref>. 1 Thanks to Phil Wadler for this observation. 6.6 Related Work 83 functor SkewBinomialHeap (structure E : ORDERED) : HEAP = struct structure Elem = E datatype Tree = Node of int fi Elem.T fi Elem.T list fi Tree list type Heap = Tree list exception EMPTY <p> We will consider a simplification of their data structure in Chapter 8. Finger search trees <ref> [GMPR77, Tsa85] </ref> support not only random access in O (log d) worst-case time, but also insertions and deletions at arbitrary locations. Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown [Bro78]. <p> we had to pass one debit from the original A, we always pass at most five debits. 2 8.6 Related Work Recursive Slowdown Kaplan and Tarjan introduced recursive slowdown in [KT95], and used it again in [KT96b], but it is closely related to the regularity constraints of Guibas et al. <ref> [GMPR77] </ref>. Brodal [Bro95] used a similar technique to implement heaps. 8.6 Related Work 125 Implicit Recursive Slowdown and Binomial Heaps Lazy implementations of binomial heaps [Kin94, Oka96b] can be viewed as using implicit recursive slowdown.
Reference: [Gri81] <editor> David Gries. </editor> <booktitle> The Science of Programming. Texts and Monographs in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year> <pages> (pp. 16, 18, 37) </pages>
Reference-contexts: : ff Queue ! ff (fl raises EMPTY if queue is empty fl) val tail : ff Queue ! ff Queue (fl raises EMPTY if queue is empty fl) end (Etymological note: snoc is cons spelled backward and means cons on the right.) A common representation for purely functional queues <ref> [Gri81, HM81, Bur82] </ref> is as a pair of lists, F and R, where F contains the front elements of the queue in the correct order and R contains the rear elements of the queue in reverse order. <p> He avoids the problems of persistence by insisting that data structures only be used in a single-threaded fashion. Queues Gries <ref> [Gri81, pages 250251] </ref> and Hood and Melville [HM81] first proposed the queues in Section 3.1.1.
Reference: [GS78] <author> Leo J. Guibas and Robert Sedgewick. </author> <title> A dichromatic framework for balanced trees. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 821, </pages> <month> October </month> <year> 1978. </year> <note> (p. 49) </note>
Reference-contexts: However, since it is usually too expensive to restore perfect balance after every update, most implementations settle for approximations of perfect balance that are at most a constant factor slower. Examples of this approach include AVL trees [AVL62] and red-black trees <ref> [GS78] </ref>. However, provided no update disturbs the balance too drastically, an attractive alternative is to postpone rebalancing until after a sequence of updates, and then to rebalance the entire structure, restoring it to perfect balance. We call this approach batched rebuilding.
Reference: [GT86] <author> Hania Gajewska and Robert E. Tarjan. </author> <title> Deques with heap order. </title> <journal> Information Processing Letters, </journal> <volume> 22(4):197200, </volume> <month> April </month> <year> 1986. </year> <note> (p. 58) </note>
Reference-contexts: The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques <ref> [Hoo82, GT86, Sar86, CG93] </ref>, catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work <ref> [GT86, Sar86, CG93] </ref>. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently.
Reference: [H + 92] <editor> Paul Hudak et al. </editor> <title> Report on the functional programming language Haskell, </title> <journal> Ver sion 1.2. SIGPLAN Notices, </journal> <volume> 27(5), </volume> <month> May </month> <year> 1992. </year> <note> (p. 7) </note>
Reference-contexts: Chapter 2 Lazy Evaluation and $-Notation Lazy evaluation is an evaluation strategy employed by many purely functional programming languages, such as Haskell <ref> [H + 92] </ref>. This strategy has two essential properties. First, the evaluation of a given expression is delayed, or suspended, until its result is needed.
Reference: [Hen93] <author> Fritz Henglein. </author> <title> Type inference with polymorphic recursion. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2):253289, </volume> <month> April </month> <year> 1993. </year> <note> (p. 103) </note>
Reference-contexts: However, such an implementation would be both complicated and slow. Brodal and Okasaki simplify this implementation in [BO96], using skew binomial heaps (Section 6.4.2) and structural abstraction (Section 7.2.2). Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as <ref> [Myc84, Hen93, KTU93] </ref>. One complication is that type inference is undecidable in the presence of polymorphic recursion [Hen93, KTU93], even though it is tractable in practice. <p> Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as [Myc84, Hen93, KTU93]. One complication is that type inference is undecidable in the presence of polymorphic recursion <ref> [Hen93, KTU93] </ref>, even though it is tractable in practice.
Reference: [HJ94] <author> Paul Hudak and Mark P. Jones. </author> <title> Haskell vs. Ada vs. C++ vs. . . . An experiment in software prototyping productivity, 1994. </title> <editor> (p. </editor> <volume> 1) </volume>
Reference-contexts: This thesis addresses this imbalance by specifically considering the design and analysis of functional data structures. 1.1 Functional vs. Imperative Data Structures The methodological benefits of functional languages are well known <ref> [Bac78, Hug89, HJ94] </ref>, but still the vast majority of programs are written in imperative languages such as C. This apparent contradiction is easily explained by the fact that functional languages have historically been slower than their more traditional cousins, but this gap is narrowing.
Reference: [HM76] <author> Peter Henderson and James H. Morris, Jr. </author> <title> A lazy evaluator. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 95103, </pages> <month> January </month> <year> 1976. </year> <note> (p. 11) </note>
Reference-contexts: Friedman and Wise [FW76] and Henderson and Morris <ref> [HM76] </ref> extended Landin's streams with memoization.
Reference: [HM81] <author> Robert Hood and Robert Melville. </author> <title> Real-time queue operations in pure Lisp. In formation Processing Letters, </title> <address> 13(2):5053, </address> <month> November </month> <year> 1981. </year> <journal> (pp. </journal> <volume> 16, 18, 37, 41, 48, 51, </volume> <pages> 58) </pages>
Reference-contexts: : ff Queue ! ff (fl raises EMPTY if queue is empty fl) val tail : ff Queue ! ff Queue (fl raises EMPTY if queue is empty fl) end (Etymological note: snoc is cons spelled backward and means cons on the right.) A common representation for purely functional queues <ref> [Gri81, HM81, Bur82] </ref> is as a pair of lists, F and R, where F contains the front elements of the queue in the correct order and R contains the rear elements of the queue in reverse order. <p> He avoids the problems of persistence by insisting that data structures only be used in a single-threaded fashion. Queues Gries [Gri81, pages 250251] and Hood and Melville <ref> [HM81] </ref> first proposed the queues in Section 3.1.1. <p> Queues such as these that support all operations in O (1) worst-case time are called real-time queues <ref> [HM81] </ref>. In the original data structure, queues are rotated using ++ and reverse. Since reverse is monolithic, our first task is finding a way to perform rotations incrementally. This can be done by executing one step of the reverse for every step of the ++. <p> The form of scheduling described here was first applied to queues in [Oka95c] and later generalized in [Oka96b]. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and Melville <ref> [HM81] </ref> presented the first purely functional implementation of real-time queues, based on a technique known as global rebuilding [Ove83], which will be discussed further in the next chapter. Their implementation does not use lazy evaluation and is more complicated than ours. <p> Global rebuilding can be implemented purely functionally, and has been several times. For example, the real-time queues of Hood and Melville <ref> [HM81] </ref> are based on this technique. Unlike batched rebuilding, global rebuilding has no problems with persistence. Since no one operation is particularly expensive, arbitrarily repeating operations has no effect on the time bounds. Unfortunately, global rebuilding is often quite complicated. <p> The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues <ref> [HM81] </ref>, real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> It has since been used in many situations, including real-time queues <ref> [HM81] </ref>, real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81].
Reference: [Hoo82] <author> Robert Hood. </author> <title> The Efficient Implementation of Very-High-Level Programming Lan guage Constructs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> August </month> <year> 1982. </year> <type> (Cornell TR 82-503). (p. 58) BIBLIOGRAPHY 141 </type>
Reference-contexts: The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques <ref> [Hoo82, GT86, Sar86, CG93] </ref>, catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood <ref> [Hoo82] </ref> first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81].
Reference: [Hoo92a] <author> Rob R. Hoogerwoord. </author> <title> A logarithmic implementation of flexible arrays. </title> <booktitle> In Con ference on Mathematics of Program Construction, volume 669 of LNCS, pages 191207. </booktitle> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1992. </year> <note> (p. 84) </note>
Reference-contexts: ts j insertAll (x :: xs, ts) = insertAll (xs, insert (x , ts)) in insertAll (xs, mergeTrees (rev c, normalize ts 0 )) end end 84 Numerical Representations Random-Access Lists Random-access lists are usually implemented in purely functional languages as balanced trees, such as AVL trees [Mye84], Braun trees <ref> [Hoo92a, Pau91] </ref>, or leftist left-perfect leaf trees [KD96]. Such trees easily support O (log n) lookups and updates (O (log i) in the case of Braun trees), but require O (log n) time for cons or tail .
Reference: [Hoo92b] <author> Rob R. Hoogerwoord. </author> <title> A symmetric set of efficient list operations. </title> <journal> Journal of Functional Programming, </journal> <volume> 2(4):505513, </volume> <month> October </month> <year> 1992. </year> <pages> (pp. 55, 58) </pages>
Reference-contexts: swapping the roles of F and R. fun reverse (Queue fF = f , LenF = lenF , R = r , LenR = lenRg) = Queue fF = r , LenF = lenR, R = f , LenR = lenF g Many other implementations of deques share this property <ref> [Hoo92b, CG93] </ref>. Rather than essentially duplicating the code for the functions on the front element and the functions on the rear element, we could define the functions on the rear element in terms of reverse and the corresponding functions on the front element. <p> Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord <ref> [Hoo92b] </ref> proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently. The real-time deques in Figure 5.3 first appeared in [Oka95c].
Reference: [HU73] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Set merging algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 2(4):294303, </volume> <month> December </month> <year> 1973. </year> <note> (p. 12) </note>
Reference-contexts: The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues [ST86b, FT87], and search trees [DSST89]. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression <ref> [HU73, TvL84] </ref>. Syntax for Lazy Evaluation Early versions of CAML [W + 90], a close cousin of Standard ML, offered support for lazy evaluation similar to the $-notation proposed here.
Reference: [Hug85] <author> John Hughes. </author> <title> Lazy memo functions. </title> <booktitle> In Conference on Functional Program ming Languages and Computer Architecture, volume 201 of LNCS, </booktitle> <pages> pages 129 146. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1985. </year> <note> (p. 11) </note>
Reference-contexts: Friedman and Wise [FW76] and Henderson and Morris [HM76] extended Landin's streams with memoization. Memoization Michie [Mic68] coined the term memoization to denote the augmentation of functions with a cache of argument-result pairs. (The argument field is dropped when memoiz-ing suspensions by regarding suspensions as nullary functions.) Hughes <ref> [Hug85] </ref> later applied 12 Lazy Evaluation and $-Notation memoization, in the original sense of Michie, to functional programs. Algorithmics Both components of lazy evaluation delaying computations and memoizing the results have a long history in algorithm design, although not always in combination.
Reference: [Hug86] <author> John Hughes. </author> <title> A novel representation of lists and its application to the function reverse. </title> <journal> Information Processing Letters, </journal> <volume> 22(3):141144, </volume> <month> March </month> <year> 1986. </year> <pages> (pp. 82, 101) </pages>
Reference-contexts: For example, difference lists [SS86] in Prolog support a notion of lists with negative length; appending a list of length 15 and a list of length 10 results in a list of length 5. This behavior is also possible using the catenable lists of Hughes <ref> [Hug86] </ref>, which are the functional counterpart of difference lists. 1 As another example, Brodal and Okasaki [BO96] support a delete function on heaps using two primitive heaps, one containing positive elements and one containing negative elements. <p> Structural decomposition and structural abstraction had previously been used in [Die82] and [DST94], respectively. Catenable Lists Although it is relatively easy to design alternative representations of persistent lists that support efficient catenation (see, for example, <ref> [Hug86] </ref>), such alternative representations seem almost inevitably to sacrifice efficiency on the head and/or tail functions. Myers [Mye84] described a representation based on AVL trees that supports all relevant list functions in O (log n) time. Driscoll, Sleator, and Tarjan achieved the first sub-logarithmic implementation in [DST94].
Reference: [Hug89] <author> John Hughes. </author> <title> Why functional programming matters. </title> <journal> The Computer Journal, </journal> <volume> 32(2):98107, </volume> <month> April </month> <year> 1989. </year> <pages> (pp. 1, 58) </pages>
Reference-contexts: This thesis addresses this imbalance by specifically considering the design and analysis of functional data structures. 1.1 Functional vs. Imperative Data Structures The methodological benefits of functional languages are well known <ref> [Bac78, Hug89, HJ94] </ref>, but still the vast majority of programs are written in imperative languages such as C. This apparent contradiction is easily explained by the fact that functional languages have historically been slower than their more traditional cousins, but this gap is narrowing. <p> Coroutines and Lazy Evaluation Streams (and other lazy data structures) have frequently been used to implement a form of coroutining between the producer of a stream and the consumer of a stream. Landin [Lan65] first pointed out this connection between streams and coroutines. See <ref> [Hug89] </ref> for some compelling applications of this feature. 5.5 Related Work 59 functor RealTimeDeque (val c : int) : DEQUE = (fl c = 2 or c = 3 fl) struct datatype ff Queue = Queue fF : ff Stream, LenF : int, SF : ff Stream, R : ff Stream,
Reference: [Jon92] <author> Richard Jones. </author> <title> Tail recursion without space leaks. </title> <journal> Journal of Functional Pro gramming, </journal> <note> 2(1):7379, January 1992. (p. 135) </note>
Reference-contexts: Then, the inner evaluation might produce and memoize a value that is subsequently overwritten by the result of the outer evaluation. Note, however, that evaluating a suspension that forces itself will not terminate unless side effects are involved. If desired, the blackhole technique <ref> [Jon92] </ref> can be used to detect such circular suspensions and guarantee that a given suspension is only memoized once. 3 136 The Definition of Lazy Evaluation in Standard ML
Reference: [Jon95] <author> Mark P. Jones. </author> <title> A system of constructor classes: overloading and implicit higher order polymorphism. </title> <journal> Journal of Functional Programming, </journal> <note> 5(1):135, January 1995. (p. 129) </note>
Reference-contexts: Clearly, however, it would be cleaner, and much less error-prone, to provide a single module-to-module transformation that performs the bootstrapping. In the case of bootstrapped heaps, Simon Pey-ton Jones and Jan Nicklisch [private communication] have recently shown how to implement the desired recursion using constructor classes <ref> [Jon95] </ref>. Pattern Matching Ironically, pattern matching one of the most popular features in functional programming languages is also one of the biggest obstacles to the widespread use of efficient functional data structures.
Reference: [Jos89] <author> Mark B. Josephs. </author> <title> The semantics of lazy functional languages. </title> <institution> Theoretical Com puter Science, 68(1):105111, </institution> <month> October </month> <year> 1989. </year> <note> (p. 10) </note>
Reference-contexts: Vuillemin [Vui74] later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy. The formal semantics of lazy evaluation has been studied extensively <ref> [Jos89, Lau93, OLT94, AFM + 95] </ref>. 2.2 Historical Notes 11 signature STREAM = sig datatype ff StreamCell = Nil j Cons of ff fi ff Stream withtype ff Stream = ff StreamCell susp val ++ : ff Stream fi ff Stream ! ff Stream (fl stream append fl) val take :
Reference: [KD96] <author> Anne Kaldewaij and Victor J. Dielissen. </author> <title> Leaf trees. </title> <institution> Science of Computer Pro gramming, 26(13):149165, </institution> <month> May </month> <year> 1996. </year> <pages> (pp. 64, 66, 84) </pages>
Reference-contexts: Trees in numerical representations typically exhibit a very regular structure. For example, in binary numerical representations, all trees have sizes that are powers of 2. Three common kinds of trees that exhibit this structure are complete binary leaf trees <ref> [KD96] </ref>, binomial trees [Vui78], and pennants [SS90]. <p> Assuming that elements are not rearranged, each of the three kinds of trees can be linked or unlinked in O (1) time. We next describe two existing data structures in terms of this framework: the one-sided flexible arrays of Kaldewaij and Dielissen <ref> [KD96] </ref>, and the binomial queues of Vuillemin [Vui78, Bro78]. 6.2 Binary Representations 65 @ @ @ A A A A C C C C C C C C fl fl fl fl fl fl fl fl (a) s s s s s A A A C C C C fl fl <p> A signature for random-access lists is shown in Figure 6.4. Kaldewaij and Dielissen <ref> [KD96] </ref> describe an implementation of random-access lists in terms of leftist left-perfect leaf trees. We can easily translate their implementation into the framework of numerical representations as a binary representation using complete binary leaf trees. <p> = insertAll (xs, insert (x , ts)) in insertAll (xs, mergeTrees (rev c, normalize ts 0 )) end end 84 Numerical Representations Random-Access Lists Random-access lists are usually implemented in purely functional languages as balanced trees, such as AVL trees [Mye84], Braun trees [Hoo92a, Pau91], or leftist left-perfect leaf trees <ref> [KD96] </ref>. Such trees easily support O (log n) lookups and updates (O (log i) in the case of Braun trees), but require O (log n) time for cons or tail . Myers [Mye83] describes the first implementation of random-access lists based on skew binary numbers.
Reference: [Kin94] <author> David J. King. </author> <title> Functional binomial queues. </title> <booktitle> In Glasgow Workshop on Functional Programming, </booktitle> <pages> pages 141150, </pages> <month> September </month> <year> 1994. </year> <pages> (pp. 84, 125) </pages>
Reference-contexts: Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown [Bro78]. King <ref> [Kin94] </ref> showed that binomial heaps could be implemented elegantly in a purely functional language (in his case, Haskell). Fagerberg [Fag96] describes a generalization of binomial heaps in which the set D i of allowable digits at position i in a sequence of digits can be different for each i. <p> Brodal [Bro95] used a similar technique to implement heaps. 8.6 Related Work 125 Implicit Recursive Slowdown and Binomial Heaps Lazy implementations of binomial heaps <ref> [Kin94, Oka96b] </ref> can be viewed as using implicit recursive slowdown. Such implementations support insert in O (1) amortized time and all other operations in O (log n) amortized time. [Oka96b] extends a lazy implementation of binomial heaps with scheduling to improve these bounds to worst-case.
Reference: [KL93] <author> Chan Meng Khoong and Hon Wai Leong. </author> <title> Double-ended binomial queues. </title> <booktitle> In International Symposium on Algorithms and Computation, volume 762 of LNCS, </booktitle> <pages> pages 128137. </pages> <publisher> Springer-Verlag, </publisher> <month> December </month> <year> 1993. </year> <note> (p. 103) </note>
Reference-contexts: It is much simpler than Kaplan and Tarjan's, but yields amortized bounds rather than worst-case bounds. Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues <ref> [KL93] </ref>, Fibonacci heaps [FT87], relaxed heaps [DGST88], V-heaps [Pet87], bottom-up skew heaps [ST86b], and pairing heaps [FSST86].
Reference: [Knu73] <author> Donald E. Knuth. </author> <title> Seminumerical Algorithms, </title> <booktitle> volume 2 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year> <note> (p. 62) 142 BIBLIOGRAPHY </note>
Reference-contexts: Example abstractions for which numerical representations are particularly useful include random-access lists (also known as flexible arrays) and heaps (also known as priority queues). 6.1 Positional Number Systems A positional number system <ref> [Knu73] </ref> is a notation for writing a number as a sequence of digits b 0 : : : b m1 . The digit b 0 is called the least significant digit and the digit b m1 is called the most significant digit.
Reference: [KT95] <author> Haim Kaplan and Robert E. Tarjan. </author> <title> Persistent lists with catenation via recursive slow-down. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 93102, </pages> <month> May </month> <year> 1995. </year> <journal> (pp. </journal> <volume> 6, 84, 103, 105, 107, 124, 128, 130, </volume> <pages> 142) </pages>
Reference-contexts: Data-structural bootstrapping comes in two flavors: structural decomposition, in which unbounded solutions are bootstrapped from bounded solutions, and structural abstraction, in which efficient solutions are bootstrapped from inefficient solutions. Chapter 8 describes implicit recursive slowdown, a lazy variant of the recursive-slowdown technique of Kaplan and Tarjan <ref> [KT95] </ref>. As with lazy rebuilding, implicit recursive slowdown is significantly simpler than recursive slowdown, but yields amortized rather than worst-case bounds. Again, we can recover the worst-case bounds using scheduling. <p> The difficulty with updates is that his scheme contains many redundant pointers. Removing those redundant pointers yields a structure isomorphic to the skew binary random-access lists of Section 6.4.1, which first appeared in [Oka95b]. Kaplan and Tarjan <ref> [KT95] </ref> recently introduced the algorithmic notion of recursive slowdown, and used it to design a new, purely functional implementation of real-time deques. <p> Kaplan and Tarjan <ref> [KT95] </ref> finally achieved an implementation that supports catenation and all other usual list functions in O (1) worst-case time. Their data structure is based on the technique of recursive slowdown. We will describe recursive slowdown in greater detail in Chapter 8. <p> Brodal [Bro95, Bro96] achieves equivalent worst-case bounds. His original data structure [Bro95] can be implemented purely functionally (and thus made persistent) by combining the recursive-slowdown technique of Kaplan and Tarjan <ref> [KT95] </ref> with a purely functional implementation of real-time deques, such as the real-time deques of Section 5.4.3. However, such an implementation would be both complicated and slow. Brodal and Okasaki simplify this implementation in [BO96], using skew binomial heaps (Section 6.4.2) and structural abstraction (Section 7.2.2). <p> Haskell 1.3 [P + 96] sidesteps this problem by allowing polymorphic recursion whenever the programmer provides an explicit type signature. 104 Data-Structural Bootstrapping Chapter 8 Implicit Recursive Slowdown Implicit recursive slowdown is a lazy variant of the recursive-slowdown technique of Kaplan and Tarjan <ref> [KT95] </ref>. We first review recursive slowdown, and then show how lazy evaluation can significantly simplify this technique. <p> Consecutive yellow levels are grouped in a block to support efficient access to the first non-yellow level. Kaplan and Tarjan <ref> [KT95] </ref> describe two implementations based on this template: real-time deques and real-time catenable lists. 8.2 Implicit Recursive Slowdown The essence of the recursive-slowdown implementation of binary numbers is a method for executing carries incrementally. By now we have seen many examples of incremental functions implemented with lazy evaluation. <p> Since we pass four of these debits exactly in the case that we had to pass one debit from the original A, we always pass at most five debits. 2 8.6 Related Work Recursive Slowdown Kaplan and Tarjan introduced recursive slowdown in <ref> [KT95] </ref>, and used it again in [KT96b], but it is closely related to the regularity constraints of Guibas et al. [GMPR77]. <p> It is no surprise that the best persistent implementations of data structures such as catenable lists (Section 7.2.1) and caten-able deques (Section 8.5) are all purely functional (see also <ref> [KT95, KT96a] </ref>). 9.3 Programming Language Design Next, we briefly discuss the implications of this work on programming language design. 1 As partial evidence for this fact, we note that only one of these implementations takes more than one page. 9.3 Programming Language Design 129 Order of Evaluation Most functional programming languages <p> Unfortunately, views are not supported by any major functional programming language. 130 Conclusions Implementation Finally, we note that functional catenable lists are an essential ingredient in the implementation of certain sophisticated control structures [FWFD88]. The advent of new, efficient implementations of catenable lists, both here and in <ref> [KT95] </ref>, makes the efficient implementation of such control structures possible for the first time. 9.4 Open Problems We conclude by describing some of the open problems related to this thesis. * What are appropriate empirical measurements for persistent data structures? Standard benchmarks are misleading since they do not measure how well
Reference: [KT96a] <author> Haim Kaplan and Robert E. Tarjan. </author> <title> Purely functional lists with catenation via recursive slow-down. Draft revision of [KT95], </title> <month> August </month> <year> 1996. </year> <pages> (pp. 125, 128) </pages>
Reference-contexts: Our implementation improves that bound to O (1) for all operations, although in the amortized rather than worst-case sense. Kaplan and Tarjan have independently developed a similar implementation with worst-case bounds <ref> [KT96a] </ref>. <p> It is no surprise that the best persistent implementations of data structures such as catenable lists (Section 7.2.1) and caten-able deques (Section 8.5) are all purely functional (see also <ref> [KT95, KT96a] </ref>). 9.3 Programming Language Design Next, we briefly discuss the implications of this work on programming language design. 1 As partial evidence for this fact, we note that only one of these implementations takes more than one page. 9.3 Programming Language Design 129 Order of Evaluation Most functional programming languages
Reference: [KT96b] <author> Haim Kaplan and Robert E. Tarjan. </author> <title> Purely functional representations of catenable sorted lists. </title> <booktitle> In ACM Symposium on Theory of Computing, </booktitle> <pages> pages 202211, </pages> <month> May </month> <year> 1996. </year> <pages> (pp. 4, 82, 84, 124) </pages>
Reference-contexts: Until this research, it was widely believed that amortization was incompatible with persistence [DST94, Ram92]. However, we show that memoization, in the form of lazy evaluation, is the key to reconciling the two. Furthermore, as noted by Kaplan and Tarjan <ref> [KT96b] </ref>, functional programming is a convenient medium for developing new persistent data structures, even when the data structure will eventually be implemented in an imperative language. <p> For instance, might a numerical representation based on floating point numbers be useful in approximation algorithms? 6.6 Related Work Numerical Representations Data structures that can be cast as numerical representations are surprisingly common, but only rarely is the connection to a variant number system noted explicitly <ref> [GMPR77, Mye83, CMP88, KT96b] </ref>. 1 Thanks to Phil Wadler for this observation. 6.6 Related Work 83 functor SkewBinomialHeap (structure E : ORDERED) : HEAP = struct structure Elem = E datatype Tree = Node of int fi Elem.T fi Elem.T list fi Tree list type Heap = Tree list exception EMPTY <p> We will consider a simplification of their data structure in Chapter 8. Finger search trees [GMPR77, Tsa85] support not only random access in O (log d) worst-case time, but also insertions and deletions at arbitrary locations. Kaplan and Tarjan apply their methods to purely functional finger search trees in <ref> [KT96b] </ref>. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown [Bro78]. King [Kin94] showed that binomial heaps could be implemented elegantly in a purely functional language (in his case, Haskell). <p> Since we pass four of these debits exactly in the case that we had to pass one debit from the original A, we always pass at most five debits. 2 8.6 Related Work Recursive Slowdown Kaplan and Tarjan introduced recursive slowdown in [KT95], and used it again in <ref> [KT96b] </ref>, but it is closely related to the regularity constraints of Guibas et al. [GMPR77]. Brodal [Bro95] used a similar technique to implement heaps. 8.6 Related Work 125 Implicit Recursive Slowdown and Binomial Heaps Lazy implementations of binomial heaps [Kin94, Oka96b] can be viewed as using implicit recursive slowdown.
Reference: [KTU93] <author> Assaf J. Kfoury, Jerzy Tiuryn, and Pawel Urzyczyn. </author> <title> Type reconstruction in the presence of polymorphic recursion. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2):290311, </volume> <month> April </month> <year> 1993. </year> <note> (p. 103) </note>
Reference-contexts: However, such an implementation would be both complicated and slow. Brodal and Okasaki simplify this implementation in [BO96], using skew binomial heaps (Section 6.4.2) and structural abstraction (Section 7.2.2). Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as <ref> [Myc84, Hen93, KTU93] </ref>. One complication is that type inference is undecidable in the presence of polymorphic recursion [Hen93, KTU93], even though it is tractable in practice. <p> Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as [Myc84, Hen93, KTU93]. One complication is that type inference is undecidable in the presence of polymorphic recursion <ref> [Hen93, KTU93] </ref>, even though it is tractable in practice.
Reference: [Lan65] <author> P. J. Landin. </author> <title> A correspondence between ALGOL 60 and Church's lambda notation: Part I. </title> <journal> Communications of the ACM, </journal> <volume> 8(2):89101, </volume> <month> February </month> <year> 1965. </year> <pages> (pp. 11, 58) </pages>
Reference-contexts: in $drop 0 (n, s) end fun reverse s = let fun reverse 0 ($Nil, r ) = r j reverse 0 ($Cons (x , s), r ) = reverse 0 (s, Cons (x , $r )) in $reverse 0 (s , Nil) end end Streams Landin introduced streams in <ref> [Lan65] </ref>, but without memoization. Friedman and Wise [FW76] and Henderson and Morris [HM76] extended Landin's streams with memoization. <p> The real-time deques in Figure 5.3 first appeared in [Oka95c]. Coroutines and Lazy Evaluation Streams (and other lazy data structures) have frequently been used to implement a form of coroutining between the producer of a stream and the consumer of a stream. Landin <ref> [Lan65] </ref> first pointed out this connection between streams and coroutines.
Reference: [Lau93] <author> John Launchbury. </author> <title> A natural semantics for lazy evaluation. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 144154, </pages> <month> January </month> <year> 1993. </year> <note> (p. 10) </note>
Reference-contexts: Vuillemin [Vui74] later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy. The formal semantics of lazy evaluation has been studied extensively <ref> [Jos89, Lau93, OLT94, AFM + 95] </ref>. 2.2 Historical Notes 11 signature STREAM = sig datatype ff StreamCell = Nil j Cons of ff fi ff Stream withtype ff Stream = ff StreamCell susp val ++ : ff Stream fi ff Stream ! ff Stream (fl stream append fl) val take :
Reference: [LS81] <author> Benton L. Leong and Joel I. Seiferas. </author> <title> New real-time simulations of multihead tape units. </title> <journal> Journal of the ACM, </journal> <note> 28(1):166180, January 1981. (p. 58) </note>
Reference-contexts: Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines <ref> [Sto70, FMR72, LS81] </ref>. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently. The real-time deques in Figure 5.3 first appeared in [Oka95c].
Reference: [Mic68] <author> Donald Michie. </author> <title> Memo functions and machine learning. </title> <booktitle> Nature, </booktitle> <address> 218:1922, </address> <month> April </month> <year> 1968. </year> <pages> (pp. 2, 11) </pages>
Reference-contexts: Furthermore, once a given argument is evaluated, the value of that argument is cached so that if it is ever needed again, it can be looked up rather than recomputed. This caching is known as memoization <ref> [Mic68] </ref>. Each evaluation order has its advantages and disadvantages, but strict evaluation is clearly superior in at least one area: ease of reasoning about asymptotic complexity. In strict languages, exactly which subexpressions will be evaluated, and when, is for the most part syntactically apparent. <p> Friedman and Wise [FW76] and Henderson and Morris [HM76] extended Landin's streams with memoization. Memoization Michie <ref> [Mic68] </ref> coined the term memoization to denote the augmentation of functions with a cache of argument-result pairs. (The argument field is dropped when memoiz-ing suspensions by regarding suspensions as nullary functions.) Hughes [Hug85] later applied 12 Lazy Evaluation and $-Notation memoization, in the original sense of Michie, to functional programs.
Reference: [MT94] <author> David B. MacQueen and Mads Tofte. </author> <title> A semantics for higher-order functors. </title> <booktitle> In European Symposium on Programming, </booktitle> <pages> pages 409423, </pages> <month> April </month> <year> 1994. </year> <note> (p. 100) </note>
Reference-contexts: The implementation of a heap is a functor that is parameterized by the element type and the comparison function. Therefore, the functor that we use to bootstrap heaps maps heap functors to heap functors, rather than heap structures to heap structures. Using higher-order functors <ref> [MT94] </ref>, this can be expressed as functor Bootstrap (functor MakeH (structure E : ORDERED) : sig include HEAP sharing Elem = E end) (structure E : ORDERED) : HEAP = . . . The Bootstrap functor takes the MakeH functor as an argument.
Reference: [MTH90] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year> <pages> (pp. 4, 131) </pages>
Reference-contexts: As a modest step in this direction, we propose $-notation, which allows the use of lazy evaluation in a strict language with a minimum of syntactic overhead. 1.4 Source Language All source code will be presented in Standard ML <ref> [MTH90] </ref>, extended with primitives for lazy evaluation. However, the algorithms can all easily be translated into any other functional language supporting both strict and lazy evaluation. <p> Appendix A The Definition of Lazy Evaluation in Standard ML The syntax and semantics of Standard ML are formally specified in The Definition of Standard ML <ref> [MTH90] </ref>. This appendix extends the Definition with the syntax and semantics of the lazy evaluation primitives ($-notation) described in Chapter 2. This appendix is designed to be read in conjunction with the Definition; it describes only the relevant changes and additions.
Reference: [Myc84] <author> Alan Mycroft. </author> <title> Polymorphic type schemes and recursive definitions. </title> <booktitle> In Interna tional Symposium on Programming, volume 167 of LNCS, </booktitle> <pages> pages 217228. </pages> <address> Spring-Verlag, </address> <month> April </month> <year> 1984. </year> <pages> (pp. 87, 103) </pages>
Reference-contexts: For these reasons, we will often present code as if Standard ML supported non-uniform recursive function definitions, also known as polymorphic recursion <ref> [Myc84] </ref>. This code will not be executable but will be easier to read. We will then sketch the coercions necessary to eliminate the polymorphic recursion and make the code executable. 7.1.2 Queues Revisited Consider the use of ++ in the banker's queues of Section 3.4.2. <p> However, such an implementation would be both complicated and slow. Brodal and Okasaki simplify this implementation in [BO96], using skew binomial heaps (Section 6.4.2) and structural abstraction (Section 7.2.2). Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as <ref> [Myc84, Hen93, KTU93] </ref>. One complication is that type inference is undecidable in the presence of polymorphic recursion [Hen93, KTU93], even though it is tractable in practice.
Reference: [Mye83] <author> Eugene W. Myers. </author> <title> An applicative random-access stack. </title> <journal> Information Processing Letters, </journal> <volume> 17(5):241248, </volume> <month> December </month> <year> 1983. </year> <pages> (pp. 76, 82, 84) </pages>
Reference-contexts: Unfortunately, such data structures are too complicated to be useful in practice. We next consider another number system, skew binary numbers, that usually achieves similar asymptotic benefits, but that is simpler and faster in practice. In skew binary numbers <ref> [Mye83, Oka95b] </ref>, the weight w i of the ith digit is 2 i+1 1, rather than 2 i as in ordinary binary numbers. Digits may be 0, 1, or 2 (i.e., D i = f0; 1; 2g). For example, the decimal number 92 could be written 002101 (least-significant digit first). <p> Such a number is said to be in canonical form. Henceforth, we will assume that all skew binary numbers are in canonical form. Theorem 6.1 (Myers <ref> [Mye83] </ref>) Every natural number has a unique skew binary canonical form. Recall that the weight of digit i is 2 i+1 1 and note that 1 + 2 (2 i+1 1) = 2 i+2 1. <p> For instance, might a numerical representation based on floating point numbers be useful in approximation algorithms? 6.6 Related Work Numerical Representations Data structures that can be cast as numerical representations are surprisingly common, but only rarely is the connection to a variant number system noted explicitly <ref> [GMPR77, Mye83, CMP88, KT96b] </ref>. 1 Thanks to Phil Wadler for this observation. 6.6 Related Work 83 functor SkewBinomialHeap (structure E : ORDERED) : HEAP = struct structure Elem = E datatype Tree = Node of int fi Elem.T fi Elem.T list fi Tree list type Heap = Tree list exception EMPTY <p> Such trees easily support O (log n) lookups and updates (O (log i) in the case of Braun trees), but require O (log n) time for cons or tail . Myers <ref> [Mye83] </ref> describes the first implementation of random-access lists based on skew binary numbers. He augments a standard singly-linked list with auxiliary pointers allowing one to skip arbitrarily far ahead in the list.
Reference: [Mye84] <author> Eugene W. Myers. </author> <title> Efficient applicative data types. </title> <booktitle> In ACM Symposium on Prin ciples of Programming Languages, </booktitle> <pages> pages 6675, </pages> <month> January </month> <year> 1984. </year> <pages> (pp. 84, 101) BIBLIOGRAPHY 143 </pages>
Reference-contexts: ], ts) = ts j insertAll (x :: xs, ts) = insertAll (xs, insert (x , ts)) in insertAll (xs, mergeTrees (rev c, normalize ts 0 )) end end 84 Numerical Representations Random-Access Lists Random-access lists are usually implemented in purely functional languages as balanced trees, such as AVL trees <ref> [Mye84] </ref>, Braun trees [Hoo92a, Pau91], or leftist left-perfect leaf trees [KD96]. Such trees easily support O (log n) lookups and updates (O (log i) in the case of Braun trees), but require O (log n) time for cons or tail . <p> Catenable Lists Although it is relatively easy to design alternative representations of persistent lists that support efficient catenation (see, for example, [Hug86]), such alternative representations seem almost inevitably to sacrifice efficiency on the head and/or tail functions. Myers <ref> [Mye84] </ref> described a representation based on AVL trees that supports all relevant list functions in O (log n) time. Driscoll, Sleator, and Tarjan achieved the first sub-logarithmic implementation in [DST94]. They represent catenable lists as n-ary trees with the elements at the leaves.
Reference: [Oka95a] <author> Chris Okasaki. Amortization, </author> <title> lazy evaluation, and persistence: Lists with catena tion via lazy linking. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 646654, </pages> <month> October </month> <year> 1995. </year> <pages> (pp. 37, 103) </pages>
Reference-contexts: Examples of offending functions include list catenation and set union.) The idea that lazy evaluation could reconcile amortization and persistence first appeared, in rudimentary form, in [Oka95c]. The theory and practice of this technique was further developed in <ref> [Oka95a, Oka96b] </ref>. Amortization and Functional Data Structures In his thesis, Schoenmakers [Sch93] studies amortized data structures in a strict functional language, concentrating on formal derivations of amortized bounds using the traditional physicist's method. <p> Their data structure is based on the technique of recursive slowdown. We will describe recursive slowdown in greater detail in Chapter 8. The implementation of catenable lists in Section 7.2.1 first appeared in <ref> [Oka95a] </ref>. It is much simpler than Kaplan and Tarjan's, but yields amortized bounds rather than worst-case bounds.
Reference: [Oka95b] <author> Chris Okasaki. </author> <title> Purely functional random-access lists. </title> <booktitle> In Conference on Func tional Programming Languages and Computer Architecture, </booktitle> <pages> pages 8695, </pages> <month> June </month> <year> 1995. </year> <pages> (pp. 76, 78, 84) </pages>
Reference-contexts: Unfortunately, such data structures are too complicated to be useful in practice. We next consider another number system, skew binary numbers, that usually achieves similar asymptotic benefits, but that is simpler and faster in practice. In skew binary numbers <ref> [Mye83, Oka95b] </ref>, the weight w i of the ith digit is 2 i+1 1, rather than 2 i as in ordinary binary numbers. Digits may be 0, 1, or 2 (i.e., D i = f0; 1; 2g). For example, the decimal number 92 could be written 002101 (least-significant digit first). <p> Hint to Practitioners: Skew binary random-access lists are a good choice for applications that take advantage of both the list-like aspects and the array-like aspects of random-access lists. Although there are better implementations of lists, and better implementations of (persistent) arrays, none are better at both <ref> [Oka95b] </ref>. 6.4 Skew Binary Numbers 79 structure SkewBinaryRandomAccessList : RANDOMACCESSLIST = struct datatype ff Tree = Leaf of ff j Node of ff fi ff Tree fi ff Tree type ff RList = (int fi ff Tree) list (fl integer is the weight of the tree fl) exception EMPTY and INDEX <p> The difficulty with updates is that his scheme contains many redundant pointers. Removing those redundant pointers yields a structure isomorphic to the skew binary random-access lists of Section 6.4.1, which first appeared in <ref> [Oka95b] </ref>. Kaplan and Tarjan [KT95] recently introduced the algorithmic notion of recursive slowdown, and used it to design a new, purely functional implementation of real-time deques.
Reference: [Oka95c] <author> Chris Okasaki. </author> <title> Simple and efficient purely functional queues and deques. </title> <journal> Journal of Functional Programming, </journal> <volume> 5(4):583592, </volume> <month> October </month> <year> 1995. </year> <journal> (pp. </journal> <volume> 37, 42, 43, 48, </volume> <pages> 58) </pages>
Reference-contexts: Most notably, they cannot be applied to data structures supporting functions that combine two or more versions. Examples of offending functions include list catenation and set union.) The idea that lazy evaluation could reconcile amortization and persistence first appeared, in rudimentary form, in <ref> [Oka95c] </ref>. The theory and practice of this technique was further developed in [Oka95a, Oka96b]. Amortization and Functional Data Structures In his thesis, Schoenmakers [Sch93] studies amortized data structures in a strict functional language, concentrating on formal derivations of amortized bounds using the traditional physicist's method. <p> Just rewriting the pseudo-constructor queue to call rotate (f , r , $Nil ) instead f ++ reverse r , and making no other changes, already drastically improves the worst-case behavior of the queue operations from O (n) to O (log n) (see <ref> [Oka95c] </ref>), but we can further improve the worst-case behavior to O (1) using scheduling. We begin by adding a schedule to the datatype. <p> Others have used ad hoc techniques similar to scheduling to eliminate amortization from specific data structures such as relaxed heaps [DGST88] and implicit binomial queues [CMP88]. The form of scheduling described here was first applied to queues in <ref> [Oka95c] </ref> and later generalized in [Oka96b]. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and Melville [HM81] presented the first purely functional implementation of real-time queues, based on a technique known as global rebuilding [Ove83], which will be discussed further in the next chapter. <p> The form of scheduling described here was first applied to queues in <ref> [Oka95c] </ref> and later generalized in [Oka96b]. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and Melville [HM81] presented the first purely functional implementation of real-time queues, based on a technique known as global rebuilding [Ove83], which will be discussed further in the next chapter. Their implementation does not use lazy evaluation and is more complicated than ours. <p> These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently. The real-time deques in Figure 5.3 first appeared in <ref> [Oka95c] </ref>. Coroutines and Lazy Evaluation Streams (and other lazy data structures) have frequently been used to implement a form of coroutining between the producer of a stream and the consumer of a stream. Landin [Lan65] first pointed out this connection between streams and coroutines.
Reference: [Oka96a] <author> Chris Okasaki. </author> <title> Functional data structures. </title> <booktitle> In Advanced Functional Programming, volume 1129 of LNCS, </booktitle> <pages> pages 131158. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year> <pages> (pp. 90, 103) </pages>
Reference-contexts: Since these queues are not recursive, we have no need for polymorphic recursion. This variation is explored in greater detail in <ref> [Oka96a] </ref>. 7.2 Structural Abstraction 91 Hint to Practitioners: In practice, variations on these queues are the fastest known implementations for applications that use persistence sparingly, but that require good behavior even in pathological cases. 7.2 Structural Abstraction The second kind of data-structural bootstrapping is structural abstraction, which is typically used to <p> However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting <ref> [Oka96a] </ref>, and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal [Bro95, Bro96] achieves equivalent worst-case bounds.
Reference: [Oka96b] <author> Chris Okasaki. </author> <title> The role of lazy evaluation in amortized data structures. </title> <booktitle> In ACM SIGPLAN International Conference on Functional Programming, </booktitle> <pages> pages 6272, </pages> <month> May </month> <year> 1996. </year> <pages> (pp. 37, 48, 125) </pages>
Reference-contexts: Examples of offending functions include list catenation and set union.) The idea that lazy evaluation could reconcile amortization and persistence first appeared, in rudimentary form, in [Oka95c]. The theory and practice of this technique was further developed in <ref> [Oka95a, Oka96b] </ref>. Amortization and Functional Data Structures In his thesis, Schoenmakers [Sch93] studies amortized data structures in a strict functional language, concentrating on formal derivations of amortized bounds using the traditional physicist's method. <p> Burton [Bur82] proposed a similar implementation, but without the restriction that the front list be non-empty whenever the queue is non-empty. (Burton combines head and tail into a single operation, and so does not require this restriction to support head efficiently.) The queues in Section 3.4.2 first appeared in <ref> [Oka96b] </ref>. 2 There is a clear analogy here to the spontaneous creation and mutual annihilation of particle-antiparticle pairs in physics. <p> Others have used ad hoc techniques similar to scheduling to eliminate amortization from specific data structures such as relaxed heaps [DGST88] and implicit binomial queues [CMP88]. The form of scheduling described here was first applied to queues in [Oka95c] and later generalized in <ref> [Oka96b] </ref>. Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and Melville [HM81] presented the first purely functional implementation of real-time queues, based on a technique known as global rebuilding [Ove83], which will be discussed further in the next chapter. <p> Brodal [Bro95] used a similar technique to implement heaps. 8.6 Related Work 125 Implicit Recursive Slowdown and Binomial Heaps Lazy implementations of binomial heaps <ref> [Kin94, Oka96b] </ref> can be viewed as using implicit recursive slowdown. Such implementations support insert in O (1) amortized time and all other operations in O (log n) amortized time. [Oka96b] extends a lazy implementation of binomial heaps with scheduling to improve these bounds to worst-case. <p> Such implementations support insert in O (1) amortized time and all other operations in O (log n) amortized time. <ref> [Oka96b] </ref> extends a lazy implementation of binomial heaps with scheduling to improve these bounds to worst-case.
Reference: [OLT94] <author> Chris Okasaki, Peter Lee, and David Tarditi. </author> <title> Call-by-need and continuation passing style. Lisp and Symbolic Computation, </title> <address> 7(1):5781, </address> <month> January </month> <year> 1994. </year> <note> (p. 10) </note>
Reference-contexts: Vuillemin [Vui74] later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy. The formal semantics of lazy evaluation has been studied extensively <ref> [Jos89, Lau93, OLT94, AFM + 95] </ref>. 2.2 Historical Notes 11 signature STREAM = sig datatype ff StreamCell = Nil j Cons of ff fi ff Stream withtype ff Stream = ff StreamCell susp val ++ : ff Stream fi ff Stream ! ff Stream (fl stream append fl) val take :
Reference: [Ove83] <author> Mark H. Overmars. </author> <title> The Design of Dynamic Data Structures, </title> <booktitle> volume 156 of LNCS. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1983. </year> <journal> (pp. </journal> <volume> 6, 48, 49, 51, </volume> <pages> 58) </pages>
Reference-contexts: Chapter 5 describes lazy rebuilding, a lazy variant of global rebuilding <ref> [Ove83] </ref>. Lazy rebuilding is significantly simpler than global rebuilding, but yields amortized rather than worst-case bounds. By combining lazy rebuilding with the scheduling techniques of Chapter 4, the worst-case bounds can be recovered. Chapter 6 explores numerical representations, implementations designed in analogy to representations of numbers (typically binary numbers). <p> Queues The queue implementation in Section 4.2 first appeared in [Oka95c]. Hood and Melville [HM81] presented the first purely functional implementation of real-time queues, based on a technique known as global rebuilding <ref> [Ove83] </ref>, which will be discussed further in the next chapter. Their implementation does not use lazy evaluation and is more complicated than ours. Chapter 5 Lazy Rebuilding The next four chapters describe general techniques for designing functional data structures. <p> Their implementation does not use lazy evaluation and is more complicated than ours. Chapter 5 Lazy Rebuilding The next four chapters describe general techniques for designing functional data structures. We begin in this chapter with lazy rebuilding, a variant of global rebuilding <ref> [Ove83] </ref>. 5.1 Batched Rebuilding Many data structures obey balance invariants that guarantee efficient access. The canonical example is balanced binary search trees, which improve the worst-case running time of many tree operations from the O (n) required by unbalanced trees to O (log n). <p> Under persistent usage, the amortized bounds degrade to the cost of the rebuilding transformation because it is possible to trigger the transformation arbitrarily often. In fact, this is true for all data structures based on batched rebuilding. 5.2 Global Rebuilding 51 5.2 Global Rebuilding Overmars <ref> [Ove83] </ref> developed a technique for eliminating the amortization from batched rebuilding. He called this technique global rebuilding. The basic idea is to execute the rebuilding transformation incrementally, performing a few steps per normal operation. This can be usefully viewed as running the rebuilding transformation as a coroutine. <p> The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in <ref> [Ove83] </ref>. It has since been used in many situations, including real-time queues [HM81], real-time deques [Hoo82, GT86, Sar86, CG93], catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding.
Reference: [P + 96] <author> John Peterson et al. </author> <title> Haskell 1.3: A non-strict, purely functional language. </title> <note> http://haskell.cs.yale.edu/haskell-report/haskell-report.html, May 1996. (pp. 103, 129) </note>
Reference-contexts: Polymorphic Recursion Several attempts have been made to extend Standard ML with polymorphic recursion, such as [Myc84, Hen93, KTU93]. One complication is that type inference is undecidable in the presence of polymorphic recursion [Hen93, KTU93], even though it is tractable in practice. Haskell 1.3 <ref> [P + 96] </ref> sidesteps this problem by allowing polymorphic recursion whenever the programmer provides an explicit type signature. 104 Data-Structural Bootstrapping Chapter 8 Implicit Recursive Slowdown Implicit recursive slowdown is a lazy variant of the recursive-slowdown technique of Kaplan and Tarjan [KT95]. <p> We can usually sidestep this restriction by rewriting the datatypes to be uniform, but then the types fail to capture the desired invariants and the type system will not catch bugs involving violations of those invariants. All in all, we believe the compromise taken by Haskell 1.3 <ref> [P + 96] </ref> is best: allow polymorphic recursion in those cases where the programmer explicitly provides a type signature, and disallow it everywhere else. Higher-order, Recursive Modules The bootstrapped heaps of Section 7.2.2 (see also [BO96]) demonstrate the usefulness of higher-order, recursive modules.
Reference: [Pau91] <author> Laurence C. Paulson. </author> <title> ML for the Working Programmer. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year> <note> (p. 84) </note>
Reference-contexts: ts j insertAll (x :: xs, ts) = insertAll (xs, insert (x , ts)) in insertAll (xs, mergeTrees (rev c, normalize ts 0 )) end end 84 Numerical Representations Random-Access Lists Random-access lists are usually implemented in purely functional languages as balanced trees, such as AVL trees [Mye84], Braun trees <ref> [Hoo92a, Pau91] </ref>, or leftist left-perfect leaf trees [KD96]. Such trees easily support O (log n) lookups and updates (O (log i) in the case of Braun trees), but require O (log n) time for cons or tail .
Reference: [Pet87] <author> Gary L. Peterson. </author> <title> A balanced tree scheme for meldable heaps with updates. </title> <type> Tech nical Report GIT-ICS-87-23, </type> <institution> School of Information and Computer Science, Geor-gia Institute of Technology, </institution> <year> 1987. </year> <note> (p. 103) </note>
Reference-contexts: Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues [KL93], Fibonacci heaps [FT87], relaxed heaps [DGST88], V-heaps <ref> [Pet87] </ref>, bottom-up skew heaps [ST86b], and pairing heaps [FSST86]. However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved.
Reference: [Pip96] <author> Nicholas Pippenger. </author> <title> Pure versus impure Lisp. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104109, </pages> <month> January </month> <year> 1996. </year> <pages> (pp. 2, 128) </pages>
Reference-contexts: Furthermore, theoreticians have established lower bounds suggesting that functional programming languages may be fundamentally less efficient than imperative languages in some situations <ref> [BAG92, Pip96] </ref>. In spite of all these points, this thesis shows that it is often possible to devise functional data structures that are asymptotically as efficient as the best imperative solutions. 1.2 Strict vs. <p> When used in a mostly single-threaded fashion, our implementations are often slower than competing implementations not based on memoization, because most of the time spent doing memoization is wasted. However, when persistence is used heavily, memoization more than pays for itself and our implementations fly. In a followup to <ref> [Pip96] </ref>, Bird, Jones, and de Moor [BJdM96] have recently exhibited a problem for which a lazy solution exists that is asymptotically superior to any possible strict solution. However, this result depends on several extremely restrictive assumptions. Our work suggests a promising approach towards removing these restrictions.
Reference: [PPN96] <author> Pedro Palao Gostanza, Ricardo Pena, and Manuel Nunez. </author> <title> A new look at pattern matching in abstract data types. </title> <booktitle> In ACM SIGPLAN International Conference on Functional Programming, </booktitle> <pages> pages 110121, </pages> <month> May </month> <year> 1996. </year> <note> (p. 129) 144 BIBLIOGRAPHY </note>
Reference-contexts: The seductive allure of pattern matching leads many functional programmers to abandon sophisticated data structures in favor of simple, known representations such as lists, even when doing so causes an otherwise linear algorithm to explode to quadratic or even exponential time. Views [Wad87] and their successors <ref> [BC93, PPN96] </ref> offer one way of reconciling the convenience of pattern matching with the desirability of data abstraction. In fact, $-patterns are just a special case of views.
Reference: [Ram92] <author> Rajeev Raman. </author> <title> Eliminating Amortization: On Data Structures with Guaranteed Response Times. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, University of Rochester, </institution> <month> October </month> <year> 1992. </year> <pages> (pp. 4, 37, 39, 48) </pages>
Reference-contexts: Until this research, it was widely believed that amortization was incompatible with persistence <ref> [DST94, Ram92] </ref>. However, we show that memoization, in the form of lazy evaluation, is the key to reconciling the two. <p> It is interesting that debits arise in Tarjan's analysis of path compression since path compression is essentially an application of memoization to the find function. Amortization and Persistence Until this work, amortization and persistence were thought to be incompatible. Several researchers <ref> [DST94, Ram92] </ref> had noted that amortized data structures could not be made efficiently persistent using existing techniques for adding persistence to ephemeral data structures, such as [DSST89, Die89], for reasons similar to those cited in Section 3.2. <p> However, in some application areas, it is important to bound the running times of individual operations, rather than sequences of operations. In these situations, a worst-case data structure will often be preferable to an amortized data structure, even if the amortized data structure is simpler and faster overall. Raman <ref> [Ram92] </ref> identifies several such application areas, including * Real-time systems: In real-time systems, predictability is more important than raw speed [Sta88]. <p> Note the similarity to the potential function from the physicist's analysis in Section 3.5.2. Since this total is bounded by 2n, the collection as a whole contains only O (n) unevaluated suspensions, and therefore sort takes only O (n) worst-case time. 4.4 Related Work Eliminating Amortization Dietz and Raman <ref> [DR91, DR93, Ram92] </ref> have devised a framework for eliminating amortization based on pebble games, where the derived worst-case algorithms correspond to winning strategies in some game.
Reference: [San90] <author> David Sands. </author> <title> Complexity analysis for a lazy higher-order language. </title> <booktitle> In European Symposium on Programming, volume 432 of LNCS, </booktitle> <pages> pages 361376. </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1990. </year> <note> (p. 38) </note>
Reference-contexts: In fact, a better name for these debits might be anticredits. 38 Amortization and Persistence via Lazy Evaluation Time-Analysis of Lazy Programs Several researchers have developed theoretical frameworks for analyzing the time complexity of lazy programs <ref> [BH89, San90, San95, Wad88] </ref>. However, these frameworks are not yet mature enough to be useful in practice. One difficulty is that these frameworks are, in some ways, too general.
Reference: [San95] <author> David Sands. </author> <title> A nave time analysis and its theory of cost equivalence. </title> <journal> Journal of Logic and Computation, </journal> <volume> 5(4):495541, </volume> <month> August </month> <year> 1995. </year> <note> (p. 38) </note>
Reference-contexts: In fact, a better name for these debits might be anticredits. 38 Amortization and Persistence via Lazy Evaluation Time-Analysis of Lazy Programs Several researchers have developed theoretical frameworks for analyzing the time complexity of lazy programs <ref> [BH89, San90, San95, Wad88] </ref>. However, these frameworks are not yet mature enough to be useful in practice. One difficulty is that these frameworks are, in some ways, too general.
Reference: [Sar86] <author> Neil Sarnak. </author> <title> Persistent Data Structures. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, </institution> <address> New York University, </address> <year> 1986. </year> <note> (p. 58) </note>
Reference-contexts: The complete implementation appears in Figure 5.3. 5.5 Related Work Global Rebuilding Overmars introduced global rebuilding in [Ove83]. It has since been used in many situations, including real-time queues [HM81], real-time deques <ref> [Hoo82, GT86, Sar86, CG93] </ref>, catenable deques [BT95], and the order maintenance problem [DS87]. Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. <p> Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work <ref> [GT86, Sar86, CG93] </ref>. These implementations are all similar to techniques used to simulate multihead Turing machines [Sto70, FMR72, LS81]. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently.
Reference: [Sch92] <author> Berry Schoenmakers. </author> <title> Data Structures and Amortized Complexity in a Func tional Setting. </title> <type> PhD thesis, </type> <institution> Eindhoven University of Technology, </institution> <month> September </month> <year> 1992. </year> <note> (p. 15) </note>
Reference-contexts: Similarly, we can convert the physicist's method to the banker's method by converting potential to credits, and placing all credits on the root. It is perhaps surprising that the knowledge of locations in the banker's method offers no extra power, but the two methods are in fact equivalent <ref> [Tar85, Sch92] </ref>. The physicist's method is usually simpler, but it is occasionally convenient to take locations into account.
Reference: [Sch93] <author> Berry Schoenmakers. </author> <title> A systematic analysis of splaying. </title> <journal> Information Processing Letters, </journal> <note> 45(1):4150, January 1993. (p. 37) </note>
Reference-contexts: Examples of offending functions include list catenation and set union.) The idea that lazy evaluation could reconcile amortization and persistence first appeared, in rudimentary form, in [Oka95c]. The theory and practice of this technique was further developed in [Oka95a, Oka96b]. Amortization and Functional Data Structures In his thesis, Schoenmakers <ref> [Sch93] </ref> studies amortized data structures in a strict functional language, concentrating on formal derivations of amortized bounds using the traditional physicist's method. He avoids the problems of persistence by insisting that data structures only be used in a single-threaded fashion.
Reference: [SS86] <author> Leon Sterling and Ehud Shapiro. </author> <title> The Art of Prolog: Advanced Programming Techniques. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year> <note> (p. 82) </note>
Reference-contexts: However, this analogy can also be extended to other kinds of numbers. For example, difference lists <ref> [SS86] </ref> in Prolog support a notion of lists with negative length; appending a list of length 15 and a list of length 10 results in a list of length 5.
Reference: [SS90] <author> Jorg-Rudiger Sack and Thomas Strothotte. </author> <title> A characterization of heaps and its applications. Information and Computation, </title> <address> 86(1):6986, </address> <month> May </month> <year> 1990. </year> <note> (p. 64) </note>
Reference-contexts: Trees in numerical representations typically exhibit a very regular structure. For example, in binary numerical representations, all trees have sizes that are powers of 2. Three common kinds of trees that exhibit this structure are complete binary leaf trees [KD96], binomial trees [Vui78], and pennants <ref> [SS90] </ref>. Definition 6.1 (Complete binary leaf trees) A complete binary tree of rank 0 is a leaf and a complete binary tree of rank r &gt; 0 is a node with two children, each of which is a complete binary tree of rank r 1.
Reference: [ST85] <author> Daniel D. K. Sleator and Robert E. Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32(3):652686, </volume> <month> July </month> <year> 1985. </year> <note> (p. 21) </note>
Reference-contexts: Subsequent operations may then access the memoized result directly. This is exactly the desired behavior! Remark: In retrospect, the relationship between lazy evaluation and amortization is not surprising. Lazy evaluation can be viewed as a form of self-modification, and amortization often involves self-modification <ref> [ST85, ST86b] </ref>. However, lazy evaluation is a particularly disciplined form of self-modification not all forms of self-modification typically used in amortized ephemeral data structures can be encoded as lazy evaluation. <p> Lazy evaluation can be viewed as a form of self-modification, and amortization often involves self-modification [ST85, ST86b]. However, lazy evaluation is a particularly disciplined form of self-modification not all forms of self-modification typically used in amortized ephemeral data structures can be encoded as lazy evaluation. In particular, splaying <ref> [ST85] </ref> does not appear to be amenable to this technique. 3 3.3.2 A Framework for Analyzing Lazy Data Structures We have just shown that lazy evaluation is necessary to implement amortized data structures purely functionally. Unfortunately, analyzing the running times of programs involving lazy evaluation is notoriously difficult.
Reference: [ST86a] <author> Neil Sarnak and Robert E. Tarjan. </author> <title> Planar point location using persistent search trees. </title> <journal> Communications of the ACM, </journal> <volume> 29(7):669679, </volume> <month> July </month> <year> 1986. </year> <note> (p. 68) </note>
Reference-contexts: This reconstruction is called path copying <ref> [ST86a] </ref> and is necessary for persistence. fun update (Zero :: ts, i , y) = Zero :: update (ts, i , y) j update (One t :: ts, i , y) = if i &lt; size t then One (updateTree (t , i , y)) :: ts else One t ::
Reference: [ST86b] <author> Daniel D. K. Sleator and Robert E. Tarjan. </author> <title> Self-adjusting heaps. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(1):5269, </volume> <month> February </month> <year> 1986. </year> <pages> (pp. 12, 21, 103) </pages>
Reference-contexts: Algorithmics Both components of lazy evaluation delaying computations and memoizing the results have a long history in algorithm design, although not always in combination. The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues <ref> [ST86b, FT87] </ref>, and search trees [DSST89]. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression [HU73, TvL84]. <p> Subsequent operations may then access the memoized result directly. This is exactly the desired behavior! Remark: In retrospect, the relationship between lazy evaluation and amortization is not surprising. Lazy evaluation can be viewed as a form of self-modification, and amortization often involves self-modification <ref> [ST85, ST86b] </ref>. However, lazy evaluation is a particularly disciplined form of self-modification not all forms of self-modification typically used in amortized ephemeral data structures can be encoded as lazy evaluation. <p> Mergeable Heaps Many imperative implementations support insert , merge, and findMin in O (1) amortized time, and deleteMin in O (log n) amortized time, including binomial queues [KL93], Fibonacci heaps [FT87], relaxed heaps [DGST88], V-heaps [Pet87], bottom-up skew heaps <ref> [ST86b] </ref>, and pairing heaps [FSST86]. However, of these, only pairing heaps appear to retain their amortized efficiency when combined with lazy evaluation in a persistent setting [Oka96a], and, unfortunately, the bounds for pairing heaps have only been conjectured, not proved. Brodal [Bro95, Bro96] achieves equivalent worst-case bounds.
Reference: [Sta88] <author> John A. Stankovic. </author> <title> Misconceptions about real-time computing: A serious problem for next-generation systems. </title> <booktitle> Computer, </booktitle> <address> 21(10):1019, </address> <month> October </month> <year> 1988. </year> <note> (p. 39) </note>
Reference-contexts: In these situations, a worst-case data structure will often be preferable to an amortized data structure, even if the amortized data structure is simpler and faster overall. Raman [Ram92] identifies several such application areas, including * Real-time systems: In real-time systems, predictability is more important than raw speed <ref> [Sta88] </ref>.
Reference: [Sto70] <institution> Hans-Jorg Sto. K-band simulation von k-Kopf-Turing-maschinen. Computing, 6(3):309317, </institution> <year> 1970. </year> <note> (p. 58) </note>
Reference-contexts: Deques Hood [Hoo82] first modified the real-time queues of [HM81] to obtain real-time deques based on global rebuilding. Several other researchers later duplicated this work [GT86, Sar86, CG93]. These implementations are all similar to techniques used to simulate multihead Turing machines <ref> [Sto70, FMR72, LS81] </ref>. Hoogerwoord [Hoo92b] proposed amortized deques based on batched rebuilding, but, as always with batched rebuilding, his implementation is not efficient when used persistently. The real-time deques in Figure 5.3 first appeared in [Oka95c].
Reference: [Tar83] <author> Robert E. Tarjan. </author> <title> Data Structures and Network Algorithms, </title> <booktitle> volume 44 of CBMS Regional Conference Series in Applied Mathematics. Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, </address> <year> 1983. </year> <note> (p. 37) BIBLIOGRAPHY 145 </note>
Reference-contexts: The amortized cost of sort is therefore O (n) + (n) = O (n). 3.6 Related Work 37 3.6 Related Work Debits Some analyses using the traditional banker's method, such as Tarjan's analysis of path compression <ref> [Tar83] </ref>, include both credits and debits. Whenever an operation needs more credits than are currently available, it creates a credit-debit pair and immediately spends the credit. The debit remains as an obligation that must be fulfilled.
Reference: [Tar85] <author> Robert E. Tarjan. </author> <title> Amortized computational complexity. </title> <journal> SIAM Journal on Alge braic and Discrete Methods, </journal> <volume> 6(2):306318, </volume> <month> April </month> <year> 1985. </year> <pages> (pp. 14, 15) </pages>
Reference-contexts: Expensive operations decrease the accumulated savings and cheap operations increase it. The key to proving amortized bounds is to show that expensive operations occur only when the accumulated savings are sufficient to cover the cost, since otherwise the accumulated savings would become negative. Tarjan <ref> [Tar85] </ref> describes two techniques for analyzing ephemeral amortized data structures: the banker's method and the physicist's method. In the banker's method, the accumulated savings are represented as credits that are associated with individual locations in the data structure. These credits are used to pay for future accesses to these locations. <p> Similarly, we can convert the physicist's method to the banker's method by converting potential to credits, and placing all credits on the root. It is perhaps surprising that the knowledge of locations in the banker's method offers no extra power, but the two methods are in fact equivalent <ref> [Tar85, Sch92] </ref>. The physicist's method is usually simpler, but it is occasionally convenient to take locations into account.
Reference: [Tsa85] <author> Athanasios K. Tsakalidis. </author> <title> AVL-trees for localized search. Information and Con trol, </title> <address> 67(13):173194, </address> <month> October </month> <year> 1985. </year> <note> (p. 84) </note>
Reference-contexts: We will consider a simplification of their data structure in Chapter 8. Finger search trees <ref> [GMPR77, Tsa85] </ref> support not only random access in O (log d) worst-case time, but also insertions and deletions at arbitrary locations. Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin [Vui78] and extensively studied by Brown [Bro78].
Reference: [TvL84] <author> Robert E. Tarjan and Jan van Leeuwen. </author> <title> Worst-case analysis of set union algo rithms. </title> <journal> Journal of the ACM, </journal> <volume> 31(2):245281, </volume> <month> April </month> <year> 1984. </year> <pages> (pp. 12, 14) </pages>
Reference-contexts: The idea of delaying the execution of potentially expensive computations (often deletions) is used to good effect in hash tables [WV86], priority queues [ST86b, FT87], and search trees [DSST89]. Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression <ref> [HU73, TvL84] </ref>. Syntax for Lazy Evaluation Early versions of CAML [W + 90], a close cousin of Standard ML, offered support for lazy evaluation similar to the $-notation proposed here. <p> This freedom opens 14 Amortization and Persistence via Lazy Evaluation up a wide design space of possible solutions, and often yields new solutions that are simpler and faster than worst-case solutions with equivalent bounds. In fact, for some problems, such as the union-find problem <ref> [TvL84] </ref>, there are amortized solutions that are asymptotically faster than any possible worst-case solution (assuming certain modest restrictions) [Blu86].
Reference: [Vui74] <author> Jean Vuillemin. </author> <title> Correct and optimal implementations of recursion in a simple programming language. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9(3):332354, </volume> <month> December </month> <year> 1974. </year> <note> (p. 10) </note>
Reference-contexts: By including the StreamCell datatype in the signature, we have deliberately chosen to expose the internal representation in order to support pattern matching on streams. 2.2 Historical Notes Lazy Evaluation Wadsworth [Wad71] first proposed lazy evaluation as an optimization of normal-order reduction in the lambda calculus. Vuillemin <ref> [Vui74] </ref> later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy.
Reference: [Vui78] <author> Jean Vuillemin. </author> <title> A data structure for manipulating priority queues. </title> <booktitle> Communica tions of the ACM, </booktitle> <address> 21(4):309315, </address> <month> April </month> <year> 1978. </year> <journal> (pp. </journal> <volume> 61, 64, 68, 73, </volume> <pages> 84) </pages>
Reference-contexts: Call an implementation designed in this fashion a numerical representation. The typical representation of lists can be viewed as a numerical representation based on unary numbers. However, numerical representations based on binary numbers are also common; the best known of these is the binomial queues of Vuillemin <ref> [Vui78] </ref>. Incrementing a unary number takes O (1) time, so inserting an element into a unary representation also usually takes O (1) time. However, adding two unary numbers takes O (n) time, so combining two containers in a unary representation takes O (n) time. <p> Trees in numerical representations typically exhibit a very regular structure. For example, in binary numerical representations, all trees have sizes that are powers of 2. Three common kinds of trees that exhibit this structure are complete binary leaf trees [KD96], binomial trees <ref> [Vui78] </ref>, and pennants [SS90]. Definition 6.1 (Complete binary leaf trees) A complete binary tree of rank 0 is a leaf and a complete binary tree of rank r &gt; 0 is a node with two children, each of which is a complete binary tree of rank r 1. <p> Assuming that elements are not rearranged, each of the three kinds of trees can be linked or unlinked in O (1) time. We next describe two existing data structures in terms of this framework: the one-sided flexible arrays of Kaldewaij and Dielissen [KD96], and the binomial queues of Vuillemin <ref> [Vui78, Bro78] </ref>. 6.2 Binary Representations 65 @ @ @ A A A A C C C C C C C C fl fl fl fl fl fl fl fl (a) s s s s s A A A C C C C fl fl fl fl (c) pennant. <p> O (log n) worst-case time. lookup and update take at most O (log n) time to find the right tree, and then at most O (log n) time to find the right element in that tree, for a total of O (log n) worst-case time. 6.2.2 Binomial Heaps Binomial queues <ref> [Vui78, Bro78] </ref> are a classical implementation of mergeable priority queues. To avoid confusion with FIFO queues, we will henceforth refer to priority queues as heaps and binomial queues as binomial heaps. <p> Finger search trees [GMPR77, Tsa85] support not only random access in O (log d) worst-case time, but also insertions and deletions at arbitrary locations. Kaplan and Tarjan apply their methods to purely functional finger search trees in [KT96b]. Binomial Heaps Binomial heaps were introduced by Vuillemin <ref> [Vui78] </ref> and extensively studied by Brown [Bro78]. King [Kin94] showed that binomial heaps could be implemented elegantly in a purely functional language (in his case, Haskell).
Reference: [W + 90] <author> Pierre Weis et al. </author> <title> The CAML reference manual (version 2.6.1). </title> <type> Technical Report 121, </type> <institution> INRIA-Rocquencourt, </institution> <month> September </month> <year> 1990. </year> <note> (p. 12) </note>
Reference-contexts: Memoization, on the other hand, is the basic principle of such techniques as dynamic programming [Bel57] and path compression [HU73, TvL84]. Syntax for Lazy Evaluation Early versions of CAML <ref> [W + 90] </ref>, a close cousin of Standard ML, offered support for lazy evaluation similar to the $-notation proposed here. Rather than providing a single lazy constructor, however, CAML allowed any data constructor to be tagged as lazy, after which all applications of the constructor would be evaluated lazily.
Reference: [Wad71] <author> Christopher P. Wadsworth. </author> <title> Semantics and Pragmatics of the Lamda-Calculus. </title> <type> PhD thesis, </type> <institution> University of Oxford, </institution> <month> September </month> <year> 1971. </year> <note> (p. 10) </note>
Reference-contexts: By including the StreamCell datatype in the signature, we have deliberately chosen to expose the internal representation in order to support pattern matching on streams. 2.2 Historical Notes Lazy Evaluation Wadsworth <ref> [Wad71] </ref> first proposed lazy evaluation as an optimization of normal-order reduction in the lambda calculus. Vuillemin [Vui74] later showed that, under certain restricted conditions, lazy evaluation is an optimal evaluation strategy.
Reference: [Wad87] <author> Philip Wadler. </author> <title> Views: A way for pattern matching to cohabit with data abstraction. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 307313, </pages> <month> January </month> <year> 1987. </year> <note> (p. 129) </note>
Reference-contexts: The seductive allure of pattern matching leads many functional programmers to abandon sophisticated data structures in favor of simple, known representations such as lists, even when doing so causes an otherwise linear algorithm to explode to quadratic or even exponential time. Views <ref> [Wad87] </ref> and their successors [BC93, PPN96] offer one way of reconciling the convenience of pattern matching with the desirability of data abstraction. In fact, $-patterns are just a special case of views.
Reference: [Wad88] <author> Philip Wadler. </author> <title> Strictness analysis aids time analysis. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 119132, </pages> <month> January </month> <year> 1988. </year> <note> (p. 38) </note>
Reference-contexts: In fact, a better name for these debits might be anticredits. 38 Amortization and Persistence via Lazy Evaluation Time-Analysis of Lazy Programs Several researchers have developed theoretical frameworks for analyzing the time complexity of lazy programs <ref> [BH89, San90, San95, Wad88] </ref>. However, these frameworks are not yet mature enough to be useful in practice. One difficulty is that these frameworks are, in some ways, too general.

References-found: 106

