URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/ipps97-relbarrier.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/wormhole_pub.html
Root-URL: 
Email: Email: fsivaram,pandag@cis.ohio-state.edu Email: stunkel@watson.ibm.com  
Title: A Reliable Hardware Barrier Synchronization Scheme  
Author: Rajeev Sivaram Craig B. Stunkel Dhabaleswar K. Panda 
Address: P. O. Box 218 Columbus, OH 43210 Yorktown Heights, NY 10598  
Affiliation: Dept. of Computer and Information Science IBM T. J. Watson Research Center The Ohio State University  
Date: April 1997, Geneva.  
Note: To appear in the Proceedings of the 11th IEEE International Parallel Processing Symposium (IPPS '97),  
Abstract: Barrier synchronization is a crucial operation for parallel systems. Many schemes have been proposed in the literature to achieve fast barrier synchronization through software, hardware, or a combination of these mechanisms. However, few of these schemes emphasize fault-tolerant barrier operations. In this paper, we describe inexpensive support that can be added to network switches for achieving reliable hardware-based barrier synchronization while recovering from lost or corrupted messages. Necessary modifications to the switch architecture and the associated fault-tolerant message-passing protocols are presented. The protocols are optimized for the no-fault case while providing means to detect the failure of any step of the operation and to recover from it. The proposed scheme shows significant potential for use in parallel systems, especially the emerging systems based on networks of workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. B. Andrews, C. J. Beckmann, and D. K.Poulsen. </author> <title> Notifi cation and multicast networks for synchronization and co herence. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15:332350, </volume> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [1, 10] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. <p> Each input port contains a FIFO that can buffer 16 flits. A switch modified for packlet multicast and combining is shown in Fig. 2 (this is similar to the switch described in <ref> [1] </ref>). One additional input and output have been added to the in rier logic. ternal crossbar, providing paths to and from a new functional unit: the barrier unit. The barrier unit stores and processes barrier information for up to B concurrent barriers in the system.
Reference: [2] <author> C. J. Beckmann and C. D. Polychronopoulos. </author> <title> Broadcast Net works for Fast Synchronization. </title> <booktitle> In Proceedings of the In ternational Conference on Parallel Processing, </booktitle> <pages> pages I:220 224, </pages> <year> 1991. </year>
Reference-contexts: This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [2, 4, 8, 10] </ref>. The T3E [10] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessorthe T3D [5]maintained a separate network for this purpose.
Reference: [3] <author> W. J. Dally. </author> <title> Performance analysis of k-ary n-cube intercon nection networks. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(6):775 785, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: We assume that each switch in an interconnection network connects k flit-wide input ports to k flit-wide output ports via a k fi k crossbar. For assessing performance, we assume wormhole routing and flit-wide links, where a flit is the sub-packet unit upon which network flow-control is performed <ref> [3] </ref>. Each input port contains a FIFO that can buffer 16 flits. A switch modified for packlet multicast and combining is shown in Fig. 2 (this is similar to the switch described in [1]).
Reference: [4] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer: De signing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-32(2):175189, </volume> <month> Feb </month> <year> 1983. </year>
Reference-contexts: This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [2, 4, 8, 10] </ref>. The T3E [10] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessorthe T3D [5]maintained a separate network for this purpose. <p> In this section we describe an architecture which can perform these combining and multicast functions only for packlets. Note this is a much simpler requirement than supporting general packet multicast [11] because of the small fixed size of the packlet, and it is simpler than supporting general combining functions <ref> [4] </ref>. The Cray T3E [10] network already performs pack-let multicast (called eureka) and applies a combination of eureka and packlet combining hardware to perform barrier-sync. 2.1.
Reference: [5] <author> R. E. Kessler and J. L. Schwarzmeier. </author> <title> Cray T3D: A new dimension for Cray Research. </title> <booktitle> In Proc. 38th IEEE Int. Com puter Conf., </booktitle> <pages> pages 176182, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. fl This research is supported in part by NSF Grant MIP-9309627, NSF Career Award MIP-9502294 and an IBM Cooperative Fellowship. To address high latency, several machines <ref> [5, 6] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [6] <author> C. E. Leiserson et al. </author> <title> The network architecture of the Con nection Machine CM-5. </title> <booktitle> In Proc. 1992 Symp. Parallel Algo rithms and Architectures, </booktitle> <pages> pages 272285. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: This software methodology is easily portable to new systems, but results in a high latency for barrier operations. fl This research is supported in part by NSF Grant MIP-9309627, NSF Career Award MIP-9502294 and an IBM Cooperative Fellowship. To address high latency, several machines <ref> [5, 6] </ref> have implemented dedicated hardware networks to perform barrier-sync and some other important collective communication operations. This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations.
Reference: [7] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiproces sors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1):2165, </volume> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Barriers provide a means of ensuring that no process in a group of processes advances beyond a particular point in a computation, until all have arrived at that point <ref> [7] </ref>. The barrier is useful for bringing the group of processes to a known global state before proceeding to a new phase of computation. The efficiency of barrier-sync affects how small the grain-size of tasks in an application can become without significantly degrading performance.
Reference: [8] <author> D. K. Panda. </author> <title> Fast Barrier Synchronization in Wormhole k ary n-cube Networks with Multidestination Worms. </title> <booktitle> In Inter national Symposium on High Performance Computer Archi tecture, </booktitle> <pages> pages 200209, </pages> <year> 1995. </year>
Reference-contexts: This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [2, 4, 8, 10] </ref>. The T3E [10] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessorthe T3D [5]maintained a separate network for this purpose.
Reference: [9] <author> I. D. Scherson and C.-H. Chien. </author> <title> Least common ancestor networks. </title> <booktitle> In Proc. 7th Int. Parallel Processing Symp., </booktitle> <pages> pages 507513, </pages> <year> 1993. </year>
Reference-contexts: Setup of Barrier State We assume in Section 3 that the Parent Port, TreeInfo, and Combine fields of the barrier state are correctly initialized before any barriers of that group are initiated. Figure 5 illustrates a bidirectional MIN <ref> [9] </ref> network in which these fields have been initialized for a barrier group. The arrows indicate the path followed during the Combine or Ack phases (the Multicast phase is along the opposite path). This setup can be accomplished in various ways.
Reference: [10] <author> S. L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS-VII, </booktitle> <month> Sept. </month> <year> 1996. </year>
Reference-contexts: This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. A compromise is to provide hardware support for barrier-sync within the existing data network <ref> [2, 4, 8, 10] </ref>. The T3E [10] is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessorthe T3D [5]maintained a separate network for this purpose. <p> This approach results in much lower latency for barrier synchronization, but for a large parallel processor it may be expensive to maintain multiple networks for specialized operations. A compromise is to provide hardware support for barrier-sync within the existing data network [2, 4, 8, 10]. The T3E <ref> [10] </ref> is particularly noteworthy for embedding barrier-sync trees within the data network, because its predecessorthe T3D [5]maintained a separate network for this purpose. To our knowledge, the fault-tolerant operation of barrier-sync has not been specifically addressed in the literature, and this is the prime focus of our paper. <p> The hardware modifications and protocols are equally applicable to both direct and indirect (switch-based) networks. We do not focus on the parallel application performance improvement possible via network hardware barrier operations. Several papers have adequately addressed this topic <ref> [1, 10] </ref> and our hardware scheme should be similar in performance to other tree-based hardware combining schemes. <p> Note this is a much simpler requirement than supporting general packet multicast [11] because of the small fixed size of the packlet, and it is simpler than supporting general combining functions [4]. The Cray T3E <ref> [10] </ref> network already performs pack-let multicast (called eureka) and applies a combination of eureka and packlet combining hardware to perform barrier-sync. 2.1.
Reference: [11] <author> R. Sivaram, D. K. Panda, and C. B. Stunkel. </author> <title> Efficient Broad cast and Multicast on Multistage Interconnection Networks using Multiport Encoding. </title> <booktitle> In Proceedings of the Eighth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 3645, </pages> <month> Oct </month> <year> 1996. </year>
Reference-contexts: In this section we describe an architecture which can perform these combining and multicast functions only for packlets. Note this is a much simpler requirement than supporting general packet multicast <ref> [11] </ref> because of the small fixed size of the packlet, and it is simpler than supporting general combining functions [4]. The Cray T3E [10] network already performs pack-let multicast (called eureka) and applies a combination of eureka and packlet combining hardware to perform barrier-sync. 2.1. <p> This setup can be accomplished in various ways. Special circuit setup packets may be sent to every rectional MIN network switch involved in the associated multicast/combine tree, in a manner similar to ATM virtual path/channel setup. Alternatively, if the switches already support a means of general multicast <ref> [11] </ref>, a multicast packet could be used to carry the barrier tag, and the SRAM barrier state pointed to by the barrier tag would then store the identity of the calculated local output ports of the mul-ticast packet.
Reference: [12] <author> R. Sivaram, C. B. Stunkel, and D. K. Panda. </author> <title> Reliable Hardware Barrier Synchronization Schemes. </title> <type> Technical Re port OSU-CISRC-09/96-TR45, </type> <institution> The Ohio State University, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Our experiments show that in general the gain in performance of a hardware based scheme increases with system size. Even for a small system size of 16 nodes gains by factors of 1.5-3.75 can be achieved (as shown in <ref> [12] </ref>). We also studied the impact of switch size on barrier latency. The reader is requested to refer to [12] for further details. 5. Conclusions and Future Research In this paper, we have proposed a scheme to achieve reliable hardware-based barrier synchronization in parallel systems. <p> Even for a small system size of 16 nodes gains by factors of 1.5-3.75 can be achieved (as shown in <ref> [12] </ref>). We also studied the impact of switch size on barrier latency. The reader is requested to refer to [12] for further details. 5. Conclusions and Future Research In this paper, we have proposed a scheme to achieve reliable hardware-based barrier synchronization in parallel systems. Reliability is achieved by incorporating inexpensive support to the network switches/routers.
Reference: [13] <author> C. B. Stunkel, D. G. Shea, B. Abali, et al. </author> <title> The SP2 high performance switch. </title> <journal> IBM System Journal, </journal> <volume> 34(2):185204, </volume> <year> 1995. </year>
Reference-contexts: The barrier unit stores and processes barrier information for up to B concurrent barriers in the system. To simplify the diagram, we have not shown any flow-control signals; our simulations in Section 4 assume a token-based flow-control similar to that used in the IBM SP2 <ref> [13] </ref>. 2.2. The Barrier Unit The barrier unit, detailed in Fig. 3, contains a B-entry SRAM. Entry i in the SRAM stores the current barrier state for the barrier group i, and is addressed by packlets with barrier tag i. <p> The protocols described in Section 3 accomplish this in cooperation with switch barrier units. We do not consider permanent faults, because these faults are typically handled by system reconfiguration software in commercial systems <ref> [13] </ref>. Depending on the permanent fault, reconfiguration software may need to reinitialize the barrier state for affected barriers. 3. Protocols to Overcome Message Loss In this section we present the protocols used by the switches, root and non-root processes so as to ensure that a barrier completes successfully.
Reference: [14] <author> H. Xu, Y.-D. Gui, and L. M. Ni. </author> <title> Optimal Software Multicast in Wormhole-Routed Multistage Networks. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 703712, </pages> <year> 1994. </year>
Reference-contexts: Alternative Schemes We first present two alternative implementations of barrier synchronizationsoftware and hardware schemes. Software Scheme (UMin) Various methods for software multicast on bidi-MINs have been proposed. The U-Min algorithm <ref> [14] </ref> uses a contention free binomial tree for performing multicast with the source node as root. The multicast proceeds by having intermediate destinations act as secondary sources in succeeding phases of the multicast. A binomial tree constructed for multicast can also be used for barrier synchronization [15]. <p> Once the root of the binomial tree learns that all participants have reached the barrier, it informs everyone that they can proceed from the barrier by sending a multicast (using the binomial tree) 4 . Under the first scheme, we use the binomial tree constructed using the U-Min algorithm <ref> [14] </ref> to perform software barrier synchronization. Under this approach the worst case barrier performance occurs when the process that arrives at the barrier last is a leaf node in the binomial tree that is farthest from the root. The best case occurs when the last arriving processor is the root.
Reference: [15] <author> H. Xu, P. K. McKinley, and L. Ni. </author> <title> Efficient Implementation of Barrier Synchronization in Wormhole-routed Hypercube Multicomputers. </title> <journal> Journal of Parallel and Distributed Com puting, </journal> <volume> 16:172184, </volume> <year> 1992. </year> <month> 7 </month>
Reference-contexts: The IBM SP2, the Intel Paragon, and most networks of workstations implement a totally software-based messaging approach: a total data-less exchange is performed by sending several phases of messages via the system message-transmission routines <ref> [15] </ref>. Each of these messages incurs a significant software startup overhead, and the number of messages sent is proportional to log 2 N , where N is the number of processes in the group. <p> The U-Min algorithm [14] uses a contention free binomial tree for performing multicast with the source node as root. The multicast proceeds by having intermediate destinations act as secondary sources in succeeding phases of the multicast. A binomial tree constructed for multicast can also be used for barrier synchronization <ref> [15] </ref>. The basic idea is to use the reverse binomial tree to collect information about the participants that have reached the barrier.
References-found: 15

