URL: http://phobos.cs.ucdavis.edu:8001/papers/chinagen.ps.gz
Refering-URL: http://phobos.cs.ucdavis.edu:8001/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Genetic Classification System via Bayesian Induction P(Mauve Class) P(Fuschia Class) P(Azure Class) Class Framitz
Author: Kenrick J. Mock 
Keyword: Bayesian Induction Class Fuzul  
Note: P(Class)  P(C)P(e |C) i 1  may also serve as classifier systems [7]. However, not very much work has been performed  is (0.5)(0.9)(0.4), while the probability that the object belongs in the Fuzul class is (0.5)(0.2)(0.3). Since  
Address: Davis, California 95616  
Affiliation: Department of Computer Science University of California, Davis  
Pubnum: 0.5 0.5  
Email: mock@cs.ucdavis.edu  
Date: 0.9 0.2  
Abstract: Key words : AI, Learning, Genetic Algorithms, simple product. Given a set of training data, a table of probabilities can be easily constructed to approximate P(e|C). For example, if we are given an object with two possible classifications (Framitz or Fuzul) determined by the single feature of color (Mauve, Fuschia, or Azure), then a table with the following probabilities might be constructed based upon the training data: Abstract This paper examines the feasibility of merging Bayesian induction with a genetic algorithm to create a Genetic/Bayesian hybrid classification system. Two schemes are discussed: 1) Using a genetic algorithm to optimize a single table of Bayesian probabilities assuming conditional independence, and 2) Using a genetic algorithm to optimize a set of probability tables to learn disjunctive concepts. The schemes have been tested on Fisher's Iris data and the boolean multiplexer problem with results comparable or better than existing classification techniques. The system may be applied to any classification task, for example, predicting heart disease or financial forecasting. When given a new object to classify, under conditional independence we simply choose the class which maximizes the following expression derived from the Bayes Rule: 1 Previous Work 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Booker, L.B. and Goldberg, D.E and Holland, J.H. </author> <title> "Classifier Systems and Genetic Algorithms," </title> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990, </year> <pages> 404-427. </pages>
Reference: [2] <author> Davis, Lawrence. </author> <title> Handbook of Genetic Algorithms. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> NY, </address> <year> 1991. </year>
Reference: [3] <author> Duda, R.O. and Hart, P.E. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> NY, </address> <year> 1973. </year> <title> 8 Summary By combining genetic algorithms with Bayesian induction, classifier systems can be created which perform competitively with other classification schemes. In particular, the scheme performs better than Bayesian induction alone and offers a more general classifier system than rule-based systems. With the modification of allowing multiple probability tables for each individual and a new "Don't Know" class, it is possible to learn disjunctive concepts. </title>
Reference-contexts: At the end of K generations, the single individual with the best fitness is used as the classifier system. Bayesian classification under conditional independence on the Iris domain yields a training error of 3.3% and a test error of 5.0%. Although the Iris data is not linearly separable <ref> [3] </ref>, the linear-discriminating Bayesian classifier still performs fairly well, indicating that the data is almost linearly separable. The genetic algorithm used throughout this paper conforms to the "Pitt" approach developed by De Jong and Smith at the University of Pittsburgh [4].
Reference: [4] <author> Grefenstette, John. </author> <title> "Multilevel Credit Assignment in a Genetic Learning System," </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms , (1987), </booktitle> <pages> 202-209. </pages>
Reference-contexts: Although the Iris data is not linearly separable [3], the linear-discriminating Bayesian classifier still performs fairly well, indicating that the data is almost linearly separable. The genetic algorithm used throughout this paper conforms to the "Pitt" approach developed by De Jong and Smith at the University of Pittsburgh <ref> [4] </ref>. This approach creates a complete system which handles all classification for each individual in the population; i.e., each individual classifies all possible cases. When the genetic algorithm is finished, the single best individual is chosen as the classifier.
Reference: [5] <author> Parodi, Alexandre and Bonelli, Pierre. </author> <title> "The Animat and the Physician," </title> <booktitle> Proceedings of the First International Conference on Simulation of Adaptive Behavior , (1991), </booktitle> <pages> 50-57. </pages> <note> 9 Future Work </note>

Reference: [7] <author> Koza, John. </author> <title> "Genetic Evolution and CoEvolution of Computer Programs," </title> <booktitle> Artificial Life II , (1990), </booktitle> <pages> 603-629. </pages>
Reference: [8] <author> Weiss, Sholom and Kulikowski, </author> <title> Casimir. </title> <publisher> Computer Systems That Learn . Morgan Kaufmann, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Similar to a decision tree, rule-based systems are limited to drawing "rectangles" in the solution space. On the other hand, the Bayesian probability tables are capable of drawing "rectangles" or "lines" <ref> [8] </ref>. The linear nature of Bayesian induction under conditional independence is well-known and proven in [8]. To show that the probability tables can perform the same classification as a rule-based system, we need to consider only the case where probabilities of 0 and 1 are allowed. <p> Similar to a decision tree, rule-based systems are limited to drawing "rectangles" in the solution space. On the other hand, the Bayesian probability tables are capable of drawing "rectangles" or "lines" <ref> [8] </ref>. The linear nature of Bayesian induction under conditional independence is well-known and proven in [8]. To show that the probability tables can perform the same classification as a rule-based system, we need to consider only the case where probabilities of 0 and 1 are allowed. The rule A=0 becomes two entries in the probability table; P (A=0|C)=1, and P (A=1|C)=0.
Reference: [9] <author> Wilson, Stewart. </author> <title> "Classifier Systems and the Animat Problem," </title> <booktitle> Machine Learning V2 N3 , (1987). </booktitle>
Reference: [10] <author> Zhou, Hayong. </author> <title> "CSM: A Computational Model of Cumulative Learning," </title> <booktitle> Machine Learning V2 N5 , (1990). </booktitle> <address> Acknowledgments </address>
References-found: 9

