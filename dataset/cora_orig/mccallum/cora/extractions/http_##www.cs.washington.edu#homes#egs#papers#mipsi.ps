URL: http://www.cs.washington.edu/homes/egs/papers/mipsi.ps
Refering-URL: http://www.cs.washington.edu/homes/egs/mipsi/mipsi.html
Root-URL: 
Title: Measuring Limits of Fine-grained Parallelism  
Author: Emin Gun Sirer Prof. Andrew W. Appel 
Date: January 23, 1997  
Affiliation: Princeton University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Andrew Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, New York, </address> <year> 1992. </year>
Reference-contexts: Instrumentation on a parallel machine, on the other hand, might have a probe effect and disturb the data being collected. And finally, the most important drawback is that instrumentation does not work with programs that generate code on the fly, such as ML, Smalltalk or Lisp programs [3] <ref> [1] </ref> [?] (unless one modifies each and every one of them to generate instrumented code). 1.5 Previous Work Mipsi is largely based on SPIM by James Larus at the University of Wis-consin [7].
Reference: [2] <author> Andrew W. Appel. </author> <title> Callee-save registers in continuation-passing style. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 5 </volume> <pages> 191-221, </pages> <year> 1992. </year>
Reference-contexts: Simple rearrangement of code can reportedly account for up to 20% change in total time <ref> [2] </ref>. We naturally extend our scope to look at future architectures that can take advantage of low-level parallelism during such wait-states.
Reference: [3] <author> Robert Harper, David B. MacQueen, and Robin Milner. </author> <title> Standard ML. </title> <year> 1985. </year>
Reference-contexts: Instrumentation on a parallel machine, on the other hand, might have a probe effect and disturb the data being collected. And finally, the most important drawback is that instrumentation does not work with programs that generate code on the fly, such as ML, Smalltalk or Lisp programs <ref> [3] </ref> [1] [?] (unless one modifies each and every one of them to generate instrumented code). 1.5 Previous Work Mipsi is largely based on SPIM by James Larus at the University of Wis-consin [7].
Reference: [4] <author> Norman P. Jouppi. </author> <title> The nonuniform distribution of instruction-level and machine parallelism and its effect on performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 12(38) </volume> <pages> 1645-1658, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: This allows simulating machines that are more powerful than the existing parallel architectures. In fact, mipsi contains schedulers for both superscalar/VLIW type architectures and superpipelined machines that are far more complicated than their current counterparts. <ref> [4] </ref>, [8]. Finally, Mipsi gives some perquisites to the regular user in addition to researchers; namely, speed in instruction level simulation, symbolic debugging capabilities (a la gdb) and an option to trace system calls.
Reference: [5] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: Mipsi's implementation closely resembles the logical breakdown of the MIPS design <ref> [5] </ref>. At the core of the program is the combined decoder/execution unit. No preprocessing is done on the instruction stream; it is decoded on the fly. This is accomplished by a straight-forward switch statement that does the decoding and executes the appropriate code.
Reference: [6] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on par allelism. </title> <booktitle> Proceedings of the SIGPLAN'92 Conference on Programming Language Design and Implementation, SIGPLAN Notices, </booktitle> <pages> pages 46-57, </pages> <year> 1992. </year>
Reference-contexts: His approach corresponds more to a superscalar/VLIW one. Most of our data, on the other hand, is collected with a superpipelined architecture. Indeed, our model may impose more resource limitations on parallelism than his, though the difference is not too important. Lam and Wilson describe similar work <ref> [6] </ref> wherein they make some strong assumptions about compiler technology, especially with regards to loop optimization. Consequently, they achieve fairly high levels of parallelism. 4.3 Scheduling In order to explore the parallelism of a program, we simulate it using mipsi and schedule each instruction as it is executed.
Reference: [7] <author> James R. Larus. SPIM S20: </author> <title> A MIPS R2000 Simulator. </title> <institution> University of Wisconsin, Madison, WI, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: not work with programs that generate code on the fly, such as ML, Smalltalk or Lisp programs [3] [1] [?] (unless one modifies each and every one of them to generate instrumented code). 1.5 Previous Work Mipsi is largely based on SPIM by James Larus at the University of Wis-consin <ref> [7] </ref>.
Reference: [8] <author> Devin B. Theobald, Guang R. Gao, and Laurie J. Hendren. </author> <title> On the limits of program parallelism and its smoothability. </title> <journal> SIGMICRO Newsletter, </journal> <volume> 1(23) </volume> <pages> 10-19, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: This allows simulating machines that are more powerful than the existing parallel architectures. In fact, mipsi contains schedulers for both superscalar/VLIW type architectures and superpipelined machines that are far more complicated than their current counterparts. [4], <ref> [8] </ref>. Finally, Mipsi gives some perquisites to the regular user in addition to researchers; namely, speed in instruction level simulation, symbolic debugging capabilities (a la gdb) and an option to trace system calls.
Reference: [9] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <journal> SIGPLAN No tices, </journal> <volume> 4(26) </volume> <pages> 176-188, </pages> <month> April </month> <year> 1991. </year> <month> 36 </month>
Reference-contexts: Finally, we have looked at how varying such parameters of the machine as instruction latency or scheduler windowing size affects overall cycles required. 4.2 Previous Work Wall indicates that "even with impossibly good techniques, average parallelism rarely exceeds 7, with 5 more common <ref> [9] </ref>." Our results verify his findings. Wall uses trace analysis and does not have the benefit of access to the machine state. Thus he does not perform memory aliasing and must make conservative decisions in the absence of dynamic information. On the other hand, Wall's analyzer does register renaming.
References-found: 9

