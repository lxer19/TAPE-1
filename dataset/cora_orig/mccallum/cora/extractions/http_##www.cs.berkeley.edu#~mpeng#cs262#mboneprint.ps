URL: http://www.cs.berkeley.edu/~mpeng/cs262/mboneprint.ps
Refering-URL: http://www-plateau.cs.berkeley.edu/courseware/cs294/fall97/projects/
Root-URL: 
Email: msreedha-@cs.berkeley.edu  
Title: End-to-End Measurements on the MBone  
Author: Micah Peng Sylvia Ratnasamy Sreedhar Mukkamalla -mpeng, sylviar, 
Abstract: The Multicast Backbone (MBone) has proven to be an ideal framework on which shared applications can be built. These applications and the protocols that support them can greatly benefit by knowledge of the characteristics of multicast traffic. In this paper we present our measurement gathering tool: the Multicast Network Probe Daemon (M-NPD), and analyze the data gathered by performing active, end-to-end measurements at 7 MBone sites running our M-NPD. The M-NPD provides us with a measurement tool that is flexible, easily deployed and scalable. We use the data gathered to analyze the loss behavior of different receivers, occurrences of packet reordering and replication. 
Abstract-found: 1
Intro-found: 1
Reference: [Amir] <author> E. Amir, S. McCanne, M. Vetterli. </author> <title> "A Layered DCT Coder for Internet Video". </title> <booktitle> IEEE International Conference on Image Processing. </booktitle> <month> Sept., </month> <year> 1996. </year> <institution> Lausanne, Switzerland. </institution>
Reference-contexts: Developers of tools targeted at large-scale MBone sessions will have to keep this in mind. It remains to be seen how well schemes such as layered encodings <ref> [Amir] </ref> will solve this problem. The presence of even a small proportion of very lossy receivers can dramatically reduce the probability of a packet reaching all receivers. In large-scale MBone sessions, it is reasonable to assume that this will be the case [Handley].
Reference: [Bolot 93] <author> J-C. Bolot. </author> <title> "End-to-End Packet Delay and Loss Behavior in the Internet". </title> <booktitle> Proc. SIGCOMM '93, </booktitle> <pages> pp. 289-298, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Bolot et al. <ref> [Bolot 93, 95] </ref> studied temporally correlated loss in fixed bit-rate audio streams. The work was related to the work by Yajnik, and found that, with respect to temporally correlated loss, losses appeared to be isolated.
Reference: [Bolot 95] <author> J-C. Bolot, H.Crepin, A. Vega-Garcia. </author> <title> "Analysis of Audio Packet Loss in the Internet". </title> <booktitle> Proc. 1995 Workshop on Network and Operating System Support for Audio and Video, </booktitle> <pages> pp. 163-174, </pages> <year> 1995. </year>
Reference-contexts: The graph indicates a predominance of solitary losses (a single lost packet preceded and followed by successfully received packets) as was observed by [Yajnik] and <ref> [Bolot 95] </ref>. Bursts of long length do not account for a significant fraction of the lost packets for this receiver (as is indicated by the second series in Figure 5.1). lengths of loss bursts for the host at Maryland.
Reference: [Casner] <author> S. Casner. </author> <title> "Frequently Asked Questions on the Multicast Backbone (MBONE)". </title> <institution> USC Information Sciences Institute. </institution> <month> December </month> <year> 1994. </year> <month> ftp://ftp.isi.edu/mbone/faq.txt. </month>
Reference-contexts: In Section 5 we present an analysis of the end-to-end measurements collected. Sections 6 completes the paper with conclusions and future work. 2 MBone Background and Related Work 2.1 MBone Background The MBone is an experimental virtual network for multicasting data to any connected host <ref> [Casner] </ref>. It was designed to test the feasibility of various shared applications such as streaming audio and video, shared whiteboards and real-time conferencing. The underlying feature of these applications is one-to-many delivery without explicitly creating one-to-one connections from the sender to the receivers.
Reference: [Floyd] <author> S. Floyd, V. Jacobson. </author> <title> "The Synchronisation of Periodic Routing Messages". </title> <journal> ACM Transactions on Networking, V.2 N. </journal> <volume> 2, </volume> <pages> pp. 122-136. </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Figure 5.6 shows the loss pattern occurring at the receiver at Stanford for one trace. The reason for these losses is not clear. In the past, periodicity of losses in the network has been attributed to router bugs and the synchronization of routing updates <ref> [Floyd] </ref>. However, this is unlikely to be the case in our observations, since the duration of each loss burst is around 24 seconds.
Reference: [Handley] <author> M. Handley. </author> <title> "An Examination of MBone Performance". UCL and ISI. </title> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Finally, Handley examined MBone performance from the point of view of a single endsystem by logging RTP/RTCP packets and using an mtrace daemon to trace routes through the MBone in <ref> [Handley] </ref>. This work extensively examined packet loss, with the goal of characterizing performance to aid in the design of multicast applications. Paxson performed extensive measurements of end-to-end Internet packet dynamics in [Paxson 96, 97]. <p> Another drawback in our experiments is that all the sites are at universities and as such are members of the networking research community. Thus, these sites cannot be considered as a representative cross-section of sites found in typical MBone sessions. <ref> [Handley] </ref> reports higher loss rates when monitoring MBone sessions at real receiver sites although the pattern of loss remains the same. 4 Data Collection Our measurements were performed by running the measurement daemons (outlined in the Section 3) at 7 different sites on the Mbone over a period of one week, <p> Handley mentions this issue when he discusses processor starvation suffered by vic <ref> [Handley] </ref>. <p> We think this issue bears further investigation. 5.6 Periodic Packet Losses In some traces, we have observed receivers experiencing periodic packet loss. Others have reported similar observations <ref> [Yajnik, Handley] </ref>, where receivers saw packet losses for about 0.6 seconds occurring at 30 second intervals. In our traces, some receivers experienced losses of up to 300 packets at 60 second intervals. Figure 5.6 shows the loss pattern occurring at the receiver at Stanford for one trace. <p> The presence of even a small proportion of very lossy receivers can dramatically reduce the probability of a packet reaching all receivers. In large-scale MBone sessions, it is reasonable to assume that this will be the case <ref> [Handley] </ref>. In the context of reliable multicast, this implies that retransmission would be required for a majority of packets. We think reliable multicast protocols could possibly benefit from policing techniques that prevent a single lossy receiver from degrading overall session performance.
Reference: [Paxson 96] <author> V.Paxson. </author> <title> "End-to-End Routing Behavior in the Internet". </title> <booktitle> Proc.SIGCOMM 1996, </booktitle> <pages> pp. 25-38, </pages> <year> 1996. </year>
Reference-contexts: This work extensively examined packet loss, with the goal of characterizing performance to aid in the design of multicast applications. Paxson performed extensive measurements of end-to-end Internet packet dynamics in <ref> [Paxson 96, 97] </ref>. In this work, Paxson actively performed unicast experiments by controlling both the sender and the receiver processes. End-to-end behavior of the Internet was characterized by examining unusual events such as packet reordering, replication, corruption, asymmetrical routes and other network pathologies. <p> Previous studies on MBone traffic have all been limited to passive monitoring and recording of multicast packets. Our study differs from previous work in that the M-NPD software allows us to actively control the traffic injected into the network <ref> [Paxson 96, 97] </ref>. This design decision allows us to experiment with the characteristics of the injected data (from the source site) such as the packet length, inter-packet spacing and transmission rate. <p> Depending on the number of "alive" group members periodically multicast "alive" messages. Logs all incoming data Periodically mails all logs back to us at UCB. 3.3 Controlling the M-NPD Existing measurement frameworks as described in [Yajnik] and <ref> [Paxson 96, 97] </ref> rely on a centralized control program that remotely controls the measurement software running at the end sites. The control program periodically sends control commands to the data collection daemon to start/stop a measurement session or otherwise control them. <p> For example, when the network has lost connectivity between the host running the control program and a site potentially conducting a measurement session, the network can predict that no measurement will occur. The effect of this is a tendency to underestimate the prevalence of network connectivity problems <ref> [Paxson 96] </ref>. In order to avoid these problems and ensure the scalability of our framework we chose not to rely on a centralized control program for the scheduling of experiments. <p> Hence, any assumptions of the route between a sending host and a receiving host being a FIFO queue are violated and out-of-order delivery of packets is possible. Out-of-order delivery of packets can be viewed as an indicator of route stability - its presence indicates route "flaps" or "flutter" <ref> [Paxson 96] </ref>. (Its absence, however, does not indicate the stability of routes.) We count reordered packets in the same manner as Paxson [Paxson 97]. For each arriving packet p i , we check whether it was sent after the last non-reordered packet. If so, it becomes the last non-reordered packet.
Reference: [Paxson 97] <author> V.Paxson. </author> <title> "End-to-End Internet Packet Dynamics". </title> <institution> University of California, Berkeley, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: Out-of-order delivery of packets can be viewed as an indicator of route stability - its presence indicates route "flaps" or "flutter" [Paxson 96]. (Its absence, however, does not indicate the stability of routes.) We count reordered packets in the same manner as Paxson <ref> [Paxson 97] </ref>. For each arriving packet p i , we check whether it was sent after the last non-reordered packet. If so, it becomes the last non-reordered packet. Otherwise, its arrival is counted as an instance of packet reordering. <p> 9 out of the 27 experiments, while the receivers at UCLA and MIT only experienced reordering in 2 experiments. 5.4 Packet Replication Unlike the case of unicast, for which it is difficult to imagine how replication of packets (i.e. the network delivering multiple copies of the same packet) could occur <ref> [Paxson 97] </ref>, it is conceivable that in a multicast network, such as the MBone, packet replication could occur due to routing pathologies and route instability. Of the experiments we conducted, 12 had instances of packet replication. <p> Such measurements could easily be made using the M-NPD framework, especially since the M-NPDs already timestamp packets. A fundamental property of a network connection is its bottleneck bandwidth that sets the upper limit on how quickly the network can deliver the sender's data to the receiver <ref> [Paxson 97] </ref>. <p> After completing transmission through the bottleneck their spacing will be exactly the bottleneck bandwidth. Paxson <ref> [Paxson 97] </ref> discusses the use of "packet pair" and "packet bunch" techniques to estimate the bottleneck bandwidth. A similar study for the MBone using the M-NPD measurement framework would be very interesting as this gives an indication of how fast a connection can possibly transmit data.
Reference: [Yajnik] <author> M. Yajnik, J. Kurose, D. Towsley. </author> <title> "Packet Loss Correlation in the MBone Multicast Network." </title> <booktitle> Proc IEEE Global Internet Conference. </booktitle> <address> London, </address> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Since it is less mature than the Internet, MBone instabilities and characteristics are not widely understood; only a few studies have been conducted to measure the characteristics of the MBone. 2.2 Related Work The work of Yajnik et al. <ref> [Yajnik] </ref> analyzed packet loss correlation in the MBone. In that work, packet loss data was collected by using dedicated monitoring software at 17 sites. The data were examined to determine spatial and temporal correlations of packet loss. <p> Depending on the number of "alive" group members periodically multicast "alive" messages. Logs all incoming data Periodically mails all logs back to us at UCB. 3.3 Controlling the M-NPD Existing measurement frameworks as described in <ref> [Yajnik] </ref> and [Paxson 96, 97] rely on a centralized control program that remotely controls the measurement software running at the end sites. The control program periodically sends control commands to the data collection daemon to start/stop a measurement session or otherwise control them. <p> The graph indicates a predominance of solitary losses (a single lost packet preceded and followed by successfully received packets) as was observed by <ref> [Yajnik] </ref> and [Bolot 95]. Bursts of long length do not account for a significant fraction of the lost packets for this receiver (as is indicated by the second series in Figure 5.1). lengths of loss bursts for the host at Maryland. <p> One way to reduce its impact would be to use as large buffers as possible at the receivers and to run the measurement daemons only on very lightly loaded hosts. Yajnik et al. <ref> [Yajnik] </ref> reported that they monitored multicast sessions at two hosts on the same LAN to determine whether there were any occurrences of packets that were lost by one receiver and not the other and found that end-host loss was zero or very close to zero in all cases. <p> We think this issue bears further investigation. 5.6 Periodic Packet Losses In some traces, we have observed receivers experiencing periodic packet loss. Others have reported similar observations <ref> [Yajnik, Handley] </ref>, where receivers saw packet losses for about 0.6 seconds occurring at 30 second intervals. In our traces, some receivers experienced losses of up to 300 packets at 60 second intervals. Figure 5.6 shows the loss pattern occurring at the receiver at Stanford for one trace. <p> The incorporation of a route tracing facility into the M-NPD framework would be very useful in studies of the spatial correlation of loss <ref> [Yajnik] </ref>. Lastly, it would be useful to develop an automatic trace analysis tool to complement the M-NPD trace gathering tool. 7 Acknowledgements This work would not have been possible without the help of several people who let us conduct our measurements at their sites.
References-found: 9

