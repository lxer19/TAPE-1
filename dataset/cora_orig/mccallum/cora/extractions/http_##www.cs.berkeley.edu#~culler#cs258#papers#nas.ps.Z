URL: http://www.cs.berkeley.edu/~culler/cs258/papers/nas.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~culler/cs258/
Root-URL: 
Abstract: NAS PARALLEL BENCHMARK RESULTS 10-94 David H. Bailey, Eric Barszcz, Leonardo Dagum and Horst D. Simon 1 NAS Technical Report NAS-94-001 October 1994 Abstract The NAS Parallel Benchmarks have been developed at NASA Ames Research Center to study the performance of parallel supercomputers. The eight benchmark problems are specified in a "pencil and paper" fashion. In other words, the complete details of the problem to be solved are given in a technical document, and except for a few restrictions, benchmarkers are mostly free to select the language constructs and implementation techniques best suited for a particular system. This paper presents performance results of various systems using the NAS Parallel Benchmarks. These results represent the best results that have been reported to us for the specific systems listed. Some changes and clarifications to the benchmark rules are also described. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Bailey, E. Barszcz, J. T. Barton, D. S. Browning, R. L. Carter, L. Dagum, R. A. Fatoohi, P. O. Frederickson, T. A. Lasinski, R. S. Schreiber, H. D. Simon, V. Venkatakrishnan, and S. K. Weeratunga, </author> <title> "The NAS Parallel Benchmarks", </title> <journal> Intl. Journal of Supercomputer Applications, v. </journal> <volume> 5, no. </volume> <month> 3 (Fall </month> <year> 1991), </year> <pages> pp. 63 - 73. </pages>
Reference-contexts: Space does not permit a complete description of these benchmark problems. A more detailed description of these benchmarks, together with the rules and restrictions associated with the benchmarks, may be found in <ref> [1] </ref>. The full specification of the benchmarks is given in [2]. Sample Fortran programs implementing the NPB on a single processor system are available as an aid to implementors.
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon, eds., </author> <title> "The NAS Parallel Benchmarks", </title> <type> NASA Technical Memorandum 103863, </type> <institution> Ames Research Center, Moffett Field, </institution> <address> CA 94035-1000, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The choice of data structures, processor allocation and memory usage are generally left open to the discretion of the implementer. The eight problems consist of five "kernels" and three "simulated computational fluid dynamics (CFD) applications". Each of these is defined fully in <ref> [2] </ref>. The five kernels are relatively compact problems, each emphasizing a particular type of numerical computation. Compared with the simulated CFD applications, they can be implemented fairly readily and provide insight as to the general levels of performance that can be expected on these specific types of numerical computations. <p> Space does not permit a complete description of these benchmark problems. A more detailed description of these benchmarks, together with the rules and restrictions associated with the benchmarks, may be found in [1]. The full specification of the benchmarks is given in <ref> [2] </ref>. Sample Fortran programs implementing the NPB on a single processor system are available as an aid to implementors. <p> The run times in each case are elapsed time of day figures, measured in accordance with the specifications given in <ref> [2] </ref>. With the exception of the Integer Sort benchmark, these standard flop counts were determined by using the hardware performance monitor on either the Cray Y-MP or the Cray C90, and we believe that they are close to the minimal counts required for these problems. <p> Therefore this scheme is no longer allowed and results employing this loophole will not be reported in future releases of this report. A strict interpretation of the benchmark specification <ref> [2] </ref> precludes this scheme since it is clearly stated that the conjugate gradient method will be used to compute the solution z to Az = x, and as part of this method the vector q must be computed via the product q = Ap. <p> An unfortunate inconsistency has developed in the specification of the Class A size CG benchmark. The original benchmark description (as written in RNR Technical Report RNR-91-002) specified 15 iterations, however subsequent publications (specifically <ref> [2] </ref>) specify 25 iterations.
Reference: [3] <author> D.H. Bailey and P.O. Frederickson, </author> <title> "Performance Results for Two of the NAS Parallel Benchmarks", </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque NM, </address> <pages> pp. 166-173, </pages> <month> Nov 18-22, </month> <year> 1991. </year>
Reference-contexts: This problem is typical of many "Monte-Carlo" applications. Since it requires almost no communication, in some sense this benchmark provides an estimate of the upper achievable limits for floating point performance on a particular system. Discussion on the parallel implementation of this benchmark may be found in <ref> [3] </ref>. Results for the embarrassingly parallel benchmark are shown in Table 2. Not all systems exhibit high rates on this problem. <p> This kernel performs the essence of many "spectral" codes. It is a good test of long-distance communication performance. Discussion on the parallel implementation of this benchmark may be found in <ref> [3] </ref>. The rules of the NAS Parallel Benchmarks specify that assembly-coded, library routines may be used to perform matrix multiplication and one-dimensional, two-dimensional or three-dimensional FFTs. Thus this benchmark is somewhat unique in that computational library routines may be legally employed. Results are shown in Table 5.
Reference: [4] <author> E. Barszcz, R. Fatoohi, V. Venkatakrishnan, and S. Weeratunga, </author> <title> "Solution of Regular Sparse Triangular Linear Systems on Vector and Distributed Memory Multiprocessors", </title> <type> Tech Report RNR-93-07, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA 94035, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: This problem exhibits a somewhat limited amount of parallelism compared to the next two. Discussion of the serial algorithm underlying this benchmark may be found in [15]. Discussion of the parallel algorithms may be found in <ref> [4] </ref>. The second simulated CFD application is called the scalar pentadiagonal (SP) benchmark. In this benchmark, multiple independent systems of non-diagonally dominant, scalar pentadiagonal equations are solved. The third simulated CFD application is called the block tridiagonal (BT) benchmark.
Reference: [5] <author> G. Bhanot, K. Jordan, J. Kennedy, J. Richardson, D. Sandee and M. Zagha, </author> <title> "Implementing the NAS Parallel Benchmarks on the CM-2 and CM200 Supercomputers", </title> <institution> Thinking Machines Corp, </institution> <address> Cambridge, MA 02142. </address>
Reference-contexts: However, references to technical papers describing these methods have been included whenever such papers are available. In particular, details of the implementation of these benchmarks on the TC2000, the CM2, the CM200, the SP-1 and the IBM Cluster may be found in <ref> [5, 6, 11, 13] </ref>. General discussion on architectural requirements for the benchmarks may be found in [8]. Readers are referred to these documents for full details.
Reference: [6] <author> S. Breit, W. Celmaster, W. Coney, R. Foster, B. Gaiman, G. Montry and C. Selvidge, </author> <title> "The Role of Computational Balance in the Implementation of the NAS parallel Benchmarks on the BBN TC2000 Computer", </title> <booktitle> FED-Vol. 156, CFD Algorithms and Applications, ASME, </booktitle> <year> 1993. </year>
Reference-contexts: However, references to technical papers describing these methods have been included whenever such papers are available. In particular, details of the implementation of these benchmarks on the TC2000, the CM2, the CM200, the SP-1 and the IBM Cluster may be found in <ref> [5, 6, 11, 13] </ref>. General discussion on architectural requirements for the benchmarks may be found in [8]. Readers are referred to these documents for full details.
Reference: [7] <author> L. Dagum, </author> <title> "Parallel Integer Sorting with Medium and Fine-Scale Parallelism", </title> <journal> International Journal of High Speed Computing, </journal> <volume> Vol. 5, No. 4, </volume> <pages> pp. 503-522, </pages> <year> 1993. </year>
Reference-contexts: The sorting operation is used to reassign particles to the appropriate cells. This benchmark tests both integer computation speed and communication performance. For discussion on general parallel algorithms for this benchmark see <ref> [7] </ref>. This problem is unique in that floating point arithmetic is not involved. Significant data communication, however, is required. Results are shown in Table 6. Intel Paragon results are due to to S. Gupta and B. 9 Computer System Date No.
Reference: [8] <author> M. Fillo, </author> <title> Architectural Support for Scientific Applications on Multicomputers, Hartung Gorre Verlag, </title> <booktitle> Series in Microelectronics, </booktitle> <volume> Volume 27, </volume> <pages> Konstanz, </pages> <address> Germany, </address> <year> 1993. </year>
Reference-contexts: In particular, details of the implementation of these benchmarks on the TC2000, the CM2, the CM200, the SP-1 and the IBM Cluster may be found in [5, 6, 11, 13]. General discussion on architectural requirements for the benchmarks may be found in <ref> [8] </ref>. Readers are referred to these documents for full details.
Reference: [9] <author> B. Hendrickson, R. Leland, and S. Plimpton, </author> <title> "An Efficient Parallel Algorithm for Matrix-Vector Multiplication", </title> <type> Sandia Report SAND92-2765, </type> <institution> Sandia National Lab, </institution> <address> Albuquerque, NM 87185, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: The irregular communication requirement of this benchmark is evidently a challenge for all systems. Results are shown in Table 4. CM-2 results are due to J. Richardson of TMC. Intel iPSC/860 and nCUBE-2 results are by B. Hendrickson, R. Leland, and S. Plimpton of Sandia National Laboratory <ref> [9] </ref>. Paragon results are due to S. Gupta of Intel, R. van de Geijn of U.T. Austin and John Lewis of BCS [10]. Cray EL and C90 results are due to M. Zagha of Carnegie Mellon University. VPP500 results are due to J. Wang of Fujitsu.

Reference: [11] <author> V. K. Naik, </author> <title> "Performance Issues in Implementing NAS Parallel Benchmark Applications on IBM SP-1", </title> <type> Research Report, T.J. </type> <institution> Watson Research Center, IBM, </institution> <note> (in preparation) 1993. </note>
Reference-contexts: However, references to technical papers describing these methods have been included whenever such papers are available. In particular, details of the implementation of these benchmarks on the TC2000, the CM2, the CM200, the SP-1 and the IBM Cluster may be found in <ref> [5, 6, 11, 13] </ref>. General discussion on architectural requirements for the benchmarks may be found in [8]. Readers are referred to these documents for full details.
Reference: [12] <author> T. H. Pulliam. </author> <title> "Efficient Solution Methods for the Navier-Stokes Equations", </title> <booktitle> Lecture Notes for The Von Karman Institute for Fluid Dynamics Lecture Series, Jan. </booktitle> <volume> 20 - 24, </volume> <year> 1986. </year>
Reference-contexts: SP and BT are similar in many respects, but there is a fundamental difference with respect to the communication to computation ratio. Discussion of the serial algorithm underlying this benchmark may be found in <ref> [12] </ref>. Performance figures for the three simulated CFD applications are shown in Tables 7, 8 and 9. Timings are cited as complete run times, in seconds, as with the other benchmarks. A complete solution of the LU benchmark requires 250 iterations. For the SP benchmark, 400 iterations are required.
Reference: [13] <author> F. Sukup and J. Fritscher, </author> <title> "Efficiency Evaluation of Some Parallelization Tools on a Workstation Cluster Using the NAS Parallel Benchmarks", </title> <institution> Computing Center, Vienna University of Technology, Vienna, Austria. </institution>
Reference-contexts: However, references to technical papers describing these methods have been included whenever such papers are available. In particular, details of the implementation of these benchmarks on the TC2000, the CM2, the CM200, the SP-1 and the IBM Cluster may be found in <ref> [5, 6, 11, 13] </ref>. General discussion on architectural requirements for the benchmarks may be found in [8]. Readers are referred to these documents for full details.
Reference: [14] <author> S. White, </author> <title> "NAS Benchmark on Virtual Parallel Machines", </title> <type> Master's thesis, </type> <institution> Emory University, </institution> <year> 1993. </year>
Reference-contexts: Cownie and K. Pickard of Meiko and L. Meadows of Portland Group. SX-3 results are due to G.M. Sastri of NEC. Power Challenge and Power Indigo results are due to J. Richardson of SGI. Distributed workstation results are due to S. White of Emory University <ref> [14] </ref> except for the SGI results which are due to D. Browning of the NAS System Development branch. <p> Montry of Southwest Software. VPP500 results are due to J.C.H. Wang of Fujitsu. CS-2 results are due to J. Cownie of Meiko. SX-3 results are due to G.M. Sastri of NEC. Distributed workstation results are due to S. White of Emory University <ref> [14] </ref> using PVM 2.4 and Ethernet except where noted otherwise. 3.3 Conjugate Gradient (CG) Benchmark In this benchmark, a conjugate gradient method is used to compute an approximation to the smallest eigenvalue of a large, sparse, symmetric positive definite matrix. <p> KSR1 and KSR2 results are due to S. Breit and J. Middlecoff of KSR. CS-2 results are due to D. Daniel of Meiko. Power Challenge and Power Indigo results are due to F. Shakib of SGI. Distributed workstation results are due to S. White of Emory University <ref> [14] </ref> using PVM 2.4 and Ethernet except where noted otherwise. 3.4 3-D FFT PDE (FT) Benchmark In this benchmark a 3-D partial differential equation is solved using FFTs. This kernel performs the essence of many "spectral" codes. It is a good test of long-distance communication performance.
Reference: [15] <author> S. Yoon, D. Kwak, and L. Chang, </author> <title> "LU-SGS Implicit Algorithm for Implicit Three Dimensional Navier-Stokes Equations with Source Term", </title> <type> AIAA Paper 89-1964-CP, </type> <institution> American Institute of Aeronautics and Astronautics, </institution> <address> Washington, D.C., </address> <year> 1989. </year>
Reference-contexts: This problem represents the computations associated with a newer class of implicit CFD algorithms, typified at NASA Ames by the code "INS3D-LU". This problem exhibits a somewhat limited amount of parallelism compared to the next two. Discussion of the serial algorithm underlying this benchmark may be found in <ref> [15] </ref>. Discussion of the parallel algorithms may be found in [4]. The second simulated CFD application is called the scalar pentadiagonal (SP) benchmark. In this benchmark, multiple independent systems of non-diagonally dominant, scalar pentadiagonal equations are solved. The third simulated CFD application is called the block tridiagonal (BT) benchmark.
Reference: [16] <author> M. Zagha and G.E. Blelloch, </author> <title> "Radix Sort for Vector Multiprocessors", </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 712-721, </pages> <address> New York, NY, </address> <month> Nov. </month> <year> 1991. </year> <month> 33 </month>
Reference-contexts: CM-2, CM-200 and MasPar results use a library sorting routine. Cray Y-MP results are due to CRI. Cray C-90 and EL results are due to M. Zagha of Carnegie Mellon University using a radix sort optimized for interleaved memories <ref> [16] </ref>. VPP500 results are due to B. Elton of Fujitsu. RS6000-590, SP-1 and SP-2 results are due to F. Gustavson, M. Zubair and R. Agarwal of IBM. KSR1 and KSR2 results are due to C.
References-found: 15

