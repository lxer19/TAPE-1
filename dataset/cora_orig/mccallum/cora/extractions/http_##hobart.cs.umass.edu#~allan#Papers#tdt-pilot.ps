URL: http://hobart.cs.umass.edu/~allan/Papers/tdt-pilot.ps
Refering-URL: http://hobart.cs.umass.edu/~allan/Papers/tdt-pilot-abstract.html
Root-URL: 
Title: Topic Detection and Tracking Pilot Study Final Report continues in a new project involving larger
Author: James Allan Jaime Carbonell George Doddington Jonathan Yamron and Yiming Yang James Allan, UMass Brian Archibald, CMU Doug Beeferman, CMU Adam Berger, CMU Ralf Brown, CMU Jaime Carbonell, CMU Ira Carp, Dragon Bruce Croft, UMass, George Doddington, DARPA Larry Gillick, Dragon Alex Hauptmann, CMU John Lafferty, CMU Victor Lavrenko, UMass Xin Liu, CMU Steve Lowe, Dragon Paul van Mulbregt, Dragon Ron Papka, UMass Thomas Pierce, CMU Jay Ponte, UMass Mike Scudder, UMass Charles Wayne, DARPA Jon Yamron, Dragon Yiming Yang, 
Note: DARPA, Dragon Systems, and CMU The TDT work  
Address: Amherst, CMU,  CMU  
Affiliation: UMass  
Abstract: Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative to investigate the state of the art in finding and following new events in a stream of broadcast news stories. The TDT problem consists of three major tasks: (1) segmenting a stream of data, especially recognized speech, into distinct stories; (2) identifying those news stories that are the first to discuss a new event occurring in the news; and (3) given a small number of sample news stories about an event, finding all following stories in the stream. The TDT Pilot Study ran from September 1996 through October 1997. The primary participants were DARPA, Carnegie Mellon University, Dragon Systems, and the University of Massachusetts at Amherst. This report summarizes the findings of the pilot study. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Beeferman, A. Berger, and J. Lafferty, </author> <title> A model of lexical attraction and repulsion, </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> Madrid, Spain, </address> <year> 1997. </year>
Reference-contexts: The method used to construct the adaptive model is to treat the static trigram model as a default distribution, and then to add certain features based on semantic word classes in order to form a family of conditional exponential models. The details of this model are described in <ref> [1] </ref>. Since the adaptive model should improve as it sees more material from the current topic (or event), a segment boundary is likely to exist when the adaptive model suddenly shows a dip in performancea lower assigned probability to the observed wordscompared to the short-range model.
Reference: 2. <author> A. Berger, S. Della Pietra, and V. Della Pietra, </author> <title> A maximum entropy approach to natural language processing, </title> <booktitle> Computational Linguistics, </booktitle> <address> 22(1):3971, </address> <year> 1996. </year>
Reference-contexts: Feature Induction The procedure for combining the evidence in the language models and the lexical features is based on a statistical framework called feature induction for random fields and exponential models <ref> [2, 4] </ref>. The idea is to construct a model which assigns to each position in the data stream a probability that a boundary belongs at that position. This probability distribution is incrementally constructed as a log-linear model that weighs different features of the data.
Reference: 3. <author> A. Bookstein and S.T. Klein, </author> <title> Detecting content-bearing words by serial clustering, </title> <booktitle> Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 319327, </pages> <year> 1995. </year>
Reference-contexts: The model is trained using segmented data. Unknown word probabilities were handled with a very simple smoothing method. Additional Features. In addition to the word probabilities, other features were modeled. These included sentence length (which would be implicit in a word based segmenter), serial clustering tendency <ref> [3] </ref>, and distance from previous occurrence. Each of these features was measured as a standard score, and state probabilities were estimated from the training data. These three features yielded a very slight improvement over the words alone.
Reference: 4. <author> S. Della Pietra, V. Della Pietra, and J. Lafferty, </author> <title> Inducing features of random fields, </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(4):380393, </volume> <month> April </month> <year> 1997. </year>
Reference-contexts: Feature Induction The procedure for combining the evidence in the language models and the lexical features is based on a statistical framework called feature induction for random fields and exponential models <ref> [2, 4] </ref>. The idea is to construct a model which assigns to each position in the data stream a probability that a boundary belongs at that position. This probability distribution is incrementally constructed as a log-linear model that weighs different features of the data. <p> The training algorithm for choosing the parameters to minimize the divergence is the Improved Iterative Scaling algorithm presented in <ref> [4] </ref>. <p> In this manner, an exponential model is incrementally built up using the most informative features. See <ref> [4] </ref> for details. Results The exponential models derived using feature induction give a probability p (b = YES j !) that a boundary exists at a given position in the text. In order to actually segment text, this probability is computed in an on-line manner, scanning the text sequentially.
Reference: 5. <author> W.B. Croft and D.J. Harper, </author> <title> Using probabilistic models of document retrieval without relevance information, </title> <journal> Journal of Documentation, </journal> <volume> 37:285295, </volume> <year> 1979. </year>
Reference-contexts: The first method makes use of the technique of local context analysis (LCA) [16]. LCA was developed as a method for automatic expansion of ad hoc queries for information retrieval. It is somewhat like the method of local feedback <ref> [5] </ref> but has been shown to be more effective and more robust.
Reference: 6. <author> D.R. Cutting, D.R. Karger, J.O. Pedersen, and J.W. Tukey, Scatter/Gather: </author> <title> a Cluster-based Approach to Browsing Large Document Collections, </title> <booktitle> In 15th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'92), </booktitle> <year> 1992. </year> <title> 9 The TDT2 study will focus on this mode of detection. 10 Effectiveness numbers should be viewed skeptically because of the very small sample size of the test corpus. </title>
Reference-contexts: When setting the threshold to a value of 0.23, we obtained a partition of 5,907 clusters which yielded the optimal result evaluated using the 25 events labeled by humans (see Section ). Group-average based clustering The core part of CMU's method is an agglomerative algorithm named Group Average Clustering <ref> [8, 6] </ref> which maximizes the average pairwise similarity between stories in each cluster.
Reference: 7. <author> M.A. Hearst, </author> <title> Multi-paragraph Segmentation of Expository Text, </title> <booktitle> in Proceedings of the ACL, </booktitle> <year> 1994. </year>
Reference-contexts: There is a relatively small but varied body of previous work that has addressed the problem of text segmentation. This work includes methods based on semantic word networks [10], vector space techniques from information retrieval <ref> [7] </ref>, and decision tree induction algorithms [11].
Reference: 8. <author> T. Feder and D. Greene, </author> <title> Optimal Algorithms for Approximate Clustering. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on the Theory of Computing (STOC), </booktitle> <pages> pp. 434-444, </pages> <year> 1988. </year>
Reference-contexts: When setting the threshold to a value of 0.23, we obtained a partition of 5,907 clusters which yielded the optimal result evaluated using the 25 events labeled by humans (see Section ). Group-average based clustering The core part of CMU's method is an agglomerative algorithm named Group Average Clustering <ref> [8, 6] </ref> which maximizes the average pairwise similarity between stories in each cluster.
Reference: 9. <author> S. Katz, </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400401, </volume> <month> March, </month> <year> 1997. </year>
Reference-contexts: The Reuters experiments use a trigram model that has a vocabulary of 20,000 words and is trained on approximately 38 million words of Wall Street Journal data. Both models use the Katz backoff scheme <ref> [9] </ref> for smoothing. The method used to construct the adaptive model is to treat the static trigram model as a default distribution, and then to add certain features based on semantic word classes in order to form a family of conditional exponential models.
Reference: 10. <author> H. Kozima, </author> <title> Text Segmentation Based on Similarity between Words, </title> <booktitle> in Proceedings of the ACL, </booktitle> <year> 1993. </year>
Reference-contexts: There is a relatively small but varied body of previous work that has addressed the problem of text segmentation. This work includes methods based on semantic word networks <ref> [10] </ref>, vector space techniques from information retrieval [7], and decision tree induction algorithms [11].
Reference: 11. <author> D.J. Litman and R.J. Passonneau, </author> <title> Combining Multiple Knowledge Sources for Discourse Segmentation, </title> <booktitle> in Proceedings of the ACL, </booktitle> <year> 1995. </year>
Reference-contexts: There is a relatively small but varied body of previous work that has addressed the problem of text segmentation. This work includes methods based on semantic word networks [10], vector space techniques from information retrieval [7], and decision tree induction algorithms <ref> [11] </ref>. The research on segmentation carried out under the TDT study has led to the development of several new and complementary approaches that do not directly use the methods of this previous work, although all of the approaches share a common rationale and motivation. 2.1.
Reference: 12. <author> J.M. Ponte and W.B. Croft, </author> <title> Text Segmentation by Topic, </title> <booktitle> in Proceedings of the First European Conference on Research and Advanced Technology for Digitial Libraries, </booktitle> <pages> pp. 120129, </pages> <year> 1997. </year>
Reference-contexts: The original sentence is then replaced with the LCA concepts and the effect is that sentences which originally had few or perhaps no words in common will typically have many LCA concepts in common. The original LCA method was derived from that described in <ref> [12] </ref>. The text is indexed at the sentence level using offsets to encode the positions of the LCA features. For example, suppose the feature O. J. Simpson occurs in sentence 1, 3, and 10.
Reference: 13. <author> G. Salton, </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: As implementation, CMU uses the mechanisms provided in SMART, a benchmarking retrieval system developed by the Salton group at Cornell <ref> [13] </ref>. The term preprocessing includes removal of stop words, stemming, and then term weighting.
Reference: 14. <editor> C.J. van Rijsbergen, </editor> <booktitle> Information Retrieval (2nd edition), </booktitle> <address> But-terworths, London, </address> <year> 1979. </year>
Reference-contexts: Five evaluation measures are reported in this study: miss rate, false alarm rate, recall, precision, and the F 1 measure. The miss and false alarm rates were the official measures of the pilot study. The F 1 measure <ref> [14] </ref> was used as a way of balancing recall and precision, in a way that each of them is given equal weight. A more general form of the F-measure is F fi (r; p) = fi 2 p+r where fi is the parameter allowing differential weighting of p and r.
Reference: 15. <author> E.M. Voorhees, </author> <title> Implementing agglomerative hierarchic clustering algorithms for use in document retrieval, </title> <booktitle> Information Processing & Management, 22:6, </booktitle> <pages> 465-476, </pages> <year> 1986. </year>
Reference-contexts: The GAC algorithm has a quadratic complexity in both time and space, although envisioned improvements based on <ref> [15] </ref> and other work at CMU should yield sub-quadratic space complexity, without increasing time complexity. In order to reduce the effective complexity and to exploit natural temporal groupings of events in news-streams CMU used the following modified form of GAC clustering: 1.
Reference: 16. <author> J. Xu and W.B. Croft, </author> <title> Query Expansion Using Local and Global Document Analysis, </title> <booktitle> in Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 411, </pages> <year> 1996. </year>
Reference-contexts: UMass Approach Content Based LCA Segmentation UMass has developed two largely complementary segmentation methods. The first method makes use of the technique of local context analysis (LCA) <ref> [16] </ref>. LCA was developed as a method for automatic expansion of ad hoc queries for information retrieval. It is somewhat like the method of local feedback [5] but has been shown to be more effective and more robust.
References-found: 16

