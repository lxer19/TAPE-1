URL: http://www.cs.washington.edu/homes/sgberg/quals/papers/TorPaper.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/sgberg/quals/index.html
Root-URL: 
Email: ftorerik,eggersg@cs.washington.edu  
Phone: (206) 543-9349 (206) 543-2118  
Title: Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations. algorithms eliminated an
Author: Tor E. Jeremiassen and Susan J. Eggers 
Note: The  
Address: Seattle, Washington 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Abstract: We have developed compiler algorithms that analyze coarse-grained, explicitly parallel programs and restructure their shared data to minimize the number of false sharing misses. The algorithms analyze the per-process data accesses to shared data, use this information to pinpoint the data structures that are prone to false sharing and choose an appropriate transformation to reduce it. 
Abstract-found: 1
Intro-found: 1
Reference: [AG88] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications un der mach. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality <ref> [AG88] </ref>, is coherency overhead caused by false sharing [TLH90, EJ91]. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although they do not actually share data, they incur its costs, because coherency operations are often cache block-based. <p> of the total running time. 4 Transformations In order to eliminate or significantly reduce the number of false sharing misses, data must be restructured so that (1) data that is only, or overwhelmingly, accessed by one processor is grouped together, and (2) write shared data objects with no processor locality <ref> [AG88] </ref> do not share cache lines. Two transformations, group and transpose and indirection [Anoa], address item (1); the third, padding , is aimed at item (2). Group and transpose physically groups data together by changing the layout of the data structures in memory.
Reference: [Anoa] <author> Anonymous. </author> <note> Reference omitted for anonymous reviewing. 14 </note>
Reference-contexts: For these reasons we have automated the elimination of false sharing. We have developed and incorporated into the Parafrase-2 [PGH + 89] source-to-source restructurer a series of compiler-directed algorithms <ref> [Anoa, Anob] </ref> and a suite of transformations that restructure shared data at compile time. Our algorithms analyze explicitly parallel programs, producing information about their cross-processor memory reference patterns that identifies data structures susceptible to false sharing and then chooses appropriate transformations to eliminate it. <p> The compiler analysis involves three separate stages. The first determines which sections of code each process executes by computing its control flow graph <ref> [Anoa] </ref>. The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob]. <p> Two transformations, group and transpose and indirection <ref> [Anoa] </ref>, address item (1); the third, padding , is aimed at item (2). Group and transpose physically groups data together by changing the layout of the data structures in memory. It groups together vectors in which adjacent elements are accessed by different processors and then transposes the group.
Reference: [Anob] <author> Anonymous. </author> <note> Reference omitted for anonymous reviewing. </note>
Reference-contexts: For these reasons we have automated the elimination of false sharing. We have developed and incorporated into the Parafrase-2 [PGH + 89] source-to-source restructurer a series of compiler-directed algorithms <ref> [Anoa, Anob] </ref> and a suite of transformations that restructure shared data at compile time. Our algorithms analyze explicitly parallel programs, producing information about their cross-processor memory reference patterns that identifies data structures susceptible to false sharing and then chooses appropriate transformations to eliminate it. <p> The first determines which sections of code each process executes by computing its control flow graph [Anoa]. The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them <ref> [Anob] </ref>. The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis [Bar78, Ban79, Mye81, CK88b] and static profiling [Wal91] on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). <p> Once all stages of the static analysis have been performed, we use a number of heuristics to detect which data structures are susceptible to false sharing and which transformation should be applied to eliminate it <ref> [Anob] </ref>. 5 Methodology The static analysis and false sharing detection algorithms were implemented as separate passes in Parafrase-2 [PGH + 89]. We perform two kinds of experiments to measure the effects of the data transformations on the programs in our workload. False sharing reductions were measured using trace-driven simulation.
Reference: [Anoc] <author> Anonymous. </author> <note> Reference omitted for anonymous reviewing. </note>
Reference-contexts: They were included in our workload to see if our static analysis could improve upon programmer 3 Infinite caches can be used to approximate very large (on the order of several megabytes) second level caches <ref> [Anoc] </ref>. 6 Program Description Lines of C LocusRoute VLSI standard cell router 6709 Maxflow maximum flow in a directed graph 810 Mp3d rarefied fluid flow 1653 Topopt topological optimization 2206 Pverify logical verification 2759 Water n-body molecular dynamics 1451 Table 1: Benchmarks used in our study. efforts to reduce false sharing.
Reference: [Ban79] <author> J.P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Sixth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob]. The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling [Wal91] on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). In this paper we only discuss those aspects of the static analysis that clarify how it effects the detection of false sharing.
Reference: [Bar78] <author> J. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> CACM, </journal> <volume> 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob]. The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling [Wal91] on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). In this paper we only discuss those aspects of the static analysis that clarify how it effects the detection of false sharing.
Reference: [Car88] <author> F. J. Carrasco. </author> <title> A parallel maxflow implementation. </title> <type> CS411 Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: In Pverify [MDWSV87], Topopt [DN87] and Maxflow <ref> [Car88] </ref> the programmers made no attempt to improve locality, because of the added complexity of (multiprocessor) cache-conscious programming. 6 Results We present two sets of results to describe the impact of our analysis and transformations on the benchmarks.
Reference: [CK88a] <author> David Callahan and Ken Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel pro gramming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (5):517-550, 1988. 
Reference-contexts: To improve the accuracy we allow multiple regular section descriptors to exist for each array. Instead of eagerly merging descriptors <ref> [CK88a, HK91] </ref> (and losing information in the process), we only merge descriptors when very little or no information will be lost, or when the number of descriptors for a single array exceeds some small preset limit. (None of the arrays used in our benchmarks required more than 10 descriptors).
Reference: [CK88b] <author> K.D. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 57-66, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob]. The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling [Wal91] on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). In this paper we only discuss those aspects of the static analysis that clarify how it effects the detection of false sharing.
Reference: [DN87] <author> S. Devadas and A. R. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <booktitle> In IEEE Transactions on Computer-Aided Design, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: In Pverify [MDWSV87], Topopt <ref> [DN87] </ref> and Maxflow [Car88] the programmers made no attempt to improve locality, because of the added complexity of (multiprocessor) cache-conscious programming. 6 Results We present two sets of results to describe the impact of our analysis and transformations on the benchmarks.
Reference: [DSR + 93] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P. Stenstrom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Third, the Sequent Symmetry has a low memory latency relative to the processor speed, so reductions in bus traffic have less impact than on the KSR. 7 Related Work Related work describes either hardware solutions or compile time techniques that reorganize control structures rather shared data. Dubois et al. <ref> [DSR + 93] </ref> reduced false sharing by either delaying invalidations (at the sender, receiver or both) until special acquire or release instructions are executed, or performing invalidations on a word basis. Delaying invalidations both at the sender and the receiver and invalidating cache subblocks consistently perform well.
Reference: [EJ91] <author> S.J. Eggers and T.E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 377-381, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality [AG88], is coherency overhead caused by false sharing <ref> [TLH90, EJ91] </ref>. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although they do not actually share data, they incur its costs, because coherency operations are often cache block-based. <p> In some coarse-grain, explicitly parallel applications, misses due to false sharing comprise between 40% and 90% of all cache misses (over block sizes ranging from 8 to 256 bytes) <ref> [EJ91] </ref>. False sharing is caused by a mismatch between the memory layout of write-shared data and the cross-processor memory reference pattern to it. <p> Manually changing the placement of this data to better conform to the memory reference pattern (based on profiles derived from trace-driven simulations) can reduce false sharing misses by up to 75% <ref> [TLH90, EJ91] </ref>. <p> Notice that the miss rate axis varies across the programs. same cache block. Our static analysis accurately detects per-process accesses to arrays and structures, but cannot identify when write-shared scalars cause false sharing. Topopt and Pverify had previously been analyzed and transformed manually <ref> [EJ91] </ref>. The static analysis achieved reductions in false sharing that well exceeded the previous results. For Topopt the manual approach succeeded in eliminating an average of 59% of false sharing misses, while our compile time approach 8 eliminated 79%. Similar figures for Pverify were 76% versus 91%.
Reference: [EKKL90] <author> S.J. Eggers, D.R. Keppel, E.J. Koldinger, and H.M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: False sharing reductions were measured using trace-driven simulation. Each program was traced (both before and after shared data was transformed) on a 20-processor Sequent Symmetry [LT88], using a software tracing tool for parallel programs <ref> [EKKL90] </ref>. Cache miss rates were analyzed with a multiprocessor simulator that emulates a simple, shared memory architecture. The processors are assumed to be RISC-like, with a 32 KB first level cache and an infinite second level cache 3 .
Reference: [GP91] <author> M. Gupta and D. A. Padua. </author> <title> Effects of program parallelization and stripmining transformations on cache performance in a multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 301-304, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Their restructuring algorithm applies loop transformations (such as loop distribution) and data layout transformations (accessing arrays in row or column major order), according to a coherency cost function. The restructuring provided a 25% improvement in execution time of the loops for a 64 KB cache. Gupta and Padua <ref> [GP91] </ref> also examined sequential programs that were automatically parallelized at the loop level. They strip-mined the loops to the size of the cache block and assigned each strip to a different processor.
Reference: [Gra93] <author> E. D. Granston. </author> <title> Toward a compile-time methodology for reducing false sharing and commu nication traffic in shared virtual memory systems. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Sixth Workshop on Languages and Compilers for Parallelism, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The first approach requires a change in the instruction set architecture, as well as hardware to implement invalidation buffers at each processor node. The second requires an invalid bit per word in the cache block, and causes more invalidations when the writes exhibit spatial locality. Granston <ref> [Gra93] </ref> presented a theory to identify and eliminate page-level sharing between processors that occur in parallel do-loops. The transformations select blocking and alignment factors that cause minimal overlap between sets of pages accessed by different processors.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Per-process control flow analysis (stage 1) detects the first case, and summary side-effect analysis and process differentiating variables 1 (stage 3) help detect the second. The side-effect analysis represents the sections of each array that each process accesses using bounded regular section descriptors 2 to describe the index expressions <ref> [HK91] </ref>. When a regular section descriptor indicates that a process differentiating variable is used in the index expressions for array accesses, we test whether the descriptor identifies disjoint sections of the array for different values of the variable. The array is implicitly partitioned across processes if the sections are disjoint. <p> To improve the accuracy we allow multiple regular section descriptors to exist for each array. Instead of eagerly merging descriptors <ref> [CK88a, HK91] </ref> (and losing information in the process), we only merge descriptors when very little or no information will be lost, or when the number of descriptors for a single array exceeds some small preset limit. (None of the arrays used in our benchmarks required more than 10 descriptors).
Reference: [JD91] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Fourth Workshop on Languages and Compilers for Parallelism. </booktitle> <publisher> Springer Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: The transformations select blocking and alignment factors that cause minimal overlap between sets of pages accessed by different processors. No results have been reported, as the theory and transformations have yet to be implemented. Ju and Dietz <ref> [JD91] </ref> restructured a program fragment of several loops accessing array elements. Their restructuring algorithm applies loop transformations (such as loop distribution) and data layout transformations (accessing arrays in row or column major order), according to a coherency cost function.
Reference: [KSR92] <institution> Kendall Square Research. KSR-1 Principles of Operation, </institution> <year> 1992. </year>
Reference-contexts: These programs are typical of those that currently execute on small to medium scale, bus-based multiprocessors, both commercially (e.g., Sequent Symmetry [LT88] and the KSR1 <ref> [KSR92] </ref>) and in research environments (e.g., DASH [LLG + 92]). 2 . . cell2 = cell_num2 [proc]; cell1 = cell_num1 [proc]; int proc; SubPart1 (proc) . . . . . Work () while (converged != 0) Wait_Barrier (&MyBarr1); . <p> The processors are assumed to be RISC-like, with a 32 KB first level cache and an infinite second level cache 3 . Execution times were measured on two different shared-memory multiprocessors, a 56-processor Kendall Square Research KSR1 <ref> [KSR92] </ref>, and the Sequent. They represent two very different points in the shared-memory multiprocessor design space, in particular with respect to their memory subsystem designs. The KSR1 memory subsystem has big caches, big cache blocks and long memory latencies.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: These programs are typical of those that currently execute on small to medium scale, bus-based multiprocessors, both commercially (e.g., Sequent Symmetry [LT88] and the KSR1 [KSR92]) and in research environments (e.g., DASH <ref> [LLG + 92] </ref>). 2 . . cell2 = cell_num2 [proc]; cell1 = cell_num1 [proc]; int proc; SubPart1 (proc) . . . . . Work () while (converged != 0) Wait_Barrier (&MyBarr1); .
Reference: [LT88] <author> R. Lovett and S. Thakkar. </author> <title> The symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These programs are typical of those that currently execute on small to medium scale, bus-based multiprocessors, both commercially (e.g., Sequent Symmetry <ref> [LT88] </ref> and the KSR1 [KSR92]) and in research environments (e.g., DASH [LLG + 92]). 2 . . cell2 = cell_num2 [proc]; cell1 = cell_num1 [proc]; int proc; SubPart1 (proc) . . . . . Work () while (converged != 0) Wait_Barrier (&MyBarr1); . <p> We perform two kinds of experiments to measure the effects of the data transformations on the programs in our workload. False sharing reductions were measured using trace-driven simulation. Each program was traced (both before and after shared data was transformed) on a 20-processor Sequent Symmetry <ref> [LT88] </ref>, using a software tracing tool for parallel programs [EKKL90]. Cache miss rates were analyzed with a multiprocessor simulator that emulates a simple, shared memory architecture. The processors are assumed to be RISC-like, with a 32 KB first level cache and an infinite second level cache 3 .
Reference: [MDWSV87] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: In Pverify <ref> [MDWSV87] </ref>, Topopt [DN87] and Maxflow [Car88] the programmers made no attempt to improve locality, because of the added complexity of (multiprocessor) cache-conscious programming. 6 Results We present two sets of results to describe the impact of our analysis and transformations on the benchmarks.
Reference: [MR93] <author> S. P. Masticola and B. G. Ryder. </author> <title> Non-concurrency analysis. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 129-138, </pages> <month> May </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: The compiler analysis involves three separate stages. The first determines which sections of code each process executes by computing its control flow graph [Anoa]. The second performs non-concurrency analysis <ref> [MR93] </ref> by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob].
Reference: [Mye81] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [Anob]. The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling [Wal91] on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). In this paper we only discuss those aspects of the static analysis that clarify how it effects the detection of false sharing.
Reference: [PC89] <author> J.K. Peir and R. Cytron. </author> <title> Minimum distance: A method for partitioning recurrences for mul tiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(8) </volume> <pages> 1203-1211, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: They strip-mined the loops to the size of the cache block and assigned each strip to a different processor. The decline in miss ratios ranged from 4% to almost 60%, as block size was increased to 128 bytes. No execution times were reported. Peir and Cytron <ref> [PC89] </ref> partitioned loops to minimize inter-processor communication when processing recurrences. Their mechanism for partitioning utilizes loop unrolling and dependence vectors. Partitions are then scheduled on different processors. For the compiler-based approaches, the workload consisted of either loops or library routines that have 13 fine-grain parallelism.
Reference: [PGH + 89] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C.L. Lee, B. Leung, and D. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: For these reasons we have automated the elimination of false sharing. We have developed and incorporated into the Parafrase-2 <ref> [PGH + 89] </ref> source-to-source restructurer a series of compiler-directed algorithms [Anoa, Anob] and a suite of transformations that restructure shared data at compile time. <p> of the static analysis have been performed, we use a number of heuristics to detect which data structures are susceptible to false sharing and which transformation should be applied to eliminate it [Anob]. 5 Methodology The static analysis and false sharing detection algorithms were implemented as separate passes in Parafrase-2 <ref> [PGH + 89] </ref>. We perform two kinds of experiments to measure the effects of the data transformations on the programs in our workload. False sharing reductions were measured using trace-driven simulation.
Reference: [SWG91] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: The workload consisted of six coarse-grained, explicitly parallel programs, all written in C (Table 1). Water, Mp3d and LocusRoute are from the Stanford SPLASH benchmarks <ref> [SWG91] </ref>. They are all programs in which considerable programming effort has been expended to improve data locality, including eliminating false sharing.
Reference: [TLH90] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Shared data placement optimizations to re duce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality [AG88], is coherency overhead caused by false sharing <ref> [TLH90, EJ91] </ref>. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although they do not actually share data, they incur its costs, because coherency operations are often cache block-based. <p> Manually changing the placement of this data to better conform to the memory reference pattern (based on profiles derived from trace-driven simulations) can reduce false sharing misses by up to 75% <ref> [TLH90, EJ91] </ref>.
Reference: [Wal91] <author> D. W. Wall. </author> <title> Predicting program behavior using real or esitmated profiles. </title> <booktitle> In Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 59-70, </pages> <month> June </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: The third stage performs an enhanced interprocedural, flow-insensitive, summary 3 side-effect analysis [Bar78, Ban79, Mye81, CK88b] and static profiling <ref> [Wal91] </ref> on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two). In this paper we only discuss those aspects of the static analysis that clarify how it effects the detection of false sharing. <p> The second problem with summary side-effect analysis is that it does not differentiate between accesses that occur inside and outside loops, thereby failing to discern the dominant (most frequently executed) memory reference pattern. To counter this, we use static profiling information <ref> [Wal91] </ref> to produce a weighting of the side-effects with respect to estimated execution frequency. This allows us to disregard data structures that are accessed infrequently, or are read frequently but seldom written.
References-found: 28

