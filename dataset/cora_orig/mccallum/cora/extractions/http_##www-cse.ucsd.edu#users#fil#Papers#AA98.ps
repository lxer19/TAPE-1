URL: http://www-cse.ucsd.edu/users/fil/Papers/AA98.ps
Refering-URL: http://www-cse.ucsd.edu/users/fil/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ffil,rikg@cs.ucsd.edu  
Title: Adaptive Information Agents in Distributed Textual Environments  
Author: Filippo Menczer and Richard K. Belew 
Address: La Jolla, CA 92093-0114, USA  
Affiliation: Computer Science and Engineering Department University of California, San Diego  
Abstract: Hypertext environments such as the Web are rich with both word and link cues that can be exploited by autonomous agents performing distributed tasks on behalf of the user. This paper characterizes such environments and identifies the features that are most useful and readily available. We describe the adaptive representation of an ecology of retrieval agents who attempt to capture important features of their surroundings, and base their behaviors upon them. We discuss how such a representation allows the agents to interact with the environments where they are situated. Agents can internalize words that are locally correlated with fitness, based on user feedback. They are shown to outperform nonadaptive search by an order of magnitude. Furthermore, each agent learns new strategies at local time and space scales, while the population evolves at a global scale. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Balabanovic. </author> <title> An adaptive web page recommendation service. </title> <booktitle> In 1st Intl. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: Techniques such as weighted keyword vector representations and relevance feedback, in conjunction with genetic algorithms and/or paradigms inspired by natural or economic systems, have been applied to information retrieval and filtering <ref> [21, 18, 1] </ref>. In these approaches, agents require some sort of supervision in order to adapt to the preferences of the user and/or to the external environment. The user may supervise the agents, for example, by allowing them to look over his shoulders, or by providing them with relevance feedback. <p> Therefore genotypes also comprise a vector or real-valued weights, initialized randomly with uniform distribution in a small interval centered around 0. The neural net has an input for each keyword in its genotype; it uses the hyperbolic tangent as its squashing function, with input and activation values in <ref> [1; +1] </ref>, and a single real-valued output also in [1; +1]. The keywords represent an agent's opinion of what terms best discriminate documents relevant to the user from the rest. The weights represent the interactions of such terms with respect to relevance. <p> The neural net has an input for each keyword in its genotype; it uses the hyperbolic tangent as its squashing function, with input and activation values in <ref> [1; +1] </ref>, and a single real-valued output also in [1; +1]. The keywords represent an agent's opinion of what terms best discriminate documents relevant to the user from the rest. The weights represent the interactions of such terms with respect to relevance. <p> Second, it is computed online and therefore uses document frequencies based on the contents of the cache rather than the entire collection. The hyperbolic tangent is used to normalize energy intakes into the appropriate range <ref> [1; +1] </ref> | the same range as the corresponding neural nets prediction. In step (6) of the algorithm, the agent can compare the relevance (assessed or estimated) of the current document with the estimate of the link that led to it.
Reference: [2] <author> R. Belew. </author> <title> Adaptive information retrieval: Using a connectionist representation to retrieve and learn about documents. </title> <booktitle> In ACM Special Interest Group on Information Retrieval, </booktitle> <pages> pages 11-20, </pages> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The collaborative, social sharing of search expertise adapted on behalf of one user, then exploited by another, has been considered by others <ref> [2, 20, 7] </ref>.
Reference: [3] <editor> R. Belew and M. Mitchell, editors. </editor> <title> Adaptive Individuals in Evolving Populations: Models and Algorithms. Santa Fe Institute Studies in the Sciences of Complexity. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: The neural net's weights are updated by back-propagation of error, using teaching input: E (D a ) + max l where is a discount factor. In the current ARACHNID implementation, learned changes to the weights are "Lamarkian" in that they are inherited by offspring at reproduction <ref> [3] </ref>. At reproduction (step (7) of the algorithm), the offspring clone is mutated to provide the evolutionary process with the necessary power of exploration.
Reference: [4] <author> W. Cooper. </author> <title> Expected search length: A single measure of retrieval effectiveness based on weak ordering action of retrieval systems. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 19 </volume> <pages> 30-41, </pages> <year> 1968. </year>
Reference-contexts: of the Propaedia with approximately 700 nodes, and the associated graph of approximately 11,000 articles. 4.1 Macro analysis In the first experiment we measure one version of search length, a criterion combining recall and precision, defined as the number of non-relevant documents visited until the first relevant document is found <ref> [4] </ref>. This measure provides us with a "macro" analysis because the collective behavior of the agent population is observed as a whole, and averaged across a large number of queries. The search length of ARACHNID is compared with that of a simple non-adaptive algorithm, namely BFS (breadth-first-search).
Reference: [5] <author> P. De Bra and R. Post. </author> <title> Information retrieval in the world wide web: Making client-based searching feasible. </title> <booktitle> In 1st Intl. WWW Conf., </booktitle> <address> Geneva, </address> <year> 1994. </year>
Reference-contexts: The user may supervise the agents, for example, by allowing them to look over his shoulders, or by providing them with relevance feedback. In other systems, agents are completely unsupervised but cannot learn or adapt. For example, in the Fish Search algorithm <ref> [5] </ref> agents in a population of identical clones follow fixed, exhaustive search strategies. We propose that agents should be able to perform and adapt in a completely autonomous fashion in the absence of supervision from the user, while making use of the user's feedback when this is available.
Reference: [6] <author> Encyclopaedia britannica. </author> <note> http://www.eb.com/. </note>
Reference-contexts: The R and G measures are based on retrieved sets by the Lycos and Britannica search engines, respectively <ref> [6, 14] </ref>. The lower bound is R = G. (Reprinted from [16].) environments. 2 Text as environment The agent paradigm can be reduced to a very simple algorithm in which the agent repeatedly receives input encoding state features and produces output encoding actions. <p> We have therefore selected a subset of the Web as an information environment, the corpus of the Encyclopaedia Britannica <ref> [6] </ref>. The advantage is that we can make use of readily available relevant sets of articles associated with a large number of queries. Articles are in fact organized as the leaves of a hierarchical topical tree, called Propaedia, which is manually built and updated by skilled human editors.
Reference: [7] <author> Filmfinder. </author> <note> http://www.filmfinder.com/. </note>
Reference-contexts: The collaborative, social sharing of search expertise adapted on behalf of one user, then exploited by another, has been considered by others <ref> [2, 20, 7] </ref>.
Reference: [8] <author> W. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: Agents obey Internet etiquette by complying with the proposed standard for robot exclusion [10]. Agents also employ standard information retrieval tools such as a filter for noise words and a stemmer based on Porter's algorithm <ref> [8] </ref>. Finally, agents store an efficient representation of visited documents in a shared cache on the client machine. Each document is represented by a list of links and stemmed keywords.
Reference: [9] <author> T. Joachims, D. Freitag, and T. Mitchell. Webwatcher: </author> <title> A tour guide for the world wide web. </title> <booktitle> In Proc. IJCAI, </booktitle> <year> 1997. </year>
Reference-contexts: Finally, agents can execute in parallel on different server machines because the environment is distributed. Several machine learning techniques have been suggested to produce effective information agents, yielding for example agents that perform look-ahead searches and provide suggestions to the user on the basis of reinforcement learning <ref> [9] </ref>. Techniques such as weighted keyword vector representations and relevance feedback, in conjunction with genetic algorithms and/or paradigms inspired by natural or economic systems, have been applied to information retrieval and filtering [21, 18, 1].
Reference: [10] <author> M. Koster. </author> <title> The web robots pages. </title> <address> http://info.webcrawler.com/mak/projects/robots/ robots.html, </address> <year> 1997. </year>
Reference-contexts: The Web interface is based on the libwww-perl library [12]. The prototype is written in C and Perl and runs on UNIX and Macintosh platforms. Agents obey Internet etiquette by complying with the proposed standard for robot exclusion <ref> [10] </ref>. Agents also employ standard information retrieval tools such as a filter for noise words and a stemmer based on Porter's algorithm [8]. Finally, agents store an efficient representation of visited documents in a shared cache on the client machine.
Reference: [11] <author> K. </author> <title> Laws. </title> <journal> The computists' communique. </journal> <note> http://www.computists.com/. [12] http://www.ics.uci.edu/pub/websoft/libwww-perl. </note>
Reference-contexts: This type of editorial focusing is the ultimate goal of our information-seeking agents, but comparison against the currently available alternatives (e.g., the Computists' Communique newsletter <ref> [11] </ref> shows just how far we have to go. 6 Acknowledgments We are grateful to Encyclopaedia Britannica, Inc. for allowing us to make use of their data in our experiments.
Reference: [13] <author> L.-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: In step (6) of the algorithm, the agent can compare the relevance (assessed or estimated) of the current document with the estimate of the link that led to it. By using the connectionist version of Q-learning <ref> [13] </ref>, the neural net can be adjusted to improve the accuracy of the link estimator. After agent a visits document D a , E (D a ) is used as reinforcement signal.
Reference: [14] <author> Lycos. </author> <note> http://www.lycos.com/. </note>
Reference-contexts: The R and G measures are based on retrieved sets by the Lycos and Britannica search engines, respectively <ref> [6, 14] </ref>. The lower bound is R = G. (Reprinted from [16].) environments. 2 Text as environment The agent paradigm can be reduced to a very simple algorithm in which the agent repeatedly receives input encoding state features and produces output encoding actions.
Reference: [15] <author> P. Maes. </author> <title> Agents that reduce work and information overload. </title> <journal> Comm. of the ACM, </journal> <volume> 37(7) </volume> <pages> 31-40, </pages> <year> 1994. </year>
Reference-contexts: Agents, or semi-intelligent programs making automatic decisions on behalf of the user, are viewed by many as a way of decreasing the amount of human-computer interaction necessary for the information management task <ref> [15] </ref>. The Web possesses many of the features making it an ideal target for adaptive agents. Many different agents in a population can adapt to the local characteristics of the different places where each is situated within the large, heterogeneous environment.
Reference: [16] <author> F. Menczer. Arachnid: </author> <title> Adaptive retrieval agents choosing heuristic neighborhoods for information discovery. </title> <booktitle> In Proc. 14th Intl. Conf. on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: The ARACHNID system was built to test the suitability of this approach [17]. In a recent paper we analyzed the high-level behavior of the algorithm, and its interaction with an abstraction of the environment, from a theoretical perspective <ref> [16] </ref>. Here we will focus on a representation of ARACHNID agents allowing for the exploitation of the wealth of statistical and topological cues present in distributed text 1 environments, the Web and Britannica Online. <p> The R and G measures are based on retrieved sets by the Lycos and Britannica search engines, respectively [6, 14]. The lower bound is R = G. (Reprinted from <ref> [16] </ref>.) environments. 2 Text as environment The agent paradigm can be reduced to a very simple algorithm in which the agent repeatedly receives input encoding state features and produces output encoding actions. <p> This superimposes a structure upon the collection of documents, for the very purpose of guiding the browsing user | or agent. We have conjectured that even in unstructured information spaces, authors effectively cluster documents about related topics by letting them point to each other <ref> [16] </ref>. To see this, call R the conditional probability that a document is relevant with respect to a query, given that it is pointed by a link from another document that is relevant with respect to the same query. <p> At a lower level, we considered the behavior of the ARACHNID ecology in response to a single query. This way we analyzed the contributions of different parts of the system, namely reinforcement learning and relevance feedback. It was shown elsewhere <ref> [16] </ref> that Q-learning alone can significantly accelerate the discovery of relevant documents, shortening search length by a factor of 10 and improving precision 5 rank k I 1 COURT 0.034 2 SYSTEM 0.023 2 PARTI 0.023 2 GOVERN ? 0.023 5 POLIT ? 0.020 5 POWER 0.020 7 ADMINISTR 0.017 8
Reference: [17] <editor> F. Menczer and R. Belew. </editor> <booktitle> Arachnid software demo. 1st Intl. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: The ARACHNID system was built to test the suitability of this approach <ref> [17] </ref>. In a recent paper we analyzed the high-level behavior of the algorithm, and its interaction with an abstraction of the environment, from a theoretical perspective [16].
Reference: [18] <author> A. Moukas and G. Zacharia. </author> <title> Evolving a multi-agent information filtering solution in amalthaea. </title> <booktitle> In 1st Intl. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: Techniques such as weighted keyword vector representations and relevance feedback, in conjunction with genetic algorithms and/or paradigms inspired by natural or economic systems, have been applied to information retrieval and filtering <ref> [21, 18, 1] </ref>. In these approaches, agents require some sort of supervision in order to adapt to the preferences of the user and/or to the external environment. The user may supervise the agents, for example, by allowing them to look over his shoulders, or by providing them with relevance feedback.
Reference: [19] <author> D. Rus, R. Gray, and D. Kotz. </author> <title> Transportable information agents. </title> <booktitle> In 1st Intl. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: Secure languages and protocols are needed before information providers will welcome trusted autonomous agents to their servers; agent research is providing systems technology with the thrust that may enable such mobile agents developments feasible very soon <ref> [19] </ref>. Many different models of interaction among agents are also worth exploration. For example, research is under way into the issues of agents learning from other agents, agent collaboration, and agent communication languages. The first form of direct agent interaction that we are going to address is recombination.
Reference: [20] <author> U. Shardanand and P. Maes. </author> <title> Social information filtering: algorithms far automating "word of mouth". </title> <booktitle> In Proc. ACM Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 210-217, </pages> <address> New York, NY, 1995. </address> <publisher> ACM. </publisher>
Reference-contexts: The collaborative, social sharing of search expertise adapted on behalf of one user, then exploited by another, has been considered by others <ref> [2, 20, 7] </ref>.
Reference: [21] <author> B. Sheth and P. Maes. </author> <title> Evolving agents for personalized information filtering. </title> <booktitle> In 9th IEEE Conf. on AI for Applications, </booktitle> <year> 1993. </year>
Reference-contexts: Techniques such as weighted keyword vector representations and relevance feedback, in conjunction with genetic algorithms and/or paradigms inspired by natural or economic systems, have been applied to information retrieval and filtering <ref> [21, 18, 1] </ref>. In these approaches, agents require some sort of supervision in order to adapt to the preferences of the user and/or to the external environment. The user may supervise the agents, for example, by allowing them to look over his shoulders, or by providing them with relevance feedback.
Reference: [22] <author> K. Sparck Jones. </author> <title> A statistical interpretation of term specificity and its application in retrieval. </title> <journal> Journal of Documentation, </journal> <volume> 28 </volume> <pages> 111-121, </pages> <year> 1972. </year>
Reference-contexts: Such a weighting formula differs from more traditional T F IDF schemes <ref> [22] </ref> in at least two respects. First, it is not aimed at weighting terms based on how well they describe documents, but rather on how well they correlate with relevance.
Reference: [23] <author> A. Steier and R. Belew. </author> <title> Exporting phrases: A statistical analysis of topical language. </title> <editor> In R. Casey and B. Croft, editors, </editor> <booktitle> 2nd Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1994. </year> <month> 8 </month>
Reference-contexts: They can differ substantially in their perceptions of what features are relevant to the user, based on where they are situated and on their evolutionary and life histories. This is a crucial consequence of our representation model, reflecting the fact that context refines the meaning of words and phrases <ref> [23] </ref>. For example, agents A and B have internalized some common features (SYSTEM, GOVERN) and some individual ones (e.g., POLIT vs. OFFICI). Their differences point to the role played by the documents visited by their ancestors, as well as those in which they are born, in shaping their representations.
References-found: 22

