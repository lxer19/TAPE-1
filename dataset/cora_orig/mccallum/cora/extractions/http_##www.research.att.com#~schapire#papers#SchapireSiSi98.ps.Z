URL: http://www.research.att.com/~schapire/papers/SchapireSiSi98.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: fschapire,singer,singhalg@research.att.com  
Title: SIGIR '98 Boosting and Rocchio Applied to Text Filtering  
Author: Robert E. Schapire, Yoram Singer, Amit Singhal 
Address: 180 Park Avenue, Florham Park, NJ 07932  
Affiliation: AT&T Labs Research  
Abstract: We discuss two learning algorithms for text filtering: modified Rocchio and a boosting algorithm called AdaBoost. We show how both algorithms can be adapted to maximize any general utility matrix that associates cost (or gain) for each pair of machine prediction and correct label. We first show that AdaBoost significantly outperforms another highly effective text filtering algorithm. We then compare AdaBoost and Rocchio over three large text filtering tasks. Overall both algorithms are comparable and are quite effective. AdaBoost produces better classifiers than Rocchio when the training collection contains a very large number of relevant documents. However, on these tasks, Roc-chio runs much faster than AdaBoost. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James Allan. </author> <title> Incremental relevance feedback for information filtering. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 270-278. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness.
Reference: [2] <author> Chidanand Apte, Fred Damerau, and Sholom Weiss. </author> <title> Towards language independent automated learning of text categorisation methods. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 23-30, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness.
Reference: [3] <author> Avrim Blum. </author> <title> Empirical support for winnow and weighted-majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 64-72, </pages> <year> 1995. </year> <title> Per class error, utility, average precision, and F-measure values for this study are available from: </title> <address> http://www.research.att.com/~singhal/sigir98-rocboost.dat </address>
Reference-contexts: Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks [10, 25, 4]. We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum <ref> [3] </ref>, studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer [8]. This algorithm has been shown to be more effective than many current text filtering algorithms [8]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts.
Reference: [4] <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California at Berke-ley, </institution> <year> 1996. </year>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [10, 25, 4] </ref>. We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer [8]. This algorithm has been shown to be more effective than many current text filtering algorithms [8].
Reference: [5] <author> Chris Buckley. </author> <title> The importance of proper weighting methods. </title> <booktitle> In Human Language Technology. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Similar behavior is observed for many other classes, e.g., dfl, instal-debt, and sun-meal. This also indicates why using average precision to evaluate text filtering is not sufficient. Our current implementation of AdaBoost does not utilize term weights, which are known to be crucial for most IR tasks <ref> [5] </ref> and are the basis of good performance of Rocchio's algorithm. We believe that AdaBoost would benefit significantly by using term weights, and we are currently studying ways of incorporating these weights into AdaBoost.
Reference: [6] <author> Chris Buckley and Gerard Salton. </author> <title> Optimization of relevance feedback weights. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 351-357, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering. In recent years, the IR community has proposed several modifications to Rocchio's algorithm that have vastly improved the performance of this algorithm: better term weighting [26, 35], query-zoning [36], and dynamic feedback optimization <ref> [6] </ref> being the three most notable improvements. In this study, we adapt a state of the art Rocchio's algorithm for the text filtering task, and compare it to a fairly new ML algorithm called boosting. <p> They proposed sampling of the non-relevant documents to form a query zone. 3. Dynamic Feedback Optimization: Buckley et al. <ref> [6] </ref> have shown that further optimizing the term weights proposed by Rocchio's formulation on the training collection improves the quality of a feedback query for the test data. We view these techniques as tools that bring a Rocchio query closer to the ideal query. <p> Term weights in this query of n w words and n p phrases are further optimized using three-pass dynamic feedback optimization (DFO) with pass ratios 1.00, 0.50, and 0.25 <ref> [6] </ref>. Since DFO optimizes average precision in the training collection, and a fixed number of top documents are ranked in this process [6], we rank the top MAX (500; 5R) doc uments in this step. 5. <p> weights in this query of n w words and n p phrases are further optimized using three-pass dynamic feedback optimization (DFO) with pass ratios 1.00, 0.50, and 0.25 <ref> [6] </ref>. Since DFO optimizes average precision in the training collection, and a fixed number of top documents are ranked in this process [6], we rank the top MAX (500; 5R) doc uments in this step. 5. The optimized feedback query is used to rank the Lnu weighted training documents (using inner-product similarity).
Reference: [7] <author> J.P. Callan. </author> <title> Information filtering with inference networks. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 262-269. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness.
Reference: [8] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 307-315, </pages> <year> 1996. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer <ref> [8] </ref>. This algorithm has been shown to be more effective than many current text filtering algorithms [8]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts. We then compare AdaBoost to Rocchio's method and show that the two algorithms are quite competitive. <p> We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer <ref> [8] </ref>. This algorithm has been shown to be more effective than many current text filtering algorithms [8]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts. We then compare AdaBoost to Rocchio's method and show that the two algorithms are quite competitive. <p> One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision. <p> Even though the filtering effectiveness of a system is related to its ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Van Rijsbergen's F-measures: Used in <ref> [20, 22, 8, 39] </ref> to evaluate filtering, this is a single valued measure that depends upon the relative importance a user assigns to recall and precision (see [37], pp. 168-176). <p> 6 Experiments and Results This section discusses our experiments and results. 6.1 AdaBoost compared to Sleeping Experts We first give experimental results which show that our adaptation of AdaBoost for text filtering achieves better results than, Sleeping-Experts, another effective algorithm for text filtering studied recently by Cohen and Singer in <ref> [8] </ref>. We compare the performance of AdaBoost and Sleeping-Experts on the AP-Body and the Reuters-21578 tasks. Figure 2 shows the results of this comparison. The scatter plot on the left hand side of of relevant documents in the training collection for the Reuters collection. <p> This result contradicts the claims made in several previous studies <ref> [22, 8, 39, 15] </ref> that infer that Rocchio's method is inferior to state of the art machine learning algorithms.
Reference: [9] <author> Yoav Freund and Robert E. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 148-156, </pages> <year> 1996. </year>
Reference-contexts: In this study, we adapt a state of the art Rocchio's algorithm for the text filtering task, and compare it to a fairly new ML algorithm called boosting. We first develop a text filtering algorithm based on Freund and Schapire's AdaBoost algorithm <ref> [9] </ref>, which is currently the Permission to make digital/hard copy of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear,
Reference: [10] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <year> 1997. </year>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [10, 25, 4] </ref>. We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer [8]. This algorithm has been shown to be more effective than many current text filtering algorithms [8]. <p> utility measures used in this study are: u rel+ u rel u nrel+ u nrel Error: 0 1 1 0 Util-1: 3 0 2 0 Util-2: 3 1 1 0 3 Boosting for text filtering In this section, we describe how we have adapted Freund and Schapire's AdaBoost boosting algorithm <ref> [10] </ref> for text filtering. The main idea of boosting is to combine many rules-of-thumb. <p> See Freund and Schapire <ref> [10] </ref> for more complete motivation for this choice of ff s . For our task, we also allow ff s to be negative. This will be the case whenever a weak hypothesis h s is found with error * s greater than 1=2.
Reference: [11] <author> Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. </author> <title> Using and combining predictors that specialize. </title> <booktitle> In Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 334-343, </pages> <year> 1997. </year>
Reference-contexts: Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks [10, 25, 4]. We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. <ref> [11] </ref>, and first applied to text filtering by Cohen and Singer [8]. This algorithm has been shown to be more effective than many current text filtering algorithms [8]. We show that, for text filtering, AdaBoost is definitively superior to Sleeping-Experts.
Reference: [12] <author> D. K. </author> <title> Harman. </title> <booktitle> Overview of the third Text REtrieval Conference (TREC-3). In Proceedings of the Third Text REtrieval Conference (TREC-3), </booktitle> <pages> pages 1-19. </pages> <note> NIST Special Publication 500-225, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: In our experiments, we use the ninety TOPIC categories that have at least one relevant (positive) training documents and at least one relevant test document. 5.2 AP-Body This test collection is made up of documents from the AP newswire included in the TREC disks 1-3 <ref> [12] </ref>. 142,791 documents from the years 1988 and 1989 are used as the training collection, and 66,992 documents from the year 1990 are used as the test collection. <p> Details of what documents were skipped in the creation of this collection are available from David Lewis (lewis@research.att.com). 5.3 TREC-3 This collection, once again from the TREC disks 1-3, was used in the routing task in the third Text REtrieval Conference (TREC-3) <ref> [12] </ref>. The training collection contains all documents on TREC disks 1 and 2, whereas the test collection is made up of all the document contained in disk 3. There are a total of 741,856 training documents and 336,310 test documents. <p> There are a total of 741,856 training documents and 336,310 test documents. Fifty TREC topics, numbered 101-150 are used as individual classes in this collection <ref> [12] </ref>. Even though these TREC topics are long user-queries which contain many useful words for text filtering, we again emphasize that we do not use the topic texts in either of our systems. The relevance judgments for these topics are also available with the TREC data.
Reference: [13] <author> David Hull. </author> <title> The TREC-6 filtering track: Description and analysis. </title> <booktitle> In Proceedings of the Sixth Text REtrieval Conference (TREC-6), 1998 (to appear). </booktitle>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [18, 19, 13] </ref>. This study also presents results using utility that can be used by other researchers for comparison purposes in the future. <p> non-interpolated average precision is, 1 jRelj X jfd 0 jd 0 2 Rel; Rank (d 0 ) Rank (d)gj Rank (d) : 2 www.research.att.com/~singhal/sigir98-rocboost.dat Utility Recently, the TREC text filtering evaluations have been using utility measures, which assign rewards (or penalties) for each pair of machine prediction and correct label <ref> [19, 13, 14] </ref>. Let r + be the number of relevant documents that are classified relevant by the machine, and r the number of relevant documents misclassified as irrelevant. Similarly, n + and n are the number of non-relevant documents classified as relevant and irrelevant, respectively.
Reference: [14] <author> David Hull, Jan Pedersen, and Hinrich Schutze. </author> <title> Method combination for document filtering. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 279-288, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> non-interpolated average precision is, 1 jRelj X jfd 0 jd 0 2 Rel; Rank (d 0 ) Rank (d)gj Rank (d) : 2 www.research.att.com/~singhal/sigir98-rocboost.dat Utility Recently, the TREC text filtering evaluations have been using utility measures, which assign rewards (or penalties) for each pair of machine prediction and correct label <ref> [19, 13, 14] </ref>. Let r + be the number of relevant documents that are classified relevant by the machine, and r the number of relevant documents misclassified as irrelevant. Similarly, n + and n are the number of non-relevant documents classified as relevant and irrelevant, respectively.
Reference: [15] <author> Thorsten Joachims. </author> <title> Text categorization with support vector machines: Learning with many relevant features. </title> <note> In European Conference on Machine Learning (ECML'98), 1998 (to appear). </note>
Reference-contexts: One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision. <p> This result contradicts the claims made in several previous studies <ref> [22, 8, 39, 15] </ref> that infer that Rocchio's method is inferior to state of the art machine learning algorithms.
Reference: [16] <author> David Lewis. </author> <type> Personal Communication. </type>
Reference-contexts: The aim of a filtering system is to obtain as high a break-even point as possible. This measure, though popular, has several problems for evaluating a filtering system <ref> [16] </ref>: * Often, we need to interpolate the scores to obtain the break-even point. Interpolation gives values not achiev able by the system. * The point where recall equals precision is neither a desir able nor an informative target from a user's perspective.
Reference: [17] <author> David Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 37-50, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Finally, Section 7 concludes the study. 2 Evaluation measures Past studies on text filtering have used a variety of measures for evaluating performance. One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in <ref> [17] </ref>, this measure has been the measure of choice in many studies on text filtering [17, 21, 8, 24, 39, 23, 15]. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision. <p> One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision.
Reference: [18] <author> David Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 246-255, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [18, 19, 13] </ref>. This study also presents results using utility that can be used by other researchers for comparison purposes in the future.
Reference: [19] <author> David Lewis. </author> <title> The TERC-5 filtering track. </title> <booktitle> In Proceedings of the Fifth Text REtrieval Conference (TREC-5), </booktitle> <pages> pages 75-96. </pages> <note> NIST Special Publication 500-238, </note> <month> November </month> <year> 1997. </year>
Reference-contexts: As described in the next section, most evaluation measures used in the past for evaluating filtering effectiveness are unfit for the purpose. Recently the TREC conferences have been moving toward the use of utility as the measure of choice for evaluating text filtering <ref> [18, 19, 13] </ref>. This study also presents results using utility that can be used by other researchers for comparison purposes in the future. <p> non-interpolated average precision is, 1 jRelj X jfd 0 jd 0 2 Rel; Rank (d 0 ) Rank (d)gj Rank (d) : 2 www.research.att.com/~singhal/sigir98-rocboost.dat Utility Recently, the TREC text filtering evaluations have been using utility measures, which assign rewards (or penalties) for each pair of machine prediction and correct label <ref> [19, 13, 14] </ref>. Let r + be the number of relevant documents that are classified relevant by the machine, and r the number of relevant documents misclassified as irrelevant. Similarly, n + and n are the number of non-relevant documents classified as relevant and irrelevant, respectively.
Reference: [20] <author> David Lewis and William Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3-12, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Even though the filtering effectiveness of a system is related to its ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Van Rijsbergen's F-measures: Used in <ref> [20, 22, 8, 39] </ref> to evaluate filtering, this is a single valued measure that depends upon the relative importance a user assigns to recall and precision (see [37], pp. 168-176). <p> Each document in this collection has a distinct title field (marked by the SGML tag &lt;HEAD&gt;), and a distinct body field (marked by the SGML tag &lt;TEXT&gt;). We only use the body of a document in our experiments. There are twenty classes in this collection. See <ref> [20] </ref> for a description of how the classes for this corpus were derived. This collection was first used by Lewis et al. in [22].
Reference: [21] <author> David Lewis and Mark Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <address> Las Vegas, NV, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision.
Reference: [22] <author> David Lewis, Robert Schapire, James Callan, and Ron Papka. </author> <title> Training algorithm for linear text classifiers. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 298-306, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness. <p> Even though the filtering effectiveness of a system is related to its ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Van Rijsbergen's F-measures: Used in <ref> [20, 22, 8, 39] </ref> to evaluate filtering, this is a single valued measure that depends upon the relative importance a user assigns to recall and precision (see [37], pp. 168-176). <p> We only use the body of a document in our experiments. There are twenty classes in this collection. See [20] for a description of how the classes for this corpus were derived. This collection was first used by Lewis et al. in <ref> [22] </ref>. One should note that even though the original distribution of this data has 79,919 documents from the year 1988, and 84,678 from the year 1989, making a total of 164,597 training documents, 21,806 of the documents that have non-standard formats or structures have been omitted by Lewis et al. <p> This result contradicts the claims made in several previous studies <ref> [22, 8, 39, 15] </ref> that infer that Rocchio's method is inferior to state of the art machine learning algorithms.
Reference: [23] <author> Andrew McCallum and Kamal Nigam. </author> <title> Employing em in pool-based active learning for text classification. </title> <note> In ICML-98, 1998 (to appear). </note>
Reference-contexts: One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision.
Reference: [24] <author> H.T. Ng, W.B. Gog, and K.L. </author> <title> Low. Feature selection, perceptron learning, and a usability case study for text categorization. </title> <booktitle> In Proceedings of the Twentieth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 67-73, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision.
Reference: [25] <author> J. R. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730, </pages> <year> 1996. </year>
Reference-contexts: The main idea of boosting is to generate many, relatively weak classification rules and to combine these into a single highly accurate classification rule. Boosting algorithms have attractive theoretical properties, and have also been shown to perform well experimentally on more standard machine learning tasks <ref> [10, 25, 4] </ref>. We compare AdaBoost to Sleeping-Experts, an algorithm proposed by Blum [3], studied further by Freund et al. [11], and first applied to text filtering by Cohen and Singer [8]. This algorithm has been shown to be more effective than many current text filtering algorithms [8].
Reference: [26] <author> S.E. Robertson and S. Walker. </author> <title> Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 232-241. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering. In recent years, the IR community has proposed several modifications to Rocchio's algorithm that have vastly improved the performance of this algorithm: better term weighting <ref> [26, 35] </ref>, query-zoning [36], and dynamic feedback optimization [6] being the three most notable improvements. In this study, we adapt a state of the art Rocchio's algorithm for the text filtering task, and compare it to a fairly new ML algorithm called boosting. <p> Several techniques are known to improve the effectiveness of Rocchio's method. The three new developments that have been quite effective in conjunction with Rocchio's algorithm are: 1. Better Term Weights: A much better understanding of term weights has been developed in the IR community in recent years <ref> [26, 35] </ref>. Better term weights in the training documents yield a better Rocchio query. A better Rocchio query along with better term weights for the test documents yields much improved scores (i.e. better rank ing) for the test documents. 2.
Reference: [27] <author> J.J. Rocchio. </author> <title> Document Retrieval Systems-Optimization and Evaluation. </title> <type> PhD thesis, </type> <institution> Harvard Computational Laboratory, </institution> <address> Cam-bridge, MA, </address> <year> 1966. </year>
Reference-contexts: A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms <ref> [27, 28] </ref>. Rocchio's algorithm was developed in the framework of the vector space model [32]. When documents are to be ranked for a query, an ideal query should rank all the relevant documents above all non-relevant documents.
Reference: [28] <author> J.J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <booktitle> In The SMART Retrieval SystemExperiments in Automatic Document Processing, </booktitle> <pages> pages 313-323, </pages> <address> Englewood Cliffs, NJ, 1971. </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more [17, 20, 40, 2, 41, 14, 22, 8, 24]. Most studies use Rocchio's method <ref> [28] </ref>, a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering. <p> A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms <ref> [27, 28] </ref>. Rocchio's algorithm was developed in the framework of the vector space model [32]. When documents are to be ranked for a query, an ideal query should rank all the relevant documents above all non-relevant documents.
Reference: [29] <author> Gerard Salton. </author> <title> Automatic text processingthe transformation, analysis and retrieval of information by computer. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: In an information filtering scenario, once several documents have been marked as relevant to a user's information need, a user profile is created using Rocchio's formulation (Eq. (3)). Any new article that has high similarity (we use vector inner-product as the similarity measure in all our experiments, see <ref> [29] </ref>, page 318) to this user profile is considered potentially useful for the user and is sent to the user. Several techniques are known to improve the effectiveness of Rocchio's method. The three new developments that have been quite effective in conjunction with Rocchio's algorithm are: 1.
Reference: [30] <author> Gerard Salton and Chris Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: Automatic query formulation using relevance feedback, once the user has marked some of the documents (possibly retrieved by the initial user-query) as relevant and some as nonrelevant, has been one of the most successful methods in IR <ref> [30] </ref>. A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms [27, 28]. Rocchio's algorithm was developed in the framework of the vector space model [32]. <p> Coefficients have been introduced in Rocchio's formulation which control the contribution of the original query, the relevant articles, and the non-relevant articles to the feedback query. These modifications yield the following query reformulation function <ref> [30] </ref>: ~ Q new = ff ~ Q orig + fi 1 X ~ d fl N R d62Rel This formulation, which was developed for ranking documents after relevance feedback, mainly in interactive settings, has also been used successfully for text filtering.
Reference: [31] <author> Gerard Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill Book Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: These measures are intended to evaluate the ranking effectiveness of a system <ref> [31] </ref>, not its filtering effectiveness.
Reference: [32] <author> Gerard Salton, A. Wong, and C.S. Yang. </author> <title> A vector space model for information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 18(11) </volume> <pages> 613-620, </pages> <month> November </month> <year> 1975. </year>
Reference-contexts: A feedback query creation algorithm developed by Rocchio in the mid-1960's has, over the years, proven to be one of the best relevance feedback algorithms [27, 28]. Rocchio's algorithm was developed in the framework of the vector space model <ref> [32] </ref>. When documents are to be ranked for a query, an ideal query should rank all the relevant documents above all non-relevant documents.
Reference: [33] <author> Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference, </booktitle> <year> 1997. </year>
Reference-contexts: See Schapire et al. <ref> [33] </ref> for further discussion. document. Rocchio calls this an optimal query ([28], page 315).
Reference: [34] <author> Amit Singhal. </author> <title> AT&T at TREC-6. </title> <booktitle> In Proceedings of the Sixth Text REtrieval Conference (TREC-6), 1998 (to appear). </booktitle>
Reference-contexts: The test documents are Lnu weighted. If a test document has a similarity higher than the threshold to a feedback query (classifier), then it is classified as relevant, other wise non-relevant. The above algorithm is quite similar to the routing algorithm used in <ref> [34] </ref> except for the following differences: * Since the user query is not being used in any way, the relevant centroid is used to form the query zone.
Reference: [35] <author> Amit Singhal, Chris Buckley, and Mandar Mitra. </author> <title> Pivoted document length normalization. </title> <booktitle> In Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-29. </pages> <institution> Association for Computing Machinery, </institution> <address> New York, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering. In recent years, the IR community has proposed several modifications to Rocchio's algorithm that have vastly improved the performance of this algorithm: better term weighting <ref> [26, 35] </ref>, query-zoning [36], and dynamic feedback optimization [6] being the three most notable improvements. In this study, we adapt a state of the art Rocchio's algorithm for the text filtering task, and compare it to a fairly new ML algorithm called boosting. <p> Several techniques are known to improve the effectiveness of Rocchio's method. The three new developments that have been quite effective in conjunction with Rocchio's algorithm are: 1. Better Term Weights: A much better understanding of term weights has been developed in the IR community in recent years <ref> [26, 35] </ref>. Better term weights in the training documents yield a better Rocchio query. A better Rocchio query along with better term weights for the test documents yields much improved scores (i.e. better rank ing) for the test documents. 2.
Reference: [36] <author> Amit Singhal, Mandar Mitra, and Chris Buckley. </author> <title> Learning routing queries in a query zone. </title> <booktitle> In Proceedings of the Twentieth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 25-32, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering. In recent years, the IR community has proposed several modifications to Rocchio's algorithm that have vastly improved the performance of this algorithm: better term weighting [26, 35], query-zoning <ref> [36] </ref>, and dynamic feedback optimization [6] being the three most notable improvements. In this study, we adapt a state of the art Rocchio's algorithm for the text filtering task, and compare it to a fairly new ML algorithm called boosting. <p> Better term weights in the training documents yield a better Rocchio query. A better Rocchio query along with better term weights for the test documents yields much improved scores (i.e. better rank ing) for the test documents. 2. Query Zoning: Recently Singhal et al. <ref> [36] </ref> have proposed that only a selected set of non-relevant documents that have some relationship to a user's interest should be used in Rocchio's method. They proposed sampling of the non-relevant documents to form a query zone. 3. <p> We use all these techniques in our version of Rocchio. Since there is no initial user-query in a text filtering scenario, the first factor in Eq. (3), ff ~ Q orig , is not used in our system. Also, when using query zones, Singhal et al. <ref> [36] </ref> have shown that fi = fl in Eq. (3) is a reasonable parameter setting. Therefore for text filtering, we are back to using the original Rocchio formulation of Eq. (2) instead of Eq. (3). <p> In previous work on query zones, the initial user query was used for this purpose <ref> [36] </ref>. * The query zone size (MAX ( N 100 ; R)) grows with the class size for classes that have more than N 100 training relevant documents. * DFO is also modified to rank more documents for very large classes (MAX (500; 5R)). * Word cooccurrence pairs are not used. <p> We therefore used a subset of the training collection as a training set. We selected the top 10; 000 documents retrieved from the training collection by a query learned using Rocchio's method (following the idea of query-zoning as in <ref> [36] </ref>). In addition, we added all relevant documents not retrieved by the above procedure to the training set. We also applied the same procedure to the collection of test documents.
Reference: [37] <author> C. J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Van Rijsbergen's F-measures: Used in [20, 22, 8, 39] to evaluate filtering, this is a single valued measure that depends upon the relative importance a user assigns to recall and precision (see <ref> [37] </ref>, pp. 168-176). The main drawbacks of this measure are that its value is not directly interpretable by a user, and it is usually hard for a user to judge the relative importance of recall and precision.
Reference: [38] <author> E. M. Voorhees and D. K. Harman. </author> <title> Overview of the sixth Text REtrieval Conference (TREC-6). </title> <editor> In E. M. Voorhees and D. K. Har-man, editors, </editor> <booktitle> Proceedings of the Sixth Text REtrieval Conference (TREC-6), 1998 (to appear). </booktitle>
Reference-contexts: Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing <ref> [38] </ref>), as a comparison baseline for their classifiers. However, most such studies use a weak version of Rocchio's algorithm, not well-suited for text filtering.
Reference: [39] <author> Yimin Yang. </author> <title> An evaluation of statistical approaches to text categorization. </title> <type> Technical Report CMU-CS-97-127, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: One measure that is frequently used in doing cross-system comparisons is the recall-precision break-even point. Proposed by Lewis in [17], this measure has been the measure of choice in many studies on text filtering <ref> [17, 21, 8, 24, 39, 23, 15] </ref>. Roughly speaking, break-even point is the point at which recall of a filtering system is the same as its precision. <p> Even though the filtering effectiveness of a system is related to its ranking effectiveness, this relationship is not strong enough to use ranking evaluation measures to evaluate text filtering. * Van Rijsbergen's F-measures: Used in <ref> [20, 22, 8, 39] </ref> to evaluate filtering, this is a single valued measure that depends upon the relative importance a user assigns to recall and precision (see [37], pp. 168-176). <p> This result contradicts the claims made in several previous studies <ref> [22, 8, 39, 15] </ref> that infer that Rocchio's method is inferior to state of the art machine learning algorithms.
Reference: [40] <author> Yiming Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. </title> <booktitle> In Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 13-22, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness.
Reference: [41] <author> Yiming Yang. </author> <title> Noise reduction in a statistical approach to text categorization. </title> <booktitle> In Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 256-263, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The problem of text filtering has been studied in two different communities machine learning (ML) and information retrieval (IR). Many algorithms for text filtering have been proposed and evaluated in the past, for example, Bayesian classifiers, k nearest neighbors, neural networks, rule-learning algorithms, and many more <ref> [17, 20, 40, 2, 41, 14, 22, 8, 24] </ref>. Most studies use Rocchio's method [28], a well known algorithm in the IR community (traditionally used for relevance feedback and more recently for document routing [38]), as a comparison baseline for their classifiers. <p> Some other measures that have been used to evaluate text filtering are: * Average precision, or precision at a fixed rank cutoff: Many studies have used one of these measures to evaluate filtering effectiveness <ref> [2, 40, 41, 22, 1, 7] </ref>. These measures are intended to evaluate the ranking effectiveness of a system [31], not its filtering effectiveness.
References-found: 41

