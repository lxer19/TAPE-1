URL: http://www.cs.toronto.edu/~paullu/Papers/impl.sb.coots97.fixed.ps.Z
Refering-URL: http://www.cs.toronto.edu/~paullu/cvpublications.html
Root-URL: 
Email: paullu@sys.utoronto.ca  
Title: Implementing Optimized Distributed Data Sharing Using Scoped Behaviour and a Class Library good performance of
Author: Paul Lu 
Note: The  
Address: 10 King's College Road Toronto, Ontario, M5S 3G4 Canada  
Affiliation: Dept. of Computer Science University of Toronto  
Abstract: Sometimes, it is desirable to alter or optimize the be-haviour of an object according to the needs of a specific portion of the source code (i.e., context), such as a particular loop or phase. One technique to support this form of optimization flexibility is a novel approach called scoped behaviour. Scoped behaviour allows the programmer to incrementally tune applications on a per-object and per-context basis within standard C++. We explore the use of scoped behaviour in the implementation of the Aurora distributed shared data (DSD) system. In Aurora, the programmer uses scoped be-haviour as the interface to various data sharing optimizations. We detail how a class library implements the basic data sharing functionality and how scoped behaviour coordinates the compile-time and run-time interaction between classes to implement the optimizations. We also explore how the library can be expanded with new classes and new optimization behaviours. 
Abstract-found: 1
Intro-found: 1
Reference: [ACD + 96] <author> C. Amza, A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: On distributed-memory platforms, the lack of hardware support to directly access remote memories has prompted a variety of software-based, logically-shared systems. Broadly speaking, there are distributed shared memory (DSM) <ref> [Li88, BCZ90, ACD + 96] </ref> and distributed shared data (DSD) [BKT92, SGZ93, JKW95] systems. Support for distributed data sharing, whether it is page-based as with DSM, or object-based (or region-based) as with DSD, is an active area of research. <p> Since Ivy [Li88], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems (for example, <ref> [BCZ90, BKT92, BZS93, SGZ93, JKW95, ACD + 96] </ref> ). Related work in parallel array classes (for example, [LQ92]) has also addressed the basic problem of transparently sharing data. Different access patterns on shared data can be optimized through type-specific protocols and run-time annotations. <p> The data in a PSR is always accessed using C-style pointers, which is efficient, but it does not allow the system to selectively intervene in data accesses. Lastly, Aurora supports multiple writers to the same distributed vector object, which can be important for performance <ref> [ACD + 96] </ref>, while PSRs only allow a single writer. 10 Concluding Remarks Researchers have explored a variety of different implementation techniques for DSM and DSD systems.
Reference: [AG96] <author> S.V. Adve and K. Gharachorloo. </author> <title> Shared Memory Consistency Models: A Tutorial. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 66-76, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Now, for example, if a shared vector is updated in a loop and if the updates do not need to be performed immediately, then the loop can use release consistency <ref> [GLL + 90, AG96] </ref> and batch the writes (see Figure 1 (b), shown side-by-side for easy comparison).
Reference: [BCZ90] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Proc. 1990 Conference on Principles and Practice of Parallel Programming. </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: On distributed-memory platforms, the lack of hardware support to directly access remote memories has prompted a variety of software-based, logically-shared systems. Broadly speaking, there are distributed shared memory (DSM) <ref> [Li88, BCZ90, ACD + 96] </ref> and distributed shared data (DSD) [BKT92, SGZ93, JKW95] systems. Support for distributed data sharing, whether it is page-based as with DSM, or object-based (or region-based) as with DSD, is an active area of research. <p> Since Ivy [Li88], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems (for example, <ref> [BCZ90, BKT92, BZS93, SGZ93, JKW95, ACD + 96] </ref> ). Related work in parallel array classes (for example, [LQ92]) has also addressed the basic problem of transparently sharing data. Different access patterns on shared data can be optimized through type-specific protocols and run-time annotations. <p> Related work in parallel array classes (for example, [LQ92]) has also addressed the basic problem of transparently sharing data. Different access patterns on shared data can be optimized through type-specific protocols and run-time annotations. Both Munin <ref> [BCZ90] </ref> and Blizzard [FLR + 94] provide protocols customized to specific data sharing behaviours. Run-time libraries, such as shared regions [SGZ93], SAM [SL94], and CRL [JKW95], associate 12 coherence actions with access annotations (i.e., function calls).
Reference: [BKT92] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: On distributed-memory platforms, the lack of hardware support to directly access remote memories has prompted a variety of software-based, logically-shared systems. Broadly speaking, there are distributed shared memory (DSM) [Li88, BCZ90, ACD + 96] and distributed shared data (DSD) <ref> [BKT92, SGZ93, JKW95] </ref> systems. Support for distributed data sharing, whether it is page-based as with DSM, or object-based (or region-based) as with DSD, is an active area of research. <p> Since Ivy [Li88], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems (for example, <ref> [BCZ90, BKT92, BZS93, SGZ93, JKW95, ACD + 96] </ref> ). Related work in parallel array classes (for example, [LQ92]) has also addressed the basic problem of transparently sharing data. Different access patterns on shared data can be optimized through type-specific protocols and run-time annotations.
Reference: [Boo91] <author> G. Booch. </author> <title> Object-Oriented Design with Applications. </title> <address> Benjamin/Cummings, </address> <year> 1991. </year>
Reference-contexts: In general, the application programmer is only expected to use the classes with a single template argument for the data element type (labelled User in Figure 5 and highlighted in gray). These classes hide the more com 2 The notation is based on Booch <ref> [Boo91] </ref>, but with some simplifications and changes to better suit this presentation. plex templating and class hierarchy considerations that the System must deal with. For data sharing using immediate access, the important classes are GSHandle and GVHandle (shown inside the box in Figure 5).
Reference: [BZS93] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Proc. 38th IEEE International Computer Conference (COMPCON Spring'93), </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Since Ivy [Li88], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems (for example, <ref> [BCZ90, BKT92, BZS93, SGZ93, JKW95, ACD + 96] </ref> ). Related work in parallel array classes (for example, [LQ92]) has also addressed the basic problem of transparently sharing data. Different access patterns on shared data can be optimized through type-specific protocols and run-time annotations.
Reference: [Cop92] <author> J.O. Coplien. </author> <title> Advanced C++: Programming Styles and Idioms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: For the system and class designer, scoped behaviour is an interface between collaborating classes that changes the implementation of the selected methods. Some of the ideas behind scoped behaviour have been explored as part of the handle-body and envelope-letter idioms in object-oriented programming <ref> [Cop92] </ref> (to be discussed 4 (a) Scoped Behaviour Macro #define NewBehaviour ( XX, YY, ZZ ) n // Macro provided by aurora.H GPortal&lt;GVector<ZZ&gt; &gt; AU ## XX ( XX ); n YY&lt;ZZ&gt; XX ( AU ## XX ); template &lt;class C OrigHandle&gt; // Class template provided by aurora.H class GPortal - <p> By design, these classes collaborate to support scoped behaviour. 6 6.1 Handle-Body Composite Objects The main architectural feature of the shared-data class library is the use of the handle-body idiom to create composite objects <ref> [Cop92, OEPW96] </ref> for shared data (Figure 4). The handle object defines the programmer's interface to the shared data. The body object (or objects) contain the actual data. The extra level of indirection afforded by a composite handle-body approach allows for: 1. Data distribution. <p> The creates-a relationship exists when at least one of the methods of a class returns an object of another class. For example, an overloaded subscript operator (i.e., operator []) can return an object which encodes information about a specific vector element <ref> [Cop92] </ref>. We can also distinguish the classes by the way they are, or are not, templated. Class GHandle is not templated in order to simplify the implementation of mechanisms that only require limited functionality from a handle.
Reference: [DGLS93] <author> N.E. Doss, W.D. Gropp, E. Lusk, and A. </author> <note> Skjellum. </note>
Reference-contexts: The distributed-memory platform used for these experiments is a cluster of PowerPC 604 workstations with 133 MHz CPUs, 96 MB of main memory, and a single, non-switched 100 Mbit/s Fast Ethernet network. The software includes IBM's AIX 4.1 operating system, AIX's pthreads, and the MPICH (version 1.0.13) <ref> [DGLS93] </ref> implementation of MPI. Two trends can be noted in the performance results. First, for these three programs, additional processors improves speedup, albeit with diminishing returns. Second, as the size of the data set increases, the overall granularity of work, and thus speedup, also increases.
References-found: 8

