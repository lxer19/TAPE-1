URL: ftp://ftp.research.microsoft.com/users/lomet/pub/dis-indx.ps
Refering-URL: http://www.research.microsoft.com/users/lomet/pub/default.htm
Root-URL: http://www.research.microsoft.com
Email: lomet@microsoft.com  
Title: Replicated Indexes for Distributed Data  
Author: David Lomet 
Address: One Microsoft Way, Bldg. 9 Redmond, WA 98052  
Abstract: We describe a distributed index structure, in which data is distributed among multiple sites and indexes to the data are replicated over multiple sites. This permits good scalability as storage and accessing load are distributed over the sites and each site with an index replica has fast local access to the index structure, making remote requests at most for data at the leaves of the index tree. We call our method the dPi-tree because it is based on the Pi-tree. We replicate the index without the need for coherence messages. This works whether the index replica is persistent or a transient cached copy. We generalize a technique first used to provide recovery for Pi-tree indexes to independently and lazily maintain the index replicas. A further result is that each index replica is fully recoverable, an area not treated previously in replication schemes. We also show how the data in the leaves of the index can be distributed and re-distributed at very low cost. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Carey,M., DeWitt, D., Richardson, J., and Shekita, E. </author> <title> Object and File Management in the EXODUS Extensible Database System. </title> <booktitle> Proc. Very Large Databases Conf.(Sept. </booktitle> <year> 1986) </year> <month> 91-100. </month>
Reference-contexts: In this way, the new replica receives a batch of information about accessing the underlying data. The new replica is a partial one, sharing some of the index structure of the basis. This is similar to the technique used in Exodus <ref> [1] </ref> for creating a new version, but in this case, the intent is to optimize creation of the index replica. Thus, instead of cloning the path to an updated data node, as done in Exodus, we clone paths as we proceed through the tree doing searches.
Reference: [2] <author> Devine, R. </author> <title> Design and Implementation of DDH: A Distributed Dynamic Hashing Algorithm. </title> <booktitle> 4th Int'l Conf. on Foundations of Data Organization and Algorithms. </booktitle> <address> (Oct. 1993) Evanston, IL </address>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> This work attacks the cost of maintaining distributed and replicated search structures. The work of <ref> [9, 2, 13] </ref> is targeted at hashing structures.
Reference: [3] <author> Evangelidis, G., Lomet, D., and Salzberg, B. </author> <title> The hB-Pi-Tree: A Modified hB-tree Supporting Concurrency, Recovery, and Node Consolidation. </title> <booktitle> Proc. Very Large Databases Conf.(Sept. </booktitle> <address> 1995) Zurich, </address> <publisher> Switz. </publisher> <pages> 551-561. </pages>
Reference-contexts: This precludes the direct application of our techniques to spatial search using the R-tree [5]. We feel, however, that the hB-Pi-tree <ref> [3] </ref> is a more robust search structure in any event, and our techniques apply immediately to it. Acknowledgments Witold Litwin provided a valuable critical reading of an earlier version of this paper.
Reference: [4] <author> Gray, J. and Reuter, A. </author> <title> Transaction Processing: </title> <publisher> Concepts and Techniques Morgan Kaufmann (1993) San Mateo, </publisher> <address> CA </address>
Reference-contexts: This can most easily be handled by the Pi-tree technique itself [11] but other techniques are possible so long as side index terms are maintained <ref> [4, 11, 12] </ref>. There is no guarantee to the clients that any data node that it may have read will remain valid while the client caches it (in the traditional way).
Reference: [5] <author> Guttman, A. R-trees: </author> <title> A Dynamic Index Structure for Spatial Searching. </title> <booktitle> Proc. ACM SIGMOD Conf.(May 1984) Boston, </booktitle> <address> MA 47-57. </address>
Reference-contexts: What we do require is that the search space for which a node is responsible not increase since we deal with node deletes in a rather special and less efficient fashion. This precludes the direct application of our techniques to spatial search using the R-tree <ref> [5] </ref>. We feel, however, that the hB-Pi-tree [3] is a more robust search structure in any event, and our techniques apply immediately to it. Acknowledgments Witold Litwin provided a valuable critical reading of an earlier version of this paper.
Reference: [6] <author> Johnson, T., and Krishna, P. </author> <title> Lazy Updates for Distributed Search Structure. </title> <booktitle> Proc. ACM SIGMOD Conf.(May 1993) Washington, D.C. </booktitle> <pages> 337-346. </pages>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. The DRT of [7] is a distributed (not replicated) binary tree. 1.2 Our Approach Our system and communication models are similar to <ref> [6] </ref>, and our search structure shares some structural and operation characteristics with their proposed dB-tree which is based on the B-link tree of [8]. Like the distributed DRT tree and RPs* of [10], we employ a form of "correction message". <p> And we do this with no need to keep the index replicas coherent, even eventually, and hence we can dispense with messages used to maintain coherent index structures <ref> [6] </ref>. 3 Lazy Update of Replicated Indexes This section describes how we deal with data node splitting which results, of course, from data being inserted into a leaf of the index structure. We begin with a primitive technique and proceed to a more effective but still very simple technique. <p> We do not treat node consolidation (deletion) here, which we see as decidedly secondary. This is left to section 4. We assume that the data that is being indexed is not itself replicated. We are only concerned, as was <ref> [6] </ref>, with index replication, by which we mean "interior nodes", not leaf nodes. How data (leaf nodes) might be distributed among several processors of a distributed system is also discussed in section 4. Data distribution is orthogonal to the handling of the indexing structure and its replication.
Reference: [7] <author> Kroll, B. and Widmayer, P. </author> <title> Distributing a Search Tree Among a Growing Number of Processors. </title> <booktitle> Proc. ACM SIGMOD Conf.(May, </booktitle> <address> 1994) Minneapolis, MN 265-276. </address>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. The DRT of <ref> [7] </ref> is a distributed (not replicated) binary tree. 1.2 Our Approach Our system and communication models are similar to [6], and our search structure shares some structural and operation characteristics with their proposed dB-tree which is based on the B-link tree of [8]. <p> This is the same strategy as used with Pi-trees to ensure that interrupted splits eventually post an index term to the parent node. It is not unlike the correction messages of <ref> [7] </ref>, but here such messages are always in response to explicit access requests. Each index replica can maintain itself based only on this information, and hence index replicas need not exchange messages to ensure structure convergence. Such convergence is not required. <p> Thus, it only requires modest access activity from an index site for the index maintained there to provide very effective access to the underlying data. Note also, that unlike <ref> [7] </ref>, a tree merge is not required. Rather, the path returned is appended to the existing replica by replacing a remote reference to a reference to a local copy of the path. Also, we deal easily with the paths that are missing from the replica as the replica is built.
Reference: [8] <author> Lehman, P., and Yao, B. </author> <title> Efficient Locking for Concurrent Operations on B-trees. </title> <journal> ACM Trans. </journal> <note> on Database Systems 6,4 (Dec. </note> <year> 1981) </year> <month> 650-670. </month>
Reference-contexts: The DRT of [7] is a distributed (not replicated) binary tree. 1.2 Our Approach Our system and communication models are similar to [6], and our search structure shares some structural and operation characteristics with their proposed dB-tree which is based on the B-link tree of <ref> [8] </ref>. Like the distributed DRT tree and RPs* of [10], we employ a form of "correction message". DRT, RPs*, and dB-tree all use lazy techniques to maintain distributed index trees over multiple processors. We call our search structure the dPi-tree as it is a derivative of the Pi-tree of [11]. <p> It was described then as a generalization of the Blink-tree <ref> [8] </ref> in that both search structures have side pointers connecting nodes at the same level on the search tree that are used to permit tree re-structuring that separate node splitting from the posting of the index term for the new node (see Figure 1.).
Reference: [9] <author> Litwin, W., Neimat, M-A, and Schneider, D. </author> <title> Linear Hashing for Distributed Files. </title> <booktitle> Proc. ACM SIGMOD Conf.(May 1993) Washington, D.C. </booktitle> <pages> 327-336. </pages>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> This work attacks the cost of maintaining distributed and replicated search structures. The work of <ref> [9, 2, 13] </ref> is targeted at hashing structures.
Reference: [10] <author> Litwin, W., Neimat, M-A, and Schneider, D. </author> <title> RP*: A Family of Order-Preserving Scaleable Distributed Data Structures. </title> <booktitle> Proc. Very Large Databases Conf.(Sept. 1994) Santiago, </booktitle> <address> Chile </address>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> Like the distributed DRT tree and RPs* of <ref> [10] </ref>, we employ a form of "correction message". DRT, RPs*, and dB-tree all use lazy techniques to maintain distributed index trees over multiple processors. We call our search structure the dPi-tree as it is a derivative of the Pi-tree of [11].
Reference: [11] <author> Lomet, D. and Salzberg, B. </author> <title> Access Method Concur-rency with Recovery. </title> <booktitle> Proc. ACM SIGMOD Conf.(May 1992) San Diego, </booktitle> <address> CA 351-360. </address>
Reference-contexts: Like the distributed DRT tree and RPs* of [10], we employ a form of "correction message". DRT, RPs*, and dB-tree all use lazy techniques to maintain distributed index trees over multiple processors. We call our search structure the dPi-tree as it is a derivative of the Pi-tree of <ref> [11] </ref>. There, a correction mechanism was used to enable high concurrency and robust recovery for a centrally stored tree. When dealing with distributed and replicated trees, the correction mechanism requires messages, which we send lazily, and only when other information is requested. <p> When dealing with distributed and replicated trees, the correction mechanism requires messages, which we send lazily, and only when other information is requested. Correction is triggered in the same way as index recovery is triggered in <ref> [11] </ref>, via detection of a misdirected search. What distinguishes our techniques are their great simplicity and low cost, enabled by the extremely lazy way that we handle index structure maintenance. Unlike the dB-tree or an RP* tree, the dPi-tree does not require convergence of index replica node structure. <p> It also exploits a very lazy approach to information propagation. We conclude in section 5 with a discussion of what has been accomplished and what might further be done. 2 Pi-Trees 2.1 Pi-tree Structure The Pi-tree was introduced in <ref> [11] </ref> to provide a high concurrency index tree that also supported recovery. <p> This can most easily be handled by the Pi-tree technique itself <ref> [11] </ref> but other techniques are possible so long as side index terms are maintained [4, 11, 12]. There is no guarantee to the clients that any data node that it may have read will remain valid while the client caches it (in the traditional way). <p> This can most easily be handled by the Pi-tree technique itself [11] but other techniques are possible so long as side index terms are maintained <ref> [4, 11, 12] </ref>. There is no guarantee to the clients that any data node that it may have read will remain valid while the client caches it (in the traditional way). <p> Hence, no coherence strategy of any sort is required for them. 5.2 Concurrency and Recovery Each replica supports high concurrency and recovery at its site, an issue not addressed previously. Index concurrency and recovery can be handled at a single site by means of the Pi-tree algorithm <ref> [11] </ref>. Multi-site coordination is not needed. Index node locking is needed only to provide synchronization among local updates to a replica. <p> one level of the tree is not dangling when used to access the next level. 5.5 Generality The laziness of our method is enabled by the fact that Pi-trees maintain not only a side pointer to sibling nodes, but an entire side index term with both space description and pointer <ref> [11] </ref>. It is this index term that provides the source for lazy updating of index replicas as all replicas eventually store this index term.
Reference: [12] <author> Mohan, C. and Levine, F. ARIES/IM: </author> <title> An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging. </title> <booktitle> Proc. ACM SIGMOD Conf.(May 1992) San Diego, </booktitle> <address> CA 371-380. </address>
Reference-contexts: This can most easily be handled by the Pi-tree technique itself [11] but other techniques are possible so long as side index terms are maintained <ref> [4, 11, 12] </ref>. There is no guarantee to the clients that any data node that it may have read will remain valid while the client caches it (in the traditional way).
Reference: [13] <author> Vingralek, R., Breitbart, Y., and Weikum, G. </author> <title> Distributed File Organization with Scaleable Cost/Performance. </title> <booktitle> Proc. ACM SIGMOD Conf.(May, </booktitle> <address> 1994) Minneapolis, MN 253-264. </address>
Reference-contexts: What search structure permits low cost maintenance when distributed over several processors? What search structure permits several replicas of its index to be easily and inexpensively maintained? There has been recent interesting work on distributed and/or replicated search structures <ref> [2, 6, 7, 9, 10, 13] </ref>. This work attacks the cost of maintaining distributed and replicated search structures. The work of [9, 2, 13] is targeted at hashing structures. <p> This work attacks the cost of maintaining distributed and replicated search structures. The work of <ref> [9, 2, 13] </ref> is targeted at hashing structures.
References-found: 13

